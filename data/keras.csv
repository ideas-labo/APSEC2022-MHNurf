,Repository,Number,State,Title,Body,Labels,Comments,Codes,Commands,class,related
0,keras,11776,closed,Improve performance (GPU utilization) of `multi_gpu_model` for small models.,"Improve performance (GPU utilization) of  for small models (see [this analysis](https://github.com/rossumai/keras-multi-gpu/blob/master/blog/docs/measurements.md)).

Note created by @fchollet in the ""Requests for contributions"".

",Enhancement stat:contributions welcome,"[""Based on the measurements it seems that  two thinks would help: avoiding copy from python to TF memory (using `tf.Dataset` and possibly with plain prefetch to RAM) and prefetching to GPU RAM asychronously. The first should be now supported fine, for the latter I'm not sure if it supports multi GPU so far.""]",[],['multi_gpu_model'],1,0
1,keras,8707,closed,Good accuracy on test data but bad prediction behaviour with LSTM,[Deleted],,[],[],[],1,0
2,keras,11267,closed,"model.fit very slow validation, more evident with Intel Phi","I have a similar problem of this old [issue](https://github.com/keras-team/keras/issues/7723)(wrongly closed because stale), in all epochs, the validation calculation is slower than the epoch training part.
Both validation and training sets have the same number of examples.

The two phases take the same time with a 18 cores Xeon CPU, but validation takes 4 times the training time with Intel Phi architeclure (and tensorflow MKL binary).

I think/suspect that model evaluation for validation calculation does not take advantage of parallelization/multi-threading. This could be the core of the problem. Please check.

Regards

Please make sure that the boxes below are checked before you submit your issue.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.

Thank you!

- [x ] Check that you are up-to-date with the master branch of Keras. You can update with:


- [x ] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] THE LINKED OLD ISSUE ALREADY HAS IT. Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short). 
",type:support,['I’m encountering the same issue on GPU with a MSE loss.  At least 4x as long to validate 1/10th of the samples used in training. '],[],['pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps'],1,0
3,keras,511,closed,Very low or NaN accuracy in the examples when running on a GPU with float32,"I've been trying all the MNIST examples from the Keras documentation as well as the cifar10 and they just don't work if I use the GPU.

The Accuracy is always below 0.1 or NaN.

If I do it on the CPU it works correctly and if I do it in the GPU with float64 it also works correctly although slowly.

I've tried to restart the laptop |(like another user suggests) and also cleaning and purging theano-cache, but the problem remains.

My system is a Mac OS X 10.10. With a GTX 750M.

Theano without Keras works properly in GPU float32 with other libraries/examples.
",,"['All these examples work fine on my GT650M on OSX. I guess this is a 750M issue.\n\nTry lowering the `epsilon` value for the GPU in `optimizers.py` and tell me what you get.\n', 'I tried to change the `epsilon` for several values ranging from 1e-6 to 1e-20 in `optimizers.py` and the bug remains the same.\n\nAbout being a 750M issue, I doubt it because like said, this sort of operations work fine directly in Theano, or using Lasagne or even an handmade library. The accuracy problem only arrises when using Keras.\n', ""> About being a 750M issue, I doubt it because like said, this sort of operations work fine directly in Theano, or using Lasagne or even an handmade library. The accuracy problem only arrises when using Keras.\n\nIt's a float32 overflow, and it happens on the 750M but not the 650M for the same code. Surely you can understand this. \n\n> from 1e-6 to 1e-20 in optimizers.py and the bug remains the same.\n\nI realize I wasn't clear. I meant the epsilon in `objectives.py`, and I meant lowering the exponent. Try 10e-5 and 10e-4. You can also try changing the epsilon value in the optimizer you are using, by lowering the exponent, but I believe the one value that is causing the overflow would be the one in objectives.py.\n"", ""Tried changing for `epsilon` to 10e-5 and 10e-4 to no avail. I also constructed a model using `SGD` propagation which doesn't use `epsilon` and it also only works in float64.\n"", ""I have the same issue whenever I try to train GRU or LSTM with categorical_crossentropy and adam, rmsprop or SGD. The network will converge just fine for a while and then suddenly go to NaN.\n\nI've also tried fiddling with epsilon, to no avail. At this point, keras is effectively unusable for me because of it :(. \n"", 'Just as a clarification, I found out that `float64` is actually using the CPU even if you configure Theano to use the GPU. So this is a issue arising when you use GPU with Keras.\n', ""All GPUs only perform `float32` operations. Well I guess you could perform virtual `float64` operations emulated via hardware `float32` operations, but that's not something Theano would do.\n"", 'The interesting thing here is that the issue is specific to certain GPUs, which means the cause of the float32 overflow is unlikely to be found in the Keras code (if the Keras code was at fault, the overflow would happen _every time_ the code is run in float32, i.e. on every GPU).\n', ""But on the other side, if the Keras code was **not** at fault the issue would happen every time you would try something similar with any library that uses Theano. Yet, I've tried similar runs in pure Theano and in Lasagne (another library that uses Theano as the backend like Keras does) and it runs just fine. So it must be something that Keras does and it's not supposed to do, even if it works fine with other cards.\n"", 'Could you pls try [`NanGuardMode`](http://deeplearning.net/software/theano/library/compile/nanguardmode.html#module-nanguardmode) and tell us what you get?\n', ""Where can I change that setting?\nI've tried `NanGuardMode = True` in `.theanorc` and the results remain the same.\n"", 'I ran into this problem as well.\nI had to upgrade scipy to 0.16.0 to fix it (if you are using gensim, that might be problematic though). \n', 'I have scipy 0.16.0 installed and the problem remains.\n\nAn added information, an user in the Keras mailing list thread about this problem says he also has it and that he uses a Titan X.\n', ""> But on the other side, if the Keras code was not at fault the issue would happen every time you would try something similar with any library that uses Theano.\n\nThis is most likely due to a specific feature of Theano being used by Keras and which doesn't play well with this specific GPU (Titan X). You can still run Keras on any other GPU, or use other Theano features on the Titan X.\n\nIt would be interesting to determine which feature is at fault, specifically. Then we can open an issue with the Theano devs.\n\nIn any case, since this problem is GPU-specific and since Keras does not have any GPU-specific code (it simply calls Theano as its computation backend), the issue definitely lies with Theano.\n"", ""@morgado-developer I have had the same issue with Keras code. I replaced the get_output() function for Convolutional2D class with the code below and it solved the issue. If you are using cuDNN make sure to remove it completely (by deleting all the *.so files and cudnn.h file from cuda installation) because theano uses cuDNN by default, if present, and set optimizer_including=conv_gemm in the THEANO_FLAGS. Please let me know if the fix works for you - \n\n``` python\ndef get_output(self, train):\n        X = self.get_input(train)\n        border_mode = self.border_mode\n        if border_mode == 'same':\n            border_mode = 'full'\n#         pdb.set_trace()\n        X = gpu_contiguous(X)\n        conv_out = theano.sandbox.cuda.blas.GpuCorrMM(border_mode=border_mode,\\\n                subsample=self.subsample)\\\n                (X, self.W)\n#         conv_out = theano.tensor.nnet.conv.conv2d(X, self.W,\n#             border_mode=border_mode, subsample=self.subsample)\n\n        if self.border_mode == 'same':\n            shift_x = (self.nb_row - 1) // 2\n            shift_y = (self.nb_col - 1) // 2\n            conv_out = conv_out[:, :, shift_x:X.shape[2] + shift_x, shift_y:X.shape[3] + shift_y]\n\n        return self.activation(conv_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n```\n"", 'just set optimizer_excluding=cudnn would also work. but it seems like a major setback if cudnn cannot be used\n', '> just set optimizer_excluding=cudnn would also work. but it seems like a major setback if cudnn cannot be used\n\nThis recent change might have helped, check it out: https://github.com/fchollet/keras/blob/master/keras/layers/convolutional.py#L152-L164\n', 'I had this problem and solved it by getting the latest Theano from github. I am using cuDNN v3 with a Titan X on Win8. \n', 'great, upgrading to the github theano solved the problem\n', 'So, everyone confirms that this issue is definitely solved with the latest Theano and latest Keras? \n', ""I had gotten bleeding edge keras and theano 2 weeks ago to no available, \nhaven't tried since.\n\nOn 27 August 2015 23:01:01 François Chollet notifications@github.com wrote:\n\n> So, everyone confirms that this issue is definitely solved with the latest \n> Theano and latest Keras?\n> \n> ---\n> \n> Reply to this email directly or view it on GitHub:\n> https://github.com/fchollet/keras/issues/511#issuecomment-135553852\n"", 'I am using keras  2c30d503eada5cb5429b6f6d8ced0e996760e40e (github on august 27th) and theano 0.7.0.dev-856aa0b6d3454ff1b4d00575e1ec38f27aedb7d9.\n', ""Just tried with keras head `332d43e023073561fec53828ee21e206ac1b34b1` and theano head '0.7.0.dev-dc13bfcaa165b0d2d24ec509944da9f29114470b' using `python3.4 mnist_cnn.py` on a Tesla K40m prints a nan loss. The when using cpu configuration works fine.\n"", 'Just for the fun, also tried with python2.7 (instead of python3.4) and there I have the same nan behaviour.\n', 'Just adding some more evidence for this.\n\nI am using an iMac with GTX 980M and was seeing the same problem. I was running examples/addition_rnn.py and it never converges on GPU but did on CPU.\n\nRunning with optimizer_excluding=cudnn fixes the issue. E.g. \nTHEANO_FLAGS=device=gpu,floatX=float32,cuda.root=/usr/local/cuda,optimizer_excluding=cudnn python examples/addition_rnn.py\n\nI also updated to the bleeding edge theano as suggested on this thread:\npip install --upgrade --no-deps git+git://github.com/Theano/Theano.git\n\nAnd this now works fine. I.e. the original failing version now works correctly, as reported by others on this thread.\nTHEANO_FLAGS=device=gpu,floatX=float32,cuda.root=/usr/local/cuda python examples/addition_rnn.py\n', ""Excellent. We'll consider this resolved then.\n\nI believe the NaN loss on GPU reported by @rodrigob is a different issue (which we also encountered in the past). We'll resolve it separately.\n"", 'Hi all, just ran into this bug (or a very similar bug) running 0.1.3, updating to the current master (c18a9cd405f29040ebb259aef74963c4b2134494) has fixed this problem for me.\n', 'Hi all. I fixed the problem by replacing the layer ReLU as SOFTPLUS. Maybe, you can try it. \nI use the Bleeding Edge version of Theano and the latest version of Kereas.\n', 'Issues still remains, although disabling cudnn optimisation:\n\nCode: mnist_cnn.py with SGD\nGraphics: GTX980 Ti\n\n.thenorc:\n[global]\nfloatX=float32\noptimizer_excluding=cudnn \n\n[lib]\ncnmem=0.7\n\n[nvcc]\nfastmath=True\n\nusing bleeding edge theano and keras as well as with keras (20dc637)\n']",[],[],1,0
4,keras,12812,closed,"Validation loss >> train loss, same data","Training a GRU autoencoder on EEG data, I discover the model predicts lot more poorly than train loss (mse) suggests; _evaluate_, _test_on_batch_, _predict_ with hand-coded loss, and _fit_ all agree the loss to be x10 the train values; some results below.

Exploring various predictions, it often seems that the network performs well _except_ for a bias (offset). Aside this, I'm without a clue to the cause underlying the discrepancy; disabling dropout & batch norm doesn't help.

Any clues? Help is appreciated.

<hr>
<strong>Additional details</strong>:<br>

 - CuDNNGRU stateful implementation, TensorFlow backend <br>
 - Layers:  --  -- <br>
 - Output: <br>
 -  for all layers<br>
 -  +  at (=after) input,  at encoder,  at latent<br>
 -  between encoder and latent, latent and decoder<br>
 -  - 25 *separate*, 10-min sequences fed 400 timesteps
 (=1 sec) at a time (as 10*60=600 'windows' in parallel, non-shuffled)
 -   applied before testing on new x25 10-min sequences
 -  for training, but results don't differ from 
- Keras 2.2.4, Python 3.6, Spyder 3.3.4 via Anaconda

<hr><strong>Results</strong>

    model.fit(x,x,validation_data=(x,x))

    25/25 [==============================] - 1s 42ms/step - loss: 0.1780 - val_loss: 2.9175
    25/25 [==============================] - 1s 39ms/step - loss: 0.1794 - val_loss: 2.9380

[![enter image description here][1]][1]




  [1]: https://i.stack.imgur.com/1nYU5.png",,[],"['(300,80,150)', '(encoder, latent, decoder)', '(selu, tanh, tanh)', ""TimeDistributed(Dense(units=input_dim, activation='linear'))"", 'return_sequences=True', 'GaussianNoise', 'Dropout', 'AlphaDropout', 'Dropout', 'BatchNormalization', 'batch_size=25, timesteps=400, input_dim=16', 'reset_states()', 'train_on_batch', 'fit']",[],1,0
5,keras,4108,closed,Different behavior with custom loss function.,"Hi,

I wrote a custom loss function that just uses the built-in binary crossentropy loss:



However, the behavior is different when I use this function instead of passing 'binary_crossentropy' to the model's compile method. This function yields worse results. I suspect it might be a numerical problem but I don't know how to fix it.

In [this gist,](https://gist.github.com/EhsanEI/90203acd0026d915699a7cbb9af584a9) I get ~99% validation accuracy on MNIST with the built-in function and ~96% validation accuracy with my_loss after the first epoch. The difference is much bigger on my own dataset.
",stale,"[""I have seen pretty much the same problem. I wrote this function:\n\n``` python\ndef mcc(output, target, from_logits=False):\n    if from_logits:\n        output = T.nnet.softmax(output)\n    else:\n        # scale preds so that the class probas of each sample sum to 1\n        output /= output.sum(axis=-1, keepdims=True)\n\n    # avoid numerical instability with _EPSILON clipping\n    output = T.clip(output, keras.backend.common._EPSILON, 1.0 - keras.backend.common._EPSILON)\n\n    return T.nnet.categorical_crossentropy(output, target)\n```\n\nwhich is a near-exact copy of the function from the keras source, but when I compile with loss=mcc instead of loss='categorical_crossentropy' the loss drops to ~1e-14 within two epochs.\n\nI'm using keras 1.1.0 in an ipython notebook.\n\n**NEVERMIND**\n\nI just discovered that the names of the arguments must by y_true and y_pred. Once I made that change my code started working.\n"", 'I\'m seeing the exact same issue, also with binary_crossentropy.\r\n\r\nI copied the code for the function from https://github.com/fchollet/keras/blob/master/keras/losses.py , added ""my_"" in front \r\n```\r\n\r\ndef my_binary_crossentropy(y_true, y_pred):\r\n    return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)\r\n```\r\n\r\nBut calling my version of the cross-entropy instead of passing the string ""binary_crossentropy"" yields vastly different results.\r\n```\r\n\r\nautoencoder.compile(optimizer=\'adadelta\', loss=\'binary_crossentropy\') # works\r\nautoencoder.compile(optimizer=\'adadelta\', loss=my_binary_crossentropy) # sucks\r\n```\r\n\r\n', ""I'm also seeing the same issue. I guess this is related to #6680"", ""I had exactly the same issue, metrics showing the same value but accuracy in the custom case sucks. It looks like the training is working equally, but the accuracy metric is not working. I could fix it by using\r\n\r\n`def acc(y_true, y_pred):\r\n    return K.mean(K.round(y_pred)*y_true + (1.-K.round(y_pred))*(1.-y_true))`\r\n\r\nas the accuracy (which gives exactly the same as metric=['accuracy'] in the normal case but is not buggy with the custom loss)."", '@NakramR did you ever find a solution?', ""@NakramR @CA4GitHub One possible reason for the problem in the example given above by @NakramR is that the cross-entropy function does not return a mean. It returns a tensor that is averaged later by higher level code to produce a single loss value. I'm not sure, but I believe this is related to using the output from the function for back-propagation."", '@CA4GitHub Yes. I was new to Keras back then and I had mistakenly used this binary-crossentropy loss for a multi-class classification task. IIRC the accuracy metric when paired with binary-crossentropy loss assumed two classes or a different kind of encoding in the labels, so the network was trained well but the reported accuracy was wrong. I might be misremembering some details but this is the overall idea.', ""Guys,\r\n\r\nDid anyoe have a convincing solution to make custom_binarycrossentropy work?\r\n\r\nI tried all possible methods (even making the whole training data size same as the bacth size to eliminate the dependence on global averaging during batch wise processing.). But i see significant difference between my binary cross entropy implementation and the one from keras ( by specifying loss = 'binary_crossentropy')\r\n\r\nMy crustom binary cross entropy code is as follows\r\ndef _loss_tensor(y_true, y_pred):\r\n    y_pred = K.clip(y_pred, _EPSILON, 1.0-_EPSILON)\r\n    out = (y_true * K.log(y_pred) + (1.0 - y_true) * K.log(1.0 - y_pred))\r\n    return -K.mean(out)\r\ndef _loss_tensor2(y_true, y_pred):\r\n    y_pred = K.clip(y_pred, _EPSILON, 1.0-_EPSILON)\r\n    out = -(y_true * K.log(y_pred) + -(1.0 - y_true) * K.log(1.0 - y_pred))\r\n    return out\r\ndef _loss_tensor2(y_true, y_pred):\r\n    loss1 = K.binary_crossentropy(y_true, y_pred)\r\n    return loss1\r\nNone of these methods work. It doesnt work even if i do K.mean() before ir eturn the results from custom loss function. \r\n\r\nI am not able to understand what special does using loss = 'binary_crossentropy' does. When i use my custom loss function , the training sucks and it does work as expected.\r\n\r\nI need my custom loss function to manipulate the loss function depending on the error and penalizing a certain type of classification error more.\r\n\r\n"", '@sreenivasaupadhyaya I think your _loss_tensor2 function should work, but there is a mistake in the equation. It should be\r\n\r\nout = -(y_true * K.log(y_pred) + (1.0 - y_true) * K.log(1.0 - y_pred))\r\n\r\nAnother thing to consider is that y_pred must have passed through a softmax activation layer or otherwise have values that take the form of probabilities before the binary cross-entropy is calculated.\r\n\r\nYour first function will not work correctly because it takes a mean. It seems that the third function should work, so it makes me wonder what form your y_pred tensor has.', '@JimBiardCics  Thanks a lot for your time to solve the issue.\r\nYes, my model is a binary classifier with Dense layer of size 1 with sigmoid activation. Hence the custom loss function input y_pred will be probability values which is necessary for computing the  bianry cross entropy.\r\n\r\nSecondly, The formaula you correction you mentioned (adding a negative sign), is already taken care in my formula but its rather applied separately to the two parts of the calculation like\r\n\r\n"" **-**(y_true * K.log(y_pred) + **-**(1.0 - y_true) * K.log(1.0 - y_pred)) ""\r\n\r\nDo you have any comments for the implementation below:\r\n`def _loss_tensor2(y_true, y_pred):\r\nloss1 = K.binary_crossentropy(y_true, y_pred)\r\nreturn loss1`\r\n\r\nLooking forward to your response.\r\n', ""@sreenivasaupadhyaya It may be correct in your code, but the equation in your second function is not correct. What you wrote is\r\n> out = -(y_true * K.log(y_pred) + -(1.0 - y_true) * K.log(1.0 - y_pred))\r\n\r\nBreaking out with indentation, you can see\r\n\r\nout = -(\r\n        y_true * K.log(y_pred) +\r\n        -(1.0 - y_true) * K.log(1.0 - y_pred)\r\n)\r\n\r\nThere is an excess minus sign on the second term.\r\n\r\nRegarding the _loss_tensor2 function above, I would tend to think that it should work. If it doesn't then there is probably something unexpected in your code. If you'd like to share your code, I'd be willing to take a look at it."", ""@JimBiardCics  Thanks again.\r\nyou were right , indeed i misplaced the - sign when i put the code here.\r\n\r\nHere is the test setup i was using,\r\n\r\n```\r\nimport tensorflow as tf\r\nimport keras\r\nimport keras.backend as K\r\ny_true = tf.convert_to_tensor(np.array([1,1,1,1,1,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0]))\r\ny_true = K.cast(y_true, dtype ='float32')\r\ny_pred1   = tf.convert_to_tensor(np.array([0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.50,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]))\r\ny_pred1 = K.cast(y_pred1, dtype ='float32')\r\nloss_reweight = reweight2(y_true,y_pred)\r\nloss1 = K.binary_crossentropy(y_true,y_pred)\r\nloss2 = _loss_tensor2(y_true,y_pred)\r\nprint('loss reweight is ', loss_reweight)\r\nprint('loss1 is ',loss1)\r\nprint('loss2 is ',loss2)\r\n\r\n\r\n\r\n_EPSILON = K.epsilon()#1e-7\r\ndef _loss_tensor2(y_true, y_pred):\r\n    y_pred = K.clip(y_pred, _EPSILON, 1.0-_EPSILON)\r\n    out = -(y_true * K.log(y_pred) + (1.0 - y_true) * K.log(1.0 - y_pred))\r\n    return out\r\n\r\n```\r\n\r\nI couldnt get the same result as K.binary_crossentropy() using the custom code. The end goal of this exercise was to write a custom loss function to weight (TP,tn,FP and FN) differently.\r\nfinally I got the custom code working and i used the inbuilt K.binary_crossentropy in the custom function. I am sharing the function below\r\n\r\n```\r\ndef reweight2(y_true, y_pred, tp_weight=1.0, tn_weight=1.0, fp_weight=1.0, fn_weight=1.0):\r\n    # Get predictions\r\n    y_true = K.cast(y_true,dtype='float32')\r\n    y_pred = K.cast(y_pred, dtype='float32')\r\n    y_pred_classes = K.greater_equal(y_pred, 0.5)\r\n    y_pred_classes_float = K.cast(y_pred_classes, dtype ='float32')\r\n    # Get misclassified examples\r\n    wrongly_classified = K.not_equal(y_true, y_pred_classes_float)\r\n    wrongly_classified_float = K.cast(wrongly_classified, dtype ='float32')\r\n\r\n    # Get correctly classified examples\r\n    correctly_classified = K.equal(y_true, y_pred_classes_float)\r\n    correctly_classified_float = K.cast(correctly_classified, dtype ='float32')\r\n\r\n    loss = K.binary_crossentropy(y_true,y_pred)\r\n    # Get tp, fp, tn, fn\r\n    tp = tf.math.multiply( y_true , loss )#tf.math.multiply( y_true , K.log(y_pred))\r\n    tp = tf.math.multiply(tp,correctly_classified_float)\r\n\r\n    tn = tf.math.multiply(loss, (1-y_true))#tf.math.multiply((1 - y_true) , K.log(1-y_pred))\r\n    tn = tf.math.multiply(tn,correctly_classified_float)\r\n\r\n    fn = tf.math.multiply(loss,y_true)# tf.math.multiply(y_true , K.log(y_pred))\r\n    fn= tf.math.multiply(fn,wrongly_classified_float)\r\n\r\n\r\n    fp =  tf.math.multiply(loss,(1-y_true))#tf.math.multiply((1 - y_true) , K.log(1-y_pred))\r\n    fp = tf.math.multiply(fp,wrongly_classified_float)\r\n\r\n    # Get weights\r\n    weight_tensor =  tf.math.add_n([tf.math.multiply(tp_weight , tp) , tf.math.multiply(fp_weight , fp) , tf.math.multiply(tn_weight , tn) , tf.math.multiply(fn_weight , fn)])\r\n    return weight_tensor\r\n```\r\n\r\n**some of the type coversions and some lines are redundant/unnecessary. the earlier attempts were to make the function working with formula method and not use inbuilt K.binary_crossentropy!!\r\n\r\n@JimBiardCics  let me know your comments."", ""@sreenivasaupadhyaya Part of the problem may be a mismatch between tensorflow and keras depending on the versions used. There are internal changes (I think) in tensorflow 2.0+ that could cause problems if you try to use independent keras with tensorflow. You should be sure to use the tensorflow version of keras. The other thing to keep in mind is that the result of a function composed of tensorflow objects and operations is a tensor has to be evaluated before you can tell what's going on. The tensorflow.keras.backend.eval() function produces a numpy array from a tensor result. I slightly modified your test code and found that the results from all loss functions were nearly identical. The only difference was for the elements where both y_true and y_pred were zero. There is  a 1.9e-8 difference between those elements of loss1 and loss2. This is due to the slightly different way the binary cross-entropy function is written in tensorflow. You can see the difference at [this link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L4728).\r\n\r\nI added a loss_tensor3 function that calculates the binary cross-entropy the same way tensorflow does. Here's the code that worked for me on my Mac.\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport tensorflow.keras.backend as K\r\n\r\n_EPSILON = K.epsilon()#1e-7\r\n\r\n\r\ndef reweight2(y_true, y_pred, tp_weight=1.0, tn_weight=1.0, fp_weight=1.0, fn_weight=1.0):\r\n    # Get predictions\r\n    y_pred_classes = K.greater_equal(y_pred, 0.5)\r\n    y_pred_classes_float = K.cast(y_pred_classes, dtype ='float32')\r\n\r\n    # Get misclassified examples\r\n    wrongly_classified = K.not_equal(y_true, y_pred_classes_float)\r\n    wrongly_classified_float = K.cast(wrongly_classified, dtype ='float32')\r\n\r\n    # Get correctly classified examples\r\n    correctly_classified = K.equal(y_true, y_pred_classes_float)\r\n    correctly_classified_float = K.cast(correctly_classified, dtype ='float32')\r\n\r\n    loss = K.binary_crossentropy(y_true,y_pred)\r\n\r\n    # Get tp, fp, tn, fn\r\n    tp = tf.math.multiply( y_true , loss )#tf.math.multiply( y_true , K.log(y_pred))\r\n    tp = tf.math.multiply(tp,correctly_classified_float)\r\n\r\n    tn = tf.math.multiply(loss, (1-y_true))#tf.math.multiply((1 - y_true) , K.log(1-y_pred))\r\n    tn = tf.math.multiply(tn,correctly_classified_float)\r\n\r\n    fn = tf.math.multiply(loss,y_true)# tf.math.multiply(y_true , K.log(y_pred))\r\n    fn= tf.math.multiply(fn,wrongly_classified_float)\r\n\r\n\r\n    fp =  tf.math.multiply(loss,(1-y_true))#tf.math.multiply((1 - y_true) , K.log(1-y_pred))\r\n    fp = tf.math.multiply(fp,wrongly_classified_float)\r\n\r\n    # Get weights\r\n    weight_tensor =  tf.math.add_n([tf.math.multiply(tp_weight , tp) , tf.math.multiply(fp_weight , fp) , tf.math.multiply(tn_weight , tn) , tf.math.multiply(fn_weight , fn)])\r\n    return weight_tensor\r\n\r\n\r\ndef _loss_tensor2(y_true, y_pred):\r\n    y_pred = K.clip(y_pred, _EPSILON, 1.0 - _EPSILON)\r\n    out    = -(y_true * K.log(y_pred) + (1.0 - y_true) * K.log(1.0 - y_pred))\r\n\r\n    return out\r\n\r\n\r\ndef _loss_tensor3(y_true, y_pred):\r\n    y_pred = K.clip(y_pred, _EPSILON, 1.0 - _EPSILON)\r\n    out    = -(y_true * K.log(y_pred + _EPSILON) + (1.0 - y_true) * K.log(1.0 - y_pred + _EPSILON))\r\n\r\n    return out\r\n\r\n\r\ny_true = tf.convert_to_tensor(np.array([1,1,1,1,1,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0]))\r\ny_true = K.cast(y_true, dtype =np.float32)\r\n\r\ny_pred = tf.convert_to_tensor(np.array([0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.50,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]))\r\ny_pred = K.cast(y_pred, dtype =np.float32)\r\n\r\nloss_reweight = K.eval(reweight2(y_true,y_pred))\r\n\r\nloss1 = K.eval(K.binary_crossentropy(y_true,y_pred))\r\nloss2 = K.eval(_loss_tensor2(y_true,y_pred))\r\nloss3 = K.eval(_loss_tensor3(y_true,y_pred))\r\n\r\nprint('loss reweight is ', loss_reweight)\r\nprint('\\nloss1 is ',loss1)\r\nprint('\\nloss2 is ',loss2)\r\nprint('\\nloss3 is ',loss3)\r\nprint('\\ndiff loss_reweight - loss1 is ', (loss_reweight - loss1))\r\nprint('\\ndiff loss1 - loss2 is ', (loss1 - loss2))\r\nprint('\\ndiff loss1 - loss3 is ', (loss1 - loss3))\r\nprint('\\ndiff loss2 - loss3 is ', (loss2 - loss3))\r\n```"", ""@JimBiardCics Thanks for trying out. I just repeated your experiment and found similar results.\r\nHowever, when 'K.binary_crossentropy' or '_loss_tensor3' used as loss function in Keras model yields a different result.\r\n\r\nHowever, I achieved what I wanted using the 'reweight2' function. Like you suggested, there might be possible mismatch in keras and tf versions which in turn results in a different way of handling these cases.\r\n\r\n"", ""@JimBiardCics @sreenivasaupadhyaya : I am sorry I am new to ML but did you try something like following? \r\n\r\ndef custom_loss(y_actual, y_predicted):\r\n    return K.mean(K.binary_crossentropy(y_actual, y_predicted), axis=-1)\r\n\r\ndef acc(y_true, y_pred): return K.mean(K.round(y_pred)*y_true + (1.-K.round(y_pred))*(1.-y_true))\r\n\r\nmodel.compile(loss=custom_loss,optimizer=opt,metrics=[acc])\r\n\r\nI think you need to change your metrics because Keras automatically selects which accuracy implementation to use according to the loss, and this won't work if you use a custom loss. ""]","["" python\nfrom keras.objectives import get as get_objective\ndef my_loss(y_true, y_pred):\n    co = get_objective('binary_crossentropy')\n    return co(y_true, y_pred)\n""]",[],1,0
6,keras,4633,closed,Theano memory issues. ,"Hello there, 

I don't have an exact message on me now as I am in the middle of training my model under TF but this is an issue i've had on two setups:

Setup 1: 32Gb RAM, no GPU
Setup 2: p2.xlarge instance - 60GB RAM, 1/2 Tesla K80

Model goes like this: LSTM 2, LSTM16, LSTM8, Dense1 ( but this happens on any lstm/gru network) 
Data is 3GB CSV (4*0.75GB)
Memory usage on Theano is like:
![15321436_1588947471135425_2108629537_o png](https://cloud.githubusercontent.com/assets/9210039/20986714/125561f4-bcca-11e6-9c38-47bfb33ad91b.jpeg)

What happens there roughly is theano invokes g++ and each compilation blows up (OOM)
What happens there with TF, it plateaus on 35GB of memory and runs happily, so even though Setup 1 might be out of question (leave optimization), Setup 2 works like a charm.

   
I think it'd be worth to investigating.
Theano==0.9.0.dev3
Keras==1.1.0
 ",stale,"[""So, you tell that g++ is missing memory, not Theano?\n\nCan you use a different g++ version? It seem a bug in g++, not Theano.\n\nOn Wed, Dec 7, 2016 at 10:16 PM, maxaug <notifications@github.com> wrote:\n\n> Hello there,\n>\n> I don't have an exact message on me now as I am in the middle of training\n> my model under TF but this is an issue i've had on two setups:\n>\n> Setup 1: 32Gb RAM, no GPU\n> Setup 2: p2.xlarge instance - 60GB RAM, 1/2 Tesla K80\n>\n> Model goes like this: LSTM 2, LSTM16, LSTM8, Dense1 ( but this happens on\n> any lstm/gru network)\n> Data is 3GB CSV\n> Memory usage on Theano is like:\n> [image: 15321436_1588947471135425_2108629537_o png]\n> <https://cloud.githubusercontent.com/assets/9210039/20986714/125561f4-bcca-11e6-9c38-47bfb33ad91b.jpeg>\n>\n> What happens there roughly is theano invokes g++ and each compilation\n> blows up (OOM)\n> What happens there with TF, it plateaus on 35GB of memory and runs\n> happily, so even though Setup 1 might be out of question (leave\n> optimization), Setup 2 works like a charm.\n>\n> I think it'd be worth to investigating.\n> Theano==0.9.0.dev3\n> Keras==1.1.0\n>\n> —\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/fchollet/keras/issues/4633>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AALC-3nirmmKm-oIys22M3VDG8W6IXMAks5rFyIigaJpZM4LHHjU>\n> .\n>\n"", ""I've tried both g++ 5.4.0 and g++ 4.8.4, same thing happens. What I believe is that maybe how theano uses g++ may be a problem. If that intrigues you, @nouiz I can provide more data in a few days. "", '@maxaug did you find out the reason for the memory spike? I am trying to train a VGG network and am facing similar issues during model compilation.', ""@varunagrawal: didn't have chance to investigate any further: If you can just switch to TF, do it. If that's not an option, I think you can try llvm-based compiler as it seemed faster to me on smaller workloads "", ""IF you use convolution or scan, make sure to update Theano. significant\nupdates was merged in some of the 0.9rc* version. Make sure to use 0.9rc4\nof the current dev version.\n\nIf after that you still have the problem, I'll need a way to reproduce\nthis. Can someone give me a script that when I run it give me this problem?\n\nIt would be better to give that script in a Theano issue that reference\nthis issue. I check them more frequently then keras issue.\n\nOn Wed, Dec 7, 2016 at 4:16 PM maxaug <notifications@github.com> wrote:\n\n> Hello there,\n>\n> I don't have an exact message on me now as I am in the middle of training\n> my model under TF but this is an issue i've had on two setups:\n>\n> Setup 1: 32Gb RAM, no GPU\n> Setup 2: p2.xlarge instance - 60GB RAM, 1/2 Tesla K80\n>\n> Model goes like this: LSTM 2, LSTM16, LSTM8, Dense1 ( but this happens on\n> any lstm/gru network)\n> Data is 3GB CSV\n> Memory usage on Theano is like:\n> [image: 15321436_1588947471135425_2108629537_o png]\n> <https://cloud.githubusercontent.com/assets/9210039/20986714/125561f4-bcca-11e6-9c38-47bfb33ad91b.jpeg>\n>\n> What happens there roughly is theano invokes g++ and each compilation\n> blows up (OOM)\n> What happens there with TF, it plateaus on 35GB of memory and runs\n> happily, so even though Setup 1 might be out of question (leave\n> optimization), Setup 2 works like a charm.\n>\n> I think it'd be worth to investigating.\n> Theano==0.9.0.dev3\n> Keras==1.1.0\n>\n> —\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/fchollet/keras/issues/4633>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AALC-3nirmmKm-oIys22M3VDG8W6IXMAks5rFyIigaJpZM4LHHjU>\n> .\n>\n"", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],1,0
7,keras,6688,closed,Keras accuracy is not increasing over 50%,"I am trying to build a binary classification algorithm (output is 0 or 1) on a dataset that contains normal and malicious network packets. The dataset shape (after converting IP @'s and hexa to decimal) is:

![capture1](https://cloud.githubusercontent.com/assets/18170760/26241844/86366da2-3c86-11e7-9620-24e6dd6f74e0.PNG)

Note: The final column is the output.

And the Keras model is:



However, I tried different optimizers, activation functions, number of layers, but the accuracy is reaching 0.5 at most:

![capture](https://cloud.githubusercontent.com/assets/18170760/26241854/8c530380-3c86-11e7-92c6-c9afd759eac0.PNG)

Even I tried Grid search for searching the best parameters, but the maximum is 0.5. Does anyone knows why the output is always like that? and how can I enhance it. Thanks in advance!",,"['1. You need to take care of input numerical scale. Try to normalize every feature dimension into [-1, 1] or [0, 1].\r\n2. Maybe some feature are categorical but not scalar, you may need to study how to deal with these kind of feature.\r\n3. If your data is not in a large scale, I will suggest you to use xgboost model.', 'The data has also to be standardized:\r\n(x - x_mean) / x_std\r\n\r\nPlease ask questions on stackoverflow. We have here so many issues; many of them still open.', 'Your issue is having a RELU activation in the last layer. Use sigmoid!', '@joelthchao Do you mean that the inputs must be normalized before using them in the model? And if yes do you know a method in keras for doing that?', '@StefanoD I used `standardized_X = preprocessing.scale(X)` and the result becomes:\r\n\r\n![capture](https://cloud.githubusercontent.com/assets/18170760/26295521/87ece82e-3eca-11e7-932e-99917c2eaf30.PNG)\r\n\r\nWhich is great!\r\nBut the question is that a right approach? \r\nBecause when I predicted if a packet is normal or malicious after training, the model predicts a wrong one :/\r\n', '@myhussien  I tried using that and the result becomes 0%', 'Your test data has also to be standardized before prediction.', '@StefanoD  when I am standardizing the data before prediction (See below), the output is all `[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]].`\r\n\r\n![capture](https://cloud.githubusercontent.com/assets/18170760/26296681/0c8cae7a-3ed0-11e7-898f-2b0bb418aafd.PNG)\r\n\r\n\r\n', '@Ahmid you have to use the same transformer that you fitted with the training data.\r\n\r\n```python\r\nscaler = StandardScaler()\r\nX_train_scaled = scaler.fit_transform(train_packet)\r\n# <train here>\r\n\r\nX_test_scaled = scaler.transform(test_packet)\r\npreds = loaded_model.predict(X_test_scaled)\r\n```', '@avsolatorio  Thank you! Solved my problem 👍 ', '> The data has also to be standardized:\r\n> (x - x_mean) / x_std\r\n> \r\n> Please ask questions on stackoverflow. We have here so many issues; many of them still open.\r\n\r\nDoes the data has to be standardized , normalized or both ? ', ""> Does the data has to be standardized , normalized or both ?\r\n\r\nWhere's the difference between standardized and normalized?"", ""> > Does the data has to be standardized , normalized or both ?\r\n> \r\n> Where's the difference between standardized and normalized?\r\n\r\nIn case of standardization we use the formula: **_(x - mean) / standard_deviation_** \r\nWhile in case of normalization we use the formula : **_(x - xmin) / (xmax - xmin)_**"", ""> In case of standardization we use the formula: **_(x - mean) / standard_deviation_**\r\n> While in case of normalization we use the formula : **_(x - xmin) / (xmax - xmin)_**\r\n\r\nI'm not sure, if there is a big difference, because the goals are similar, see [Normalization Wikipedia](https://en.wikipedia.org/wiki/Normalization_%28statistics%29).\r\n\r\nAnd I wouldn't mix different methods, if I were you. I don't see the point. But this is just an opinion.\r\nI would just test what works best.""]","['\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense\r\nfrom sklearn import preprocessing\r\nimport numpy\r\nimport os\r\n\r\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\r\nseed = 4\r\nnumpy.random.seed(seed)\r\n\r\ndataset = numpy.loadtxt(""NetworkPackets.csv"", delimiter="","")\r\nX = dataset[:, 0:11].astype(float)\r\nY = dataset[:, 11]\r\n\r\nmodel = Sequential()\r\nmodel.add(Dense(12, input_dim=11, kernel_initializer=\'normal\', activation=\'relu\'))\r\nmodel.add(Dense(12, kernel_initializer=\'normal\', activation=\'relu\'))\r\nmodel.add(Dense(1, kernel_initializer=\'normal\', activation=\'relu\'))\r\n\r\nmodel.compile(loss=\'binary_crossentropy\', optimizer=\'Adam\', metrics=[\'accuracy\'])\r\nmodel.fit(X, Y, nb_epoch=100, batch_size=5)\r\n\r\nscores = model.evaluate(X, Y)\r\nprint(""\\n%s: %.2f%%"" % (model.metrics_names[1], scores[1]*100))\r\n']",[],1,0
8,keras,9762,closed,shuffle is very slow when using fcn ,"when I turn on shuffle on training, the speed become 1/3 .
I use fully connected network, with 5m sample, 300 feature",,"['It is recommended to create a generator and do shuffle inside it, then turn off the shuffle in fit_generator which is implemented to be very slow.\r\n\r\nhttps://github.com/keras-team/keras/pull/9704/files provides an `_array_list_generator` examples to shuffle inside and using `shufffle = False` to turn off outside shuffle.']",[],[],1,0
9,keras,9766,closed,learning (fiting) performance,"Hi,

I am new to Keras with two trials centered around MNIST sequential model. The first one I used traditional approach while in the second I used functional one.
Performance-wise, the first was more than 10 fold faster than the functional
Even with that, the first gave accuracy about 97% while the second was about 50%

I am using CPU-based tensorflow backend. Any clues?",,[],[],[],1,0
10,keras,5671,closed,Loss and metric are completely different,"I have put my loss function within my metric:



Yet my metric is entirely different, for example here is some training output:



Note that my output per sample is two (coordinate for x and y). I can't work out why this it's so different. This different is consistent even when a new epoch starts.

I train with , like so:



Where my generator outputs multiple inputs



Where shape of  is of shape , which is similar to the inputs.  in this instance is 128.",stale,"['The loss displayed is a running average over the training epoch, whereas the metric is calculated over the test set at the end of the epoch.', 'I assume the test set in this instance is just the batch?\r\n\r\nBut still, even on a new epoch for the first batch these numbers are entirely different. ', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', 'Check if you have regularizers in your network. While the function .fit() is called, the loss displayed is the entire loss, ie (Loss from your loss metrics) + (Regularization term). The model is optimized over this entire loss function. \r\nBut the loss displayed as part of the metrics, is only the loss metric. The regularization term is not added there.']","['\r\ndef mean_euc_dist_sq(y_true, y_output):\r\n    return K.mean(K.sum(K.square(y_true - y_output), axis=-1))\r\n\r\ndef mean_euc_dist(y_true, y_output):\r\n    return K.mean(K.sqrt(K.sum(K.square(y_true - y_output), axis=-1)))\r\n\r\ndef max_euc_dist(y_true, y_output):\r\n    return K.max(K.sqrt(K.sum(K.square(y_true - y_output), axis=-1)))\r\n\r\ndef min_euc_dist(y_true, y_output):\r\n    return K.min(K.sqrt(K.sum(K.square(y_true - y_output), axis=-1)))\r\n\r\ndef stddev_euc_dist(y_true, y_output):\r\n    return K.std(K.sqrt(K.sum(K.square(y_true - y_output), axis=-1)))\r\n\r\n# ...\r\n\r\nmodel.compile(optimizer=sgd, loss=mean_euc_dist_sq, metrics=[mean_euc_dist_sq, mean_euc_dist, min_euc_dist, max_euc_dist, stddev_euc_dist])\r\n', '\r\n35328/41344 [========================>.....] - ETA: 78s - loss: 3040.6511 - mean_euc_dist_sq: 6.4321 - mean_euc_dist: 2.1256 - min_euc_dist: 0.1785 - max_euc_dist: 8.9086 - stddev_euc_dist: 1.3548\r\n', '\r\ncheckpoint = ModelCheckpoint(args.out, monitor=monitor_loss, period=1, save_best_only=True) tensorboard = TensorBoard(log_dir=args.log_dir, histogram_freq=1, write_images=True)\r\n\r\nmodel.fit_generator(data(all_data_points, batch_size=batch_size, mean=mean_imgs, aug=args.aug_count > 0),\r\n                                  nb_epoch=epochs_to_do, \r\n                                  samples_per_epoch=iterations_per_epoch * batch_size, \r\n                                  callbacks=[checkpoint, tensorboard],\r\n                                  validation_data=val,\r\n                                  nb_val_samples=amount_of_val,\r\n                                  verbose=1)\r\n', ""\r\nyield { 'in1': in1, 'in2': in2, 'in3\r\n: in3, 'in4\r\n: in4 }, batch_outputs\r\n""]","['fit_generator', 'batch_outputs', '(batch_size, 2)', 'batch_size']",1,0
11,keras,1063,closed,LSTM training is really slow,"Hello, I am training a single LSTM layer with these parameters :
number of examples : 27,000
size of sample : (2500, 1 , dtype=float64)
size of target : (250, 1 , dtype=float64)
batch size : 64
activation : linear

I use a GeForce GTX 970 GPU and the training is really slow : a single epoch takes 678 seconds, which seems really slow. Any idea of what could be happening/how I can speed things up?  

thanks!
",stale,"[""It may be, and it is not only a matter of GPU in these cases most of overhead is inside the scan loop for recurrent neural networks.\nTry to profile your output and you will understand where is the overhead. set training epoch to 1 and the theano flag profile=1.\nIt will return a profile indicating where is the overhead of your model. Probably most of the time is spend inside loops.\n\nUnfortunately in Keras at the moment there is no easy solution for that it's due to the fact that the Theano scan is really slow.\n"", ""We merged a speed up in scan last week. So maybe updating Theano can speed\nthinkgs up. It was giving up to 10% speed up depending of the use case.\n\nOn Mon, Nov 23, 2015 at 9:20 AM, Daniele Bonadiman <notifications@github.com\n\n> wrote:\n> \n> It may be, and it is not only a matter of GPU in these cases most of\n> overhead is inside the scan loop for recurrent neural networks.\n> Try to profile your output and you will understand where is the overhead.\n> set training epoch to 1 and the theano flag profile=1.\n> It will return a profile indicating where is the overhead of your model.\n> Probably most of the time is spend inside loops.\n> \n> Unfortunately in Keras at the moment there is no easy solution for that\n> it's due to the fact that the Theano scan is really slow.\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/fchollet/keras/issues/1063#issuecomment-158944950.\n"", ""Thanks you @nouiz . That's a good news.\n"", 'It seems, in the output [profile](https://github.com/Vict0rSch/random/blob/master/profile_1.txt) that indeed @dbonadiman  most of the time is spent in the scan. This means, (thanks @nouiz) that there is nothing I can do to speed things up except updating theano and hoping for ~10% speed up? \n', ""I don't know how old is your theano version there has been other optimisation in the recent past maybe you can obtain something more.\nIn my model that uses 2 `LSTM` i obtained a speedup from 38s-40s per epoch to 35s per epoch ~10% speed up as pointed out by @nouiz .\n\nIf you cannot deal with these times you can try to unfold the scan as it was done in the `Lasagne` library but it works only some times and you need to partially modify `Keras`. \n"", ""If your Theano is a few mounts old, you could get more speed up from an\nupdate. We got very big speed up in Scan in the last ~6 mounts.\n\nThe problem isn't the scan, you spend 93% of time in scan. Scan have just a\n2% overhead. Inside scan, you spend 80% of your time in gemm. So the gemm\nare your real bottleneck, not scan.\n\nBut there is trick that can be done. In the DLT LSTM[1] example, there is a\ntrick that was used to speed up the computation. It is to bundle some of\nthe weights in only one shared variable and do a big gemm instead of a few\nsmaller one. I don't remember the speed difference, but it gave a\nsignificant difference, but I'm not sure of the magnitude.\n\nThere is another thread/PR to keras with speed up of 4x to a modele with\nscan that gave 4x speed up from memory of reading rapidly keras related\nemails. Check it, maybe you can reuse the same tricks.\n\n[1]http://deeplearning.net/tutorial/lstm.html#lstm\n\nOn Mon, Nov 23, 2015 at 10:12 AM, Daniele Bonadiman <\nnotifications@github.com> wrote:\n\n> I don't know how old is your theano version there has been other\n> optimisation in the recent past maybe you can obtain something more.\n> In my model that uses 2 LSTM i obtained a speedup from 38s-40s per epoch\n> to 35s per epoch ~10% speed up as pointed out by @nouiz\n> https://github.com/nouiz .\n> \n> If you cannot deal with these times you can try to unfold the scan as it\n> was done in the Lasagne library but it works only some times and you need\n> to partially modify Keras.\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/fchollet/keras/issues/1063#issuecomment-158964124.\n"", '@pranv mentioned 4x speedup in PR with nerual turing machines.\n', 'following up @elanmart comment. @pranv suggestion was the same as the one proposed by @nouiz which is just concat a big tensor and run a single big gemm. @fchollet seemed interested in that approach in the discussion about generalized backends as well. At one point we will have to go back in our RNN models (LSTM, GRU, and Neural Turing Machines) and unify the multiplications for speed up. This will make the code less readable, but people usually feel happier with performance. Also, with good comments on the code the problem can be reduced. \n\nBut for now, @Vict0rSch please try updating your Theano and letting us know what happens.\n', ""It appears my version of Theano was not so old since I did'nt see any improvement updating Theano... Thank you very much anyway, I keep you posted\n"", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],1,0
12,keras,12843,closed,Inconsistency for the convolutional layer between Tensorflow and Theano backends,"### Configuration

| Library                        | Version  |
| ------------------------------ | -------- |
| python                         | 3.6.8    |
| GCC                            | 7.3.0    |
| tensorflow-base/tensorflow-gpu | 1.13.1   |
| keras-gpu/keras-base           | 2.2.4    |
| theano                         | 1.0.4    |
| cudnn                          | 7.3.1    |
| cudatoolkit                    | 10.0.130 |

Machine Configuration：

Ubuntu 18.04.2 LTS

Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz 128GB RAM

CUDA Version: 10.2 

Double GeForce GTX 1080ti 

---

### Description

I used the same pre-trained keras model (a LeNet structure on fashion-mnist datasets) on tensorflow and theano backends respectively, and noticed that there is a big difference between the two backends. The accuracy on tensorflow is 91.6% while the accuracy on theano is just 40.6%.

![acc](https://user-images.githubusercontent.com/17698785/58078171-abfebd00-7be0-11e9-8980-a3e2686cac16.png)

The prediction results on the two backends are shown as below, respectively:

![tensorflow-result](https://user-images.githubusercontent.com/17698785/58078197-bcaf3300-7be0-11e9-91a5-2ad46142c1ef.png)
![theano-result](https://user-images.githubusercontent.com/17698785/58078200-bcaf3300-7be0-11e9-83f3-4ed88c93991d.png)

Based on the results, the problem seems to occur at the backend of Theano. I also tried to localize the problem, and found that the inconsistency may happen at the first layer of the model, which is a convolutional layer:


I compared the outputs of the layer from the two backends, and found that many values are not close (or the same).

This layer is also used in many other models but there is no such inconsistency. Therefore, I suspect this problem happens under some specific context (such as parameters). In particular, I constructed a model only with this layer (using the same parameters and shuffling the weights), and the inconsistency still exists.

I attched the code and the pretrained model (.h5 file) to help reproduce this inconsistency.

[inconsistency_1.zip](https://github.com/keras-team/keras/files/3201344/inconsistency_1.zip)
",,[],"[""python\r\nkeras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28, 28, 1))\r\n""]",[],1,0
13,keras,7724,closed,Decrease loss in keras training usnig lstm,"I have an input like this:


which  means increase in one metric and  means decrease in it and  means no change in the metric. Each array has 83 items for 83 fields and the output(labels) for each array is a categorical array that shows effect of these metrics on a single metric:



I used  and  in the following code:

 
but the loss after 100 epochs is:


How can I decrease this loss percentage?",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', 'Any update on this ?']","['\r\n    x_train = [\r\n        [0,0,0,1,-1,-1,1,0,1,0,...,0,1,-1],\r\n        [-1,0,0,-1,-1,0,1,1,1,...,-1,-1,0],\r\n        ...\r\n        [1,0,0,1,1,0,-1,-1,-1,...,-1,-1,0]\r\n    ]\r\n\r\n', '\r\n    [[ 0.  0.  1.]\r\n     [ 1.  0.  0.],\r\n     [ 0.  0.  1.],\r\n     ...\r\n     [ 0.  0.  1.],\r\n     [ 1.  0.  0.]]\r\n', ""\r\n   def train(x, y, x_test, y_test):\r\n        x_train = np.array(x)\r\n        y_train = np.array(y)\r\n        y_train = to_categorical(y_train, 3)\r\n        model = Sequential()\r\n        model.add(Embedding(x_train.shape[0], output_dim=256))\r\n        model.add(LSTM(128))\r\n        model.add(Dropout(0.5))\r\n        model.add(Dense(3, activation='softmax'))\r\n        opt = optimizers.SGD(lr=0.001)\r\n        model.compile(loss='categorical_crossentropy',\r\n                optimizer=opt,\r\n                metrics=['accuracy'])\r\n        model.fit(x_train, y_train, batch_size=128, nb_epoch=100)\r\n        y_test = to_categorical(y_test, 3)\r\n        score = model.evaluate(x_test, y_test, batch_size=128)\r\n        prediction = model.predict(x_test, batch_size=128)\r\n        print score\r\n        print prediction\r\n\r\n""]","['1', '-1', '0', 'keras', 'lstm', '1618/1618 [==============================] - 0s - loss: 0.7328 - acc: 0.5556\r\n']",1,0
14,keras,12343,closed,"fit_generator: same data, different loss","Hi! I have strange behavior, trying make transfer learning on MobileNetV2 in Keras. I use keras.utils.Sequence class-inheritant as data-generators. One for train, one for validation. But while train loss decreases, validation loss increases since training start. Even if I use single generator for training and for validation (absolutely same data).
My model: https://gist.github.com/dzubape/efeefa77fdb1901d8f99c201b54381df
It seems, that validation loss could be different to training loss, but not like that: 2.5 vs 0.01
Where is my way of thinking wrong?)",,"['I got same behavior without Dropout', ""You should check your datasets，Maybe it's a label problem, or data normalization problem."", 'I am having the same problem. please help francois!']",[],[],1,0
15,keras,6199,closed,teano.function is slower as compared to model.predict,"For my application i need 2 output from the trained network (one at final layer and one at intermediate layer). Since model.predict() gives output from only last layer, i wrote theano function to fetch output at both intermediate layer and final layer. It works fine, but it is almost 20 times slower as compared to model.predict(). 

Can anybody help me to understand why theano function is slower and how can i make it faster.",stale type:support,"['I am still stuck on this. Any pointer will be of great help.', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],1,0
16,keras,4171,closed,Very high loss (~7) when doing binary classification,"I'm trying to train a simple MLP model on numerical data for binary classification, and when I train it this is what I see:



This continues forever...

You can view the code in question here, without the data loading portion:



If I look at the output of the model, I can see that this is what it predicts:



**Things I have tried:**

I have tried on several machines with different hardware and Ubuntu versions, with different combinations of Keras (1.0.7, 1.0.8, latest commit) & Theano (0.8.2 & latest commit) versions, and both CPU and GPU training - with the same result in every case.

I've tried removing the sigmoid activation and optimising MSE instead, but that results in a loss about 10 digits long. I've tried having a single dense neuron with no activation (and other various simplified architecures), and I've tried several different optimizers (Adam, sgd, rmsprop). No matter how I change the model, I cannot seem to get anything else.

I've verified that there are no NaNs or Infs in the matrix, and that the y values are binary. The matrix passed to Keras is a numpy array.

Any help would be very much appreciated, as I have been labouring over this problem for hours without being able to fix it.
",,"['Is your dataset balanced? This could be a result of your network simply learning to output the most common target value.\n', ""@kgrm It's relatively balanced (45% positive class). Interestingly, when running regression instead the predictions I vary a lot, all the way from minus billions to plus billions, so there is definitely something else wrong.\n"", 'What is the scale of your input data? Does it vary between samples, and, if so, does your training/validation split account for this? Have you tried normalising it, or using `BatchNormalization`?\n', ""Input data wasn't scaled, but it was consistent between samples. Adding `BatchNormalization` as the input layer to the net solved it. Thank you!!\n""]","['\nTrain on 9600 samples, validate on 2400 samples\nEpoch 1/100\n9600/9600 [==============================] - 4s - loss: 7.1742 - val_loss: 6.9442\nEpoch 2/100\n9600/9600 [==============================] - 3s - loss: 7.1726 - val_loss: 6.9442\nEpoch 3/100\n9600/9600 [==============================] - 3s - loss: 7.1726 - val_loss: 6.9442\n', "" python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cross_validation import KFold\nimport keras as k\nimport keras.layers as l\nfrom keras.layers.advanced_activations import PReLU\n\n# Issue occurs regardless of architecture\ndef keras_model(dim):\n    m = k.models.Sequential()\n    m.add(l.Dense(1024, input_dim=dim))\n    m.add(PReLU())\n    m.add(l.Dense(512))\n    m.add(PReLU())\n    m.add(l.Dense(1))\n    m.add(l.Activation('sigmoid'))\n\n    m.compile(loss='binary_crossentropy', optimizer='adam')\n    return m\n\n# Data loading.\n# One Pandas dataframe 'train' with only numerical data, with two lists 'features' and 'target' to denote X/y split\n\nkf = KFold(len(train.index), n_folds=nfolds, shuffle=True, random_state=37)\nfor train_index, test_index in kf:\n    X_train, X_valid = train[features].loc[train_index], train[features].loc[test_index]\n    y_train, y_valid = train[target].loc[train_index], train[target].loc[test_index]\n\n    model = keras_model(X_train.shape[1])\n\n    print(X_train.isnull().any().any()) # False, there are no NaNs/Infs in the matrix.\n\n    # Convert to numpy array before training.\n    # y_train is a 1-dimensional array of binary values\n    model.fit(X_train.values, y_train.values, verbose=1, validation_data=[X_valid.values, y_valid.values], batch_size=64, nb_epoch=100)\n"", '\n[[ 0.]    \n [ 0.]    \n [ 0.]    \n ...,     \n [ 0.]    \n [ 0.]    \n [ 0.]]   \n']",[],1,0
17,keras,13389,closed,val_loss not making sense with multiple outputs and weighted loss,"**System information**  
- Have I written custom code (as opposed to using example directory):  Yes, custom Keras Sequence
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  1.14.0
- Keras version:  2.3.0
- Python version:  Python 3.6.8
- CUDA/cuDNN version:  10.1
- GPU model and memory:  ASUS Dual GeForce® RTX 2080 Ti  (2 pieces)

You can obtain the TensorFlow version with:  
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""  
You can obtain the Keras version with:  
python -c 'import keras as k; print(k.__version__)'  

**Describe the current behavior**  
I have a model with multiple outputs, and hence there is loss for each of the output and an overall weighted loss. The weighted sum of all the validation loss of the different outputs doesn't add up to the overall validation loss, val_loss.

**Describe the expected behavior**  
Expecting val_loss to be a weighted sum of all the validation loss of model output.

**Code to reproduce the issue**  


**Other info / logs**  
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.  
",type:support,"['Here are the 3 columns of data: val_loss, val_regression_output_loss, and val_classification_output_loss with weights [20, 1], 20 for val_regression_output_loss and 1 for val_classification_output_loss.\r\n\r\nval_loss:\r\n0.169534176588059\r\n0.407849103212357\r\n0.244819268584251\r\n0.149829715490341\r\n0.144160956144333\r\n0.200807094573975\r\n0.074971824884415\r\n0.100794419646263\r\n0.071464695036411\r\n0.384763598442078\r\n\r\nval_regression_output_loss:\r\n0.004071841016412\r\n0.004012452904135\r\n0.004082964267582\r\n0.004099677316844\r\n0.004057640675455\r\n0.004182121250778\r\n0.004099731799215\r\n0.004059502854943\r\n0.004109856206924\r\n0.004216863773763\r\n\r\nval_classification_output_loss:\r\n0.135637700557709\r\n0.145183801651001\r\n0.136177375912666\r\n0.146444052457809\r\n0.128644600510597\r\n0.173463732004166\r\n0.132748082280159\r\n0.152826607227325\r\n0.160107001662254\r\n0.138926386833191\r\n\r\nAs can be observed in the values outputted by CSVLogger, there is little difference in the val_regression_output_loss and val_classification_output_loss, but vast difference in val_loss.', 'I replaced the parralel_model with conventional single GPU model and get the same irratic val_loss too. What could be the problem?', 'All this is run inside a Docker container based on a custom Docker image based on \r\ntensorflow/tensorflow:latest-gpu-py3-jupyter\r\n\r\n```\r\nFROM tensorflow/tensorflow:latest-gpu-py3-jupyter\r\n\r\nRUN apt-get update\r\nRUN apt-get install --yes nano vim\r\nRUN apt-get install --yes python3-pip\r\n# RUN apt-get install --yes nodejs npm\r\nRUN apt-get install --yes libsm6 libxext6 libxrender-dev\r\n# RUN apt-get install --yes git\r\n\r\n# RUN npm install -g configurable-http-proxy\r\n\r\n# RUN pip3 install jupyterhub\r\nRUN pip3 install notebook\r\nRUN pip3 install ipython\r\nRUN pip3 install scipy\r\nRUN pip3 install pandas\r\nRUN pip3 install ""dask[complete]""\r\nRUN pip3 install seaborn\r\nRUN pip3 install plotly\r\nRUN pip3 install scikit-learn\r\nRUN pip3 install xgboost\r\nRUN pip3 install lightgbm\r\nRUN pip3 install catboost\r\nRUN pip3 install imbalanced-learn\r\nRUN pip3 install keras\r\nRUN pip3 install torch==1.0 torchvision\r\nRUN pip3 install opencv-python\r\nRUN pip3 install scikit-image\r\nRUN pip3 install imgaug\r\n#RUN pip3 install statsmodel\r\nRUN pip3 install nltk\r\nRUN pip3 install gensim\r\n\r\nCMD [""/bin/bash""]\r\n\r\n```', ""Training loss is ok, but val_loss don't add up. This is weirld..."", 'Output of:\r\n\r\n` lsb_release -a`\r\n\r\n```\r\nNo LSB modules are available.\r\nDistributor ID: Ubuntu\r\nDescription:    Ubuntu 16.04.6 LTS\r\nRelease:        16.04\r\nCodename:       xenial\r\n```', 'I am experiencing the same issue, with tf 1.13.1, 1.14.0, and 2.0. Also for me the training loss is fine but the val_loss doesnt add up:\r\n`loss: 0.3240 - regression_loss: 0.2500 - classification_loss: 0.0740 - val_loss: 8.6638e-07 - val_regression_loss: 0.3861 - val_classification_loss: 0.2899\r\n`', 'This is giving me lots of confusion in my work projects. I just tried with replacing keras with tf.keras and it seems to work as expected. I guess Keras is being abandoned.', ""Hello,\r\nI have the exact problem but I didn't understand how you solve it. Could you explain me?\r\nThankss in advance"", ""> Hello,\r\n> I have the exact problem but I didn't understand how you solve it. Could you explain me?\r\n> Thankss in advance\r\n\r\nTry different combinations of TF + Keras, maybe TF 1.13.1 and Keras 2.2.4.\r\nOr use tf.keras instead of keras."", 'Thanks for the fast response, the problem solved when I downgraded the keras to 2.2.5', 'Same problem here, solved by moving to tensorflow keras', ""I have the same problem. I've noticed that when passing a fixed validation set (as array) the losses sum up correctly. When using a generator for the validation (implemented with keras sequence), the overall loss doesn't make sense anymore (while it makes sense for the training set, implemented with the same generator)""]","['\r\ninput_score = Input(shape=(None, None, 3), name=""regression_input"")\r\ninput_class = Input(shape=(None, None, 3), name=""classification_input"")\r\n\r\nmodel_base = ResNet50(weights=""imagenet"", include_top=False)\r\ngap = GlobalAveragePooling2D()\r\n\r\nshared_dense_1 = Dense(2048)\r\nshared_bn_1 = BatchNormalization()\r\nshared_relu_1 = ReLU()\r\n\r\nshared_dense_2 = Dense(2048)\r\nshared_bn_2 = BatchNormalization()\r\nshared_relu_2 = ReLU()\r\n\r\nx_score = model_base(input_score)\r\nx_score = gap(x_score)\r\n\r\nx_score = shared_dense_1(x_score)\r\nx_score = shared_bn_1(x_score)\r\nx_score = shared_relu_1(x_score)\r\n\r\nx_score = shared_dense_2(x_score)\r\nx_score = shared_bn_2(x_score)\r\nx_score = shared_relu_2(x_score)\r\n\r\nscore_out = Dense(1, activation=""sigmoid"", name=""regression_output"")(x_score)\r\n\r\nx_class = model_base(input_class)\r\nx_class = gap(x_class)\r\n\r\nx_class = shared_dense_1(x_class)\r\nx_class = shared_bn_1(x_class)\r\nx_class = shared_relu_1(x_class)\r\n\r\nx_class = shared_dense_2(x_class)\r\nx_class = shared_bn_2(x_class)\r\nx_class = shared_relu_2(x_class)\r\n    \r\nclass_out = Dense(1, activation=""sigmoid"", name=""classification_output"")(x_class)\r\n\r\nwith tf.device(\'/cpu:0\'):\r\n    model = Model([input_score, input_class], [score_out, class_out])\r\n\r\ntry:\r\n    parallel_model = multi_gpu_model(model, gpus=gpus)\r\nexcept:\r\n    parallel_model = model\r\n\r\nparallel_model.compile(optimizer=Adam(lr=0.0001), \r\n              loss=[\r\n                  ""mse"", \r\n                  ""binary_crossentropy""\r\n              ],\r\n              loss_weights=[\r\n                  20, \r\n                  1\r\n              ],\r\n              metrics={\r\n                ""regression_output"": [""mse"", ""mae""],\r\n                ""classification_output"": ""acc""\r\n              })\r\nparallel_model.summary()\r\n\r\nhist = parallel_model.fit_generator(dataset_train, \r\n                    validation_data=dataset_test,\r\n                    callbacks=callbacks,\r\n                    workers=workers, \r\n                    shuffle=True, \r\n                    epochs=10,\r\n                    initial_epoch=0)\r\n']",[],1,0
18,keras,10832,closed,Slowdown by factor 100 when adding second GPU,"
I recently bought a second GPU in order to speed up my neural network computations. I use Keras 2.2.2 with  Tensorflow 1.9.0. When I run a normal model it is sped up by one GPU as might be expected. It makes my model run about 60 times faster compared to the CPU (AMD 1950X).


When I use the instruction  the combined power of both cards results in a slowdown by about a **factor 100**. That happens to all my models: RNN, CNN and Dense. Both cards seem to work, however, they seem to alternate the work between each other. Also the CPU activity does increase quite a lot. When using both cards the CPU activity of one core is about 50%, using one card this is about 8%-10%. 

When selecting each GPU separately by using  the usual speed up occurs. I cannot use multi_gpu_model in that case. When I select both GPU’s with   and multi_gpu_model then the slow down occurs. I printed the topology (see below) but could not find anything strange, but I am far from an expert on this. 


This is not the same issue as issue #9204. That issue discusses why some models cause some slowdown when using multiple GPU's. As far as I can see my issue is not related to any specific model.  
Does anyone have an idea what exactly is wrong in my setup?

Keras 2.2.2, Tensorflow 1.9.0, python 3.5.5, Ubuntu Mate 18.04 and Nvidia drivers 390.48 (same problems with Ubuntu Mate 16.04.4 and Nvidia drivers 390.130).

Below is some simple MNIST code that I used to benchmark my setup.

",,"[""I'm not 100% confident about my answer, but I ran into this issue, my mentor suggested it's because, only for really huge batch sizes (ex:256, 512) you are going to see speed improvement, if for smaller batches it's just a lot of time for the CPU to split batches and dump them into the GPUs.\r\nBut still, 505 secs on two GPUs really makes me ponder on what's going on."", 'Can you try a batch size of 512 just to see if it\'s a communication bottleneck happening?\r\n\r\nAlso, you should build your model on CPU instead since you don\'t have NVLink:\r\n```\r\n# multi-gpu model\r\nimport tensorflow as tf\r\nfrom keras.utils.training_utils import multi_gpu_model\r\n\r\nwith tf.device(""/cpu:0""):\r\n\t\t# initialize the model\r\n\t\tmodel = XXX\r\n\t\r\n# make the model parallel\r\nparallel_model = multi_gpu_model(model, gpus=2)\r\nparallel_model.compile()\r\n```', 'Thank you both for your suggestions. Both the batch size increase and building the model on CPU helped me to solve the problem. I did some experiments for the MNIST code and tabled the results:\r\n```\r\n                Built on CPU\r\nbs   1GPU 2GPU  1GPU 2GPU\r\n  64 5.88   -     40 9.06\r\n 128 3.55  499    36 5.79\r\n 512 2.32  127    35 2.30\r\n1024 2.31   64    34 1.21\r\n```\r\n\r\nIt seems that for this simple model the break even in speed is reached for a batch size of 512. With a batch size of 1024 I get the about factor two speedup I had hoped for by buying this extra card. I next experimented with an RNN network and got the following results:\r\n```\r\n  bs layers type 1GPU 2GPU (built on CPU)\r\n 128    3    GRU  124  251\r\n1024    3    GRU   45   42\r\n2038    3    GRU   -    34\r\n1024    5    GRU   85   88\r\n1024    3   LSTM  125   75\r\n1024    5   LSTM   -   130\r\n```\r\nA batch size of 2048 is the max I was able to build, for 4096 the system ran out of memory. What I should have done is also measure the accuracy of the computations to see whether the parameters chosen have impact on the results. I found out though, that when all remains equal GRU results in a better accuracy per epoch for this task than LSTM.\r\n\r\nAs to NVLink: am I right to guess that it is a kind of hardware connection and that it is not available for my ""cheapo"" cards like the 1080Ti? Nvidia is not completely clear on that.\r\n\r\nThe RNN code I tested you\'ll find below. When you ""comment in"" the code then you\'ll have the code for 5 recurrent layers as mentioned in the second table.\r\n\r\n```\r\ndef create_network(X, n_vocab):\r\n    """""" create the structure of the neural network """"""\r\n    with tf.device(""/cpu:0""):\r\n        model = Sequential()\r\n        model.add(GRU(512, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\r\n        model.add(Dropout(0.3))\r\n        \r\n        model.add(GRU(512, return_sequences=True))\r\n        model.add(Dropout(0.3))\r\n        """"""\r\n        model.add(GRU(512, return_sequences=True))\r\n        model.add(Dropout(0.3))\r\n        \r\n        model.add(GRU(512, return_sequences=True))\r\n        model.add(Dropout(0.3))\r\n        """"""\r\n        model.add(GRU(512))\r\n        \r\n        model.add(Dense(256))\r\n        model.add(Dropout(0.3))\r\n        \r\n        model.add(Dense(n_vocab))\r\n        model.add(Activation(\'softmax\'))\r\n        \r\n    model.summary()\r\n        \r\n    model = multi_gpu_model(model, gpus=2)\r\n\r\n    return model\r\n\r\n\r\n```', 'I did some more systematic benchmarks for the RNN because I did not understand the results mentioned above. I also measured accuracy and validation accuracy. For RNN in my case it seems that a second GPU card is overkill. \r\n\r\n```Trainable parameters: 1,084,800\r\n  bs type layers size   1  acc val_acc\r\n  64 GRU     3    256 191 0.21 0.21\r\n 128 GRU     3    256  96 0.19 0.20\r\n 256 GRU     3    256  47 0.16 0.19\r\n 512 GRU     3    256  24 0.13 0.17\r\n1024 GRU     3    256  13 0.08 0.08\r\n\r\nTrainable parameters: 1,084,800\r\n  bs type layers size    2  acc val_acc\r\n  64 GRU     3    256 190 0.21 0.20\r\n 128 GRU     3    256  96 0.19 0.19\r\n 256 GRU     3    256  48 0.17 0.18\r\n 512 GRU     3    256  24 0.12 0.17\r\n1024 GRU     3    256  13 0.08 0.08\r\n\r\nTrainable parameters: 4,102,528\r\n  bs type layers size   1  acc val_acc\r\n  64 GRU     3    512 190 0.13 0.16\r\n 128 GRU     3    512  95 0.20 0.20\r\n 256 GRU     3    512  50 0.15 0.18\r\n 512 GRU     3    512  38 0.12 0.16\r\n1024 GRU     3    512  37 0.08 0.09\r\n\r\nTrainable parameters: 4,102,528\r\n  bs type layers size   2  acc val_acc\r\n  64 GRU     3    512 365 0.07 0.06\r\n 128 GRU     3    512 197 0.07 0.07\r\n 256 GRU     3    512 105 0.07 0.08\r\n 512 GRU     3    512  56 0.07 0.08\r\n1024 GRU     3    512  34 0.08 0.07\r\n\r\nTrainable parameters: 7,251,328\r\n  bs type layers size   1  acc val_acc\r\n  64 GRU     5    512 312 0.21 0.19\r\n 128 GRU     5    512 157 0.21 0.21\r\n 256 GRU     5    512  83 0.16 0.19\r\n 512 GRU     5    512  71 0.07 0.08\r\n1024 GRU     5    512 100 0.09 0.08\r\n\r\nTrainable parameters: 7,251,328\r\n  bs type layers size   2  acc val_acc\r\n  64 GRU     5    512   -    -    -\r\n 128 GRU     5    512 318 0.07 0.06\r\n 256 GRU     5    512 169 0.07 0.06\r\n 512 GRU     5    512  86 0.07 0.08\r\n1024 GRU     5    512  58 0.07 0.08\r\n```\r\n']","['\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 390.48                 Driver Version: 390.48                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:08:00.0 Off |                  N/A |\r\n|  0%   45C    P2   182W / 250W |  10663MiB / 11178MiB |     88%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 108...  Off  | 00000000:41:00.0  On |                  N/A |\r\n|  0%   59C    P2    60W / 250W |  10643MiB / 11178MiB |      1%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      3081      C   ...old/bin/anaconda/envs/ml-gpu/bin/python 10651MiB |\r\n|    1      1410      G   /usr/lib/xorg/Xorg                           205MiB |\r\n|    1      3000      G   ...old/bin/anaconda/envs/ml-gpu/bin/python    38MiB |\r\n|    1      3081      C   ...old/bin/anaconda/envs/ml-gpu/bin/python 10387MiB |\r\n+-----------------------------------------------------------------------------+\r\n', '\r\n$ nvidia-smi topo -m\r\n\tGPU0\tGPU1\tCPU Affinity\r\nGPU0\t X \tSYS\t0-31\r\nGPU1\tSYS\t X \t0-31\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n', '\r\n\r\n\'\'\'\r\nTrains a simple convnet on the MNIST dataset.\r\n5 seconds per epoch on one (1) Geforce 1080 Ti GPU.\r\n505 seconds when using the multi_model option for two 1080 Ti\'s\r\n\'\'\'\r\n\r\n#import os\r\n#os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""   # see issue #152\r\n#os.environ[""CUDA_VISIBLE_DEVICES""] = ""1""\r\n\r\nimport keras\r\nfrom keras.datasets import mnist\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\r\nfrom keras.utils import multi_gpu_model\r\n\r\nbatch_size = 128\r\nnum_classes = 10\r\nepochs = 5\r\n\r\n# input image dimensions\r\nimg_rows, img_cols = 28, 28\r\n\r\n# the data, shuffled and split between train and test sets\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\nx_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\ninput_shape = (img_rows, img_cols, 1)\r\n\r\nx_train = x_train.astype(\'float32\')\r\nx_test = x_test.astype(\'float32\')\r\nx_train /= 255\r\nx_test /= 255\r\nprint(\'\')\r\nprint(\'x_train shape:\', x_train.shape)\r\nprint(x_train.shape[0], \'train samples\')\r\nprint(x_test.shape[0], \'test samples\')\r\n\r\n# convert class vectors to binary class matrices\r\ny_train = keras.utils.to_categorical(y_train, num_classes)\r\ny_test = keras.utils.to_categorical(y_test, num_classes)\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(32, kernel_size=(3, 3),\r\n                 activation=\'relu\',\r\n                 input_shape=input_shape))\r\nmodel.add(Conv2D(64, (3, 3), activation=\'relu\'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Dropout(0.25))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(128, activation=\'relu\'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(num_classes, activation=\'softmax\'))\r\n\r\n#model = multi_gpu_model(model, gpus=2)\r\n\r\nmodel.compile(loss=keras.losses.categorical_crossentropy,\r\n              optimizer=keras.optimizers.Adadelta(),\r\n              metrics=[\'accuracy\'])\r\n\r\nmodel.fit(x_train, y_train,\r\n          batch_size=batch_size,\r\n          epochs=epochs,\r\n          verbose=1,\r\n          validation_data=(x_test, y_test))\r\nscore = model.evaluate(x_test, y_test, verbose=0)\r\nprint(\'Validation accuracy:\', score[1])\r\n\r\n']","['multi_gpu_model', 'CUDA_VISIBLE_DEVICES', 'CUDA_VISIBLE_DEVICES=0,1']",1,0
19,keras,8273,closed,Noisy validation loss in Keras when using fit_generator,"Any idea about **why** our **training loss** is **smooth** and our **validation loss** is that **noisy** across epochs?

![image](https://user-images.githubusercontent.com/4671752/32121045-4b16b5b8-bb31-11e7-86e0-8690ce9f867c.png)

 We are implementing a deep learning model for diabetic retinopathy detection (**binary classification**) using the data set of fundus photographs provided by [this Kaggle competition][2]. We are using **Keras 2.0** with **Tensorflow** backend.

As the data set is too big to fit in memory, we are using , with  randomly taking images from training and validation folders:

    # TRAIN THE MODEL
    model.fit_generator(
        train_generator,
        steps_per_epoch= train_generator.samples // training_batch_size,
        epochs=int(config['training']['epochs']),
        validation_data=validation_generator,
        validation_steps= validation_generator.samples // validation_batch_size,
        class_weight=None)

Our CNN architecture is VGG16 with dropout = 0.5 in the last two fully connected layers, batch normalization only before the first fully connected layer, and data augmentation (consisting on flipping the images horizontally and vertically). Our training and validation samples are normalized using the training set mean and standard deviation. Batch size is 32. Our activation is a  and the loss function is the . [You can find our implementation in Github][3]

It definitely has nothing to do with overfitting, as we tried with a highly regularized model and the behavior was quite the same. **Is it related with the sampling from the validation set?** Has any of you had a similar problem before?

Thanks!!

  [2]: https://www.kaggle.com/c/diabetic-retinopathy-detection
  [3]: https://github.com/ignaciorlando/cnn-dr-kaggle",,"['Are you by any chance training on a Power8 Linux Server? I have the same behaviour with Batch Normalization when training there. In every other environment I tested, the problem did not occur.', 'I met the same problem and have not idea. Have you figure out?', ""I ran into this as well. When running code locally, the validation loss and accuracy both evolved smoothly. When running on Kaggle's server (which is using a different Keras version), I consistently see a very noisy validation curve even though the validation accuracy improves smoothly. I plan to provide more details shortly. ""]","['fit_generator', 'ImageDataGenerator', 'sigmoid', 'binary_crossentropy']",[],1,0
20,keras,11858,closed,BatchNormalization produces NaN weights without NaN loss,"Hi,

Not sure this can be labelled as a bug, but it's problematic. BatchNormalization seems to silently produce NaN weights when training a  if the training dataset size is not a multiple of .

For example, with a training dataset (1825, 401, 401, 3), validation dataset (140, 401, 401, 3), , , 



The training apparently goes fine



but the weights have NaNs, e.g.



I think this happens because the training dataset of 1825 gets split into sets of . So there's going to be a set of 1 training image, and maybe that doesn't work with BatchNormalization.

Inference with the trained model gives



A solution is to make sure that the number of training images is a multiple of .",stat:awaiting response,"['This issue is more appropriate on TensorFlow repo. Can you please post it on TensorFlow repo from [here](https://github.com/tensorflow/tensorflow/issues/new/choose)? Thanks!', 'Done, now in [TensorFlow issue 24320](https://github.com/tensorflow/tensorflow/issues/24320).']","[""python\r\n    # instantiate model\r\n    with tf.device('/cpu:0'):\r\n        # DenseNet121: blocks=[6, 12, 24, 16]\r\n        base_model = densenet.DenseNet121(include_top=False, weights=None,\r\n                                       input_shape=(401, 401, 3), pooling='avg')\r\n        x = Dense(units=1, activation='sigmoid', name='fc1')(base_model.output)\r\n        model = Model(inputs=base_model.input, outputs=x)\r\n\r\n    # compile model\r\n    parallel_model = multi_gpu_model(model, gpus=gpu_number)\r\n    parallel_model.compile(loss={'fc1': 'binary_crossentropy'},\r\n                           optimizer='Adadelta',\r\n                           metrics={'fc1': ['acc']})\r\n\r\n    # train model\r\n    tic = datetime.datetime.now()\r\n    parallel_model.fit(train_onecell_im,\r\n                       {'fc1': (train_onecell_dice >= quality_threshold).astype(np.float32)},\r\n                       validation_data=(test_onecell_im,\r\n                                        {'fc1': (test_onecell_dice >= quality_threshold).astype(np.float32)}),\r\n                       batch_size=batch_size, epochs=epochs, initial_epoch=0)\r\n    toc = datetime.datetime.now()\r\n    print('Training duration: ' + str(toc - tic))\r\n"", '\r\nTrain on 1825 samples, validate on 140 samples\r\nEpoch 1/1\r\n1825/1825 [==============================] - 108s 59ms/step - loss: 0.6604 - acc: 0.6323 - val_loss: 0.6932 - val_acc: 0.4643\r\nTraining duration: 0:02:08.115762\r\n\r\n', ""\r\nmodel.get_layer('conv1/bn').get_weights()\r\n[array([1.0001292 , 1.        , 0.9996672 , 0.9999442 , 1.000509  ,\r\n       1.0001016 , 1.0002009 , 1.0004678 , 0.9999988 , 0.999962  ,\r\n       1.0003603 , 1.0001667 , 0.9999296 , 0.9999381 , 1.00001   ,\r\n       0.99967813, 0.9999821 , 0.99981546, 0.9999899 , 1.0002408 ,\r\n       0.9999446 , 0.9999995 , 0.99989605, 1.0000395 , 1.0000094 ,\r\n       0.9999432 , 0.999968  , 0.99994946, 0.9997129 , 1.0000957 ,\r\n       0.99997395, 1.000016  , 0.99995   , 0.99981534, 0.99984217,\r\n       0.9999743 , 0.99999624, 1.0005921 , 1.0001019 , 1.000008  ,\r\n       0.99993116, 0.99998087, 0.9999631 , 0.9999878 , 0.9999804 ,\r\n       1.0003394 , 0.999895  , 0.9997747 , 0.9999677 , 0.99998355,\r\n       1.000003  , 0.9998863 , 0.9999338 , 0.9998308 , 1.0000825 ,\r\n       1.000022  , 0.9999998 , 0.9997648 , 1.0000801 , 1.000631  ,\r\n       1.0000259 , 0.9996165 , 1.0001084 , 0.9996289 ], dtype=float32), array([ 0.00369794, -0.00307915, -0.0148356 ,  0.01176912, -0.00456085,\r\n        0.00461122,  0.00392016, -0.00510793, -0.00388927,  0.00678776,\r\n       -0.0033672 ,  0.0020039 ,  0.00688829,  0.00877651,  0.00838199,\r\n       -0.0217527 , -0.00673187, -0.01623467,  0.00523926, -0.0005527 ,\r\n        0.00700372, -0.00372984, -0.01347521, -0.00636716,  0.00206494,\r\n        0.00884918, -0.00814271, -0.00801541, -0.02038615,  0.00171547,\r\n        0.00709944, -0.00221861,  0.00538696, -0.01515745, -0.01330438,\r\n        0.00306095,  0.00399868, -0.0049634 , -0.00725381,  0.00373429,\r\n       -0.01107734, -0.00610222, -0.00854702, -0.00504343, -0.0080514 ,\r\n       -0.00920443,  0.00863727, -0.01750346,  0.00656873, -0.00534429,\r\n        0.00434025, -0.01352841, -0.00819136, -0.01453205, -0.00043049,\r\n       -0.00257635, -0.00448346, -0.01370949,  0.00355583, -0.00480247,\r\n        0.00179911, -0.01858746, -0.00059417, -0.0123234 ], dtype=float32), array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\r\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\r\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\r\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\r\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\r\n      dtype=float32), array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\r\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\r\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\r\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\r\n       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\r\n      dtype=float32)]\r\n"", '\r\nfoo = model.predict(test_onecell_im)\r\n\r\nfoo\r\narray([[nan],\r\n       [nan],\r\n       [nan],\r\n...\r\n       [nan],\r\n       [nan]], dtype=float32)\r\n']","['multi_gpu_model', 'batch_size', 'epochs=1', 'batch_size=16', 'gpu_number=2', 'batch_size=16', 'batch_size']",1,0
21,keras,8792,closed,regression in training accuracy with VGG16 application,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

When running under both Keras 2.1.2 as well as Keras from GitHub master the validation accuracy achieved in the dogs vs. cats example in Chapter 5.3 of the DL w/Python book (https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/5.3-using-a-pretrained-convnet.ipynb) is ~ 90%.

However, the validation accuracy when running under Keras 2.0.9 is ~ 96%.

The code (as well as the cats vs. dogs images) required to reproduce this is in the following GitHub repo (the script is also inline below):

https://github.com/jjallaire/keras-vgg16-accuracy

One other noteworthy thing is that with Keras 2.1.2 case training takes ~ 22 seconds/epoch whereas with Keras 2.0.9 it's ~ 50 seconds per epoch (this on an Amazon p2.xlarge instance).


 
",,"['I am also seeing the ~ 90% validation accuracy in Keras 2.1.0 so it appears that a change between 2.0.9 and 2.1.0 precipitated this.', 'Please try to run a single `train_on_batch` from a known state (e.g. freshly initialized optimizer and imagenet weights, same input array and target array, no preprocessing/augmentation) and compare the outputs with 2.0.9 and 2.1.2.', 'From git bisect I found that this commit is the one where the behavior starts deviating: https://github.com/keras-team/keras/commit/c25fa38deb4efc5445f64af3ec17eae0eb660d2f', 'Do you get the old behavior by removing `conv_base.trainable = False`?', 'Yes, if I remove `conv_base.trainable = False` when running under 2.1.0 then I get the old behavior! (higher accuracy, ~ 50 seconds / epoch training time)', ""The current behavior is correct. Past behavior was a hard-to-detect bug. There's no easy fix here."", 'Okay, very well, glad we figured this out!\r\n\r\n']","[""python\r\nimport os\r\nimport keras\r\nfrom keras import models\r\nfrom keras import layers\r\nfrom keras import optimizers\r\nfrom keras.applications import VGG16\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\n\r\n\r\nbase_dir = 'cats_and_dogs_small'\r\ntrain_dir = os.path.join(base_dir, 'train')\r\nvalidation_dir = os.path.join(base_dir, 'validation')\r\ntest_dir = os.path.join(base_dir, 'test')\r\n\r\n\r\nconv_base = VGG16(weights='imagenet',\r\n                  include_top=False,\r\n                  input_shape=(150, 150, 3))\r\n                  \r\nmodel = models.Sequential()\r\nmodel.add(conv_base)\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(256, activation='relu'))\r\nmodel.add(layers.Dense(1, activation='sigmoid'))\r\n\r\nconv_base.trainable = False\r\n\r\n\r\ntrain_datagen = ImageDataGenerator(\r\n      rescale=1./255,\r\n      rotation_range=40,\r\n      width_shift_range=0.2,\r\n      height_shift_range=0.2,\r\n      shear_range=0.2,\r\n      zoom_range=0.2,\r\n      horizontal_flip=True,\r\n      fill_mode='nearest')\r\n\r\ntest_datagen = ImageDataGenerator(rescale=1./255)\r\n\r\ntrain_generator = train_datagen.flow_from_directory(\r\n        train_dir,\r\n        target_size=(150, 150),\r\n        batch_size=20,\r\n        class_mode='binary')\r\n\r\nvalidation_generator = test_datagen.flow_from_directory(\r\n        validation_dir,\r\n        target_size=(150, 150),\r\n        batch_size=20,\r\n        class_mode='binary')\r\n\r\n\r\nmodel.compile(loss='binary_crossentropy',\r\n              optimizer=optimizers.RMSprop(lr=2e-5),\r\n              metrics=['acc'])\r\n\r\nhistory = model.fit_generator(\r\n      train_generator,\r\n      steps_per_epoch=100,\r\n      epochs=30,\r\n      validation_data=validation_generator,\r\n      validation_steps=50,\r\n      verbose=1)\r\n""]",[],1,0
22,keras,5720,closed,Image Augmentation- flow - hanging forever and not training,"I am trying to train a segmentation network. I am running into issues when trying to use image augmentation. Below is attached my function that sets the generator, fits it (though I dont think I really need to), it then zips the label and image generators. 

It is when trying to zip these that my program hangs and never moves on. Playing around it seems that it has to do with the flow function. Whenever I call the flow function it just runs on it's own indefinitely. 


I have made some changes to the image.py file to allow for 3D manipulations, but I have check/doublechecked / even gone back and re-implemented my changes in a clean copy of image.py  to make sure I wasn't changing anything that should effect this process. 
[image.py.zip](https://github.com/fchollet/keras/files/836158/image.py.zip)


I've attached a zip of my updated image.py. Most of the functions at the start are left as the 2D counterparts, its within the random_transform that most of the 3D manipulations are located. 

Does anyone have any thoughts? 

Thanks, 

Anthony. 



",,"[""By definition, you can't `zip` two infinite generators. You'll need to create a third generator to pick one `yield`ed item from each of the original generators, and then yield them."", ""Thanks for that. Do you have any suggestions or do you know of any resources that have done something similar?  All I'll need is something general. I've never done something of the sort and couldn't find anything on a quick search. \n\nIf not, no worries, I'll try to figure it out and post back here for future reference. \n\nThanks,\n\nAnthony\n\n> On Mar 11, 2017, at 5:38 PM, Pat York <notifications@github.com> wrote:\n> \n> By definition, you can't zip two infinite generators. You'll need to create a third generator to pick one yielded item from each of the original generators, and then yield them.\n> \n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n"", '```python\r\ndef combine_generator(gen1, gen2):\r\n    yield (gen1.next(), gen2.next())\r\n```', 'Thanks a million for that advice. I havent had a chance to try and implement (on vacation), will try this afternoon and report back. I had tried something similar... almost exactly the same.... but used return and not yield. \r\n\r\nReturn just gave me an array. So, yield will return the augmentation generator and when the result of the function is called it will continue to produce new augmentations forever? and not just the same one over and over? Just want to make sure I understand and am using it correctly. \r\n\r\nThanks, \r\n\r\nAnthony. ', '> So, yield will return the augmentation generator and when the result of the function is called it will continue to produce new augmentations forever? and not just the same one over and over?\r\n\r\nYes, `yield` will make this function a generator and iteratively produce next value without end.', '@joelthchao Im assuming this isn\'t an issue with the combine_generator yield, but can\'t think of anything else. Im getting an error when trying to train saying that the generator is yielding `None`. The error is below, with my code at the bottom. \r\n\r\nThanks again for your help. \r\n\r\n```\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/threading.py"", line 801, in __bootstrap_inner\r\n    self.run()\r\n  File ""/usr/lib/python2.7/threading.py"", line 754, in run\r\n    self.__target(*self.__args, **self.__kwargs)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 429, in data_generator_task\r\n    generator_output = next(self._generator)\r\nStopIteration\r\n\r\nTraceback (most recent call last):\r\n  File ""trainNetwork.py"", line 56, in <module>\r\n    network.trainAugmentation(model, trainImages[:70,:,:,:,:], trainLabels[:70,:,:,:,:], dataLocation, newWeights, oldWeights, nb_epoch=60, batch=2, rotation_range=6, horizontal_flip=True, seed=1)\r\n  File ""/vol/programs/neuralSeg/simpleOneTissueNetwork/simpleNetwork.py"", line 639, in trainAugmentation\r\n    model.fit_generator(combinedGenerator, samples_per_epoch=70, nb_epoch=nb_epoch)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 1532, in fit_generator\r\n    str(generator_output))\r\nValueError: output of generator should be a tuple (x, y, sample_weight) or (x, y). Found: None\r\n```\r\n\r\n```\r\ndef combine_generator(gen1, gen2):\r\n    yield(gen1.next(), gen2.next())\r\n\r\ndef trainAugmentation(model, images, labels, dataLocation, newWeights, oldWeights, nb_epoch=60, batch=2, rotation_range=6, horizontal_flip=True, seed=1):\r\n    print(\'-\'*30)\r\n    print(\'Create image generators...\')\r\n    print(\'-\'*30)\r\n    image_datagen = ImageDataGenerator(rotation_range=rotation_range, horizontal_flip=horizontal_flip, order=3) \r\n    mask_datagen = ImageDataGenerator(rotation_range=rotation_range, horizontal_flip=horizontal_flip, order=0)\r\n    print(\'-\'*30)\r\n    print(\'Start fitting image generators...\')\r\n    print(\'-\'*30)\r\n    image_datagen.fit(images, augment=True, seed=seed)\r\n    mask_datagen.fit(labels, augment=True, seed=seed)\r\n    print(\'-\'*30)\r\n    print(\'Start image generators...\')\r\n    print(\'-\'*30)\r\n    image_generator = image_datagen.flow(images, batch_size=2, seed=seed, shuffle=True)\r\n    mask_generator = mask_datagen.flow(labels, batch_size=2, seed=seed, shuffle=True)\r\n    print(\'-\'*30)\r\n    print(\'Combine image generators...\')\r\n    print(\'-\'*30)\r\n    combinedGenerator = combine_generator(image_generator, mask_generator)\r\n    print(\'-\'*30)\r\n    print(\'Fitting model...\')\r\n    print(\'-\'*30)\r\n    model.load_weights(dataLocation + oldWeights)\r\n    model.fit_generator(combinedGenerator, samples_per_epoch=70, nb_epoch=nb_epoch) \r\n```', 'Sorry my bad. I forget to make it loop.\r\n```python\r\ndef combine_generator(gen1, gen2):\r\n    while True:\r\n        yield(gen1.next(), gen2.next())\r\n```', ""No need to apologize, you're helping me out a ton. That did the trick! \r\n\r\nNow I'm having  NaN issue with my loss, but I'm sure that's something I've screwed up inside the image augmentation file :). \r\n\r\nThanks! "", 'For anyone that might come along this and want some help with generators, I came across this article today that helps. \r\n\r\nhttp://pybit.es/generators.html', ""@gattia Did you manage to resolve this? I am following the official example from Keras: https://keras.io/preprocessing/image/\r\n\r\n```\r\n# we create two instances with the same arguments\r\ndata_gen_args = dict(featurewise_center=True,\r\n                     featurewise_std_normalization=True,\r\n                     rotation_range=90.,\r\n                     width_shift_range=0.1,\r\n                     height_shift_range=0.1,\r\n                     zoom_range=0.2)\r\nimage_datagen = ImageDataGenerator(**data_gen_args)\r\nmask_datagen = ImageDataGenerator(**data_gen_args)\r\n\r\n# Provide the same seed and keyword arguments to the fit and flow methods\r\nseed = 1\r\nimage_datagen.fit(images, augment=True, seed=seed)\r\nmask_datagen.fit(masks, augment=True, seed=seed)\r\n\r\nimage_generator = image_datagen.flow_from_directory(\r\n    'data/images',\r\n    class_mode=None,\r\n    seed=seed)\r\n\r\nmask_generator = mask_datagen.flow_from_directory(\r\n    'data/masks',\r\n    class_mode=None,\r\n    seed=seed)\r\n\r\n# combine generators into one which yields image and masks\r\ntrain_generator = zip(image_generator, mask_generator)\r\n\r\nmodel.fit_generator(\r\n    train_generator,\r\n    steps_per_epoch=2000,\r\n    epochs=50)\r\n```\r\n\r\nbut it loops indefinitely in `zip()`"", 'It\'s been about a year since I worked with this, and it\'s not my forte, however, I think the problem is that your train_generator should be a function with ""yield"" in it. I\'ve re-written what you have posted in the way that I think will fix your problem. \r\n\r\n    # combine generators into one which yields image and masks\r\n    def combineGenerator(gen1, gen2):\r\n        while True:\r\n            yield(gen1.next(), gen2.next())\r\n    train_generator = combineGenerator(image_generator, mask_generator)\r\n\r\nBelow I have copied snippets of code that Im currently using that rely on this method. Things might be a bit wonky because I wrote a custom version of image augmentation that works in 3D, but you should be able to get the general point. \r\n\r\n    def combine_generator(gen1, gen2):\r\n        while True:\r\n            yield(gen1.next(), gen2.next())    \r\n\r\n    image_datagen = threeDimageAug.ImageDataGenerator(rotation_range=rotation_range, horizontal_flip=horizontal_flip, order=1, height_shift_range=height_shift_range, width_shift_range=width_shift_range, slice_flip=True)`\r\n    mask_datagen = threeDimageAug.ImageDataGenerator(rotation_range=rotation_range, horizontal_flip=horizontal_flip, order=0, height_shift_range=height_shift_range, width_shift_range=width_shift_range, slice_flip=True)\r\n    image_generator = image_datagen.flow(images[:,:,:,:,:], batch_size=batch, seed=seed, shuffle=True)\r\n    mask_generator = mask_datagen.flow(labels[:,:,:,:,:], batch_size=batch, seed=seed, shuffle=True)\r\n    combinedGenerator = combine_generator(image_generator, mask_generator)\r\n\r\n    model.fit_generator(combinedGenerator, steps_per_epoch=steps_per_epoch, epochs=iterations_save, callbacks=[csv_checkpoint], verbose=verbose)`', ""@gattia what if we want to write the augmented image and masks on the disk. How can we use the\r\ntrain_generator = zip(image_generator, mask_generator)\r\nto write on the disk the generated images a long with their masks!\r\n\r\nI'm doing the following but the generated augmented masks don't match the generated augmented images:\r\n\r\nimage_datagen.fit(images, augment=True, seed=seed)\r\nmask_datagen.fit(masks, augment=True, seed=seed)\r\n\r\nimage_gen=image_datagen.flow(images, batch_size=1,shuffle=False, save_to_dir=dir_image, save_prefix='aug',save_format='png')\r\n\r\nmask_gen=image_datagen.flow(masks, batch_size=1,shuffle=False, save_to_dir=dir_mask, save_prefix='aug',save_format='png')\r\n\r\ni = 0\r\nfor batch in zip(image_gen, mask_gen):\r\ni += 1\r\nif i > 10:\r\nbreak""]","[""\r\ndef train(dataLocation):\r\n    print('-'*30)\r\n    print('Creating and compiling model...')\r\n    print('-'*30)\r\n    ## Augmentation Stuff\r\n    seed = 1\r\n    image_datagen = ImageDataGenerator(rotation_range=15, horizontal_flip=True, order=3) \r\n    mask_datagen = ImageDataGenerator(rotation_range=15, horizontal_flip=True, order=0)\r\n\r\n    image_datagen.fit(trainImages2, augment=True, seed=seed) \r\n    mask_datagen.fit(trainLabels2, augment=True, seed=seed)\r\n    print('start image generator')\r\n    image_generator = image_datagen.flow(trainImages2, batch_size=2, seed=seed)\r\n    print('start label generator')\r\n    mask_generator = mask_datagen.flow(trainLabels2, batch_size=2, seed=seed)\r\n    print('start zip')\r\n    train_generator = zip(image_generator, mask_generator)\r\n    print('done zipping? ')\r\n    \r\n    print('loadingModelweights')\r\n    #Get and fit model \r\n    model = get_vnet()\r\n    model.load_weights(dataLocation + 'vNet_Simplified_Femur_March_5_2017_UpdatingWeights_fromTibia.hdf5')\r\n    model_checkpoint = ModelCheckpoint(dataLocation + 'vNet_Simplified_Femur_March_11_2017_UpdatingWeights_fromTibia_Augmentation.hdf5', monitor='loss', save_best_only=True)\r\n    \r\n    print('-'*30)\r\n    print('Fitting model...')\r\n    print('-'*30)\r\n    model.fit_g\r\n\r\ntrain(dataLocation)\r\n""]",[],1,0
23,keras,11866,closed,Number of parameters of a dense layer with a 2D input,"I am using 2D data in a classification problem using keras.
So I am defining a keras model as following:




which returns a compiled model with the following number of parameters parameters


Reading the documentation and inspecting the [code](https://github.com/keras-team/keras/blob/88af7d0c97497b5c3a198ee9416b2accfbc72c36/keras/layers/core.py#L880) if the input shape has more than 1D it is going to use the last dimension as input dimension. 
Thus the layer has 1100 parameters = 10*100+ 100 (weights + bias).

What I don't understand is how the dot product between the input and the weights is performed. Because if the data is 2D with 5\*10 data points, and I am using only 100\*10 weights how are the weights and input values multiplied? If I change the input data from (5,10) --> (500,10) something changes?

I always have the idea that the output of the neuron k was something like:

where the x_1, x_2 are ALL the previous outpus, however in this case that is 2D I don't know what is happening here.


",type:support,"['It\'ll refer to ""5"" as batch.. What\'s the shape of your labels? Maybe you should consider adding `Flatten()` layer after the input.', '> It\'ll refer to ""5"" as batch.. What\'s the shape of your labels? Maybe you should consider adding `Flatten()` layer after the input.\r\n\r\nSupposing my data were a time series with 5 features and 10 timesteps, all my 5 different features would be multiplied by the same weights, right?\r\n\r\nOn the other side this were just the first layers of the network, then I a `Flatten()` layer, thanks for the suggestion btw.']","[""python\r\nin_ = Input((5, 10))\r\nout = Dense(100, activation='relu', name = 'dense_1')(in_)\r\n\r\nmodel = Model(in_, out)\r\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\r\nmodel.summary() \r\n"", 'python\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_9 (InputLayer)         (None, 5, 10)             0         \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 5, 100)            1100      \r\n=================================================================\r\nTotal params: 1,100\r\nTrainable params: 1,100\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n']","['f_k = Relu(w_{k,1}*x_1 + w_k2*x_2 + ... + w_{k,n} * x_n + b_k)']",1,0
24,keras,3675,closed,Memory consumption when using fit generator,"Hi, 

I have noticed that when I use fit generator with pickle safe to True, number of workers > 1, and max generator queue > 2 the python processes accumulate memory leading to hanging of the execution due to no more available memory (I have 40GB of RAM).

This does not happening when setting pickle safe to False. 
",,"['The process should accumulate memory as the queue is being filled. If you run out of memory, consider reducing `max_q_size`.\n\n`pickle_safe=False` will use [threading](https://docs.python.org/library/threading.html) instead of [multiprocessing](https://docs.python.org/library/multiprocessing.html), which is lighter on memory use but slower.\n', 'I meet the same problem. When I set `pickle_safe=True`, I find my memory is accumulating, which leads to memory out problem. However, I use fit generator with pickle safe to False, no memory error is raised.\n I wonder the reason why it happens.\n', ""I've run into the same thing and noticed that many child processes are being created (1 per training epoch) but those are not being joined after the epoch, resulting in a memory leak.\n"", 'I am facing the same problem. My data is about 167G and I used two processes which costs 123G+106G. I think the main problem is that the new batches are added to the multiprocessing queue which pickles the data. This is really slow and costs a lot of memory.\r\n\r\nA better idea is to maintain this data in a pre-allocated big array according to **max_q_size** and then put the index into the queue.', ""Yeah, refactoring would be welcome. I've moved over to setting `max_q_size=1`, `nb_worker=1` and using an external multiprocessing.Queue instead."", 'Perhaps Keras would benefit from looking into [Dask integration like tflearn](https://github.com/tflearn/tflearn/blob/master/examples/basics/use_dask.py) instead.', 'I watched [this video](https://www.youtube.com/watch?v=jLQ2wAwapRg&list=PLGB9meziqbzpRP7mVyihOihNzm_J2Kx9I&index=6) and it looks nice.\r\n\r\nThe big advantage is that it can utilize multiple gpu and one gpu for multiple process. But I am not sure it is compatible with Theano.\r\n\r\n', ""My thought was that you'd run the Dask graph on CPU and leave the GPU alone for Keras and model training. I'd bet most preprocessing is faster than the training graph, otherwise one could also use joblib.Memory or something. Of course, it should also be possible to just add all preprocessing into Theano/TensorFlow's graph and set device properly, but that might be overkill."", 'I agree and for me the preprocessing is just normalization which can be added to the model directly. \r\nAnd I also found that although the free memory is low, most memory is used for disk cache. This is reasonable since I used mmap to load the data.', '@yanyongluan  Did you know the reason of the problem?', 'I encounter the same problem using fit_generator on a large train set with pickle_safe=False and a threading generator, it just keep allocating ram until it eventually goes to swap. Anyone founds a solution?', ""@lucabergamini Have you found a solution? I run into the same issue, using Keras 2.0.2 and Tensorflow 1.1.0. I've set pickle_safe=False, workers=1, but the memory usage just keeps growing.\r\n\r\nTried downgrading to Tensorflow 1.0.1 and Keras 1.1.1, but to no success."", '@riley-x  eventually I switched to another machine and now I\'m not experiencing the memory allocation bug, but I also changed my code a lot implementing multi-thread. Any way my sw configuration is the following:\r\n\r\n- tensorflow==1.2.0rc0\r\n\r\n- Keras==2.0.4\r\n\r\nAnd this is how my generator class  looks like now (I do some GC calls, but honestly i don\'t know if those make any differences or if I just leave them there from  previous version, you can try to remove them and see if something changes...):\r\n\r\n    def __init__(self, base_folder, generator_batch, label_size, image_shape):\r\n        self.train_data_list = numpy.asarray(sorted([i for i in os.listdir(base_folder) if i.endswith("".jpg"")]))\r\n        self.train_labels_list = numpy.asarray(sorted([i for i in os.listdir(base_folder) if i.endswith("".npy"") and ""_p"" not in i]))\r\n        self.generator_batch = generator_batch\r\n        self.base_folder = base_folder\r\n        self.label_size = label_size\r\n        assert len(self.train_data_list) == len(self.train_labels_list)\r\n        self.lock = threading.Lock()\r\n        self.image_shape = image_shape\r\n        self.i = 0\r\n\r\n    def __iter__(self):\r\n        return self\r\n\r\n    def get_len(self):\r\n        return len(self.train_data_list)\r\n\r\n    def next(self):\r\n        # LOCK\r\n        with self.lock:\r\n            if self.i == 0 or len(\r\n                    self.train_data_list[self.i * self.generator_batch:(self.i + 1) * self.generator_batch]) == 0:\r\n                self.i = 0\r\n                # shuffle\r\n                perm = numpy.random.permutation(self.get_len())\r\n                self.train_data_list = self.train_data_list[perm]\r\n                self.train_labels_list = self.train_labels_list[perm]\r\n\r\n            train_data_list_t = self.train_data_list[self.i * self.generator_batch:(self.i + 1) * self.generator_batch]\r\n            train_labels_list_t = self.train_labels_list[\r\n                                  self.i * self.generator_batch:(self.i + 1) * self.generator_batch]\r\n            self.i += 1\r\n            # GC\r\n            if self.i + 1 % 100 == 0:\r\n                for i in xrange(10):\r\n                    gc.collect()\r\n\r\n        # NO LOCK\r\n       \r\n        train_data = numpy.zeros((len(train_data_list_t), self.image_shape[0], self.image_shape[1], 3), dtype=""float16"")\r\n        train_labels = numpy.zeros((len(train_labels_list_t), self.label_size, 4, 2), dtype=""float16"")\r\n        for j in xrange(len(train_data_list_t)):\r\n            image_t = cv2.imread(self.base_folder + train_data_list_t[j])\r\n            label_t = numpy.load(self.base_folder + train_labels_list_t[j])[0:self.label_size]\r\n           # DATA AUGMENTATION HERE\r\n            train_data[j] = image_t\r\n            train_labels[j] = label_t\r\n        return (train_data, train_labels)\r\n\r\n\r\n\r\n', ""Im having similar issues with the latest tf and keras (1.2 and 2.0.4)\r\n\r\ni define my generator:\r\n\r\n```\r\ndef generate_data(N=32):\r\n\r\n    while True:\r\n        x  = []\r\n        y = []\r\n\r\n        D = generate_action_space(N)\r\n        for d in D:\r\n            x.append(cube2np(d[0]))\r\n            y.append(to_categorical(possible_moves.index((str(d[1]))),len(possible_moves)))\r\n\r\n        x = np.asarray(x)\r\n        x = x.reshape(x.shape[0],18, 3, 1)\r\n        x = x.astype('float32')\r\n\r\n        y = np.asarray(y)\r\n        y = y.reshape(y.shape[0],y.shape[2] )\r\n\r\n        count+=1\r\n        \r\n        yield (x,y)\r\n\r\n```\r\nif i use fit_generator directly:\r\n ```\r\nmodel.fit_generator(generator= generate_data(32),steps_per_epoch=50,\r\n                                  epochs=30,verbose=2,validation_data=None,callbacks = [tbCallBack],max_queue_size=1,use_multiprocessing=True,workers=6,initial_epoch =0)\r\n\r\n```\r\n \r\n\r\ni find that my ram increases continually until my system runs out of memory.\r\n\r\nA work around that I found is using fit_generator is using a for loop : \r\n\r\n```\r\nfor j in range(10):\r\n    model.fit_generator(generator= generate_data(32),steps_per_epoch=50,\r\n                                  epochs=3,verbose=2,validation_data=None,callbacks = [tbCallBack],max_queue_size=1,use_multiprocessing=True,workers=6,initial_epoch =0)\r\n```\r\nThis limits my ram and i am able to train as much as I want. My accuracy seems to be steadily increasing. However, this doesn't behave nicely with the tensorboard background. It also seems bodgy and like it might not be the best way to implement this. Does anyone have suggestions how to make it work with tensorboard and if there is a better way to do this?\r\n"", ""I also have an issue I believe is the same as or related to this one.\r\n\r\nIf I use `fit_generator` with `use_multiprocessing` in a Jupyter notebook, then stop the execution (the equivalent of a `KeyboardInterrupt`) and restart the kernel, then Tensorflow still has GPU memory allocated. This keeps me from running code again until I restart Jupyter. Usually this wouldn't be a problem, but it's an issue if I have other code in the middle of the long training process."", 'Any solution to this yeT?', 'same problem...\r\nrunning into ResourceExhaustedError: OOM\r\n\r\n', ""Same here, running into an OOM after a few iterations. I am using a Sequence generator in my case:\r\n```\r\n    def __getitem__(self, i):        \r\n        for n, (image_path, label_path) in enumerate(zip(self.image_path_list[i*self.batch_size:(i+1)*self.batch_size], self.label_path_list[i*self.batch_size:(i+1)*self.batch_size])):\r\n\r\n            image = cv2.imread(image_path, 1)\r\n            label = cv2.imread(label_path, 0)\r\n            if self.resize_shape:\r\n                image = cv2.resize(image, self.resize_shape)\r\n                label = cv2.resize(label, self.resize_shape)\r\n\r\n            # Do augmentation (only if training)\r\n            if self.mode == 'training':\r\n                if self.horizontal_flip and random.randint(0,1):\r\n                    image = cv2.flip(image, 1)\r\n                    label = cv2.flip(label, 1)\r\n                if self.vertical_flip and random.randint(0,1):\r\n                    image = cv2.flip(image, 0)\r\n                    label = cv2.flip(label, 0)\r\n                if self.brightness:\r\n                    factor = 1.0 + abs(random.gauss(mu=0, sigma=self.brightness))\r\n                    if random.randint(0,1):\r\n                        factor = 1.0/factor\r\n                    image = (255.0*((image/255.0)**factor)).astype(np.uint8)\r\n                if self.rotation:\r\n                    angle = random.gauss(mu=0.0, sigma=self.rotation)\r\n                else:\r\n                    angle = 0.0\r\n                if self.zoom:\r\n                    scale = random.gauss(mu=1.0, sigma=self.zoom)\r\n                else:\r\n                    scale = 1.0\r\n                if self.rotation or self.zoom:\r\n                    M = cv2.getRotationMatrix2D((image.shape[1]//2, image.shape[0]//2), angle, scale)\r\n                    image = cv2.warpAffine(image, M, image.shape[:2])\r\n                    label = cv2.warpAffine(label, M, label.shape[:2])\r\n                if self.crop_shape:\r\n                    image, label = _random_crop(image, label, self.crop_shape)\r\n\r\n            self.X[n] = image\r\n            self.Y1[n] = to_categorical(cv2.resize(label, (label.shape[1]//4, label.shape[0]//4)), self.n_classes).reshape((label.shape[0]//4, label.shape[1]//4, -1))   \r\n            self.Y2[n] = to_categorical(cv2.resize(label, (label.shape[1]//8, label.shape[0]//8)), self.n_classes).reshape((label.shape[0]//8, label.shape[1]//8, -1))\r\n            self.Y3[n] = to_categorical(cv2.resize(label, (label.shape[1]//16, label.shape[0]//16)), self.n_classes).reshape((label.shape[0]//16, label.shape[1]//16, -1))         \r\n        \r\n        return self.X, [self.Y1, self.Y2, self.Y3]\r\n```\r\n\r\n```\r\nnet.fit_generator(train_generator, len(train_generator), opt.epochs, callbacks=[checkpoint, tensorboard, lr_decay], validation_data=val_generator, validation_steps=len(val_generator), workers=opt.n_cpu, use_multiprocessing=True, shuffle=True)\r\n```\r\n\r\nAre you aware of this issue @fchollet?       "", 'Had the same issue. Added gc.collect() to the end of my custom generator and it helped to get rid of memory errors.', 'Exact same issue here. Anyone has a solution? ', '@oleg-panichev solution worked for me,  but after setting use_multiprocessing to false', 'Hello there. I had the same problem and ran out of memory after  some time. I have sticked with the solution of @carlthome and written a custom MP queue, which I use with fit_generator and use_mutliprocessing=False. To save some of you some hassle, you can find the code attached. Since for my application each batch is generated randomly, the current implementation does not support indexed batches.\r\nFor me this implementation works and I have not experienced out-of-memory problems since. However, as documented in https://github.com/keras-team/keras/issues/6499#issuecomment-351068255 Python\'s multiprocessing needs to be handled with care.\r\n\r\n```\r\nfrom keras.utils import Sequence\t\r\nfrom multiprocessing import Process, Queue\r\nfrom abc import ABCMeta, abstractmethod\r\n\r\nclass ClBatchGenerator(Process):\r\n    __metaclass__ = ABCMeta\r\n\r\n    @abstractmethod\r\n    def generateBatch(self):\r\n        ""Please implement me.""\r\n        batch = None\r\n        return batch\r\n\r\n    def __init__(self, batchQueue, notificationQueue):\r\n        Process.__init__(self)\r\n        self._batchQueue = batchQueue\r\n        self._notificationQueue = notificationQueue\r\n\r\n    def run(self):\r\n        while True:\r\n            msg = self._notificationQueue.get()\r\n            if msg==""terminate"":\r\n                break\r\n            elif msg==""generateBatch"":\r\n                batch = self.generateBatch()\r\n                self._batchQueue.put(batch)\r\n            else:\r\n                raise ValueError(""Message %s unknown""%msg)\r\n\r\nclass ClMPDataGenerator(Sequence):\r\n    """"""\r\n    Multiprocessing data generator to be passed to fit_generator. \r\n    This spawns multiple instances of ClBatchGenerator.\r\n    """"""\r\n\r\n    def __init__(self, workers, max_queue_size, steps_per_epoch)\r\n        self._workers = workers\r\n        self._max_queue_size = max_queue_size\r\n        self._steps_per_epoch = steps_per_epoch\r\n        self._batchQueue = Queue(max_queue_size)\r\n        self._notificationQueue = Queue()\r\n        self._workerList = []\r\n        for i in range(workers):\r\n            oWorker = ClBatchGenerator(self._batchQueue, self._notificationQueue)\r\n            oWorker.daemon = True #makes sure the workers terminate if the main process is exited\r\n            oWorker.start()\r\n            self._workerList.append(oWorker)\r\n        for i in range(max_queue_size):\r\n            self._notificationQueue.put(""generateBatch"")\r\n\r\n    def __len__(self):\r\n        return self._steps_per_epoch\r\n\r\n    def __getitem__(self, idx):\r\n        batch = self._batchQueue.get()\r\n        self._notificationQueue.put(""generateBatch"")\r\n        return batch\r\n\r\n    def __del__(self):\r\n        #put termination message\r\n        for i in range(self._workers)\r\n            self._notificationQueue.put(""terminate"")\r\n        #empty queues, otherwise the workers won\'t terminate\r\n        for i in range(self._max_queue_size):\r\n            self._batchQueue.get(timeout=10)\r\n```', '@check0104 thanks for sharing. Do you by any chance have the code that was causing the OOM so we can have it for reference to help debug? I am compiling a sample on my own and would love to see if the issue appears with similar configs?', ""I'm having the same problem. If I set an higher number of workers the training process seems faster but then it get stuck. It looks like the GPU is not able to consume all the data in the queue, however it never achieves 100% utilisation. Did someone find the solution? \r\n\r\n```python\r\n    model.fit_generator(generator=training_generator,\r\n                        validation_data=validation_generator,\r\n                        epochs=epochs,\r\n                        verbose=1,\r\n                        max_queue_size=10,\r\n                        use_multiprocessing=True,\r\n                        workers=workers,\r\n                        shuffle=False,\r\n                        callbacks=callbacks)\r\n```"", ""I have just discovered that the memory (RAM) consumption keeps increasing during the training. Then, at a certain point, the memory get full and the training process get stuck. I guess that the problem is in the generator when I use h5py to read an image from a one of the .h5 files (__data_generation method), however I don't know how to solve it\r\n\r\n```python\r\nimport keras\r\nimport numpy as np\r\nimport h5py\r\nimport os\r\n\r\n\r\nclass DataGeneratorC(keras.utils.Sequence):\r\n    'Generates data for Keras'\r\n    def __init__(self, h5files, batch_size=32, shuffle=True):\r\n        self.batch_size = batch_size\r\n        self.h5files = h5files\r\n        self.indexes = np.array([], dtype=np.int64).reshape(0, 3)\r\n        self.shuffle = shuffle\r\n        self.n_images = 0\r\n        self.generate_indexes()\r\n        self.on_epoch_end()\r\n\r\n    def __len__(self):\r\n        'Denotes the number of batches per epoch'\r\n        # total number of images in the dataset\r\n        return int(np.floor(self.n_images/self.batch_size))\r\n\r\n    def __getitem__(self, index):\r\n        'Generate one batch of data'\r\n        # index goes from 0 to the number of batches\r\n        # Generate indexes of the batch\r\n        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\r\n\r\n        # Generate data\r\n        x, y = self.__data_generation(indexes)\r\n\r\n        return x, y\r\n    \r\n    def generate_indexes(self):\r\n        # content not important \r\n\r\n    def on_epoch_end(self):\r\n        'Updates indexes after each epoch'\r\n        if self.shuffle:\r\n            np.random.shuffle(self.indexes) \r\n\r\n    def __data_generation(self, indexes):\r\n        'Generates data containing batch_size samples'\r\n        # Initialization\r\n        x = np.empty([self.batch_size, 100, 100])\r\n        y = np.empty([self.batch_size], dtype=int)\r\n\r\n        # Generate data\r\n        for i, row in enumerate(indexes):\r\n            filename = self.h5files[row[0]]\r\n\r\n            h5f = h5py.File(filename, 'r')\r\n            # Store image\r\n            x[i, ] = h5f['section/array_images'][row[1]]\r\n            h5f.close()\r\n            # Store class\r\n            y[i] = row[2]\r\n\r\n        x = x.reshape(x.shape[0], 1, 100, 100)\r\n\r\n        return x, y\r\n```"", 'Hello,\r\nThis issue is over a year old, you may want to submit a new issue with an up-to-date version of Keras/TF.\r\n\r\nPlease submit a reproducible example.\r\nSide note: HDF5 are not easily shareable across processes and this may be the cause of your issue.', ""> I have just discovered that the memory (RAM) consumption keeps increasing during the training. Then, at a certain point, the memory get full and the training process get stuck. I guess that the problem is in the generator when I use h5py to read an image from a one of the .h5 files (__data_generation method), however I don't know how to solve it\r\n> \r\n> ```python\r\n> import keras\r\n> import numpy as np\r\n> import h5py\r\n> import os\r\n> \r\n> \r\n> class DataGeneratorC(keras.utils.Sequence):\r\n>     'Generates data for Keras'\r\n>     def __init__(self, h5files, batch_size=32, shuffle=True):\r\n>         self.batch_size = batch_size\r\n>         self.h5files = h5files\r\n>         self.indexes = np.array([], dtype=np.int64).reshape(0, 3)\r\n>         self.shuffle = shuffle\r\n>         self.n_images = 0\r\n>         self.generate_indexes()\r\n>         self.on_epoch_end()\r\n> \r\n>     def __len__(self):\r\n>         'Denotes the number of batches per epoch'\r\n>         # total number of images in the dataset\r\n>         return int(np.floor(self.n_images/self.batch_size))\r\n> \r\n>     def __getitem__(self, index):\r\n>         'Generate one batch of data'\r\n>         # index goes from 0 to the number of batches\r\n>         # Generate indexes of the batch\r\n>         indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\r\n> \r\n>         # Generate data\r\n>         x, y = self.__data_generation(indexes)\r\n> \r\n>         return x, y\r\n>     \r\n>     def generate_indexes(self):\r\n>         # content not important \r\n> \r\n>     def on_epoch_end(self):\r\n>         'Updates indexes after each epoch'\r\n>         if self.shuffle:\r\n>             np.random.shuffle(self.indexes) \r\n> \r\n>     def __data_generation(self, indexes):\r\n>         'Generates data containing batch_size samples'\r\n>         # Initialization\r\n>         x = np.empty([self.batch_size, 100, 100])\r\n>         y = np.empty([self.batch_size], dtype=int)\r\n> \r\n>         # Generate data\r\n>         for i, row in enumerate(indexes):\r\n>             filename = self.h5files[row[0]]\r\n> \r\n>             h5f = h5py.File(filename, 'r')\r\n>             # Store image\r\n>             x[i, ] = h5f['section/array_images'][row[1]]\r\n>             h5f.close()\r\n>             # Store class\r\n>             y[i] = row[2]\r\n> \r\n>         x = x.reshape(x.shape[0], 1, 100, 100)\r\n> \r\n>         return x, y\r\n> ```\r\n\r\nHave found out how to solve it? I run into the exact same issue. My generator is pretty similar to yours. It doesn't matter if use just one worker and max_q to 1 and multi processing to false. the memory is just being filled with every batch.\r\n\r\nI would really appreciate any help. As it is right now, the fit generator is a worse choice compared to the normal fit function."", ""> Since for my application each batch is generated randomly, the current implementation does not support indexed batches.\r\nhttps://github.com/keras-team/keras/issues/3675#issuecomment-352185331\r\n@csandmann isn't there a probability of batches getting repeated, How do you keep track of batches?"", 'https://fantashit.com/linearly-increasing-memory-with-use-multiprocessing-and-keras-sequence/#comment-254237']",[],[],1,0
25,keras,11356,closed,ConvLSTM2D very slow during training,"When training LSTM models, switching to CuDNNLSTM improves the training speed significantly on GPU machines. Training ConvLSTM2D models is extremely slow, is there a plan (or is it even possible) to make sth like CuDNNConvLSTM2D?",type:support,[],[],[],1,0
26,keras,8289,closed,"Why use adam optimizer ,train loss only at the beginning of each epoch it drops soon and then slows down？","I found use adam optimizer ,train loss  only at the beginning of each epoch it drops soon and then slows down。


like this：
epoch 1/100： loss 70.0 -> 60.0 ->55. -> ...... ->40.0  ->39.95 ->39.90->39.89->.....
epoch 2/100： loss 30 -> 25 ->20 -> ...... ->18.0  ->17.9  ->.....->17.75 ->17.73
epoch 3/100： loss 10 -> 8 ->7 -> ...... ->6  ->6.9  ->.....
...








",,[],[],[],1,0
27,keras,8802,closed,"When Dropout layer is shared Siamese-style, dropped units are not synchronized.","When sharing  layers Siamese-style, I wasn't able to synchronize dropped units. For example, in the code below,  has no effect.  parameter has no effect either. Shared  layers should synchronize dropped units by default, otherwise they are not shared in any meaningful way.

Output:


Note that
1. during inference nothing is dropped, so loss is 0, as expected
2. if dropout rate=0, the loss is 0 during training, as expected
3. Note that on Epochs 2,3,7,8,9 the loss is 4, which I don't understand at all. Maybe another bug, I need to investigate further.",,"[""Dropout is a random operation, so the dropout pattern is unique for every data flow.\r\n\r\nTo create a dropout pattern that will be shared across different data flows, you can create a custom layer, which would do the following:\r\n\r\n- maintain a non-trainable weight `dropout_mask`, the size of the layer input, initialized to ones\r\n- multiples its inputs by `dropout_mask` (times a scaling factor) in train mode only, in `call`\r\n- adds an update op to vary `dropout_mask` at every step. You can also make this update be train-mode-only.\r\n\r\nSomething like;\r\n\r\n```python\r\n\r\nclass MyDP(Layer):\r\n    def __init__(self, rate, **kwargs):\r\n       self.rate = rate\r\n       super(...).__init__(**kwargs)\r\n\r\n    def build(self, input_shape):\r\n       self.dropout_mask = self.add_weight(shape=input_shape, initializer='ones', trainable=False)\r\n\r\n   def call(self, inputs):\r\n      dropped =  K.in_train_phase(self.dropout_mask * inputs * ...)\r\n      if not self.updates:\r\n         self.add_update(self.dropout_mask, K.dropout(self.dropout_mask * 0 + 1, self.rate)\r\n     return droppped\r\n```\r\n\r\nWarning: untested, incomplete, etc"", ""I think that item 3 above (loss=4) is due to the fact that Dropout multiplies activations by `1/(1-p)` during training, a well-known trick to avoid dividing during inference). So that's correct.\r\n\r\nWith unsynchronized masks, shared Dropout layers are not actually shared in any meaningful way. I can't tell if it's a bug/misfeature in Keras or the TF backend (if so, I'll file the bug report there). Of course it could be impractical due to performance or architectural reasons, in which case it should be clearly documented.\r\n\r\nTo large degree this is an issue of mental model of shared layers - I (probably erroneously) thought that everything of physically collocated, so shared masks are automatic.\r\n\r\nUnshared masks were unexpected to me, and I came to this realization purely accidentally, while debugging Siamese network training. When fed identical data, two networks with supposedly shared everything, produce different final activations.\r\n\r\nApparently, I have multiple sources of stochasticity, which I resolve one-by-one. The ones I identified so far are `Dropout` and `BatchNormalization`, but there could be more.\r\n\r\nReplacing standard dropout layer with a custom one is awkward, because the base network is pretrained, so I'd need to do network surgery, which I am not entirely sure how to.\r\n\r\nBTW, setting `seed` does appear to work after all (there was a bug in my earlier version of the test).\r\n"", '`seed` also survives `save_model()/load_model()`, so that gives a workable workaround.', 'almost certainly not, since it would make no sense and would be a severe bug, not likely to go unnoticed. ', 'I think the issue @ozabluda posted is very good/important. If we have a Siamese network, there is a possibility that it is risky to apply certain techniques like Dropout and Batch Normalization. I personally tried this in a Siamese network and I noticed the performance was very bad!  \r\n\r\nDo you have any idea how to improve this yet @ozabluda? Thanks!', ""OK so I implemented a shared Dropout layer by myself, similar to what @fchollet suggested.\r\n\r\n````\r\nclass SharedDropout(Layer):\r\n\t# learnt this from this link: https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/ops/nn_ops.py\r\n\tdef __init__(self, keep_prob_rate=0.5, **kwargs):\r\n\t\tself.keep_prob_rate = keep_prob_rate\r\n\t\tsuper(SharedDropout, self).__init__(**kwargs)\r\n\r\n\tdef build(self, input_shape):\r\n\t\tsuper(SharedDropout, self).build(input_shape)\r\n\r\n\tdef call(self, inputs):\r\n\t\tinput_left = inputs[0]\r\n\t\tinput_right = inputs[1]\r\n\t\trandom_tensor = self.keep_prob_rate\r\n\t\trandom_tensor += tf.random_uniform(tf.shape(input_left), dtype=input_left.dtype)\r\n\t\tbinary_tensor = tf.floor(random_tensor)\r\n\r\n\t\tdef DropoutLeft():\r\n\t\t\tret_left = tf.divide(input_left, self.keep_prob_rate) * binary_tensor\r\n\t\t\treturn ret_left\r\n\r\n\t\tdef DropoutRight():\r\n\t\t\tret_right = tf.divide(input_right, self.keep_prob_rate) * binary_tensor\r\n\t\t\treturn ret_right\r\n\r\n\t\treturn [K.in_train_phase(DropoutLeft, input_left, training=None), K.in_train_phase(DropoutRight, input_right, training=None)]\r\n\r\n\tdef compute_output_shape(self, input_shapes):\r\n\t\treturn [(input_shapes[0][0], input_shapes[0][1]), (input_shapes[1][0], input_shapes[1][1])]\r\n\r\n````\r\n\r\nExample to call the layer:\r\n````\r\nx_input_left = Input(shape=(10,), name='x_input_left')\r\n\r\nx_input_right = Input(shape=(10,), name='x_input_right')\r\n\r\nshared_dropout = SharedDropout()\r\n\r\nx_left, x_right = shared_dropout([x_input_left, x_input_right])\r\n````\r\n\r\nIt took me a few hours to implement/test this. So I am proud of this for a moment :D ""]","[""\r\n# https://gist.github.com/ozabluda/bbc6c84c0e69bfd9ca55170fd3ab040d\r\n# https://github.com/keras-team/keras/issues/8802\r\n\r\nfrom keras.models import Sequential, Model\r\nfrom keras.layers import Dense, Dropout, Input, Lambda\r\nfrom keras import backend as K\r\nimport numpy as np\r\n\r\nm = Sequential([\r\n    Dropout(rate=0.5, input_shape=(1,), noise_shape=(1,1))\r\n])\r\nm.summary()\r\n\r\ninput_a = Input(shape=(1,))\r\ninput_b = Input(shape=(1,))\r\n\r\nprocessed_a = m(input_a)\r\nprocessed_b = m(input_b)\r\n\r\ndef l1_distance((x1, x2)):\r\n    return K.sum(K.abs(x1-x2), axis=1)\r\n\r\nc = Lambda(l1_distance, output_shape=(1,))([processed_a, processed_b])\r\ns = Model([input_a, input_b], c)\r\ns.compile(optimizer='sgd', loss='mse')\r\ns.summary()\r\n\r\nx0 = np.array([1])\r\nx1 = np.array([1])\r\nx  = [x0,x1]\r\ny  = np.array([0])\r\n\r\ns.fit(x, y, verbose=1, epochs=10)\r\n\r\nprint(s.evaluate(x,y), s.predict(x))\r\n"", '\r\nEpoch 1/10\r\n1/1 [==============================] - 1s 1s/step - loss: 0.0000e+00\r\nEpoch 2/10\r\n1/1 [==============================] - 0s 3ms/step - loss: 4.0000\r\nEpoch 3/10\r\n1/1 [==============================] - 0s 3ms/step - loss: 4.0000\r\nEpoch 4/10\r\n1/1 [==============================] - 0s 4ms/step - loss: 0.0000e+00\r\nEpoch 5/10\r\n1/1 [==============================] - 0s 3ms/step - loss: 0.0000e+00\r\nEpoch 6/10\r\n1/1 [==============================] - 0s 3ms/step - loss: 0.0000e+00\r\nEpoch 7/10\r\n1/1 [==============================] - 0s 3ms/step - loss: 4.0000\r\nEpoch 8/10\r\n1/1 [==============================] - 0s 3ms/step - loss: 4.0000\r\nEpoch 9/10\r\n1/1 [==============================] - 0s 3ms/step - loss: 4.0000\r\nEpoch 10/10\r\n1/1 [==============================] - 0s 4ms/step - loss: 0.0000e+00\r\n1/1 [==============================] - 0s 14ms/step\r\n(0.0, array([ 0.], dtype=float32))\r\n']","['Dropout', 'noise_shape', 'seed', 'Dropout']",1,0
28,keras,1133,closed,Why the performance of new keras reduce so much?,"When i use the cnn with new keras, eg keras 0.3.0, i get a quite slow cnn training using mnist_cnn.py 2x faster than cpu version. Well, keras 0.2.0 got a almost 60x faster than cpu. 
I don't understand.
What happend in Dec 01, 2015?
",,"[""The example script has changed quite a bit so it's not an apples to apples comparison. \n\nOne thing that did change on the Theano side was the removal of the `image_shape` conv2d argument. It is to be deprecated anyway.\n""]",[],[],1,0
29,keras,10350,closed,Incorrect accuracy measure for padded target sequence in seq2seq model,"Hi,

I used the basic character-level seq2seq model [example](https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py) for French-to-English translation.

The accuracy measure seems to comparing the entire decoder output, i.e max_decoder_output_length.
and I would like the accuracy to represent only the characters till first occurrence of the stop character ('\n' in this case) and not compare the predicted output to padded target output.

For example:
Consider the example that gives a 63 character long decoder sequence:

i.e **decoder_target_output** is One Hot Representation of ( '' empty string represents a zero padded input vector)
['C', 'o', 'u', 'r', 'e', 'z', '\xe2', '\x80', '\xaf', '!', '\n', '', '', '', '' '', '', '', '', '', '', '' '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '' '', '', '', '', '' '', '', '', '', '', '', '', '', '', '', '', '' '', '', '', '', '', '', '', '']

after fitting, the **predicted_decoder_output** gives output using Argmax on One Hot Representation as

['C', 'o', 'u', 'r', 'e', 'z', '\xe2', '\x80', '\xaf', '!', '\n', 'i', 'r', ' ', '?', '\n', 'i', 'l', '\xc3', '\xa9', ' ', '?', '\n', 'i', ' ', 'd', 'e', 'u', 's', ' ', 'c', 'o', 'u', 'r', 'e', '\xe2', '\x80', '\xaf', '!', '\n', '\n', '\n', ' ', '!', '\n', '\n', 'i', ' ', 'c', 'h', 'i', 'e', '\xe2', '\x80', '\xaf', '!', '\n', '\n', '\n', ' ', 'd', 'i', 'r', 'e'] 

keras_accuracy = 11/63

however, the relevant accuracy is = 11/11 as I don't need the model to consider accuracy for beyond the stop character.
Is there a way to get correct accuracy by ignoring the padded output ?
",,[],[],[],1,0
30,keras,4728,closed,Layer state value always zero,"I am new to keras and deep learning in general. I am trying to implementation Visual Attention based Image Caption generation based on [Xu et. al](https://arxiv.org/pdf/1502.03044.pdf) I have created a new class AttentionLSTM based on the existing LSTM class. I want to retrieve the value of one of the states (alpha - the weights of features vectors), however whenever I access it (at the end of each batch), it is always comes up as an all-zero tensor. My model is as follows:

	SEQUENCE_LENGTH = 45
	MAX_SENTENCE_LENGTH = SEQUENCE_LENGTH - 3 # 1 for image, 1 for start token, 1 for end token
	OUTPUT_DIM = 512
	ANNOTATION_DIM = 512
	WORD_DIM = 512
	ANNOTATION_SIZE=196

	x_inp = Input(shape=(SEQUENCE_LENGTH-1, VOCAB_COUNT))
	z_inp = Input(shape=(ANNOTATION_SIZE, ANNOTATION_DIM,))
	z_mean = Input(shape=(ANNOTATION_DIM,))
	h_Dense = Dense(OUTPUT_DIM, input_dim=ANNOTATION_DIM, activation='softmax')(z_mean)
	c_Dense = Dense(OUTPUT_DIM, input_dim=ANNOTATION_DIM, activation='softmax')(z_mean)
	xt_dense = TimeDistributed(Dense(WORD_DIM))(x_inp)
	aLstm_Layer = AttentionLSTM(output_dim=WORD_DIM, z_dim=ANNOTATION_DIM, W_regularizer=l2(0.01), U_regularizer=l2(0.01), Z_regularizer=l2(0.01), dropout_W=0.3, dropout_U=0.3, dropout_Z=0.3, return_sequences=True)
	aLstm = aLstm_Layer([xt_dense, h_Dense, c_Dense, z_inp])
	tdense = TimeDistributed(Dense(VOCAB_COUNT))(aLstm)
	act = Activation('softmax')(tdense)
	model = Model(input=[x_inp, z_inp, z_mean], output=act)

	model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])
	model.fit(x_train, y_train, batch_size=1, nb_epoch=1, verbose=1)

My attention has the following code in  function

	def step(self, x, states):
		prev_h1 = states[0]
		prev_c1 = states[1]
		proj_z = states[2]
		alphaz = states[3]
		B_U = states[4]
		B_W = states[5]
		B_Z = states[6]

		proj_state = K.dot(prev_h1, self.Wd_att)
		proj_z = proj_z + proj_state[:, None, :]
		proj_list = []
		proj_list.append(proj_z)
		proj_z = K.tanh(proj_z)

		alpha = K.dot(proj_z, self.U_att ) + self.b2_att
		alpha_shape = alpha.shape
		alpha = K.softmax(alpha.reshape((alpha_shape[0], alpha_shape[1])))

		alphaz = alpha
		self.alphaz = alpha

		z = (self.initial_z * alpha[:, :, None]).sum(1)
                #Remaing code same as LSTM.step()

To get the alpha value, I have defined the following function:
    alphaz = aLstm_Layer.states[3]
    alpha_func = K.function([x_inp, z_inp, z_mean], alphaz)
    al = alpha_func(x_train)
    print(al)

The above print statement always returns


I am setting alpha to zero in  and . 

Am I doing something wrong (with the model or the way I retrieve alpha) ? Is there a better way to get the value of  ? (I am doing this because I don't know if there's a way to make a layer give multiple outputs)

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).",,['#41 seems to have solved the issue. Closing this.'],[],"['step', ""b'CudaNdarray([[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\\n   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\\n   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\\n   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\\n   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\\n   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\\n   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\\n   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\\n   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\\n   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\\n   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]])'"", 'reset_states()', 'get_initial_states()', 'layer.states']",1,0
31,keras,11897,closed,Batch Norm Layer weights/parameters turning NaNs during training of a Siamese Network with Triplet Loss,"With reference to https://stackoverflow.com/questions/53400472/keras-model-weights-for-some-layers-become-all-nans , what I have noticed that the ""NaNs"" are present only in the ""batch_norm"" layers. Why must this be happening?

Is there some problem with the training - like Vanishing Gradient or Gradients exploding? or are the NaNs only placeholders for biases?

(Just a prior - I am trying to train a Siamese Network with Triplet Loss as mentioned here https://stackoverflow.com/questions/53400472/keras-model-weights-for-some-layers-become-all-nans (architecture and loss) ) 

Sample Weights listed here-
https://stackoverflow.com/questions/53830547/batch-norm-layer-weights-parameters-turning-nans-during-training-of-a-siamese-ne

Link to the Gist for the code I am using for training - https://gist.github.com/sidgairo18/dca347edd4588484237a231d7dab9a63

Please make sure that the boxes below are checked before you submit your issue.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.

Thank you!

- [Yes] Check that you are up-to-date with the master branch of Keras. You can update with:


- [Yes] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [Yes] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).",,"['Is there a chance that there is a numerical instability matter associated with BN in Keras or Tensorflow as earlier experienced in Caffe?\r\n\r\nLink : https://github.com/BVLC/caffe/pull/5136', ""Let's close this issue and track the bug with #11927."", '@gabrieldemarmiesse Hello. \r\nI see the issues closed one by one, but it seems that for 2 years still nonone has fixed the issue.\r\nIs anyone working on the topic?\r\n\r\nI also experience this problem in 2020 and first mentions of the problem are from 2018.\r\n\r\nI believe there is a fundamental problem with how shared vision model instances not aggregate correctly...\r\nIf that is the case, maybe more things are going wrong abut data exchange between shared layers instances.\r\nIt is worth to investigate with slightly higher priority.\r\n\r\nI fear that if tasks are closed one by one, noone will notice and take care of the persisting, reoccurring issue.']",[],['pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps'],1,0
32,keras,2172,closed,Accuracy always 1 when using LSTM,"Hi, 

I am modelling a simple text classification task with 3 target labels. I am using word2vec embeddings as input to the LSTM layer. 
 is the size of each sequence.  is the word2vec dimension. 
 is a 3D tensor of the shape 
 is a simple vector of labels like [0,2,1,0,1,0,...]



But while training this model, I always get accuracy scores of 1 like this:



What could be the issue here?
PS: I am using Keras 0.3.2
",stale,"['> I am modelling a simple text classification task with 3 target labels.\n\nThe model in your has only one output...\n\n> `y_train` is a simple vector of labels like [0,2,1,0,1,0,...]\n\n`y_train` has to be 2D, i.e. `(n_samples, output_dimsize)`\n', ""Thanks @NasenSpray, that solves it. I didn't find it mentioned anywhere in the docs though, and having used sklearn for long, I just assumed `y` to be one dimensional.\n"", '@ankeshanand，I am doing Chinese text setiment classification（positive and negative）and plan to use word2vec、LSTM 。I want to know what your accuracy  can reach，more than 90%？\n', 'I wan to know how do you put your wordvec into embedding level\n', 'Can you share your entire code plz? the fixed one\n']","[""\nmodel = Sequential()\nmodel.add(LSTM(400, input_shape=(sequence_size, dimsize)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam')\n\nbatch_size = 32\nmodel.fit(X_train, y_train, batch_size=batch_size, nb_epoch=3,\n          validation_data=(X_test, y_test), show_accuracy=True)\nscore, acc = model.evaluate(X_test, y_test,\n                            batch_size=batch_size,\n                            show_accuracy=True)\n""]","['sequence_size', 'dimsize', 'X_train', '(n_samples, sequence_size, dimsize)', 'y_train', '\nEpoch 1/3\n31168/47226 [==================>...........] - ETA: 2142s - loss: -8.0915 - acc: 1.0000\n']",1,0
33,keras,3713,closed,Training slows down when using larger dataset,"Hi,

I am training a deep recurrent model on batches of a dataset at a time, by manually slicing the data and calling model.fit() on it. I noticed that when I use a larger dataset (same batch size, but more validation data), training slows down quite a bit, and monitoring CPU usage I can see that the process seems to be IO bound in some way (one core is constantly at 100%, and the other cores sporadically spike slightly, seemingly on minibatch start and end).

For reference, here is the some of the relevant part of my training code:



Just to clarify: 
- the model gets the same amount of training data in each  call, and that data is not a view but a copy of the relevant chunk of the big matrix)
- My systems RAM is far from being fully used
- I supply 2.5% of the whole dataset as validation data, so the amount of val data is higher if I use a larger dataset (note though that val testing does not seem to be what is slowing down training, as the CPU usage is poor during the epoch, not during testing)
- the larger the dataset, the more time one chunk takes to train. At 200k samples, it's around 240 seconds per  call, and at 2 million samples, it goes to around 1000 seconds.
- the time each epoch takes seems to be increasing with each call to 
- I've tried both RMSprop and SGD as optimizers, SGD seems to slow down less, but CPU usage is still bad on the large dataset

What could be slowing this down? If I drop the dataset size so that there are only around 2 chunks, CPU usage is basically perfect, and RAM does not seem to be the problem.

EDIT: the training time definitely appears to slow down with each epoch:


",stale,"['I may have worked out what it is: with more data, my vocabulary size also increases, and the model effectively trains more parameters in the Embedding layers.\n\nHowever, the issue of this causing poor CPU usage still bugs me a bit. The Embedding layer uses `K.gather` to retrieve word vectors, and as best as I can tell this becomes a bottleneck for shorter batch  sizes and large vocabularies. Is there any chance that this could be sped up or parallelized somehow?\n\nAlso, this still does not quite explain why training is slowing down from one epoch to the next.\n', 'Which backend does this happen with? Does it also happen with the other backend?\n', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs, but feel free to re-open it if needed.\n']","[' python\n# train_inputs is a list of large (in dim 0, i.e. number of samples) matrices, each matrix is the data for one model input\n\nchunk_size = 10000\nnum_points = Y_train.shape[0]\nnum_chunks = num_points // chunk_size + 1\nfor chunk_no in range(num_chunks):\n    slice_start, slice_end = chunk_no * chunk_size, min((chunk_no + 1) * chunk_size, num_points)\n\n    # use advanced indexing so we create copies\n    slice_indexes = range(slice_start, slice_end)\n\n    # part refers to each of my model input matrices, all this does is create parallel slices of input_1, input_2, ...\n    train_inputs_slice = [part[slice_indexes] for part in train_inputs]\n    Y_train_slice = Y_train[slice_indexes]\n    sample_weights_slice = sample_weights_train[slice_indexes]\n\n    self.model.fit(\n        train_inputs_slice, Y_train_slice,\n        batch_size=32, nb_epoch=1,\n        validation_data=(val_inputs, Y_val),\n        callbacks=[checkpoint],\n        sample_weight=sample_weights_slice\n    )\n', '\nTrain on 10000 samples, validate on 4978 samples\nEpoch 1/1\n 9984/10000 [============================>.] - ETA: 0s - loss: 3.1736 - acc: 0.6670Epoch 00000: val_acc improved from -inf to 0.75733, saving model to tmp_best_exp_weights_1473252153.81.hdf5\n10000/10000 [==============================] - **242s** - loss: 3.1744 - acc: 0.6666 - val_loss: 0.5002 - val_acc: 0.7573\n2016-09-07 14:47:01,922 DEBUG    main: Training on chunk 2 / 18 (iteration 1 / 500)\nTrain on 10000 samples, validate on 4978 samples\nEpoch 1/1\n 9984/10000 [============================>.] - ETA: 0s - loss: 3.0495 - acc: 0.7313Epoch 00000: val_acc improved from 0.75733 to 0.81559, saving model to tmp_best_exp_weights_1473252153.81.hdf5\n10000/10000 [==============================] - **246s** - loss: 3.0511 - acc: 0.7312 - val_loss: 0.4576 - val_acc: 0.8156\n2016-09-07 14:51:08,711 DEBUG    main: Training on chunk 3 / 18 (iteration 1 / 500)\nTrain on 10000 samples, validate on 4978 samples\nEpoch 1/1\n 9984/10000 [============================>.] - ETA: 0s - loss: 3.0236 - acc: 0.7467Epoch 00000: val_acc did not improve\n10000/10000 [==============================] - **314s** - loss: 3.0238 - acc: 0.7467 - val_loss: 0.4797 - val_acc: 0.7909\n2016-09-07 14:56:22,999 DEBUG    main: Training on chunk 4 / 18 (iteration 1 / 500)\nTrain on 10000 samples, validate on 4978 samples\nEpoch 1/1\n 9984/10000 [============================>.] - ETA: 0s - loss: 3.0511 - acc: 0.7590Epoch 00000: val_acc did not improve\n10000/10000 [==============================] - **426s** - loss: 3.0547 - acc: 0.7588 - val_loss: 0.4879 - val_acc: 0.8114\n2016-09-07 15:03:29,099 DEBUG    main: Training on chunk 5 / 18 (iteration 1 / 500)\n\n']","['fit', 'fit', 'fit']",1,0
34,keras,9350,closed,LSTM is responding slow,"I have been running LSTM keras default program using Sentiment corpus in jupyter notebook for 5 hours but not getting results. It show continue processing.
There is no gup error recovered.

The details of my system are as under:
 Linux-x86_64 operating system, 
NVIDIA Driver Version: 384.111, 
GPUs: GeForce GTX 960M (GPU 0)s 

I run example code of keras lstm first  to know the process than I shall run this code on my own corpus

datafile:
data = pd.read_csv('/home/mazhar/Downloads/Sentiment.csv')
# Keeping only the neccessary columns
data = data[['text','sentiment']]

Model:

embed_dim = 128
lstm_out = 196

model = Sequential()
model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1], dropout=0.2))
model.add(LSTM(lstm_out, dropout_U=0.2, dropout_W=0.2))
model.add(Dense(2,activation='softmax'))
model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])
print(model.summary())

Please help and guide me to resolve my problem",,['My experience is that replacing LSTM with CuDNNLSTM will speed 10+ times.'],[],[],1,0
35,keras,8856,closed,Why the training loss changed even the trainable-params is zero？,"I found that there is a shift for the  training loss, and when I tried to see why there is a shift, I set my own model to  non trainable:
![image](https://user-images.githubusercontent.com/13257051/34256650-4dd550c6-e691-11e7-8942-5bd249f5c1ed.png)
Then I  choose ten images as my training samples(they were also be used as the validation data), and set the batch-size to 5 and shuffle the data.Before I training my model, I do evaluate for my samples, the result is as below:
![image](https://user-images.githubusercontent.com/13257051/34256707-93529636-e691-11e7-8f7f-fa3b04684517.png)
Then I trained my model(actually it could not be trained)， and print the fit history, the result is as below:
![image](https://user-images.githubusercontent.com/13257051/34256735-bace98cc-e691-11e7-8b58-47f349909d8e.png)
It can be found that even the model is not be trained, and the validation loss never change, but the training loss were different among three epochs. And I found that if I set the shuffle to false, the result will be just right. I wanna know if this is a bug or the shuffle will make some difference when training. 
",,"[""Hi! \nI think this is because the training error is a running average of the error on each batch. This is noy the case with the validation loss, because it's a normal average. So the images appearing at the start of the training will have less impact on the training loss. That's why shuffling the data changes the training loss (I think). \nI hope it helps. "", '@gabrieldemarmiesse  Thank you very much.\r\nI have read the source code about how the displayed loss is computed, I found that the displayed loss is a running average of the error on each batch just like you said. That can explain why the displayed loss changed each batch, but I am still confused why the final loss will be different while the model was not be trained.Shuffling the data only change the order of computation but the average loss should be the same while the model was not be trained.\r\nAnd I found that if the batch-size is 1, then the result will be just right even shuffling the data.']",[],[],1,0
36,keras,6808,closed,Training much slower on Keras 2.0.4 than on Keras 1.2.2,"Hi everyone, 

I recently updated Keras to version 2.0.4 and I saw a big drop in performance compared to Keras 1.2.2.

I'm basically trying to find a mapping between movie titles and movie plot summaries (called synopses) using seq2seq LSTMs. While my model was pretty fast with Keras 1.2.2, it took a lots of time to train in the last version.

    model = Sequential()
    model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_LENGTH, mask_zero=True))
    model.add(LSTM(1024, return_sequences=True))
    model.add(LSTM(1024, return_sequences=True))
    model.add(TimeDistributed(Dense(vocab_size, activation='softmax')))
    model.compile(loss='sparse_categorical_crossentropy', optimizer=(Adam()), metrics=['accuracy'])

I did not change anything to this model or to the code in general between Keras 1.2.2 and Keras 2.0.4, trained on a Nvidia Quadro K6000 with Theano up-to-date in both cases and simply ran:  and  to update it.

However, the training is much slower.

Using Keras 1.2.2:


Using Keras 2.0.4: 


In the example above, the delta is only about a few seconds but when training on the whole dataset, it is much much slower.

Does anyone know what I'm doing wrong and could point me in the right direction ?",stale,"['Could you look, if your GPU is really used?\r\n\r\n```\r\nwatch nvidia-smi\r\n```', ""\r\nYes, I'm sure my GPU is used to perform the calculations: \r\n![selection_39](https://cloud.githubusercontent.com/assets/7816661/26659914/6645e910-466c-11e7-9402-2a7cddd657de.jpg)\r\n![selection_40](https://cloud.githubusercontent.com/assets/7816661/26659916/679d094c-466c-11e7-8729-9e75d08e9ca8.jpg)\r\n\r\nI was more worried about my model, actually. Maybe there's an optimization I need to apply in Keras 2.0.4 to make it better since I didn't change anything to the code (simply updated Keras' version). Unfortunately, I received no warnings or errors from Keras' APIs.\r\n\r\nBut the perf drop tells me there's definitely something going on behind the scenes."", 'Well, 0% GPU is used.', 'Maybe starting the training at the same time would have helped... Sorry for that:\r\n\r\n![selection_41](https://cloud.githubusercontent.com/assets/7816661/26674050/db480226-46b6-11e7-986c-1dfcd856265f.jpg)\r\n\r\n\r\n', 'Did you switched backends? Tensorflow is the new default since version 2.', 'No I didn,\'t. My Keras config is still the same: \r\n\r\n`{\r\n    ""image_dim_ordering"": ""th"",\r\n    ""epsilon"": 1e-07,\r\n    ""floatx"": ""float32"",\r\n    ""backend"": ""theano""\r\n}`\r\n\r\nShould I switch to Tensorflow ?', ""I don't see any reasons to switch to Tensorflow."", 'What is you image_data_format? Channel first oder last?', ""I don't know what this means, but here we are: \r\n![selection_42](https://cloud.githubusercontent.com/assets/7816661/26676875/7d96c49a-46c1-11e7-851a-6324f97bf9f9.jpg)\r\n\r\nEDIT: I switched to `channels_first` but got the same results...\r\nI'm using the latest Theano gpu backend `gpuarray`, it might be important: [Theano gpuarray](https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29)"", '@Blockost how do you run the training? Note that if you are using fit_generator the parameters have changed in Keras 2. You might be running more steps than previously.\r\n\r\n```\r\n- model.fit_generator(train_generator, train_gen.n, 1)  (old way)\r\n+ model.fit_generator(train_generator, train_gen.n/train_gen.batch_size, 1) (new way)\r\n```', ""@holli I don't use `fit_generator() `. I simply use `fit()` and pass my dataset as a whole in parameters"", ""Hi @holli , I'm using fit_generator and I couldn't figure out why keras 2 is much slower than 1.2. A pice of my code is \r\n\r\n      # fit the model on the batches generated by datagen.flow()\r\n      if int(keras.__version__.split('.')[0]) < 2:\r\n            # Keras 1.2\r\n            model.fit_generator(datagen.flow(img_train, gt_train,\r\n                                         batch_size=batch_size, shuffle=True,),\r\n                            samples_per_epoch=data_augmentation,\r\n                            nb_epoch=nb_epoch,\r\n                            validation_data=(img_test, gt_test),\r\n                            verbose=1, callbacks=callbacks)\r\n        else:\r\n            # Keras 2.0.4\r\n\r\n            # Total number of steps (batches of samples) to yield from generator\r\n            steps_per_epoch = data_augmentation / batch_size\r\n\r\n            model.fit_generator(datagen.flow(img_train, gt_train,\r\n                                         batch_size=batch_size, shuffle=True,),\r\n                            steps_per_epoch=steps_per_epoch,\r\n                            epochs=nb_epoch,\r\n                            validation_data=(img_test, gt_test),\r\n                            verbose=1, callbacks=callbacks)\r\n\r\n\r\nso I suppose that I'm using the fit_generator in the right way. Anyone has experienced something like this?"", '_data_augmentation_ is really a weird name. Is this the training sample size?', 'Yes, it is the number of training samples per epoch.', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', ""I'm experiencing the same issue since my update to the Keras 2 API (even after correctly making changes to migrate from samples_per_epoch to steps_per_epoch). @Blockost, Could you figure out why this is happening? "", ""@sir-avinash Sorry, but I don't use Keras anymore. If I remember correctly, the quick fix was to downgrade back to 1.2.2 to save some training time... Sorry for not being able to help much with that.  ""]",[],"['pip uninstall keras', 'pip install keras', 'Epoch 1/10\r\n100/100 [==============================] - 5s - loss: 2.2261 - acc: 0.8375', 'Epoch 1/10\r\n100/100 [==============================] - 18s - loss: 3.3189 - acc: 0.8277']",1,0
37,keras,10906,closed,Keras LSTM has different output for single input,"I came to a weird problem when use Keras LSTM model. I build a single layer LSTM and try to play with it. I found the the output of model is different between single input and multiple inputs, as shown in the following code.
   
    def lstmTest(training, latent_dim=10):
            _, time_dim, input_dim = training.shape

            # Define an input sequence and process it.
            encoder_inputs = Input(shape=(time_dim, input_dim), name='input')
            encoder = LSTM(latent_dim, return_state=False, name='lstm')
            encoder_outputs = encoder(encoder_inputs)

            model = Model(encoder_inputs, encoder_outputs)

            return model

    def trainingTest(model, training, nb_epoch=10, batch_size=300):
            model.compile(optimizer='adam', loss='mse', metrics=['acc'])
            history = model.fit(training, training[:, -1, :10],
                                epochs=nb_epoch,
                                batch_size=batch_size,
                                shuffle=True,
                                verbose=1,
                                ).history
        return history

    myVector = [[[i]*20]*8 for i in range(100)]
    myVector = np.array(myVector)

    lstmTest = lstmTest(myVector)
    history = trainingTest(lstmTest, myVector)

    vector = myVector[:2]
    res1 = lstmTest.predict(vector)

    vector = myVector[:1]
    res2 = lstmTest.predict(vector)

    res2[0] - res1[0]


    array([0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 5.8207661e-11,
           0.0000000e+00, 2.3283064e-10, 0.0000000e+00, 0.0000000e+00,
           0.0000000e+00, 0.0000000e+00], dtype=float32)
`
But If I change res2 to the same as res1 I got the expected result

    array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)

I use tensorflow as the backend ",,[],[],['\r\nI got the following result\r\n'],1,0
38,keras,667,closed,Regression output with LSTM input layer,"Hi,

I was wondering if there is an example for an lstm layer that eventually feeds into a regression output layer. I am having a hard time putting one together.

I am hoping to feed in one sample at a time, where sample looks like this: 

input: [[x,x,x,x],[x,x,x,x],[[x,x,x,x],] output: [y]

This way, I would expect an lstm layer that takes in 4 inputs and after 3 iterations produces an output h to feed into a regression layer.

Cheers
",,"[""I have an example similar to yours but the raw input is actually more like: \n\n`[0,0,0,45,98,43,23]`\n\nThe integers are indices which are fed into an embedding layer, and 0 is a masking index. If that is helpful, let me know and I'll post some code.\n"", ""I don't necessarily need an embedding layer. I would just to have an lstm take in a sequence of vectors as input.\n\nIf you feel it's still related I would appreciate you sharing the code.\n"", 'In that case it is probably not sufficiently related, sorry.\n', '`input: [[x,x,x,x],[x,x,x,x],[[x,x,x,x],] output: [y]`\n\nHow is this not the canonical use case of LSTM in Keras? What problem are you having?\n', ""I got it running. I had an odd issue with theano complaining about too many parameters. I thought it might be related to my code, it looks like it wasn't.\n\nI will leave my implementation here in case others are looking for the same thing.\n\ndef gen_data() :\n    X, Y = [], []\n    for x in xrange(1000) :\n        X.append(np.random.random_sample(5,))\n        Y.append(np.random.random_sample())\n    return np.asarray(X), np.asarray(Y)\ndef temp_data() :\n    X, Y = [], []\n    for x in xrange(1000) :\n        sample = []\n        for x in xrange(4) :\n            sample.append(np.random.random_sample(5,))\n        X.append(sample)\n        Y.append(np.random.random_sample())\n    return np.asarray(X), np.asarray(Y)\nif **name** == '**main**':\n    X,Y =  temp_data()\n    model = Sequential()\n    model.add(LSTM(5, 10, activation='sigmoid', inner_activation='hard_sigmoid'))\n    model.add(Dropout(0.5))\n    model.add(Dense(10,1, init='uniform', activation='linear'))\n    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n    model.compile(loss='mean_squared_error', optimizer=sgd)\n\n```\nmodel.fit(X,Y, nb_epoch=20, batch_size=10)\nprint model.evaluate(X,Y, batch_size=10)\n```\n""]",[],[],1,0
39,keras,6300,closed,Regression in initial Dense model prediction between Keras v1.2.2 and v2.0,"There is a regression in predicted values for a simple Dense model between Keras v1.2.2 and v2.0. Code to reproduce is below.

Using Keras v1.2.2 commit 4fa7e5d
Using Keras April 17 v2.0 commit 73bf06fb023a8b37ddf2e2a168bbf920c7a6c766
Using Theano April 17 TOT commit 388805f946685e86225cdf602eb8a4f0059f9667
Running on Theano.Cuda with GeForce GT 750M, CuDNN v5105

",,"['I believe the difference is 1.22 is using numpy random numbers and 2.0 is using Theano random numbers to initialize the output weights. So no surprise they would be different.', ""Wouldn't the initializer of 'one' eliminate random numbers from being used?\r\n\r\nI also see this regression with tensorflow backend as well."", ""Your output layer has weights too! And those aren't initialized at 1"", 'You can specify the initializer just as was done in previous layer', 'This issue is resolved. \r\n\r\n```\r\n$python test.py \r\nUsing TensorFlow backend.\r\nUsing Keras version: 2\r\n1.0\r\n$python test.py \r\nUsing TensorFlow backend.\r\nUsing Keras version: 1\r\n1.0\r\n```\r\n\r\n```\r\n""""""\r\nTesting for Regression in initial Dense model prediction between Keras v1.2.2 and v2.0.\r\nImports are included here to use as code snippet for Github issue.\r\n""""""\r\nimport numpy as np\r\nimport random\r\nnp.random.seed(7)\r\nrandom.seed(7)\r\n\r\nimport keras\r\nimport keras.layers as k_layers\r\nfrom keras.layers import Dense, Input\r\nfrom keras.models import Model\r\n\r\nx_train = np.array([[1]])\r\nx_input = Input(shape=(1,))\r\ninitializer = \'one\'\r\n\r\nprint ""Using Keras version: %s"" % keras.__version__[0]\r\n\r\nif keras.__version__[0] == \'2\':\r\n    x = Dense(1, kernel_initializer=initializer)(x_input)\r\n    output = Dense(1, kernel_initializer=initializer, activation=\'linear\', name=\'output_layer\')(x)\r\n    test_model = Model(inputs=x_input, outputs=output)\r\nelse:\r\n    x = Dense(1, init=initializer)(x_input)\r\n    output = Dense(1, kernel_initializer=initializer, activation=\'linear\', name=\'output_layer\')(x)\r\n    test_model = Model(input=x_input, output=output)\r\n\r\ntest_model.compile(\r\n    optimizer=""adam"",\r\n    loss=""mse"",\r\n    metrics=[""accuracy""])\r\n\r\npredict_val = test_model.predict(x_train)\r\n\r\n# Keras_v1.2.2, this value should be 1.0\r\n# Keras_v2.0, this value should be 1.0\r\nprint predict_val.sum().sum()\r\n#self.assertAlmostEquals(predict_val.sum().sum(), 1.0)\r\n```', ""I'm seeing a similar issue when upgrading to Keras v2.  Is it actually true that Theano uses its own source of entropy for computing random values?  \r\n\r\nThe default `glorot_uniform` initializer appears to still rely on numpy, but is clearly producing different results. ""]","['\r\n""""""\r\nTesting for Regression in initial Dense model prediction between Keras v1.2.2 and v2.0.\r\nImports are included here to use as code snippet for Github issue.\r\n""""""\r\nimport numpy as np\r\nimport random\r\nnp.random.seed(7)\r\nrandom.seed(7)\r\n\r\nimport keras\r\nimport keras.layers as k_layers\r\nfrom keras.layers import Dense, Input\r\nfrom keras.models import Model\r\n\r\nx_train = np.array([[1]])\r\nx_input = Input(shape=(1,))\r\ninitializer = \'one\'\r\n\r\nprint ""Using Keras version: %s"" % keras.__version__[0]\r\n\r\nif keras.__version__[0] == \'2\':\r\n    x = Dense(1, kernel_initializer=initializer)(x_input)\r\n    output = Dense(1, activation=\'linear\', name=\'output_layer\')(x)\r\n    test_model = Model(inputs=x_input, outputs=output)\r\nelse:\r\n    x = Dense(1, init=initializer)(x_input)\r\n    output = Dense(1, activation=\'linear\', name=\'output_layer\')(x)\r\n    test_model = Model(input=x_input, output=output)\r\n\r\ntest_model.compile(\r\n    optimizer=""adam"",\r\n    loss=""mse"",\r\n    metrics=[""accuracy""])\r\n\r\npredict_val = test_model.predict(x_train)\r\n\r\n# Keras_v1.2.2, this value should be -1.4677111\r\n# Keras_v2.0, this value should be 1.7169169\r\nprint predict_val.sum().sum()\r\n#self.assertAlmostEquals(predict_val.sum().sum(), -1.4677111)\r\n']",[],1,0
40,keras,2718,closed,Noisy Validation Accuracy,"Does anyone have any ideas as to why the validation accuracy curve would be this noisy?

![screen shot 2016-05-13 at 10 22 04 am](https://cloud.githubusercontent.com/assets/6086781/15250847/92ada084-18f4-11e6-9527-b0871e776065.png)

For completeness, I'll mention that the ""validation set"" is systematically different than the training set (this is a transfer learning situation).  I can provide more details as needed about the specific application and model, but was interested if there was some kind of generic explanation for the noisy accuracy curve.
",,"['Small validation set?\n\nAlso, accuracy is a funny metric in that it changes discontinuously as a function of the underlying predicted probabilities. Is cross-entropy loss this noisy too?\n', ""@bkj Would you mind telling how did you get the accuracy vector to get this curve plotted? I guess you've referred to an attribute of the model.fit() history object.\nIdeally, I want to run a couple epochs and get the average of the validation accuracy for those epochs.\n\nAnd for your question, as @dansbecker said, the problem might have a thing to do with the inadequacy  of validation set size. \nIf you're using stochastic gradient descent, a small batch size might cause noise with regards to the convergence of the cost function, subsequently this will affect the accuracy as well. Here's a video by Andrew Ng explaining the same https://www.youtube.com/watch?v=y-xIqvhf8T4\n"", ""I believe (IIRC) that it was due to not shuffling the data -- I was using\nfit_generator, which doesn't always lend itself to shuffling, so the\npeaks/valleys of error rate correspond to the same sets of records in the\ndata. Possibly the way my data was sorted was also nonrandom, but I didn't\nlook into that very deeply.\n\nOn Thursday, June 16, 2016, akilat90 notifications@github.com wrote:\n\n> @bkj https://github.com/bkj Would you mind telling how did you get the\n> accuracy vector to get this curve plotted? I guess you've referred to an\n> attribute of the model.fit() history object.\n> Ideally, I want to run a couple epochs and get the average of the\n> validation accuracy for those epochs.\n> \n> And for your question, as @dansbecker https://github.com/dansbecker\n> said, the problem might have a thing to do with the inadequacy of\n> validation set size.\n> If you're using stochastic gradient descent, a small batch size might\n> cause noise with regards to the convergence of the cost function,\n> subsequently this will affect the accuracy as well. Here's a video by\n> Andrew Ng explaining the same https://www.youtube.com/watch?v=y-xIqvhf8T4\n> \n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/fchollet/keras/issues/2718#issuecomment-226396601,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AFzgfRPGzrHglEWZeX4LgF5dJ2vOjiuJks5qMOkDgaJpZM4IeD3W\n> .\n""]",[],[],1,0
41,keras,4256,closed,cuDNN 5 and Theano 0.9.0 not improving LSTM performance,"Theano [recently introduced](https://github.com/Theano/Theano/pull/4915) cuDNN5 support for recurrent networks and I decided to give it a spin with a Keras LSTM network. However, I noticed only a marginal benefit that seems more likely noise than anything else. Any reason I shouldn't be seeing bigger performance improvements?

cuDNN 4, theano 0.8.2, keras 1.1.0
Train on 3000000 samples, validate on 857222 samples
Epoch 1/100
3000000/3000000 [==============================] - 781s - loss: 2.2539 - acc: 0.6299 - val_loss: 1.6206 - val_acc: 0.7274

cuDNN 5, theano 0.9.0, keras 1.1.0
Train on 3000000 samples, validate on 857222 samples
Epoch 1/100
3000000/3000000 [==============================] - 765s - loss: 2.2588 - acc: 0.6289 - val_loss: 1.6214 - val_acc: 0.7274

cuDNN 5, theano 0.9.0, keras 1.1.1
Train on 3000000 samples, validate on 857222 samples
Epoch 1/100
3000000/3000000 [==============================] - 761s - loss: 2.2578 - acc: 0.6292 - val_loss: 1.6234 - val_acc: 0.7271
",stale,"['I think this will need new code in keras to get the speed up. There was\nanother issue related to that.\n\nLe 1 nov. 2016 16:20, ""mthmn20"" notifications@github.com a écrit :\n\n> Theano recently introduced https://github.com/Theano/Theano/pull/4915\n> cuDNN5 support for recurrent networks and I decided to give it a spin with\n> a Keras LSTM network. However, I noticed only a marginal benefit that seems\n> more likely noise than anything else. Any reason I shouldn\'t be seeing\n> bigger performance improvements?\n> \n> cuDNN 4, theano 0.8.2, keras 1.1.0\n> Train on 3000000 samples, validate on 857222 samples\n> Epoch 1/100\n> 3000000/3000000 [==============================] - 781s - loss: 2.2539 -\n> acc: 0.6299 - val_loss: 1.6206 - val_acc: 0.7274\n> \n> cuDNN 5, theano 0.9.0, keras 1.1.0\n> Train on 3000000 samples, validate on 857222 samples\n> Epoch 1/100\n> 3000000/3000000 [==============================] - 765s - loss: 2.2588 -\n> acc: 0.6289 - val_loss: 1.6214 - val_acc: 0.7274\n> \n> cuDNN 5, theano 0.9.0, keras 1.1.1\n> Train on 3000000 samples, validate on 857222 samples\n> Epoch 1/100\n> 3000000/3000000 [==============================] - 761s - loss: 2.2578 -\n> acc: 0.6292 - val_loss: 1.6234 - val_acc: 0.7271\n> \n> —\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/fchollet/keras/issues/4256, or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AALC-3KFkUGXHHY3AZLmb_dGvxBYFUd0ks5q558qgaJpZM4KmiDm\n> .\n', 'Has there been any progress on this or a plan for new keras code :) ?', ""Not that I know, but I don't follow closely enough keras to be sure.\nHopefully someone else will reply. Otherwise grep in less for gpuarray.dnn\nand check if it seem related.\n\nLe lun. 6 mars 2017 17:48, pyzow <notifications@github.com> a écrit :\n\n> Has there been any progress on this or a plan for new keras code :) ?\n>\n> —\n> You are receiving this because you commented.\n>\n>\n> Reply to this email directly, view it on GitHub\n> <https://github.com/fchollet/keras/issues/4256#issuecomment-284560054>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AALC-4vIr8NKPlzl5726_uetJeWR-xyUks5rjI1WgaJpZM4KmiDm>\n> .\n>\n"", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],1,0
42,keras,5285,closed,Customize loss function in a way that ignore the samples with large loss,"I have a lot of sample images in three classes, but some of the labels of them are wrong, so the trained classifier can not achieve good performance, and correct the wrong labels needs to consume a lot of time.
To eliminate the effect of error labels, I intend to redefine the loss function like that: When calculating the losses of a batch of data, the largest loss is removed from the others at this batch. And then calculate the mean of the remaining elements as the loss value of this batch.
What should I do to achieve this goal, or is there any other better way to achieve the same purpose?",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs, but feel free to re-open it if needed.\n']",[],[],1,0
43,keras,6825,closed,loss function doesn't ignore large masked values,"

If you comment out the  lines, or change  to a smaller number then it works fine (MSE loss is 1.0).",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']","[""python\r\nimport numpy as np\r\nfrom keras.models import Model\r\nfrom keras.layers import Input, Masking\r\n\r\nmaskval = 1e20\r\nl1 = Input(shape=(10, 4))\r\nl2 = Masking(mask_value=maskval)(l1)\r\n\r\nm = Model(inputs=l1, outputs=l2)\r\nm.compile(optimizer='sgd', loss='mse')\r\n\r\nx = np.random.rand(6, 10, 4)\r\ny = np.copy(x)\r\nx[1, 3:, :] = maskval\r\nx[3, 5:, :] = maskval\r\nx[5, 7:, :] = maskval\r\n\r\ny += 1\r\ny[1, 3:, :] = maskval\r\ny[3, 5:, :] = maskval\r\ny[5, 7:, :] = maskval\r\n\r\nm.test_on_batch(x, y) # returns nan\r\n""]","['y[...] = maskval', 'maskval']",1,0
44,keras,2730,closed,Improving the speed of fit_generator,"I have found fit_generator to be quite slow.
This makes the disk read speed a major bottleneck when the network is relatively shallow.

Using the multiprocessing module instead of the threading one, I was able to get significant improvements in speed (half the time) as shown in this [gist example](https://gist.github.com/tdeboissiere/195dde7fddfcf622a82a895b90d2c800).

I could look into making a pull request with this new implementation if you think the speed gain is worth it.
",,"['Sounds awesome.\n', 'Sounds interesting. How safe is it?\n\nA PR would be welcome.\n', ""I'll write a PR asap then.\n\nI have not made any safety test but have never encountered any issue with this snippet.\n\nA few handwaving arguments for safety (over threading):\n- multiprocessing launches several independent processes. When one process crashes, it should not affect the other processes (whereas a crashing thread can)\n- multiprocessing effectively bypasses GIL limitations which supposedly is a plus for stability\n\nOne catch I had with my approach was the need to reset the random seed for each process. That's because processes forked from the same parent process share the parent's seed. That may make reproducibility a bit harder...\n"", '@fchollet :\n\nI have written a new generator_queue function to implement the **multiprocessing approach**.\n\nFrom unit tests, the memory usage increases quite significantly when you add more processes (with 4 processes, memory usage = x2 wrt **threading approach**)\n\nI can write the PR with multiprocessing as an option so that the user chooses what best fits his system. Let me know what you think.\n', 'I replaced the import part in training.py and now it works fine:\n\n```\nimport multiprocessing as threading\nimport multiprocessing as queue\n```\n\nalso replace `threading.thread` with `threading.Process`. \n\nOtherwise the original generator thread is very stupid and it can be witnessed that there are no samples in the queue at some period due to the existence of GIL.\n', 'I have also implemented my own caching generator using multiprocessing in order to address this bottleneck. Any idea on when this can be incorporated on head?\n', '> I can write the PR with multiprocessing as an option so that the user chooses what best fits his system. Let me know what you think.\n\nYou can just submit a PR using multiprocessing, with no further options.\n', ""@fchollet : OK, will PR asap\n\n@bobchennan : Did you verify you obtained a speedup ? I'd be surprised if you got any without actually specifying to use multiple processes somewhere in the code.\n"", ""@tdeboissiere : Regarding reproducible random numbers in threads: I didn't look at your code to see how you're re-seeding, nor to see if it's been resolved, but why can't it be re-seeded in a consistent way?   The simplest form [not suitable for security/cryptography applications] might be something like: seed(random() + incremental_thread_id).  Due to random being shared among threads, each thread would have to also have the random state saved (random.getstate() or numpy.random.get_state()) or, probably preferable, its own instance of the random class (http://stackoverflow.com/questions/5836335/consistenly-create-same-random-numpy-array/5837352#5837352).   If keeping various states (the most irritating way), one would probably best keep a pool of random numbers to minimize the overhead of state changing, but I think this is unnecessary with the separate class instances."", 'Keras 1.2.2 does have this multiprocessing included, but my fit_generator() is still about 4x slower than fit().  Can this be further sped up?', 'I installed Keras 2.0.2 and the same code with fit_generator is slower than using Keras version 1.1. I think the changes were not incorporated to increase the speed.', 'Same question here!!!\r\nAfter updated to keras 2.0.2, fit_generator seems slower. Any update about this issue?', 'Same here :( !!', 'Can confirm it is 10x to 100x slower. I downgraded to 1.2.2', 'for people complaining about keras 2 being way slower. make sure that you made the following change when fitting your model: \r\n\r\n```\r\n- model.fit_generator(train_generator, train_gen.n, 1)  (old way)\r\n+ model.fit_generator(train_generator, train_gen.n/train_gen.batch_size, 1) (new way)\r\n```\r\n\r\nsimilar issue: https://github.com/fchollet/keras/issues/6406#issuecomment-308248241']",[],[],1,1
45,keras,3755,closed,Validation loss increases while validation accuracy is still improving,"I am training a deep CNN (4 layers) on my data. I used ""categorical_crossentropy"" as the loss function.
During training, the training loss keeps decreasing and training accuracy keeps increasing until convergence.  But the validation loss started increasing while the validation accuracy is still improving. 
The curves of loss and accuracy are shown in the following figures:
![acc_cnn32p-32p-32p-32p_adam1e-4_5000](https://cloud.githubusercontent.com/assets/8138843/18455543/3b15fc32-7910-11e6-93a5-72374837a78d.png)
![loss_cnn32p-32p-32p-32p_adam1e-4_5000](https://cloud.githubusercontent.com/assets/8138843/18455542/3b14ebee-7910-11e6-873b-ca2a4dc3dfe7.png)

It also seems that the validation loss will keep going up if I train the model for more epochs. Does anyone have idea what's going on here? 

Thanks a lot!
",stale,"['This indicates that the model is overfitting.  It continues to get better and better at fitting the data that it sees (training data) while getting worse and worse at fitting the data that it does not see (validation data).\n', ""@jerheff Thanks for your reply. We can say that it's overfitting the training data since the training loss keeps decreasing while validation loss started to increase after some epochs. \nHowever, both the training and validation accuracy kept improving all the time. How can we explain this?\n"", ""The graph test accuracy looks to be flat after the first 500 iterations or so.  It can remain flat while the loss gets worse as long as the scores don't cross the threshold where the predicted class changes.\n"", '@jerheff Thanks so much and that makes sense! \nOne more question: What kind of regularization method should I try under this situation?\n', '@fish128 Did you find a way to solve your problem (regularization or other loss function)? Thanks in advance.', 'I am training a deep CNN (using vgg19 architectures on Keras) on my data. I used ""categorical_cross entropy"" as the loss function. During training, the training loss keeps decreasing and training accuracy keeps increasing slowly. But the validation loss started increasing while the validation accuracy is not improved.\r\nThe curve of loss are shown in the following figure:\r\n![epoch loss_relation](https://cloud.githubusercontent.com/assets/24401730/21601812/2a7f5da2-d1c9-11e6-9a42-e8c7e76db14f.png)\r\nIt also seems that the validation loss will keep going up if I train the model for more epochs. Does anyone have idea what\'s going on here?\r\nthanks!\r\n', 'Just as jerheff mentioned above it is because the model is overfitting on the training data, thus becoming extremely good at classifying the training data but generalizing poorly and causing the classification of the validation data to become worse. You could solve this by stopping when the validation error starts increasing or maybe inducing noise in the training data to prevent the model from overfitting when training for a longer time.', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', 'Even I am also experiencing the same thing. Validation loss is increasing, and validation accuracy is also increased and after some time ( after 10 epochs ) accuracy starts dropping. ', 'The question is still unanswered. Validation loss increases but validation accuracy also increases. ', '> The question is still unanswered. Validation loss increases but validation accuracy also increases.\r\n\r\nDoes this indicate that you overfit a class or your data is biased, so you get high accuracy on the majority class while the loss still increases as you are going away from the minority classes?', 'I would like to have a follow-up question on this, what does it mean if the validation loss is fluctuating ? and not monotonically increasing or decreasing ?\r\n', 'I think your model was predicting more accurately and less certainly about the predictions', '> I think your model was predicting more accurately and less certainly about the predictions\r\n\r\nWhat does this even mean? I have the same situation where val loss and val accuracy are both increasing.\r\n\r\nAnd when I tested it with test data (not train, not val), the accuracy is still legit and it even has lower loss than the validation data!\r\n\r\nVery confusing......', 'this question is still unanswered i am facing same problem while using ResNet  model on my own data.  \r\nLearning rate:  0.0001\r\n73/73 [==============================] - 9s 129ms/step - loss: 0.1621 - acc: 0.9961 - val_loss: 1.0128 - val_acc: 0.8093\r\n\r\nEpoch 00100: val_acc did not improve from 0.80934\r\n\r\nhow can i improve this i have no idea (validation loss is 1.01128 👎 )', 'Who has solved this problem? any one can give some point?', 'Check your model loss is implementated correctly. It kind of helped me to\nspot a bug.\n\nOn Fri, Sep 27, 2019, 5:12 PM sanersbug <notifications@github.com> wrote:\n\n> Who has solved this problem? any one can give some point?\n>\n> —\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/keras-team/keras/issues/3755?email_source=notifications&email_token=ACRE6KF7OYJRTHAHJYQZKUTQLXWSZA5CNFSM4CPMOKN2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD7YUCJY#issuecomment-535904551>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ACRE6KA7RIP7QGFGXW4XXRTQLXWSZANCNFSM4CPMOKNQ>\n> .\n>\n', '> I would like to have a follow-up question on this, what does it mean if the validation loss is fluctuating ? and not monotonically increasing or decreasing ?\r\n\r\nI experienced the same issue but what I found out is because the validation dataset is much smaller than the training dataset. This causes the validation fluctuate over epochs.', '我也遇到这个问题\r\n', 'Hello I also encountered a similar problem. My training loss and verification loss are relatively stable, but the gap between the two is about 10 times, and the verification loss fluctuates a little, how to solve', '![Figure 2020-08-11 214728](https://user-images.githubusercontent.com/60482083/89922205-59ae1f80-dc1c-11ea-86e5-bb3baabe9af2.png)\r\n![Figure 2020-08-11 214654](https://user-images.githubusercontent.com/60482083/89922213-5adf4c80-dc1c-11ea-99ca-f6ed4e52026e.png)\r\n\r\nI have the same problem my training accuracy improves and training loss decreases but my validation accuracy gets flattened and my validation loss decreases to some point and increases at the initial stage of learning say 100 epochs (training for 1000 epochs),\r\nEven though I added L2 regularisation and also introduced a couple of Dropouts in my model I still get the same result.\r\nI am working on a time series data so data augmentation is still a challege for me. I used 80:20% train:test split. Can anyone suggest some tips to overcome this? Thanks in advance ', 'This might be helpful: https://discuss.pytorch.org/t/loss-increasing-instead-of-decreasing/18480/4', 'The model is overfitting the training data.\r\nTo solve this problem you can try \r\n1.Regularization\r\n2.Try to add more add to the dataset or try data augumentation', ""I'm experiencing similar problem. I tried regularization and data augumentation.\r\ncould you give me advice?""]",[],[],1,0
46,keras,12977,closed,training performance are different with exact same data and architecture. The only difference is using .Sequential() or .Model(),"the model below is from  [website][1] and it behaves exactly as expected. It is defined with . I want to convert it to be defined with  to make it more flexible for my future use. But after my conversion, the performance plummeted. 

The original model you can find on  website:


The following code is my conversion:

The only difference I can see is  doesn't count  while  counts it, but I don't believe they make the model structure different. However, the performance of  is:

[![enter image description here][2]][2]


While the performance of the  I converted is:

[![enter image description here][3]][3]


  [1]: https://www.tensorflow.org/tutorials/keras/basic_regression
  [2]: https://i.stack.imgur.com/PdtrA.png
  [3]: https://i.stack.imgur.com/vfvxp.png

### Can anyone tell me what I did wrong?
### Some other context:
I have read this [post](https://github.com/keras-team/keras/issues/8001), but my code are all run on CPU in Google Colab





Code to plot the losses:

Code to train the model(it's exact same for both models):


Any suggestion is appreciated!",,"[""it's fixed, there is no activation function in the original model. ""]","[""\r\ndef build_model():\r\n  model = Sequential([\r\n    layers.Dense(64, activation=tf.nn.relu, input_shape=[len(train_dataset.keys())]),\r\n    layers.Dense(64, activation=tf.nn.relu),\r\n    layers.Dense(1)\r\n  ])\r\n\r\n  optimizer = keras.optimizers.Adam()\r\n  model.compile(loss='mean_squared_error',\r\n                optimizer=optimizer,\r\n                metrics=['mean_absolute_error', 'mean_squared_error'])\r\n  return model\r\n\r\nmodel = build_model()\r\nmodel.summary()\r\n\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ndense_22 (Dense)             (None, 64)                640       \r\n_________________________________________________________________\r\ndense_23 (Dense)             (None, 64)                4160      \r\n_________________________________________________________________\r\ndense_24 (Dense)             (None, 1)                 65        \r\n=================================================================\r\nTotal params: 4,865\r\nTrainable params: 4,865\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n"", '\r\ndef build_model_base():\r\n  input = Input(shape=[len(train_dataset.keys())])\r\n  x = Dense(64, activation=\'relu\', name=""dense1"")(input)   \r\n  x = Dense(64, activation=\'relu\', name=""dense2"")(x)\r\n  output = Dense(1, activation=\'sigmoid\', name=\'output\')(x)\r\n  model = Model(input=[input], output=[output])\r\n  optimizer = keras.optimizers.Adam()\r\n\r\n  model.compile(loss=\'mean_squared_error\', \r\n                optimizer=optimizer,\r\n                metrics=[\'mean_absolute_error\', \'mean_squared_error\'])\r\n  return model\r\n\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_18 (InputLayer)        (None, 9)                 0         \r\n_________________________________________________________________\r\ndense1 (Dense)               (None, 64)                640       \r\n_________________________________________________________________\r\ndense2 (Dense)               (None, 64)                4160      \r\n_________________________________________________________________\r\noutput (Dense)               (None, 1)                 65        \r\n=================================================================\r\nTotal params: 4,865\r\nTrainable params: 4,865\r\nNon-trainable params: 0\r\n', ""\r\ndef plot_history(history):\r\n  hist = pd.DataFrame(history.history)\r\n  hist['epoch'] = history.epoch\r\n  \r\n  plt.figure()\r\n  plt.xlabel('Epoch')\r\n  plt.ylabel('Mean Abs Error [MPG]')\r\n  plt.plot(hist['epoch'], hist['mean_absolute_error'],\r\n           label='Train Error')\r\n  plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\r\n           label = 'Val Error')\r\n  y_max = max(hist['val_mean_absolute_error'])\r\n  plt.ylim([0,y_max])\r\n  plt.legend()\r\n  \r\n  plt.figure()\r\n  plt.xlabel('Epoch')\r\n  plt.ylabel('Mean Square Error [$MPG^2$]')\r\n  plt.plot(hist['epoch'], hist['mean_squared_error'],\r\n           label='Train Error')\r\n  plt.plot(hist['epoch'], hist['val_mean_squared_error'],\r\n           label = 'Val Error')\r\n  y_max = max(hist['val_mean_squared_error'])\r\n  plt.ylim([0,y_max])\r\n  plt.legend()\r\n  plt.show() \r\n"", '\r\nhis_seq = model.fit(normed_train_data.values, train_labels.values,\r\n          batch_size=128,\r\n          validation_split = 0.1,\r\n          epochs = 100,\r\n          verbose=0)\r\nplot_history(his_seq)\r\n']","['Keras', 'keras.models.Sequential()', 'keras.models.Model()', 'Keras', '.Sequential', 'input layer', '.Model', '.Sequential', '.Model()', 'print(keras.__version__) # 2.0.4', 'print(tf.__version__) #1.14.0-rc1']",1,0
47,keras,3766,closed,Fluctuating validation accuracy in LSTM Network and Different ways of sentence representation,"I run the example code for LSTM networks that uses imdb dataset in Keras. One can find the code in the following link. [https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py](url)

My problem is that as code progresses the training loss decreases and training accuracy increases as expected but validation accuracy fluctuates in an interval and validation loss increases to a high value. I attach a part of the log of the training phase below. Even I observe that when training loss is very small (~ 0.01-0.03) sometimes it increases in the next epoch and then it decreases again. What I mention can be seen in epochs 75-77. But in general it decreases. Is this behaviour expected at higher number of epochs? (I mean the fluctuations in the train_loss and train_acc.)

What I expect is that training accuracy always increases up to 0.99-1 and training loss always decreases. Moreover, the validation accuracy should start from maybe 0.4 and raise to for example 0.8 in the end. If validation accuracy does not improve over epochs what is the point of waiting during epochs? Also the test accuracy is close 0.81 at the end. I checked overfitting. But I could not come up with an explanation for fluctuations in val_acc with overfitting. What may be causing these fluctuations?

I also tried with my own data and came up with same situation. I processed my data in a similar way. I mean my training, validation and test points are processed in same logic as the ones in this example code.

Besides, I think the code takes the last output from LSTM for all time steps in a sample before feeding it to a dense layer.  What if I want to take the mean or max of all outputs for all time steps in LSTM layer before dense layer? How will I apply these operations in Keras? As far as I know return_sequence=True must be used. But how to combine outputs for all time steps in a sample to obtain a single representation of a sentence before dense layer?

Any help would be appreciated.

Using Theano backend.
Loading data...
25000 train sequences
25000 test sequences
Pad sequences (samples x time)
X_train shape: (25000, 80)
X_test shape: (25000, 80)
Build model...
Train...
Train on 22500 samples, validate on 2500 samples
Epoch 1/100
22500/22500 [==============================] - 236s - loss: 0.5438 - acc: 0.7209 - val_loss: 0.4305 - val_acc: 0.8076
Epoch 2/100
22500/22500 [==============================] - 237s - loss: 0.3843 - acc: 0.8346 - val_loss: 0.3791 - val_acc: 0.8332
Epoch 3/100
22500/22500 [==============================] - 245s - loss: 0.3099 - acc: 0.8716 - val_loss: 0.3736 - val_acc: 0.8440
Epoch 4/100
22500/22500 [==============================] - 243s - loss: 0.2458 - acc: 0.9023 - val_loss: 0.4206 - val_acc: 0.8372
Epoch 5/100
22500/22500 [==============================] - 239s - loss: 0.2120 - acc: 0.9138 - val_loss: 0.3844 - val_acc: 0.8384
....
....
Epoch 75/100
22500/22500 [==============================] - 238s - loss: 0.0134 - acc: 0.9868 - val_loss: 0.9045 - val_acc: 0.8132
Epoch 76/100
22500/22500 [==============================] - 241s - loss: 0.0156 - acc: 0.9845 - val_loss: 0.9078 - val_acc: 0.8211
Epoch 77/100
22500/22500 [==============================] - 235s - loss: 0.0129 - acc: 0.9883 - val_loss: 0.9105 - val_acc: 0.8234
",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs, but feel free to re-open it if needed.\n']",[],[],1,0
48,keras,10943,closed,Inconsistent run when using pre-trained h5 weight file on Theano vs TensorFlow/CNTK,"Hi,
I have a pre-trained model that I try to use. It has an h5 weight file that I loaded into Keras.
But when I run it using Theano, the output is not the same as TensorFlow/CNTK (TensorFlow and CNTK agree with each other)

On further investigation, I found out that the attribute original_backend in this file is set to ""plaidml"" which from what I understand is one of the computational engines for TensorFlow. But in the function ""_need_convert_kernel"" in engine/saving.py, it does not convert if the original_backend is not in the lookup table. Which mean when I sue Theano we need to convert the weight but Keras does not which lead to inconsistent output. I have top1 accuracy for TensorFlow and CNTK around 91% but only 44% for Theano.",,[],[],[],1,0
49,keras,6339,closed,Large Differences in Performance between Keras 2.0.3 and Keras 1.1.0 – Potential Bug?,"I’ve upgraded to the new verison of Keras and I’m noticing a huge a drop in the performance: from 200 seconds per epoch on the old Keras to 40,000 seconds per epoch on the new Keras.

Attached are two shots showing training on the old Keras and on the new. The models have same architecture are confirmed to both be utilizing the same GPU during training. They are both coded with the functional API. The problem does not change when using different tensorflow backends (0.12 and 1.0). 

After doing these side-by-side tests, I’m thinking this may be a bug with Keras.

Training Output of Keras 1.1.0 (270 sec/epoch):
![kerasold](https://cloud.githubusercontent.com/assets/25262161/25255979/b878359e-25fb-11e7-857a-2c37d7be4561.png)

Training Output of Keras 2.0.3 (38000 sec/epoch):

![kerasnew](https://cloud.githubusercontent.com/assets/25262161/25256006/d7bdf786-25fb-11e7-8df3-93f0bdbf1ea3.png)

",,"[""No one can help without seeing any code. Lots of possible explanations. For example, the code for calculating the output shape moved. If you're using some old output shape calculation then something could be off by an order of magnitude. There is nothing in Keras-2 that would slow things down like that. You have to be doing something in your code that is causing the issue.\r\n\r\nCheers\r\n\r\n"", 'Hi Ben, \r\n\r\nThanks for your response. Attached is a copy of my code, written in Keras 1.1.0 and Keras 2.0.3. From what I can tell, they are the same models. Keras 2.0.3 runs much slower.\r\n\r\n[keras1.1.0.txt](https://github.com/fchollet/keras/files/946120/keras1.1.0.txt)\r\n\r\n[keras2.0.3.txt](https://github.com/fchollet/keras/files/946121/keras2.0.3.txt)\r\n\r\n\r\n\r\n\r\n\r\n\r\n', ""Keras 1 `fit_generator` goes by total number of samples, which should be `bs*1000`. Keras 2 `fit_generator` goes by number of batches, which should be `1000`. That would explain some of it. You're off by a factor of 140 and that would explain 32 of it.\r\n\r\nI was thinking the stride changes might've been it but they all look correct. Need to read the code closely. The model.summary looks exactly the same for both? What is the output?\r\n\r\nIf there is any difference between keras1 and keras2, it could be from the BatchNorm changes. Do you see no difference if you just comment out all the batchnorm layers from both files?\r\n\r\nCheers\r\n\r\n"", ""Hi Ben, thanks for finding the error in my generator. My batch size is actually 128, not 32, so I'm getting comparable run times to the Keras 1.1.0 version now that it's fixed. \r\n\r\nModel summaries are exactly the same for both, and below is a screen shot of the Keras 2.0.3 run with the corrections you suggested.\r\n\r\nThanks so much for your help - I'm happy that it was a stupid mistake on my part and not a bug. \r\n\r\n![image](https://cloud.githubusercontent.com/assets/25262161/25309442/5cd2f0fc-279b-11e7-8038-a5202b2f90f1.png)\r\n""]",[],[],1,0
50,keras,10947,closed,model.predict() runs faster when test examples are passed in as batch vs one by one.,"Hi there,

Running on my Macbook Pro (no GPU).

I have a simple CNN very similar to [this](https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py) example. What I'm finding is that doing:



Is faster than the total time when running each example individually. 



Any reason for this behavior? Is it always faster to run in batches like this? 

 ",,"['You should ask this on Stack Overflow', 'inference on GPU is always better when done in batch mode ', 'Doing batches instead of single samples is expected to be faster on a CPU as well, for at least two reasons:\r\n* Less overhead in calling compiled functions (once vs 5 times)\r\n* Takes advantage of all your CPU cores, potentially even using vectorized (SIMD) code.']","['\r\nstart_time = timeit.default_timer()\r\nmodel.predict(five_example_images_stacked_toegther, batch_size=5) \r\ntot_time = timeit.default_timer() - start_time\r\n', '\r\ntotal_time = 0\r\nfor i in range(0, 5):\r\n    start_time = timeit.default_timer()\r\n    model.predict(example_images[i])\r\n    time_for_single_example = timeit.default_timer() - start_time\r\n    total_time += time_for_single_example\r\n']",[],1,0
51,keras,707,closed,Wrong accuracy with masking,"As far as I understand, it seems that accuracy doesn't take into account possible timesteps masking (through ).

In  model, categorical accuracy is computed as:



while a more correct approach will take into account :



However, I see in [theano documentation](http://deeplearning.net/software/theano/library/tensor/basic.html#indexing) that it won't work, and indeed it doesn't :)

Any intention/idea how to fix it?
",,"['Apparently, I was wrong, and accuracy is indeed correct... (the masking is done in `weighted_objective`)\n']","['\ntrain_accuracy = T.mean(T.eq(T.argmax(self.y, axis=-1), T.argmax(self.y_train, axis=-1)))\n', '\nif mask is not None:\n    train_accuracy = T.mean(T.eq(T.argmax(self.y, axis=-1)[mask.nonzero()], \n                       T.argmax(self.y_train, axis=-1)[mask.nonzero()]))\n']","['sample_weight', 'Sequential', 'mask']",1,0
52,keras,6341,closed,Different Behaviour Between fit and fit_generator,"I am trying to understand the behavior of fit_generator so that I can use it with dataset too large to fit in my memory, so I use the following generator function to imitate the behavior of fit (keras version 1.2.0):

1. load x_data_train and y_data_train into the memory

2.  create a simple sequential model

3. define the generator function in the following way:
     

4. use the generator function in fit_generator

The fit_generator works fine with my simple toy data, but for true data, the result is totally different from using fit with the same batch_size and the same dataset: the loss either does not get better at all or generates some crazy output. In addition, the fit_generator is faster even though I expect it to be slower. To make things more weird, fit_generator is faster than fit even if I am loading dataset from hard disk with similar generator function.

I do not see any difference between this fit_generator and fit expect my generator is sampling with replacement, but I do not think it will create such a big difference in behaviour. ",,"['Code looks fine. Are you sure the number of batches for fit and fit_generator work out to be the same?', 'When using fit_generator, I set the samples_per_epoch to be equal to the total number of samples used in fit, so from the verbose output they look similar. Anything I am missing?', 'Turns out it is my mistake to place the random generator before the loop, so only a single batch is repeatedly used. I am closing this issue.']","['\r\ndef generateDataFromMemory(batch_size):\r\n    ind_row = np.random.randint(0, x_data_train.shape[0], size=batch_size)\r\n    while True:\r\n        x_data = x_data_train[ind_row]\r\n        y_data = y_data_train[ind_row].reshape(-1,1)\r\n        yield (x_data, y_data)\r\n']",[],1,0
53,keras,198,closed,NaN when accuracy reaches 1 with logistic loss,"Training a binary classification net with the  loss function, I found that when the  method has managed to correctly classify all test cases, the error will become 0 and thus the loss function becomes NaN. Any further training steps cause all future calls to  to return only NaNs as well, which is certainly not intended.

Ideally, the net would stop training once this occurs. At the very least, the case of the loss function becoming NaN should be handled gracefully.
",,"['I can look into this (I assume this can be solved with a fuzz factor). If you have any code and data to repro this, that would be helpful.\n', ""I actually encouter the same issue. But not with binary_crossentropy but with categorical crossentropy and with MSE. The network suddenly starts spitting out NaN values and from that point one never stops doing it.\nI'd like to give you a sample code, but its part of a big project so not an easy thing to do. My intuition tells me, that its maybe because of overflows. Theano is limited to float32. Maybe the weights are overflowing?\nIf you can give any hints on how to debug this issue than I'd gladly dive into it. Cant proceed with training if this isnt solved. I have a quite mature net now, but it needs further training. However, when I load the weights (which I can share if needed), the NaN errors appears after a couple of iterations.\n"", 'I believe this was resolved a while ago.\n', 'still happens on newest checkout\n', ""I'm getting this behaviour just by running the mnist_cnn.py example.\n""]",[],"['binary_crossentropy', 'fit', 'predict']",1,0
54,keras,3782,closed,Theano is running very slow on GPU,"I'm using Theano backend on Keras running on jupyter ipython notebook. Everything was working fine couples of days ago, but today I notice that it is running very very slow !, I think running it on CPU is much faster than that. I'm running [ mnist example](https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py) without making any changes. The batch (128 size) takes too long to process, around 5 minutes or something to finish, the next batch takes the same time almost or longer, it used to run mnist example very very fast (170 seconds for the whole epoch). 

Please help me how could I figure out the problem
Note: I tried it using anaconda then I tried it using winpython and the same problem occur.

I'm using windows 10, 16GB ram, Nvidia GTX 660 2 GB, cuDNN either enabled or disabled the same problem occur

[Theano issues thread](https://github.com/Theano/Theano/issues/4974)
",stale,"['@fchollet sir could you help me with this please ?\n', 'No one knows what happened during the last few days. You should first re-install every drivers and cuda/cudnn. \n', '@ibrahim-amer I am having the same issue. I am going to reinstall all of the drivers, but let me know if you figure out what was causing it.\n', 'I reinstalled all the drivers (gforce, cuDNN, cuda) but nothing changed, it is still running quite slow. I want first to verify if theano is actually using my GPU or not, when I run this line ""import keras"" this is the feedback I got: \nUsing Theano backend.\nUsing gpu device 0: GeForce GTX 660 (CNMeM is enabled with initial size: 50.0% of memory, cuDNN 5005)\n\nI think it is using GPU if I get this message. I also tried [this code here ](http://deeplearning.net/software/theano/tutorial/using_gpu.html#returning-a-handle-to-device-allocated-data)\nBut it prints:\n\n[HostFromGpu(<CudaNdarrayType(float32, vector)>), Elemwise{exp,no_inplace}(HostFromGpu.0), GpuFromHost(Elemwise{exp,no_inplace}.0)]\nLooping 1000 times took 14.408000 seconds\nResult is CudaNdarray([ 1.23178029  1.61879337  1.52278066 ...,  2.20771813  2.29967761\n  1.62323284])\nNumpy result is [ 1.23178029  1.61879337  1.52278066 ...,  2.20771813  2.29967761\n  1.62323284]\nUsed the cpu\n\nHere are my THEANO_FLAGS look like : \nfloatX=float32,device=gpu,nvcc.fastmath=True,lib.cnmem=0.5,exception_verbosity=high,optimizer=None\n', 'I know mine is using the GPU since I can use the command nvidia-smi in the terminal to see that the GPU is running.   I have similar theano flags, except I am using 90% of the GPU memory.  Mine runs for a while, and then gets slower as time goes on.  \n', '@rmkemker how much time the epoch take to finish ?\n', ""I am training a different network with a different dataset.  It took about 13 hours to get through 1 epoch (it is a very large dataset) and it took over 2 days to get through the 2nd epoch.  That doesn't make sense to me.\n"", '@rmkemker  Damn!! ... Please download [GPU-Z](https://www.techpowerup.com/downloads/2627/techpowerup-gpu-z-v0-8-7) and see what is GPU loading percentage while you train your network\n', ""That's a Windows product.  I am using Ubuntu 14.04.  I can still get all of those stats.\n![image](https://cloud.githubusercontent.com/assets/14336785/18610429/3a4a51d0-7cea-11e6-804f-f6d3a8db3e74.png)\n\nI am obviously using the first three GPUs.\n"", 'I do notice that the load bounces from max to basically zero.  I am pretty sure that is the fit generator in keras loading more data into the GPU.\n', ""This worked for me:\r\n\r\n`$ THEANO_FLAGS='mode=FAST_RUN' python myKerasScript.py`\r\n\r\nA short explanation [here](http://stackoverflow.com/questions/37768735/why-is-theano-running-so-slow). You can also set this configuration in your `~/theanorc` file."", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],1,0
55,keras,13521,closed,Got different results between using fit and fit_generator,"Hello,

I noticed I always got different results between fit_generator and fit.
Specifically, I noticed I always got good result **faster** with fit compared to using fit_generator. By faster I mean given the same epoch training the model with fit always gives better than fit_generator. I should also note that I make sure I implemented generator properly including shuffling with fit_generator.

Also I often noticed using fit gets better result compared to using fit_generator.

I am just curious: Do you observe the same behavior? If so, what is in fit that fit_generator does not have?",type:support,[],[],[],1,0
56,keras,2259,closed,Bug: Recurrent dropout fails silenty when set on loaded model,"Pretrained models are often finetuned on small datasets. For these use cases it can be relevant to add dropout to prevent overfitting. However, adding recurrent dropout to a loaded model (and recompiling the model) has no effect at all.

I've modified the  to demonstrate this. With recurrent dropout of 0.999, the model should not be able to learn anything, but it actually starts overfitting dramatically (it gets an accuracy of ~99% after 3 epochs). See the code below.


",,"[""`loaded_model.layers[2]` (dropout layer) uses the cached LSTM layer with dropout=0. Adding `loaded_model.cache_enabled = False` just after `loaded_model.load_weights('imdb_lstm_weights.h5')` solves the issue. \n"", 'That works perfectly. Thanks!\n\nFor other readers: A good idea is to set ``loaded_model.cache_enabled = True` after recompilation if you use any functionality that recompiles the model again later on.\n']","['\n\'\'\'Train a LSTM on the IMDB sentiment classification task.\n\nThe dataset is actually too small for LSTM to be of any advantage\ncompared to simpler, much faster methods such as TF-IDF+LogReg.\n\nNotes:\n\n- RNNs are tricky. Choice of batch size is important,\nchoice of loss and optimizer is critical, etc.\nSome configurations won\'t converge.\n\n- LSTM loss decrease patterns during training can be quite different\nfrom what you see with CNNs/MLPs/etc.\n\nGPU command:\n    THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python imdb_lstm.py\n\'\'\'\n\nfrom __future__ import print_function\nimport numpy as np\nnp.random.seed(1337)  # for reproducibility\n\nfrom keras.preprocessing import sequence\nfrom keras.utils import np_utils\nfrom keras.models import Sequential, model_from_yaml\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.recurrent import LSTM\nfrom keras.datasets import imdb\n\nmax_features = 20000\nmaxlen = 100  # cut texts after this number of words (among top max_features most common words)\nbatch_size = 32\n\nprint(\'Loading data...\')\n(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features,\n                                                      test_split=0.2)\nprint(len(X_train), \'train sequences\')\nprint(len(X_test), \'test sequences\')\n\nprint(""Pad sequences (samples x time)"")\nX_train = sequence.pad_sequences(X_train, maxlen=maxlen)\nX_test = sequence.pad_sequences(X_test, maxlen=maxlen)\nprint(\'X_train shape:\', X_train.shape)\nprint(\'X_test shape:\', X_test.shape)\n\nprint(\'Build model...\')\nmodel = Sequential()\nmodel.add(Embedding(max_features, 128, input_length=maxlen))\nmodel.add(LSTM(128))  # try using a GRU instead, for fun\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1))\nmodel.add(Activation(\'sigmoid\'))\n\n# try using different optimizers and different optimizer configs\nmodel.compile(loss=\'binary_crossentropy\',\n              optimizer=\'adam\',\n              class_mode=""binary"")\n\nprint(""Train..."")\nmodel.fit(X_train, y_train, batch_size=batch_size, nb_epoch=3,\n          validation_data=(X_test, y_test), show_accuracy=True)\nscore, acc = model.evaluate(X_test, y_test,\n                            batch_size=batch_size,\n                            show_accuracy=True)\nprint(\'Test score:\', score)\nprint(\'Test accuracy:\', acc)\n\n\nprint("""")\nprint(\'Saving weights.\')\n\nyaml_string = model.to_yaml()\nopen(\'imdb_lstm_config.yaml\', \'w+\').write(yaml_string)\nmodel.save_weights(\'imdb_lstm_weights.h5\', overwrite=True)\n\nprint(\'Reloading weights in new model with recurrent dropout.\')\nprint("""")\n\nloaded_model = model_from_yaml(open(\'imdb_lstm_config.yaml\').read())\nloaded_model.load_weights(\'imdb_lstm_weights.h5\')\nloaded_model.layers[0].dropout = 0.99\nloaded_model.layers[1].dropout_U = 0.99\nloaded_model.layers[1].dropout_W = 0.99\n\nloaded_model.compile(loss=\'binary_crossentropy\',\n              optimizer=\'adam\',\n              class_mode=""binary"")\n\nprint(""Train..."")\nloaded_model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=3,\n                 validation_data=(X_test, y_test), show_accuracy=True)\nscore, acc = loaded_model.evaluate(X_test, y_test,\n                                   batch_size=batch_size,\n                                   show_accuracy=True)\nprint(\'Test score:\', score)\nprint(\'Test accuracy:\', acc)\n']",['imdb_lstm'],1,0
57,keras,724,closed,Predictions using RNNs - Accuracy always 1.0,"I have a list of sequential values. I want to feed them into a RNN to predict the next value in the sequence.

[ 0.43589744  0.44230769  0.49358974 ...,  0.71153846  0.70833333 0.69230769]

I keep getting an accuracy of 1.0. I found a similar issue for classification but no methods used there worked for me.

I get a decreasing loss but my accuracy is always 1.0

How can I fix this?

model = Sequential()
model.add(SimpleRNN(1, 100))
model.add(Dense(100, 1, activation = ""sigmoid""))
model.compile(loss=""mean_squared_error"", optimizer = ""sgd"")

Epoch 0
1517/1517 [==============================] - 0s - loss: 0.0726 - acc: 1.0000 - val_loss: 0.0636 - val_acc: 1.0000
Epoch 1
1517/1517 [==============================] - 0s - loss: 0.0720 - acc: 1.0000 - val_loss: 0.0629 - val_acc: 1.0000
...
",,"[""You're working on a time-series type regression problem here -- all the `acc: 1.0000` means is that when the true value is > 0.5, so is your prediction, and vice versa. Focus on the `loss` field, this is the `mse` that you actually care about. When you call `.fit()`, I recommend setting `show_accuracy = False`.\n"", ""As Luke points out, accuracy is not relevant at all for a regression problem. \n\nBut even if your targets were binary labels, since your output is a scalar (instead of a binary categorical vector), you would need to set `class_mode='binary'` in `compile` for the accuracy metric to make sense. By default it's set to `categorical`.\n"", 'When my model finishes training, how should I measure the accuracy?\nsklearn.metrics.accuracy_score or manually compare y_tests with model.predict(x_tests)?\n\nEdit: Nevermind this accuracy question, I forgot about model.evaluate(...)\n\nFurthermore, am I using RNNs correctly? Is nb_timesteps inconsequential for RNNs or do I need to define it with something like an Embedding layer or the .shape of the x_trains?\n\nThanks for quick replies.\n', ""Embedding layer is usually designed for words. Google word2vec if you're not familiar with that.\n\nI think you're using RNNs correctly from my experience.\n\nIf your loss is 0, it means your accuracy is 100%. But again computing an accuracy does not make sense for continuous output. If you're predicting if the time series is going UP or DOWN, you can derive an accuracy because it's a classification problem.\n""]",[],[],1,0
58,keras,1237,closed,loss increases as training progresses,"Hi, 
I'm trying to build a bi-directional GRU
 to do gender recognition on text. My model is as follows 

model = Graph()
    model.add_input(name = 'input', input_shape = (None,max_features), dtype = 'float')
    model.add_node(GRU(100, activation='sigmoid', inner_activation='hard_sigmoid'), name='forward', input='input')
    #backwards in time direction
    model.add_node(GRU(100, activation='sigmoid', inner_activation='hard_sigmoid', go_backwards = True), name='backward', input='input')
    model.add_node(Dropout(0.5), name = 'first_dropout', inputs = ['forward','backward'])
    model.add_node(Dense(1, activation='sigmoid'), name='sigmoid', input='first_dropout')
    model.add_output(name='output', input='sigmoid')
    model.compile('SGD',  {'output': 'binary_crossentropy'})

My data is a 3D numpy array (x,y,z) where x is the number of samples, y is the time dimension (Words in the document example) and z is the word2vec encoding of this word (input dimension). The x and y are ints and the z are floats. The labels are binary.

Unfortunately, when I train, the cross entropy loss seems to jump all over the place, and I end up with a test accuracy of 0. Do you have any idea why this is? 
<img width=""458"" alt=""screen shot 2015-12-10 at 8 48 14 am"" src=""https://cloud.githubusercontent.com/assets/3630063/11716984/67eca744-9f1b-11e5-941c-3ca033196ba4.png"">

Thank you very much!
",stale,"['Did you try to reduce the (default) learning rate of SGD?\n\n/e: as a quick check, maybe try another optimizer that adapts the learning rate, e.g. rmsprop, and check if you get the desired behavior.\n', 'I have not, I will try that right away. It seems strange though that the model would give me a 0 test accuracy, should a model get approx 0.5 just by random guessing?\n', ""I've also encountered with this problem.\nHoping to get the resolving method.\n""]",[],[],1,0
59,keras,8929,closed,Loss and accuracy are not decreasing,"I am posting this issue here since no body in stackoverflow seems to know what it is the problem. I assume then that there is a bug and post here.

I am implementing a variant of  the CNN described by [this paper][1].
My problem is that the loss isn't decreasing and I don't understand why. Same have to be said concerning accuracy(stuck at 0.5 more or less).

This a problem of 2 classes classification. I am using the data from this  [website][2]:

I suspected the optimizer so I changed it whithout any improvements. I am pretty sure the data I am using is ok because I used it on an LSTM and the classifier was fine. 


Here is my code:

    from keras.layers import Embedding
    from keras.layers import Conv2D
    from keras.models import Sequential
    from keras.layers import MaxPooling2D
    from keras.layers import Reshape
    from keras.layers import Flatten
    from keras.layers import Dense
    from keras.layers import Dropout
    from keras.layers import Input
    from keras import backend as K
    
    from keras.models import Model
    import tensorflow as tf
    from sklearn import preprocessing
    import keras
    import numpy as np
    import pdb
    
    #using multiple filters
    config = tf.ConfigProto()
     
    # Don't pre-allocate memory; allocate as-needed
    config.gpu_options.allow_growth = True
     
    # Only allow a total of half the GPU memory to be allocated
    config.gpu_options.per_process_gpu_memory_fraction = 0.7
     
    # Create a session with the above options specified.
    config.gpu_options.per_process_gpu_memory_fraction = 0.65
    K.tensorflow_backend.set_session(tf.Session(config=config))
    class Classifier():
        def __init__(self,Vocab,maxlen=75):
            self.model=Sequential()
            self.EMBED_DIM=30
            self.batch_size=30
            self.MAX_SEQUENCE_LENGTH=maxlen
            self.nb_labels=2
            self.model = Sequential()
            self.Vocab=Vocab
        def fit(self, X,y):
            #pdb.set_trace()
            
            mainIn=Input(shape=(self.MAX_SEQUENCE_LENGTH,), dtype='int32', name='main_input')
            
            x=Embedding(len(self.Vocab)+2,self.EMBED_DIM,input_length=self.MAX_SEQUENCE_LENGTH)(mainIn)
            
            x=Reshape((1,self.MAX_SEQUENCE_LENGTH, self.EMBED_DIM))(x)
            
            x1=Conv2D(128, strides=2,kernel_size=5 ,activation=""relu"", padding='same')(x)
    
            x1=MaxPooling2D((self.MAX_SEQUENCE_LENGTH-5+1,1),padding='same')(x1)
            x1=Flatten()(x1)
            x2=Conv2D(128, strides=2, kernel_size=4, activation=""sigmoid"", padding='same')(x)
            x2=MaxPooling2D((self.MAX_SEQUENCE_LENGTH-4+1,1),padding='same')(x2)
            
            x2=Flatten()(x2)
            x3=Conv2D(128, strides=2, kernel_size=3, activation=""tanh"", padding='same')(x)
            x3=MaxPooling2D((self.MAX_SEQUENCE_LENGTH-3+1,1),padding='same')(x3)
            
            x3=Flatten()(x3)
            
            combinedX=keras.layers.concatenate([x1,x2,x3],axis=1)
            
            
            combinedX=Dense(64, activation=""relu"")(combinedX)
            combinedX=Dropout(0.2)(combinedX)
            #output=Dense(self.nb_labels, activation=""sigmoid"")(combinedX)
            #output=Dense(2, activation=""softmax"")(combinedX)
            output=Dense(1, activation=""sigmoid"")(combinedX)
    
            
            #encoder =preprocessing.LabelEncoder()
            #encoder.fit(y)
            #encoded_Y = encoder.transform(y)
            #labels=keras.utils.to_categorical(encoded_Y, num_classes=2)
            labels=y
            pdb.set_trace()
            inputs2=X
            self.model = Model(inputs=mainIn, outputs=output)
            
           # self.model.compile(loss='binary_crossentropy',
           #                 optimizer='adam',
           #                 metrics=['acc'])
            
            self.model.compile(loss='binary_crossentropy',
                            optimizer='rmsprop',
                            metrics=['acc'])
            
            self.model.fit(inputs2,labels,epochs=7, batch_size=self.batch_size)
        def predict(self, X):
            return self.model.predict(np.array(X))
    
        def predict_proba(self, X):
            
            return self.model.predict(np.array(X), self.batch_size)
 
Here is my code to preprocess the data:

  

         #loading the file
    file2=pd.read_csv(""Sentiment_Analysis Dataset.csv"",error_bad_lines=False)
            #splitting into train et test set
    
    from sklearn.model_selection import train_test_split
    Text=list(file2.SentimentText)
    file2.groupby('Sentiment').count()
    train_data,test_data,train_label,test_label=train_test_split(Text, file2.Sentiment, test_size=0.4, random_state=42)
    #Buidling the dictionary
    
    vocabDic=dict()
    for document in train_data:
        document=document.split("" "")
        for word in document:
            if word not in vocabDic:
                vocabDic[word]=len(vocabDic)+1
    vocabDic['unk']=len(vocabDic)+1
    
    #coding the documents
    def codeDocuments(documents,dictionnary):
        documentsArray=list()
        for i,document in enumerate(documents):
            tempList=list()
            document=document.split("" "")
            for word in document:
                if word in vocabDic:
                    word=vocabDic[word]
                else:
                    word=vocabDic['unk']
                tempList.append(word)
            documentsArray.append(tempList)
        return np.array(documentsArray)
    train_docs=codeDocuments(train_data,vocabDic)
    test_docs=codeDocuments(test_data,vocabDic)
    
    #padding the documents
    
    
    from keras.preprocessing import sequence
    
    maxlen=75
    train_set = sequence.pad_sequences(train_docs, maxlen=maxlen)
    test_set = sequence.pad_sequences(test_docs, maxlen=maxlen)
    #calling the model
    model=Classifier(vocabDic,maxlen)
    model.fit(train_set[:50000],train_label[:50000])





  [1]: https://arxiv.org/abs/1408.5882
  [2]: http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/

",,[],[],[],1,0
60,keras,7394,closed,Normalization of training weights,"https://github.com/fchollet/keras/blob/master/keras/engine/training.py#L463 is strange.

The code used for weighting is:

        # apply sample weighting
        if weights is not None:
            # reduce score_array to same ndim as weight array
            ndim = K.ndim(score_array)
            weight_ndim = K.ndim(weights)
            score_array = K.mean(score_array, axis=list(range(weight_ndim, ndim)))
            score_array *= weights
            score_array /= K.mean(K.cast(K.not_equal(weights, 0), K.floatx()))
        return K.mean(score_array)

I think the line 

            score_array /= K.mean(K.cast(K.not_equal(weights, 0), K.floatx()))

should become

            score_array /= K.mean(weights)

This would make the weighted loss be normalized. For instance, with N samples and weights w[i] with K non-zero weights, the weighted loss is sum(loss[i] * w[i] for i in range(N)) / K.

I think it should instead be sum(loss[i] * w[i] for i in range(N)) / sum(w).",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],1,0
61,keras,7404,closed,z_mean and z_log_var in variational_autoencoder.py,"z_mean and z_log_var look like same.
.....
x = Input(batch_shape=(batch_size, original_dim))
h = Dense(intermediate_dim, activation='relu')(x)
z_mean = Dense(latent_dim)(h)
z_log_var = Dense(latent_dim)(h)
.....
How is it possible?",,"['Hi, I have the same question as you. Do you get the answer? ', 'They are different, because each get through different layer of same level of the neural network. \r\nBy optimization, z_mean will be close to mean, and z_log_var be close to log std.\r\nHere I had another question: why use log std, not std?\r\nBecause log std is more numerically stable, see the link below.\r\n\r\nhttps://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/\r\n\r\n \r\n \r\n\r\n', ""Hi, thanks very much for the reply and the link. \r\nBut can I ask you another question: for the Custom loss layer, there're slight differences between your link and https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder_deconv.py\r\nfor this: xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean), why original_dim is multiplied here?\r\n\r\nAnd, for the link you provided, why there are just sums , no means, shouldn't\r\n 'recon = K.sum(K.binary_crossentropy(y_pred, y_true), axis=1)'\r\nbe sum and then mean?\r\n\r\nI will  appreciate it very much if you could reply."", 'See the source of metrics.binary_crossentropy. It returns mean of the cross entropy.\r\n\r\ndef binary_crossentropy(y_true, y_pred):\r\n    return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)\r\n\r\nSo to get the sum, multiplying original_dim is reasonable.\r\nBut again, there are many ways of handling losses. You can select the best one as much as it works.\r\n\r\nAs described in the ""Variational Autoencoder: Intuition and Implementation"":\r\n...\r\nAs we might already know, maximizing E[logP(X|z)] is a maximum likelihood estimation. We basically see it all the time in discriminative supervised model, for example Logistic Regression, SVM, or Linear Regression. In the other words, given an input z and an output X, we want to maximize the conditional distribution P(X|z) under some model parameters. So we could implement it by using any classifier with input z and output X, then optimize the objective function by using for example log loss or regression loss.\r\n...\r\n']",[],[],1,0
62,keras,7409,closed,keras is very slow with theano,"platform: i5 7500, 1080ti, newest keras and backends, windows 10
Model(MNIST, CNN):
[https://github.com/minimaxir/keras-cntk-benchmark/blob/master/test_files/mnist_cnn.py](url)
model = Sequential()
model.add(Conv2D(20, (5, 5), activation='relu',
                 input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(40, (5, 5), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.5))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))

this model takes 2s per epoch with tensorflow, 3s with cntk and 11s with theano.
I have my own simple code to train MNIST and takes 1.3s per epoch with theano, 2.0s with tensorflow and 3.1s with cntk. ",,"['Did you setup theano correctly? In my experiments theano is faster than TF.\r\nHere is my .theanorc config:\r\n```\r\n[global]\r\nfloatX=float32\r\ndevice=cuda0\r\n\r\n[nvcc]\r\nflags=-D_FORCE_INLINES\r\n\r\n[cuda]\r\nroot=/usr/local/cuda/bin\r\n\r\n[dnn]\r\nenabled=True\r\n\r\n[gpuarray]\r\npreallocate=.90\r\n```', 'This problem is solved. I changed backend by using “os.environ” before and didn\'t set ""image_data_format"" to ""channels_first"".  Backend should be changed by modifying ~/.keras/keras.json.']",[],[],1,0
63,keras,3826,closed,Keras Binary Classification of Time Series with Static Metadata,"I have created feature vectors from a irregular multivariate time series that additionally contains static metadata features. I have then converted all values into a one hot encoded representation (continuous time series values via binning to established thresholds and then one hot encoding them). I then time order them (irregular intervals) and concatenate them together like the following simplified example:



I then pad them to be the same length with zeros:



My fundamental question is if this actually an appropriate data representation?  I have read about approach to creating ""stationary"" time series representations that usually involve creating standardized distance metrics.  I have not run into lit similar to this simple an approach.  I have tried LSTM models more appropriate for time series (are very slow and low accuracy in my case), but I find treating this as a traditional binary classification problem gives me really exceptional accuracy on my task (even with dropout, l2 regularization applied). Any advice or insight would be appreciated!
",stale,[],"['\nEach Measurement Always Contains 5 Elements\n2 static_values 3 dynamic_values (one of which is time related itself) \nXX              XXX\n01110 for example\n\nid  feature_vector(time-ordered)    indicator\n1   01101 01001 01101 01100 01000       0\n2   11001 11001 11101                   1 \n3   01010                               0\n', '\nid   feature_vector(time-ordered)   indicator\n 1   01101 01001 01101 01100 01000       0\n 2   11001 11001 11101 00000 00000       1 \n 3   01010 00000 00000 00000 00000       0\n']",[],1,0
64,keras,249,closed,MNIST example 2x slower on GPU,"The MNIST example seems to be running about 2x slower for me on a GPU (Tesla K10.G1.8GB) than on CPU.

When I run it with profiling it seems that it is spending a large amount of time in GpuFromHost.  This would indicate that the data is not being loaded onto the GPU with shared variables (and in fact I can't really tell where in the Keras code this would be taking place) and thus each minibatch must be transferred to the GPU at each iteration of training.

Yet all the other examples give instructions for running on GPU, and thus Keras is presumably optimized for GPU. Am I missing something here?
",,"[""> This would indicate that the data is not being loaded onto the GPU with shared variables (and in fact I can't really tell where in the Keras code this would be taking place) and thus each minibatch must be transferred to the GPU at each iteration of training.\n\nData is being loaded on the GPU one batch at a time, as it should be. Loading all the data at once would only be viable for very small datasets (like MNIST) and would crash your GPU most of the time. \n\nI just upped the `batch_size` from 64 to 128. The example now runs 2x faster on GPU compared to CPU, and accuracy is slightly improved as well.\n"", 'I tried use keras with imagenet. The speed is about 170 times slower than using just theano. Probably the only difference is copy per batch or per chunk. \n', 'Then adjust the batch size. \n', 'Maybe it would be worth including some brief notes on performance\noptimisation somewhere in the docs.\n\nOn 13 August 2015 at 13:41, François Chollet notifications@github.com\nwrote:\n\n> Then adjust the batch size.\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/fchollet/keras/issues/249#issuecomment-130520899.\n\n## \n\n---\n\nTennessee Leeuwenburg\nhttp://myownhat.blogspot.com/\n""Don\'t believe everything you think""\n', 'Yes, I think we need a FAQ to answer these questions, as well as many others.\n', ""I am already use batch_size = 128. If I increase it too much then the GPU memory (4GB) is not enough to hold the model. This is the model I use. I am still checking whether I made some error somewhere to cause such drastic speed difference.\n\n```\ndef ImageNet(nb_classes):\n    model = Sequential()\n\n    model.add(Convolution2D(64, 3, 11, 11, subsample=(4, 4)))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(poolsize=(3, 3), stride=(2, 2)))\n\n    model.add(Convolution2D(96, 64, 5, 5, border_mode='same'))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(poolsize=(3, 3), stride=(2, 2)))\n\n    model.add(Convolution2D(128, 96, 3, 3, border_mode='same'))\n    model.add(Activation('relu'))\n\n    model.add(Convolution2D(256, 128, 3, 3, border_mode='same'))\n    model.add(Activation('relu'))\n\n    model.add(Convolution2D(256, 256, 3, 3, border_mode='same'))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(poolsize=(3, 3), stride=(2, 2)))\n\n    model.add(Flatten())\n    model.add(Dense(256 * 6 * 6, 4096))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n\n    model.add(Dense(4096, 4096))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n\n    model.add(Dense(4096, nb_classes))\n    model.add(Activation('softmax'))\n    return model\n```\n"", ""This is part of the profiling result:\n\n```\nClass\n---\n<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>\n  99.7%    99.7%     192.067s       9.60e+01s     C        2       1   theano.tensor.nnet.ConvGrad3D.ConvGrad3D\n   0.1%    99.8%       0.155s       1.93e-02s     C        8       4   theano.sandbox.cuda.basic_ops.HostFromGpu\n   0.1%    99.8%       0.110s       1.10e-02s     C       10       5   theano.sandbox.cuda.dnn.GpuDnnConv\n   0.1%    99.9%       0.100s       1.25e-02s     C        8       4   theano.sandbox.cuda.dnn.GpuDnnConvGradI\n   0.0%    99.9%       0.091s       1.13e-02s     C        8       4   theano.sandbox.cuda.dnn.GpuDnnConvGradW\n   0.0%   100.0%       0.038s       3.01e-04s     C      126      63   theano.sandbox.cuda.basic_ops.GpuElemwise\n   0.0%   100.0%       0.025s       2.53e-03s     C       10       5   theano.sandbox.cuda.basic_ops.GpuFromHost\n   0.0%   100.0%       0.014s       1.76e-03s     C        8       4   theano.sandbox.cuda.blas.GpuDot22\n   0.0%   100.0%       0.011s       1.83e-03s     C        6       3   theano.sandbox.cuda.dnn.GpuDnnPoolGrad\n   0.0%   100.0%       0.010s       1.61e-03s     C        6       3   theano.sandbox.cuda.blas.GpuGemm\n   0.0%   100.0%       0.006s       9.69e-04s     C        6       3   theano.sandbox.cuda.dnn.GpuDnnPool\n```\n\nConvGrad3D takes too much time. I found the solution [here](http://theano-users.narkive.com/Twhmp86s/cnn-with-cudnn-order-of-magnitude-slower-than-should-be).\n\n> the problem is that this op is being used to implement the\n> gradient of a strided convolution: theano.tensor.nnet.ConvGrad3D.ConvGrad3D\n> \n> It runs on the CPU, so it's very slow (as you can see in the profile). To\n> work around this, you can use theano.sandbox.cuda.dnn.dnn_conv instead of\n> theano.tensor.nnet.conv.conv2d in your code\n\nI changed the part of related part of code in keras, and now it is 170 times faster!\n"", '> ConvGrad3D takes too much time. I found the solution here.\n\nInteresting discussion. Would you like to submit a PR with this change (GPU-only)?\n', 'PR is here: https://github.com/fchollet/keras/pull/524\nNote that now cudnn is required and it can only run on GPU.\n', 'Was this merged?\n']",[],[],1,0
65,keras,6906,closed,teano.function is slower as compared to model.predict,"For my application i need 2 output from the trained network (one at final layer and one at intermediate layer). Since model.predict() gives output from only last layer, i wrote theano function to fetch output at both intermediate layer and final layer. It works fine, but it is almost 20 times slower as compared to model.predict().

Can anybody help me to understand why theano function is slower and how can i make it faster.",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],1,0
66,keras,3325,closed,Misclassification on small training set with pre-trained word vectors,"I am one of the many users having issues with passing the pre-trained embedding matrix to the Keras Embedding layer. My code currently looks like the following:



The first row of the pre-trained matrix  contains a random vector for the special token 'PAD'. This token is assigned the index 0 in the corpus and I assume that this index has special meaning in the lookup. The second row in this matrix contains a trained vector for the 'UNK' token which is returned whenever a word is not found in the vocabulary ('PAD' != 'UNK').

In the Embedding layer the input_dim is , which is the same number of rows in the  matrix. The output_dim is , which is the dimension of the word vectors. The input_length is  which is the (fixed) number of words in the padded sentences (padding with 0). The mask_zero argument is set to False because the matrix  already has a vector assigned in the first row for the 'PAD' token (index 0).

After running this on a very small training set of ~200 labeled sentences (3 labels) the accuracy is around 55% in both training and validation, but the prediction completely fails even on the training set. Almost all sentences therein are labeled the same.

So now it comes the questions:
1. Is it reasonable to use such a small training set to predict labels on sentences given a pre-trained word embeddnig matrix?
2. We have tried **not** passing the pre-trained embedding matrix and it produces the ""same"" results in terms of accuracy and predicted labels. How can we make sure this matrix is being used by the LSTM classifier?
3. Is it necessary to have a zero vector for the 'PAD' token or it can be random like we did?
4. We observe that the accuracy is highly correlated to the proportion of the labels in the training set. Do you think this information can help further debug the issue?
5. Could you add a complete example to the documentation with multiclass classification of sentences given word vectors? It seems that many people are struggling to get it working.

Please let me know if you need anything else in order to help with this issue.
",,"['Hi @juliohm, please find some of my thoughts on this:\n- Is it reasonable to use such a small training set to predict labels on sentences given a pre-trained word embeddnig matrix?\n  _Actually I expect pre-trained matrix will specially help when you have a smaller data set, as long as the language features in your classification problem are not so different from those used to train the embedding vectors._\n- We have tried not passing the pre-trained embedding matrix and it produces the ""same"" results in terms of accuracy and predicted labels. How can we make sure this matrix is being used by the LSTM classifier?\n  _Do you mean that you used an embedding layer without pre-trained matrix or without an embedding layer at all (e.g., by one-hot encoding)?_\n- Is it necessary to have a zero vector for the \'PAD\' token or it can be random like we did?\n  _To be frank I haven\'t totally figured out how this masking works. But usually using a zero vector for padding is a good idea because it essentially correspond to dropping out that part of sequence when you are using LSTM or 1d CNN._\n- We observe that the accuracy is highly correlated to the proportion of the labels in the training set. Do you think this information can help further debug the issue?\n  _There are many examples within Keras showing how to do multi classification with texts by using different models, e.g., with or without an embeddding, with an LSTM or 1D CNN and etc._\n- Could you add a complete example to the documentation with multiclass classification of sentences given word vectors? It seems that many people are struggling to get it working.\n  _I think the [pretrained_word_embedding example](https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py) is what you are looking for._\n\n_And finally you probably need to set your embedding layer\'s `trainable=False` as shown in the example above to fix the pretrained embedding vectors in classification. That\'s what people usually do. And just make sure your sequences in train_X have word hashing index starting from 1 to voacb_size, with 0s as paddings._\n', ""Hi @dolaameng, thank you for taking the time. Continuing the discussion:\n1. I specifically asked about a training set with only ~200 labeled sentences. Is there any use case with such small number?\n2. I meant what I wrote: we kept the embedding layer, but didn't pass the pre-trained matrix.\n3. Apparently we have the same question. We both don't know how the mask works and the underlying conventions implemented in the library.\n4. I don't see the relation between what I asked and what you wrote.\n5. Why the example you linked doesn't mention LSTM?\n\nI hope that my questions are clearer now, I am still at the same page not knowing what is the cause of the issue.\n"", 'Hi @juliohm , \n1. whether 200 is too small or not depends on many other things, e.g., distribution of classes (is one of them dominating? ), avg seq len and etc. To be able to finger out which part of model is bringing down the performance, you need to benchmark it with at least a baseline (not necessarily from Keras), e.g., Naive Bayesian, Bag-of-words + MLP and etc. It\'s hard to tell if another use case is similar to yours just based on the # of sentences and # of classes. \n2. You can get the output from embedding layer and test if your weights are being used.\n3. Sorry I thought you were asking about the reason why people usually use zero vector to represent the padding and how a random vector would change the result. That\'s the part that I am not fully sure of. For the implementation in Keras, you can always refer to the source code.\n4. Simply saying ""accuracy is highly correlated to the proportion of the labels in the training set. "" won\'t reveal any further information, e.g., if 90% of your sentences are label 1 of course you can get 90% accuracy by just guessing everything is label 1. I was suggesting to use a baseline model to understand whether the problem is on your usage of embedding or other things. Again many Keras examples serve the educational purpose very well.\n5. I thought you were asking about ""multiclass classification of sentences given word vectors""?  LSTM is only one way of doing text classification. In fact, it is not even accepted as the best way compared to CNN when it comes to classifying the documents.\n\nIf your question is not directly related to identified bugs in Keras, I suggest you might want to post your thread to the user discussion group https://groups.google.com/forum/#!forum/keras-users for more answers. Hope it helps!\n', 'Hi @dolaameng, thank you for the helpful comments.\n1. I have trained a simple neural network on the same dataset, please find the code below:\n\n``` python\n# Step 9: Train neural net\n\nfrom sknn.mlp import Classifier, Layer\nfrom sklearn import cross_validation\n\ndef sentence_to_vec(sentence_words):\n    widxs = [dictionary.get(word, 1) for word in sentence_words]\n    return np.mean(final_embeddings[widxs,:], axis=0)\n\nnn = Classifier(layers=[Layer(""Tanh"", units=20),\n                        Layer(""Softmax"")],\n                learning_rate=0.001,\n                n_iter=4000)\n\nallvecs = np.array([sentence_to_vec(entry[0]) for entry in interp])\nalllabels = np.array([entry[1] for entry in interp])\n\nscores = cross_validation.cross_val_score(nn, allvecs, alllabels, cv=5)\nprint(""Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))\n```\n\nThe accuracy I get is 0.63 (+/- 0.10), but more importantly, the prediction on unseen data at least gives different labels (3 possible labels), which makes me conclude that something is wrong with my Keras LSTM implementation.\n1. Could you please elaborate on how I could perform this check in the original code?\n2. Yes, I will end up having to read the source code, this is kind of sub-optimal. I wish this convention was documented somewhere in Keras.\n3. It seems that the issue is in the RNN. According to 1.) a simple neural net on the average of vectors is outperforming the LSTM classification.\n4. That is interesting, could you please provide an example in the Keras repository that is known to be superior for sentence classification? What neural network architecture is known to work best?\n\nI will definitely refer to the Google forum if I don\'t get more insight here, thanks for sharing the resource.\n', 'I have good news that I would like to share with other users.\n\nIf you are not getting good results, definitely consider trying different optimizers and learning rates. It was just this tiny change that caused the entire net to fail in the sentence classification task.\n\n@dolaameng thank you for the support, I am closing the issue.\n', '@juliohm Can you share your improved accuracy numbers? I am a little surprised that you got an acceptable classifier with for a 3-class problem with just 200 examples. \r\n\r\nI am also trying to evaluate a similar model with some pre-trained embeddings, so it would interesting to know what other optimizers and learning rates did you try?']","["" python\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dropout, Dense\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\n\ntrainX = [sentence_to_vecidxs(entry[0]) for entry in train_data]\ntrainY = [entry[1] - 1 for entry in train_data]\n\ntestX = [sentence_to_vecidxs(entry[0]) for entry in test_data]\ntestY = [entry[1] - 1 for entry in test_data]\n\nhidden_units = 100\nmaxwords = 50 # pad sentences to contain this number of words\n\n# padding sentences\ntrainX = pad_sequences(trainX, maxlen=maxwords)\ntestX = pad_sequences(testX, maxlen=maxwords)\n\n# class hot vectors\ntrainY = to_categorical(trainY, nb_classes=nlabels)\ntestY = to_categorical(testY, nb_classes=nlabels)\n\n# pre-trained word embeddings of size (vocabulary_size x embedding_size)\nW = final_embeedings\n\nmodel = Sequential()\nmodel.add(Embedding(vocabulary_size, embedding_size, input_length=maxwords, mask_zero=True, weights=[W]))\nmodel.add(LSTM(hidden_units, return_sequences=False))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(nlabels, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\nmodel.fit(trainX, trainY, validation_data=(testX, testY), nb_epoch=30, batch_size=16, verbose=2)\n""]","['final_embeddings', 'vocabulary_size', 'final_embeddings', 'embedding_size', 'maxwords', 'final_embeddings']",1,0
67,keras,13053,closed,improve LSTM accuracy ,"

I'm trying to build LSTM architecture to predict sickness rate. I'm actually stuck in 40% accuracy, I'm new in machine learning and I tried several tips like changing the optimzer, the layer node number and the dropout value without any improving. So could you guys help me with some advice.

the x array is composed of 10 columns

the y array is only one column the sickness rate

here is my model 

this is the output of . evaluate()

and thank you in advance",,"['Hi there,\r\n\r\nYou have a mismatch:\r\n\r\n1. Your model has an output activation ``linear`` and loss ``mean_square_error`. It is not constrained to predict between the interval [0, 1]. \r\n\r\n2. Accuracy as a metric is assuming probabilities.\r\n\r\nIf you are trying to do classification, change the loss to crossentropy and the output activation to sigmoid/softmax (binary/categorical). If you are trying to do regression, accuracy metric is misplaced.\r\n\r\nBest', ""Thanks @briannemsick  for your reply to clarify more my problem is a regression one .I'm trying to predict the skinnes rate (between 0-100%) using historical information of this disease . So which activation and loss function should i use for the out put?"", 'In that case ``linear`` and ``mean_square_error`` are both fine, ``accuracy`` is not a valid metric  in this case (not a classification problem). Consider using ``mean_square_error`` (the loss function) or ``mean_absolute_error`` as a metric.', ""So if I understood well the accuracy doesn't have any sens for my case.Then based on your experience from which value of mean_absolute_error I can say that I have efficient model with good prediction."", ""@cabiste007, I think you should have a look at what `mean_square_error` actual means and/or how it is computed. In its essence, MSE measures the average squared error of our predictions. For every prediction, the square difference between its prediction and its target value is computed. All squared differences are then averaged.  \r\n\r\nAn acceptable MSE value will be different for different datasets. We cannot quantify an 'optimal MSE score'. \r\nTo consider an MSE score to be 'good', you must consider the values you are predicting, as well as the distribution of the variables in your original dataset. \r\n\r\nAs an example, let's consider two mock datasets. I'll only compare these datasets, based on their variable ranges (or scale). Both datasets are well distributed.\r\n\r\n1)  First dataset has reasonable large values, such as car prices. Cars in this mock dataset are all somewhere in the range of 1000 to 80.000 usd. \r\nIf our model has an MSE of 50 we could say that the **average squared difference** between the estimate values and the predicted values is about 50.  \r\nOur model's performance is quite good, we will only predict values that are not that far off the truth.\r\n\r\n2) Second dataset has smaller values, such as lengths of persons (in cm). Our variables are all somewhere in the range of 120 to 250 cm. \r\nIf our second model as an MSE of 50 as well, we have to notice that this is quite a large error; as the possible range of our values is smaller, a MSE of 50 suddenly makes the average squared error for our predictions a lot further away from the truth than those in our first model. \r\n"", ""Adding to @RooieRakkert 's answer, there are two methods you can use to check how if your model is performing well for regression task:\r\n- If you're using `root_mean_squared` metric, make sure that training, validation, and testing error are low and close to each other in magnitude.\r\n- Use R^2 (coefficient of determination) metric from `sklearn` library. The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get an R^2 score of 0.0. So you can check if your R^2 score is close to 1 then it's a good model."", '@briannemsick , @RooieRakkert and @dabasajay  I\'m so grateful thnak to you everything is clear in my mind right now. So I need to change the metrics to  `mean_absolute_error`, `mean_square_error` or ` R^2` and run `fit()` and  wait until my model converge close 0.0.\r\nSo now I implemented this architecture now:\r\n`def lstm_model():\r\n model = Sequential()\r\n model.add(LSTM(10, input_shape=(1,10), return_sequences= True))\r\n model.add(Dropout(0.2))\r\n model.add(LSTM(100, return_sequences= True)) \r\n model.add(LSTM(100, return_sequences= False))\r\n model.add(Dense(1,activation=""linear""))\r\n model.compile(optimizer=\'adam\',loss=\'mean_squared_error\',metrics=[\'mean_squared_error\'])\r\n return model\r\nlstm = lstm_model()\r\nnewlstmhis = lstm.fit(xtrr,ytr,epochs=1000 , validation_data=(xtstt, ytst),verbose=2, shuffle=True)`\r\n\r\nbut my model didn\'t converge knowing that using this ANN converge and give me mse=0.8 which is reasonable for my case (my values are between 0%-100%) :\r\n`import numpy\r\nnumpy.random.seed(8)\r\ndef build_dropout_model2(rate):\r\n model = Sequential()\r\n model.add(Dense(1000, input_shape=(10,), activation=""relu""))\r\n model.add(Dense(500, activation=""relu""))\r\n model.add(Dense(1))\r\n model.summary()\r\n model.compile(loss=""mean_squared_error"", optimizer=""adam"", metrics=[""mean_squared_error""])\r\n return model\r\nmodel2 = build_dropout_model2()`\r\n\r\nBased on your experience what should I change in the lstm model to make a good prediction and thank you']",[],"['def lstm_model():\r\n model = Sequential()\r\n model.add(LSTM(10, input_shape=(1,10), return_sequences= True))\r\n model.add(Dropout(0.2))\r\n model.add(LSTM(100, return_sequences= True))\r\n model.add(LSTM(100, return_sequences= False))\r\n model.add(Dropout(0.2))\r\n model.add(Dense(50,kernel_constraint=NonNeg(),kernel_initializer=\'normal\' ,activation=""relu""))\r\n model.add(Dense(1,activation=""linear""))\r\n model.compile(optimizer=\'adam\',loss=\'mean_squared_error\',metrics=[\'accuracy\'])\r\n return model\r\nlstm = lstm_model()', '1275/1275 [==============================] - 1s 526us/sample - loss: 0.0015 - acc: 0.3930\r\n0.0014869439909029204   0.3930161']",1,0
68,keras,11006,closed,Predict point by point is very slow,"Hi,
I am using Keras LSTM on time series prediction (let time series is y[n]). Network input is time shifted vector y[0], y[-1], y[-2] etc.... After network training, I calculate one step prediction, then add prediction to input vector on first place and release last value. So i get prediction to e.g. 50 future points. Problem is, that predict method is very slow for this case. I noticed, that prediction time does change only a little with changing network size (for small networks - e.g.  1-50 LSTM cells). I know, that prediction of whole array of input of vectors is significantly faster, but that is not what i want. I tried to change Keras backend to CNTK or Theano, but it was worse than Tensorflow. 
Is there possibility to speed it up? What about of adding fast predict method for only one input vector? Would be there a way to use Tensorflow or CNTK directly without running through Keras?

Model I use (example):









My function for multiple step prediction (I know, converting array to lists an backwards is not optimal):












`
Keras version is 2.2.2
Tensorflow version is 1.5.0 (I have problems with newer versions)

Profiling shows, that bottle neck ist in _pywrap_tensorflow_internal.TF_Run method.


",,[],[],"['model = Sequential()', 'model.add(LSTM(units=10, input_shape = (49,1), return_sequences = False)) ', 'model.add(Dropout(0.2))', 'model.add(Dense(1))', 'model.add(Activation(""linear""))', 'model.compile(loss=""mse"", optimizer=""adam"")', 'model.summary()', 'def createPrediction(testData, n_future_steps):', '_x = array([testData])', '_result = []', '_for i in range(0, n_future_steps):', ' __prediction = model.predict(x)', ' __result.append(prediction[0, 0])', ' __p = [prediction[0, 0]] + x[0, :-1, 0].tolist()', ' __r = array(p)', ' __x = r.reshape(1, -1, 1)', '_result = array(result)', '_return result']",1,0
69,keras,11014,closed,nan loss although multiple outputs loss not nan,"Using model with multiple outputs I am getting nan loss despite all outputs loss seems valid. screenshot attached.
This behavior remains when I change loss functions (I have tried standard 'mae', 'mse', 'categorical_crossentropy' and some customized loss functions as well).
This behavior remains when I change optimizers (I have tries Adam and SGD with momentum).

Any advice would be appreciated.

multiple loss do seem to converge though :-| (screenshot attached).
![image](https://user-images.githubusercontent.com/25052915/44705551-a2a85000-aaa7-11e8-9b94-be5a7b3f0311.png)

![image](https://user-images.githubusercontent.com/25052915/44705587-bb186a80-aaa7-11e8-818b-7f7a4478c5cc.png)

loss & val loss are nan -
![image](https://user-images.githubusercontent.com/25052915/44705694-06cb1400-aaa8-11e8-8a43-5ee17f353eed.png)

",,"[""I'm hitting this now. Did you ever sort this out?"", ""I'm not sure if it resolves your issue or not, but try to change your input data type to 'float64'.\r\n`x_train = x_train.astype('float64')`\r\nIt worked in my case.""]",[],[],1,0
70,keras,4365,closed,Embedding with TensorFlow very slow: converts indices to dense gradients,"I've noticed that the Embedding layer with TensorFlow backend is converting sparse gradient updates to dense ones and killing the performance, as well as gobbling up lots of memory.  This is making it unusable for a large scale problem with a large Embedding layer.

Here is a script that makes a model with single large embedding layer using Keras and TensorFlow directly.  In Keras, it takes about 2.3 seconds / batch and uses > 9 GB of memory while training.  In TensorFlow it only takes 20 ms / batch (100X faster) and uses < 4 G of memory.

This is using TensorFlow 0.11.0rc2 and the master branch of Keras.



Outputs:



With TensorFlow:


 
Outputs:

",stale,"[""After some debugging, it turns out this is due to the Keras optimizers and the way in which they compute gradient updates.  The sparse gradient updates produced by the embedding layer need to be handled in a different manner then the dense gradient updates.  TensorFlow optimizers provide a mechanism to do this (methods `_apply_sparse` and `_apply_dense`).  This problem goes away by using a TensorFlow optimizer, e.g.:\n\n```\nmodel.compile(loss='mse', \n              optimizer=TFOptimizer(tf.train.GradientDescentOptimizer(0.1)))\n```\n\noutputs\n\n```\n100 loops, best of 3: 17.7 ms per loop\n```\n"", 'I applied a similar approach as you suggested  and training started working much faster -  thanks !\r\nKera can not save the model though - \r\n\r\n\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\keras\\models.py:114: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\r\n\r\n\r\n\r\n', 'Also, when running this in an AWS machine I got the following error - \r\n\r\n        for attr \'tensor_type\'\r\n        ; NodeDef: embedding_1/embeddings/_77 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_142_embedding_1/embeddings"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^Adagrad/learning_rate/_59, ^Adagrad/update_embedding_1/embeddings/UnsortedSegmentSum, ^Adagrad/update_embedding_1/embeddings/Unique); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\nE tensorflow/core/common_runtime/executor.cc:594] Executor failed to create kernel. Invalid argument: AttrValue must not have reference type value of float_ref\r\n         for attr \'tensor_type\'\r\n        ; NodeDef: embedding_1/embeddings/_77 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_142_embedding_1/embeddings"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^Adagrad/learning_rate/_59, ^Adagrad/update_embedding_1/embeddings/UnsortedSegmentSum, ^Adagrad/update_embedding_1/embeddings/Unique); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n         [[Node: embedding_1/embeddings/_77 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_142_embedding_1/embeddings"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^Adagrad/learning_rate/_59, ^Adagrad/update_embedding_1/embeddings/UnsortedSegmentSum, ^Adagrad/update_embedding_1/embeddings/Unique)]]\r\nTraceback (most recent call last):\r\n  File ""/home/ubuntu/anaconda2/envs/python35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1022, in _do_call\r\n    return fn(*args)\r\n  File ""/home/ubuntu/anaconda2/envs/python35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1004, in _run_fn\r\n    status, run_metadata)\r\n  File ""/home/ubuntu/anaconda2/envs/python35/lib/python3.5/contextlib.py"", line 66, in __exit__\r\n    next(self.gen)\r\n  File ""/home/ubuntu/anaconda2/envs/python35/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: AttrValue must not have reference type value of float_ref\r\n         for attr \'tensor_type\'\r\n        ; NodeDef: embedding_1/embeddings/_77 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_142_embedding_1/embeddings"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^Adagrad/learning_rate/_59, ^Adagrad/update_embedding_1/embeddings/UnsortedSegmentSum, ^Adagrad/update_embedding_1/embeddings/Unique); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n         [[Node: embedding_1/embeddings/_77 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_142_embedding_1/embeddings"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^Adagrad/learning_rate/_59, ^Adagrad/update_embedding_1/embeddings/UnsortedSegmentSum, ^Adagrad/update_embedding_1/embeddings/Unique)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""Trainer.py"", line 30, in <module>\r\n    Train()\r\n  File ""Trainer.py"", line 24, in Train\r\n    validation_data=testGenerator.generate())\r\n  File ""/home/ubuntu/anaconda2/envs/python35/lib/python3.5/site-packages/keras/legacy/interfaces.py"", line 87, in wrapper\r\n    return func(*args, **kwargs)\r\n  File ""/home/ubuntu/anaconda2/envs/python35/lib/python3.5/site-packages/keras/engine/training.py"", line 1876, in fit_generator\r\n    class_weight=class_weight)\r\n  File ""/home/ubuntu/anaconda2/envs/python35/lib/python3.5/site-packages/keras/engine/training.py"", line 1620, in train_on_batch\r\n    outputs = self.train_function(ins)\r\n  File ""/home/ubuntu/anaconda2/envs/python35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py"", line 2075, in __call__\r\n    feed_dict=feed_dict)\r\n  File ""/home/ubuntu/anaconda2/envs/python35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 767, in run\r\n    run_metadata_ptr)\r\n  File ""/home/ubuntu/anaconda2/envs/python35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File ""/home/ubuntu/anaconda2/envs/python35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File ""/home/ubuntu/anaconda2/envs/python35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: AttrValue must not have reference type value of float_ref\r\n         for attr \'tensor_type\'\r\n        ; NodeDef: embedding_1/embeddings/_77 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_142_embedding_1/embeddings"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^Adagrad/learning_rate/_59, ^Adagrad/update_embedding_1/embeddings/UnsortedSegmentSum, ^Adagrad/update_embedding_1/embeddings/Unique); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n         [[Node: embedding_1/embeddings/_77 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_142_embedding_1/embeddings"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^Adagrad/learning_rate/_59, ^Adagrad/update_embedding_1/embeddings/UnsortedSegmentSum, ^Adagrad/update_embedding_1/embeddings/Unique)]]\r\n\r\n\r\nI used the following optimizer - \r\n\r\noptimizer = opt.TFOptimizer(tf.train.AdagradOptimizer(0.01))', ""Keras can save the model, just not its optimizer. It's no big deal, just\nrecompile the model after loading.\n\nAnyone interested in adding support for sparse gradient updates in Keras\noptimizers?\n\nOn 3 April 2017 at 23:24, pianoman4873 <notifications@github.com> wrote:\n\n> I applied a similar approach as you suggested and training started working\n> much faster - thanks !\n> Kera can not save the model though -\n>\n> \\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\keras\\models.py:114:\n> UserWarning: TensorFlow optimizers do not make it possible to access\n> optimizer attributes or optimizer state after instantiation. As a result,\n> we cannot save the optimizer as part of the model save file.You will have\n> to compile your model again after loading it. Prefer using a Keras\n> optimizer instead (see keras.io/optimizers).\n>\n> —\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/fchollet/keras/issues/4365#issuecomment-291278604>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AArWb5NR5FqPdUl1PpJCQ8yXE6-2y1aCks5rsWOXgaJpZM4Kwju5>\n> .\n>\n"", 'thanks @fchollet , but look at the bigger issue when running this on a machine with GPU and the line\r\n\r\noptimizer = opt.TFOptimizer(tf.train.AdagradOptimizer(0.01))\r\n\r\n\r\nE tensorflow/core/common_runtime/executor.cc:594] Executor failed to create kernel. Invalid argument: AttrValue must not have reference type value of float_ref\r\nfor attr \'tensor_type\'\r\n; NodeDef: embedding_1/embeddings/_77 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_142_embedding_1/embeddings"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^Adagrad/learning_rate/_59, ^Adagrad/update_embedding_1/embeddings/UnsortedSegmentSum, ^Adagrad/update_embedding_1/embeddings/Unique); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n[[Node: embedding_1/embeddings/_77 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_142_embedding_1/embeddings"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^Adagrad/learning_rate/_59, ^Adagrad/update_embedding_1/embeddings/UnsortedSegmentSum, ^Adagrad/update_embedding_1/embeddings/Unique)]]\r\nTraceback (most recent call last):\r\nFile ""/home/ubuntu/anaconda2/envs/python35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1022, in _do_call\r\nreturn fn(*args)\r\nFile ""/home/ubuntu/anaconda2/envs/python35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1004, in _run_fn\r\nstatus, run_metadata)\r\nFile ""/home/ubuntu/anaconda2/envs/python35/lib/python3.5/contextlib.py"", line 66, in exit\r\nnext(self.gen)\r\nFile ""/home/ubuntu/anaconda2/envs/python35/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status\r\npywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: AttrValue must not have reference type value of float_ref\r\nfor attr \'tensor_type\'\r\n; NodeDef: embedding_1/embeddings/_77 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_142_embedding_1/embeddings"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^Adagrad/learning_rate/_59, ^Adagrad/update_embedding_1/embeddings/UnsortedSegmentSum, ^Adagrad/update_embedding_1/embeddings/Unique); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n[[Node: embedding_1/embeddings/_77 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_142_embedding_1/embeddings"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^Adagrad/learning_rate/_59, ^Adagrad/update_embedding_1/embeddings/UnsortedSegmentSum, ^Adagrad/update_embedding_1/embeddings/Unique)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\nFile ""Trainer.py"", line 30, in \r\nTrain()\r\nFile ""Trainer.py"", line 24, in Train\r\nvalidation_data=testGenerator.generate())\r\nFile ""/home/ubuntu/anaconda2/envs/python35/lib/python3.5/site-packages/keras/legacy/interfaces.py"", line 87, in wrapper\r\nreturn func(*args, **kwargs)\r\nFile ""/home/ubuntu/anaconda2/envs/python35/lib/python3.5/site-packages/keras/engine/training.py"", line 1876, in fit_generator\r\nclass_weight=class_weight)\r\nFile ""/home/ubuntu/anaconda2/envs/python35/lib/python3.5/site-packages/keras/engine/training.py"", line 1620, in train_on_batch\r\noutputs = self.train_function(ins)\r\nFile ""/home/ubuntu/anaconda2/envs/python35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py"", line 2075, in call\r\nfeed_dict=feed_dict)\r\nFile ""/home/ubuntu/anaconda2/envs/python35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 767, in run\r\nrun_metadata_ptr)\r\nFile ""/home/ubuntu/anaconda2/envs/python35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 965, in _run\r\nfeed_dict_string, options, run_metadata)\r\nFile ""/home/ubuntu/anaconda2/envs/python35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1015, in _do_run\r\ntarget_list, options, run_metadata)\r\nFile ""/home/ubuntu/anaconda2/envs/python35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1035, in _do_call\r\nraise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: AttrValue must not have reference type value of float_ref\r\nfor attr \'tensor_type\'\r\n; NodeDef: embedding_1/embeddings/_77 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_142_embedding_1/embeddings"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^Adagrad/learning_rate/_59, ^Adagrad/update_embedding_1/embeddings/UnsortedSegmentSum, ^Adagrad/update_embedding_1/embeddings/Unique); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n[[Node: embedding_1/embeddings/_77 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_142_embedding_1/embeddings"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^Adagrad/learning_rate/_59, ^Adagrad/update_embedding_1/embeddings/UnsortedSegmentSum, ^Adagrad/update_embedding_1/embeddings/Unique)]]\r\n\r\n\r\nWhen I ran it with optimizer = opt.Adagrad() it runs fine but becomes very slow do to the conversion to dense gradient update when the embedding size is big...\r\n\r\n', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']","[""python\r\nimport numpy as np\r\n\r\nfrom keras.layers import Embedding, Input\r\nfrom keras.models import Model\r\n\r\n# a model with just one Embedding layer\r\ntoken_ids = Input(batch_shape=(128, 20),\r\n                          dtype='int32', name='token_ids')\r\ntoken_embedding = Embedding(793471,\r\n    512, mask_zero=False, input_length=20)(token_ids)\r\n\r\nmodel = Model(input=[token_ids], output=token_embedding)\r\nmodel.compile(loss='mse', optimizer='sgd')\r\n\r\nX = np.random.randint(0, 793471, (128, 20)).astype(np.int32)\r\ny = np.random.rand(128, 20, 512)\r\n\r\n# compile model\r\nmodel.train_on_batch(X, y)\r\n\r\n# now time\r\n%timeit model.train_on_batch(X, y)\r\n"", '\r\nUsing TensorFlow backend.\r\n/Users/matthewp/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gradients.py:87: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 406257152 elements. This may consume a large amount of memory.\r\n  ""This may consume a large amount of memory."" % num_elements)\r\n1 loop, best of 3: 2.32 s per loop\r\n', 'python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ntoken_ids = tf.placeholder(tf.int32, [128, 20])\r\nW = tf.Variable(tf.zeros([793471, 512]))\r\ntoken_embedding = tf.gather(W, token_ids)\r\ny_ = tf.placeholder(tf.float32, [128, 20, 512])\r\nloss = tf.reduce_mean((token_embedding - y_) ** 2)\r\n\r\ntrain_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\r\n\r\ninit = tf.initialize_all_variables()\r\n\r\nX = np.random.randint(0, 793471, (128, 20)).astype(np.int32)\r\ny = np.random.rand(128, 20, 512)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    sess.run(train_step, feed_dict={token_ids: X, y_: y})\r\n    %timeit sess.run(train_step, feed_dict={token_ids: X, y_: y})\r\n', '\r\n10 loops, best of 3: 20.8 ms per loop\r\n']",[],1,0
71,keras,270,closed,Inconsistent behavior of batch normalization layer,"As you can see by the code below, the behavior of batch normalization seems to be dependent on the batch of samples. Any given sample should always be classified exactly the same (in mode 0 at least). Note that for the single item batch, the output of model.predict(arr1[:1]) is identical to the output of model.predict(arr1[1:]).



I've read the source code a thousand times and cannot find a reason for this to happen. I've got a working theory that there is an issue with running mean and running std being mutable within get_output(). Those values are also not inspectable as they are not stored as a tensor value:



This issue makes batch normalization fairly useless to me as I cannot guarantee that a sample will always generate the same prediction.
",,"[""> I've got a working theory that there is an issue with running mean and running std being mutable within get_output(). Those values are also not inspectable as they are not stored as a tensor value\n\nSo what would be your suggestion for a fix? Put these quantities inside Theano tensors?\n"", ""It appears that trying to mutate the running mean and std during training time is considered destructive, and according to theano's documentation should be handled by optimizers. I'm trying to figure out what this means for how the operation needs to be implemented, but I think this might only solve the issue with inspection. I keep looking over the math for computing the batch normalization result and the logic really does seem sound. The resulting mean and std should definitely have the correct dimensions, which means that by all logic this issue shouldn't be happening at all even with the mutation occurring within the function.\n""]","[""\n>>> import numpy\n>>> \n>>> from keras.models import Sequential\n>>> from keras.layers.core import Dense, Activation\n>>> from keras.layers.normalization import BatchNormalization\n>>> from keras.optimizers import SGD\n>>> \n>>> model = Sequential()\n>>> model.add(Dense(200,5))\n>>> model.add(BatchNormalization(5))\n>>> model.add(Activation('softmax'))\n>>> sgd = SGD(lr=0.01, decay=3e-6, momentum=0.9, nesterov=False)\n>>> model.compile(loss='categorical_crossentropy', optimizer=sgd)\n>>> arr1 = numpy.random.rand(2,200)\n>>> y1 = numpy.random.rand(2,5)\n>>> model.train(arr1,y1)\narray(3.693775110217432)\n>>> model.predict(arr1)\n2/2 [==============================] - 0s\narray([[ 0.21000414,  0.20690492,  0.19180691,  0.19093657,  0.20034746],\n       [ 0.1894262 ,  0.19338509,  0.20825628,  0.20882832,  0.2001041 ]])\n>>> model.predict(arr1[:1])\n1/1 [==============================] - 0s\narray([[ 0.19959944,  0.20018073,  0.20001223,  0.1998318 ,  0.2003758 ]])\n>>> model.predict(arr1[1:])\n1/1 [==============================] - 0s\narray([[ 0.19959944,  0.20018073,  0.20001223,  0.1998318 ,  0.2003758 ]])\n"", ""\n>>> import theano\n>>> theano.printing.debugprint(model.layers[1].running_mean)\nElemwise{true_div,no_inplace} [@A] ''   \n |Sum{axis=[0], acc_dtype=float64} [@B] ''   \n | |Elemwise{add,no_inplace} [@C] ''   \n |   |dot [@D] ''   \n |   | |<TensorType(float64, matrix)> [@E]\n |   | |<TensorType(float64, matrix)> [@F]\n |   |DimShuffle{x,0} [@G] ''   \n |     |<TensorType(float64, vector)> [@H]\n |DimShuffle{x} [@I] ''   \n   |Subtensor{int64} [@J] ''   \n     |Elemwise{Cast{float64}} [@K] ''   \n     | |Shape [@L] ''   \n     |   |Elemwise{add,no_inplace} [@C] ''   \n     |Constant{0} [@M]\n""]",[],1,0
72,keras,13582,closed,Training slows down after a fixed num of iterations,"Hi! I'm training a small Keras model (just 32k params) with a dataset composed of 50000 samples, each of them is stored on disk (HDD). Each sample has a size of 249.1 kB. The validation dataset has 8000 samples.
I use a custom data generator during training that unpickles data from disk and feeds it to the model. Each epoch last 390 iterations with a batch_size of 128 (plus 64 iters for validation). I set  for the fit_generator.
I also use threadsafe iterations.

I'm using Ubuntu 16.04 with Python 3, keras 2.2.4  and tensorflow-gpu 1.13.1.
My PC has 32 GB of RAM (15 used) and 32 GB of swapping memory (5 used).

THE PROBLEM:
During training, the first ~250 iterations of the first epoch last just a few seconds and later the training is slowed down. Then the GPU usage is ~0% and the disk usage is: . The first epoch lasts ~550 sec. and the second one ~1700 sec.

Can this problem be related to the HDD disk usage? I cannot find the reason for this slowing down.",type:bug/performance,[],[],"['use_multiprocessing = False, workers = 8, max_queue_size=10', 'Load average: 2.15 2.30 2.19']",1,0
73,keras,13585,closed,BatchNormalization produces NaN weights without NaN loss[NOT duplicate],"**System information**  
- Have I written custom code (as opposed to using example directory):  
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  CentOS Linux release 7.2.1511 (Core)
- TensorFlow backend (yes / no):  YES
- TensorFlow version:  v1.12.3-0-g41e0a4f56c 1.12.3
- Keras version:  2.1.6-tf
- Python version:  python 3.6.5
- CUDA/cuDNN version:  9.0.176/7.6.3.30
- GPU model and memory:  Tesla P100-PCIE 16GB x4

My model is shown below, with many BN layers.



My test set is (128 * n, 40, 40, 1), batch_size = 128
It's normal for me to run my code on a single GPU
However, when I use multiple GPUs（multi_gpu_num）, the weight of the BN layer becomes NAN.
Strangely, this seems to be related only to GPU_NUM :
When GPU_NUM = 2, the weights become Nan at the 129th step of training,
When GPU_NUM = 4, the weights become Nan at the 65th step of training,
Whether I change the batch_size to 64, 128, or 512, the above behaviors still exist

code:



How can I solve this problem？",type:bug/performance,[],"[""\r\n def DnCNN(depth, filters=64, image_channels=1, use_bnorm=True):\r\n    layer_count = 0\r\n    inpt = Input(shape=(None, None, image_channels), name='input' + str(layer_count))\r\n    # 1st layer, Conv+relu\r\n    layer_count += 1\r\n    x = Conv2D(filters=filters, kernel_size=(3, 3), strides=(1, 1), kernel_initializer='Orthogonal', padding='same',\r\n               name='conv' + str(layer_count))(inpt)\r\n    layer_count += 1\r\n    x = Activation('relu', name='relu' + str(layer_count))(x)\r\n    # depth-2 layers, Conv+BN+relu\r\n    for i in range(depth - 2):\r\n        layer_count += 1\r\n        x = Conv2D(filters=filters, kernel_size=(3, 3), strides=(1, 1), kernel_initializer='Orthogonal', padding='same',\r\n                   use_bias=False, name='conv' + str(layer_count))(x)\r\n        if use_bnorm:\r\n            layer_count += 1\r\n            # x = BatchNormalization(axis=3, momentum=0.1,epsilon=0.0001, name = 'bn'+str(layer_count))(x)\r\n            x = BatchNormalization(axis=3, momentum=0.0, epsilon=0.0001, name='bn' + str(layer_count))(x)\r\n        layer_count += 1\r\n        x = Activation('relu', name='relu' + str(layer_count))(x)\r\n        # last layer, Conv\r\n    layer_count += 1\r\n    x = Conv2D(filters=image_channels, kernel_size=(3, 3), strides=(1, 1), kernel_initializer='Orthogonal',\r\n               padding='same', use_bias=False, name='conv' + str(layer_count))(x)\r\n    layer_count += 1\r\n    x = Subtract(name='subtract' + str(layer_count))([inpt, x])  # input - noise\r\n    model = Model(inputs=inpt, outputs=x)\r\n\r\n    return model \r\n"", '\r\nif __name__ == \'__main__\':\r\n    # model selection\r\n\r\n    # with tf.device(\'/cpu:0\'):\r\n    #     model = DnCNN(depth=17, filters=64, image_channels=1, use_bnorm=True)\r\n    model = DnCNN(depth=17, filters=64, image_channels=1, use_bnorm=True)\r\n    model.summary()\r\n\r\n    # for reproducibility\r\n    np.random.seed(seed=10)\r\n\r\n    # load the last model in matconvnet style\r\n    initial_epoch = findLastCheckpoint(save_dir=save_dir)\r\n    if initial_epoch > 0:\r\n        print(\'resuming by loading epoch %03d\' % initial_epoch)\r\n        model = load_model(os.path.join(save_dir, \'model_%03d.hdf5\' % initial_epoch), compile=False)\r\n\r\n    # compile the model\r\n    gpu_num = 2\r\n    print(""[INFO] training with {} GPUs..."".format(gpu_num))\r\n    # model = DnCNN(depth=17, filters=64, image_channels=1, use_bnorm=True)\r\n    parallel_model = multi_gpu_model(model, gpus=gpu_num)\r\n    # model.compile(optimizer=Adam(0.001), loss=sum_squared_error)\r\n    parallel_model.compile(optimizer=Adam(0.001), loss=sum_squared_error)\r\n\r\n    # use call back functions\r\n    args.train_data = r\'data/Train400\' #train set\r\n    args.batch_size = 512\r\n    args.epoch = 2\r\n    checkpointer = ModelCheckpoint(os.path.join(save_dir, \'model_{epoch:03d}.hdf5\'),\r\n                                   verbose=1, save_weights_only=False, period=args.save_every)\r\n    csv_logger = CSVLogger(os.path.join(save_dir, \'log.csv\'), append=True, separator=\',\')\r\n    lr_scheduler = LearningRateScheduler(lr_schedule)\r\n    data = train_datagen(batch_size=args.batch_size, data_dir=args.train_data)\r\n    # history = model.fit_generator(data,\r\n    #                               steps_per_epoch=2000, epochs=args.epoch, verbose=1, initial_epoch=initial_epoch,\r\n    #                               callbacks=[checkpointer, csv_logger, lr_scheduler])\r\n    history = parallel_model.fit_generator(data,\r\n                                  steps_per_epoch=2000,epochs=args.epoch, verbose=1, initial_epoch=initial_epoch,\r\n                                  callbacks=[csv_logger, lr_scheduler])\r\n    model.save(\'test.h5\')\r\n    parallel_model.save(\'testpa.h5\')\r\n\r\n']",[],1,0
74,keras,7954,closed,Wrong result for cosine proximity: keras 2.0.8,"# Conclusion: Observation of keras cosine proximity stuck as -1/3 #
As noted by numerous post, Keras seriously currently has an issue with cosine proximity:

https://github.com/fchollet/keras/issues/3031
https://github.com/fchollet/keras/issues/5046

Here is the code in jupyter notebook for simple test:




The printed result is 


**So the true cosine proximity is actually 0.9986, but keras shows near -1/3. Of course keras would use the negative of cosine proximity for minimization purpose, but it should be -0.9986.., in any case, don't trust the outcome of metric in keras cosine proximity**
",,[],"[""\r\nimport keras\r\nfrom keras.layers import Input, Dense\r\nfrom keras.models import Model\r\nimport numpy as np\r\n\r\n# --> print keras version\r\nprint keras.__version__\r\n\r\n# --> compute average cosine between all angles samples\r\ndef computeMeanConsineAngle(x,y):\r\n    cosMean = 0\r\n    numSample = x.shape[0]\r\n    for i in xrange(numSample):\r\n        cosMean += np.dot(x[i,:],y[i,:])/np.sqrt(np.dot(x[i,:],x[i,:])*np.dot(y[i,:],y[i,:]))\r\n        \r\n    return cosMean/float(numSample)\r\n\r\nX = np.random.random((1000,3))\r\nY = X\r\n\r\ninputs = Input(shape=(3,))\r\npreds = Dense(3,activation='linear')(inputs)\r\nmodel = Model(inputs=inputs,outputs=preds)\r\n\r\nsgd=keras.optimizers.Adam(lr=1e-2)\r\nmodel.compile(optimizer=sgd ,loss='mse',metrics=['cosine_proximity'])\r\nmodel.fit(X,Y, batch_size=1000, epochs=500, shuffle=False)\r\n\r\npred = model.predict(X)\r\n\r\nfrom sklearn.metrics import mean_squared_error\r\nmse = mean_squared_error(X, pred)\r\n\r\n\r\n%pylab\r\n%matplotlib inline\r\nplt.scatter(pred,Y)\r\n\r\nprint 'mse = ', mse\r\nprint computeMeanConsineAngle(pred, Y)\r\n\r\ntestX = np.array([[1,0]])\r\ntestY = np.array([[1,0]])\r\n- computeMeanConsineAngle(testX,testY)\r\n"", '\r\nEpoch 500/500\r\n1000/1000 [==============================] - 0s - loss: 7.1132e-04 \r\n- cosine_proximity: -0.3329\r\nUsing matplotlib backend: TkAgg\r\nPopulating the interactive namespace from numpy and matplotlib\r\nmse =  0.000703760391565\r\n0.998615947541\r\n']",[],1,1
75,keras,13601,closed,Wrong Validation Loss with Multi-Output Architecture,"**System information**  
- TensorFlow backend (yes / no):   yes
- TensorFlow version:  1.13.2
- Keras version:  2.3.1
- Python version:  3.6
- CUDA/cuDNN version:  10.0

**Describe the current behavior**  
While training a network with multiple output layers (3 in my case), i observe wrong validation loss:


**Describe the expected behavior**  
The validation loss should look like the one of the training. Even for the training, the loss is not the sum of all outputs, but it has the same dimension.
I don't have this problem with keras 2.2.4. This Problem pop up from version 2.2.5 upwards. Unfortunately, i can't go back to 2.2.4, because I need the new on_test_begin callback.
**Code to reproduce the issue**  
Provide a reproducible test case that is the bare minimum necessary to generate the problem.  

**Other info / logs**  



",type:bug/performance,[],[],"['Epoch 20/100', '1281/1281 [==============================] - 1549s 1s/step - loss: 219.5982 - l_out_loss: 49.2567 - r_out_loss: 42.5549 - s_out_loss: 110.9972 - val_loss: 1.7971 - val_l_out_loss: 13944.2723 - val_r_out_loss: 20445.2947 - val_s_out_loss: 9993.8637', 'mod.compile(optimizer, loss=[myLossE1, myLossE2, myLossE3])', 'mod.fit(x_train, [y_train, y_train, y_train],\r\n            steps_per_epoch=len(train_dataset) // train_dataset.batch_size,\r\n            epochs=1000,\r\n            validation_data=(x_val, [y_val, y_val, y_val]),\r\n            validation_steps=len(validation_dataset) // validation_dataset.batch_size,\r\n            callbacks=callbacks_list)']",1,0
76,keras,6447,closed,model.predict() gives same output for all inputs,"I'm trying to learn a regression problem. The data is mostly one-hot encoded categorical variables, one continuous. The target output is a probability (0-1). Here is the code:


It sure seems to be learning _something_:
![hist](https://cloud.githubusercontent.com/assets/3537118/25560256/935f61a0-2d04-11e7-8406-14281273edb4.png)

But print(preds[0:10]) gives:

Even though print(evals) gives a loss and mse of:


It even does that when I call  on training data.

I've tried no regularization, more regularization, different optimizers, different learning rates, mean/std normalization, less depth, more depth, all with the same result. 

Any ideas?",type:support,"['> model.predict() gives same output for all inputs\r\n> It sure seems to be learning something:\r\n\r\nI would assume that is precisely what the model is learning: to predict the same ""optimal"" output regardless of the input.', ""Right, I figure it's probably learning the average target value for the training cases, or something similar, it just seems odd that the val/test MSE is then so low as well, but perhaps my data has a similar average for any given batch of cases."", 'More epochs helped in my case. Not a real solution for @schmolze but something to try if others have the same issue.\r\n\r\nI thought I had the same problem. I am performing multi-label classification. Whether I use the predict() method or the predict_classes() method, I got the same output prediction for every test case. But when I increased the number of epochs from 10 to 250, my model was no longer suffering from this problem. ', ""I'm using an many to many LSTM for categorical prediction of images that have been sub-sampled (tiled). While training the acc and val_acc hit 100% and the loss and val_loss decrease to 0.03 over 100 epochs. I use model.predict() on the training and validation set, getting 100% prediction accuracy, then feed in a quarantined/shuffled set of tiled images and get 33% prediction accuracy every time. \r\n\r\nEven after shuffling and making another prediction, the outputs are exactly the same (same sequence of classes predicted). Not sure what to do.\r\n\r\n```\r\nopt = Adam(lr=.00001)\r\nmodel.add(LSTM(200,input_shape=(100, 3600), return_sequences=True, init='he_normal', \r\n                                                         inner_init='he_normal'))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(6))#output size, each number being a future value. 1 is what we want\r\nmodel.add(Activation('sigmoid'))\r\nmodel.compile(loss='categorical_crossentropy',\r\n                         optimizer=opt,\r\n                         metrics=['accuracy'])\r\n           \r\nmodel.fit(trainX, \r\n               trainY,\r\n               validation_data=(testX,testY),\r\n               validation_split=0.8,\r\n               nb_epoch=250,\r\n               shuffle=True,\r\n               batch_size=402,\r\n               verbose=1)\r\n```"", 'Update:\r\n\r\nI compared the prediction using 250 epochs to a 15 epoch prediction. There are different predictions, but still the same accuracy of 33%?', '@schmolze if it helps, I started to fix this by adding `validation_split=0.4`.', ""Even I am facing a similar issue. \r\n\r\nI am using a sequence to sequence model to extract key-phrases from a text document. I have trained the stacked LSTM model (for encoding) over 15 text documents (I have a dataset of 450 documents, but because of limitations in RAM available we are using a very small dataset for validation purpose).\r\n\r\nThe following model is used to train the encoder and the encoder,\r\n```\r\nencoder_inputs = Input(shape=(tx, f))\r\nencoder1 = LSTM(lstm_ip[0], return_sequences = True)(encoder_inputs)\r\nencoder2 = LSTM(lstm_ip[1], return_sequences = True)(encoder1)\r\nencoder3 = LSTM(lstm_ip[2], return_state = True)\r\nencoder_outputs, state_h, state_c = encoder3(encoder2)\r\nencoder_states = [state_h, state_c]\r\n\r\n\r\n#DECODER\r\ndecoder_inputs = Input(shape=(None, vs_op))\r\ndecoder_lstm = LSTM(lstm_ip[2], return_sequences = True, return_state = True)\r\ndecoder_outputs,_,_ = decoder_lstm(decoder_inputs, initial_state = encoder_states)\r\ndecoder_dense = Dense(vs_op, activation='softmax')\r\ndecoder_outputs = decoder_dense(decoder_outputs)\r\n\r\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\r\n```\r\nTo test (inference) the following model is used\r\n```\r\n#ENCODER\r\nencoder_model = Model(encoder_inputs, encoder_states)\r\n\r\n#DECODER\r\ndecoder_state_input_h = Input(shape=(lstm_ip[2],))\r\ndecoder_state_input_c = Input(shape=(lstm_ip[2],))\r\n\r\ndecoder_state_input = [decoder_state_input_h, decoder_state_input_c]\r\ndecoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state = decoder_state_input)\r\n\r\ndecoder_states = [state_h, state_c]\r\ndecoder_outputs = decoder_dense(decoder_outputs)\r\ndecoder_model = Model([decoder_inputs] + decoder_state_input, [decoder_outputs] + decoder_states)\r\n```\r\nThis is how I have compiled and trained the data\r\n```\r\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy')\r\nmodel.fit([encoder_input_data, decoder_input_data], decoder_output_data,\r\n          batch_size=8,\r\n          epochs=50)\r\n```\r\nmodel.predict() seems to give the same output irrespective of the input as we are getting exact same results for different inputs. I am calling the predict() for the same data the model is trained on.\r\nIs it that the data we have used for training small which is causing the problem? Or is the number of epochs less?"", 'model.predict() seems to be giving me the same output irrespective of the input . However, model.predict_on_batch seems to be giving me proper results . Is there any reason for this ?', 'Hi @aarif96 , have you found any solutions on this?', 'Try reducing the batch size. You may have a smaller dataset for the given problem.', 'Hi @varunranga , I do have a small dataset, however, I am using a generator, which should help me.\r\nI tried reducing the batch size to no effect. Do you have more ideas?', '@saketkarve  I am also facing the same problem what should I do?', '@tragu in my case: reduced the number of output classes, increased the number of samples of each class and fixed inconsistencies using a pre-trained model', 'I also encountered this kind of problem before, it turns out that there is one NaN value in my dataset. Another fix might be scale your dataset first.', ""@labrax no i haven't found any solutions regarding this. "", 'I got this issue in a dense model in keras, which was solved by using more neurons, more layers and adding more dropout. Also lowering learning rate almost always helps.', 'Also had this problem. Im new to the topic, so i relied on snippets from samples i found. All of them had an activation function in the input layer, as well as yours. Im far from being an expert, but thats different than all the theorie I read about nn. When I did **remove the activation from input layer**, my model finally produced varying outputs. If that only works because it produces a faulty architecture, someone with a more solid background may explain me why.', 'Same problem in my network, so far am trying to run it with less fully connected layers after the resNet50', '> Same problem in my network, so far am trying to run it with less fully connected layers after the resNet50\r\n\r\nthis solution has worked!', 'Closing as this is resolved', '> I also encountered this kind of problem before, it turns out that there is one NaN value in my dataset. Another fix might be scale your dataset first.\r\n\r\nScaling the dataset fixed this for me.', '> I also encountered this kind of problem before, it turns out that there is one NaN value in my dataset. Another fix might be scale your dataset first.\r\n\r\nScaling helped me . Thanks !', 'Scaling solved this problem for me as well.', ""My model is giving almost 70% accuracy even for validation.  there is no NaN value in dataset and it predicted the exact same output for any data. I tried on test and train data as well. Scaling didn't help. I even normalized the data. What can be done? \r\n\r\n![falut](https://user-images.githubusercontent.com/44817120/55700744-8609ba00-599e-11e9-8432-8329c7ea748d.png)\r\n"", ""Consider that this might actually be the best accuracy on your data set.\n\nOne thing you could try is augmentating your data...more data helps.\nAnother,try to reduce your learning rate...it helps allow the model learn\nmore.\nIf you dont reduce the learning rate, change the batch size...reduce it.\n\nlastly shuffling the training data during can also help.\n\nOn Mon, Apr 8, 2019, 08:36 Shubhra Deshpande <notifications@github.com>\nwrote:\n\n> My model is giving almost 70% accuracy even for validation. there is no\n> NaN value in dataset and it predicted the exact same output for any data. I\n> tried on test and train data as well. Scaling didn't help. I even\n> normalized the data. What can be done?\n>\n> [image: falut]\n> <https://user-images.githubusercontent.com/44817120/55700744-8609ba00-599e-11e9-8432-8329c7ea748d.png>\n>\n> —\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/keras-team/keras/issues/6447#issuecomment-480689937>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ATTiheimYWqvUBLKIDTnFYvSvzlNOv9Eks5vetVRgaJpZM4NMbkD>\n> .\n>\n"", ""> Consider that this might actually be the best accuracy on your data set. One thing you could try is augmentating your data...more data helps. Another,try to reduce your learning rate...it helps allow the model learn more. If you dont reduce the learning rate, change the batch size...reduce it. lastly shuffling the training data during can also help.\r\n> […](#)\r\n> On Mon, Apr 8, 2019, 08:36 Shubhra Deshpande ***@***.***> wrote: My model is giving almost 70% accuracy even for validation. there is no NaN value in dataset and it predicted the exact same output for any data. I tried on test and train data as well. Scaling didn't help. I even normalized the data. What can be done? [image: falut] <https://user-images.githubusercontent.com/44817120/55700744-8609ba00-599e-11e9-8432-8329c7ea748d.png> — You are receiving this because you commented. Reply to this email directly, view it on GitHub <[#6447 (comment)](https://github.com/keras-team/keras/issues/6447#issuecomment-480689937)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ATTiheimYWqvUBLKIDTnFYvSvzlNOv9Eks5vetVRgaJpZM4NMbkD> .\r\n\r\nThank you. Reducing batch size and augmenting data actually solved the issue. It wasn't the best accuracy perhaps but I think my notebook was autosaved at a certain time. Restarting the whole notebook worked. Thanks "", ' in my case Reducing batch size does not  change my output    of problem no #6447  there is any other thing i can do ?', 'After banging my head against the wall for over an hour with this same problem, and having tried all of the suggestions here without success, I eventually found the issue in my training data. I had an error in my normalization function and as a result, I had negative values in the training set. If none of the above suggestions work, this might be worth looking at. Cheers, Shane.', ""Hi, I'm sorry to bring this problem up again but my attempts of all solutions above do not meet ideal effects in the end.\r\n\r\nI'm currently coping with audio inputs and planning to output one single number with my regression network. However, after scaling the inputs, reducing the batch size(even to 1), augmenting the data, re-sampling the data to be more evenly distributed, and even reducing the model to only one hidden layer, I displayed the variance of predicted results and found out that it shrank to zero half way in the first epoch and never came back to non-negative values(which indicates that the model generates only one prediction again). \r\n\r\nI also tried to reduce the size of training data to let it overfit,  but the outcome was eerie. Attempts have also been implemented to include the penalty over predicting the only one same value into loss function, but even though sometimes by luck there's non-zero variance in the predicted values, the error(or more specific in my regression model, mse) gets increasingly lower through epochs.\r\n\r\nCould there be any further solutions to such an issue? Thank you so much for your attention and help in advance."", ""Just suggesting a couple of pointers.\n1. Try to make sure the data is balanced, by that I mean, make sure all\nclasses are well represented.\n2. Consider using RNNs if you aren't, choose or use models with LSTMs.\n3.Use a shallower network first and use the deeper ones later.\n4.Perhaps more importantly, make sure you are using the right activation\nfunctions.\n\n\nOn Tue, 9 Jul 2019, 22:16 xxx041, <notifications@github.com> wrote:\n\n> Hi, I'm sorry to bring this problem up again but my attempts of all\n> solutions above do not meet ideal effects in the end.\n>\n> I'm currently coping with audio inputs and planning to output one single\n> figure with my regression network. However, after scaling the inputs,\n> reducing the batch size(even to 1), augmenting the data, re-sampling the\n> data to be more evenly distributed, and even reducing the model to only one\n> hidden layer, I displayed the variance of predicted results and found out\n> that it shrank to zero half way in the first epoch and never came back to\n> non-negative values(which indicates that the model generates only one\n> prediction again).\n>\n> I also tried to reduce the size of training data to let it overfit, but\n> the outcome was eerie. Attempts have also been implemented to include the\n> penalty over predicting the only one same value into loss function, but\n> even though sometimes by luck there's non-zero variance in the predicted\n> values, the error(or more specific in my regression model, mse) gets\n> increasingly lower through epochs.\n>\n> Could there be any further solutions to such an issue? Thank you so much\n> for your attention and help in advance.\n>\n> —\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/keras-team/keras/issues/6447?email_source=notifications&email_token=AE2OFBJ74JYO3N2B4ZHUCLDP6TPYHA5CNFSM4DJRXEB2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZRHYDA#issuecomment-509770764>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AE2OFBNIZ4653MRMI5PLP2TP6TPYHANCNFSM4DJRXEBQ>\n> .\n>\n"", '> Just suggesting a couple of pointers. 1. Try to make sure the data is balanced, by that I mean, make sure all classes are well represented. 2. Consider using RNNs if you aren\'t, choose or use models with LSTMs. 3.Use a shallower network first and use the deeper ones later. 4.Perhaps more importantly, make sure you are using the right activation functions.\r\n> […](#)\r\n> On Tue, 9 Jul 2019, 22:16 xxx041, ***@***.***> wrote: Hi, I\'m sorry to bring this problem up again but my attempts of all solutions above do not meet ideal effects in the end. I\'m currently coping with audio inputs and planning to output one single figure with my regression network. However, after scaling the inputs, reducing the batch size(even to 1), augmenting the data, re-sampling the data to be more evenly distributed, and even reducing the model to only one hidden layer, I displayed the variance of predicted results and found out that it shrank to zero half way in the first epoch and never came back to non-negative values(which indicates that the model generates only one prediction again). I also tried to reduce the size of training data to let it overfit, but the outcome was eerie. Attempts have also been implemented to include the penalty over predicting the only one same value into loss function, but even though sometimes by luck there\'s non-zero variance in the predicted values, the error(or more specific in my regression model, mse) gets increasingly lower through epochs. Could there be any further solutions to such an issue? Thank you so much for your attention and help in advance. — You are receiving this because you commented. Reply to this email directly, view it on GitHub <#6447?email_source=notifications&email_token=AE2OFBJ74JYO3N2B4ZHUCLDP6TPYHA5CNFSM4DJRXEB2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZRHYDA#issuecomment-509770764>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AE2OFBNIZ4653MRMI5PLP2TP6TPYHANCNFSM4DJRXEBQ> .\r\n\r\nThank you so much for your super timely and effective answer!\r\n\r\nI\'m not quite sure if I understand your points right, but I actually tried some variants of them before, and here are a few responses to your kind suggestions:\r\n\r\n1. Since the data I have at hand are for regression models as well, they do not fall under specific classes but only have different values. And as you said correctly, they are not quite evenly distributed(most larger than the rest). \r\nAccording to my understanding and attempts, I try to ""re-sample"" the data by using the smaller numbers more frequently in training. Nonetheless, the only difference is that the predicted number changes in value(just like the different mean values we obtain by applying different weights to data). I think the problem could be more likely within my model.\r\n\r\n\r\n2. The current model I\'m using is composed fully of BLSTM layers (and of course a dense layer in the end). I referred to the excellent paper of Alex Graves(Framewise phoneme classification with bidirectional LSTM and other neural network architectures), so I also revised the last dense layer to be preceded by a TimeDistributed Dense layer and hence a Lambda layer to obtain the mean values that connected to the ultimate Dense(1) layer. However, there\'s no improvement at all.\r\n\r\n\r\n3. Currently the simplification of the model shall be my major breakthroughs to be made here, but even with one single BLSTM of 8 units failed to give me sanguine outcomes. I\'m still working to figure out the reason that led me to the bottleneck!\r\n\r\n\r\n4. The activation I\'m using after each BLSTM layer is the rectified one(default).\r\nI did add a clipped ReLU after the final Dense layer to ensure the output does not exceed the range it is supposed to stay in. According to your nice suggestion, I removed it, but within the first few epochs I monitored, the predictions converge to the mean value even from Epoch 2, and hence this clipped ReLU is not working to confine the model at all.\r\n\r\n\r\nTo specify my model more clearly, I am using batch training. But because of the varied lengths of audio inputs, I padded zeros to the end of audios except the longest one in each batch. I\'m wondering if it is the zeros padded that cause such a mess.\r\n\r\nThank you so much again for your warm replies! They are all helpful and I\'m still working to implement them fully in depth. ', 'I had a similar issue, the problem on my end is I had too many neurons per layer, causing over-saturation of the model. I reduced the number of neurons by a large amount and that seemed to fix it on my end. ', 'Same problem here!\r\n\r\nI am training a small network and the training seems to go fine, the val loss decreases, I reach validation accuracy around 80, and it actually stops training once there is no more improvement (patience=10). It trained for 40 epochs. However, it keeps predicting only one class for every test image! \r\n\r\nI tried to initialize the conv layers randomly, I added regularizers, I switched from Adam to SGD, I added clipvalue, I added dropouts, I downsized the network. I also switched to softmax (I have only two labels but I saw some recommendation on using softmax and Dense layer with 2 neurons). Some or one of these helped with the overfitting, but nothing worked for the prediction problem.\r\n\r\n **The data is very balanced** (for training I have 32354 / 31681 for val I have 9092 / 9860 samples per class), so it doesn\'t make sense that it reaches 80% if it predicts the same labels for evaluation set as well.\r\n\r\nWhat is wrong with my model and how can I fix it? Is it the model, is it a bug or am I doing something wrong with predictions? Any comments are welcome.\r\n\r\n```\r\n#Import some packages to use\r\nimport cv2\r\nimport numpy as np\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nimport os\r\nfrom keras.regularizers import l2\r\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\r\nfrom keras.layers.core import Dense, Dropout, Flatten\r\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\r\nfrom keras.initializers import RandomNormal\r\n\r\nos.environ[""CUDA_VISIBLE_DEVICES""]=""0""\r\n\r\nepochs = 200\r\ncallbacks = []\r\n#schedule = None\r\ndecay = 0.0\r\n\r\nearlyStopping = EarlyStopping(monitor=\'val_loss\', patience=10, verbose=0, mode=\'min\')\r\nmcp_save = ModelCheckpoint(\'.mdl_wts.hdf5\', save_best_only=True, monitor=\'val_loss\', mode=\'min\')\r\nreduce_lr_loss = ReduceLROnPlateau(monitor=\'val_loss\', factor=0.1, patience=3, verbose=1, epsilon=1e-5, mode=\'min\')\r\n\r\ntrain_dir = \'/home/d/Desktop/s/data/train\'\r\neval_dir = \'/home/d/Desktop/s/data/eval\'\r\ntest_dir = \'/home/d/Desktop/s/data/test\'\r\n\r\n\r\n\r\n# create a data generator\r\ntrain_datagen = ImageDataGenerator(rescale=1./255,   #Scale the image between 0 and 1\r\n                                    rotation_range=40,\r\n                                    width_shift_range=0.2,\r\n                                    height_shift_range=0.2,\r\n                                    shear_range=0.2,\r\n                                    zoom_range=0.2,\r\n                                    horizontal_flip=True,)\r\n\r\nval_datagen = ImageDataGenerator(rescale=1./255)  #We do not augment validation data. we only perform rescale\r\n\r\ntest_datagen = ImageDataGenerator(rescale=1./255)  #We do not augment validation data. we only perform rescale\r\n\r\n# load and iterate training dataset\r\ntrain_generator = train_datagen.flow_from_directory(train_dir,  target_size=(224,224),class_mode=\'categorical\', batch_size=16, shuffle=\'True\', seed=42)\r\n# load and iterate validation dataset\r\nval_generator = val_datagen.flow_from_directory(eval_dir,  target_size=(224,224),class_mode=\'categorical\', batch_size=16, shuffle=\'True\', seed=42)\r\n# load and iterate test dataset\r\ntest_generator = test_datagen.flow_from_directory(test_dir,  target_size=(224,224), class_mode=None, batch_size=1, shuffle=\'False\', seed=42)\r\n#We will use a batch size of 32. Note: batch size should be a factor of 2.***4,8,16,32,64...***\r\n#batch_size = 4\r\n\r\n\r\n\r\n#from keras import layers\r\nfrom keras import models\r\nfrom keras import optimizers\r\n#from keras.layers import Dropout\r\n#from keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.preprocessing.image import img_to_array, load_img\r\n\r\nmodel = models.Sequential()\r\nmodel.add(Conv2D(64, (3, 3), activation=\'relu\', name=\'block1_conv1\', kernel_initializer=RandomNormal(\r\n        mean=0.0, stddev=0.05), bias_initializer=RandomNormal(mean=0.0, stddev=0.05), input_shape=(224, 224, 3)))\r\nmodel.add(Conv2D(64, (3, 3), activation=\'relu\', name=\'block1_conv2\', kernel_initializer=RandomNormal(\r\n        mean=0.0, stddev=0.05), bias_initializer=RandomNormal(mean=0.0, stddev=0.05)))\r\nmodel.add(MaxPooling2D((2, 2)))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(Conv2D(128, (3, 3), activation=\'relu\', name=\'block2_conv1\', kernel_initializer=RandomNormal(\r\n        mean=0.0, stddev=0.05), bias_initializer=RandomNormal(mean=0.0, stddev=0.05)))\r\nmodel.add(Conv2D(128, (3, 3), activation=\'relu\', name=\'block2_conv2\',kernel_initializer=RandomNormal(\r\n        mean=0.0, stddev=0.05), bias_initializer=RandomNormal(mean=0.0, stddev=0.05)))\r\nmodel.add(MaxPooling2D((2, 2), name=\'block2_pool\'))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(Conv2D(256, (3, 3), activation=\'relu\', name=\'block3_conv1\', kernel_initializer=RandomNormal(\r\n        mean=0.0, stddev=0.05), bias_initializer=RandomNormal(mean=0.0, stddev=0.05)))\r\nmodel.add(Conv2D(256, (3, 3), activation=\'relu\', name=\'block3_conv2\', kernel_initializer=RandomNormal(\r\n        mean=0.0, stddev=0.05), bias_initializer=RandomNormal(mean=0.0, stddev=0.05)))\r\nmodel.add(Conv2D(256, (3, 3), activation=\'relu\', name=\'block3_conv3\', kernel_initializer=RandomNormal(\r\n        mean=0.0, stddev=0.05), bias_initializer=RandomNormal(mean=0.0, stddev=0.05)))\r\nmodel.add(MaxPooling2D((2, 2), name=\'block3_pool\'))\r\nmodel.add(Dropout(0.2))\r\n#model.add(layers.Conv2D(512, (3, 3), activation=\'relu\', name=\'block4_conv1\'))\r\n#model.add(layers.Conv2D(512, (3, 3), activation=\'relu\', name=\'block4_conv2\'))\r\n#model.add(layers.Conv2D(512, (3, 3), activation=\'relu\', name=\'block4_conv3\'))\r\n#model.add(layers.MaxPooling2D((2, 2), name=\'block4_pool\'))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(256, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation=\'relu\', kernel_initializer=\'he_uniform\'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(2, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation=\'softmax\'))\r\n\r\n#Lets see our model\r\nmodel.summary()\r\n\r\n#We\'ll use the RMSprop optimizer with a learning rate of 0.0001\r\n#We\'ll use binary_crossentropy loss because its a binary classification\r\n#model.compile(loss=\'binary_crossentropy\', optimizer=optimizers.SGD(lr=1e-5, momentum=0.9), metrics=[\'acc\'])\r\nmodel.compile(loss=\'categorical_crossentropy\',\r\n                   #optimizer=optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=decay),\r\n                    optimizer=optimizers.SGD(lr= 0.0001, clipvalue = 0.5, decay=1e-6, momentum=0.9, nesterov=True),\r\n              metrics=[\'accuracy\'])\r\n\r\n\r\n\r\n#The training part\r\n#We train for 64 epochs with about 100 steps per epoch\r\nhistory = model.fit_generator(train_generator,\r\n                              steps_per_epoch=train_generator.n // train_generator.batch_size,\r\n                              epochs=epochs,\r\n                              validation_data=val_generator,\r\n                              validation_steps=val_generator.n // val_generator.batch_size,\r\n                              callbacks=[earlyStopping, mcp_save]) #, reduce_lr_loss])\r\n\r\n\r\n#Save the model\r\nmodel.save_weights(\'/home/d/Desktop/s/categorical_weights.h5\')\r\nmodel.save(\'/home/d/Desktop/s/categorical_model_keras.h5\')\r\n\r\n#lets plot the train and val curve\r\n#get the details form the history object\r\nacc = history.history[\'acc\']\r\nval_acc = history.history[\'val_acc\']\r\nloss = history.history[\'loss\']\r\nval_loss = history.history[\'val_loss\']\r\n\r\nepochs = range(1, len(acc) + 1)\r\n\r\n#Train and validation accuracy\r\nplt.plot(epochs, acc, \'b\', label=\'Training accuracy\')\r\nplt.plot(epochs, val_acc, \'r\', label=\'Validation accuracy\')\r\nplt.title(\'Training and Validation accuracy\')\r\nplt.legend()\r\n\r\nplt.figure()\r\n#Train and validation loss\r\nplt.plot(epochs, loss, \'b\', label=\'Training loss\')\r\nplt.plot(epochs, val_loss, \'r\', label=\'Validation loss\')\r\nplt.title(\'Training and Validation loss\')\r\nplt.legend()\r\n\r\nplt.show()\r\n\r\nmodel.evaluate_generator(generator=val_generator, steps=val_generator.n // val_generator.batch_size)\r\n\r\nSTEP_SIZE_TEST=test_generator.n//test_generator.batch_size\r\ntest_generator.reset()\r\npred=model.predict_generator(test_generator,\r\nsteps=STEP_SIZE_TEST,\r\nverbose=1)\r\n\r\npredicted_class_indices=np.argmax(pred,axis=1)\r\n\r\nlabels = (train_generator.class_indices)\r\nnp.save(\'/home/d/Desktop/s/classes\', labels)\r\n\r\nlabels = dict((v,k) for k,v in labels.items())\r\npredictions = [labels[k] for k in predicted_class_indices]\r\n\r\nfilenames=test_generator.filenames\r\nresults=pd.DataFrame({""Filename"":filenames,\r\n                      ""Predictions"":predictions})\r\nresults.to_csv(""categorical_results.csv"",index=False)\r\n```\r\n', 'I faced the same issue. I have a medium model size (400k parameters bi-GRU with attention). The model fluctuates in the first 5 epochs to a very low accuracy (epoch size 25k text docs). The model is also predicting the same value regardless of the input. Since the sixth epoch, it magically starts to learn magically and reaches a good accuracy and no longer predicts the same value. So I guess I just have to be more patient.', 'I also have the same problem,but i solve it by decreasing batch_size (batch_size = 1) and simplifing cnn structure .', 'Generally, the more complex the network, the more weights to the network\nhas to tune, so many of them become dead in the process, hence same output\n\nOn Mon, Dec 9, 2019 at 7:11 PM redbooks-jdc <notifications@github.com>\nwrote:\n\n> I also have the same problem,but i solve it by decreasing batch_size\n> (batch_size = 1) and simplifing cnn structure .\n>\n> —\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/keras-team/keras/issues/6447?email_source=notifications&email_token=AE2OFBN27WD2LMYHS7QKL5LQX4CG5A5CNFSM4DJRXEB2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEGMNYEQ#issuecomment-563665938>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AE2OFBNPS7E4VQBSLIG4JC3QX4CG5ANCNFSM4DJRXEBQ>\n> .\n>\n\n\n-- \nNsamba Taufeeq\nMobile 1: +256 751 830778\nMobile 2: +256 775 625741\nEmail:nsambataufeeq@gmail.com\n', '> After banging my head against the wall for over an hour with this same problem, and having tried all of the suggestions here without success, I eventually found the issue in my training data. I had an error in my normalization function and as a result, I had negative values in the training set. If none of the above suggestions work, this might be worth looking at. Cheers, Shane.\r\n\r\nI had the same problem and when i saw this i printed a csv of my data, and found NaN values, after fixing my data it worked properly. Thank you', ""For me, changing the activation function for my hidden layers from **tanh** to **relu**'s solved the problem. My guess is my issue was caused by some form of vanishing gradients."", ""> For me, changing the activation function for my hidden layers from **tanh** to **relu**'s solved the problem. My guess is my issue was caused by some form of vanishing gradients.\r\n\r\nyep this worked for me too"", 'I suspect it means there is likely no signal in your covariates (could be a mistake in your data prep) for it to use so it just defaults to optimizing one single output that minimizes error as much as possible.']","['\r\ndef read_lines(filename):\r\n\tlines = []\r\n\r\n\twith open(filename) as file:\r\n\t\tfor line in file:\r\n\t\t\tline = line.strip()\r\n\t\t\tlines.append(line)\r\n\r\n\treturn lines\r\n\r\n\r\n# read target survival probabilities and patient IDs\r\ntargets = np.loadtxt(""../survival/target_probs.txt"", delimiter="","")\r\n\r\nall_patient_ids = read_lines(""../survival/target_patient_ids.txt"")\r\n\r\n\r\n# read available patient IDs and variable names\r\npatient_ids = read_lines(""clinical_patient_ids.txt"")\r\nvar_names = read_lines(""clinical_var_names.txt"")\r\n\r\n# only use available cases\r\npt_idxs = [all_patient_ids.index(x) for x in patient_ids]\r\n\r\ntargets = targets[pt_idxs]\r\n\r\n# determine number of cases for 60/10/30 train/val/test split\r\nn_cases = len(patient_ids)\r\nn_train = int(round(n_cases*.6))\r\nn_val = int(round(n_cases*.1))\r\nn_test = int(round(n_cases*.3))\r\n\r\n\r\n# extract training, val, and test patient IDs\r\ntrain_patient_ids = patient_ids[:n_train]\r\nval_patient_ids = patient_ids[n_train:n_train+n_val]\r\ntest_patient_ids = patient_ids[n_train+n_val:]\r\n\r\nY_train = targets[:n_train]\r\nY_val = targets[n_train:n_train+n_val]\r\nY_test = targets[n_train+n_val:n_cases]\r\n\r\n# load data\r\ndata = np.loadtxt(""clinical_data.txt"", delimiter="","")\r\n\r\n# preprocess\r\nmin_max_scaler = preprocessing.MinMaxScaler()\r\n\r\nmin_max_scaler.fit(data)\r\n\r\ndata = min_max_scaler.transform(data)\r\n\r\n# set up  model architecture\r\nmodel = Sequential()\r\n\r\nmodel.add(Dense(32, activation=""relu"", input_dim=len(var_names),\r\n\tkernel_regularizer=regularizers.l2(0.01)))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(Dense(20, activation=""relu"", kernel_regularizer=regularizers.l2(0.01)))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(Dense(16, activation=""relu"", kernel_regularizer=regularizers.l2(0.01)))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(Dense(16, activation=""relu"", kernel_regularizer=regularizers.l2(0.01)))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(Dense(1, activation=""sigmoid"", kernel_regularizer=regularizers.l2(0.01)))\r\n\r\nX_train = data[:n_train]\r\nX_val = data[n_train:n_train+n_val]\r\nX_test = data[n_train+n_val:n_cases]\r\n\r\n# train on clinical data\r\nearly_stop = EarlyStopping(monitor=""val_loss"", patience=5)\r\n\r\nreduce_lr = ReduceLROnPlateau(monitor=""val_loss"", factor=0.2,\r\n\tpatience=2, min_lr=0.001)\r\n\r\nsgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\r\n\r\nmodel.compile(loss=""mse"", optimizer=""sgd"", metrics=[""mse""])\r\n\r\nhist = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), \r\n\tepochs=1000, batch_size=32, callbacks=[reduce_lr, early_stop])\r\n\r\n\r\npreds = model.predict(X_test)\r\n\r\nevals = model.evaluate(X_test, Y_test)\r\n\r\nprint(evals)\r\nprint(preds[0:10])\r\n\r\n# summarize history for loss\r\nplt.plot(hist.history[""loss""])\r\nplt.plot(hist.history[""val_loss""])\r\nplt.title(""model loss"")\r\nplt.ylabel(""loss"")\r\nplt.xlabel(""epoch"")\r\nplt.legend([""train"", ""val""], loc=""upper left"")\r\nplt.show()\r\n', '\r\n[[ 0.87765867]\r\n [ 0.87765765]\r\n [ 0.87766296]\r\n [ 0.87765878]\r\n [ 0.87765783]\r\n [ 0.87765902]\r\n [ 0.87765855]\r\n [ 0.87765938]\r\n [ 0.87766141]\r\n [ 0.87766016]]\r\n']","['[0.012566652174742577, 0.0076035054909729212]', 'model.predict()']",1,0
77,keras,4404,closed,too low accuracy in the example of mnist_cnn,"Hello,

I run the example of mnist_cnn by CPU on my win7. The result is very bad, with fixed test accuracy of 0.098 even after 12 epochs. 
I am new in keras, and want to know how to debug it.
Thx for any suggestion.

The packages have been updated to latest version:
Keras version: 1.1.1
Theano version: 0.8.2

Here is the log
Train on 60000 samples, validate on 10000 samples
Epoch 1/12
60000/60000 [==============================] - 5584s - loss: nan - acc: 0.0988 - val_loss: nan - val_acc: 0.0980
Epoch 2/12
60000/60000 [==============================] - 5578s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 3/12
60000/60000 [==============================] - 5577s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 4/12
60000/60000 [==============================] - 5556s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 5/12
60000/60000 [==============================] - 5559s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 6/12
60000/60000 [==============================] - 5549s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 7/12
60000/60000 [==============================] - 5550s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 8/12
60000/60000 [==============================] - 5546s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 9/12
60000/60000 [==============================] - 5548s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 10/12
60000/60000 [==============================] - 5550s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 11/12
60000/60000 [==============================] - 5549s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Epoch 12/12
60000/60000 [==============================] - 5565s - loss: nan - acc: 0.0987 - val_loss: nan - val_acc: 0.0980
Test score: nan
Test accuracy: 0.098",,"['loss is nan, so there are some errors on the data or so. full code?\n', 'just the example of mnist_cnn, without any modification.\n', ""really? that's strange. I just ran it, keras version is 1.1.0 and it works well. could you check it out on 1.1.0?\n"", 'I try again, with the same situation.\nBut cifar10_cnn seems working well:\nEpoch 1/200\n  800/50000 [..............................] - ETA: 18821s - loss: 2.3023 - acc: 0.1037\n', '@keunwoochoi\nwhich version of python do you use, 2 or 3?\nThere is a note in cifar10_cnn:\n""Note: the data was pickled with Python 2, and some encoding issues might prevent you\nfrom loading it in Python 3. You might have to load it in Python 2,\nsave it in a different format, load it in Python 3 and repickle it.""\n', 'Oh, I\'m using python2.7. That could be the reason. \n\n> On 17 Nov 2016, at 02:15, kingaza notifications@github.com wrote:\n> \n> @keunwoochoi\n> which version of python do you use, 2 or 3?\n> There is a note in cifar10_cnn:\n> ""Note: the data was pickled with Python 2, and some encoding issues might prevent you\n> from loading it in Python 3. You might have to load it in Python 2,\n> save it in a different format, load it in Python 3 and repickle it.""\n> \n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n', 'same situation in python 2.7\nand i check the train and test data in python 3.5, which are all right. So it is not the problem of pickle data parsing.\nStill asking for help~~~\n', 'no problem any more, since I move to tensorflow as backend.', 'You should use Theano dev version with keras, not Theano 0.8.2. We released\nTheano 0.9rc1 this week.\n\nOn Wed, Feb 22, 2017 at 10:32 PM kingaza <notifications@github.com> wrote:\n\n> no problem any more, since I move to tensorflow as backend.\n>\n> —\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/fchollet/keras/issues/4404#issuecomment-281884302>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AALC-_wgK4cERPHzvr-KiOk28OhvAvs6ks5rfP3agaJpZM4K0z3r>\n> .\n>\n']",[],[],1,0
78,keras,13621,closed,Using output as a separate acitvation layer causing very poor accuracy.,"Following are the last two lines of my model:


If I use predictions as the model output, the model does not learn anything, but if I remove the predictions layer and add the activations parameter to outputs Dense layer, it works fine. ",,"[""@Ehsan1997 `predictions` here is just an object of Activation class.\r\n\r\nI would suggest you  to build model like this:\r\n```\r\nmodel = Sequential()\r\nmodel.add(Dense(512, activation='relu')\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(256, activation='relu'))\r\nmodel.add(Dropout(0.25))\r\nmodel.add(Dense(10, activation='softmax'))\r\n```\r\nIn this way you will not get confused. You can also visualize your model using this code to cross-check.\r\n```\r\nfrom keras.utils import plot_model\r\nplot_model(model, to_file='model.png')\r\n```\r\n \r\n\r\n"", ""> @Ehsan1997 `predictions` here is just an object of Activation class.\r\n> \r\n> I would suggest you to build model like this:\r\n> \r\n> ```\r\n> model = Sequential()\r\n> model.add(Dense(512, activation='relu')\r\n> model.add(Dropout(0.5))\r\n> model.add(Dense(256, activation='relu'))\r\n> model.add(Dropout(0.25))\r\n> model.add(Dense(10, activation='softmax'))\r\n> ```\r\n> \r\n> In this way you will not get confused. You can also visualize your model using this code to cross-check.\r\n> \r\n> ```\r\n> from keras.utils import plot_model\r\n> plot_model(model, to_file='model.png')\r\n> ```\r\n\r\nCould you please explain why isn't it part of the network??"", ""Okay it seems as if there was some problem with colab's version of keras, I just changed it to tensorflow.keras and now everything seems fine.""]","[""\r\noutputs = Dense(4)(act_fc1)\r\npredictions = Activation(activation='softmax')(outputs)\r\n""]",[],1,0
79,keras,3894,closed,"The training accuracy is very high, while the validation accuracy is very low?","I got a strange question. I train a two layers CNN using .flow_from_directory(), the training accuracy is very high, while the validation accuracy is very low. following is my code ,very simple.
from keras.preprocessing.image import ImageDataGenerator

from keras.models import Sequential
from keras.layers import Convolution2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras.callbacks import EarlyStopping

model=Sequential()
model.add(Convolution2D(32, 5,5, input_shape=(3,28,28)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Convolution2D(32, 3,3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Flatten())
model.add(Dense(128))
model.add(Activation('relu'))
model.add(Dropout(0.5))

model.add(Dense(10))
model.add(Activation('softmax'))

train_datagen=ImageDataGenerator(rescale=1./255,
                                 shear_range=0.2,
                                 zoom_range=0.2,
                                 horizontal_flip=True)
test_datagen=ImageDataGenerator(rescale=1./255)

train_generator=train_datagen.flow_from_directory(
    r'C:\Users\zhx\Desktop\mnist\train',
    target_size=(28,28),
    classes=['0','1','2','3','4','5','6','7','8','9'],
    batch_size=60,
    class_mode='categorical',
    shuffle=True)

validation_generator=test_datagen.flow_from_directory(
    r'C:\Users\zhx\Desktop\mnist\test',
    target_size=(28, 28),
    classes=['0','1','2','3','4','5','6','7','8','9'],
    batch_size=100,
    class_mode='categorical',
    shuffle=True)

model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
early_stopping =EarlyStopping(monitor='val_loss', patience=2)
model.fit_generator(train_generator, samples_per_epoch=60000, nb_epoch=10, validation_data=validation_generator, callbacks=[early_stopping],nb_val_samples=10000)

json_string=model.to_json()
open(r'C:\Users\zhx\Desktop\mnistmodel\mnistcnn_arc.json','w').write(json_string)
model.save_weights(r'C:\Users\zhx\Desktop\mnistmodel\mnistcnn_weights.h5')
score=model.evaluate_generator(validation_generator, 10000)

print('Test score:', score[0])
print('Test accuracy:', score[1])

Here is the log(only the last )

Using Theano backend.
Found 60000 images belonging to 10 classes.
Found 10000 images belonging to 10 classes.
Epoch 1/10

   60/60000 [..............................] - ETA: 812s - loss: 2.2756 - acc: 0.1667
  120/60000 [..............................] - ETA: 690s - loss: 2.2897 - acc: 0.1667
  180/60000 [..............................] - ETA: 688s - loss: 2.2728 - acc: 0.1833
  240/60000 [..............................] - ETA: 721s - loss: 2.2692 - acc: 0.1792
  300/60000 [..............................] - ETA: 768s - loss: 2.2633 - acc: 0.1800
  360/60000 [..............................] - ETA: 833s - loss: 2.2561 - acc: 0.1750
  420/60000 [..............................] - ETA: 884s - loss: 2.2437 - acc: 0.1786
Warning (from warnings module):
  File ""C:\WinPython-64bit-3.4.4.4Qt5\python-3.4.4.amd64\lib\site-packages\keras\callbacks.py"", line 67
    % delta_t_median)
UserWarning: Method on_batch_end() is slow compared to the batch update (0.484438). Check your callbacks.

  480/60000 [..............................] - ETA: 1002s - loss: 2.2285 - acc: 0.2042
  540/60000 [..............................] - ETA: 978s - loss: 2.2102 - acc: 0.2241 
  600/60000 [..............................] - ETA: 979s - loss: 2.1920 - acc: 0.2317
  660/60000 [..............................] - ETA: 1013s - loss: 2.1655 - acc: 0.2439
Warning (from warnings module):
  File ""C:\WinPython-64bit-3.4.4.4Qt5\python-3.4.4.amd64\lib\site-packages\keras\callbacks.py"", line 67
    % delta_t_median)
UserWarning: Method on_batch_end() is slow compared to the batch update (0.578522). Check your callbacks.

  720/60000 [..............................] - ETA: 1057s - loss: 2.1447 - acc: 0.2514
  780/60000 [..............................] - ETA: 1038s - loss: 2.1188 - acc: 0.2577
  840/60000 [..............................] - ETA: 1029s - loss: 2.1058 - acc: 0.2631

.........

59400/60000 [============================>.] - ETA: 11s - loss: 0.1535 - acc: 0.9568
59460/60000 [============================>.] - ETA: 10s - loss: 0.1537 - acc: 0.9567
59520/60000 [============================>.] - ETA: 9s - loss: 0.1537 - acc: 0.9568 
Warning (from warnings module):
  File ""C:\WinPython-64bit-3.4.4.4Qt5\python-3.4.4.amd64\lib\site-packages\keras\callbacks.py"", line 67
    % delta_t_median)
UserWarning: Method on_batch_end() is slow compared to the batch update (0.705150). Check your callbacks.

59580/60000 [============================>.] - ETA: 8s - loss: 0.1536 - acc: 0.9567
Warning (from warnings module):
  File ""C:\WinPython-64bit-3.4.4.4Qt5\python-3.4.4.amd64\lib\site-packages\keras\callbacks.py"", line 67
    % delta_t_median)
UserWarning: Method on_batch_end() is slow compared to the batch update (0.606548). Check your callbacks.

59640/60000 [============================>.] - ETA: 6s - loss: 0.1536 - acc: 0.9567
59700/60000 [============================>.] - ETA: 5s - loss: 0.1535 - acc: 0.9568
59760/60000 [============================>.] - ETA: 4s - loss: 0.1535 - acc: 0.9568
59820/60000 [============================>.] - ETA: 3s - loss: 0.1535 - acc: 0.9568
Warning (from warnings module):
  File ""C:\WinPython-64bit-3.4.4.4Qt5\python-3.4.4.amd64\lib\site-packages\keras\callbacks.py"", line 67
    % delta_t_median)
UserWarning: Method on_batch_end() is slow compared to the batch update (0.461924). Check your callbacks.

59880/60000 [============================>.] - ETA: 2s - loss: 0.1535 - acc: 0.9567
59940/60000 [============================>.] - ETA: 1s - loss: 0.1535 - acc: 0.9567
60000/60000 [==============================] - 1189s - loss: 0.1535 - acc: 0.9567 - val_loss: 8.0909 - val_acc: 0.1783
Test score: 8.09085823536
Test accuracy: 0.178299999908

Can any one give me a suggestion? thanks.
",stale,"['Please wrap your code with ``` for readability.\nYour model is overfitting and there can be too many reasons. One of them is because your dataset is too small compared to the model capacity. \n', ""@keunwoochoi \nI ran it again, but the val_acc was still very low.\nThe dataset is mnist, I used 60000 training pictures and 10000 validation pictures.\nI used the same model as the example mnist-cnn.py in keras, the only difference is the data input method. what is wrong with my code or my data? \nHere is the log:\nFound 60000 images belonging to 10 classes.\nFound 10000 images belonging to 10 classes.\nEpoch 1/10\n60000/60000 [==============================] - 154s - loss: 0.0226 - acc: 0.8690 - val_loss: 0.1159 - val_acc: 0.1716\nEpoch 2/10\n60000/60000 [==============================] - 152s - loss: 0.0085 - acc: 0.9559 - val_loss: 0.1242 - val_acc: 0.1542\nEpoch 3/10\n60000/60000 [==============================] - 154s - loss: 0.0063 - acc: 0.9673 - val_loss: 0.1230 - val_acc: 0.1543\nTest score: 0.122995681057\nTest accuracy: 0.154300000757\n\nHere is my code:\nfrom keras.preprocessing.image import ImageDataGenerator\n\nfrom keras.models import Sequential\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras.callbacks import EarlyStopping\n#from keras.optimizers import SGD\nmodel=Sequential()\nmodel.add(Convolution2D(32, 5,5, border_mode='valid',input_shape=(1,28,28)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))#model.add(Dropout(0.5))\nmodel.add(Convolution2D(32, 3,3))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(10))\nmodel.add(Activation('sigmoid'))\n\ntrain_datagen=ImageDataGenerator(rescale=1./255)\ntest_datagen=ImageDataGenerator(rescale=1./255)\n\ntrain_generator=train_datagen.flow_from_directory(\n    r'E:\\zhx\\mnist\\train',\n    target_size=(28,28),\n    classes=['0','1','2','3','4','5','6','7','8','9'],\n    color_mode='grayscale',\n    batch_size=10,\n    class_mode='categorical',\n    shuffle=True)\n\nvalidation_generator=test_datagen.flow_from_directory(\n    r'E:\\zhx\\mnist\\test',\n    target_size=(28, 28),\n   classes=['0','1','2','3','4','5','6','7','8','9'],\n    batch_size=100,\n    color_mode='grayscale',\n    class_mode='categorical',\n    shuffle=False)\n\nmodel.compile(loss='mean_squared_error', optimizer='adadelta', metrics=['accuracy'])\nearly_stopping =EarlyStopping(monitor='val_loss', patience=1)\nmodel.fit_generator(train_generator, samples_per_epoch=60000, nb_epoch=10, validation_data=validation_generator, callbacks=[early_stopping],nb_val_samples=10000)\n\njson_string=model.to_json()\nopen(r'E:\\zhx\\mnist\\model\\mnistdir2.json','w').write(json_string)\nmodel.save_weights(r'E:\\zhx\\mnist\\model\\mnistdir2_weight.h5')\n\nscore=model.evaluate_generator(validation_generator, 10000)\n\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n"", 'Hello! Have you solved the problem? I have met the same problem. I got very high training accuracy and very low validation accuracy. ', 'I changed all the activation relu functions to sigmoid one and solved the overfitting problem. I still do not know what the real problem. You can try it as well. May it help you. @baorave ', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', 'Hello, please check if you have shuffled the data before training. Because the validation splitting in keras is performed before shuffle, so maybe you have chosen an unbalanced dataset as your validation set, thus you got the low accuracy.\r\n', '@ChenWentai thanks. It helped.', '@ChenWentai \r\nI met the same issue.\r\ni shuffled the dataset,then split the dataset into train_data,val_data.\r\nand the dataset are unbalanced.\r\ndo you give some advices. thanks', 'did you try to increase batch_size? example 50 - 100 - 200.\r\nand disable rescale=1./255 on ImageDataGenerator.', 'How to shuffle the data. As in I have used validation_split = 0.2 but it simply means that the last 20% of data is used for validation. Instead of that, can I give some arguments so that the data shuffles before splitting?\r\n\r\nI am using fit_generator and ImageDataGenerator for training', '@ChenWentai thank you!. I spend a lot of augmentation and other tricks when the problem was so easy)', 'Hey there. How do I shuffle the data to train my CNN? I have split the training and testing pictures in two folders inside my directory. ', '> Hey there. How do I shuffle the data to train my CNN? I have split the training and testing pictures in two folders inside my directory.\r\n\r\nHi ShikharGhimire,\r\n\r\nThe functions in keras(evaluate, fit etc) do have an argument called ""Shuffle"" Set the Booloean value to true and check if that works. It worked for me.\r\n\r\nInform the updates regarding this\r\nThank you\r\nSrinath', '> @ChenWentai\r\n> I met the same issue.\r\n> i shuffled the dataset,then split the dataset into train_data,val_data.\r\n> and the dataset are unbalanced.\r\n> do you give some advices. thanks\r\n\r\nSir can u help me out i am facing the same problem of validation accuracy being less and testing accuracy being very high']",[],[],1,0
80,keras,11070,closed,Accuracy oscillates between ~0% and ~70% when creating new models,"Hi all,

**I've found that the problem doesn't occur when TensorFlow is forced to use the CPU** -- (I think) this implies that it's a TensorFlow bug and not a Keras bug, so maybe this issue can be closed.

First off, I'm using the Keras that's distributed with TensorFlow 1.10.0 so let me know if I should have opened the issue on their repo instead.

I'm using a sequence-to-sequence model based on the [Keras blogpost](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html) which I've wrapped into a fairly complicated object (although the issue also occurs with a simplified version linked below). When I create a new model (which I have to do for gridsearch and for clearing the TF session when the graph gets too big and slows down training) it starts with accuracy of either 0% or 70%.

Here are a pair of screenshots that show what I mean:
 * Good: https://i.imgur.com/7mT5Siv.png
 * Bad: https://i.imgur.com/MZ3NdCB.png

You can see that in the first screenshot, the accuracy is low but trending upwards. In the second, the accuracy of two models starts at 70% and doesn't increase (another model starts at 3% and also doesn't increase).

This happens whether I create new, blank models or load pretrained weights into new models with .

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [X] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [X] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

Here is a minimal Gist which reproduces the problem: https://gist.github.com/ChrisSwinchatt/97304761e9f875dfd34e3339891a5475",,[],[],['model.load_weights()'],1,0
81,keras,6470,closed,"Keras 2.0 is too slow, even with theano 0.9","Guys I've upgraded my keras and theano to keras 2.0 and theano 9.0. I've tested GPU and it works fine, the test script is ok. I've also installed cuDNN 5110 and it works fine too.,but the script runs too slowly. I've even used some configuration in theano to make the performance better:Forced to use dnn, to use float32 (warn if float is 64), made some optimizations, but the learning process goes too long anyway. I had a CNN with about 16 layers (2 FC layers).**With keras 1, the 1 epoch took ~1m, but with keras2.0 it takes ~3.24m**.Is there any idea how I can make the speed better? almost all tests related with theano and gpu run without any problem.",,"['Check that `image_data_format` is `channels_first`', 'I have also noticed that my code with Keras 2 is much more slow', 'everything is ok with ""image_data_format"", cause I had an issue, the change the channels and it works.If I\'m not mistaken image_data_format by default is channel_last.I noticed the speed even during the first run (after upgrade).I run my model and it worked so slowly.', '`channels_last` will work but usually run more slowly under theano.', 'so you suggest to use ""channel_first""? I used this to, but it\'s the same.', 'Yes, use `channels_first` with Theano, `channels_last` with Tensorflow for best performance. ', ""And of course make sure the input dimensions match the `image_data_format`, i.e. (3,32,32) for `channels_first`. Things will often run even if it doesn't, but won't do what you want."", 'ok. Will play a little bit with channels and let you know what happened.', 'thank you @the-moliver .The channel_first solved the problem.now it takes ~1m again.Of course it would be great, if it took ~0.8m for new version, but anyway. this is not bad.', '@Hazarapet Could you please share how you solved this problem?']",[],[],1,0
82,keras,9543,closed,cifar10_resnet.py Multi-gpu is slower than mono gpu ,"Hi everyone,

On a personal project, the bi-gpu was slower than mono-gpu. Therefore, I wanted to check if the problem was my project or something else. Using the cifar10_resnet.py model, I just added

Just before the model compilation annnnnnd:

1 GPU, 1 epoch = 27 sec
2 GPUs, 1 epoch = 32 sec

Any ideas ?

Thanks you !
",,['You can try `version = 2` in this code since the new model has a better scalability.'],[],"['model = multi_gpu_model(model, gpus=2)']",1,0
83,keras,5960,closed,lack of consistency in Keras Model,"Hi guys,
I have executed an LSTM in my dataset for about 7 epochs.

       inputs = Input(shape = (40,) , dtype = 'int32')
        Embs = Embedding(19449, embedding_vecor_length, input_length=40, weights = [w1],mask_zero=True,trainable=False)
        Embsyo = Embs(inputs)
         lstm1 = LSTM(300,activation='tanh', recurrent_activation='sigmoid')(Embsyo)
         out = Dense(5, activation = 'softmax')(lstm1)
         model = Model(input = inputs, output = out)
         model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])

However, each time I run the model, I get a different accuracy on my test dataset. Like one time I get 32% and the next time I get 26%. I keep the epochs fixed at 7 and even removed dropouts to avoid randomness. Is there any way I can get Keras to give me the same or atleast very similar accuracy percentage each time I run the code? Thanks in advance ",stale,"['if you use a fixed seed, in order to initialize all the layers to the same value, would you get the same accuracy oscillations or not? To understand if it is because of random layer initialization or because of some bugs.', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],1,0
84,keras,10063,closed,accuracy and loss are not changing ,"I want to use VGG in my code. But accuracy and loss are not changing. How can i  solve this problem?
My dataset is medical image(Predicting IDC or non IDC) 
Here is my code. 
-------------------------------------



And result is here, 
Epoch 1/8
1299/1298 [==============================] - 27s 21ms/step - loss: 0.5580 - acc: 0.7216 - val_loss: 0.5227 - val_acc: 0.7386
Epoch 2/8
1299/1298 [==============================] - 27s 21ms/step - loss: 0.5260 - acc: 0.7466 - val_loss: 0.5321 - val_acc: 0.7298
Epoch 3/8
1299/1298 [==============================] - 27s 21ms/step - loss: 0.5175 - acc: 0.7512 - val_loss: 0.5170 - val_acc: 0.7412
Epoch 4/8
1299/1298 [==============================] - 27s 21ms/step - loss: 0.5166 - acc: 0.7556 - val_loss: 0.5086 - val_acc: 0.7528
Epoch 5/8
1299/1298 [==============================] - 27s 21ms/step - loss: 0.5141 - acc: 0.7562 - val_loss: 0.5017 - val_acc: 0.7572
Epoch 6/8
1299/1298 [==============================] - 27s 21ms/step - loss: 0.5119 - acc: 0.7602 - val_loss: 0.5061 - val_acc: 0.7515
Epoch 7/8
1299/1298 [==============================] - 27s 21ms/step - loss: 0.5090 - acc: 0.7591 - val_loss: 0.4999 - val_acc: 0.7611
Epoch 8/8
1299/1298 [==============================] - 27s 21ms/step - loss: 0.5100 - acc: 0.7624 - val_loss: 0.5043 - val_acc: 0.7539

Keras CNN #1C - accuracy: 0.7538994800234126 




Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",,[],"['\r\nimport pandas as pd\r\nimport numpy as np\r\nimport os\r\nfrom glob import glob\r\nimport itertools\r\nimport fnmatch\r\nimport random\r\nimport matplotlib.pylab as plt\r\nimport seaborn as sns\r\nimport cv2\r\nfrom scipy.misc import imresize, imread\r\nimport sklearn\r\nfrom sklearn import model_selection\r\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold, learning_curve, GridSearchCV\r\nfrom sklearn.metrics import confusion_matrix, make_scorer, accuracy_score\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.tree import DecisionTreeClassifier\r\nfrom sklearn.neighbors import KNeighborsClassifier\r\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\r\nfrom sklearn.naive_bayes import GaussianNB\r\nfrom sklearn.svm import SVC, LinearSVC\r\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\r\nimport keras\r\nfrom keras import backend as K\r\nfrom keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.utils.np_utils import to_categorical\r\nfrom keras.models import Sequential, model_from_json\r\nfrom keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta\r\nfrom keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPool2D, MaxPooling2D\r\n%matplotlib inline\r\n\r\nimagePatches = glob(\'../daeun/kaggle/input/IDC_regular_ps50_idx5/**/*.png\', recursive=True)\r\n\r\nimage_name = ""../work/kaggle/input/IDC_regular_ps50_idx5/9135/1/9135_idx5_x1701_y1851_class1.png"" #Image to be used as query\r\n\r\npatternZero = \'*class0.png\'\r\npatternOne = \'*class1.png\'\r\n\r\nclassZero = fnmatch.filter(imagePatches, patternZero)\r\nclassOne = fnmatch.filter(imagePatches, patternOne)\r\n\r\nprint(""IDC(-)\\n\\n"",classZero[0:5],\'\\n\')\r\nprint(""IDC(+)\\n\\n"",classOne[0:5])\r\n\r\ndef proc_images(lowerIndex,upperIndex):\r\n    x = []\r\n    y = []\r\n    WIDTH = 50\r\n    HEIGHT = 50\r\n    for img in imagePatches[lowerIndex:upperIndex]:\r\n        full_size_image = cv2.imread(img) \r\n        x.append(cv2.resize(full_size_image, (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC))\r\n        if img in classZero:\r\n            y.append(0)\r\n        elif img in classOne:\r\n            y.append(1)\r\n        else:\r\n            return\r\n    return x,y\r\n\r\nX,Y = proc_images(0,90000)\r\ndf = pd.DataFrame()\r\ndf[""images""]=X\r\ndf[""labels""]=Y\r\nX2=df[""images""]\r\nY2=df[""labels""]\r\nX2=np.array(X2)\r\nimgs0=[]\r\nimgs1=[]\r\nimgs0 = X2[Y2==0] # (0 = no IDC, 1 = IDC)\r\nimgs1 = X2[Y2==1]\r\n\r\ndict_characters = {0: \'IDC(-)\', 1: \'IDC(+)\'}\r\n\r\nX=np.array(X)\r\nX=X/255.0\r\n\r\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\r\n\r\nX_train = X_train[0:300000] \r\nY_train = Y_train[0:300000]\r\nX_test = X_test[0:300000] \r\nY_test = Y_test[0:300000]\r\n\r\nY_trainHot = to_categorical(Y_train, num_classes = 2)\r\nY_testHot = to_categorical(Y_test, num_classes = 2)\r\n\r\nlab = df[\'labels\']\r\ndist = lab.value_counts()\r\nsns.countplot(lab)\r\nprint(dict_characters)\r\n\r\nX_trainShape = X_train.shape[1]*X_train.shape[2]*X_train.shape[3]\r\nX_testShape = X_test.shape[1]*X_test.shape[2]*X_test.shape[3]\r\nX_trainFlat = X_train.reshape(X_train.shape[0], X_trainShape)\r\nX_testFlat = X_test.reshape(X_test.shape[0], X_testShape)\r\n\r\nfrom imblearn.over_sampling import RandomOverSampler\r\nfrom imblearn.under_sampling import RandomUnderSampler\r\nros = RandomUnderSampler(ratio=\'auto\')\r\nX_trainRos, Y_trainRos = ros.fit_sample(X_trainFlat, Y_train)\r\nX_testRos, Y_testRos = ros.fit_sample(X_testFlat, Y_test)\r\n\r\nY_trainRosHot = to_categorical(Y_trainRos, num_classes = 2)\r\nY_testRosHot = to_categorical(Y_testRos, num_classes = 2)\r\n\r\nfor i in range(len(X_trainRos)):\r\n    height, width, channels = 50,50,3\r\n    X_trainRosReshaped = X_trainRos.reshape(len(X_trainRos),height,width,channels)\r\n\r\nfor i in range(len(X_testRos)):\r\n    height, width, channels = 50,50,3\r\n    X_testRosReshaped = X_testRos.reshape(len(X_testRos),height,width,channels)\r\n\r\ndfRos = pd.DataFrame()\r\ndfRos[""labels""]=Y_trainRos\r\nlabRos = dfRos[\'labels\']\r\ndistRos = lab.value_counts()\r\nsns.countplot(labRos)\r\nprint(dict_characters)\r\n\r\ndef runKerasCNNAugment(a,b,c,d,e,f):\r\n    batch_size = 128\r\n    num_classes = 2\r\n    epochs = 8\r\n    img_rows,img_cols=50,50\r\n    input_shape = (img_rows, img_cols, 3)\r\n    base_model = VGG19(weights = \'imagenet\', include_top=False, input_shape=(img_rows, img_cols, 3))\r\n    xx = base_model.output\r\n    xx = Flatten()(xx)\r\n    predictions = Dense(num_classes, activation=\'softmax\')(xx)\r\n    model = Model(inputs=base_model.input, outputs=predictions)\r\n    for layer in base_model.layers:\r\n        layer.trainable = False\r\n    model.compile(loss=keras.losses.categorical_crossentropy,\r\n                  optimizer=\'adam\',\r\n                  metrics=[\'accuracy\'])\r\n    callbacks_list = [keras.callbacks.EarlyStopping(monitor=\'val_acc\', patience=3, verbose=1)]\r\n    datagen = ImageDataGenerator(\r\n        featurewise_center=False,  # set input mean to 0 over the dataset\r\n        samplewise_center=False,  # set each sample mean to 0\r\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\r\n        samplewise_std_normalization=False,  # divide each input by its std\r\n        zca_whitening=False,  # apply ZCA whitening\r\n        rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\r\n        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\r\n        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\r\n        horizontal_flip=True,  # randomly flip images\r\n        vertical_flip=True)  # randomly flip images\r\n    history = model.fit_generator(datagen.flow(a,b, batch_size=32),\r\n                        steps_per_epoch=len(a) / 32, epochs=epochs,class_weight=f, validation_data = [c, d],callbacks = [MetricsCheckpoint(\'logs\')])\r\n    score = model.evaluate(c,d, verbose=0)\r\n    print(\'\\nKeras CNN #1C - accuracy:\', score[1],\'\\n\')\r\n    y_pred = model.predict(c)\r\n    map_characters = {0: \'IDC(-)\', 1: \'IDC(+)\'}\r\n    print(\'\\n\', sklearn.metrics.classification_report(np.where(d > 0)[1], np.argmax(y_pred, axis=1), target_names=list(map_characters.values())), sep=\'\')    \r\n    Y_pred_classes = np.argmax(y_pred,axis=1) \r\n    Y_true = np.argmax(d,axis=1) \r\n\r\nrunKerasCNNAugment(X_trainRosReshaped, Y_trainRosHot, X_testRosReshaped, Y_testRosHot,2,class_weight2)\r\n\r\n']",['--------------------------------------------------------------------------------------------------------------'],1,0
85,keras,13136,closed,"Training accuracy increases, val accuracy stays the same","I am trying to train a CNN using frames that portray me shooting a ball through a basket. And my aim is for the network to be able to classify the result( hit or miss) correctly. When I train the network, the training accuracy increases slowly until it reaches 100%, while the validation accuracy remains around 65% (It is important to mention here that 65% is the percentage of shots that have a Miss label)   Does anyone have experience with a similar problem?
I am using PyTorch and Resnet18 ( have tried other architectures as well but they all gave the same result). My frames are jpg images of sie 224.
As an optimizer, both Adam and SGD gave the same result 
Thank you in advance ",type:support,['@WassimOrabi Can you please post this issue in a pytorch support related group or stack overflow. Also you can refer to a [similar issue](https://datascience.stackexchange.com/questions/13607/validation-loss-and-accuracy-remain-constant) here. Thanks!'],[],[],1,0
86,keras,12625,closed,Clearing GPU memory in Keras,"
80% my GPU memory get's full after loading pre-trained Xception model. but after deleting my model , memory doesn't get empty or flush.
I've also used codes like : K.clear_session() , gc.collect() , tf.reset_default_graph() , del model but none of them worked. Gpu properties say's 85% of memory is full. 

Nothing flush gpu memory except numba.cuda.close() but won't allow me to use my gpu again. The only way to clear it is restarting kernel and rerun my code.

I'm looking for any script code to add my code allow me to use my code in for loop and clear gpu in every loop.

Part of my code :

image_input = Input(shape=(224, 224, 3))
base_model = Xception(input_tensor=image_input, include_top=False,weights='imagenet')
base_model.compile(loss='categorical_crossentropy',optimizer='adadelta',metrics=['accuracy'])
hist = base_model.fit(X,Y,epochs=2)

System information

    Have I written custom code :
    Windows 10 64-bit
    TensorFlow installed from conda install tensorflow-gpu
    TensorFlow version: 1.3
    Python version: 3.6
    CUDA/cuDNN version: 9.2
    GPU model and memory: Asus GTX 1060 6gb
",type:support,"['This function looks promising, stolen from [fastai forums](https://forums.fast.ai/t/how-could-i-release-gpu-memory-of-keras/2023/18)\r\n```python\r\nfrom keras.backend.tensorflow_backend import set_session\r\nfrom keras.backend.tensorflow_backend import clear_session\r\nfrom keras.backend.tensorflow_backend import get_session\r\nimport tensorflow\r\n\r\n# Reset Keras Session\r\ndef reset_keras():\r\n    sess = get_session()\r\n    clear_session()\r\n    sess.close()\r\n    sess = get_session()\r\n\r\n    try:\r\n        del classifier # this is from global space - change this as you need\r\n    except:\r\n        pass\r\n\r\n    print(gc.collect()) # if it\'s done something you should see a number being outputted\r\n\r\n    # use the same config as you used to create the session\r\n    config = tensorflow.ConfigProto()\r\n    config.gpu_options.per_process_gpu_memory_fraction = 1\r\n    config.gpu_options.visible_device_list = ""0""\r\n    set_session(tensorflow.Session(config=config))\r\n```', '@nateraw \r\nThank you very much !!!\r\nThe only script that works to rerun training session without restarting.\r\nyou helped me not to run every single time of rerun and now i can use for loop to train multiple times.', '> \r\n> \r\n> This function looks promising, stolen from [fastai forums](https://forums.fast.ai/t/how-could-i-release-gpu-memory-of-keras/2023/18)\r\n> \r\n> ```python\r\n> from keras.backend.tensorflow_backend import set_session\r\n> from keras.backend.tensorflow_backend import clear_session\r\n> from keras.backend.tensorflow_backend import get_session\r\n> import tensorflow\r\n> \r\n> # Reset Keras Session\r\n> def reset_keras():\r\n>     sess = get_session()\r\n>     clear_session()\r\n>     sess.close()\r\n>     sess = get_session()\r\n> \r\n>     try:\r\n>         del classifier # this is from global space - change this as you need\r\n>     except:\r\n>         pass\r\n> \r\n>     print(gc.collect()) # if it\'s done something you should see a number being outputted\r\n> \r\n>     # use the same config as you used to create the session\r\n>     config = tensorflow.ConfigProto()\r\n>     config.gpu_options.per_process_gpu_memory_fraction = 1\r\n>     config.gpu_options.visible_device_list = ""0""\r\n>     set_session(tensorflow.Session(config=config))\r\n> ```\r\n\r\nI keep getting ""CUDA_ERROR_OUT_OF_MEMORY"" when running the above function.\r\nThe only thing that clears up my memory is restarting my computer. ', '> This function looks promising, stolen from [fastai forums](https://forums.fast.ai/t/how-could-i-release-gpu-memory-of-keras/2023/18)\r\n> \r\n> ```python\r\n> from keras.backend.tensorflow_backend import set_session\r\n> from keras.backend.tensorflow_backend import clear_session\r\n> from keras.backend.tensorflow_backend import get_session\r\n> import tensorflow\r\n> \r\n> # Reset Keras Session\r\n> def reset_keras():\r\n>     sess = get_session()\r\n>     clear_session()\r\n>     sess.close()\r\n>     sess = get_session()\r\n> \r\n>     try:\r\n>         del classifier # this is from global space - change this as you need\r\n>     except:\r\n>         pass\r\n> \r\n>     print(gc.collect()) # if it\'s done something you should see a number being outputted\r\n> \r\n>     # use the same config as you used to create the session\r\n>     config = tensorflow.ConfigProto()\r\n>     config.gpu_options.per_process_gpu_memory_fraction = 1\r\n>     config.gpu_options.visible_device_list = ""0""\r\n>     set_session(tensorflow.Session(config=config))\r\n> ```\r\n\r\nThanks! Works perfect!!', ""running the above reset_keras() function still throws an OOM error on Ubuntu with 16GB of GPU memory.\r\n\r\n> InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory\r\n\r\nTensorFlow installed from conda install tensorflow-gpu\r\nTensorFlow version: 1.14\r\nPython version: 3.6\r\nCUDA/cuDNN version: 10.0.168\r\nGPU model and memory: Tesla V100-PCIE-16GB 16gb\r\n\r\n\r\nSame when I try running:\r\n```\r\n#from keras import backend as K\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import backend as K\r\ncurr_session = tf.get_default_session()\r\n# close current session\r\nif curr_session is not None:\r\n    curr_session.close()\r\n# reset graph\r\nK.clear_session()\r\n# create new session\r\ns = tf.InteractiveSession()\r\nK.set_session(s)\r\n```\r\n\r\nI find it fascinating that the TensorFlow team has not made a very straightforward way to clear GPU memory from a session.  So much is broken with TF.  Little annoyances like this; a user reasonably expects TF to handle clearing CUDA memory or have memory leaks, yet there appears no explicit way to handle this.  Even `K.clear_session()` doesn't work.  This is not unreasonable.  Maybe the blame should be directed towards nvidia, as even the following code doesn't clear the memory:\r\n\r\n```\r\nfrom numba import cuda\r\ncuda.select_device(0)\r\ncuda.close()\r\n```\r\n\r\nAfter several hours of scouring StackOverflow and the Github issues, and trying the above approaches (none of which worked for some reason), I'm left with the decidedly inelegant approach of restarting the entire kernel.   Frustrating.\r\n"", 'Why do they close issues that are not solved like at all?', ""Having this issue too.  Restarting doesn't always fix it either.  Even shutting down Jupyter doesn't necessarily fix it.  Using Windows, no idea how to fix this when it pops up.  Any time I restart the kernel with TF on it there's a random chance it'll happen after restarting.  "", ""Is this still happening if you use `fit_generator()`, looking at OP's issue with full memory I guess its because of the dataset being too big?"", 'Any solution, yet ?', "">After several hours of scouring StackOverflow and the Github issues, and trying the above approaches (none of which worked for some reason), I'm left with the decidedly inelegant approach of restarting the entire kernel. Frustrating.\r\n\r\nI have to do the same :frowning_face: "", 'Tensorflow version:  2.1.0\r\nKeras version:  2.3.1\r\nWhen use reset_keras() function; I get this error\r\n`get_session` is not available when using TensorFlow 2.0.\r\nDo you have a working function for this version :))\r\n', '> Tensorflow version: 2.1.0\r\n> Keras version: 2.3.1\r\n> When use reset_keras() function; I get this error\r\n> `get_session` is not available when using TensorFlow 2.0.\r\n> Do you have a working function for this version :))\r\n\r\nIt works for me. Google colab with TF 2.3.0\r\n```\r\n#reset Keras Session\r\ndef reset_keras():\r\n    sess = tf.compat.v1.keras.backend.get_session()\r\n    tf.compat.v1.keras.backend.clear_session()\r\n    sess.close()\r\n    sess = tf.compat.v1.keras.backend.get_session()\r\n\r\n    try:\r\n        del classifier # this is from global space - change this as you need\r\n    except:\r\n        pass\r\n\r\n    # use the same config as you used to create the session\r\n    config = tf.compat.v1.ConfigProto()\r\n    config.gpu_options.per_process_gpu_memory_fraction = 1\r\n    config.gpu_options.visible_device_list = ""0""\r\n    tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\r\n```\r\n\r\n', '> > Tensorflow version: 2.1.0\r\n> > Keras version: 2.3.1\r\n> > When use reset_keras() function; I get this error\r\n> > `get_session` is not available when using TensorFlow 2.0.\r\n> > Do you have a working function for this version :))\r\n> \r\n> It works for me. Google colab with TF 2.3.0\r\n> \r\n> ```\r\n> #reset Keras Session\r\n> def reset_keras():\r\n>     sess = tf.compat.v1.keras.backend.get_session()\r\n>     tf.compat.v1.keras.backend.clear_session()\r\n>     sess.close()\r\n>     sess = tf.compat.v1.keras.backend.get_session()\r\n> \r\n>     try:\r\n>         del classifier # this is from global space - change this as you need\r\n>     except:\r\n>         pass\r\n> \r\n>     # use the same config as you used to create the session\r\n>     config = tf.compat.v1.ConfigProto()\r\n>     config.gpu_options.per_process_gpu_memory_fraction = 1\r\n>     config.gpu_options.visible_device_list = ""0""\r\n>     tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\r\n> ```\r\n\r\nThanks a lot for sharing your code in TF 2.0, but I still cannot fix it up with TF==2.2.0& keras ==2.3.1. And after I run your code, the console shows following messages **BUT** GPU still almost full which confuses me a lot :\r\n\r\n> 2021-05-09 23:12:03.111033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\n> pciBusID: 0000:21:00.0 name: Quadro P2000 computeCapability: 6.1\r\n> coreClock: 1.4805GHz coreCount: 8 deviceMemorySize: 5.00GiB deviceMemoryBandwidth: 130.53GiB/s\r\n> 2021-05-09 23:12:03.111607: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n> 2021-05-09 23:12:03.111866: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n> 2021-05-09 23:12:03.112118: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n> 2021-05-09 23:12:03.112363: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n> 2021-05-09 23:12:03.112637: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n> 2021-05-09 23:12:03.112897: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n> 2021-05-09 23:12:03.113170: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n> 2021-05-09 23:12:03.113497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n> 2021-05-09 23:12:03.113796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2021-05-09 23:12:03.114042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n> 2021-05-09 23:12:03.114196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n> 2021-05-09 23:12:03.114435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3841 MB memory) -> physical GPU (device: 0, name: Quadro P2000, pci bus id: 0000:21:00.0, compute capability: 6.1)\r\n\r\nMay I ask you have you ever met this issue?\r\n**BTW**, could you please me what the `classifier (from the global space)` is in your own code? should I just  `del model`?\r\n']",[],[],1,0
87,keras,1360,closed,why lstm loss is NaN for pre-trained word2vec,"I'm a theano and keras fresher, and want to learn them , which I think very interesting and helpful.
The following question confuses me about for one week. But I can't work it out after try some ways mentioned before.
I want to do sentiment analysis for texts to three classes. And I train word2vec(dim = 600) with gensim.
My train data is 10475 sequences in different length. label shape is [10475,3] After setting maxlen of sequence 200, every sequence are converted to 200*600 2D array.If some sequence's length is less than 200, then the remaining values is filled with 0(padding), resulting some rows are all zeroes. And then I feed them into LSTM, 
## LSTM code as following:



model.fit(train,label,batch_size=100,nb_epoch=4,verbose=1,shuffle=True,validation_split=0.1,show_accuracy=True)
## But Getting:
## loss: nan

Train on 9430 samples, validate on 1048 samples
Epoch 1/4
9430/9430 [==============================] - 99s - loss: nan - acc: 0.2992 - val_loss: nan - val_acc: 0.1355
Epoch 2/4
9430/9430 [==============================] - 96s - loss: nan - acc: 0.2992 - val_loss: nan - val_acc: 0.1355
Epoch 3/4
9430/9430 [==============================] - 96s - loss: nan - acc: 0.2992 - val_loss: nan - val_acc: 0.1355
Epoch 4/4
1600/9430 [====>.........................] - ETA: 75s - loss: nan - acc: 0.3038

I test different optimizer,also improve epsilon value, set clipnorm(in optimizer above) and different loss functions('mean_squared_error', 'categorical_crossentropy') and so on, but failed.
#### Also in cpu or gpu mode, loss value is also nan.
# ##Even I switch to Convolution2D:


### The loss values remain nan
### Ways to solve?

So I'm wondering what's the real reason for the NaN loss value? How to solve or debug it?
Is the word2vec data wrong , padding method wrong or other?
If keras can't solve, I have to choose another deep learning package, or the reason is theano?
what can I do then? please help.
",stale,"[""Are you sure I got nan loss with `categorical_crossentropy`? What are your labels look like?\n\n> resulting some rows are all zeroes\n\nYou shouldn't have some rows are all zeros - LSTM takes as input a 3D tensor with shape `(nb_samples, input_length, input_dim)`\n\nSo, from the side view, your input data should look like this:\n\n```\n|0000xxxx|/\n|00xxxxxx|/\n|xxxxxxxx|/\n|00000xxx|/\n```\n\nwhere `/` denotes the dimension of word2vec vectors\n"", ""@jgc128  Thanks to your reply and help.\nmy label likes [[0,1,0], [1,0,0]....[0,0,1]], whose shape is (nb_samples, nb_classes), here nb_samples = 10475, nb_classes=3;\nBut I'm not sure what's the meaning of your symbols. From my perspective, nb_samples is the number of all sequences. And every sequence is represented as a 2D array, every row is a word vector, the number of column is word2vec's dim(here is 600). And if I don't pad some all zeros rows, how to make sure every 2D array have same input_length, considering every sequence has different lengths?\nThanks!\n"", ""@liyi193328 sorry, I was unclear. \n\nYes, `nb_samples` is the number of all sequences (10475), `input_length` is the length of the longest sequence and `input_dim` is the dimension of word2vec vectors (600).\n\nSo the input matrix looks like this:\n![keras_lstm_input](https://cloud.githubusercontent.com/assets/669142/12021366/1f7547fe-ad54-11e5-8188-78e5b6546665.png)\n\nwhere `View 1` is represented in my post above. 0's mean the zeros for padding (note we pad from the left), and x's are some vectors.\n\nIn the code it looks like this: \n\n```\nX = np.zeros((nb_samples, input_length, input_dim)) \n```\n"", '@jgc128 Thanks. Wonderful details about input 3D array.\nMore specifically, three sentences like: [ [ He, like, keras], [learning], [like, keras] ]\nthe word vector(4 dim) each is:\nHe -> [1,1,1,1], like->[2,2,2,2], keras->[3,3,3,3], learning->[5,5,5,5]\nthen after padding ,the 3D array shape is (2,3,4), like:\n[ \n   [  [1,1,1,1], [2,2,2,2],[3,3,3,3]   ],\n   [  [0,0,0,0], [0,0,0,0], [5,5,5,5]  ],\n   [  [0,0,0,0], [2,2,2,2], [3,3,3,3]  ]\n]\nif the specific example right?\nThanks.\n', ""@liyi193328 Yes, it's right. It should work.\n"", '@jgc128 Thanks.\nAfter padding like the example,  I still get loss NaN.\nIt\'s a little confusing.\nmodel.add(LSTM(output_dim=300,input_length=200,input_dim=600))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation(\'softmax\'))\nmodel.compile(loss=\'categorical_crossentropy\',\n                      optimizer=\'adam\', class_mode=""categorical"")\nAre there other tricks?\n', 'Has your target array been formatted correctly? \n\nEg. One class per column ?\n', 'Can you show your code for creating the input data?\n', '@dandxy89 @jgc128 Thanks.\nYes, my targel array.shape is (nsamples,nb_classes). One class per column  (0,1,3). And I try your code in #853 (padd word index array with left zeros, add embedding layer ). Everything goes well and I get 66.1% acc for 3 classes with 8583 sequence.But embedding vectors cost much memory, resulting to GPU device memory can\'t allocate(only choose cpu). So I want to it faster with fix word vectors.\n\nMy code for input data is:\n\n```\nmaxlen=200\ndef init(textDatapath=\'./allData.txt\', word2vecPath=\'./word2vec\',maxlen=200,nb_classes=3,updated=False,vecDataPath=\'./trainVec(part).pickle\',newvecDataPath=\'./trainVec(new).pickle\'):\n    Train = list()\n    seqTokens = list()\n    if os.path.isfile(vecDataPath) and updated == False:\n        ft = codecs.open(vecDataPath,""rb"")\n        print(""find "", vecDataPath)\n        D = pickle.load(ft)\n        Train = D[\'train\']\n        Label = D[\'label\']\n        ft.close()\n    else:\n        print(""begin to update"", newvecDataPath)\n        f = codecs.open(textDatapath, ""r"", ""utf-8"") #every line is a sequence\n        lines = f.readlines()\n        Label = []\n        Textokens = []\n        for line in lines:\n            t = line.split(""\\t"")  #t[0] is the target\n            tokens = jieba.lcut(t[1])  #segment sequence, get a list of tokens \n            vec = []\n            existToken = []\n            minum = 5   #sequences having least 5 tokens can be considered\n            for token in tokens:\n                try:\n                    vector = word2vec[token] #get vector of token by word2vec(gensim)\n                    vec.append(vector)\n                    existToken.append(token)\n                except KeyError:\n                    continue\n            if len(vec) <= 5:\n               continue\n            else:\n                s = np.array(vec)   #s is the sequence\'s 2D array\n            Train.append(s)\n            Label.append(int(t[0]))\n            seqTokens.append(existToken)\n        if updated == True:\n            ft = codecs.open(newvecDataPath,""wb"")\n            print(""dump to "",newvecDataPath)\n            pickle.dump({\'train\':Train,\'label\':Label,\'seqTokens\':seqTokens},ft)\n            ft.close()\n    # Label and Train is a list, in Label every element is a scalar.\n    #In my case, Label is -1,0,1, so it needs to plus 1 to become 0,1,2\n    Label = np.array(Label,dtype=\'float32\') + 1 \n    Train = np.array(Train)  # in train every element is a numpy array.\n    print(""init finished!"")\n    return [Train,Label]\n\ndef padTrainData(Train,Label):\n    print(""pre train data..."")\n    Label = np.array(Label,dtype=\'float32\')\n    Label = np_utils.to_categorical(Label,nb_classes= 3)\n    nsamples = Train.shape[0]\n    train = np.empty((nsamples,maxlen,lstm_input_dim))\n    for i in range(nsamples):\n        t = Train[i]\n        (tokens,dim)=t.shape\n        if tokens < maxlen:\n            #s is the empty array\n            s = np.empty((maxlen-tokens, 600))\n           #combine s and t\n            train[i] = np.concatenate( (s,t),axis=0)\n        else:\n            train[i] = np.array(t[0:maxlen])\n    Train = np.array(train,dtype=\'float64\')\n    return [Train,Label]\n\ntrain = None\nlabel = None\ntrain,label = init(vecDataPath=\'./trainVec(new).pickle\',updated=True)\ntrain,label = padTrainData(train,label)\nprint(""train shape:"",train.shape)\nprint(""label shape:"",label.shape)\n```\n\n```\nThe results is:\nbegin to update ./trainVec(new).pickle\ndump to  ./trainVec(new).pickle\ninit finished!\npre train data...\ntrain shape: (8583, 200, 600)\nlabel shape: (8583, 3)\n```\n\nIn my code, first step is get train data , the second is getting target label array, padding train data with zeros\nThanks to help me with great patience. \n', 'One problem is you are using `np.empty` but it does not initialize the array with zeros ([see documentation](http://docs.scipy.org/doc/numpy/reference/generated/numpy.empty.html)). Try to use `np.zeros` instead. \n\nIt should not give the nan loss though..\nHave you tried to see what is the output of the network?\n', ""@jgc128 Thank you very much. \nEverything goes fine when I change np.empty to np.zeros  !\n It's all my mistake.sorry\nwhen use np.empty to init a array, the value may be two large or too small, resulting NaN?\nAnother question is how to check the output of the network? use model.predict_proba , theano function or other ways?\nThanks with sincerely!\n"", 'Excellent!\n\nYou can use something like this `classes = model.predict_classes(X_test, batch_size=32)`. See [Getting started: 30 seconds to Keras](http://keras.io/#getting-started-30-seconds-to-keras) for details\n', ""@jgc128 Thanks.I'll dive into it.\n"", 'why don`t use keras.preprocessing.sequence.pad_sequences ?\r\n        data= list()\r\n        for individual in len( --):\r\n            express_matrix = individual.express_individual_times()  # 每个样本返回二维矩阵，N * 256;\r\n            data.append(express_matrix)\r\ntrain_matrix = sequence.pad_sequences(data,padding=\'post\', maxlen=40)\r\n\r\nI checked the data, and make sure it pads OK, but still get ""loss = NAN"" for several sample.\r\nI wonder if I should delete these samples when they looks like very normal.', 'Hi I am using np.zeros() only but still getting very less accuracy around 37% on 900 samples for a 30 class classification. I used tanh as activation function before softmax layer. all suggestions are welcomed.\r\n\r\nMy code is as follows :\r\n\r\ndef build_matrix(word_index):\r\n    #embedding_index = load_embeddings(path)\r\n    embedding_matrix = np.zeros((len(word_index) + 1, 100))\r\n    unknown_words = []\r\n    \r\n    for word, i in word_index.items():\r\n        try:\r\n            embedding_matrix[i] = w2v_model[word]\r\n        except KeyError:\r\n            unknown_words.append(word)\r\n    return embedding_matrix\r\n\r\nembedding_matrix=build_matrix(tokenizer.word_index)\r\n\r\nmodel = Sequential()\r\nmodel.add(Embedding(max_features,embedding_matrix.shape[1], weights=[embedding_matrix],input_length=MAX_LEN,trainable=False))\r\nmodel.add(SpatialDropout1D(0.3))\r\nmodel.add(LSTM(LSTM_UNITS,activation=\'relu\',return_sequences=True))\r\nmodel.add(LSTM(LSTM_UNITS))\r\nmodel.add(Dropout(0.5))\r\n#model.add(LSTM(100))\r\nmodel.add(Dense(4*LSTM_UNITS,input_shape=(1000,),activation=\'relu\'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(4*LSTM_UNITS,activation=\'tanh\'))\r\nmodel.add(Dense(30, activation=\'softmax\'))\r\nmodel.compile(loss=\'categorical_crossentropy\', optimizer=\'adam\', metrics=[\'accuracy\'])\r\nprint(model.summary())\r\n\r\nhistory=model.fit(x_train, y_train, nb_epoch=11, batch_size=64,validation_data=(x_test,y_test))\r\n# Final evaluation of the model\r\nscores = model.evaluate(x_test, y_test, verbose=0)\r\nprint(""Accuracy: %.2f%%"" % (scores[1]*100))']","['\n    sgd = SGD(lr=0.001, decay = 1e-6, momentum=0.9, nesterov=True, clipnorm=0.3)\n    rmsprop = RMSprop(clipnorm=0.1,epsilon=5e-04)\n    adam = Adam(epsilon=1e-03,clipnorm=0.1)\n    model = Sequential()\n\n    model.add(LSTM(output_dim=300,input_length=200,input_dim=600)) \n#     model.add(Dropout(0.5))\n    model.add(BatchNormalization(epsilon=1e-04))\n    model.add(Dense(nb_classes))\n    model.add(Activation(\'softmax\'))\n    model.compile(loss=\'mean_squared_error\',\n                  optimizer=\'adam\', class_mode=""categorical"")\n', '\n    nb_feature_maps = 120\n    n_gram = 10\n    model.add(Convolution2D(nb_filter = nb_feature_maps, nb_row=n_gram, nb_col=600,input_shape=(1,200,600)))\n    model.add(Activation(\'relu\'))\n\n    model.add(MaxPooling2D(pool_size=(maxlen - n_gram + 1, 1)))\n    model.add(Dropout(0.25))\n\n    model.add(Flatten())\n    model.add(Dense(128))\n    model.add(Activation(\'tanh\'))\n    model.add(Dropout(0.5))\n    model.add(Dense(3))\n    model.add(Activation(\'softmax\'))\n\n    model.compile(loss=\'mean_squared_error\',\n              optimizer=\'sgd\', class_mode=""categorical"")\n']",[],1,0
88,keras,8020,closed,Tensorflow backend is slower than Theano in CNN models,"Hello everyone.
I've been reading many discussions here and in google groups regarding the tf backend issues and I found them too **wordy** and without a bottom-line for a question, that at least in my opinion should has a short answer.

I'm using the CNN mnist model proposed here: [https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py](url)
To run some performances tests.

I'm using a p2.xlarge amazon instance with a Tesla K80 single GPU.
My python version is 3.6
Keras==2.0.8 (conda keras-gpu version)
tensorflow==1.2.0
Theano==0.9.0

My keras.json file at ~/.keras/keras.json:

and my theanorc file contains:


When I run the code I get that Theano and Tensorflow are roughly equal (Theano 8s per epoch, and Tf is 10s per epoch but loads the data faster)

But when I'm ruuning this on my model whose input shape is 224x224x3x30000 images
**Theano outperforms Tenserflow by a factor of 2.5!** (although it takes about 5 minutes for theano to initialize the images)

**Tensorflow backend:**
![image](https://user-images.githubusercontent.com/31940058/31015242-c0ba6c6c-a527-11e7-9751-c072b40aafe7.png)

**Theano backend:**
![image](https://user-images.githubusercontent.com/31940058/31018700-aa694f2e-a535-11e7-8b9a-9c81a65eb6db.png)


One more thing - when I execute ""import keras"" with tensorflow backend I receive:


While theano as backend:



**Thanks for help!**",,"['You need tensorflow-gpu not tensorflow', ""@Dref360 I'm using Tensorflow-gpu, I made sure the process is running on my GPU. (Otherwise it'd take ~200s per epoch on the MNIST)"", 'Hello\r\nMaybe your tensorflow is not loading properly CuDnn. (I think it needs version Cudnn 6 now and you appear to be using CuDnn 5).', 'Please upgrade to the latest version of TensorFlow and test again. Feel free to reopen the issue if it still persists. Thanks!']","['\r\n{\r\n    ""epsilon"": 1e-07,\r\n    ""floatx"": ""float32"",\r\n    ""image_data_format"": ""channels_last"",\r\n    ""backend"": ""theano""\r\n}\r\n', '\r\n[global]\r\ndevice = cuda\r\nfloatX = float32\r\n[cuda]\r\nroot = /usr/local/cuda\r\n', '\r\nUsing Theano backend\r\nUsing cuDNN version 5110 on context None\r\nMapped name None to device cuda: Tesla K80 (0000:00:1E.0)\r\n']",['Using Tensorflow backend.'],1,1
89,keras,1369,closed,The accuracy of using Graph to implement Bidirectional RNN for classification is note correct,"Currently, I am using Bidirectional RNN/LSTM(BRNN/BLSTM) for sequence classification (return_sequence= True, i.e., classification of each time-step). According to the official provided example, the BRNN is required to implemented by Graph (while the BidirectionalRNN API does not support mask input). Besides, before mini-batch input, I do a preprocessing (pad with 0) to make the input to (batch_size, input_length, input_features), but it seems that no explicit mask input for keras. So I listed my implementation as 











As no **show_accuracy=True** for modellstm.fit (fit of Graph does not support), the accuracy 
 doesn't take the input_mask into consideration. Besides, for example, **without explicit input_mask**, **the output result of classification np.argmax([0 0 0 0])=0 (i.e., class=1) for input np.array([0 0 ... 0]), which is indeed wrong**. Actually input_mask for input np.array([0 0 ... 0]) should be **0**, which means all 0 input and output should not be calculated for accuracy. How could I modify me code to accomplish this code. Thanks a lot for your nice assistance. @fchollet @farizrahman4u
",stale,"['This is not only for Bidirectional, in fact, all RNNs with Masking had this behavior in version 0.2.0 (now I don\'t know). In my project, I created a callback for calculate the accuracy using an unpadding method. Try with:\n\n``` python\ncallbacks = [ModelCheckpoint(filepath=weights_file, verbose=1, save_best_only=True)]\ncallbacks.append(EpochAccuracy((samples, target, batch_size), eval_func=calculate_accuracy))\n\nself.model.fit({\'input\': samples, \'output\': target}, nb_epoch=nb_epoch, batch_size=batch_size, callbacks=callbacks)\n\nclass EpochAccuracy(keras.callbacks.Callback):\n    def __init__(self, data, eval_func):\n        self.samples, self.target, self.batch_size = data\n        self.eval = eval_func\n\n    def on_epoch_end(self, epoch, logs={}):\n        candidates = np.argmax(self.model.predict({\'input\':self.samples}, verbose=0, batch_size=self.batch_size)[\'output\'], axis=-1)\n        candidates = unpad_sequences(candidates, mask_value=0)\n        accuracy = self.eval(candidates, self.target)\n        logger.info(""Accuracy after epoch {}: {}"".format(epoch, accuracy))\n\n```\n\nThe `unpad_sequences` looks like:\n\n``` python\ndef unpad_sequences(padded_sequences, mask_value=0):\n    unpadded = []\n    for sequence in padded_sequences:\n        # XXX: might remove mask_values from the middle of the sequence\n        unpadded.append(sequence[sequence != mask_value].tolist())\n    return unpadded\n```\n\nAnother way to unpad the sequences is using the original sequence as mask. \n', '@meitcher Thanks for your reply.\nIn keras v0.3.0. the accuracy is calculated as:\n`if class_mode == ""categorical"":`\n `train_accuracy = K.mean(K.equal(K.argmax(self.y, axis=-1), K.argmax(self.y_train, axis=-1)))`\n `test_accuracy = K.mean(K.equal(K.argmax(self.y, axis=-1), K.argmax(self.y_test, axis=-1)))`\nand the mask is still not considered. \n\nIn your implementation, I think it is also required to do the unpad_sequences operations to target, i.e.,\n\n `candidates = unpad_sequences(candidates, mask_value=0)`\n`self.target = unpad_sequences(self.target, mask_value=0)`\n`accuracy = self.eval(candidates, self.target)`\n\nor\n\n`self.eval(y_preds[input_mask_flat], y_true[input_mask_flat])`\n\nHow do you think about it?\n', ""In #957 is showed that is also possible to use the sample_weights parameter in fit() method, which is a simple way to do mask in the output. Did you tried this?\n\n@icemansina \nEdit: Yes, it's necessary, _sorry for the previous reply_. And if you make 1-hot vectors with `self.target`, then you need to `unvectorize` it too. Summing up:\n\n``` python\n# predict using self.model\ncandidates = self.model.predict({'input':self.samples}, verbose=0, batch_size=self.batch_size)['output']\n\n# unpad candidates and targets\ncandidates = unpad_sequences(np.argmax(candidates, axis=-1), mask_value=0)\ntargets = unpad_sequences(np.argmax(self.target, axis=-1), mask_value=0)\n\n# now you have list of lists that maps with the original sentence structure\naccuracy = self.eval(candidates, targets)\n```\n"", 'I ran into the same problem and found out that the accuracy function does not take masking into account by reading the source code... Is this considered to be a bug in the accuracy function or the documentation? Just asking because I could try to whip up a pull request ;).\n']",[],"['modellstm = Graph()', ""modellstm.add_input(name='input', input_shape=(MAX_SEQ_LENGTH, input_dim), dtype='float')"", ""modellstm.add_node(Masking(mask_value=0), name='masked_input', input='input')"", ""modellstm.add_node(LSTM(10, return_sequences=True), name='forward', input='masked_input')"", ""modellstm.add_node(LSTM(10, go_backwards= True, return_sequences=True), name='backward', input='masked_input')"", ""modellstm.add_node(Dropout(0.5), name='dropout', inputs=['forward', 'backward'])"", ""modellstm.add_node(TimeDistributedDense(N_CLASSES, activation='softmax'), name='dense', input='dropout')"", ""modellstm.add_output(name='output', input='dense')"", ""modellstm.compile('Adadelta', {'output': 'categorical_crossentropy'})"", ""acc = accuracy((y_train_accuracy),  np.argmax(np.array(modellstm.predict({'input': X_train}, batch_size=BATCH_SIZE)['output']), axis=-1))""]",1,0
90,keras,8027,closed,Why 2 almost equal CNN returns 2 quite different results,"# **Update: finally, I found the problem. It was related with the activation function and it was my fault. Sorry for that. I closed the issue.**

I'm addressing a sentence-level binary classification task. My data consists of 3 subarrays of tokens: left context, core, and right context.

I used Keras to devise several alternatives of Convolutional Neural Networks and validate which one best fit my problem.

I'm a newbie in Python and Keras and I decided to start with simpler solutions in order to test which changes improve my metrics (accuracy, precision, recall, f1 and auc-roc). The first simplification was regarding input data: I decided to ignore contexts to create a Sequential model of Keras:


As you can see, I use a fixed size of inputs so I applied a padding preprocessing. I also used an embedding layer with a Word2Vec model.

This model returns the following results:


I wished to implement how to select a subarray of input data inside my CNN by means of Lambda layers. I use the following definition of my Lambda layer:

Lambda(lambda x: x[:, 1], output_shape=(500,))(input)
And this is the summary of my new CNN (as you can see it's almost the same than the prior):


But the results were disgusting because accuracy stops at 60% and obviously, precision, recall and f1 were too low (< 0.10) regarding the first model results.

I don't know what's happening and I don't know if these networks are more different that I thought.

Any clue regarding this issue?

**Note**: I asked this question in Stackoverflow but I think it could be an issue regarding Keras implementation. Link [here](https://stackoverflow.com/questions/46491418/why-2-almost-equal-keras-cnn-returns-2-quite-different-results)",,[],"['\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         (None, 500)               0         \r\n_________________________________________________________________\r\nmasking_1 (Masking)          (None, 500)               0         \r\n_________________________________________________________________\r\nembedding_1 (Embedding)      (None, 500, 100)          64025600  \r\n_________________________________________________________________\r\nconv1d_1 (Conv1D)            (None, 497, 128)          51328     \r\n_________________________________________________________________\r\naverage_pooling1d_1 (Average (None, 62, 128)           0         \r\n_________________________________________________________________\r\ndropout_1 (Dropout)          (None, 62, 128)           0         \r\n_________________________________________________________________\r\nconv1d_2 (Conv1D)            (None, 61, 256)           65792     \r\n_________________________________________________________________\r\ndropout_2 (Dropout)          (None, 61, 256)           0         \r\n_________________________________________________________________\r\nconv1d_3 (Conv1D)            (None, 54, 32)            65568     \r\n_________________________________________________________________\r\nglobal_max_pooling1d_1 (Glob (None, 32)                0         \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 16)                528       \r\n_________________________________________________________________\r\ndropout_3 (Dropout)          (None, 16)                0         \r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 2)                 34        \r\n=================================================================\r\n', '\r\nP       0.875457875\r\nR       0.878676471\r\nF1      0.87706422\r\nAUC-ROC 0.906102654\r\n', '\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         (None, 3, 500)            0         \r\n_________________________________________________________________\r\nlambda_1 (Lambda)            (None, 500)               0         \r\n_________________________________________________________________\r\nmasking_1 (Masking)          (None, 500)               0         \r\n_________________________________________________________________\r\nembedding_1 (Embedding)      (None, 500, 100)          64025600  \r\n_________________________________________________________________\r\nconv1d_1 (Conv1D)            (None, 497, 128)          51328     \r\n_________________________________________________________________\r\naverage_pooling1d_1 (Average (None, 62, 128)           0         \r\n_________________________________________________________________\r\ndropout_1 (Dropout)          (None, 62, 128)           0         \r\n_________________________________________________________________\r\nconv1d_2 (Conv1D)            (None, 61, 256)           65792     \r\n_________________________________________________________________\r\ndropout_2 (Dropout)          (None, 61, 256)           0         \r\n_________________________________________________________________\r\nconv1d_3 (Conv1D)            (None, 54, 32)            65568     \r\n_________________________________________________________________\r\nglobal_max_pooling1d_1 (Glob (None, 32)                0         \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 16)                528       \r\n_________________________________________________________________\r\ndropout_3 (Dropout)          (None, 16)                0         \r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 2)                 34        \r\n=================================================================\r\n']",[],1,0
91,keras,863,closed,Keras slow with regularization?,"I noticed that when I include weight regularization such as L2 weight regularization, it significantly slows down training. Can someone provide an explanation for why that is?
",,"['Adding regularization will significantly complicate the computational graph of your model, as you are adding many terms to your loss function. A more complex graph will take longer to run.\n\nA while ago, Keras optimizers were added as part of the optimizer rather than as part of the loss function, which left the computation graph almost unchanged and could run very fast. However it was much less accurate than the current setup. But you could always hack it back in if speed is a big issue for your model.\n', ""I have two CNN models, both share same architecture. 'Model A' was trained without (l2)regularization and 'Model B' was trained with (l2)regularization.\r\non CPU:\r\n-  Model A Prediction time: 10 secs\r\n-  Model B Prediction time :  60 secs \r\n\r\nThe function used for prediction was 'model.predict()' and rest all conditions like image size, batch_size, Number of images were same for both models. My keras version is 1.1.0 . Model architecture was loaded from json and weights were plugged into by '.h5' file.  Please provide your guidance.""]",[],[],1,0
92,keras,866,closed,"with latest Theano updates, LSTM faster but GRU much slower","With the latest updates to scan in Theano, anybody else experiencing dramatically different training times? In particular, LSTM is faster, but GRU/JZS1-3 is much, much slower:

A relative comparison on a test network, exchanging the recurrent layer:



For this same network, LSTM pre Theano scan changes was ~24s.

Anybody have some insight into this?
",,"['If you can tell me which commit of Theano was faster on GRU and the current\ncommit that is slower and the way to reproduce it, we can investigate\nrelatively fast.\n\nOn Wed, Oct 21, 2015 at 12:09 PM, Leon Chen notifications@github.com\nwrote:\n\n> With the latest updates to scan in Theano, anybody else experiencing\n> dramatically different training times? In particular, LSTM is faster, but\n> GRU/JZS1-3 is much, much slower:\n> \n> A relative comparison on a test network, exchanging the recurrent layer:\n> \n> LSTM: 15s\n> GRU: 94s\n> JZS1: 63s\n> JZS2: 83s\n> JZS3: 91s\n> \n> For this same network, LSTM pre Theano scan changes was ~24s.\n> \n> Anybody have some insight into this?\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/fchollet/keras/issues/866.\n', '@nouiz  Theano commit https://github.com/Theano/Theano/commit/f0bd940e85e42a9844a0e91e24b30badb5f9902a\n\nGRU before ~14s / epoch on my test network vs ~90s / epoch after. Again, LSTM speed is improved, though. This is on GPU, but it seems on CPU GRU is also now also slower than LSTM, but not to the same degree.\n', ""After tinkering around a bit, it appears the source of the slowdown with the new theano changes is the `r * h_mask_tm1` part of `hh_t = self.activation(xh_t + T.dot(r * h_mask_tm1, u_h))` where `r = self.inner_activation(xr_t + T.dot(h_mask_tm1, u_r))`. LSTM doesn't have a similar expression so it is unaffected.\n"", ""How can I recreate it? I never used keras, so maybe just point me to there\nthe example/code is.\n\nAlso, which version of Theano was faster? Do you have a commit number? An\nproximate date of the checkout?\n\nWhen was your last update of Theano that caused this slow down? We got one\nsuch case in Theano master for ~2 days, but it was fixed around 2 weeks ago.\n\nOn Wed, Oct 21, 2015 at 3:15 PM, Leon Chen notifications@github.com wrote:\n\n> After tinkering around a bit, it appears the source of the slowdown with\n> the new theano changes is the r \\* h_mask_tm1 part of hh_t =\n> self.activation(xh_t + T.dot(r \\* h_mask_tm1, u_h)) where r =\n> self.inner_activation(xr_t + T.dot(h_mask_tm1, u_r)). LSTM doesn't have a\n> similar expression so it is unaffected.\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/fchollet/keras/issues/866#issuecomment-149997847.\n"", 'The first bad commit was 5dcd39997736cdb04fb55de35f6917977791e8e6 https://github.com/Theano/Theano/commit/5dcd39997736cdb04fb55de35f6917977791e8e6 in theano, though a couple commits before that were taking about 50%-100% longer for GRU than it had been. Though I may have come across a newer commit that was good, which may indicate that a performance regression was fixed, but then another one that was extremely similar got into the code again. Commit 9c19d30 might have been good. https://github.com/Theano/Theano/commit/9c19d30009f959666bdc8eab13c494aa52dc16ab\n', 'https://github.com/Theano/Theano/commit/f0bd940e85e42a9844a0e91e24b30badb5f9902a:\nmy test GRU, 85 s / epoch\n\nhttps://github.com/Theano/Theano/commit/6d21a3f84e7a2826d32ef2d649e0eeabf2e8384d:\nsame GRU, 14 s / epoch\n', ""I need to be able to run this code. I'm currently looking how to do this. If you know how, tell me.\n"", ""I changed examples/imdb_lstm.py to use Keras GRU layers instead of LSTM and I'm able to reproduce the problem.\n"", ""I've verified that https://github.com/Theano/Theano/commit/6d21a3f84e7a2826d32ef2d649e0eeabf2e8384d is also good. I suspect the performance issue was fixed but then re-introduced, or perhaps this commit was not a descendant of the commit that introduced the performance regression.\n"", 'I have a PR that fix this:\nhttps://github.com/Theano/Theano/pull/3548\n\nthanks for the report and the help!\n', 'Great work on Theano @nouiz !\n', '@nouiz wow awesome, thanks!\n', 'We merged the PR yesterday. So update Theano if you see this issue.\n']",['\nLSTM: 15s\nGRU: 94s\nJZS1: 63s\nJZS2: 83s\nJZS3: 91s\n'],[],1,0
93,keras,867,closed,Regression with MLP,"Hello, 

Is it normal that when i use a MLP to make a regression (with time series like sunspot) they are only 'loss' and 'val_loss' that decrease during the training ( acc and val_acc are always at 1.0000) ?

for information, I put a linear activation on output layer (which have only one neural) with mse and each hiden layer have 'relu' activation and dropout

thank you to anybody can give me a good reason :) 
",,"[""Accuracy is not relevant with regression. Simply don't display it (`show_accuracy=False`).\n"", 'Ok thank you very munch a lot for your quick answer :) \n']",[],[],1,0
94,keras,11119,closed,Progressively slower model save/load times,"I've noticed that the process of saving and loading saved models slows down as I generate more models. To clarify, say I have to load 10 models from hdf5 files. The 1st model I load will be loaded quickly, but every successive load will be progressively slower. The models are of the same size, and memory is not an issue. I've attached a python script to reproduce the issue. 

OS: Ubuntu 16.04
Backend: Tensorflow
Using CPU only

Thank you

### Reproducing code example:



### Example output:


",To investigate,"['Does this happen with other backends? ', ""Actually, I think I know what happens. Deleting the model with del doesn't free the memory. You should clear the session with K.clear_session() after deleting each model. Please try and tell us what happens. "", 'K.clear_session() works.', ""I'm closing this issue, feel free to reopen it if your problem isn't solved."", '@gabrieldemarmiesse @farizrahman4u thanks, that solves the issue. ']","['\r\nimport time\r\nimport numpy as np\r\nimport keras\r\n\r\n\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Activation\r\n\r\n\r\ndef get_model(X):\r\n    metric = ""mse""\r\n    loss_fn = ""mean_squared_error""\r\n    opt_meth = ""rmsprop""\r\n    \r\n    layers = [X.shape[1], 10, X.shape[1]]\r\n    act_fns = [""elu"", ""elu"", ""elu""]\r\n    \r\n    model = Sequential()\r\n\r\n    for i in range(len(layers)):\r\n        model.add(Dense(units=layers[i]))\r\n        model.add(Activation(act_fns[i]))\r\n\r\n    model.compile(optimizer=opt_meth, loss=loss_fn, metrics=[metric])\r\n    model.fit(X, X, verbose=0)\r\n    \r\n    return model\r\n\r\nif __name__ == \'__main__\':\r\n    N = 10\r\n    X = np.random.randn(1000,30)\r\n    for i in range(N):\r\n        fname = str(i)+"".hdf5""\r\n        m = get_model(X)   \r\n        t = time.time()\r\n        m.save(fname)\r\n        t = round(time.time() - t, 3)\r\n        del m\r\n        print ""iter"",i,"" model saved in time:"", t\r\n\r\n    print ""loading models:""\r\n    for i in range(N):\r\n        fname = str(i)+"".hdf5""\r\n        t = time.time()\r\n        m = keras.models.load_model(fname)\r\n        t = round(time.time()-t,3)\r\n        del m\r\n        print ""iter:"",i, ""time taken:"", t     \r\n\r\n', '\r\nUsing TensorFlow backend.\r\n2018-09-11 20:36:51.754719: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports\r\n instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\niter 0  model saved in time: 0.027\r\niter 1  model saved in time: 0.039\r\niter 2  model saved in time: 0.054\r\niter 3  model saved in time: 0.065\r\niter 4  model saved in time: 0.073\r\niter 5  model saved in time: 0.088\r\niter 6  model saved in time: 0.101\r\niter 7  model saved in time: 0.108\r\niter 8  model saved in time: 0.125\r\niter 9  model saved in time: 0.139\r\nloading models:\r\niter: 0 time taken: 0.522\r\niter: 1 time taken: 0.559\r\niter: 2 time taken: 0.592\r\niter: 3 time taken: 0.691\r\niter: 4 time taken: 0.674\r\niter: 5 time taken: 0.711\r\niter: 6 time taken: 0.751\r\niter: 7 time taken: 0.788\r\niter: 8 time taken: 0.824\r\niter: 9 time taken: 0.874\r\n\r\n']",[],1,0
95,keras,10096,closed,Speed regression with change of tensorflow backend version,"We have observed some massive slowdowns in some Keras/tensorflow models with tensorflow versions newer than 1.4.1.
Not sure if the issue is with tensorflow or with the way keras creates the tensorflow models, so I am cross-posting these issue to both repos.

Here is a script reproducing the issue:

## Setup


## Fitting



Here are some timings for the fit method:
* Tensorflow 1.4.1:  **2.91 s ± 452 ms** per loop  (obtained using ipython's  magic, 7 loops)
* Tensorflow 1.5.0:  CPU times: user **2min 19s**, sys: 5min 22s, total: 7min 41s Wall time: 1min 2s
* Tensorflow 1.6.0: CPU times: user **5min 5s**, sys: 12min 31s, total: 17min 36s Wall time: 2min 37s
* Tensorflow 1.7.0: CPU times: user **5min 5s**, sys: 12min 39s, total: 17min 45s Wall time: 2min 39s

So, it seems there was a massive slowdown in version 1,5, and then a further one in 1.6 (which similar speed in 1.7). All the tests are run on a conda environment with python 3.6.5 and keras 2.1.5, with the corresponding tensorflow versions all coming from the anaconda  channel.

The GPU accelerated version of keras/tensorflow ( conda package) does not present the issue.

Thanks in advance!",,"[""I think your test is biased.\r\nThis network will always be faster on CPU.\r\nI suspect that your time 2.91s was using CPU. I get similar results when using tensorflow and not tensorflow-gpu.\r\n\r\nusing tensorflow-gpu 1.6 and 1.8, I added `with tensorflow.device('/cpu:0'):` and I get a similar runtime on Keras master branch. "", ""I believe you misread my post. \r\n\r\nI am not comparing CPU vs GPU but different versions of tensorflow.\r\nAll my tests were run on `tensorflow`, not on `tensorflow-gpu`, so they should run on the CPU (and I see 100% CPU usage while the tests are running). \r\n\r\nWhen I run the GPU enabled version of tensorflow (`tensorflow-gpu` package) I **do not** see the slowdown, getting similar running times across all versions. Also, I did not test 1.8 as it doesn't have an official conda package yet (hence my testing of 1.4, 1.5, 1.6 and 1.7). Since the timing of 1.6 and 1.7 is the same, I don't find surprising that you find no difference with 1.8. Did you try running against 1.4.1?\r\n\r\nMoreover, I have reproduced the issue on an AWS instance with no GPU available."", ""> When I run the GPU enabled version of tensorflow (tensorflow-gpu package) I do not see the slowdown, getting similar running times across all versions\r\n\r\nDoesn't that explain your problem?\r\n\r\n"", 'Anyway, going from 2.91 s to 2min 19s indicates a fundamental runtime issue such as running with/without a BLAS. It is not Keras-related and comes from differences in setups (such differences can sometimes be introduced when upgrading TF).', ""> Doesn't that explain your problem?\r\n\r\nWhat do you mean? To me it just suggests that the issue is in some CPU specific code, as the GPU code does not have the regression. The code will still run slow on recent versions if I don't have a GPU available (eg, in AWS instances or some Docker containers).\r\n\r\n> Anyway, going from 2.91 s to 2min 19s indicates a fundamental runtime issue such as running \r\n> with/without a BLAS. It is not Keras-related and comes from differences in setups (such\r\n> differences can sometimes be introduced when upgrading TF).\r\n\r\nAll the anaconda versions should be compiled/linked against Intel MKL; the way I see it there are (at least) three possible sources of the problem:\r\n\r\n1. A compilation issue on the anaconda side (eg incorrectly passing the MKL flags)\r\n2. Some regression introduced by tensorflow\r\n3. An issue with the way Keras is translating the functional API code into tensorflow operations (perhaps due to a change in tensorflow API?)\r\n\r\nI am trying to figure out which one it is, but it is tricky as I am not familiar with the internals of either keras or tensorflow, hence my cross-posting to both github pages (and support request sent to anaconda). These issues are hard to deal with because they show up in real life situations through the interaction of many software pieces, instead of an isolated testing environment.\r\n\r\nAny type of help is appreciated, if you can compare the timings on different keras/tensorflow versions compiled from source (not coming from the anaconda packages) and don't see the slowdown, then it is likely an anaconda problem. "", 'Latest anaconda package for tensorflow 1.8 does not present the slowdown. Closing the issue.']","[""python\r\nfrom pandas import get_dummies\r\nfrom sklearn.datasets import make_classification\r\n\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout\r\nfrom keras.callbacks import EarlyStopping\r\n\r\nX, y = make_classification(n_samples=10000, n_features=10, n_informative=8,\r\n                           n_classes=5, n_clusters_per_class=1, random_state=0)\r\n\r\nmodel = Sequential()\r\nmodel.add(Dense(10, input_dim=10, activation='relu'))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(Dense(5, activation='sigmoid'))\r\nmodel.add(Dropout(0.1))\r\nmodel.add(Dense(5, activation='softmax'))\r\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\r\n\r\nyk = get_dummies(y)\r\n"", ""python\r\nmodel.fit(X, yk, epochs=10, batch_size=50, \r\n               callbacks=[EarlyStopping(monitor='loss', patience=2)])   \r\n""]","['%%timeit', 'defaults', 'keras-gpu']",1,1
96,keras,13172,closed,Low scores during training but good perormance on real data with fit_generator,"Hello everyone, there might be a problem with the metrics during training when using a generator.

**System information**  
- Have I written custom code (as opposed to using example directory):  **yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  **Windows 10**
- TensorFlow backend (yes / no):  **yes**
- TensorFlow version:  **1.10.0**
- Keras version:  **2.2.4**
- Python version:  **3.6.8**
- CUDA/cuDNN version:  **None**
- GPU model and memory:  **None**

**Current behavior**
During training,  shows very low scores.


**Expected behavior**  
After training, the model performs very well. The actual performance of the trained model on the task largely outclasses the suggested performance estimate during training.
Therefore I expect the metrics during training to show better values.

**Code to reproduce the issue**  



**Other info / logs**  
This code is just the generator. The false metrics came up when I introduced the generator to the training.
Did anyone have similar experiences?
Can anyone tell from the generator if something obvious and important is missing?

Thank you very much. 
",,[],"['\r\n 587/1000 [================>.............] - ETA: 23:53 - loss: 0.1223 - categorical_accuracy: 0.1734\r\n', ""\r\nimport numpy as np\r\nimport random\r\n\r\nfrom gensim.models import FastText\r\nfrom keras.preprocessing.sequence import pad_sequences\r\n\r\n# Load embeddings\r\nembeddings = FastText()\r\nemb_shape = embeddings.wv.vectors.shape\r\n\r\n\r\nclass simple_generator():\r\n\r\n    def __init__(self, X_train, y_train, batch_size):\r\n        self.X_train = X_train\r\n        self.y_train = y_train\r\n        self.batch_size = batch_size\r\n        self.embeddings = embeddings\r\n        self.emb_shape = emb_shape\r\n        self.X_batch = np.zeros((self.batch_size, 200, self.emb_shape[1]+custom_embedding_length))\r\n        self.y_batch = np.zeros((self.batch_size, 200, 19))\r\n\r\n    def run(self):\r\n            while True:\r\n                for i in range(self.batch_size):\r\n                    # choose random index in X\r\n                    index = random.choice(range(len(self.X_train)))\r\n                    self.X_batch[i] = self._vectorize(self.X_train[index], self.embeddings)\r\n                    self.y_batch[i] = pad_sequences([self.y_train[index]], maxlen=200, padding='post', truncating='post')[0]\r\n                \r\n                yield self.X_batch, self.y_batch\r\n\r\n    def _vectorize(self, tokens, embeddings):\r\n        tokens_padded = pad_sequences([tokens], dtype='str', maxlen=200, padding='post', truncating='post', value='[PAD]')[0]\r\n        tokens_vectorized = []\r\n        for token in tokens_padded:\r\n            token_embedding = embeddings[token]\r\n            tokens_vectorized.append(embeddings[token])\r\n        return tokens_vectorized\r\n\r\n""]",['categorical_accuracy'],1,0
97,keras,10624,closed,accuracy issue (using the 20 newsgroups example model),"The post in Keras blog [here](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) yields a 95%  validation accuracy after 2 epochs. Given the example code, I can't reproduce this (in fact the validation accuracy will not go over 75% and loss is very high as well). 

I'm following the example code here:

https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html

...with the exception that I open the GloVe vector file with encoding='utf-8' as it fails otherwise. The link is provided in the example blog post itself. ",,"['Tried with a different machine and the results are same: \r\n\r\n```\r\nIndexing word vectors.\r\nFound 400000 word vectors.\r\nProcessing text dataset\r\nFound 19997 texts.\r\nFound 174074 unique tokens.\r\nShape of data tensor: (19997, 1000)\r\nShape of label tensor: (19997, 20)\r\nPreparing embedding matrix.\r\nTraining model.\r\nTrain on 15998 samples, validate on 3999 samples\r\nEpoch 1/10\r\n15998/15998 [==============================] - 167s 10ms/step - loss: 2.4347 - acc: 0.2020 - val_loss: 1.7728 - val_acc: 0.3958\r\nEpoch 2/10\r\n15998/15998 [==============================] - 165s 10ms/step - loss: 1.5514 - acc: 0.4553 - val_loss: 1.2902 - val_acc: 0.5626\r\nEpoch 3/10\r\n15998/15998 [==============================] - 165s 10ms/step - loss: 1.1808 - acc: 0.5886 - val_loss: 1.1827 - val_acc: 0.5894\r\nEpoch 4/10\r\n15998/15998 [==============================] - 165s 10ms/step - loss: 0.9857 - acc: 0.6626 - val_loss: 1.0051 - val_acc: 0.6629\r\nEpoch 5/10\r\n15998/15998 [==============================] - 167s 10ms/step - loss: 0.8298 - acc: 0.7163 - val_loss: 1.0840 - val_acc: 0.6464\r\nEpoch 6/10\r\n15998/15998 [==============================] - 165s 10ms/step - loss: 0.7169 - acc: 0.7538 - val_loss: 0.9143 - val_acc: 0.7032\r\nEpoch 7/10\r\n15998/15998 [==============================] - 164s 10ms/step - loss: 0.6060 - acc: 0.7875 - val_loss: 0.9732 - val_acc: 0.6942\r\nEpoch 8/10\r\n15998/15998 [==============================] - 165s 10ms/step - loss: 0.5291 - acc: 0.8190 - val_loss: 0.8664 - val_acc: 0.7339\r\nEpoch 9/10\r\n15998/15998 [==============================] - 165s 10ms/step - loss: 0.4393 - acc: 0.8504 - val_loss: 0.8760 - val_acc: 0.7352\r\nEpoch 10/10\r\n15998/15998 [==============================] - 165s 10ms/step - loss: 0.3803 - acc: 0.8699 - val_loss: 0.8892 - val_acc: 0.7352\r\n```', 'Anything on this? It seems kind of big deal if a deep learning package is off 20% from the expected result. ', 'Hi there, I second that.\r\nI achieve on glove 300 after 100 epochs a validation accuracy of ca. 75%; training accuracy is 97%.\r\nThis is not as much as advertised... :-|\r\n']",[],[],1,0
98,keras,7558,closed,issue in categorical_crossentropy (keras) and softmax_cross_entropy_with_logits (tensorflow),"


the cost2 (categorical_crossentropy) causes the poor results finally, but the results are good using cost1.

I used these in the mnist cnn algorithm, as follows:

",,['Did you find out why cost2 outputs worse performance? Thanks!'],"[' python\r\n# Define loss\r\ncost1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\r\n\r\ncost2 = tf.reduce_mean(categorical_crossentropy(tf.nn.softmax(pred), y, from_logits=False))\r\n', ' python\r\n\'\'\'\r\nA Convolutional Network implementation example using TensorFlow library.\r\nThis example is using the MNIST database of handwritten digits\r\n(http://yann.lecun.com/exdb/mnist/)\r\n\r\nAuthor: Aymeric Damien\r\nProject: https://github.com/aymericdamien/TensorFlow-Examples/\r\n\'\'\'\r\n\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\nfrom keras import backend as K\r\n\r\n# Import MNIST data\r\nfrom keras.backend import categorical_crossentropy\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\n\r\nsess = tf.Session()\r\nK.set_session(sess)\r\nmnist = input_data.read_data_sets(""tmp/data/"", one_hot=True)\r\n\r\n# Parameters\r\nlearning_rate = 0.001\r\ntraining_iters = 200000\r\nbatch_size = 128\r\ndisplay_step = 10\r\n\r\n# Network Parameters\r\nn_input = 784  # MNIST data input (img shape: 28*28)\r\nn_classes = 10  # MNIST total classes (0-9 digits)\r\ndropout = 0.75  # Dropout, probability to keep units\r\n\r\n# tf Graph input\r\nx = tf.placeholder(tf.float32, [None, n_input])\r\ny = tf.placeholder(tf.float32, [None, n_classes])\r\nkeep_prob = tf.placeholder(tf.float32)  # dropout (keep probability)\r\n\r\n\r\n# Create some wrappers for simplicity\r\ndef conv2d(x, W, b, strides=1):\r\n    # Conv2D wrapper, with bias and relu activation\r\n    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding=\'SAME\')\r\n    x = tf.nn.bias_add(x, b)\r\n    return tf.nn.relu(x)\r\n\r\n\r\ndef maxpool2d(x, k=2):\r\n    # MaxPool2D wrapper\r\n    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\r\n                          padding=\'SAME\')\r\n\r\n\r\n# Create model\r\ndef conv_net(x, weights, biases, dropout):\r\n    # Reshape input picture\r\n    x = tf.reshape(x, shape=[-1, 28, 28, 1])\r\n\r\n    # Convolution Layer\r\n    conv1 = conv2d(x, weights[\'wc1\'], biases[\'bc1\'])\r\n    # Max Pooling (down-sampling)\r\n    conv1 = maxpool2d(conv1, k=2)\r\n\r\n    # Convolution Layer\r\n    conv2 = conv2d(conv1, weights[\'wc2\'], biases[\'bc2\'])\r\n    # Max Pooling (down-sampling)\r\n    conv2 = maxpool2d(conv2, k=2)\r\n\r\n    # Fully connected layer\r\n    # Reshape conv2 output to fit fully connected layer input\r\n    fc1 = tf.reshape(conv2, [-1, weights[\'wd1\'].get_shape().as_list()[0]])\r\n    fc1 = tf.add(tf.matmul(fc1, weights[\'wd1\']), biases[\'bd1\'])\r\n    fc1 = tf.nn.relu(fc1)\r\n    # Apply Dropout\r\n    fc1 = tf.nn.dropout(fc1, dropout)\r\n\r\n    # Output, class prediction\r\n    out = tf.add(tf.matmul(fc1, weights[\'out\']), biases[\'out\'])\r\n    #\r\n    # out = tf.nn.softmax(out)\r\n\r\n    # out = Dense(n_classes, activation=""softmax"")(fc1)\r\n    return out\r\n\r\n\r\n# Store layers weight & bias\r\nweights = {\r\n    # 5x5 conv, 1 input, 32 outputs\r\n    \'wc1\': tf.Variable(tf.random_normal([5, 5, 1, 32])),\r\n    # 5x5 conv, 32 inputs, 64 outputs\r\n    \'wc2\': tf.Variable(tf.random_normal([5, 5, 32, 64])),\r\n    # fully connected, 7*7*64 inputs, 1024 outputs\r\n    \'wd1\': tf.Variable(tf.random_normal([7 * 7 * 64, 1024])),\r\n    # 1024 inputs, 10 outputs (class prediction)\r\n    \'out\': tf.Variable(tf.random_normal([1024, n_classes]))\r\n}\r\n\r\nbiases = {\r\n    \'bc1\': tf.Variable(tf.random_normal([32])),\r\n    \'bc2\': tf.Variable(tf.random_normal([64])),\r\n    \'bd1\': tf.Variable(tf.random_normal([1024])),\r\n    \'out\': tf.Variable(tf.random_normal([n_classes]))\r\n}\r\n\r\n# Construct model\r\npred = conv_net(x, weights, biases, keep_prob)\r\n\r\n\r\n# categorical_labels = to_categorical(int_labels, num_classes=None)\r\n# Define loss and optimizerK\r\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\r\n\r\ncost = tf.reduce_mean(categorical_crossentropy(tf.nn.softmax(pred), y, from_logits=False))\r\n\r\n\r\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\r\n\r\n# Evaluate model\r\ncorrect_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\r\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n\r\n# Initializing the variables\r\n#init = tf.global_variables_initializer()\r\n\r\n# Launch the graph\r\n#sess.run(init)\r\ninit_op = tf.global_variables_initializer()\r\n\r\nwith sess.as_default():\r\n    sess.run(init_op)\r\n    step = 1\r\n    # Keep training until reach max iterations\r\n    while step * batch_size < training_iters:\r\n        batch_x, batch_y = mnist.train.next_batch(batch_size)\r\n        # Run optimization op (backprop)\r\n        my = sess.run([optimizer, pred], feed_dict={x: batch_x, y: batch_y,\r\n                                       keep_prob: dropout})[1]\r\n        print(my[0])\r\n        if step % display_step == 0:\r\n            # Calculate batch loss and accuracy\r\n\r\n            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\r\n                                                              y: batch_y,\r\n                                                              keep_prob: 1.})\r\n            print(""Iter "" + str(step * batch_size) + "", Minibatch Loss= "" + \\\r\n                  ""{:.6f}"".format(loss) + "", Training Accuracy= "" + \\\r\n                  ""{:.5f}"".format(acc))\r\n        step += 1\r\n    print(""Optimization Finished!"")\r\n\r\n    # Calculate accuracy for 256 mnist test images\r\n    print(""Testing Accuracy:"", \\\r\n          sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\r\n                                        y: mnist.test.labels[:256],\r\n                                        keep_prob: 1.}))\r\n']",[],1,0
99,keras,10632,closed,Low test accuracy with resnet 110 cifar10,"I train a few resnet v2 110 model with cifar10, and only get 92.x% accuracy on the test set. In the table, it shows test accuracy is about 93.x%. I wonder how to get that? 
",,"['I tried the default learning rate, regularization in cifar10_resnet.py, and a few other hyper parameters, still get 92.x% on test set. ', 'It should be replicable if you run GPU and epochs > 120', 'yes, I run on GPU, and epochs=200.', '@roatienza do you mean use the default parameters, n=12, version=2, I can reproduce the 93.x% on the test set by running cifar10_resnet10.py? or do I need to change any hyper parameters? Could you share your hyper parameter settings to reproduce the result? ', 'Yes. `n=12`, `version=2`. The best accuracy is about 93.15% as shown on the table.', '@XinLiuNvidia  increasing `batch_size` to 128 got me to 93.x% accuracy on the test set.', ""@fuzzythecat thank you very much! After sync to the latest keras version, I'm able to reproduce the accuracy 93.x% on the test set. ""]",[],[],1,0
100,keras,11665,closed,Poor memory performance of K.batch_dot under tensorflow backend relative to batched tf.matmul,"- [ x] Check that you are up-to-date with the master branch of Keras. You can update with:


- [ x] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I am performing batch matrix multiplies of two tensors of size (batch, N, M) and (batch, M, K) to get a tensor of size (batch, N, K), with the matrix products. This behavior can be done with both tf.matmul and K.batch_dot with the default axis arguments.

However in K.batch_dot, the elementwise multiplication in the line https://github.com/keras-team/keras/blob/75a35032e194a2d065b0071a9e786adf6cee83ea/keras/backend/tensorflow_backend.py#L1248 eats up a lot of memory. The elementwise multiplication followed by summing over an axis is of course mathematically equivalent to the matrix multiply, but in the two-step implementation, Tensorflow assigns memory to the intermediate very large tensor.

In this simple example, my small GPU (Nvidia 970) is able to perform the calculation using tf.matmul, but using K.batch_dot Tensorflow fails with an OOM error.



This fails when it tries to assign a tensor of size (100, 10000, 500, 32) in the elementwise multiply in batch_dot (the dimension of 10000 not being strictly necessary in this case since we are only interested in the sum).",type:bug/performance,"['Thanks for reporting. Will fix in Keras this Saturday.\r\n', '#11719\r\n']","['\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom keras import backend as K\r\n\r\na = np.random.normal(size=(100, 500, 10000)).astype(np.float32)\r\nb = np.random.normal(size=(100, 10000, 32)).astype(np.float32)\r\n\r\na_t = K.placeholder(a.shape)\r\nb_t = K.placeholder(b.shape)\r\n\r\ntd = tf.matmul(a_t, b_t)\r\nbd = K.batch_dot(a_t, b_t)\r\n\r\nsess = K.get_session()\r\nsess.run(td, feed_dict={a_t: a, b_t: b})\r\nsess.run(bd, feed_dict={a_t: a, b_t: b})\r\n']",['pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps'],1,1
101,keras,3996,closed,Model.fit takes a long time to start,"Using TF with the following model, when I run model.fit it takes about 10 minutes before it actually starts doing anything. [out.profile.zip](https://github.com/fchollet/keras/files/517347/out.profile.zip)
Any ideas?


",,"['For comparison, when I replace all the LSTMs with a single GRU it takes only about 30 seconds to start.\n', 'Hmm, just updated to Keras 1.1.0 and CUDNN 5.1 and the problem seems to have disappeared.\n']","["" py\ndef make_ixm_lstm(n_frame=100,\n                  n_freq=276,\n                  n_layers=4,\n                  n_units=300,\n                  n_bins=10,\n                  loss='mean_squared_error',\n                  eps=0.1,\n                  verbose=False):\n    inputs = Input(shape=(n_frame, n_freq))\n    lstm = LSTM(n_units, return_sequences=True)(inputs)\n    for _ in range(n_layers - 1):\n        lstm = LSTM(n_units, return_sequences=True)(lstm)\n    lstm_out = LSTM(n_bins, return_sequences=True)(lstm)\n    sigmoid_out = Activation('sigmoid')(lstm_out)\n    model = Model(input=inputs, output=sigmoid_out)\n    model.compile(optimizer=Adam(), loss=loss, metrics=['binary_accuracy',],)\n    print 'DONE MAKING LSTM MODEL'\n    return model\n""]",[],1,1
102,keras,13212,closed,The accuracy on the validation data is not the same during training and prediction,"The data are loaded with . I watched this issue https://github.com/keras-team/keras/issues/3477, and applied the tweaks:
- No shuffle
- Good order of the images on the FS
- Remove transformations

I don't have a test dataset and want to print the result of the prediction to draw the confusion matrix. I use the same datas (Validation) during the training and the prediction.

During the training, the model achieve 91.5% of accuracy. The  gives me the same accuracy. But during the prediction, I have a completely different accuracy, which is only 86%.

Training Python code:


Evaluate python code:


Prediction python code:


It runs on jupyter notebook with python 3. Is it an issue or am I missing something ?",,"['I found the problem. It has something to do with the loss function. During the training, I use `binary_crossentropy` for the loss function. But during the prediction, I compute the categorical accuracy...']","['Python\r\nvalid_datagen = ImageDataGenerator(rescale=1./255)\r\nvalid_generator = valid_datagen.flow_from_directory(\r\n    directory=VALI_PATH,\r\n    target_size=(IMG_SIZE, IMG_SIZE),\r\n    color_mode=COLOR,\r\n    batch_size=batch_size,\r\n    class_mode=CLASS_MODE,\r\n    shuffle=False\r\n)\r\nSTEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size+1\r\n', 'Python\r\nlog = model.fit_generator(generator=train_generator,\r\n                    steps_per_epoch=STEP_SIZE_TRAIN,\r\n                    validation_data=valid_generator,\r\n                    validation_steps=STEP_SIZE_VALID,\r\n                    epochs=epochs,\r\n                    callbacks=callbacks_list,\r\n                    verbose=1)\r\n', ""Python\r\nscores = model.evaluate_generator(generator=valid_generator, steps=STEP_SIZE_VALID, verbose=1)\r\nprint('Validating loss:', scores[0])\r\nprint('Validating accuracy:', scores[1])\r\n"", 'Python\r\nvalid_generator.reset()\r\npred=model.predict_generator(valid_generator,\r\n                            steps=STEP_SIZE_VALID,\r\n                            verbose=1)\r\ntemp = sum(valid_generator.classes == pred.argmax(axis=1))\r\naccuracy = 100*(temp/valid_generator.n)\r\nprint(""Test accuracy: %.4f%%"" % accuracy)\r\n']","['flow_from_directory', 'model.evalutate_generator']",1,0
103,keras,1438,closed,"Simplify relu(x, alpha=0., max_value=None) impl in tensorflow_backend.py","In a discussion in https://groups.google.com/a/tensorflow.org/d/msg/discuss/V6aeBw4nlaE/VUAgE-nXEwAJ Mark Daoust suggested an impl for leaky relu as:



Wouldn't be better to change the current impl from:



to something like:



I think the two are identical at least for alpha <= 1.

Is it preferable to use tf.nn.relu because it will be faster?

In one place you use tf.constant to cast alpha to the correct type and in other place you use tf.cast  to cast max_value. Which one is better or are they equivalent?

Thanks
",stale,[],"[' python\ndef relu(x, alpha=0., max_value=None):\n    negative_part = tf.nn.relu(-x)\n    x = tf.nn.relu(x)\n    if max_value is not None:\n        x = tf.clip_by_value(x, tf.cast(0., dtype=_FLOATX),\n                             tf.cast(max_value, dtype=_FLOATX))\n    x -= tf.constant(alpha, dtype=_FLOATX) * negative_part\n    return x\n', ' python\ndef relu(x, alpha=0., max_value=None):\n    x = tf.maximum(tf.cast(alpha, dtype=_FLOATX)*x, x)\n    if max_value is not None:\n        x = tf.minimum(tf.cast(max_value, dtype=_FLOATX), x)\n    return x\n']","['tf.maximum(alpha*x,x)']",1,0
104,keras,415,closed,slow training of LSTM,"Hi, I just started using keras. Awesome work!
I tried to use LSTM with the following code



And I compared the efficiency with [char-rnn](https://github.com/karpathy/char-rnn), and found that the implementation in keras is about 4 times slower than Karpathy's (with the same batchsize).
Am I doing something wrong? 
I've attached the theano profile [result](https://gist.github.com/ffmpbgrnn/842e1910f216b1e00e27)
Thanks you!
",,"['Training time is heavily dependent on network size and batch size. Is it even the same network (size included) at all?\n\nAlso, time_distributed_softmax is deprecated now, use softmax instead.\n', 'Yes. I tested with one layer LSTM with the same sequence length, memory cell size, input dimension, output dimension and batch size as well.\nThank you for pointing out the depreacated softmax usage.\n', 'From the [profile](https://gist.github.com/ffmpbgrnn/842e1910f216b1e00e27), it seems it spent much time on operation: `Elemwise{Composite{(i0 * log((i1 / i2)))}}`\n', 'And what batch size are you using?\n', ""I'm using the batch size of 128.\n"", '> From the profile, it seems it spent much time on operation: `Elemwise{Composite{(i0 * log((i1 / i2)))}}`\n\nThanks for the info. Try setting the `activation` and `inner_activation` parameters to `theano.tensor.nnet.ultra_fast_sigmoid`, and tell me what you get.\n', 'Hi, in `activations.py` I added\n\n``` python\ndef ultra_fast_sigmoid(x):                                                                                                                                                \n    return T.nnet.ultra_fast_sigmoid(x)\n```\n\nand rebuilt keras and then call LSTM with `LSTM(4096, 512, return_sequences=True, inner_activation=\'ultra_fast_sigmoid\')`\n\nHowever, it failed to compile with the error:\n\n```\nUsing gpu device 1: Tesla K20m\nBuild model...\nTraceback (most recent call last):\n  File ""fc_rnn.py"", line 40, in <module>\n    model.compile(loss=\'categorical_crossentropy\', optimizer=\'rmsprop\')\n  File ""build/bdist.linux-x86_64/egg/keras/models.py"", line 316, in compile\n  File ""build/bdist.linux-x86_64/egg/keras/optimizers.py"", line 75, in get_updates\n  File ""build/bdist.linux-x86_64/egg/keras/optimizers.py"", line 24, in get_gradients\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/gradient.py"", line 547, in grad\n    grad_dict, wrt, cost_name)\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/gradient.py"", line 1288, in _populate_grad_dict\n    rval = [access_grad_cache(elem) for elem in wrt]\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/gradient.py"", line 1246, in access_grad_cache\n    term = access_term_cache(node)[idx]\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/gradient.py"", line 953, in access_term_cache\n    output_grads = [access_grad_cache(var) for var in node.outputs]\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/gradient.py"", line 1246, in access_grad_cache\n    term = access_term_cache(node)[idx]\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/gradient.py"", line 953, in access_term_cache\n    output_grads = [access_grad_cache(var) for var in node.outputs]\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/gradient.py"", line 1246, in access_grad_cache\n    term = access_term_cache(node)[idx]\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/gradient.py"", line 953, in access_term_cache\n    output_grads = [access_grad_cache(var) for var in node.outputs]\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/gradient.py"", line 1246, in access_grad_cache\n    term = access_term_cache(node)[idx]\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/gradient.py"", line 953, in access_term_cache\n    output_grads = [access_grad_cache(var) for var in node.outputs]\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/gradient.py"", line 1246, in access_grad_cache\n    term = access_term_cache(node)[idx]\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/gradient.py"", line 953, in access_term_cache\n    output_grads = [access_grad_cache(var) for var in node.outputs]\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/gradient.py"", line 1246, in access_grad_cache\n    term = access_term_cache(node)[idx]\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/gradient.py"", line 953, in access_term_cache\n    output_grads = [access_grad_cache(var) for var in node.outputs]\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/gradient.py"", line 1246, in access_grad_cache\n    term = access_term_cache(node)[idx]\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/gradient.py"", line 953, in access_term_cache\n    output_grads = [access_grad_cache(var) for var in node.outputs]\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/gradient.py"", line 1246, in access_grad_cache\n    term = access_term_cache(node)[idx]\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/gradient.py"", line 953, in access_term_cache\n    output_grads = [access_grad_cache(var) for var in node.outputs]\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/gradient.py"", line 1246, in access_grad_cache\n    term = access_term_cache(node)[idx]\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/gradient.py"", line 1093, in access_term_cache\n    input_grads = node.op.grad(inputs, new_output_grads)\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/scan_module/scan_op.py"", line 1851, in grad\n    _dC_dinps_t = compute_gradient(Xt, dC_dXt)\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/scan_module/scan_op.py"", line 1783, in compute_gradient\n    return_disconnected=\'None\')\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/gradient.py"", line 547, in grad\n    grad_dict, wrt, cost_name)\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/gradient.py"", line 1288, in _populate_grad_dict\n    rval = [access_grad_cache(elem) for elem in wrt]\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/gradient.py"", line 1246, in access_grad_cache\n    term = access_term_cache(node)[idx]\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/gradient.py"", line 953, in access_term_cache\n    output_grads = [access_grad_cache(var) for var in node.outputs]\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/gradient.py"", line 1246, in access_grad_cache\n    term = access_term_cache(node)[idx]\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/gradient.py"", line 1093, in access_term_cache\n    input_grads = node.op.grad(inputs, new_output_grads)\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/tensor/elemwise.py"", line 672, in grad\n    rval = self._bgrad(inputs, ograds)\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/tensor/elemwise.py"", line 747, in _bgrad\n    scalar_igrads = self.scalar_op.grad(scalar_inputs, scalar_ograds)\n  File ""/home/archy/.local/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/scalar/basic.py"", line 890, in grad\n    self.__class__.__name__)\ntheano.gof.utils.MethodNotDefined: (\'grad\', <class \'theano.tensor.nnet.sigm.UltraFastScalarSigmoid\'>, \'UltraFastScalarSigmoid\')\n```\n\nI\'m new to theano and have no idea how to fix this. And I\'m running with `THEANO_FLAGS=mode=FAST_RUN,device=gpu1,floatX=float32,nvcc.fastmath=True,allow_gc=False python rnn.py`.\nMaybe there\'s something wrong in [this function](https://github.com/Theano/Theano/blob/master/theano/tensor/nnet/sigm.py#L91)?\n', 'Hi, I replaced both `sigmoid` and `tanh` activations with `linear`, and get the [profile](https://gist.github.com/ffmpbgrnn/842e1910f216b1e00e27#file-linear-log).\n', ""I tried running the same network, but I'm getting completely different profiles (on CPU): \n\n```\nOps\n---\n<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>\n  44.8%    44.8%      18.078s       9.42e-02s     C      192       12   Dot22\n  14.3%    59.1%       5.754s       3.60e-01s     Py      16        1   forall_inplace,cpu,grad_of_scan_fn}\n   6.2%    65.3%       2.503s       1.56e-01s     C       16        1   Elemwise{Composite{(i0 * log((i1 / i2)))}}\n   4.9%    70.1%       1.960s       1.02e-02s     C      192       12   Elemwise{Composite{(i0 - ((i1 * i2) / sqrt((i3 + i4 + i5))))}}\n   4.8%    74.9%       1.937s       1.21e-01s     Py      16        1   forall_inplace,cpu,scan_fn}\n   3.7%    78.7%       1.503s       1.88e-02s     C       80        5   Sum{axis=[0, 1], acc_dtype=float64}\n```\n"", 'I profiled on CPU as well, and the [CPU profile](https://gist.github.com/ffmpbgrnn/842e1910f216b1e00e27#file-cpu-log-L42) also shows that the  `Dot22` costs most\n', 'i noticed the result here is in a different, larger regime than i\'ve been operating in. so i wanted to share a result that is more in the range that most people have been running char-rnn.\n\non my machine (nvidia 750m) for the first epoch (228 minibatches) i see:\n- char-rnn is 0.20s/minibatch\n- keras is 0.28s/minibatch\n\nthis is using the following parameters (based on char-rnn\'s defaults):\n- using the nietzsche dataset\n- 128 units per layers\n- 2 layers\n- 2e-3 learning rate\n- 0.95 decay rate for rmsprop\n- 50 token sequence length\n- 50 token step size (no overlap)\n- 50 ""sentence"" batch size\n- gradient clip at 5\n- no dropout (dropout did not seem to have a significant effect for char-rnn or keras) \n\ni haven\'t done any profiling. this is just based on modifying `lstm_text_generation.py` to more closely match char-rnn https://github.com/kylemcdonald/keras/blob/char_rnn/examples/char_rnn.py\n\ni also haven\'t compared the loss curves between keras and char-rnn after a fixed number of iterations. i\'ll try that next. presumably they will vary a bit based on other internal parameters...\n', '@kylemcdonald , in the original version of [char-rnn](https://github.com/karpathy/char-rnn/), it predicts character by character, not predicting the next character of a sequence (in your case 50). Your version of `# the last layer has return_sequences=False` might be not comparable to the original char-rnn?\n', '> 50 ""sentence"" batch size\n\nPlaying around with the batch size might allow you to get some speedup on GPU. Batch sizes that are too small mean that a lot of time will be spent moving minibatch data to and from the GPU memory.\n\n> Your version of # the last layer has return_sequences=False might be not comparable to the original char-rnn?\n\nSo how does char-rnn work exactly? I must say I\'ve never looked at it. In any case I would not expect much computational difference between `return_sequences=False` and `return_sequences=True` since in both cases the full sequence is being computed anyway. The computation steps are the exact same. \n', '@ffmpbgrnn regarding `return_sequences`, this could also be done with `True` for all layers and then using a non-zero index on the `predict()` result to get the last index. i don\'t think it\'s really changing anything, as @fchollet explains above.\n\nbut i think i understand what you\'re saying about the prediction being different. char-rnn samples one character at a time without resetting the state of the network. the keras lstm example, however, feeds in a `maxlen` sequence for every step, presumably resetting the state of the net before each prediction? meaning any relationships longer than `maxlen` will absolutely not be captured. (if `fit()` is also resetting the state of the network, this is a moot point.)\n\ni tried modifying the prediction process to see if anything happens:\n\n``` python\n        x = np.zeros((1, len(sentence), len(chars)))\n        for t, char in enumerate(sentence):\n            x[0, t, char_indices[char]] = 1.\n        preds = model.predict(x, verbose=0)[0]\n        next_index = sample(preds, diversity)\n        next_char = indices_char[next_index]\n        sys.stdout.write(next_char)\n\n        for iteration in range(400):\n            x = np.zeros((1, 1, len(chars)))\n            x[0, 0, next_index] = 1.\n\n            preds = model.predict(x, verbose=0)[0]\n            next_index = sample(preds, diversity)\n            next_char = indices_char[next_index]\n...\n```\n\nbut this just spits out complete gibberish. this makes me think the state is reset for every `predict()`.\n\nhere is the comparison of the loss curves:\n\n<img width=""604"" alt=""screen shot 2015-07-25 at 00 20 25"" src=""https://cloud.githubusercontent.com/assets/157106/8887863/2b852bb6-3263-11e5-94ef-78f8436de829.png"">\n\nkeras drops very low, and char-rnn bottoms out somewhere around epoch 20 (i don\'t have accuracy numbers for char-rnn).\n\nbut this graph doesn\'t seem to capture what\'s actually being learned. there\'s definitely something different that char-rnn is doing compared to the keras lstm example. after 20 epochs, char-rnn prints:\n\n```\n$ th sample.lua cv/lstm_19.00e_2x128_0.00d_50_1.4290l.t7 -temperature 1\nusing CUDA on GPU 0...  \ncreating an LSTM... \nmissing seed text, using uniform probability over first character   \n--------------------------  \ngod are questions has not already capable of the inner favourite confess or wither finders and delights--and HOPEOTOL, and then seeks he is always was even for his keens to have growt in all the science, all, in the soul of--home in so Asuated equalize, more ppodenus, that\nexistence, and are natured to forth and something of the lake\nreligious spoten of heaused by merit\nof the desire them--our ofteic plumsing expluaring heredatical, relicy also, and more\ndeceivative\nciriums, ""I purpfective excitable, it is note-side any greatred of reason\nwhen consequently\n```\n\netc.\n\nbut keras prints:\n\n```\n----- diversity: 1.0\n----- Generating with seed: ""o rich in futurity. This kind of music\nexpresses b""\no rich in futurity. This kind of music\nexpresses be intlest it of the to surm of the mort eans and no\nbtill se paa antate\nfark, ind woord peans. \nW\n\n\nPoWPo)t -eeds aptel cist of beevo aloxhe coatica abol?ess, and to tame of thes to purf so:t arr-istmentated therrerepe then wath imsold to tweresce is is and in wa kesove to as mort that elustion. Nut whac allest a sukould and Bglol,. -whe worde dears\nor the world, and jorbend selt atse ald alags\no\n```\n\nto me, the keras output looks a lot like the char-rnn output before the network is ""warmed up"" by outputting enough data. compare how many words are ""real"" between the two samples, and the distance between spaces.\n\nin summary: if there is a way to run `fit()` and `predict()` without resetting the state of the network (which is presumably happening behind the scenes, but i can\'t find where) then the results might be more similar to char-rnn. and regarding this issue, it could explain the relative speed of the sampling compared to char-rnn, but not the speed of `fit()`.\n\nfinally: thanks @fchollet for the great work on this library, for me it\'s at the perfect level of abstraction for the level i\'m at right now, and i love the way it\'s designed with callbacks and lambda functions where necessary :)\n', ""@kylemcdonald it's an open issue: https://github.com/fchollet/keras/issues/98#issuecomment-122776799\n\nI expect we'll support it within the next couple weeks. I don't think it would be hard to do, we just need somebody to look into it and, you know, you just do it. I've been busy with other things so far.\n"", ""aha, great! i read through #98 hoping to be able to help, but it seems that implementing stateful training & prediction requires a deeper understanding of theano than i have right now. so i'll step back and watch from the sidelines :)\n""]","["" python\nmodel = Sequential()\nmodel.add(LSTM(4096, 512, return_sequences=True))                                                                                                                \nmodel.add(TimeDistributedDense(512, 4096))\nmodel.add(Activation('time_distributed_softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n""]",[],1,0
105,keras,9122,closed,Increased GPU memory usage after restoring saved model,"I am trying to train a simple CNN on windows with tensorflow as backend on windows. My inputs are quite large (256x256) but I can train the network on my GTX 1080.

Once the model has been trained and saved, I get OOM errors when restoring it. In order to be able to resume training, I need to divide my batch size by 4.

Is there a reason for the increased memory usage after reloading the model and a way to avoid it? 


[1st_run_output.txt](https://github.com/keras-team/keras/files/1645667/1st_run_output.txt)
[2nd_run_output.txt](https://github.com/keras-team/keras/files/1645668/2nd_run_output.txt)

",,"['The first time you are creating only one model and training it. The second time you are creating 2 models and training one model. This could be the reason for increased memory usage. Can you create the model only when saved model is not available?\r\n\r\n```\r\nif os.path.isfile(filepath):\r\n    model = keras.models.load_model(filepath)\r\nelse:\r\n    model = Sequential() \r\n    ...\r\n```', 'Thanks for the explanation and sorry for the noise, I misunderstood the behaviour of `load_model`. ']","['\r\nfrom __future__ import print_function\r\nimport keras\r\nfrom keras.datasets import mnist\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout, Flatten\r\nfrom keras.layers import Conv2D, MaxPooling2D\r\nfrom keras import backend as K\r\nimport numpy as np\r\nimport os\r\n\r\nbatch_size = 64\r\nnum_classes = 10\r\nimg_rows, img_cols = 256, 256\r\n\r\nx_train = np.random.rand(1024,img_rows,img_cols,1)\r\ny_train = np.random.rand(1024,num_classes)\r\nx_test = np.random.rand(512,img_rows,img_cols,1)\r\ny_test = np.random.rand(512,num_classes)\r\ninput_shape = (img_rows, img_cols, 1)\r\n\r\nx_train = x_train.astype(\'float32\')\r\nx_test = x_test.astype(\'float32\')\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(32, kernel_size=(3, 3),\r\n                 activation=\'relu\',\r\n                 input_shape=input_shape))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Conv2D(64, (3, 3), activation=\'relu\'))\r\nmodel.add(Dropout(0.25))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(128, activation=\'relu\'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(num_classes, activation=\'softmax\'))\r\n\r\nmodel.compile(loss=keras.losses.categorical_crossentropy,\r\n              optimizer=keras.optimizers.Adadelta(),\r\n              metrics=[\'accuracy\'])\r\n\r\nfilepath=""./test.h5""\r\nif os.path.isfile(filepath): keras.models.load_model(filepath)\r\n\r\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=3,\r\n          verbose=1, validation_data=(x_test, y_test))\r\nscore = model.evaluate(x_test, y_test, verbose=0)\r\nprint(\'Test loss:\', score[0])\r\nprint(\'Test accuracy:\', score[1])\r\n\r\nmodel.save(filepath) \r\n\r\n']",[],1,0
106,keras,933,closed,Same model but graph gives bad performance,"Hello, 

I am learning to use Graph as it seems more powerful so I implemented one of my previous model which uses Sequential. Here is the model using sequential (number of dimension set in random):



The model works fine and below is my reimplementation using Graph:



My impression is that they are exactly the same model (grateful if somebody spotted something wrong there). But the model based on Graph gives a loss of 3.6 while the loss for the other one is around 0.002. 

Is there a reason for this please ?

Thank you for your help
",,"[""Hello, \n\nI tested with another optimizer sgd, things don't seem to work either. Changing LSTM to SimpleRNN doesn't help either. Is it because Graph doesn't work with sequential model ? (I looked into test and keras doesn't test graph with sequential models) But I don't see it in the documentation though... \n"", ""It is a simply bug. \n\nself.model.add_output(name='output', input='merge') is wrong. The input is the 'activation'.\n\nThanks for developers adding examples about using Graph. \n""]","['\ndef build_generation_embedding_model(self, dim):\n    print ""Build model ...""\n    input_model = Sequential()\n    input_model.add(TimeDistributedDense(dim, input_shape=(10,10)))\n    input_model.add(LSTM(dim, return_sequences=False))\n    input_model.add(Dense(dim))\n    canonical_model = Sequential()\n    canonical_model.add(TimeDistributedDense(dim, input_shape=(15,15)))\n    canonical_model.add(LSTM(dim, return_sequences=False))\n    canonical_model.add(Dense(dim))\n    self.model = Sequential()\n    self.model.add(Merge([input_model, canonical_model], mode=\'concat\'))\n    self.model.add(Dense(15))\n    self.model.add(Activation(\'softmax\'))\n    self.model.compile(loss=\'categorical_crossentropy\', optimizer=\'rmsprop\')\n', ""\ndef build_generation_embedding_model_graph(self, dim):\n    self.model = Graph()\n    self.model.add_input(name='input1', input_shape=(10,10))\n    self.model.add_input(name='canonical', input_shape=(15,15))\n    self.model.add_node(TimeDistributedDense(dim), name='Embed_input1', input='input1')\n    self.model.add_node(TimeDistributedDense(dim), name='Embed_canonical', input='canonical')\n    self.model.add_node(LSTM(dim, return_sequences=False), name='Hidden_input1', input='Embed_input1')\n    self.model.add_node(LSTM(dim, return_sequences=False), name='Hidden_canonical', input='Embed_canonical')\n    self.model.add_node(Dense(15), name='merge', inputs=['Hidden_input1','Hidden_canonical'], merge_mode='concat')\n    self.model.add_node(Activation('softmax'), name='activation', input='merge')\n    self.model.add_output(name='output', input='merge')\n    self.model.compile('rmsprop', {'output':'categorical_crossentropy'})\n""]",[],1,0
107,keras,11692,closed,Slow Learning and Overfitting - CNN,"My CNN model learning very slow after some epochs and testing accuracy is not increasing. I'm using **learning rate = 0.0003** and **dropout = 0.55**. **Batch Size = 50**, **Training Samples = 8000** grayscale images and **Testing Samples = 1600** grayscale images. The model is given below. 



![image](https://user-images.githubusercontent.com/17239812/48775610-61fb5400-ecef-11e8-9cc3-57ee1df8f657.png)

How do I increase training and testing accuracy?

![image](https://user-images.githubusercontent.com/17239812/48775842-0aa9b380-ecf0-11e8-908a-3f6df616d0e4.png)

This is the accuracy and loss I'm getting after 30th epoch. Training accuracy is on 0.77 for last 5 epochs!!",type:support,"['Clearly you are overfitting your model, so you have to apply some kind of regularization. I\'d try ""image augmentation"" since you\'re dealing with images. In addition to that there are several things I\'d change.\r\nFirst of all, it seems you\'re trying to get a binary clasification model, so the mean squared error might not be the optimal loss, try binary_crossentropy. The Dropout after Flatten makes no sense, remove it and try image augmentation as I said. If you want to add the dropout layer, do it between the Dense ones. \r\nThe slow learning it is because you\'re using a small learning rate with ADAM, try a bigger one and let ADAM adapt the LR by itself.']","['\r\nmodel = Sequential()\r\nmodel.add(Conv2D(kernel_size=(3,3),filters=12,input_shape=(64, 64, 1),activation=""relu"",padding=""valid""))\r\nmodel.add(Conv2D(kernel_size=(3,3),filters=10,activation=""relu"",padding=""same""))\r\nmodel.add(MaxPooling2D(pool_size=(2,2),strides=(1,1)))\r\nmodel.add(Conv2D(kernel_size=(3,3),filters=4,activation=""relu"",padding=""same""))\r\nmodel.add(Conv2D(kernel_size=(2,2),filters=5,activation=""relu"",padding=""same""))\r\nmodel.add(MaxPooling2D(pool_size=(3,3),strides=(1,1)))\r\nmodel.add(Conv2D(kernel_size=(3,3),filters=5,activation=""relu"",padding=""same""))\r\nmodel.add(Flatten())\r\nmodel.add(Dropout(0.55))\r\nmodel.add(Dense(100,activation=""sigmoid""))\r\nmodel.add(Dense(1,activation=\'sigmoid\'))\r\nmodel.summary()\r\n\r\nmodel.compile(loss=keras.losses.mean_squared_error,\r\n              optimizer=keras.optimizers.Adam(lr=0.0003),\r\n              metrics=[\'accuracy\'])\r\n']",[],1,0
108,keras,8114,closed,"Prediction score extremely low, despite high (~94%) validation accuracy with bottleneck features (VGG16)","Hi there,

I'm trying to utilise ""bottleneck features"" produced from VGG16 for a new classification task with limited data.

Right now, I have the following (please, excuse code quality):



Which returns the following output:

ModelModel(outputs=Softmax.0, inputs=/input_3)

I'm fairly accustom to Keras and deep learning in general, but I've yet to use this ""bottleneck"" technique. I believe the problem is likely to be related to the corresponding of predicted output to informative labels, but I'm struggling to find a solution.

Any help would be fantastic!

Thanks,

Keiron.",,[],"['python\r\nimport numpy as np\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.applications import VGG16\r\nfrom PIL import Image\r\nfrom keras.preprocessing import image\r\nfrom keras.utils import to_categorical\r\nfrom keras.models import Sequential, Model\r\nfrom keras.layers import Dropout, Flatten, Dense, Input\r\nfrom sklearn.metrics import classification_report\r\nimport os\r\n\r\n\r\nheight = 150\r\nwidth = 150\r\nbatch_size = 16\r\n\r\ntrain_dir = ""/home/keo7/Data/PVS/train/""\r\nval_dir = ""/home/keo7/Data/PVS/val/""\r\ntest_dir = ""/home/keo7/Data/PVS/test/""\r\n\r\nnb_train_samples = 6000\r\nnb_validation_samples = 1000\r\n\r\n\r\ndef get_bottleneck_features():\r\n    data_generator = ImageDataGenerator(rescale=1. / 255)\r\n\r\n    vgg_16 = VGG16(include_top=False, weights=""imagenet"")\r\n\r\n    generator = data_generator.flow_from_directory(\r\n        train_dir,\r\n        target_size=[width, height],\r\n        batch_size=batch_size,\r\n        class_mode=None,\r\n        shuffle=False\r\n    )\r\n\r\n    train_features = vgg_16.predict_generator(generator,\r\n                                              nb_train_samples // batch_size,\r\n                                              verbose=1)\r\n    np.save(""features_train.npy"", train_features)\r\n\r\n    generator = data_generator.flow_from_directory(val_dir,\r\n                                                   target_size=[width, height],\r\n                                                   batch_size=batch_size,\r\n                                                   class_mode=None,\r\n                                                   shuffle=False)\r\n\r\n    val_features = vgg_16.predict_generator(generator, nb_validation_samples // batch_size)\r\n    np.save(""features_val.npy"", val_features)\r\n\r\n    return generator.class_indices\r\n\r\n\r\ndef train_top_model():\r\n    train_data = np.load(""features_train.npy"")\r\n    train_labels = np.array([[0] * 600, [1] * 600, [2] * 600, [3] * 600, [4] * 600,\r\n                             [5] * 600, [6] * 600, [7] * 600, [8] * 600, [9] * 600])\r\n    train_labels_one_hot = to_categorical(train_labels, 10)\r\n\r\n    validation_data = np.load(""features_val.npy"")\r\n    validation_labels = np.array([[0] * 100, [1] * 100, [2] * 100, [3] * 100, [4] * 100,\r\n                                  [5] * 100, [6] * 100, [7] * 100, [8] * 100, [9] * 100])\r\n    validation_labels_one_hot = to_categorical(validation_labels, 10)\r\n\r\n    model = Sequential()\r\n    model.add(Flatten(input_shape=train_data.shape[1:]))\r\n    model.add(Dense(256, activation=""relu""))\r\n    model.add(Dropout(0.5))\r\n    model.add(Dense(len(train_labels), activation=""softmax""))\r\n\r\n    model.compile(optimizer=\'rmsprop\', loss=\'categorical_crossentropy\', metrics=[\'accuracy\'])\r\n\r\n    model.fit(train_data, train_labels_one_hot[:len(train_data)],\r\n              epochs=5,\r\n              batch_size=batch_size,\r\n              validation_data=(validation_data, validation_labels_one_hot[:len(validation_data)])\r\n              )\r\n\r\n    model.save_weights(""fc_model.h5"")\r\n\r\ndef create_final_model():\r\n    base_model = VGG16(include_top=False, weights=\'imagenet\', input_shape=(150, 150, 3))\r\n    top_model = Sequential()\r\n    top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\r\n    top_model.add(Dense(256, activation=\'relu\'))\r\n    top_model.add(Dropout(0.5))\r\n    top_model.add(Dense(10, activation=\'softmax\'))\r\n    top_model.load_weights(""fc_model.h5"")\r\n\r\n    final_input = Input(shape=(150, 150, 3))\r\n    x = base_model(final_input)\r\n    result = top_model(x)\r\n    return Model(input=final_input, output=result)\r\n\r\ndef predict_image_class(test_image, classes, final_model):\r\n    img = Image.open(test_image)\r\n    img = img.resize((height, width))\r\n    x = image.img_to_array(img)\r\n    x = np.expand_dims(x, axis=0)\r\n    preds = final_model.predict(x / 255)[0]\r\n    return classes[np.argmax(preds)]\r\n\r\nif __name__ == ""__main__"":\r\n\r\n    get_bottleneck_features()\r\n    train_top_model()\r\n\r\n    classes = []\r\n    train_path = train_dir\r\n    for item in os.listdir(train_path):\r\n        if os.path.isdir(os.path.join(train_path, item)):\r\n            if item not in classes:\r\n                classes.append(item)\r\n\r\n\r\n    y_true = []\r\n    y_pred = []\r\n\r\n    final_model = create_final_model()\r\n\r\n    for d, sd, _ in os.walk(train_dir):\r\n        for s in sd:\r\n            for file in os.listdir(os.path.join(d, s)):\r\n                prediction = predict_image_class(os.path.join(d,s,file), classes, final_model)\r\n                y_true.append(s)\r\n                y_pred.append(prediction)\r\n\r\n    print classification_report(y_true, y_pred)\r\n']","['', '\r\n/home/keo7/.virtualenvs/deeplearning/bin/python /home/keo7/Projects/AgriDoctor/classifier/test.py\r\nUsing Theano backend.\r\nUsing cuDNN version 6021 on context None\r\nMapped name None to device cuda: GeForce GTX 1060 (0000:01:00.0)\r\nFound 6000 images belonging to 10 classes.\r\n  1/375 [..............................] - ETA: 144s\r\n...\r\n375/375 [==============================] - 80s    \r\nFound 1000 images belonging to 10 classes.\r\nTrain on 6000 samples, validate on 992 samples\r\nEpoch 1/5\r\n  16/6000 [..............................] - ETA: 0s - loss: 2.9712 - acc: 0.1250\r\n...\r\n6000/6000 [==============================] - 0s - loss: 0.9426 - acc: 0.7107 - val_loss: 0.3441 - val_acc: 0.8810\r\nEpoch 2/5\r\n  16/6000 [..............................] - ETA: 0s - loss: 0.6666 - acc: 0.7500\r\n...\r\n6000/6000 [==============================] - 0s - loss: 0.4603 - acc: 0.8427 - val_loss: 0.2814 - val_acc: 0.8931\r\nEpoch 3/5\r\n  16/6000 [..............................] - ETA: 0s - loss: 0.6545 - acc: 0.8750\r\n...\r\n6000/6000 [==============================] - 0s - loss: 0.3573 - acc: 0.8798 - val_loss: 0.1961 - val_acc: 0.9244\r\nEpoch 4/5\r\n  16/6000 [..............................] - ETA: 0s - loss: 0.1804 - acc: 0.9375\r\n...\r\n6000/6000 [==============================] - 0s - loss: 0.3017 - acc: 0.9062 - val_loss: 0.2160 - val_acc: 0.9214\r\nEpoch 5/5\r\n  16/6000 [..............................] - ETA: 0s - loss: 0.1237 - acc: 0.9375\r\n ...\r\n6000/6000 [==============================] - 0s - loss: 0.2462 - acc: 0.9218 - val_loss: 0.1746 - val_acc: 0.9456\r\n/home/keo7/Projects/AgriDoctor/classifier/test.py:97: UserWarning: Update your ', ' call to the Keras 2 API: ', '\r\n  return Model(input=final_input, output=result)\r\n\r\n             precision    recall  f1-score   support\r\n\r\n       c_15       0.00      0.00      0.00       600\r\n       c_16       0.00      0.00      0.00       600\r\n       c_24       0.00      0.00      0.00       600\r\n       c_25       0.00      0.00      0.00       600\r\n       c_28       0.00      0.00      0.00       600\r\n        c_3       0.00      0.00      0.00       600\r\n       c_30       0.00      0.00      0.00       600\r\n       c_32       0.00      0.00      0.00       600\r\n       c_33       0.00      0.00      0.00       600\r\n       c_35       0.00      0.00      0.00       600\r\n\r\navg / total       0.00      0.00      0.00      6000\r\n', '']",1,0
109,keras,3508,closed,Very low accuracy in the mnist_cnn when running on a GPU using tensorflow  backend,"hi everyone:
   I tried all the examples of MNIST and when i use cpu all of them are work well (tensorflow and theano all tried), but **when i use gpu (tensorflow  backend) mnist_cnn.py is not working, it has very low accuracy**, i guess there's some wrong with cnn on gpu.
([issue 511](https://github.com/fchollet/keras/issues/511) can not solve the problem.)
## my configuration is:
### 1、.keras/keras.json

{
    ""image_dim_ordering"": ""tf"",
    ""epsilon"": 1e-05,
    ""floatx"": ""float32"",
    ""backend"": ""tensorflow""
}
### 2、source code is changed like this：

line 35：X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)
line 36：X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)
line 51：model.add(Convolution2D(nb_filters, nb_conv, nb_conv,
line 52：                          border_mode='valid',
line 53：                          input_shape=(img_rows, img_cols, 1)))
### 3、keras version：1.0.7,  GPU:Geforce GTX1070, Cuda toolkit version：7.5, cuDNN version：6.5，tensorflow version：0.9r
## detail information
### 1、on gpu

(tensorflow):~/dllearning/mnist$ python mnist_cnn.py 
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
Using TensorFlow backend.
X_train shape: (60000, 28, 28, 1)
60000 train samples
10000 test samples
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:924] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.7845
pciBusID 0000:01:00.0
Total memory: 7.92GiB
Free memory: 1.93GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)
Train on 60000 samples, validate on 10000 samples
Epoch 1/12
60000/60000 [==============================] - 12s - loss: 2.3065 - acc: 0.1103 - val_loss: 2.3011 - val_acc: 0.1137
Epoch 2/12
60000/60000 [==============================] - 6s - loss: 2.3022 - acc: 0.1122 - val_loss: 2.3011 - val_acc: 0.1135
Epoch 3/12
60000/60000 [==============================] - 6s - loss: 2.3014 - acc: 0.1123 - val_loss: 2.3011 - val_acc: 0.1135
Epoch 4/12
60000/60000 [==============================] - 6s - loss: 2.3025 - acc: 0.1121 - val_loss: 2.3010 - val_acc: 0.1135
Epoch 5/12
60000/60000 [==============================] - 5s - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3010 - val_acc: 0.1135
Epoch 6/12
60000/60000 [==============================] - 5s - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3010 - val_acc: 0.1135
Epoch 7/12
60000/60000 [==============================] - 6s - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3021 - val_acc: 0.1135
Epoch 8/12
60000/60000 [==============================] - 6s - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3017 - val_acc: 0.1135
Epoch 9/12
60000/60000 [==============================] - 6s - loss: 2.3013 - acc: 0.1123 - val_loss: 2.3010 - val_acc: 0.1135
Epoch 10/12
60000/60000 [==============================] - 5s - loss: 2.3012 - acc: 0.1123 - val_loss: 2.3011 - val_acc: 0.1135
Epoch 11/12
60000/60000 [==============================] - 6s - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1135
Epoch 12/12
60000/60000 [==============================] - 6s - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1135
Test score: 2.30111371231
Test accuracy: 0.1135
### 2、on cpu

Train on 60000 samples, validate on 10000 samples
Epoch 1/12
60000/60000 [==============================] - 71s - loss: 0.3994 - acc: 0.8764 - val_loss: 0.1060 - val_acc: 0.9674
Epoch 2/12
60000/60000 [==============================] - 72s - loss: 0.1495 - acc: 0.9557 - val_loss: 0.0677 - val_acc: 0.9785
Epoch 3/12
60000/60000 [==============================] - 72s - loss: 0.1119 - acc: 0.9675 - val_loss: 0.0537 - val_acc: 0.9823
Epoch 4/12
60000/60000 [==============================] - 100s - loss: 0.0914 - acc: 0.9724 - val_loss: 0.0474 - val_acc: 0.9839
Epoch 5/12
60000/60000 [==============================] - 114s - loss: 0.0805 - acc: 0.9758 - val_loss: 0.0393 - val_acc: 0.9874
Epoch 6/12
60000/60000 [==============================] - 117s - loss: 0.0721 - acc: 0.9783 - val_loss: 0.0381 - val_acc: 0.9873
Epoch 7/12
60000/60000 [==============================] - 118s - loss: 0.0664 - acc: 0.9806 - val_loss: 0.0376 - val_acc: 0.9874
Epoch 8/12
60000/60000 [==============================] - 121s - loss: 0.0621 - acc: 0.9815 - val_loss: 0.0338 - val_acc: 0.9883
Epoch 9/12
60000/60000 [==============================] - 121s - loss: 0.0565 - acc: 0.9834 - val_loss: 0.0322 - val_acc: 0.9887
Epoch 10/12
60000/60000 [==============================] - 124s - loss: 0.0542 - acc: 0.9840 - val_loss: 0.0304 - val_acc: 0.9906
Epoch 11/12
60000/60000 [==============================] - 121s - loss: 0.0491 - acc: 0.9850 - val_loss: 0.0314 - val_acc: 0.9896
Epoch 12/12
60000/60000 [==============================] - 116s - loss: 0.0485 - acc: 0.9857 - val_loss: 0.0294 - val_acc: 0.9898
Test score: 0.0294442383148
Test accuracy: 0.9898
",,"['May be it\'s my tensorflow‘s problem because the following code can not work well too：\n\n``` python\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ndef weight_varible(shape):\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\ndef conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=\'SAME\')\n\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\')\n\n\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\nprint(""Download Done!"")\n\nsess = tf.InteractiveSession()\n\n# paras\nW_conv1 = weight_varible([5, 5, 1, 32])\nb_conv1 = bias_variable([32])\n\n# conv layer-1\nx = tf.placeholder(tf.float32, [None, 784])\nx_image = tf.reshape(x, [-1, 28, 28, 1])\n\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\n\n# conv layer-2\nW_conv2 = weight_varible([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\n\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n\n# full connection\nW_fc1 = weight_varible([7 * 7 * 64, 1024])\nb_fc1 = bias_variable([1024])\n\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n# dropout\nkeep_prob = tf.placeholder(tf.float32)\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n# output layer: softmax\nW_fc2 = weight_varible([1024, 10])\nb_fc2 = bias_variable([10])\n\ny_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\ny_ = tf.placeholder(tf.float32, [None, 10])\n\n# model training\ncross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n\ncorrect_prediction = tf.equal(tf.arg_max(y_conv, 1), tf.arg_max(y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nsess.run(tf.initialize_all_variables())\n\nfor i in range(20000):\n    batch = mnist.train.next_batch(50)\n\n    if i % 100 == 0:\n        train_accuacy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0})\n        print(""step %d, training accuracy %g""%(i, train_accuacy))\n    train_step.run(feed_dict = {x: batch[0], y_: batch[1], keep_prob: 0.5})\n\n# accuacy on test\nprint(""test accuracy %g""%(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})))\n```\n', '@DominicBreuker tks\n', '[https://github.com/tensorflow/tensorflow/issues/3891](url)\n', ""I have similar problem with tensorflow with gtx1070, ubuntu 16.04. I'm trying this: http://abhay.harpale.net/blog/machine-learning/deep-learning/getting-tensorflow-to-work-with-gpu-nvidia-gtx-1080-on-ubuntu-16-04-lts/\n"", '@jinchenglee 3ks\n']",[],[],1,0
110,keras,8629,closed,"Siamese network do not converge, while with raw tf the same network converges.","I created a siamese like network.

When training with keras loss it does not converge at all.

With raw tensorflow loss function and training_op it converges quickly.

https://gist.github.com/Aelphy/c73aa56bf2f410b1423c63bd7087142e",,"['have you found the reason?', 'No, just switched to raw version', ""> raw version\r\n\r\nwhat do you mean by 'raw version'? where can i find the code?\r\n""]",[],[],1,0
111,keras,949,closed,loss : nan after training second batch size with convolution 1D,"hello, I'm trying to code something like that http://arxiv.org/abs/1506.04214
to start i'm trying to implement a simple conv 1D on time series (forex). 

the goal it's to predict the 10th value of the last value of a temporal window (60 values in window and I want to predict the 70th) To have my data compatible with conv 1D I prepare my data into a array of 5019 matrix of 60 raw and 1 column

my code : 

 javascript
sizeTraining=0.9
sizeTest=0.1
window=60
h=10

print('Loading data ...')
forex = FOREX()
(X_train, y_train),(X_test,y_test) = forex.load_data(sizeTraining, sizeTest, window, h)

Xtrain_mean = X_train.mean()
Xtrain_ecart_type = X_train.std()
X_train=(X_train-Xtrain_mean)/Xtrain_ecart_type

Xtest_mean = X_test.mean()
Xtest_ecart_type = X_test.std()
X_test=(X_test-Xtest_mean)/Xtest_ecart_type

ytrain_mean = y_train.mean()
ytrain_ecart_type = y_train.std()
y_train=(y_train-ytrain_mean)/ytrain_ecart_type

ytest_mean = y_test.mean()
ytest_ecart_type = y_test.std()
y_test=(y_test-ytest_mean)/ytest_ecart_type

print('x reshape')
X_train = np.reshape(X_train, (-1,window,1))
X_test = np.reshape(X_test, (-1,window,1))
print(X_train.shape)
print(X_train)

print('y reshapet')
y_train = np.reshape(y_train, (-1,1))
y_test = np.reshape(y_train, (-1,1))
print(y_train.shape)

model = Sequential()
#premiere couche de convolution
model.add(Convolution1D(nb_filter=60,filter_length=3,border_mode=""full"",activation=""relu"",subsample_length=1,input_dim=1,input_length=60))
conv1= Activation('relu')
model.add(conv1)
#premiere couche de subsampling
model.add(MaxPooling1D(pool_length=2))
model.add(Dropout(0.25))

model.add(Convolution1D(nb_filter=60,filter_length=3,border_mode=""full"",activation=""relu"",subsample_length=1))
conv1= Activation('relu')
model.add(conv1)
#premiere couche de subsampling
model.add(MaxPooling1D(pool_length=2))
model.add(Dropout(0.25))

#on applatit la sortie pour la presenter au MLP
model.add(Flatten())

#premiere couche du MLP avec activation de type maxoutdense
model.add(Dense(output_dim=500))
model.add(Dropout(0.5))

model.add(Dense(output_dim=500))
model.add(Dropout(0.5))

model.add(Dense(1))
model.add(Activation('linear'))

model.compile(loss='mse', optimizer='adadelta')

history = model.fit(X_train, y_train, batch_size=128, nb_epoch=2, show_accuracy=False, verbose=1, validation_data=(X_test, y_test))
score = model.evaluate(X_test, y_test, show_accuracy=False, verbose=0)
'''

this my problem : 

`
",,"['Note that you adding `ReLU` twice after each convolution. If you specify it in the arguments of `Convolution1D`, no need to add another layer.\n', ""aah ! yes ! my mistake ^^ I'm going to try without it and hope it's gonna be better :) thank you very much \n""]","['', ""javascript\nEpoch 1/2\n\n 128/5019 [..............................] - ETA: 24s - loss: 0.8462\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 256/5019 [>.............................] - ETA: 24s - loss: nan   \x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 384/5019 [=>............................] - ETA: 23s - loss: nan\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 512/5019 [==>...........................] - ETA: 22s - loss: nan\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 640/5019 [==>...........................] - ETA: 21s - loss: nan\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n'''\n\nI think my input data is correct so I don't know why I have this ..\n""]",[''],1,0
112,keras,1465,closed,My impletation of feedforward is too slow,"I use Keras to run a forward pass with weights pre-trained on another network, but the forward pass is terribly slow. While it takes only 2~3 seconds on [Darknet](https://github.com/pjreddie/darknet), It takes me 27 seconds to run a forward pass. The result is as expected, so I don't know what I've done wrong here. Here is my model in keras:



It's a network similar to GoogleNet with 27 layers in total. I am using a 3GB Quadro K4000 
",stale,"['Solution: use cuDNN.\n', '@fchollet Thanks, but I have put \n\n```\nlibcudnn.so\nlibcudnn.so.7.0 \nlibcudnn.so.7.0.64\nlibcudnn_static.a\n```\n\nin my cuda-7.5 `lib`  folder as well as the `cudnn.h` in the `include`  folder.\nHere is the output:\n\n```\nUsing Theano backend.\nUsing gpu device 0: Quadro K4000 (CNMeM is enabled)\n```\n', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']","['\ndef SimpleNet(googleNet):\n    model = Sequential()\n\n    #Convolution Layer 2 & Max Pooling Layer 3\n    model.add(ZeroPadding2D(padding=(3,3),input_shape=(3,224,224)))\n    model.add(Convolution2D(64, 7, 7, weights=[googleNet.layers[1].weights,googleNet.layers[1].biases],border_mode=\'valid\',subsample=(2,2)))\n    model.add(LeakyReLU(alpha=0.1))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n    #Use a for loop to replace all manually defined layers\n    for i in range(3,27):\n        l = googleNet.layers[i]\n        if(l.type == ""CONVOLUTIONAL""):\n            model.add(ZeroPadding2D(padding=(l.size//2,l.size//2,)))\n            model.add(Convolution2D(l.n, l.size, l.size, weights=[l.weights,l.biases],border_mode=\'valid\',subsample=(1,1)))\n            model.add(LeakyReLU(alpha=0.1))\n        elif(l.type == ""MAXPOOL""):\n            model.add(MaxPooling2D(pool_size=(2, 2)))\n        elif(l.type == ""AVGPOOL""):\n            model.add(AveragePooling2D(pool_size=(7,7),strides=(7,7)))\n            #In this particular model, feature output by avg pooling needs to be flattened. TODO:Use a better way to indicate there is a flatten layer\n            model.add(Flatten())\n        elif(l.type == ""CONNECTED""):\n            model.add(Dense(l.output_size, weights=[l.weights,l.biases]))\n            model.add(LeakyReLU(alpha=0.1))\n        else:\n            print ""Error: Unknown Layer Type""\n    model.add(Activation(\'softmax\'))\n    return model\n']",[],1,0
113,keras,4030,closed,Performance issue with multiple loss functions,"I have a regression CNN model for object detection tasks.
When I use one output and one loss function, the result is very good.


Now I add another output  with **ZERO** loss weight on top of :



The performance is much worse. Note that I tried different loss weight values for , which all gave bad results.
My thought is that if loss weight of  is zero, it should has no contribution in the back-propagation updates. And because  does not alter the sub-network from  to , the learning shouldn't change either.
Any idea where I was doing wrong?
Thanks a lot!
",stale,"['Is there any update for this issue?', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],"['model = Model(inp,out1)', 'model.compile( loss=""mean_squared_error"",optimizer=SGD() )', 'out2', 'out1', 'out2 = SomeLayer(params)(out1)', 'model = Model(inp,[out1,out2])', 'model.compile( loss=""mean_squared_error"",optimizer=SGD(),loss_weights=[1.0, 0.0] )', 'out2', 'out2', 'out2', 'input', 'out1']",1,0
114,keras,8640,closed,No Improvements to training speed with GPU (partial GPU usage?!),"I am trying to train my model on a GPU instead of a CPU on an AWS p2.xlarge instance from my Jupyter Notebook. I am using tensorflow-gpu backend (only  was installed and mentioned in  and not ).

I am not seeing any speed improvements when training models on these instances compared to using a CPU, infact I am getting training speeds per epoch that is almost same as I am getting on my 4-core  laptop CPU (p2.xlarge also has 4 vCPUs with a Tesla K80 GPU). I am not sure if i need to do some changes to my code to accommodate faster/parallel processing that GPU can offer. I am pasting below my code for my model:



Also interestingly the GPU seems to be utilizing between 50%-60% of its processing power and almost all of its memory every time I check for GPU status using  (but both fall to 0% and 1MiB respectively when not training):

Also if you's like to see my logs about using the GPU from Jupyter Notebook:

Please suggest what could be the problem. Thanks a ton for looking at this anyways!",,"['Seems to run just like it should on a GPU. How much time is it taking exactly for an epoch?', '@teja-bogireddy takes 18 seconds for an epoch for dataset of dim: (34000x7x5) on GPU same as on CPU', 'If this is your first program on that GPU. Then you must have missed something in gpu-integrated installation of tensorflow. Try installing it again[purge before installing]. I have been in this situation. Hope this works! ', 'Yes it is my first try on the GPU. I am using https://aws.amazon.com/marketplace/pp/B077GCZ4GR AMI specifically `ami-9e745afb` with `tensorflow_p36` virtualenv. Also tried my own virtualenv setup with python 3.6 but to no avail.', '@teja-bogireddy Although one other thing that I am not sure perfectly relates is I was getting python error in my notebook: `error while loading shared libraries: libcublas.so.8.0` when importing Keras and figured that it had not found the correct path to the libraries and exported environment variable `export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64` and that solved this error and the notebook was then running but only with the training speed that I earlier mentioned. I am not sure it relates but throwing it here for verbosity of my case.', 'I solved this by graduating my model to CuDNNLSTMs and saw training speed cut to a third. https://stackoverflow.com/questions/47574050/keras-shows-no-improvements-to-training-speed-with-gpu-partial-gpu-usage']","[""\r\nmodel = Sequential()\r\nmodel.add(recurrent.LSTM(64, input_shape=(X_np.shape[1], X_np.shape[2]),\r\n                        return_sequences=True))\r\nmodel.add(recurrent.LSTM(64, return_sequences = False))\r\nmodel.add(core.Dropout(0.1))\r\nmodel.add(core.Dense(3, activation='softmax'))\r\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop', metrics=['accuracy'])\r\n\r\nmodel.fit(X_np, y_np, epochs=100, validation_split=0.25)\r\n"", '\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           On   | 00000000:00:1E.0 Off |                    0 |\r\n| N/A   47C    P0    73W / 149W |  10919MiB / 11439MiB |     52%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1665      C   ...ubuntu/aDash/MLenv/bin/python 10906MiB |\r\n+-----------------------------------------------------------------------------+\r\n', '\r\n[I 04:21:59.390 NotebookApp] Kernel started: c17bc4d1-fa15-4b0e-b5f0-87f90e56bf65\r\n[I 04:22:02.241 NotebookApp] Adapting to protocol v5.1 for kernel c17bc4d1-fa15-4b0e-b5f0-87f90e56bf65\r\n2017-11-30 04:22:32.403981: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2017-11-30 04:22:33.653681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-11-30 04:22:33.654041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:1e.0\r\ntotalMemory: 11.17GiB freeMemory: 11.10GiB\r\n2017-11-30 04:22:33.654070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\r\n2017-11-30 04:22:34.014329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7\r\n2017-11-30 04:22:34.015339: I tensorflow/core/common_runtime/direct_session.cc:299] Device mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7\r\n\r\n2017-11-30 04:23:22.426895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\r\n']","['tensorflow-gpu', 'requirements.txt', 'tensorflow', 'nvidia-smi']",1,0
115,keras,9672,closed,Different accuracy score between keras.model.evaluate and sklearn.accuracy_score,"I have a similar problem with this Kaggle tutorial: https://www.kaggle.com/eliotbarr/text-mining-with-sklearn-keras-mlp-lstm-cnn, so I will refer to it.

If you look to the code block number 30 and 31:



and 



I suppose the accuracy scores should be the same, but in fact, they are different. How can it be possible?
One accuracy is calculated by model.evaluate and other one is calculated by accuracy_score (sklearn).",,"[""Furthermore, I saw a huge difference on ``val_acc`` while training and the ``accuracy`` in the final step. Even I use the same test set for training and evaluation.\r\n\r\n```\r\nmodel.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch,\r\n              validation_data=(X_test, Y_test))\r\n```\r\n\r\nHere is my training process:\r\n\r\n```\r\nEpoch 16/20\r\n16256/16256 [==============================] - 3s - loss: 0.1011 - acc: 0.9604 - val_loss: 0.1191 - val_acc: 0.9545\r\nEpoch 17/20\r\n16256/16256 [==============================] - 4s - loss: 0.0986 - acc: 0.9615 - val_loss: 0.1224 - val_acc: 0.9536\r\nEpoch 18/20\r\n16256/16256 [==============================] - 3s - loss: 0.0965 - acc: 0.9622 - val_loss: 0.1197 - val_acc: 0.9550\r\nEpoch 19/20\r\n16256/16256 [==============================] - 3s - loss: 0.0946 - acc: 0.9631 - val_loss: 0.1213 - val_acc: 0.9542\r\nEpoch 20/20\r\n16256/16256 [==============================] - 3s - loss: 0.0929 - acc: 0.9634 - val_loss: 0.1288 - val_acc: 0.9519\r\n```\r\n\r\nthe ``val_acc`` reaches to 0.95.\r\n\r\nBut when I do:\r\n\r\n```\r\npreds = model.predict_classes(X_test, verbose=0)\r\nprint('prediction 8 accuracy: ', accuracy_score(test['Rating'], preds+1))\r\n```\r\n\r\nthe accuracy score is only  ``0.68``.\r\n\r\nI note that ``Y_test`` and ``y_test`` are the same\r\n```\r\nY_test = np_utils.to_categorical(y_test, nb_classes)\r\n```"", 'Both the accuracy measures are different.\r\n1. sklearn accuracy is pretty straightforward. \r\ny_pred = [0, 2, 1, 3]\r\ny_true = [0, 1, 2, 3]\r\ny_equals = [1,0,0,1]\r\nsklearn accuracy = 0.5 which is the confidence.\r\n\r\n2. keras models accuracy on the other hand calculates the mean of y_equals for binary classes. Slightly more different for categorical.\r\n\r\nYou can refer to these articles to further gain clarity on the calculations:\r\nsklearn : http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\r\nmodels.evaluate : https://datascience.stackexchange.com/a/14742 \r\n\r\n', 'thanks for your reply sir.\r\nbut after getting the predictions .i calculated accuracy as like as how keras calculates.\r\naccuracy1 =K.mean(K.equal(K.argmax(y_true, axis=-1), K.argmax(y_pred, axis=-1)))\r\nboth are same accuray1,scikit accuracy.\r\nbut not same as keras evaluate accuracy']","[""\r\nprint('Train...')\r\nmodel.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=1,\r\n          validation_data=(X_test, Y_test))\r\nscore, acc = model.evaluate(X_test, Y_test,\r\n                            batch_size=batch_size)\r\nprint('Test score:', score)\r\nprint('Test accuracy:', acc)\r\n"", '\r\nprint(""Generating test predictions..."")\r\npreds = model.predict_classes(X_test, verbose=0)\r\nprint(\'prediction 8 accuracy: \', accuracy_score(test[\'Rating\'], preds+1))\r\n']","['', '', '', '']",1,0
116,keras,9674,closed,NASNet Mobile gives low accuracy on ImageNet,"Hi! I am working on transferring NASNet Mobile weights from Keras to Pytorch.
After transferring the weights and making predictions of **the same image using both frameworks** I am getting absolutely **the same values** both in confidence and in any middle layers of the network.

However, the evaluation of the imagenet using transferred weights in Pytorch gave me
**Acc@1 38.110 Acc@5 60.844**

Also, I tested the script that I am using for evaluation on ImageNet and it works correct, as I got correct values for other architectures.

I was wondering if you could double check the weight that you provide for NASNet Mobile, probably there is some issue with them! 

",,"['This may be a legitimate issue, see https://github.com/keras-team/keras/issues/9586 as well', 'The error has been fixed in #10209.']",[],[],1,1
117,keras,13266,closed,low accuracy with incorrect one-hot encoding in lstm_seq2seq.py example,"I tested the example of ""keras/examples/lstm_seq2seq.py"" with ""accuracy"" metrics. As a result that the model didn't achieve high accuracy. I found that one-hot encoding after tokenizing wasn't applied for the back padding of sentences.",type:support,"['@tykimos Can you please send me a github gist of this issue and also what backend are you using and the version of keras. Thanks!', '@gowthamkpr \r\n- Gist Link is https://gist.github.com/tykimos/a082d8b78e49acf38e65956f927c996b\r\n- Keras version is 2.2.5.\r\n\r\nI write PR #13269 ""#13266 Update lstm_seq2seq.py(from 22% to 87% acc)"". Would you like to check it too?']",[],[],1,0
118,keras,4563,closed,Very high / Nan loss when performing multiple sequence time series regression,"I am training an LSTM model for multiple time-series regression. But, my losses are always either very high or Nan. I have tried several optimizers such as ,  and . Here's the script:


 is of the shape (N_SAMPLES_TRAIN, MAX_TIMESTEPS, MAX_FEATURES)
 is of the shape (N_SAMPLES)

I should add that the Y values I am trying to predict are very high. Any idea where I might be going wroing?",,"['Have you tried to rescale the values before training?', ""When working with very long sequences (e.g. a large MAX_TIMESTEPS), it's pretty common for underflow/overflow issues during the early stages of training, where the error is large and gets larger (or inversely, smaller and gets smaller) with each timestep. This causes NaNs, INFs, and the like. This is due to random state of the model at the start of the training, and the loss function accruing for longer with longer sequences.\r\n\r\nThis can be mitigated by:\r\n- Training on the shortest sequences first, and adding longer sequences to the training as the model moves from random towards a local minimum\r\n- Initializing the weights intelligently such that the model starts out in a less-random state\r\n- Using a clipped (min()'d or max()'d) loss function that keeps the loss within the realm of float numbers"", '@patyork Thanks a lot for the suggestions. I am trying to implement the 3rd suggestion, but it raises a Theano error. How should I modify it to work correctly?\r\n\r\n````\r\ndef root_mean_squared_error(y_true, y_pred):\r\n    return K.clip(K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)), min_value=MIN_VALUE, max_value=MAX_VALUE)\r\n````\r\n\r\n@Bo604 I did try to scale the values, but it still results in large losses in the test set. Maybe I was doing it wrong, I used the MinMaxScaler from scikit to scale features in (0,1)', 'This is a support question, you are more likely to get an answer on the mailing list (https://groups.google.com/forum/#!forum/keras-users) or Stack Overflow. I recommend you close this bug to reduce the noise on the devs.', '@Bo604 what does rescaling the values mean?', '@naisanza i meant to apply something like this to each of the input and target variables before training:\r\nhttp://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html']","[""\r\nsgd = SGD(lr=0.0008, decay=1e-6, momentum=0.9, nesterov=True)\r\n# Build the Keras Model\r\nmodel = Sequential()\r\nmodel.add(LSTM(64, input_shape=(MAX_TIMESTEPS, MAX_FEATURES)))\r\nmodel.add(Dropout(0.3))\r\nmodel.add(Dense(1))\r\nmodel.compile(loss='mean_absolute_error',\r\n              optimizer=sgd)\r\n\r\nprint 'Training the LSTM model'\r\n\r\nbatch_size = 32\r\nmodel.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=20, validation_split=0.2)\r\nmae = model.evaluate(X_test, Y_test, batch_size=batch_size)\r\n""]","['rmsprop', 'adam', 'sgd', '', 'X_train', 'Y_train']",1,0
119,keras,10706,closed,custom metric MAE and RMSE are the same,"This is a reproducible example: 

consider this 



Both RMSE and MAE are the same. This happens when i placed them together. Is there a bug?

the output is:

![image](https://user-images.githubusercontent.com/22788747/42806109-39fc16be-89e0-11e8-80d8-5e73ba8b2541.png)
",,"['Rmse calculation is wrong. Try to get rid of `axis=-1` - I had problems with RMSE before and removing the axis argument worked for me ', 'thanks man. Without the axis = -1, it seems to be correct. The axis -1 is based on the original metrics. May i ask why was the axis -1 needed in the first place? (for the Original metrics defined in tf) ', 'I have no idea in this case to be honest - if nothing else it should be axis=0. Otherwise it was doing square, mean of each number (i.e. nothing) and square root of the square (i.e. absolute value of original calculation) so it was just returning mae ', 'yes thinking about it, the mean should be computed along axis = 0, otherwise, it will simply return the original vector.\r\n\r\nbut in the case of axis = -1, wouldnt there be an error since we still have a vector even after the sqrt?', 'It is possible that somewhere in Keras code it calls the mean on the vector\r\nsince most losses/metrics have mean as the last operation (mse, Mae, logloss, etc).\r\nI will investigate...\r\n\r\nOn Tue, Jul 17, 2018, 10:53 PM Germayne <notifications@github.com> wrote:\r\n\r\n> yes thinking about it, the mean should be computed along axis = 0,\r\n> otherwise, it will simply return the original vector.\r\n>\r\n> but in the case of axis = -1, wouldnt there be an error since we still\r\n> have a vector even after the sqrt?\r\n>\r\n> —\r\n> You are receiving this because you commented.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/keras-team/keras/issues/10706#issuecomment-405800716>,\r\n> or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/AX4ZrRo-iZr4h6j5Y8TTFiQIEBme5HuLks5uHrFAgaJpZM4VSa6A>\r\n> .\r\n>\r\n', 'Update:\r\nAt least in losses we have this line that explicitly calls K.mean on the loss \r\nhttps://github.com/keras-team/keras/blob/f86bc57d621fb0325e928271b246d197611b0e65/keras/engine/training_utils.py#L422', ""@tRosenflanz  if so, like you mentioned above, the 'mean' in my code didnt work, and computed a vector of absolute differences with k.mean called at the end. Resulting in MAE as output.\r\n\r\nThis definitely also explains why MSE code work as opposed to my custom RMSE\r\n\r\nty. You have been really helpful"", 'Okay, this explains the behaviour pretty well at this point - although this is not documented explicitly from what I can tell- the losses and metrics are computed based on the mean implicitly. Can you please close the issue ? ', '@uwu-ai what version of Keras/tf and what is the shape of your output?\r\nHave you tried passing different axis arguments into the mean?', 'Output shape is 1,1 or Batch_size,1,1?', 'Does your generator output sample weights by any chance?', 'Can you disable that and see what loss you get? This way it will be easier to understand how rmse and mse actually correlate']","[""\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Activation\r\nimport numpy as np\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nimport keras.backend as K\r\n\r\ndef rmse (y_true, y_pred):\r\n    return K.sqrt(K.mean(K.square(y_pred -y_true), axis=-1))\r\n\r\n\r\n\r\n\r\nmodel = Sequential([\r\n    Dense(32, input_shape=(50,)),\r\n    Activation('relu'),\r\n    Dense(1),\r\n])\r\n\r\nmodel.compile(optimizer='adam', loss='mse', metrics=['mae',rmse])\r\n\r\n\r\n\r\n\r\n\r\ndata = np.random.random((1000, 50))\r\nlabels = np.random.randint(2, size=(1000, 1))\r\n\r\nmodel.fit(data, labels, epochs=5, batch_size=64)\r\n""]",[],1,0
120,keras,6108,closed,image_ocr example performance,"image_ocr example on kears was 10 times faster then on keras2
Why that ?
What has changed ?",,"[""Seems the batch size is now 1?  Or am I just imagining that?  I can look into it, but I didn't do any of the Keras2 conversions, so someone may have a quicker answer. "", 'declare\r\n```\r\nminibatch_size=32\r\n```\r\nthen in fit_generator\r\n\r\n```\r\nsteps_per_epoch=(words_per_epoch - val_words)/minibatch_size\r\nvalidation_steps=val_words/minibatch_size\r\n```']",[],[],1,0
121,keras,6625,closed,Replace ScikitLearn's pipelines with Keras' layers,"I am very new to Keras. That said, I am trying to avoid using Scikit's pipeline code since it is relatively slower than using the keras network.



How can you achieve the same result using Keras's implementation? Can I achieve the same using NormalizationLayer? I could not figure out how but I tried the following:



I am using Keras 2.0.4 with Tensorflow 0.12.1.

",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']","[""python\r\nestimators = []\r\nestimators.append(('standardize', StandardScaler()))\r\nestimators.append(('mlp', KerasRegressor(build_fn=wider_model, epochs=100, batch_size=5, verbose=0)))\r\npipeline = Pipeline(estimators)\r\nkfold = KFold(n_splits=10, random_state=seed)\r\nresults = cross_val_score(pipeline, X.T, Y, cv=kfold)\r\n"", ""python\r\nX_norm = utils.normalize(X)\r\nmodel = Sequential()\r\nmodel.add(Dense(20, input_dim=12, kernel_initializer='normal', activation='relu'))\r\nmodel.add(Dense(1, kernel_initializer='normal'))\r\n# Compile model\r\nmodel.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\r\n# Fit the model\r\nhistory = model.fit(X_norm.T, Y, validation_split=10, epochs=100, batch_size=15, verbose=0)\r\n""]",[],1,0
122,keras,12770,closed,Siamese Network Performance Issues,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

###################################
###################################

So I am building a Siamese Network using VGG19 as my base network. Everything went fine throughout training and testing. However, I noticed that during training, my accuracy is quite high (only took 2 epochs to reach 100% accuracy, which was amazing), but when testing, the accuracy is around 0% (sometimes even lower, as shown below). The code is in  and  located in my repo [here](https://github.com/ayaz-amin/AyazNet). 

When I run , this is the output:


When I run , this is the output:

I was expecting the output to be showing ones (0 means no similarity, 1 meaning full similarity). I am pretty sure their is nothing wrong with this code. Am I doing anything wrong? Or is it a bug? Also, I am using tf.keras from Tensorflow 2.0.
",,[],"['train.py', 'test.py', 'train.py', '\r\nEpoch 1/2\r\n1/1 [==============================] - 20s 20s/sample - loss: 0.6932 - accuracy: 0.3265\r\nEpoch 2/2\r\n1/1 [==============================] - 19s 19s/sample - loss: 0.6659 - accuracy: 1.0000\r\n', 'test.py', '\r\nPrediction:  [[[[6.8002939e-04]\r\n   [3.8743019e-07]\r\n   [0.0000000e+00]\r\n   [0.0000000e+00]\r\n   [4.1723251e-07]\r\n   [2.6953220e-04]\r\n   [8.9406967e-08]]\r\n\r\n  [[9.2387199e-07]\r\n   [1.4901161e-07]\r\n   [0.0000000e+00]\r\n   [0.0000000e+00]\r\n   [0.0000000e+00]\r\n   [0.0000000e+00]\r\n   [0.0000000e+00]]\r\n\r\n  [[1.1920929e-07]\r\n   [0.0000000e+00]\r\n   [1.3113022e-06]\r\n   [0.0000000e+00]\r\n   [0.0000000e+00]\r\n   [0.0000000e+00]\r\n   [1.9937754e-05]]\r\n\r\n  [[0.0000000e+00]\r\n   [0.0000000e+00]\r\n   [0.0000000e+00]\r\n   [0.0000000e+00]\r\n   [0.0000000e+00]\r\n   [1.0132790e-06]\r\n   [0.0000000e+00]]\r\n\r\n  [[0.0000000e+00]\r\n   [0.0000000e+00]\r\n   [0.0000000e+00]\r\n   [0.0000000e+00]\r\n   [0.0000000e+00]\r\n   [0.0000000e+00]\r\n   [0.0000000e+00]]\r\n\r\n  [[0.0000000e+00]\r\n   [0.0000000e+00]\r\n   [0.0000000e+00]\r\n   [0.0000000e+00]\r\n   [0.0000000e+00]\r\n   [0.0000000e+00]\r\n   [0.0000000e+00]]\r\n\r\n  [[0.0000000e+00]\r\n   [0.0000000e+00]\r\n   [0.0000000e+00]\r\n   [0.0000000e+00]\r\n   [0.0000000e+00]\r\n   [0.0000000e+00]\r\n   [9.3051212e-36]]]]\r\n']",[],1,0
123,keras,4580,closed,CPU usage spikes when using keras,"Hi all,

I am running a model which contains some GRU modules in an RNN with keras. While training, the CPU spikes between ~400% to ~950%, when every spike ends after several seconds and returns to 99% CPU usage baseline. Ideally I would like the CPU to stay steady in > 700%, as the spiking translates to inefficient runtime. My .theanorc file:



Any suggestions?",,"['This is not a keras issue, please close and post on the forum.']",['\r\n[global]\r\ndevice = cpu\r\nfloatx = float32\r\nallow_gc = false\r\nmode = FAST_RUN\r\nopenmp = true \r\n\r\n[scan]\r\nallow_gc = false\r\n'],[],1,0
124,keras,11236,closed,Higher validation loss (but same training loss) when using multi_gpu_model,"Hi,

I'm facing an increase in validation loss when using multi_gpu_model to train a CNN. I kept the all other parameters constant (batchsize, learning rate).

This is how I implemented it:

 I created the single-gpu model:
  
      if num_gpus > 1:
        import tensorflow as tf
        with tf.device('/cpu:0'):
            model = create_model()

Then I converted it into a parallel model:

    if num_gpus > 1:
        parallel_model = multi_gpu_model(model, gpus=num_gpus)

Then I compile it and run the training:

    parallel_model.compile(...)
    parallel_model.fit_generator(...)

Any ideas what might be the source of error?

I'm using the latest Keras (current master) with the latest tensorflow (1.10.1).

Best,
Thorsten
",To investigate,"['Can you please give us a minimal script to reproduce the error? Thank you. ', 'Unfortunately I cannot reproduce it on a small dataset, only on my big not-yet-public dataset...', 'Let""s say, one uses a batchsize of 12. The multi gpu model split it into 2 subbatches of size 6 and run it independendly on through the latest model on both GPUs. Should the updates after one step be the same for multi and single GPU?', ""Looking at the doc for multi_gpu_model :\r\n```\r\n    - Concatenate the results (on CPU) into one big batch.\r\n```\r\nIt seems the concatenation is done at the end.\r\nSo I don't quite know what you meant by 'after one step' above."", '""One step"" is for me one forward pass and backward pass of a batch. \r\n\r\nIn the meantime I\'ve updated to python3.6 as I thought the problem might be python2 specific. But it is not.\r\n\r\nCould be my batch generator be the problem? I\'m using a sequence object for doing this but if it works for a single gpu I cannot see why it should not work for multi gpu.\r\n\r\nAny recommendation how to track down the problem is highly appreciated.  ', 'It seems that I fixed it. The problem was, that I overwrote the template_model with the parallel_model before training. I fixed that and now I get comparable loss values.', 'I""m sorry, but I just mixed up the parameters. I checked it today more carefully again and the problem is still there :-( \r\n\r\nJust to provide you a comparison, I made three runs:\r\nAfter 5 Epochs on single gpu: 0.0905, 0.086,0.088\r\nAfter 5 Epochs using multi gpu: 0.0983, 0.11219, 0.10684\r\n\r\nAny recommendation how to track down the problem?', 'Could this issue be reopened?', 'I made some more experiments. Again I run the training three times for 5 epochs with multi and with single gpu:\r\n![multi_gpu_experiments](https://user-images.githubusercontent.com/1113977/47009068-d9abf100-d13b-11e8-8d04-5a04f9b5fb7a.png)\r\nAs you can see, the training loss behaving similar very similar with mutliple and single gpu.\r\nHowever, the validation loss is always worse.', ""Hi,\r\n\r\nI'm facing the same problem here when using a multi_gpu_model. I had to set cpu_merge=False to make the training loss stable after the first epoch. However, the validation loss is too high. \r\n\r\nDid you eventually solve this?\r\n\r\nThanks\r\nRaul"", 'Hi Raul,\r\n\r\nUnfortunately not :-(\r\n\r\nBest,\r\nThorsten', ""That's a pity. I will let you know if I figure this out.\r\n\r\nThanks\r\nRaul"", ""I Thorsten,\r\n\r\nIt seems that, for now, it is working in my case. My set up is the following:\r\n\r\n- Keras 2.2.4\r\n- TF 1.12.0\r\n- I am training the keras port of DeepLabv3+ (https://github.com/bonlime/keras-deeplab-v3-plus/)\r\n\r\nMy model declaration is pretty standard. I don't even declare the model under the scope of the cpu because it did not solve the problem. For me, what it fixed everything is to allow the merge in the GPU, and to create a slightly different model checkpoint callback that saves the single-GPU model instead of the multi-GPU one. So far it works in a 2-GPU setup, and the validation loss reports as expected. I hope this can be of help to you.\r\n\r\nRaul\r\n\r\n```\r\n    single_model = Deeplabv3(input_shape=crop_size + (3,), classes=len(ranges), backbone='xception')\r\n\r\n    # muli-gpu\r\n    if gpus > 1:\r\n        callbacks.append(MultiGPUModelCheckpoint(single_model, weights_path, monitor=monitor, verbose=0, save_best_only=True,\r\n                                                 save_weights_only=True, mode='auto', period=1))\r\n        model = multi_gpu_model(single_model, gpus=gpus, cpu_merge=False)\r\n    else:\r\n        callbacks.append(keras.callbacks.ModelCheckpoint(weights_path, monitor=monitor, verbose=0, save_best_only=True,\r\n                                                         save_weights_only=True, mode='auto', period=1))\r\n        model = single_model\r\n\r\n    model.compile(loss=loss, optimizer=optimizer, metrics=metric)\r\n    # model.summary()\r\n    history = model.fit_generator(train_crop_generator, steps_per_epoch=train_steps_per_epoch,\r\n                                  validation_data=val_crop_generator, validation_steps=val_steps_per_epoch,\r\n                                  epochs=epochs, callbacks=callbacks)\r\n\r\n```\r\n\r\n"", 'Hi Raul. A simple question.\r\n\r\nIn your previous post on Jan 9th, you said you failed to get proper val_loss even with cpu_merge=False option. In the post on Jan 10th, which would be right above mine, the cpu_merge option is still False, but you are saying you succeeded to get desired val_loss. \r\n\r\nSo the question is ... What was the most important change you applied to get the last result? Is it the callback you used in the code? How can a callback make a different result in terms of val_loss?\r\n\r\nBest, \r\nAngelo', 'Hi @angeloyeo ,\r\n\r\nTo this date, I have no certainty why the setup worked. It was more a sort of trial and error thing. The callback definitely helped in being able to re-use the weights elsewhere. That said, a 4-GPU setup of my code above did not work too well either. There certainly seems to be something off with Keras and multi-gpu that needs to be fixed at its core.', ""I have met the same problem. I tried to change callback to save weight with singel_model rather than multi-GPU-model, and it works well. I don't understand why I need to change CheckPoint Option, because I just need to verify on my val and test data without loading processing (Just for the compared experiment)""]",[],[],1,0
125,keras,10214,closed,Extremely low accuracy after finetuning InceptionV3 (not overfitting),"Hello everyone!

I'm using Keras 2.1.6 and TF 1.8. I want to fine-tune the pre-trained InceptionV3 on a new set of classes as shown here: https://keras.io/applications/

My code is literally copy-paste from the documentation. While doing that, I noticed that the accuracy on my training dataset is very high while the one on the validation set is very low. Immediately I suspected over-fitting but after wasting several hours I could not find anything. Then I tried to fit/evaluate on the same dataset. To my surprise the accuracy reported by fit() was 87.2% while the one reported by evaluate() was 62.9% on the SAME dataset.

How is this possible? Is there a bug or something? I tried also Keras 2.1.5 and 2.1.4 but I get the same problem. Am I doing something wrong? I following the documentation.",,"[""After digging through the issues and googling around I found out that this is related to Batch Normalization. I tried @datumbox's temporary fix (described at #9214) and now the accuracy on training/validation is the same.\r\n\r\n@Dref360 @farizrahman4u @taehoonlee @fchollet Any plans to fix this on master? It's really annoying!"", ""Also it seems there was a pull-request (#9965) to fix this problem but it was not merged. Anyone know what's the status of that?""]",[],[],1,0
126,keras,3562,closed,"If training data and validation data is the same, how come the losses differ?","I suppose I'm missing something obvious but the progress bar reports a much higher training loss than validation loss when the data is the same. Why is that? Shouldn't they be pretty much the same?

With split data (80/20, for example) everything looks fine, with a slightly worse validation loss than training loss, as expected.
",,"['Answer in [FAQ](https://keras.io/getting-started/faq/#why-is-the-training-loss-much-higher-than-the-testing-loss)\n', '@dolaameng, thanks. :+1: \n\n> [...]the training loss is the average of the losses over each batch of training data. Because your model is changing over time, the loss over the first batches of an epoch is generally higher than over the last batches. On the other hand, the testing loss for an epoch is computed using the model as it is at the end of the epoch, resulting in a lower loss.\n\nI would have expected the final progress bar print (the one that includes the validation loss) to do a proper evaluation, at least as an option, but that would be a bit computationally wasteful perhaps.\n']",[],[],1,0
127,keras,3563,closed,mnist_cnn.py: couldn't reach 99.25% test accuracy,"I  copied the code o mnist_cnn.py exactly except that modified the print lines to be compatible with Python 2.7. The np.random.seed(1337) was kept as well for reproducibility. However, I can only get 98.78%. Any clues why I couldn't reach 99.25% test accuracy? 
",stale,"['Same here. Using following versions installed via pip on an AWS P2 instance with ubuntu 16.04:\r\n\r\n```\r\nkeras==1.2.2\r\ntensorflow==1.0.0\r\n```\r\n\r\nAlso looks like https://github.com/fchollet/keras/issues/4084 is the same issue.', 'I just tried with Theano (CPU) and got also ~98,88% again. \r\n```\r\nkeras==1.2.0\r\ntheano==0.8.2\r\n```\r\nboth runs with CUDA 8 and cudnn 5.1', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs, but feel free to re-open it if needed.\n']",[],[],1,0
128,keras,6636,closed,MNIST_CNN example uses double softmax?,"Snippet from the Keras MNIST (LeNet) example ([source](https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py#L57)) :

Implementation of  uses  ([source](https://github.com/fchollet/keras/blob/master/keras/backend/tensorflow_backend.py#L2750)):

But the Tensorflow doc on  ([source](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)) states:
> **WARNING**: This op expects unscaled logits, since it performs a softmax on logits internally for efficiency. Do not call this op with the output of softmax, as it will produce incorrect results.

Am I missing something or are we violating the above warning due to  calling the softmax activation?",,"['`keras.losses.categorical_crossentropy` can take result of a softmax.\r\nhttps://github.com/fchollet/keras/blob/master/keras/backend/tensorflow_backend.py#L2731-L2732', 'Ah I see! I think combining them should be prefered (from_logits=True) as it should improve numerical stability.']","[""python\r\n(...)\r\nmodel.add(Dense(num_classes, activation='softmax'))\r\nmodel.compile(loss=keras.losses.categorical_crossentropy,\r\n(...)\r\n""]","['categorical_crossentropy', 'tf.nn.softmax_cross_entropy_with_logits()', 'tf.nn.softmax_cross_entropy_with_logits()', 'Dense()']",1,0
129,keras,13293,closed,mobilenetv2 poor accuracy,"I imported mobilenetv2 but the accuracy is incredibly low (close to 0% correct)

My code is as below

from keras.applications.mobilenet_v2 import MobileNetV2
model1 = MobileNetV2(weights='imagenet')

import cv2
x = cv2.imread('image_input.jpg')
x = cv2.resize(x,(224,224))
y = model1.predict(x[np.newaxis,:,:,:])


Wheras if I import VGG (or resnet)
from keras.applications.vgg16 import VGG16
model2 = MobileNetV2(weights='imagenet')
y = model2.predict(x[np.newaxis,:,:,:])


VGG has a much much better accuracy

Has keras trained mobilenetv2 properly.",type:support,"['Are you normalizing the images properly?\r\nI have noticed that pre-trained models tend to have a preprocess_input function. \r\nFrom the documentation:\r\nfrom keras.applications.resnet50 import preprocess_input\r\n\r\nMobileNetV2 also has one: [see here](https://github.com/keras-team/keras/blob/master/keras/applications/mobilenet_v2.py)\r\n', 'Yes input needs to be in the [0,1] range as opposed to [0,255] that it was before. Once it is in [0,1], the accuracy improves significantly.\r\n', ""That's the scale. The preprocess_input() applies scaling and normalization using standard deviation and mean. \r\n[implementation here](https://github.com/keras-team/keras-applications/blob/master/keras_applications/imagenet_utils.py)\r\nFrom my experience the images tend to become darker.""]",[],[],1,0
130,keras,1006,closed,accuracy stop improving in simple convolution network,"I've been using keras to build convolution neural networks for binary classification. Using the same input data, I've tried to vary the model structure (i.e. filter size, number of filters, number of hidden layer neurons) for better performance. I noticed that for certain models, the training accuracy remains unchanged at a low value through all 50 training epochs. One of the model structure is as follows:



I used the whole training set as validation so I can get the training accuracy at the end of each epoch. The output after each epoch looks like:



I plotted the filter weights of the convolution layer at each epoch and they stayed the same for most of the epochs and had minor changes rarely.
at the beginning of epoch 1:
![epoch1_batch1_layer0_weight_img](https://cloud.githubusercontent.com/assets/7233438/11153096/32e26b80-89fc-11e5-8baa-71a2c2b2ef53.png)
at the beginning of epoch 3:
![epoch3_batch1_layer0_weight_img](https://cloud.githubusercontent.com/assets/7233438/11153097/32ee8bf4-89fc-11e5-8252-6bedd64d4888.png)
at the beginning of epoch 5:
![epoch5_batch1_layer0_weight_img](https://cloud.githubusercontent.com/assets/7233438/11153095/32e1ff56-89fc-11e5-9f39-e5e471a69538.png)

I also tried rmsprop as optimizer, and the accuracy at each epoch was exactly the same 0.4473 . However the filter weights formed different pattern at the end of epoch 1 but remained unchanged after that.

What may I have missed? I tried this model structure directly with Theano and the problem persisted, so I guess this problem was not from keras.

To identify the cause of this problem, I think it would be helpful to track the activation and gradient values of each mini-batch. How do I do these with Keras or Theano? Or what would be the right direction to look into?

Thanks a lot.
Cheng
",,"['are you using default learning rates?\n', ""@EderSantana In the above code I used the default learning rate for SGD which is 0.01. I've also suspected this might cause the problem, so I tried 0.1, 0.3, 0.9 and got slight different patterns in the filters but the same unchanged validation accuracy. As I mentioned I also tried rmsprop with default parameters, only to get the same accuracy.\n"", 'I think your learning rates are too large, actually... and try some dropout. Maybe you are getting stuck in high eigenvalue spread regions (narrow valleys where you move a lot without changing much in the overall cost function). How large and correlated is your input data?\n', 'wait, are you using filters of size 400?\n', ""@EderSantana Thanks a lot for your advice. I tried using learning rate 0.0001 and 0.0003 and the validation accuracy improved over 30 epochs. Now I have validation accuracy (actually training accuracy on the whole data set) at around 0.6 . It seems that the problem was really caused by a large learning rate.\n\n  It's possible that the model got into a valley within a few mini-batches due to large learning rate. Is there a way that I can confirm and/or avoid this? \n\n  I've not checked my input data for correlation yet. Can I do this by unrolling the input images into vectors and test for correlation between these vectors? What should I do about correlated inputs? Maybe cluster them by correlation and pick one from each cluster?\n\n  The filter size is equal to my input image size, i.e. 400 by 400. My input 'images' are actually not images but grid of values with each position indicating certain feature. So I hope to fix the filter position without striding, and let them learn patterns from my input. I understand this may not be a legitimate way to use convolution networks. I'm trying to run with filter size at 50 by 50, but still trying to figure out a way for them to work with my limited computation resources.\n"", ""using a filter of the size of the image is what we mean by fully connected layer. It is much faster. Also, if you want convolutions, try 3x3 and stuff like that... check lenet5 architecture\n\nIf you have images and the neighboring pixels are correlated like in regular images you will probably have correlations issues, that was why batch normalization was invented. Give Keras's BatchNormalization layer a try. It seems to be working correctly now.\n"", ""@EderSantana Thanks for the comments. I understand the fully connected layer part, a filter with fixed position is indeed a hidden layer neuron. I'll try some different filter sizes.\n\nI tried adding batch normalization layer after each layer in my model, only after the convolution layer, and only after the fully connected layer. It seems only the model with batch normalization after each layer allows gradually improving of accuracy over 50 epochs. However the speed of improvement seems slower than a model with correct learning rate (e.g. 0.0003) and no batch normalization. Is it supposed to perform like this? Thanks.\n"", 'nah, its supposed to learn faster with batch normalization, maybe with it you can try a learning rates that are a little bit larger\n', ""@EderSantana Yes, I'm able to use a learning rate of 0.001 with batch normalization for faster training. Thanks for all the help.\n"", ""awesome! good to hear that\n\ndon't forget to close the issue when you are done.\n"", ""I trying to run a CNN for image classification in CIFAR, but accuracy stops improving after 10% This is my model. \n\nmodel=Sequential()\n\nmodel.add(Convolution2D(32,3,3,border_mode='same',input_shape=(3,32,32)))\nmodel.add(Activation('relu'))\nmodel.add(Convolution2D(32,3,3))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(.25))\nmodel.add(Flatten())\n\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(.5))\n\nmodel.add(Dense(10))\nmodel.add(Activation('softmax'))\n\nsgd = SGD(lr=0.003, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='categorical_crossentropy',optimizer=sgd)\nmodel.fit(train,y_train,nb_epoch=15,show_accuracy=True,verbose=1,validation_data=(test,y_test))\n\nI have tried varying the learning rate also. Nothing seems to work.\n\nTrain - > (40000,3,32,32)\nTest - > (10000,3,32,32)\n\ny_train -> (40000,10) # the classes have been binarized\ny_test -> (10000,10) # the classes have been binarized\n"", 'did you try adam instead of sgd with decay? That model should not be too bad, I think...\n', ""I'm trying to use the vgg19 architectures for my model but the validation accuracy I got is constant value around 56% and I used too many values of the learning rate such as 0.1, 0.01, 0.001,0.0001 and all of them gave the same result"", 'How did you plot the filter weights of convolutional layer. ', 'I have a trained model file with three convolutional layer and another model file with 2 convolution and 1 lstm layer. I want to plot the filter weights of each layer. How to do this.', ""I am not that much familiar with python, so i am using MATLAB for CNN training. I have a data set of 27,000 images and angles corresponding to that images.\r\nMy sample code is : \r\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\r\nlayers = [imageInputLayer([32 32 1])\r\n                  convolution2dLayer(5,50)\r\n                  reluLayer()\r\n                  maxPooling2dLayer(2,'Stride',2)\r\n                  fullyConnectedLayer(size(categories(trainAngle)))\r\n                  softmaxLayer\r\n                  classificationLayer];\r\n      \r\n      \r\n options = trainingOptions('sgdm', 'MaxEpochs', 50, ...\r\n          'InitialLearnRate', 0.0003);\r\n      \r\n convnet = trainNetwork(trainZ, trainAngle, layers,options);\r\n \r\n resultant_Train = classify(convnet,trainZ); %Training data\r\n resultant_Valid = classify(convnet,validZ); %Validation data\r\n\r\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\r\n\r\nMy training accuracy is 70% \r\nbut test accuracy is only 2%;\r\nI am completely blank what to do next. Do you have any suggestion? How can I improve my test accuracy?\r\n"", 'This is a perfect example of overfitting - try using dropout layers, data augmentation, adding noise, or make the model simpler (it is memorizing your own training data). ', ""I had a same issue. When I was training the CNN, the val_acc never updated. And there was minor updates in acc. but it seemed to vibrate. At last, I found the solution.\r\nThe issue was in my training dataset. The size was about 4000. And the size of validation dataset was 500\r\nI thought that this issue came from the network weight initialization in flatten layer.\r\nSo I changed the dataset and tested. The result was tremendous.\r\n\r\n**This was my first training part.**\r\n\r\n```\r\nmodel.fit_generator(\r\n        train_data,\r\n        steps_per_epoch=4091 // 32,\r\n        epochs=100,\r\n        validation_data=test_data,\r\n        validation_steps=474 // 32,\r\n        verbose=1,\r\n        workers=4)\r\n```\r\n\r\n**I changed it as follows:**\r\n\r\n```\r\nmodel.fit_generator(\r\n        test_data,\r\n        steps_per_epoch=474 // 32,\r\n        epochs=25,\r\n        verbose=1,\r\n        workers=4)\r\n```\r\n\r\nAnd for the next training, I saved the model weights and reused for next training.\r\n`model.save_weights('weight_model.h5')`\r\n\r\nAnd initialized network weights with this:\r\n```\r\n# Load model weights\r\nmodel.load_weights('weight_model.h5')\r\n\r\n# Compiling the ANN\r\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\r\n```\r\n\r\nThis worked perfectly. I hope this will be helpful. Thanks."", ""@Paulraita  Hey! I'm having the same problem that you described. But I don't fully understand your solution. Did you run both codes one after the other? or directly trained with your test_data?\r\nI mean, is this what you run?: \r\n----------------------------------------------------------------------------\r\nmodel.fit_generator(\r\n        train_data,\r\n        steps_per_epoch=4091 // 32,\r\n        epochs=100,\r\n        validation_data=test_data,\r\n        validation_steps=474 // 32,\r\n        verbose=1,\r\n        workers=4)\r\nmodel.fit_generator(\r\n        test_data,\r\n        steps_per_epoch=474 // 32,\r\n        epochs=25,\r\n        verbose=1,\r\n        workers=4)\r\n\r\n\r\n\r\n**Or did you only fit the test_data at first? I mean, just run:** \r\n\r\n\r\nmodel.fit_generator(  \r\n      test_data,\r\n        steps_per_epoch=474 // 32,\r\n        epochs=25,\r\n        verbose=1,\r\n        workers=4)\r\n-------------------------------------------------------------------\r\nThank you very much! "", 'Hello @cmosquer \r\nfirst, I started with small dataset. and saved the model.\r\nI increased the dataset size gradually. In this way, I could overcome this issue.\r\nI hope this will be helpful for you.\r\n', ""> I had a same issue. When I was training the CNN, the val_acc never updated. And there was minor updates in acc. but it seemed to vibrate. At last, I found the solution.\r\n> The issue was in my training dataset. The size was about 4000. And the size of validation dataset was 500\r\n> I thought that this issue came from the network weight initialization in flatten layer.\r\n> So I changed the dataset and tested. The result was tremendous.\r\n> \r\n> **This was my first training part.**\r\n> \r\n> ```\r\n> model.fit_generator(\r\n>         train_data,\r\n>         steps_per_epoch=4091 // 32,\r\n>         epochs=100,\r\n>         validation_data=test_data,\r\n>         validation_steps=474 // 32,\r\n>         verbose=1,\r\n>         workers=4)\r\n> ```\r\n> \r\n> **I changed it as follows:**\r\n> \r\n> ```\r\n> model.fit_generator(\r\n>         test_data,\r\n>         steps_per_epoch=474 // 32,\r\n>         epochs=25,\r\n>         verbose=1,\r\n>         workers=4)\r\n> ```\r\n> \r\n> And for the next training, I saved the model weights and reused for next training.\r\n> `model.save_weights('weight_model.h5')`\r\n> \r\n> And initialized network weights with this:\r\n> \r\n> ```\r\n> # Load model weights\r\n> model.load_weights('weight_model.h5')\r\n> \r\n> # Compiling the ANN\r\n> model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\r\n> ```\r\n> \r\n> This worked perfectly. I hope this will be helpful. Thanks.\r\n\r\n@Paulraita \r\n\r\nHey Paulraita,\r\nGreat stuff, but on which data you have checked for validation accuracy?\r\nOr any separate dataset for test accuracy?\r\nhow much was the improvement on training and validation accuracy ? \r\nDid you changed the number of freezed layers ?"", ""@gauravgola96 \r\nI checked its validation accuracy with separate dataset.\r\nFor some solutions, it could be helpful to start from small size of dataset.\r\nAnd we can't solve all the kind of issues like this with the method I used before.\r\nThere are many solutions to overcome this. For example, in my case, batch normalization was one of the solutions. And for your last question, I never changed it. Nowadays I often use callback to monitor the training process. Did you try to change the learning rate?"", '@Paulraita \r\nYes, I have changed the learning rate and also used ReduceLROnPlateau callback as well.\r\nWhen trained my inceptionv3 with first 90 layers freezed and trained it on small dataset of 350 images from 12 classes.I used the weights (trained on small dataset) for training the large dataset afterwards on same network and with same number of freezed layers. The training accuracy increased very much but the validation accuracy still not able to cross 60%.\r\n\r\nTrain data - 7200 images from 12 classes (balanced)\r\nTest data - 350 images from 12 classes\r\n']","["" python\n\n  n_filters=32\n  n_conv_row=n_conv_col=400\n  weight_init='he_normal'\n  img_rows=img_cols=400\n  nn_dense=25\n  n_classes=2\n  loss_func='categorical_crossentropy'\n  model = Sequential()\n  model.add(Convolution2D(n_filters, n_conv_row, n_conv_col,\n                          border_mode='valid',init=weight_init,\n                          input_shape=(1, img_rows, img_cols)))\n  model.add(Activation('relu'))\n  model.add(Flatten())\n  model.add(Dense(nn_dense, init=weight_init))\n  model.add(Activation('relu'))\n  model.add(Dense(n_classes, init=weight_init))\n  model.add(Activation('softmax'))\n  model.compile(loss=loss_func, optimizer='sgd')\n\n"", '\n259s - loss: 10.4525 - acc: 0.4903 - val_loss: 11.4543 - val_acc: 0.4473\n269s - loss: 11.4543 - acc: 0.4473 - val_loss: 11.4543 - val_acc: 0.4473\n251s - loss: 11.4543 - acc: 0.4473 - val_loss: 11.4543 - val_acc: 0.4473\n252s - loss: 11.4543 - acc: 0.4473 - val_loss: 11.4543 - val_acc: 0.4473\n...\n']",[],1,0
131,keras,3572,closed,fit generator slow ,"I am trying to test the fit generator but found the speed is rather slow to around less 100 examples processing per second, I am wondering anything wrong in my code?

 

the train.txt file format:
785 floats separate by comma, with the last as label.
",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],"['from **future** import print_function\nimport numpy as np\nnp.random.seed(1337)  # for reproducibility\n\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.optimizers import SGD, Adam, RMSprop\nfrom keras.utils import np_utils\n\nnb_classes = 10\nnb_epoch = 3\n\ndef gen_data(path=""train.txt"", batchsize=50):\n    while 1:\n        f = open(path)\n        for nb_line, line in enumerate(f):\n            batch_X = []\n            batch_y = []\n            line = line.strip(""\\n"")\n            line = line.split("","")\n            line = [np.float32(i) for i in line]\n            line_X = line[:-1]\n            line_y = int(line[-1])\n            batch_X.append(line_X)\n            batch_y.append(line_y)\n            if nb_line%50 == (batchsize-1):\n                batch_X = np.array(batch_X)\n                batch_y = np_utils.to_categorical(batch_y, 10)\n                yield (batch_X, batch_y)\n        f.close()\n\nmodel = Sequential()\nmodel.add(Dense(512, input_shape=(784,)))\nmodel.add(Activation(\'relu\'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(512))\nmodel.add(Activation(\'relu\'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10))\nmodel.add(Activation(\'softmax\'))\n\nmodel.summary()\n\nmodel.compile(loss=\'categorical_crossentropy\',\n              optimizer=RMSprop(),\n              metrics=[\'accuracy\'])\n\nhistory = model.fit_generator(gen_data(),samples_per_epoch=50000, nb_epoch=nb_epoch, nb_worker=5, pickle_safe=True)\nmodel.save(\'george_mnist.h5\')  ']",1,0
132,keras,3576,closed,Training with Adam gets slower each epoch,"I am training a simple neural network in Keras with Theano backend consisting of 4 dense layers connected to a Merge layer and then to a softmax classifier layer. Using Adam for training, the first few epochs train in about 60s each (in the CPU) but, after that, the training time per epoch starts increasing, taking more than 400s by epoch 70, making it unusable.

Is there anything wrong with my code? Is this supposed to happen or is it a bug?

This only happens when using Adam, not with sgd, adadelta, rmsprop or adagrad.  I'd use any of the other methods but Adam produces far better results.

The code:


",,"[""If it is of any use, I've found this problem to happen when using Adamax or Nadam too.\n"", 'In my case it slows down a bit, but not as much as your case, FYI.\n', ""I'm not sure why this happens, but the issue is with the default parameters in Adam. Specifically, the beta_1 and beta_2 need to be played with. I've found that by increasing beta_1 (I went from .9 to .99), the problem can be fixed. I would ~guess~ that the slowdown occurs because the parameters or gradients or whatever decay too quickly with poorly set beta values, and you run into underflow or overflow issues, but I can't pretend to know exactly how Adam works.\ntl;dr: Raise beta_1 from .9 to .99\n"", 'Same problem like you. Did you fix this?', 'Similar issue with Rmsprop. Any thoughts?', ""I'm experiencing the same problem with Adam optimiser, would be interested in finding out why."", 'Maybe this would help? (not sure if you already saw this, if so sorry for reposting)\r\n""""""I\'m not sure why this happens, but the issue is with the default parameters in Adam. Specifically, the beta_1 and beta_2 need to be played with. I\'ve found that by increasing beta_1 (I went from .9 to .99), the problem can be fixed. I would guess that the slowdown occurs because the parameters or gradients or whatever decay too quickly with poorly set beta values, and you run into underflow or overflow issues, but I can\'t pretend to know exactly how Adam works.\r\ntl;dr: Raise beta_1 from .9 to .99""""""', '@chusb40 Did you get a solution to this problem?', 'Does anybody have a solution?', 'Any solution ? Same happens for me with Adam makes the model unstable for use']","[""\nmodela = Sequential()\nmodela.add(Dense(700, input_dim=40, init='uniform', activation='relu'))\nmodelb = Sequential()\nmodelb.add(Dense(700, input_dim=40, init='uniform', activation='relu'))\nmodelc = Sequential()\nmodelc.add(Dense(700, input_dim=40, init='uniform', activation='relu'))\nmodeld = Sequential()\nmodeld.add(Dense(700, input_dim=40, init='uniform', activation='relu'))\n\nmodel = Sequential()\nmodel.add(Merge([modela, modelb, modelc, modeld], mode='concat', concat_axis=1))\nmodel.add(Dense(258, init='uniform', activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nhist = model.fit([Xa, Xb, Xc, Xd], Ycat, validation_split=.25, nb_epoch=80, batch_size=100, verbose=2)\n""]",[],1,0
133,keras,3578,closed,Fit generator slow to converge than model.fit,"Hi,
  I am training a 2 hidden layer NN and notice that the convergence is faster when using model.fit than using model.fit_generator. The optimizer is the same in both cases ('sgd') and my batch size is 32  in both cases. val_loss decreases much rapidly when using fit. Is this to be expected? Is val_loss calculated differently for fit_generator?

Thanks in advance
",,"[""I am having the same issue. I've been training exactly the same network, but with model.fit it converges much faster.\n"", 'Hi, what about the validation accuracy? Is it the same for model fit and fit_generator\n', ""Look at those losses, I am using the same optimizer(Adadelta), batch_size(=32) and same dataset. The 'samples_per_epoch' is the same as the number of images in the dataset (70k). In the first epoch the model.fit has a better loss than fit_generator with 16 epochs. the validation accuracy  follows the loss: 46% in model.fit after 5 epochs, 6% in fit_generator in 16 epochs.\n\n| Epochs | Model.fit | fit_generator |\n| --- | --- | --- |\n| 1 | 0.01297 | 0.038865 |\n| 2 | 0.00293 | 0.029753 |\n| 3 | 0.0019 | 0.028777 |\n| 4 | 0.00143 | 0.028146 |\n| 5 | 0.00120 | 0.027444 |\n| ... | ... | ... |\n| 16 | ? | 0.026156 |\n"", 'The two are functionally identical. Only significant difference is that `fit` defaults to `shuffle=True`; naturally `fit_generator` does not have a `shuffle` option. This is the cause of what you are observing.\n']",[],[],1,0
134,keras,13311,closed,Very low accuracy,"   Hi everyone, I tried to train a dnn model with Keras, but the acc and val_acc I got were very low, could someone give me some advice about how to solve it? Thank you very much in advance! Below is my code.
 
import keras
import numpy as np
import pandas as pd
from keras.models import Sequential
from keras.models import Model
from keras.layers import Dense, Dropout, Activation, Input
from keras import optimizers, losses
from keras.layers.normalization import BatchNormalization
from keras.callbacks import EarlyStopping
from keras.utils import np_utils, generic_utils
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler

def load_data_train(path, train=True):
    df = pd.read_csv(path)
    x = df.values.copy()
    if train:
        np.random.shuffle(x) 
        x, labels = x[:, 1:-1].astype(np.float32), x[:, -1]
        return x, labels
    else:
        x, ids = x[:, 1:].astype(np.float32), x[:, 0].astype(str)
        return x, ids

def load_data_test(path):
    df = pd.read_csv(path)
    x = df.values.copy()
    x, labels = x[:, 1:-1].astype(np.float32), x[:, -1]
    return x, labels

def preprocess_data(x, scaler=None):
    if not scaler:
        scaler = StandardScaler()
        scaler.fit(x)
    x = scaler.transform(x)   # another type x = StanderScaler().fit_transferom(x)
    return x, scaler

def preprocess_labels(labels, encoder=None, categorical=True):
    if not encoder:
        encoder = LabelEncoder()
        encoder.fit(labels)
    y = encoder.transform(labels).astype(np.int32)
    if categorical:
        y = np_utils.to_categorical(y)
    return y, encoder

print(""Loading data..."")
x_train, labels = load_data_train('train.csv', train=True)
x_train, scaler = preprocess_data(x_train)
y_train, encoder = preprocess_labels(labels)

x_test, labels = load_data_test('dev.csv')
x_test, scaler = preprocess_data(x_test)
y_test, encoder = preprocess_labels(labels)
nb_classes = y_train.shape[1]
print(nb_classes, 'classes')
dims = x_train.shape[1]
print(dims, 'dims')

print(""Building model..."")
model = Sequential()
model.add(Dense(128, activation='relu', input_dim=dims)) 
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Dense(nb_classes, activation='softmax'))

early_stopping = EarlyStopping(monitor='val_loss', patience=30, mode='auto', verbose=2)
sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) 
model.compile(optimizer=sgd, loss=losses.mean_squared_logarithmic_error, metrics=['accuracy'])
print(""Training Model... "")
hist = model.fit(x_train, y_train, epochs=200, batch_size=200, validation_split=0.1) 
score = model.evaluate(x_test, y_test, verbose=2)
classlabel = model.predict_classes(x_test)

print(""Generating submission..."")
print('Test score: ', score[0])
print('Test accuracy: ', score[1])
print(model.summary())


",,['Hi @Jackie-wx .  Did you solve your issue or is this still an open question?\r\nThanks.'],[],[],1,0
135,keras,10232,closed,how to use multicore cpu to do prediction?,,,"[""I suspect this should be done automatically. If not, then you'll probably have better chances finding a solution when getting in touch with the developers of the backend (i.e. tensorflow, theano, ...) that you are using. \r\n\r\nAlso, have you considered to get some help through help-sites like [stackoverflow](https://stackoverflow.com)? I have a suspicion that your question might belong there, rather than in these issue-sections of the projects you are using (unless it would indeed turn out to be a bug).""]",[],[],0,0
136,keras,12148,closed,keras can do  sparse dense?  ,"![image](https://user-images.githubusercontent.com/32533059/51807246-9ecf6280-22bf-11e9-817a-d312001698d2.png)



hello,i have some question  as show as picture ，i want using this ""https://github.com/yhenon/keras-frcnn""'s vgg model , but i want some change their model,but when i search more day ,i can't find out this question solustion 

they are some discuusion:
https://stats.stackexchange.com/questions/282282/how-is-spatial-dropout-in-2d-implemented

exactly what i need is Spatial Dropout2D on define ""pixel "",this mean i need to provide the 
[0,0,0,0,0,0,0]
[0,1,0,1,0,1,0]
[0,0,0,0,0,0,0]
[0,1,0,1,0,1,0]
[0,0,0,0,0,0,0]
[0,1,0,1,0,1,0]
[0,0,0,0,0,0,0] to like mask_drop 



",type:support,[],[],[],0,0
137,keras,12715,closed,Error when loading a model with shared layers and pretrained network,"**System information**  
- Ubuntu 16.04)
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  v1.10.0-0-g656e7a2b34 1.10.0
- Keras version:  2.2.4
- Python version:  3.5

**Describe the current behavior**  
I am training a model with shared weights using concatenate function and using a pretrained VGG16 network which has some non trainable layers as seen below




I save the model without any error, but when try to do  then the following error appears:



If I set all layers in VGG as trainable, then the error does not appear.

**Describe the expected behavior**  
How could I load the model?

**Code to reproduce the issue**  




**Other info**  
If I save  and then load it, the error does not appear. Error is only shown when trying to load the entire model. 

",,"['Hi @jcgarciaca,\r\nI have similar architecture and the same error.\r\nHave you resolved this issue? Any ideas?\r\nThanks!!', 'Hi @hanama \r\nI changed keras version and it worked', '> Hi @hanama\r\n> I changed keras version and it worked\r\n\r\nwhich version you  changed to?']","['\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_9 (InputLayer)            (None, 120, 120, 3)  0                                            \r\n__________________________________________________________________________________________________\r\ninput_10 (InputLayer)           (None, 120, 120, 3)  0                                            \r\n__________________________________________________________________________________________________\r\nsequential_4 (Sequential)       (None, 256)          15894592    input_9[0][0]                    \r\n                                                                 input_10[0][0]                   \r\n__________________________________________________________________________________________________\r\nconcatenate_3 (Concatenate)     (None, 512)          0           sequential_4[1][0]               \r\n                                                                 sequential_4[2][0]               \r\n__________________________________________________________________________________________________\r\ndense_9 (Dense)                 (None, 64)           32832       concatenate_3[0][0]              \r\n__________________________________________________________________________________________________\r\ndropout_11 (Dropout)            (None, 64)           0           dense_9[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_10 (Dense)                (None, 2)            130         dropout_11[0][0]                 \r\n==================================================================================================\r\nTotal params: 15,927,554\r\nTrainable params: 8,292,290\r\nNon-trainable params: 7,635,264\r\n', ""\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-29-1113554484e5> in <module>()\r\n----> 1 model2 = load_model('test_vgg16.h5')\r\n      2 model2.summary()\r\n\r\n~/.virtualenvs/gpu-py3/lib/python3.5/site-packages/keras/engine/saving.py in load_model(filepath, custom_objects, compile)\r\n    417     f = h5dict(filepath, 'r')\r\n    418     try:\r\n--> 419         model = _deserialize_model(f, custom_objects, compile)\r\n    420     finally:\r\n    421         if opened_new_file:\r\n\r\n~/.virtualenvs/gpu-py3/lib/python3.5/site-packages/keras/engine/saving.py in _deserialize_model(f, custom_objects, compile)\r\n    272                                                        original_keras_version,\r\n    273                                                        original_backend,\r\n--> 274                                                        reshape=False)\r\n    275         if len(weight_values) != len(symbolic_weights):\r\n    276             raise ValueError('Layer #' + str(k) +\r\n\r\n~/.virtualenvs/gpu-py3/lib/python3.5/site-packages/keras/engine/saving.py in preprocess_weights_for_loading(layer, weights, original_keras_version, original_backend, reshape)\r\n    680         weights = convert_nested_time_distributed(weights)\r\n    681     elif layer.__class__.__name__ in ['Model', 'Sequential']:\r\n--> 682         weights = convert_nested_model(weights)\r\n    683 \r\n    684     if original_keras_version == '1':\r\n\r\n~/.virtualenvs/gpu-py3/lib/python3.5/site-packages/keras/engine/saving.py in convert_nested_model(weights)\r\n    656                     weights=weights[:num_weights],\r\n    657                     original_keras_version=original_keras_version,\r\n--> 658                     original_backend=original_backend))\r\n    659                 weights = weights[num_weights:]\r\n    660 \r\n\r\n~/.virtualenvs/gpu-py3/lib/python3.5/site-packages/keras/engine/saving.py in preprocess_weights_for_loading(layer, weights, original_keras_version, original_backend, reshape)\r\n    680         weights = convert_nested_time_distributed(weights)\r\n    681     elif layer.__class__.__name__ in ['Model', 'Sequential']:\r\n--> 682         weights = convert_nested_model(weights)\r\n    683 \r\n    684     if original_keras_version == '1':\r\n\r\n~/.virtualenvs/gpu-py3/lib/python3.5/site-packages/keras/engine/saving.py in convert_nested_model(weights)\r\n    668                     weights=weights[:num_weights],\r\n    669                     original_keras_version=original_keras_version,\r\n--> 670                     original_backend=original_backend))\r\n    671                 weights = weights[num_weights:]\r\n    672         return new_weights\r\n\r\n~/.virtualenvs/gpu-py3/lib/python3.5/site-packages/keras/engine/saving.py in preprocess_weights_for_loading(layer, weights, original_keras_version, original_backend, reshape)\r\n    798                                  str(weights[0].size) + '. ')\r\n    799             weights[0] = np.reshape(weights[0], layer_weights_shape)\r\n--> 800         elif layer_weights_shape != weights[0].shape:\r\n    801             weights[0] = np.transpose(weights[0], (3, 2, 0, 1))\r\n    802             if layer.__class__.__name__ == 'ConvLSTM2D':\r\n\r\nIndexError: list index out of range\r\n"", '\r\nfrom keras import backend as K\r\nfrom keras.applications import VGG16\r\nfrom keras import layers\r\nfrom keras.layers import Input, merge, concatenate, BatchNormalization\r\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\r\nfrom keras.layers.core import Activation, Dense, Dropout, Flatten, Lambda\r\nfrom keras.models import Sequential, Model\r\nfrom keras.utils import np_utils\r\nfrom keras import regularizers\r\nfrom keras.models import load_model\r\nimport numpy as np\r\nimport os\r\n%matplotlib inline\r\n\r\ndef conv_base_network():\r\n    conv_base = VGG16(weights=\'imagenet\', include_top=False, input_shape=(120, 120, 3))\r\n        \r\n    conv_base.trainable = True\r\n    \r\n    set_trainable = False\r\n    for layer in conv_base.layers:\r\n        if layer.name == \'block5_conv1\':\r\n            set_trainable = True\r\n        if set_trainable:\r\n            layer.trainable = True\r\n        else:\r\n            layer.trainable = False\r\n            \r\n    \r\n    model = Sequential()\r\n    model.add(conv_base)\r\n    model.add(Flatten())\r\n    model.add(Dropout(0.4))\r\n    model.add(Dense(256, activation=\'relu\'))\r\n    model.add(Dropout(0.4))\r\n        \r\n    return model\r\n\r\nconv_base = conv_base_network()\r\nconv_base.summary()\r\n\r\nimage_left = Input(shape=(120, 120, 3))\r\nimage_right = Input(shape=(120, 120, 3))\r\n\r\nvector_left = conv_base(image_left)\r\nvector_right = conv_base(image_right)\r\n\r\nmerged_features = concatenate([vector_left, vector_right], axis=-1)\r\n\r\nfc1 = Dense(64, activation=\'relu\')(merged_features)\r\nfc1 = Dropout(0.4)(fc1)\r\n\r\npred = Dense(2, kernel_initializer=\'random_uniform\', activation=\'softmax\')(fc1)\r\n\r\nmodel = Model(inputs=[image_left, image_right], outputs=pred)\r\nmodel.summary()\r\n\r\nfrom keras import optimizers\r\n\r\nmodel.compile(loss=""categorical_crossentropy"", optimizer=optimizers.Adam(lr=2e-5), metrics=[""acc""])\r\n\r\nmodel.save(\'test_vgg16.h5\')\r\n\r\nmodel2 = load_model(\'test_vgg16.h5\')\r\n', ""\r\nconv_base.save('conv.h5')\r\nm = load_model('conv.h5')\r\nm.summary()\r\n\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nvgg16 (Model)                (None, 3, 3, 512)         14714688  \r\n_________________________________________________________________\r\nflatten_4 (Flatten)          (None, 4608)              0         \r\n_________________________________________________________________\r\ndropout_9 (Dropout)          (None, 4608)              0         \r\n_________________________________________________________________\r\ndense_8 (Dense)              (None, 256)               1179904   \r\n_________________________________________________________________\r\ndropout_10 (Dropout)         (None, 256)               0         \r\n=================================================================\r\nTotal params: 15,894,592\r\nTrainable params: 8,259,328\r\nNon-trainable params: 7,635,264\r\n_________________________________________________________________\r\n""]","['load_model', 'conv_base']",0,0
138,keras,9591,closed, The size of the output layer 'chairConfidence' in the neural network does not match the number of classes in the classifier.,"Hi All,
I have created CNN successfully, when i tried implement that model into the iOS project i got the following error

The size of the output layer 'chairConfidence' in the neural network does not match the number of classes in the classifier.

code is

Keras predict it properly, iOS app i got the error",,[],"['\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Conv2D\r\nfrom keras.layers import MaxPooling2D\r\nfrom keras.layers import Flatten\r\nfrom keras.layers import Dense\r\nimport coremltools\r\nprint(""Installed"")\r\n# Initialising the CNN\r\nclassifier = Sequential()\r\n# Step 1 - Convolution\r\nclassifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = \'relu\'))\r\n# Step 2 - Pooling\r\nclassifier.add(MaxPooling2D(pool_size = (2, 2)))\r\n# Adding a second convolutional layer\r\nclassifier.add(Conv2D(32, (3, 3), activation = \'relu\'))\r\nclassifier.add(MaxPooling2D(pool_size = (2, 2)))\r\n# Step 3 - Flattening\r\nclassifier.add(Flatten())\r\n# Step 4 - Full connection\r\nclassifier.add(Dense(units = 128, activation = \'relu\'))\r\nclassifier.add(Dense(units = 1, activation = \'sigmoid\'))\r\n# Compiling the CNN\r\nclassifier.compile(optimizer = \'adam\', loss = \'binary_crossentropy\', metrics = [\'accuracy\'])\r\n# Part 2 - Fitting the CNN to the images\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\ntrain_datagen = ImageDataGenerator(rescale = 1./255,\r\n                                   shear_range = 0.2,\r\n                                   zoom_range = 0.2,\r\n                                   horizontal_flip = True)\r\ntest_datagen = ImageDataGenerator(rescale = 1./255)\r\ntraining_set = train_datagen.flow_from_directory(\'dataset/training_set\',\r\n                                                 target_size = (64, 64),\r\n                                                 batch_size = 2,\r\n                                                 class_mode = \'binary\')\r\ntest_set = test_datagen.flow_from_directory(\'dataset/test_set\',\r\n                                            target_size = (64, 64),\r\n                                            batch_size = 2,\r\n                                            class_mode = \'binary\')\r\nclassifier.fit_generator(training_set,\r\n                         steps_per_epoch = 42,\r\n                         epochs = 5,\r\n                         validation_data = test_set,\r\n                         validation_steps = 8)\r\n# Part 3 - Making new predictions\r\nimport numpy as np\r\nfrom keras.preprocessing import image\r\ntest_image = image.load_img(\'dataset/single_prediction/1.jpeg\', target_size = (64, 64))\r\ntest_image = image.img_to_array(test_image)\r\ntest_image = np.expand_dims(test_image, axis = 0)\r\nresult = classifier.predict(test_image)\r\ntraining_set.class_indices\r\nprint(result[0][0])\r\nif result[0][0] == 1:\r\n    prediction = \'plastic\'\r\nelse:\r\n    prediction = \'metal\'\r\nprint(prediction)\r\ncoreml_model = coremltools.converters.keras.convert(classifier, input_names=[\'image\'], output_names=[\'chairConfidence\'], class_labels=[\'metal\',\'plastic\'], image_input_names=[\'image\'])\r\ncoreml_model.save(\'my_model.mlmodel\')\r\n']",[],0,0
139,keras,11430,closed,Why lambda layer has no weights?,"I implemented some tf operations with lambda layer, but I found this layer has no weights in my model.
So how can I train this lambda layer and load its weights?
I just want to use lambda layer to transform tf tensor into Keras tensor",type:support,"[""You don't need weights if you don't have a linear layer or some other trainable transformation.\r\nCan you show your code ?""]",[],[],0,0
140,keras,11304,closed,how to apply constraints before construct graph?,"from the source code, I find the constraints of weights in keras just apply after each gradient descent updating. could I apply it at weights directly?

for example, at Dense layer, kernel_ constraint=unit_norm, can I define x\*unit_norm(W) rather that x\*W?",type:support,['same problem: https://github.com/keras-team/keras/issues/9068'],[],[],0,0
141,keras,11315,closed,keras sequential model,"hello,I have a question can we implement LSTM encoding and decoding of text data in Sequential model API  instead of functional API.

",type:support,[],[],[],0,0
142,keras,12598,closed,kernel_initializer = 'zeros' doesnt work for Dense layers,This is not supported for kernel weights but is for bias weights.  Propose to add capability for kernel weights to be consistant across the API.,type:feature,[],[],[],0,0
143,keras,11867,closed,"How to guarantee that Keras model loading the pretrained weights correctly, by setting  skip_mismatch=True ?","Dear,
Scenario Description 

Step #1:
I replaced the backbone in yoloV3 from Darknet53 to MobileNet. The MobileNet weights is loaded from the Imagenet, by using
.

Step #2:
I also want to load pre-trained weights for detector yolov3, so I download the .cfg and .weights files from the https://pjreddie.com/darknet/yolo/ where we can find the file links as shown below in the original page.
----------------------------------------------------------------------------------
YOLOv3-416 | COCO trainval | test-dev | 55.3 | 65.86 Bn | 35 | cfg | weights
----------------------------------------------------------------------------------
Obviously, the .weights and .cfg files for YOLOv3-416 include the weights for both yoloV3 detector, and its backbone, Darknet53.

Step #3 :
when we develop the new yolo body (yolo body = backbone + yolo Detector), by using, for example, 


where  ""weights_path"" is the place where we save the  .weights file in Step #2(Of course, the .cfg and .weights have been converted to .h5 file).

Now comes the issue: the new yolo body is composed of mobilenet + yoloV3, while the loaded pre-trained .h5 weights  is for Darknet53+yoloV3. How can we guarantee that we load the correct weights to the correct model, in this scenario, obviously, I just want to load the yoloV3 detector part to the new yolo body, and the Darknet53 weights in the .h5 file will be automatically dropped. 

How to guarantee this, in a formal way? Just by simply setting ""skip_mismatch=True"" ?",type:support,['Relevant question on stackoverflow: https://stackoverflow.com/questions/47214382/how-can-i-know-if-weights-were-loaded-in-keras-model/47214490#47214490.'],[],"[""mobilenet = MobileNet(input_tensor=inputs,weights='imagenet') "", 'model_body = yolo_body(image_input, num_anchors // 3, num_classes) ', 'model_body.load_weights(weights_path, by_name=True, skip_mismatch=True) ']",0,0
144,keras,9877,closed,Data Augmentation for Object Detection with Fully Convolutional Networks ,"**Feature Request**

[This Keras blog][1] explains nicely, how a small dataset can be augmented by the following code:

    from keras.preprocessing.image import ImageDataGenerator
    
    datagen = ImageDataGenerator(
            rotation_range=40,
            width_shift_range=0.2,
            height_shift_range=0.2,
            rescale=1./255,
            shear_range=0.2,
            zoom_range=0.2,
            horizontal_flip=True,
            fill_mode='nearest')

I am sure the vanilla example introduced in this blog works well, for similarly simple scenarios. 

In a much more complicated scenario, I want to use the weights of models pretrained on the famous [COCO dataset for object detection][2], to transfer learn new classes, for which I have only a very limited amount of data (<=1000).

The labeling granularity in such datasets is not per image, but per objects inside the images. I.e., each image may contain one or more objects which are marked by polygonical bounding boxes and these bounding boxes are labeled according to the object names they contain. This complex labeling information is encoded in json format, like in the following example:

    {
	""info"": {
		""year"": 2018,
		""version"": null,
		""description"": ""Peaches"",
		""contributor"": ""ralph@r4robotics.com.au"",
		""url"": ""labelbox.io"",
		""date_created"": ""2018-04-07T10:08:51.409340+00:00""
	},
	""images"": [{
		""id"": ""cjfp6vz7xfwz20198ixce9la4"",
		""width"": 274,
		""height"": 184,
		""file_name"": ""https://firebasestorage.googleapis.com/v0/b/labelbox-193903.appspot.com/o/cjfp6hjghfuvd01147d130984%2F5a7fdf5d-201a-40d0-bfef-c36d6ed02212%2Fpeach8.jpg?alt=media&token=11337eaa-4ffd-4dfb-b3ec-9c4ee6bd2f17"",
		""license"": null,
		""flickr_url"": ""https://firebasestorage.googleapis.com/v0/b/labelbox-193903.appspot.com/o/cjfp6hjghfuvd01147d130984%2F5a7fdf5d-201a-40d0-bfef-c36d6ed02212%2Fpeach8.jpg?alt=media&token=11337eaa-4ffd-4dfb-b3ec-9c4ee6bd2f17"",
		""coco_url"": ""https://firebasestorage.googleapis.com/v0/b/labelbox-193903.appspot.com/o/cjfp6hjghfuvd01147d130984%2F5a7fdf5d-201a-40d0-bfef-c36d6ed02212%2Fpeach8.jpg?alt=media&token=11337eaa-4ffd-4dfb-b3ec-9c4ee6bd2f17"",
		""date_captured"": null
	}, {
		""id"": ""cjfp6wqfhfwyu0107il09db3p"",
		""width"": 275,
		""height"": 183,
		""file_name"": ""https://firebasestorage.googleapis.com/v0/b/labelbox-193903.appspot.com/o/cjfp6hjghfuvd01147d130984%2F5a7fdf5d-201a-40d0-bfef-c36d6ed02212%2Fpeach9.jpg?alt=media&token=39dd5e97-c411-43e9-9ba3-9f51a334c7c7"",
		""license"": null,
		""flickr_url"": ""https://firebasestorage.googleapis.com/v0/b/labelbox-193903.appspot.com/o/cjfp6hjghfuvd01147d130984%2F5a7fdf5d-201a-40d0-bfef-c36d6ed02212%2Fpeach9.jpg?alt=media&token=39dd5e97-c411-43e9-9ba3-9f51a334c7c7"",
		""coco_url"": ""https://firebasestorage.googleapis.com/v0/b/labelbox-193903.appspot.com/o/cjfp6hjghfuvd01147d130984%2F5a7fdf5d-201a-40d0-bfef-c36d6ed02212%2Fpeach9.jpg?alt=media&token=39dd5e97-c411-43e9-9ba3-9f51a334c7c7"",
		""date_captured"": null
	}],
	""annotations"": [ {
		""id"": 23,
		""image_id"": ""cjfp6vz7xfwz20198ixce9la4"",
		""category_id"": 1,
		""segmentation"": [
			[31.0, 72.0, 63.0, 84.0, 75.0, 105.0, 67.0, 134.0, 68.0, 158.0, 44.0, 174.0, 24.0, 178.0, 2.0, 172.0, 2.0, 82.0, 31.0, 72.0]
		],
		""area"": 6301.0,
		""bbox"": [2.0, 6.0, 73.0, 106.0],
		""iscrowd"": 0
	}, {
		""id"": 24,
		""image_id"": ""cjfp6vz7xfwz20198ixce9la4"",
		""category_id"": 1,
		""segmentation"": [
			[75.0, 103.0, 108.0, 76.0, 137.0, 74.0, 166.0, 89.0, 182.0, 104.0, 188.0, 145.0, 179.0, 171.0, 167.0, 183.0, 92.0, 183.0, 72.0, 158.0, 68.0, 134.0, 75.0, 103.0]
		],
		""area"": 10652.5,
		""bbox"": [68.0, 1.0, 120.0, 109.0],
		""iscrowd"": 0
	}, {
		""id"": 25,
		""image_id"": ""cjfp6vz7xfwz20198ixce9la4"",
		""category_id"": 1,
		""segmentation"": [
			[169.0, 92.0, 182.0, 66.0, 211.0, 53.0, 246.0, 66.0, 262.0, 80.0, 268.0, 95.0, 261.0, 129.0, 241.0, 145.0, 216.0, 153.0, 188.0, 143.0, 184.0, 105.0, 169.0, 92.0]
		],
		""area"": 6838.5,
		""bbox"": [169.0, 31.0, 99.0, 100.0],
		""iscrowd"": 0
	}, {
		""id"": 26,
		""image_id"": ""cjfp6wqfhfwyu0107il09db3p"",
		""category_id"": 1,
		""segmentation"": [
			[86.0, 54.0, 109.0, 56.0, 119.0, 73.0, 113.0, 92.0, 93.0, 101.0, 76.0, 92.0, 70.0, 77.0, 71.0, 63.0, 86.0, 54.0]
		],
		""area"": 1715.0,
		""bbox"": [70.0, 82.0, 49.0, 47.0],
		""iscrowd"": 0
	}, {
		""id"": 27,
		""image_id"": ""cjfp6wqfhfwyu0107il09db3p"",
		""category_id"": 1,
		""segmentation"": [
			[117.0, 95.0, 123.0, 110.0, 136.0, 118.0, 153.0, 113.0, 159.0, 99.0, 158.0, 87.0, 145.0, 79.0, 132.0, 76.0, 123.0, 84.0, 117.0, 95.0]
		],
		""area"": 1260.0,
		""bbox"": [117.0, 65.0, 42.0, 42.0],
		""iscrowd"": 0
	}, {
		""id"": 28,
		""image_id"": ""cjfp6wqfhfwyu0107il09db3p"",
		""category_id"": 1,
		""segmentation"": [
			[109.0, 54.0, 115.0, 40.0, 133.0, 32.0, 146.0, 34.0, 157.0, 43.0, 161.0, 58.0, 152.0, 72.0, 133.0, 76.0, 119.0, 71.0, 109.0, 54.0]
		],
		""area"": 1660.5,
		""bbox"": [109.0, 107.0, 52.0, 44.0],
		""iscrowd"": 0
	}],
	""licenses"": [],
	""categories"": [{
		""supercategory"": ""Peach"",
		""id"": 1,
		""name"": ""Peach""
	}]
}

Obviously, augmentation in this scenario is a bit more complicated, since  not only the images have to be distorted and rotated, but also the bounding boxes.   It would just make Keras so much more powerful and save users a lot of painstaking labeling work, if such a dataset could be augmented within a Keras pipeline.  And with the increasing relevance of Object Detection, this might be a feature to consider.

  [1]: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html
  [2]: http://cocodataset.org/#home
",,"[""How would you augment the bounding boxes in the case of for example rotations? Would you rotate the bounding box also (so that it's not aligned with the image boundaries anymore) or would you keep the boundaries of the bounding box parallel to the image axes and readjust it to enclose the rotated feature in the picture?\r\n\r\nI think that in general the ImageGenerator should be changed to allow more flexible functionality, for example passing through additional data that just gets passed through without manipulation and support for custom augmentation functions (like you would need here)."", ""Data augmentation is a great feature, with a very limited applicability. You can't use it for regression, either. ""]",[],[],0,0
145,keras,11779,closed,Add masking support to convolution layers (or at least determine whether it is feasible).,"
Note created by @fchollet in the ""Requests for contributions"".
",Enhancement stat:contributions welcome,"[""Hey there, I'd like to work on this issue. Is there any addition detail that I need to know before getting started on this? @gabrieldemarmiesse ""]",[],[],0,0
146,keras,11055,closed,"When using pycharm with remote run, the progress bar won't refresh in place.","![tim 20180901192728](https://user-images.githubusercontent.com/5326601/44947933-66fde500-ae1d-11e8-96fd-43ace664425f.png)
",,[],[],[],0,0
147,keras,10612,closed,Keras crashes with multiple sessions,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

When attempting to use multiple sessions with , the kernel crashes without error.  Possibly an OOM problem, but I currently do not know.  The code below crashes around i=3 or i=4 for me, which is not very good.

OS: Windows 10
running on CPU only


",,[],"['\r\nfrom sklearn.datasets import load_iris\r\nimport numpy as np\r\nimport sklearn\r\nimport keras\r\nimport keras.wrappers.scikit_learn\r\nimport tensorflow as tf\r\nimport keras.models\r\nimport os\r\n\r\n\r\ndef sessioned(f):\r\n    def sessioned_f(self, *args, **kwargs):\r\n        if not hasattr(self, ""sess""):\r\n            self.sess = tf.Session()\r\n        with self.sess.as_default():\r\n            return f(self, *args, **kwargs)\r\n        return result\r\n    return sessioned_f\r\n\r\nclass LogisticRegression(keras.wrappers.scikit_learn.KerasClassifier):   \r\n    def __init__(self, n_epochs=100, **kwargs):\r\n        self.n_epochs = n_epochs\r\n        super().__init__(**kwargs)\r\n    @sessioned\r\n    def fit(self, X, y,**kwargs):\r\n        # get the shape of X and one hot y\r\n        self.input_shape = X.shape[-1]\r\n        self.label_encoder = sklearn.preprocessing.LabelEncoder()\r\n        self.label_encoder.fit(y)\r\n        self.output_shape = len(self.label_encoder.classes_)\r\n        label_encoded = self.label_encoder.transform(y).reshape((-1,1))\r\n        y_onehot = sklearn.preprocessing.OneHotEncoder().fit_transform(label_encoded).toarray()\r\n        super().fit(X,y_onehot,epochs=self.n_epochs,verbose=1,**kwargs)\r\n        return self\r\n    @sessioned\r\n    def predict_proba(self, X):\r\n        return super().predict_proba(X)\r\n    def check_params(self, params):\r\n        #fuckit\r\n        pass\r\n    @sessioned\r\n    def __call__(self): # the build_fn thing\r\n        # create model\r\n        model = keras.models.Sequential()\r\n        model.add(keras.layers.Dense(self.output_shape, input_dim=self.input_shape, kernel_initializer=""normal"", activation=""softmax""))\r\n        # Compile model\r\n        model.compile(loss=\'categorical_crossentropy\', optimizer=\'adam\')\r\n        return model\r\n\r\ndata = load_iris()\r\ni=0\r\nwhile True:\r\n    print(i)\r\n    graph = tf.Graph()\r\n    with graph.as_default():\r\n        model = LogisticRegression()\r\n        model.fit(data.data, data.target)\r\n        model.sess.close()\r\n        del model\r\n    i+=1\r\n    del graph\r\n']",['keras'],0,0
148,keras,8657,closed,Introduction of global metrics (precision and recall),,,"[""Keras computes batchwise metrics in averages.\r\n\r\nSo those can't be accurately calculated .\r\n\r\nSee my improvement suggestion:\r\nhttps://github.com/fchollet/keras/issues/8607"", 'I agree that global metrics like precision and recall are important to have. @Kritikalcoder or @brge17, would you like to work on it?\r\n\r\nThe general template would be callable metrics classes (similar to layers, but simpler) that maintain weights (a state containing the global metric value). Calling an instance updates the state and returns the current state value. Makes sense?', 'Makes sense, I will give it a shot and raise a PR.', ""Note that the main change will be to add support for such metrics in `training.py`. We'll need a special case for them because *they will have update ops* and these update ops need to be incorporated into the training function and evaluate function."", 'Can we, please, have confusion matrix ASAP (i.e. making sure this new mechanism supports this use case). In fact, maybe precision and recall, etc should be calculated from the confusion matrix for a binary classifier anyway to sync the code. AUC is also an interesting use case (code design wise), probably the same very design can work if and only if it works for precision-recall curve.\r\n\r\nIn fact, maybe we can just have a single object that accumulates `y_true, y_pred, etc`, then applies whatever function you want to them, which, most of the time will be one of\r\nhttp://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics', '@ozabluda \r\n\r\nThe metric class we are describing will wrap any arbitrary metric to make it a global metric.\r\n\r\nThis includes False Positives, False Negatives, True Positive, True Negative (and there rate counterparts)\r\n\r\nThink of it like, global_metric(my_custom_metric(y_pred, y_true))', 'These are the global metrics that I think should be introduced:\r\n1. Micro precision\r\n2. Micro recall\r\n3. Micro F-score\r\n4. Macro precision\r\n5. Macro recall\r\n6. Macro F-score\r\n7. Confusion matrix', ""For the reference, here is a list of TF metrics, in case we'll (almost certainly) have to wrap both np and TF ones:\r\nhttps://www.tensorflow.org/api_docs/python/tf/metrics\r\nhttps://www.tensorflow.org/api_docs/python/tf/contrib/metrics"", 'Maybe we should have metrics classes that look like:\r\n\r\n```python\r\n\r\nclass MyMetric(object):\r\n\r\n     def __init__(self, ...):\r\n          self.true_positives = K.variable(...)\r\n          ...\r\n\r\n     def __call__(self, y_true, y_pred):\r\n         # will be called in `compile`\r\n         self.updates = [...]   # update state\r\n         return metric_value_so_far\r\n\r\n     def reset_state(self):\r\n         # will be called at the beginning of an epoch\r\n         K.batch_set_value([self.true_positives, ...], [0, ...])\r\n\r\n    def get_config(self):\r\n        ...\r\n\r\n    @classmethod\r\n    def from_config(self, config):\r\n        ...\r\n```\r\n\r\n`reset_state` should also be called when resetting model state (`model.reset_state()`, if I recall correctly).\r\n\r\nDo we need anything else to make it work?\r\n          \r\n           \r\n', 'I like that design, I was just going to pass the custom metric to the constructor:\r\n\r\n```\r\nclass GlobalMetric(object):\r\n\r\n     def __init__(self, my_metric):\r\n          self.metric_func = my_metric ...\r\n\r\n# Define the metric outside the class\r\ndef my_metric(y_pred, y_true):\r\n\t...\r\n\treturn score\r\n```\r\n\r\nWith the idea GlobalMetric could wrap any metric', '> With the idea GlobalMetric could wrap any metric\r\n\r\nI believe that would not work in the general case. The nature of the state, how to update it, and how to go from predictions/labels plus state to the current value, are all potentially-custom logic.', ""How I saw global metric wrapper above is it would work for any metric, but potentially be very expensive to compute.\r\n\r\nIt would essentially cache the predictions in the state, and recompute the metric from scratch each time new predictions are available. That solves the most general case, but is not necessarily compute efficient for every case.\r\n\r\n> The nature of the state, how to update it, and how to go from predictions/labels plus state to the current value, are all potentially-custom logic.\r\n\r\nSo for thing like False Positives or False Negatives, etc... We could write highly efficient implementations, but for a user custom metric they would get an admittedly inefficient (potentially) implementation but one that would work.\r\n\r\nThe metric I care most about, AUC, I don't have a clever way to cache it in state off the top of my head. Perhaps, I could read the TensorFlow streaming AUC for inspiration."", '> It would essentially cache the predictions in the state, and recompute the metric from scratch each time new predictions are available. That solves the most general case, but is not necessarily compute efficient for every case.\r\n\r\nThat would be expensive, in particular it would consume a lot of memory for large datasets. The streaming approach is preferable -- we typically only need to keep around a few scalars updated at each iteration, for most metrics.\r\n', 'In this PR, should the scope be limited to showing how to implement a couple key metrics in the fashion shown above? This will give people a starting point for their own custom metrics and knock out a couple of popular requests.\r\n\r\nI was thinking {false positive, false negative, true positive, true negative, precision, recall, auc}.\r\n\r\nPoint taken on the compute end.', 'Yes, exactly. We just want to push out a template, and cover precision/recall since these are very common and useful.', 'all those ""key metrics"" can be extracted from the confusion matrix.  \r\n>Can we, please, have confusion matrix ASAP (i.e. making sure this new mechanism supports this use case). In fact, maybe precision and recall, etc should be calculated from the confusion matrix for a binary classifier anyway to sync the code. AUC is also an interesting use case (code design wise), probably the same very design can work if and only if it works for precision-recall curve.', 'I will probably code it up some time next week during the week.\r\n\r\nI already have False Positive, False Negative, True Positive, True Negative (rate*), precision, recall tensor operations coded up and tested as normal keras metrics (batch wise averages). I will just need to adapt it to have state as shown above.', 'Great. This is an important feature.', 'Having issues with reset_state (also reset_states) not doing anything on epoch end.  From a quick glance at the source, it looks like reset state only works for layers. Not quite sure the best way to do this neatly, open for suggestions.\r\n\r\nHere is a quick mock up that assumes one hot encoding:\r\n\r\n```\r\nfrom abc import abstractmethod\r\n\r\nfrom keras import backend as K\r\n\r\nclass GlobalMetric(object):\r\n\r\n    @abstractmethod\r\n    def __call__(self, y_true, y_pred):\r\n        raise NotImplementedError(""Method not implemented."")\r\n\r\n    @abstractmethod\r\n    def update_states(self):\r\n        raise NotImplementedError(""Method not implemented."")\r\n\r\n    @abstractmethod\r\n    def reset_states(self):\r\n        raise NotImplementedError(""Method not implemented."")\r\n\r\nclass TruePositives(GlobalMetric):\r\n\r\n    def __init__(self, threshold=None):\r\n\r\n        self.__name__ = ""true_positives""\r\n        if threshold is None:\r\n            self.threshold = K.variable(value=0.5)\r\n        else:\r\n            self.threshold = K.variable(value=threshold)\r\n        # tp = true positives\r\n        self.tp = K.variable(value=0.0)\r\n\r\n    def __call__(self, y_true, y_pred):\r\n        self.update_states(y_true, y_pred)\r\n        return self.tp\r\n\r\n    def reset_state(self):\r\n        self.tp = K.update(self.tp, K.variable(value=0.0))\r\n\r\n    def reset_states(self):\r\n        self.tp = K.update(self.tp, K.variable(value=0.0))\r\n\r\n    def update_states(self, y_true, y_pred):\r\n\r\n        # Slice the positive score\r\n        y_true = y_true[:, 1]\r\n        y_pred = y_pred[:, 1]\r\n\r\n        # Softmax -> probabilities\r\n        y_pred = K.cast(y_pred >= self.threshold, \'float32\')\r\n        # c = correct classifications\r\n        c = K.cast(K.equal(y_pred, y_true), \'float32\')\r\n        # tp_batch = number of true positives in a batch\r\n        tp_batch = K.sum(c * y_true)\r\n\r\n        self.tp = K.update_add(self.tp, tp)\r\n```', 'This requires a couple of ugly changes to the Base Logger and Progress Bar:\r\n\r\nBasically, if metric is a GlobalMetric (turn off batch averaging) else do what currently was happening.', ""See fork: https://github.com/brge17/keras/tree/brian-dev\r\n\r\nRemaining issues:\r\n1. Find the proper place to reset state (for {train, eval, test}) currently have a hack that doesn't really work.\r\n2. Find least pass of resistance to update BaseLogger, ProgBar. This involves disabling batch averaging selectively for ProgBar etc...\r\n\r\nSolved some issues some remain.\r\n  "", ""The most critical issue is that if reset_states() is called after model.compile it does nothing as it isn't added to the graph.\r\n\r\n So either reset_states() is called at the end of an epoch, and it updates some un-initialized tensor that isn't actually part of the global metric, or it is called every batch (and we are back where we started)"", ""If all metrics will be single numbers, would there be a simple way to make it work with TensorBoard? Part of what's really useful about the metrics argument in fit is that it shows up in the TB web interface."", 'It will work with TensorBoard, I have a private fork that is getting closer to functioning.\r\n\r\n-b', ""I'm not sure if this thread is still active, but I've implemented precision/recall using the stateful metrics PR given above following from BinaryTruePositives (https://github.com/keras-team/keras/blob/master/tests/keras/metrics_test.py).  Are we working on implementing such requested metrics? \r\n\r\nFor anyone that would like it, this is what it looks like so far. I was only able to get it to work by saving the confusion matrix within the class however. \r\n\r\nhttps://gist.github.com/vsocrates/7ff65268c2ed533a62f8f9f9d86786af"", 'I tried to raise it to master, but we argued about what the definition should be for multi-class. I went with one vs others.\r\n\r\nhttps://github.com/keras-team/keras/pull/9393', ""I've seen that progress has been made (#9200, #9253), but i'm not sure - is this feature usable now?"", '@GalAvineri yes', 'Here\'s a working example:\r\n\r\n\r\n```python\r\nimport keras\r\nfrom keras.layers import Input, Dense\r\nfrom keras.models import Model\r\n\r\nimport numpy as np\r\n\r\nclass BatchCounter(keras.layers.Layer):\r\n\r\n        def __init__(self, name=""batch_counter"", **kwargs):\r\n            super(BatchCounter, self).__init__(name=name, **kwargs)\r\n            self.stateful = True\r\n            self.batches = keras.backend.variable(value=0, dtype=""int32"")\r\n\r\n        def reset_states(self):\r\n            keras.backend.set_value(self.batches, 0)\r\n\r\n        def __call__(self, y_true, y_pred):\r\n            updates = [\r\n                keras.backend.update_add(\r\n                    self.batches, \r\n                    keras.backend.variable(value=1, dtype=""int32""))]\r\n            self.add_update(updates)\r\n            return self.batches\r\n\r\n# Dummy dataset\r\nX = np.ones((50, 1))\r\ny = np.zeros((50, 1))\r\n\r\n# Dummy model\r\ninputs = Input(shape=(1,))\r\noutputs = Dense(1)(inputs)\r\nmodel = Model(inputs=inputs, outputs=outputs)\r\nmodel.compile(loss=""mse"", optimizer=""adam"", metrics=[BatchCounter()])\r\n\r\nmodel.fit(X, y, batch_size=10, epochs=10, validation_data = (X, y))\r\n```', ""That's a great example! Thank you!\r\nIt also seems that most of  the metrics requests of @Kritikalcoder are filled by previous comments."", 'They are pretty easy to write, I have internal implementations at work that only work for the TensorFlow backend.\r\n\r\nYou can pretty easily translate any TensorFlow (streaming metric) to a StateFul Metric.', 'For the lazy amongst us, I’ll post the implementations iv\'e collected:\r\nThe implementation of recall and precision were made by @briannemsick and the f1 score was implemented by the \'keras_metrics\' library:\r\n\r\n```python\r\nfrom tensorflow.keras.layers import Layer\r\nimport tensorflow.keras.backend as K\r\nfrom operator import truediv\r\n\r\n\r\nclass Recall(Layer):\r\n    \'\'\'Compute recall over all batches.\r\n    # Arguments\r\n        name: String, name for the metric.\r\n        class_ind: Integer, class index.\r\n    \'\'\'\r\n    def __init__(self, name=\'recall\', class_ind=1):\r\n        super(Recall, self).__init__(name=name)\r\n        self.true_positives = K.variable(value=0, dtype=\'float32\')\r\n        self.total_positives = K.variable(value=0, dtype=\'float32\')\r\n        self.class_ind = class_ind\r\n\r\n    def reset_states(self):\r\n        K.set_value(self.true_positives, 0.0)\r\n        K.set_value(self.total_positives, 0.0)\r\n\r\n    def __call__(self, y_true, y_pred):\r\n        \'\'\'Update recall computation.\r\n        # Arguments\r\n            y_true: Tensor, batch_wise labels\r\n            y_pred: Tensor, batch_wise predictions\r\n        # Returns\r\n            Overall recall for the epoch at the completion of the batch.\r\n        \'\'\'\r\n        # Batch\r\n        y_true, y_pred = _slice_by_class(y_true, y_pred, self.class_ind)\r\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n        total_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\r\n        # Current\r\n        current_true_positives = self.true_positives * 1\r\n        current_total_positives = self.total_positives * 1\r\n        # Updates\r\n        updates = [K.update_add(self.true_positives, true_positives),\r\n                   K.update_add(self.total_positives, total_positives)]\r\n        self.add_update(updates, inputs=[y_true, y_pred])\r\n        # Compute recall\r\n        return (current_true_positives + true_positives) / \\\r\n               (current_total_positives + total_positives + K.epsilon())\r\n\r\n\r\nclass Precision(Layer):\r\n    \'\'\'Compute precision over all batches.\r\n    # Arguments\r\n        name: String, name for the metric.\r\n        class_ind: Integer, class index.\r\n    \'\'\'\r\n    def __init__(self, name=\'precision\', class_ind=1):\r\n        super(Precision, self).__init__(name=name)\r\n        self.true_positives = K.variable(value=0, dtype=\'float32\')\r\n        self.pred_positives = K.variable(value=0, dtype=\'float32\')\r\n        self.class_ind = class_ind\r\n\r\n    def reset_states(self):\r\n        K.set_value(self.true_positives, 0.0)\r\n        K.set_value(self.pred_positives, 0.0)\r\n\r\n    def __call__(self, y_true, y_pred):\r\n        \'\'\'Update precision computation.\r\n        # Arguments\r\n            y_true: Tensor, batch_wise labels\r\n            y_pred: Tensor, batch_wise predictions\r\n        # Returns\r\n            Overall precision for the epoch at the completion of the batch.\r\n        \'\'\'\r\n        # Batch\r\n        y_true, y_pred = _slice_by_class(y_true, y_pred, self.class_ind)\r\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n        pred_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\r\n        # Current\r\n        current_true_positives = self.true_positives * 1\r\n        current_pred_positives = self.pred_positives * 1\r\n        # Updates\r\n        updates = [K.update_add(self.true_positives, true_positives),\r\n                   K.update_add(self.pred_positives, pred_positives)]\r\n        self.add_update(updates, inputs=[y_true, y_pred])\r\n        # Compute recall\r\n        return (current_true_positives + true_positives) / \\\r\n               (current_pred_positives + pred_positives + K.epsilon())\r\n\r\n\r\nclass F1(Layer):\r\n    """"""Create a metric for the model\'s F1 score calculation.\r\n    The F1 score is the harmonic mean of precision and recall.\r\n    """"""\r\n\r\n    def __init__(self, name=\'f1\', class_ind=1):\r\n        super().__init__(name=name)\r\n        self.recall = Recall(class_ind=class_ind)\r\n        self.precision = Precision(class_ind=class_ind)\r\n        self.class_ind = class_ind\r\n\r\n    def reset_states(self):\r\n        """"""Reset the state of the metrics.""""""\r\n        self.precision.reset_states()\r\n        self.recall.reset_states()\r\n\r\n    def __call__(self, y_true, y_pred):\r\n        pr = self.precision(y_true, y_pred)\r\n        rec = self.recall(y_true, y_pred)\r\n        return 2 * truediv(pr * rec, pr + rec + K.epsilon())\r\n\r\n\r\ndef _slice_by_class(y_true, y_pred, class_ind):\r\n    \'\'\' Slice the batch predictions and labels with respect to a given class\r\n    that is encoded by a categorical or binary label.\r\n    #  Arguments:\r\n        y_true: Tensor, batch_wise labels.\r\n        y_pred: Tensor, batch_wise predictions.\r\n        class_ind: Integer, class index.\r\n    # Returns:\r\n        y_slice_true: Tensor, batch_wise label slice.\r\n        y_slice_pred: Tensor,  batch_wise predictions, slice.\r\n    \'\'\'\r\n    # Binary encoded\r\n    if y_pred.shape[-1] == 1:\r\n        y_slice_true, y_slice_pred = y_true, y_pred\r\n    # Categorical encoded\r\n    else:\r\n        y_slice_true, y_slice_pred = y_true[..., class_ind], y_pred[..., class_ind]\r\n    return y_slice_true, y_slice_pred\r\n```', '> They are pretty easy to write, I have internal implementations at work that only work for the TensorFlow backend.\r\n> \r\n> You can pretty easily translate any TensorFlow (streaming metric) to a StateFul Metric.\r\n\r\n@brge17 \r\nAre you suggesting wrapping the code as follows? In which case what should go in `reset_state`?\r\n```python\r\ndef __call__(self, y_true, y_pred):\r\n     metric_val, update_op = tf.contrib.metrics.some_streaming_op(y_true, y_pred)\r\n     self.add_update(update_op, inputs=[y_true, y_pred])\r\n     return metric_val\r\n```\r\n\r\nOr did you pull apart tensorflow\'s streaming metrics and place the relevant logic within a stateful subclass of `Layer` (i.e. ""translate"")?', 'Hi @ZackHodari . @brge17  was the github tag I used for open source contributions at my last job.\r\n\r\nI did the latter.\r\n\r\nAlso worth noting in the forthcoming TensorFlow 2.0.0 release, all of the streaming metrics will work with tf.keras:\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/ba7f78b9b0535f38e3e12dcb0b095fc7a6e7bca5#diff-09395f6eebca1f408ed2e91c63477511\r\n\r\nIf you have another metric in mind, I can help.', 'Glad to see that this issue has lead to the fruition of something useful. Thank you @briannemsick and everyone else here, for pursuing this. This is my first open source ""contribution"", if one may call it that. ', '> For the lazy amongst us, I’ll post the implementations iv\'e collected:\r\n> The implementation of recall and precision were made by @briannemsick and the f1 score was implemented by the \'keras_metrics\' library:\r\n> \r\n> ```python\r\n> from tensorflow.keras.layers import Layer\r\n> import tensorflow.keras.backend as K\r\n> from operator import truediv\r\n> \r\n> \r\n> class Recall(Layer):\r\n>     \'\'\'Compute recall over all batches.\r\n>     # Arguments\r\n>         name: String, name for the metric.\r\n>         class_ind: Integer, class index.\r\n>     \'\'\'\r\n>     def __init__(self, name=\'recall\', class_ind=1):\r\n>         super(Recall, self).__init__(name=name)\r\n>         self.true_positives = K.variable(value=0, dtype=\'float32\')\r\n>         self.total_positives = K.variable(value=0, dtype=\'float32\')\r\n>         self.class_ind = class_ind\r\n> \r\n>     def reset_states(self):\r\n>         K.set_value(self.true_positives, 0.0)\r\n>         K.set_value(self.total_positives, 0.0)\r\n> \r\n>     def __call__(self, y_true, y_pred):\r\n>         \'\'\'Update recall computation.\r\n>         # Arguments\r\n>             y_true: Tensor, batch_wise labels\r\n>             y_pred: Tensor, batch_wise predictions\r\n>         # Returns\r\n>             Overall recall for the epoch at the completion of the batch.\r\n>         \'\'\'\r\n>         # Batch\r\n>         y_true, y_pred = _slice_by_class(y_true, y_pred, self.class_ind)\r\n>         true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n>         total_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\r\n>         # Current\r\n>         current_true_positives = self.true_positives * 1\r\n>         current_total_positives = self.total_positives * 1\r\n>         # Updates\r\n>         updates = [K.update_add(self.true_positives, true_positives),\r\n>                    K.update_add(self.total_positives, total_positives)]\r\n>         self.add_update(updates, inputs=[y_true, y_pred])\r\n>         # Compute recall\r\n>         return (current_true_positives + true_positives) / \\\r\n>                (current_total_positives + total_positives + K.epsilon())\r\n> ```\r\n\r\nCould you give an example for how to use one of these metrics? Doing something like\r\n```\r\n    model.compile(loss=\'binary_crossentropy\',\r\n    \t      optimizer=tf.keras.optimizers.RMSprop(lr=args[""lr""], decay=args[""decay""]),\r\n    \t      metrics=[\'acc\', Precision(), Recall()])\r\n```\r\nunfortunately didn\'t work.']",[],[],0,0
149,keras,9186,closed,clearer doc for class_weights,"Hello, 
So I wanted to use  the  option when using . 
My y object was one hot encoded, so I did not know which keys I should use for 
the dict. 
Turns out that on , the mapping is determined using using the column index of the output:

Should we make this clearer in the documentation of the  function?
Thank you very much, 
Pierre",,[],['      \r\n if y.shape[1] > 1:\r\n            y_classes = y.argmax(axis=1)\r\n'],"['class_weight', 'model.fit', 'class_weight', '_standardize_weights', '.fit']",0,0
150,keras,11371,closed,Why weight restoring is not the default option in EarlyStopping?,"I was looking over the Keras documentation, when I noticed that the default for keras.callbacks.EarlyStopping is restore_best_weights=False.

In the documentation, the described behaviour is:
restore_best_weights: whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used.

I find this a bit confusing because I think the principle of early stopping is to obtain the best results on the validation set by stopping the optimisation when the validation loss increases, which is the case without setting the patience parameter.

However, in early stopping it makes sense to set the patience parameter because you might get spurious overfittings due to momentary local optima. In that case, you would definitely want to restore the previous weights, because you might severely overfit after the end of your patience period.
Furthermore, I could not think of any reason why you would not want restore best weights in Early Stopping. The only case I can think of when you want to stop your training when there is no significant improvement on your loss, in order to reduce training time, but this is not the purpose of early stopping.
  
I'm wondering if there is any use case at all to make the False the default behaviour and if not, would not it make sense to change this?",type:support,"[""If you want to restore the weights that are giving the best performance, you have to keep tracks on them and thus have to store them. This can be costly as you have to keep another entire model in memory and can make fitting the model slower as well.\r\n\r\nI think this is why the default behavior is to stop when the monitored metric isn't improving anymore — hence the default small `patience` to try to keep a good model."", ""I see the reason now, but I still don't think that this is a good default option. I do see that you have to keep another model in memory, and this can be costly, but the model could have moved away a lot from the local optima in the patience period, meaning the model needs to be retrained from the beginning or from the last saved checkpoint, which is more costly. I think the default behaviour should be emphasised more in the documentation also when explaining the restore_best_weights, not only in the default function call, because it is very easy to look over this, due to the scrollbar.\r\n"", ""Maybe it's just the documentation that needs to be a bit clearer on this point."", ""I thought I'd have to do model checkpointing and early stopping - both - to be able to stop training early and save the model with the best performance. \r\n\r\nUntil I realised that you can just do early stopping with restore_best_weights changed to True.""]",[],[],0,0
151,keras,9284,closed,Large Model Zoo for Keras along like Caffe2 but even better for the community of contributors,"Why doesn't the Keras team have an ""official"" and ""large"" model zoo like Caffe2 [https://github.com/caffe2/caffe2/wiki/Model-Zoo]?

I see the following repositories on Github but they are indeed insufficient:

https://github.com/albertomontesg/keras-model-zoo
https://github.com/Yoctol/yoctol-keras-layer-zoo
https://github.com/david-vazquez/keras_zoo
https://github.com/GKalliatakis/Keras-Application-Zoo

What is the best way to get people to start sharing pretrained models in terms of the following:
1. opening up an issue
2. discussing this over the mailing list
3. having you send an email to the major contributors of keras or even the committers

Please let me know if you have any further questions or concerns.

Thanks!",,['Same https://github.com/keras-team/keras/issues/9276'],[],[],0,0
152,keras,11194,closed,batch_size dangerous default,"Please give a warning when using default  parameter in .

I just spent an hour debugging a magic number '32' in the model, which did not occur anywhere in my code.  
After successfully training, the following failed on the *same* input sample:  
 

finally found the culprit:  
https://keras.io/models/model/#predict  
batch_size: Integer. If unspecified, it will default to 32.


 should either use the shape of the sample as batch_size, or the same one as the model in training or use '1' as default (usually we just want to predict one item anyways), but don't fallback to an arbitrary default value without a warning please.
",type:support,"['Could you please give a sample script and the stacktrace to reproduce the error? Thank you. ', '32 is used when steps is also None :\r\n```\r\n        if batch_size is None and steps is None:\r\n            batch_size = 32\r\n```']",[],"['batch_size', 'model.predict', 'model.predict(sample)', 'predict']",0,0
153,keras,10366,closed,Brightness range not documented,In https://keras.io/preprocessing/image/#imagedatagenerator-class brightness_range is not documented.,,"[""Wouldn't that just be:\r\n\r\n`brightness_range`: Tuple of floats; range to pick a brightness value from.\r\n\r\nIf yes, more than happy to make the change in the documentation. 👍"", '@dynamicwebpaige, I find `brightness_range` supports both tuple and list type ([source](https://github.com/keras-team/keras-preprocessing/blob/ea4b8e16d48e2522e6b497d7df9c04aedc63fc7b/keras_preprocessing/image.py#L1060))\r\n\r\nWould it be more clear like: \r\n\r\n`brightness_range`: Tuple or list of two floats; range to pick a brightness value from.', 'it will be even clearer if the actual valid range is documented. Internally, it uses PIL ImageEnhance module and the documentation [there] is: (https://pillow.readthedocs.io/en/3.0.x/reference/ImageEnhance.html#PIL.ImageEnhance.Brightness)\r\n\r\n>   ""An enhancement factor of 0.0 gives a black image. A factor of 1.0 gives the original image.""', ""@assaflehr \r\nI thought it just scales the brightness of image via PIL ImageEnhance module, but if so, it should return the original image when I give ```brightness_range=(1.0, 1.0)```, however, my experiment showed that ```brightness_range = (1.0, 1.0)``` gave more brighter images than ```brightness_range = None``` (default)\r\n\r\nActually the manual in keras.io says\r\n> brightness_range: Tuple or list of two floats. Range for picking a brightness shift value from.\r\n\r\nthat 'shift' may have different meaning from scale, I guess."", ""The term 'shift' is ambiguous here. When shifting I'd expect the value 0 to mean 'leave as is', a value > to make it brigther and < 0 darker. But as @assaflehr pointed out, here 1.0 means 'leave as is'. The behaviour noted by @jaewooklee93 is odd, did you maybe do any other preprocessing steps that might mess with the brightness? \r\n\r\nI propose changing the documentation of ```brightness_range``` to \r\n>Tuple or list of two floats. Range for picking the new brightness (relative to the original image), with 1.0 being the original's brightness. Must not be smaller than 0."", ""@thunfischtoast \r\nI tried more debugging and I almost figured out why that phenomenon happens. The below is a minimally working example code.\r\n\r\n```Python\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom keras.preprocessing.image import *\r\n\r\nimg = np.random.rand(1, 500, 500, 3)\r\n\r\nfig, ax = plt.subplots(1, 5, figsize=(20, 10))\r\nax = ax.ravel()\r\nax[0].imshow(img[0])\r\nax[1].imshow(next(ImageDataGenerator().flow(img))[0])\r\nax[2].imshow(next(ImageDataGenerator(brightness_range=(0., 0.)).flow(img))[0])\r\nax[3].imshow(next(ImageDataGenerator(brightness_range=(1., 1.)).flow(img))[0])\r\nax[4].imshow(next(ImageDataGenerator(brightness_range=(1., 1.)).flow(img))[0] / 255)\r\n```\r\n\r\nOutput:\r\n![download](https://user-images.githubusercontent.com/11328376/50370859-d167aa80-05f1-11e9-9d1a-f9d07a4b3569.png)\r\n\r\n\r\nBy default, if `brightness_range` is not specified, `ImageDataGenerator` returns [0..1] ranged float images from [0..1] ranged images. (as you can see at `ax[1]`)\r\n\r\nHowever, when it comes to `brightness_range=(1., 1.)`, it returns [0..255] ranged float images even from [0..1] ranged images, and it's quite not conventional to handle float image with [0..255] ranged-values.\r\n\r\nStill, the term 'shift' is ambiguous as you mentioned, so both docs and the behavior of function itself should be updated, I guess.\r\n"", 'I agree. This is also the case for other arguments of the image data generator. There needs to be a description of the valid ranges.\r\n\r\n', 'I have also experienced that if I am using brightness augementation during training, I need to use brightness_range=(1., 1.) during inference as well, or I would get worse results. Anyone else experienced this phenomenon?', '@AzharSultan, I found that Keras use the library  keras_preprocessing  to make the data augmentation. \r\nHere is the code for random brightness \r\nhttps://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/image/affine_transformations.py#L215\r\n\r\n```\r\n    x = array_to_img(x)\r\n    x = imgenhancer_Brightness = ImageEnhance.Brightness(x)\r\n    x = imgenhancer_Brightness.enhance(brightness)\r\n    x = img_to_array(x)\r\n```\r\nand the first line will transform a numpy ndarry type to a PIL Image instance -- `x = array_to_img(x)`\r\nthe last line will return to an array  -- `x = img_to_array(x)` \r\nBut the problem is that returned x will be between 0 and 255, and the dtype is `float32`. \r\nso you need to divide by 255 manually or change the dtype to `uint8` ', ""As I found, 0.5 is remain unchange, 1 for totally white and 0 for totally dark, so I guess it's a linear project?\r\n\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom keras.preprocessing.image import *\r\nimg =128 + np.zeros([1, 500, 500, 3])\r\nfig, ax = plt.subplots(1, 5, figsize=(20, 10))\r\nax = ax.ravel()\r\nax[0].imshow(img[0].astype(int))\r\nax[1].imshow(next(ImageDataGenerator(brightness_range=(0.2, 0.2)).flow(img))[0].astype(int))\r\nax[2].imshow(next(ImageDataGenerator(brightness_range=(0., 0.)).flow(img))[0].astype(int))\r\nax[3].imshow(next(ImageDataGenerator(brightness_range=(0.5, 0.5)).flow(img))[0].astype(int))\r\nax[4].imshow(next(ImageDataGenerator(brightness_range=(1., 1.)).flow(img))[0].astype(int))\r\n\r\n![image](https://user-images.githubusercontent.com/46110602/58425997-edee9e00-80cd-11e9-8577-40ecbc4856ef.png)\r\n"", ""@zlw21gxy \r\nThat's weird...\r\nIf you use a real image, then < 1 makes it darker. > 1 makes it brighter.\r\n\r\nFor example, try to change the 4th line of your code by:\r\n\r\n```\r\nfrom skimage.data import astronaut\r\nimg = astronaut()[np.newaxis].astype(int)\r\n```\r\n\r\n![a](https://user-images.githubusercontent.com/11894291/58483661-fa700680-8160-11e9-91a8-6c02e35656e5.png)\r\n\r\n**EDIT:** it seems like the brightness operator doesn't work well for single-color images like the one from @zlw21gxy. For example, it works well for a synthetic image created like this:\r\n\r\n```\r\nimg = np.zeros([1, 500, 500, 3])\r\nimg[:, :250, :250, 0] = 128\r\nimg[:, 250:, :250, 0] = 255\r\nimg[:, :250, 250:, :] = 128\r\nimg[:, 250:, 250:, :] = 255\r\n```\r\n\r\n![Figure_2](https://user-images.githubusercontent.com/11894291/58544317-4d999600-8201-11e9-9e6d-a99127acd05d.png)"", ""Yes, that’s really weird,I try to generate random number for every pixel\nand it behave different with using a gray picture, if I use gray picture\n0.5 is stay unchanged, if use real images or random number, 1 is stay\nunchanged, that is so weird....\n\nOn Tue, May 28, 2019 at 10:01 PM Ricardo Cruz <notifications@github.com>\nwrote:\n\n> @zlw21gxy <https://github.com/zlw21gxy>\n> That's weird...\n> If you use a real image, then < 1 makes it darker. > 1 makes it brighter.\n>\n> For example, try to change the 4th line of your code by:\n>\n> from skimage.data import astronaut\n> img = astronaut()[np.newaxis].astype(int)\n>\n> [image: a]\n> <https://user-images.githubusercontent.com/11894291/58483661-fa700680-8160-11e9-91a8-6c02e35656e5.png>\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/keras-team/keras/issues/10366?email_source=notifications&email_token=AK7ZPCQ4TDFRZNBZUOLUNW3PXU3LRA5CNFSM4FDSASI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWMG3IQ#issuecomment-496528802>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AK7ZPCS3WNNYBVN4ZJ2TFSTPXU3LRANCNFSM4FDSASIQ>\n> .\n>\n"", '@rpmcruz \r\nhey bro, did u figure out the reason?\r\n""I try to generate random number for every pixel and it behave different with using a gray picture, if I use gray picture 0.5 is stay unchanged, if use real images or random number, 1 is stay unchanged""', 'I just ran across this thread after wondering why my model training went horribly badly after adding image augmentation.  Based on the little documentation in TF 2.0.0b0, ""Range for picking a brightness shift value from"" I naiively used [-0.2, 0.2]. So yeah, I +1 any of the above documentation improvements that state the ranges allowed.\r\nI also wonder why I didn\'t get an exception or assertion for using a negative value.', ""@zlw21gxy @rpmcruz this is a bug of `keras` or should say of the `keras_preprocessing` package.\r\nI am testing on the lastest verson `Keras-Preprocessing==1.1.0`\r\nIf you go to the code with regard of brightness, you will see\r\n```python\r\ndef apply_brightness_shift(x, brightness):\r\n    #...\r\n    x = array_to_img(x) # !!!note this!!!\r\n    x = imgenhancer_Brightness = ImageEnhance.Brightness(x)\r\n    x = imgenhancer_Brightness.enhance(brightness)\r\n    x = img_to_array(x)\r\n    return x\r\n```\r\n\r\nthen you go to the definition of `array_to_img`:\r\n``` python\r\ndef array_to_img(x, data_format='channels_last', scale=True, dtype='float32')\r\n```\r\nAs you can see, `array_to_img` actually has a parameter of scale, yet not set when called in `apply_brightness_shift`.\r\n\r\nAs a result, a gray image (value = 128) will be scale to a white one (value = 255).\r\nTo vertify, if you simply change the range of values in your image (e.g. even one single channel of one single pixel), you can get everything what you supposed:\r\n```python\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom keras.preprocessing.image import *\r\nimg =128 + np.zeros([1, 500, 500, 3])\r\nimg[0,0,0,0] = 255 # !!! just add this or similar !!!\r\nfig, ax = plt.subplots(1, 5, figsize=(20, 10))\r\nax = ax.ravel()\r\nax[0].imshow(img[0].astype(int))\r\nax[1].imshow(next(ImageDataGenerator(brightness_range=(0.2, 0.2)).flow(img))[0].astype(int))\r\nax[2].imshow(next(ImageDataGenerator(brightness_range=(0., 0.)).flow(img))[0].astype(int))\r\nax[3].imshow(next(ImageDataGenerator(brightness_range=(0.5, 0.5)).flow(img))[0].astype(int))\r\nax[4].imshow(next(ImageDataGenerator(brightness_range=(1., 1.)).flow(img))[0].astype(int))\r\n```\r\n![image](https://user-images.githubusercontent.com/16003088/61203657-95c33800-a71d-11e9-8b15-185d0c00fd80.png)\r\n\r\n\r\n"", 'Is this resolved yet?\r\n', ""I'm also curious if this was resolved."", 'Looking trough this, this looks like it should be removed, because it is too buggy to be used as is. And too scary to use. Looks like it behaves in very complex ways with various other parameters and also dependent on the image. As such I would refrain from using it. Although the idea of varying the brightness is very useful. But with this parameter one has to wade through dozens of other lines of code and do a fair bit of testing before being able to use it.', 'Just a note that `brightness_range` for `ImageDataGenerator` is defined differently to `tf.image.adjust_brightness`.\r\n\r\n`brightness_range` applies a multiplication operation, so 1 is no change. (At least theoretically, other commenters here have pointed out that there are changes sometimes.)\r\n\r\n`adjust_brightness` applies an addition operation, so 0 is no change.', 'Hello, I have found why the image gets shifted when suing ImageDataGenerator when it should not (when setting brightness_range = (1,1)). It is due to min-max image scaling which happens whenever brightness_range is set because of a missing parameter which defaults to True and leads to scaling. More explanation in my pull request https://github.com/keras-team/keras-preprocessing/pull/328 and issue report https://github.com/keras-team/keras-preprocessing/issues/327 .']",[],[],0,0
154,keras,12879,closed,Get gradient of preprocessing without forward passing each time ,"I'm currently working on a regression problem where I need to preprocess the raw input data in a certain way (what's begin done is not relevant) before I can feed it to my neural network. I'm aware that preprocessing is usually done outside the model, but as I also need the gradients of the output of the network wrt. the raw input data (used in the loss function), I need the preprocessing to be included in the computational graph.

From what I've gathered, I need to feed the resulting tensor as input to the neural network in order for the two parts to be connected in the graph. An important note is that the preprocessing is done on the entire training set, meaning it only needs to be done once.

I've successfully implemented this using Keras backend functions, but the problem is that the preprocessing part seems to be performed at every forward pass of the model, which is both really unnecessary, and really slow. My hope was that the input tensor would be treated as some static (but differentiable) input to the network, with the forward-backward propagation simply flowing between the network input and output during training, but it doesn't seem to be working that way.

As my code is quite large and intricate, I'll leave a ""semi-pseudic"" code to simply show the gist of what I'm trying to do.



In the preprocessing function I have added a print-statement to the tensor that prints whenever the function is called, and it does indeed print at every epoch during training. This also happens even if I don't include the gradient in the loss function, showing that it most likely prints in the forward pass. The model does seem to train and predict as intended however, so the implementation seems to be correct.

There might be some flaws in my logic/understanding of how the graph works, and I'm hoping there is a simple fix to my problem.

To summarize:

1.  How do I feed a ""constant"", differentiable tensor as input to a neural network, without the tensor being calculated anew during each forward pass through the network?

As the preprocessed tensor contains all training samples, I would also like to split and train on smaller batches, so:

2.  Given there exists a solution to 1., how does batch-training tie into this? Do I have to create a generator and fit with fit_generator, or is it possible to let Keras handle it with fit?

Hope the problem was clear enough, and I'm grateful for all help/leads!
",type:support,[],"['python\r\nimport tensorflow.keras.backend as K\r\nfrom tensorflow.keras.layers import Input, Dense\r\nfrom tensorflow.keras.models import Model\r\n\r\nraw_data, target_data = load_data()\r\nraw_tens = K.variable(raw_data)\r\nout_tens = K.variable(target_data)\r\n\r\ndef preprocess(data_tens):\r\n    input_tensor = ... # Preprocess the raw data using backend functions\r\n    # Print statement to see when this operation is being performed\r\n    input_tensor = K.print_tensor(input_tensor, message=""Passed!"")\r\n\r\n    return input_tensor\r\n\r\nin_tens = preprocess(raw_tens)\r\n\r\nmodel_input = Input(tensor=in_tens)\r\n\r\n# Define layers in the model\r\nh = Dense(100, activation=\'tanh\')(model_input)\r\n...\r\noutput = Dense(1, activation=\'linear\')(h)\r\n\r\n# Define a custom loss function using the gradient wrt raw data\r\ndef custom_loss(y_true, y_pred):\r\n    grad = K.gradients(y_pred, raw_tens)[0]\r\n\r\n    loss = ... # some loss including the computed gradient\r\n    return loss\r\n\r\n# Define model and compile model using a custom loss function\r\nmodel = Model(inputs=model_input, outputs=output)\r\nmodel.compile(loss=custom_loss, optimizer=\'adam\')\r\n\r\n# Fit model\r\nmodel.fit(y = out_tens, epochs = 1000, steps_per_epoch = 1)\r\n']",[],0,0
155,keras,11579,closed,Custom output layer / loss function,"I am new to the keras world and i am trying to build a feedfoward net which output should be a vector of parameters to be used in a linear ensembler, i want to minimize the MSE for the ensembler, then, i created the following loss function,



g_data is my matrix for single estimators predictions, which multiplyed to theta (net output) gives me the real predictions. Next, i create the model as follows,



Of course it does not run, cause y_train is a single output meanwhile i want to train a vector of outputs, as well, i would have to pass the sample index for each batch in order to use the proper rows of g_data in the loss function calculation.

Any ideas on how to working around this?",type:support,[],[],"['def loss(theta, y_train):\r\n    return np.sum(np.square(np.dot(g_data, theta) - y_train))', ""model2 = Sequential()\r\nmodel2.add(Dense(200, activation='relu'))\r\nmodel2.add(Dense(len(estimators), activation='linear'))\r\nmodel2.compile(loss=loss, optimizer='adam')\r\nmodel2.fit(x_train, y_train, epochs=2, batch_size=32, verbose=1)""]",0,0
156,keras,3510,closed,Question about mnist siamese example,"In the mnist siamese example, two mnist images are mapped onto a vector. The two vectors are merged using euclidian distance. The distance should be low if the images belong to the same number and be high if the two images represent different images.

Questions: In line 59, the labels are set for positive and negative examples. If the euclidian distance for the same numbers should be low, why is the target 1 and why is it 0 for the negative examples? Also, the vectors are created using relu which normally does not have a max_value. How is the euclidian distance kept between 0 and 1?
",stale,"['There is a `margin` in `contrastive_loss` implementation and `K.maximum(margin - y_pred, 0)` remains positive even when `y_pred` is larger than `margin`.\n\nIntuitively the whole expression `K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))` means you want `y_pred` to be close to 0 when `y_true` is 1 (for positive pairs) and `y_pred` close or bigger than 1 when `y_true` is 0 (for negative pairs).\n', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],0,0
157,keras,7488,closed,Embedding layer (Documentation),"Documentation of an Embedding Layer is bit not clear...

Does embedding layer performs word2vec or something different?
If it performs word2vec, I would suggest to mention it in [documentation](https://keras.io/layers/embeddings/))

And to add some relevant reference.
In opinion current reference  [dropout in RNN](https://arxiv.org/abs/1512.05287) is not directly connected to   word2vec.


keras/keras/layers/embeddings.py
https://github.com/fchollet/keras/blob/master/keras/layers/embeddings.py#L11",,"[""Not necessarily word2vec, it's just a lookup table. You can put any real-valued vectors into it."", 'I would like to know what Embedding Layer does. There is an issue running the embedding layer with tensorflow in distributed mode. What is the equivalent tensor in TF to achieve similarly? \r\n\r\nThe error I have:\r\n```\r\nINFO:tensorflow:Error reported to Coordinator: <class \'tensorflow.python.framework.errors_impl.InvalidArgumentError\'>, Assign requires shapes of both tensors to match. lhs shape= [66,128] rhs shape= [65,128]\r\n\t [[Node: save/Assign_14 = Assign[T=DT_FLOAT, _class=[""loc:@embedding/embeddings""], use_locking=true, validate_shape=true, _device=""/job:ps/replica:0/task:0/cpu:0""](embedding/embeddings/RMSProp_1, save/RestoreV2_14)]]\r\n\r\n```\r\n\r\nIt does not happen when I trained the model in a single machine (pure keras).', ' found the answer for my question which is:\r\n\r\nhttps://stackoverflow.com/questions/38189713/what-is-an-embedding-in-keras\r\n[from the link above]\r\nAs far as I know, the Embedding layer is a simple matrix multiplication that transforms words into their corresponding word embeddings.\r\n\r\nThe weights of the Embedding layer are of the shape (vocabulary_size, embedding_dimension). For each training sample, its input are integers, which represent certain words. The integers are in the range of the vocabulary size. The Embedding layer transforms each integer i into the ith line of the embedding weights matrix.\r\n\r\nIn order to quickly do this as a matrix multiplication, the input integers are not stored as a list of integers but as a one-hot matrix. Therefore the input shape is (nb_words, vocabulary_size) with one non-zero value per line. If you multiply this by the embedding weights, you get the output in the shape\r\n\r\n`(nb_words, vocab_size) x (vocab_size, embedding_dim) = (nb_words, embedding_dim)`\r\nSo with a simple matrix multiplication you transform all the words in a sample into the corresponding word embeddings.\r\n\r\nAnd such embedding (matrix) is learned on the fly during the optimization\r\n\r\n**P.S. but still in my opinion the documentation (regarding embedding) should be enhanced.**']",[],[],0,0
158,keras,53,closed,Accessing internal states,"Hey guys, cool project. The theano interface itself was really horrific and off-putting. 

Maybe I'm doing it wrong but is there any way to access the activations of different layers? Similar to predict but only computed half way. Would be really useful for analysis and the likes. 
",,"['Sure, you can. See the answer to this question: https://github.com/fchollet/keras/issues/41\n\nIt will probably be made more straightforward in future API upgrades.\n']",[],[],0,0
159,keras,2398,closed,Correct value for 'filter_length' for final layer(s) in Fully Convolutional Net,"Hi,

If we implement fully convolutional net in Keras, we want it to be able to work with input of arbitrary size.
In our case of 1D data - with arbitrary number of timesteps.

We can use None as an instruction that input could be of any size on the 1st dimension, e.g.:


But in the last layer(s) of  fully CN model we have to use actual length of the sequence as the filter_length in order to obtain sequence of labels correctly distributed along original timespan, e.g.:



As soon as we don't know actual sequence length before runtime, we have the following options:
1. Omit this parameter - but it's mandatory and it will not work
2. Use None value for the filter_length in order to tell Keras that this value will be available on runtime only (see input shape above) - it will cause an error (see output below)
3. Use some kind of placeholder value during network buildup and hope that it will be magically replaced by the real sequence length during runtime. We use this option and it looks like it works, but I wonder if it's correct and how other developers deal with this issue?

May be it's worth implementing filter_length = None as an option for arbitrary sequence?

Example code is here - https://gist.github.com/lukovkin/e57dc3d40c9148a65c2bf40ea6360e45
Environment - Python 3.4, TF 0.8, Keras 1.0.1

> AttributeError                            Traceback (most recent call last)
> /root/miniconda2/envs/tf/lib/python3.4/site-packages/numpy/core/fromnumeric.py in prod(a, axis, dtype, out, keepdims)
>    2488         try:
> -> 2489             prod = a.prod
>    2490         except AttributeError:
> 
> AttributeError: 'tuple' object has no attribute 'prod'
> 
> During handling of the above exception, another exception occurred:
> 
> TypeError                                 Traceback (most recent call last)
> <ipython-input-5-ae9ea8f75655> in <module>()
>       1 model_d = ufcnn_model_deconv(regression = False, output_dim=3, features=4, 
> ----> 2                                    loss=""categorical_crossentropy"", sequence_length=500, optimizer=rmsprop )
> 
> /notebook/ufcnn-keras/models/UFCNN_functional.py in ufcnn_model_deconv(sequence_length, features, nb_filter, filter_length, output_dim, optimizer, loss, regression, class_mode, activation, init)
>     285 
>     286     else:
> --> 287         conv9 = Convolution1D(nb_filter=output_dim, filter_length=None, border_mode='same', init=init, name='conv9')(relu8)
>     288         activation = Activation('softmax', name='activation')(conv9)
>     289         output = activation
> 
> /root/miniconda2/envs/tf/lib/python3.4/site-packages/Keras-1.0.1-py3.4.egg/keras/engine/topology.py in **call**(self, x, mask)
>     456                                     '')
>     457             if len(input_shapes) == 1:
> --> 458                 self.build(input_shapes[0])
>     459             else:
>     460                 self.build(input_shapes)
> 
> /root/miniconda2/envs/tf/lib/python3.4/site-packages/Keras-1.0.1-py3.4.egg/keras/layers/convolutional.py in build(self, input_shape)
>     118         input_dim = input_shape[2]
>     119         self.W_shape = (self.nb_filter, input_dim, self.filter_length, 1)
> --> 120         self.W = self.init(self.W_shape, name='{}_W'.format(self.name))
>     121         self.b = K.zeros((self.nb_filter,), name='{}_b'.format(self.name))
>     122         self.trainable_weights = [self.W, self.b]
> 
> /root/miniconda2/envs/tf/lib/python3.4/site-packages/Keras-1.0.1-py3.4.egg/keras/initializations.py in lecun_uniform(shape, name, dim_ordering)
>      41         http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf
>      42     '''
> ---> 43     fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)
>      44     scale = np.sqrt(3. / fan_in)
>      45     return uniform(shape, scale, name=name)
> 
> /root/miniconda2/envs/tf/lib/python3.4/site-packages/Keras-1.0.1-py3.4.egg/keras/initializations.py in get_fans(shape, dim_ordering)
>      13         # TF kernel shape: (..., input_depth, depth)
>      14         if dim_ordering == 'th':
> ---> 15             fan_in = np.prod(shape[1:])
>      16             fan_out = shape[0]
>      17         elif dim_ordering == 'tf':
> 
> /root/miniconda2/envs/tf/lib/python3.4/site-packages/numpy/core/fromnumeric.py in prod(a, axis, dtype, out, keepdims)
>    2490         except AttributeError:
>    2491             return _methods._prod(a, axis=axis, dtype=dtype,
> -> 2492                                   out=out, keepdims=keepdims)
>    2493         return prod(axis=axis, dtype=dtype, out=out)
>    2494     else:
> 
> /root/miniconda2/envs/tf/lib/python3.4/site-packages/numpy/core/_methods.py in _prod(a, axis, dtype, out, keepdims)
>      33 
>      34 def _prod(a, axis=None, dtype=None, out=None, keepdims=False):
> ---> 35     return umr_prod(a, axis, dtype, out, keepdims)
>      36 
>      37 def _any(a, axis=None, dtype=None, out=None, keepdims=False):
> 
> TypeError: unsupported operand type(s) for *: 'int' and 'NoneType'

Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],"[""main_input = Input(name='input', shape=(None, features))"", ""conv9 = Convolution1D(nb_filter=output_dim, filter_length=sequence_length, border_mode='same', init=init)(relu8)"", 'layer.build(batch_input_shape)']",0,0
160,keras,2608,closed,Zero-padding for ResNet shortcut connections when channel number increase,"Hello everybody,

I would like to implement the ResNet network with the shortcut connections that add zero entries when features/channels dimensions mismatch according to the original paper:

> When the dimensions increase (dotted line shortcuts
> in Fig. 3), we consider two options: (A) The shortcut still
> performs identity mapping, with extra zero entries padded
> for increasing dimensions ...
> http://arxiv.org/pdf/1512.03385v1.pdf

However wasn't able to implement it and I can't seem to find an answer on the web or on the source code. All the implementations that I found use the 1x1 convolution trick for shortcut connections when dimensions mismatch.

The layer I would like to implement would basically concatenate the input tensor with a tensor with an all zeros tensor to compensate for the dimension mismatch.

The idea would be something like this, but I could not get it working:



Does anyone has an idea on how to implement such a layer ?

Thanks a lot 
",stale,"['Edit: I posted the issue as a comment sorry\n', ""> All the implementations that I found use the 1x1 convolution trick for\n> shortcut connections when dimensions mismatch.\n\nThey do this because it's what you should be doing. Residual connections\nwith different shapes should be handled via a learned linear transformation\nbetween the two tensors, e.g. a 1x1 convolution with appropriate strides\nand border_mode, or for Dense layers, just a matrix multiplication.\n\nOn 4 May 2016 at 07:42, LouisMartin notifications@github.com wrote:\n\n> Hello everybody,\n> \n> I would like to implement the ResNet network with the shortcut connections\n> that add zero entries when features/channels dimensions mismatch according\n> to the original paper:\n> \n> When the dimensions increase (dotted line shortcuts\n> in Fig. 3), we consider two options: (A) The shortcut still\n> performs identity mapping, with extra zero entries padded\n> for increasing dimensions ...\n> http://arxiv.org/pdf/1512.03385v1.pdf\n> \n> However wasn't able to implement it and I can't seem to find an answer on\n> the web or on the source code. All the implementations that I found use the\n> 1x1 convolution trick for shortcut connections when dimensions mismatch.\n> \n> The layer I would like to implement would basically concatenate the input\n> tensor with a tensor with an all zeros tensor to compensate for the\n> dimension mismatch.\n> \n> The idea would be something like this, but I could not get it working:\n> \n> def zero_pad(x, shape):\n>     return K.concatenate([x, K.zeros(shape)], axis=1)\n> \n> Does anyone has an idea on how to implement such a layer ?\n> \n> Thanks a lot\n> \n> —\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly or view it on GitHub\n> https://github.com/fchollet/keras/issues/2608#issuecomment-216886552\n"", 'Hi fchollet,\n\nI am actually trying to implement the smallest possible network based on the SqueezeNet paper.\nThe problem is that these linear transformations add a lot of new parameters to my network which I cannot afford. The number of parameters goes from 800.000 to 1.100.000 which is too big an increase for the accuracy improvement (which I already tested).\n\nMy two options when dimensions mismatch are either:\n    - No shortcut connections (which already works well)\n    - Zero padded shortcuts with no added parameters\nI am pretty sure that the zero padded shortcuts are better than nothing so that is why I was asking.\n\nQuoting the ResNet paper:\n\n> (A) zero-padding shortcuts are used\n> for increasing dimensions, and all shortcuts are parameterfree\n> (the same as Table 2 and Fig. 4 right); (B) projection\n> shortcuts are used for increasing dimensions, and other\n> shortcuts are identity; and (C) all shortcuts are projections.\n> Table 3 shows that all three options are considerably better\n> than the plain counterpart. B is slightly better than A. We\n> argue that this is because the zero-padded dimensions in A\n> indeed have no residual learning.\n\nThey actually proved in table 3 that these Zero padded shortcuts are better than nothing.\n\n> model           top-1 err.  top-5 err.\n> plain-34             28.54   10.02\n> ResNet-34   A   25.03   7.76\n> ResNet-34   B   24.52   7.46\n> ResNet-34   C   24.19   7.40\n', 'You can use a `Lambda` layer to wrap your `zero_pad` function. It will do\nwhat you want.\n\nOn 4 May 2016 at 09:49, LouisMartin notifications@github.com wrote:\n\n> Hi fchollet,\n> \n> I am actually trying to implement the smallest possible network based on\n> the SqueezeNet paper.\n> The problem is that these linear transformations add a lot of new\n> parameters to my network which I cannot afford. The number of parameters\n> goes from 800.000 to 1.100.000 which is too big an increase for the\n> accuracy improvement (which I already tested).\n> \n> My two options when dimensions mismatch are either:\n> - No shortcut connections (which already works well)\n> - Zero padded shortcuts with no added parameters\n>   I am pretty sure that the zero padded shortcuts are better than nothing so\n>   that is why I was asking.\n> \n> Quoting the ResNet paper:\n> \n> (A) zero-padding shortcuts are used\n> for increasing dimensions, and all shortcuts are parameterfree\n> (the same as Table 2 and Fig. 4 right); (B) projection\n> shortcuts are used for increasing dimensions, and other\n> shortcuts are identity; and (C) all shortcuts are projections.\n> Table 3 shows that all three options are considerably better\n> than the plain counterpart. B is slightly better than A. We\n> argue that this is because the zero-padded dimensions in A\n> indeed have no residual learning.\n> \n> They actually proved in table 3 that these Zero padded shortcuts are\n> better than nothing.\n> \n> model top-1 err. top-5 err.\n> plain-34 28.54 10.02\n> ResNet-34 A 25.03 7.76\n> ResNet-34 B 24.52 7.46\n> ResNet-34 C 24.19 7.40\n> \n> —\n> You are receiving this because you commented.\n> Reply to this email directly or view it on GitHub\n> https://github.com/fchollet/keras/issues/2608#issuecomment-216928580\n', ""I'm trying to do the same thing. This code runs in the recent nightly build of TensorFlow but not in Theano. \n\n```\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.layers.core import Lambda\nfrom keras import backend as K\n\n\ndef zeropad(x):\n    y = K.zeros_like(x)\n    return K.concatenate([x, y], axis=1)\n\n\ndef zeropad_output_shape(input_shape):\n    shape = list(input_shape)\n    assert len(shape) == 4\n    shape[1] *= 2\n    return tuple(shape)\n\n\ndef shortcut(input_layer, nb_filters, output_shape, zeros_upsample=True):\n    # TODO: Figure out why zeros_upsample doesn't work in Theano\n    if zeros_upsample:\n        x = MaxPooling2D(pool_size=(1,1),\n                             strides=(2,2),\n                             border_mode='same')(input_layer)\n        x = Lambda(zeropad, output_shape=zeropad_output_shape)(x)\n    else:\n        # Options B, C in ResNet paper... \n```\n"", '@roryhr i have read your code below, and i am wondering is this right?\r\nfirstly, function zeropad constructs a zero tensor with same shape to input x, and then concatenate both of them to get a new tensor with the channel number doubled( may be greater than need).\r\ni am wondering what will happen in Lambda. will it cut off the redundant zeros?\r\n\r\n\r\ndef zeropad(x):\r\n    y = K.zeros_like(x)\r\n    return K.concatenate([x, y], axis=1)\r\n\r\n\r\ndef zeropad_output_shape(input_shape):\r\n    shape = list(input_shape)\r\n    assert len(shape) == 4\r\n    shape[1] *= 2\r\n    return tuple(shape)', ""@yuimo Yes it doubles the size of dimension 1 (channels, I believe). Lambda just uses the function I defined -- so no cutting off of any zeros. I haven't tried running it lately though...I think I used TF and moved on."", ""If anyone wants to have zero-padding on depth and knows how many channels they want to get to, you can try:\r\n\r\n    def pad_depth(x, desired_channels):\r\n        y = K.zeros_like(x, name='pad_depth1')\r\n        new_channels = desired_channels - x.shape.as_list()[-1]\r\n        y = y[:,:,:new_channels]\r\n        return concatenate([x,y], name='pad_depth2')\r\n\r\nIf you have a layer that you're trying to add with, find out the number of channels in that layer by:\r\n\r\n     desired_channels = some_layer.shape.as_list()[-1]\r\n\r\nThen you can call the lambda function with:\r\n\r\n    earlier_layer_with_padding= Lambda(pad_depth, name='some_name', arguments={'desired_channels':desired_channels})(some_earlier_layer)\r\n\r\nAnd finally, add `earlier_layer_with_padding` to `some_layer` via:\r\n\r\n   new_layer = add([earlier_layer_with_padding, some_layer], name='some_add')\r\n\r\nmake sure you `from keras import backend as K`. only tested with tensorflow backend"", 'You can also trick Keras\'s ZeroPadding2D layer into padding your channels by passing the ""wrong"" argument in for data_format. It will think it is padding height or width when it is actually padding channels. ', 'Another possibility is to use a 1x1 Conv2D with fixed weights as follows\r\n\r\n        identity_weights = np.eye(n_channels_in, n_channels_out, dtype=np.float32)\r\n        layer = Conv2D(\r\n            n_channels_out, kernel_size=1, strides=strides, use_bias=False, \r\n            kernel_initializer=initializers.Constant(value=identity_weights))\r\n        # Not learned!\r\n        layer.trainable = False\r\n        x = layer(x)', 'If you are still looking for it in my GitHub repository I implemented it. Please take a look to https://github.com/nellopai/deepLearningModels and in case you have question feel free to write me', ""> If anyone wants to have zero-padding on depth and knows how many channels they want to get to, you can try:\r\n> \r\n> ```\r\n> def pad_depth(x, desired_channels):\r\n>     y = K.zeros_like(x, name='pad_depth1')\r\n>     new_channels = desired_channels - x.shape.as_list()[-1]\r\n>     y = y[:,:,:new_channels]\r\n>     return concatenate([x,y], name='pad_depth2')\r\n> ```\r\n> \r\n> If you have a layer that you're trying to add with, find out the number of channels in that layer by:\r\n> \r\n> ```\r\n>  desired_channels = some_layer.shape.as_list()[-1]\r\n> ```\r\n> \r\n> Then you can call the lambda function with:\r\n> \r\n> ```\r\n> earlier_layer_with_padding= Lambda(pad_depth, name='some_name', arguments={'desired_channels':desired_channels})(some_earlier_layer)\r\n> ```\r\n> \r\n> And finally, add `earlier_layer_with_padding` to `some_layer` via:\r\n> \r\n> new_layer = add([earlier_layer_with_padding, some_layer], name='some_add')\r\n> \r\n> make sure you `from keras import backend as K`. only tested with tensorflow backend\r\n\r\nSome minor change if you want to work with batch (which i think most people are working with):\r\n```\r\ndef pad_depth(x, desired_channels):\r\n    y = K.zeros_like(x)\r\n    new_channels = desired_channels - x.shape.as_list()[-1]\r\n    y = y[..., :new_channels]\r\n    return concatenate([x, y], axis=-1)\r\n```""]","['\ndef zero_pad(x, shape):\n    return K.concatenate([x, K.zeros(shape)], axis=1)\n']",[],0,0
161,keras,8446,closed,Can not save model using model.save following multi_gpu_model,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

This is a short and simple issue. Following upgrading to Keras 2.0.9 I have been using the multi_gpu_model utility but I can seem to save my models or best weights using model.save.

The error I get is



I suspect there is some problem gaining access to the model object. Is there a work around this issue?",,"['Are you trying to save directly the model which is parallelized? That can be the problem. Could you provide a snippet that shows the steps you are following?', "" Yes - thats exactly what I am doing - I'm building the model normally, then calling the multi gpu model method function on it and after training, trying to save it with model.save. Does this need to be converted back to a non-parallelized version of the model? If so, how is this achieved? "", 'For now we recommend saving the original (template) model instead of the parallel model. I.e. call `save` on the model you passed to `multi_gpu_model`, not the model returned by it.\r\n\r\nBoth models share the same weights.', ""That's fantastic, and straightforward advice. \n\nBest\nSimon\n\n\nSLFWalsh MD MRCP FFRRCSI\nConsultant Radiologist\nKings College Hospital Foundation Trust\n\n> On 10 Nov 2017, at 19:09, François Chollet <notifications@github.com> wrote:\n> \n> For now we recommend saving the original (template) model instead of the parallel model. I.e. call save on the model you passed to multi_gpu_model, not the model returned by it.\n> \n> Both models share the same weights.\n> \n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n"", 'Just to clarify - do you mean call model.save or model.save_weights on the template model at the end of training?', 'Yes. Either method should work fine.', 'But how can I save the optimizer state by just saving the template model?', 'I am on keras 2.1.2 and encountered same problem. I tried follow [this answer](https://stackoverflow.com/questions/47210811/can-not-save-model-using-model-save-following-multi-gpu-model-in-keras#answer-48066771) on StackOverflow, and it works for me. Hope it helps.', 'Closing as this is resolved', '> Closing as this is resolved\r\n\r\nHas this been resolved by a commit and can models and weights be saved as expected when using multi_gpu_model?', 'So, as mentioned above, i should train with parallel_model but save the origin model. But what if i want save weights on every epoch as checkpoints using a callback, what should i do?', 'Are there any updates regarding this either on keras or on tf.keras?']",[],['TypeError: can’t pickle module objects '],0,0
162,keras,9496,closed,Outdated VAE example,"In the [VAE](https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder.py#L58) example, the model is compiled like this: 
In the latest master branch, the compile method is required to have the arguments optimizer and loss passed.",,"[""The loss argument is not required in `master`. It hasn't been required for some time.""]",[],"[""vae.compile(optimizer='rmsprop')""]",0,0
163,keras,5615,closed,why input layer requires to be an instance of a class layer?,"
error:

TypeError: The added layer must be an instance of class Layer. Found: Tensor(""input_1:0"", shape=(?, 784), |dtype=float32)
",,"[""Hello,\r\n\r\nInput doesn't return a layer\r\nhttps://github.com/fchollet/keras/blob/master/keras/engine/topology.py#L1142\r\n\r\nIt is a wrapper of InputLayer.\r\n\r\nAlso, if you are using Sequential, you may also skip the Input and just specify the input_shape for the first Layer. \r\n"", '@unrealwill thanks. So just to be clear in the functional api you can use it without the any of the above issues? ', ""> Hello,\r\n> \r\n> Input doesn't return a layer\r\n> https://github.com/fchollet/keras/blob/master/keras/engine/topology.py#L1142\r\n> \r\n> It is a wrapper of InputLayer.\r\n> \r\n> Also, if you are using Sequential, you may also skip the Input and just specify the input_shape for the first Layer.\r\n\r\nCould you give an example of how to just specify the input shape? "", ""model = keras.Sequential()\r\n\r\nmodel.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=(IMG_ROWS,IMG_COLS,1)))\r\nmodel.add(MaxPooling2D(2,2))\r\nmodel.add(Dropout(0.25))\r\n\r\n\r\nmodel.compile(loss=keras.losses.categorical_crossentropy, optimizer = 'adam', metrics = ['accuracy'])\r\n\r\nmodel.summary()\r\n\r\nIn this way you can enter your input shape.""]","[""\r\nreproducible error:\r\n\r\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\r\nmodel = Sequential([\r\n    Input(shape=(784,)),\r\n    Dense(200, activation='relu'),\r\n    Dropout(0.5),\r\n    Dense(200, activation='relu'),\r\n    Dropout(0.5),\r\n    Dense(100, activation='relu'),\r\n    Dropout(0.5),\r\n    Dense(nb_classes, activation='softmax')\r\n    ])\r\n""]",[],0,0
164,keras,13341,closed,NameError: name 'math_ops' is not defined,"**System information**  
- Have I written custom code (as opposed to using example directory):  
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  
- TensorFlow backend (yes / no):  
- TensorFlow version: 1.14.0 
- Keras version:  2.3.0
- Python version:  
- CUDA/cuDNN version:  
- GPU model and memory:  


**Describe the current behavior**  

  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training.py"", line 222, in compile
    masks=masks)
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training.py"", line 871, in _handle_metrics
    self._per_output_metrics[i], target, output, output_mask)
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training.py"", line 842, in _handle_per_output_metrics
    metric_fn, y_true, y_pred, weights=weights, mask=mask)
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py"", line 1022, in call_metric_function
    mask = math_ops.cast(mask, y_pred.dtype)
NameError: name 'math_ops' is not defined

**Describe the expected behavior**  

**Code to reproduce the issue**  

**Other info / logs**  
",,[],[],[],0,0
165,keras,2820,closed,   self.pool_size = tuple(pool_size)  TypeError: 'int' object is not iterable,,,[],[],[],0,0
166,keras,4015,closed, keras callbacks does not seem to work by import," imported by 
when I use callbacks in 



it's got who know why is it?
I will be appreciate.
",stale,"['@dorbodwolf Hi, there is no `Plotter` in `keras`, I don\'t find it, you can `import Plotter` from you dir, and create  \n`plotter = Plotter(show_plot_window=False, save_to_filepath=""/tmp/last_plot.png"")`, and then \n\n```\nmodel.fit(data, label, batch_size=32, nb_epoch=100, shuffle=True, validation_split=0.2, callbacks=[plotter])\n```\n\nTry it~~\n', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs, but feel free to re-open it if needed.\n']","['\nmodel.fit(data, label, batch_size=32, nb_epoch=100, shuffle=True, validation_split=0.2, callbacks=[Plotter(show_plot_window=False, save_to_filepath=""/tmp/last_plot.png"")])\n\n']","['from keras.callbacks import Plotter', '.fit()', 'ImportError\n']",0,0
167,keras,5646,closed,Image Segmentation,"I am working on Brain tumor Segmentation MRI images using CNN. but i don't know how to implement segmentation in cnn ?

Please help me with this.

Thanks
KJ",stale,"[' If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.\r\n\r\nYou just have to make one prediction per pixel. So the output of your model should be an image with nb_labels channel.', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],0,0
168,keras,11460,closed,stop the backpropagation,"
![img_20181023_154623](https://user-images.githubusercontent.com/25239334/47344598-75f56b00-d6db-11e8-86a5-a81732ee33d0.jpg)
Hi, i have a network like this. And  i want to stop the backpropagation from CNN  to Inception. The Inception just get the gradients from LSTM. What should i do ? Thank you for your help.",type:support,['```python\r\ninput = Input(...)\r\ninception = ....\r\ncnn = ...\r\nlstm = ...\r\ninception_out = inception(input)\r\ncnn_input = Lambda(K.stop_gradient)(inception_out)\r\ncnn_out = cnn(cnn_input)\r\nlstm_out = lstm(inception_out)\r\n```'],[],[],0,0
169,keras,6651,closed,Keras Image Generator MemoryError,"Hi everyone,
I have a reasonably big dataset of images, and I'm trying to train the Inception Model on it.
The problem is, when I try to use the Image Generator (To apply random transformations), I get a MemoryError.

This is weird, because, when I fit my model without the generator (So without transformations) I have no problem.
Possibly, the generator has some memory leaks ?
(I train the model with train_on_batch method and yield batch_size from the generator)

If there are no solutions, is there a way to apply transformations to images without the generator ?
(By each images possibly ?)
Thanks!",stale,"[""I had today a problem, too. I wrote a Unit Test and after the Unit Test I've got a warning about leaking files. It turned out the problem was the PIL library.\r\nPlease, try following: Replace this [line](https://github.com/fchollet/keras/blob/master/keras/preprocessing/image.py#L320) with\r\n```python\r\nfrom skimage import io\r\nimg = io.imread(path)\r\n```\r\nand report if you still have problems."", ""Thanks for your answer!\r\nI resolved my problem using the random_tranform method of the generator to do it image by image.\r\n\r\nI'll try your solution for the next train! Big thanks!"", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],0,0
170,keras,9198,closed,Freeze layers with the multi_gpu_model in Keras,"I'm trying to fine-tune a modified InceptionV3 model in Keras.

I follow the example ""Fine-tune InceptionV3 on a new set of classes"" on [this page][1].

So I first trained the top dense layers that were added to the InceptionV3 base model with the following code:

    model = Model(inputs=base_model.input, outputs=predictions)

    for layer in base_model.layers:
        layer.trainable = False

    parallel_model = multi_gpu_model(model, gpus=2)
    
    parallel_model.compile(optimizer='rmsprop', loss='categorical_crossentropy')
    
    history = parallel_model.fit_generator(generate_batches(path), steps_per_epoch = num_images/batch_size, epochs = num_epochs)

After that, I try to fine-tune the top 2 inception blocks from InceptionV3. And according to the example, what I should do is:

    for layer in model.layers[:249]:
       layer.trainable = False
    for layer in model.layers[249:]:
       layer.trainable = True
    
    model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')

    model.fit_generator(...)

But I'm using the , so I don't know how to freeze the first 249 layers. 

I mean, if I freeze the layers in the no-gpu model (like the example), and use  to freeze the layers in the , then the weights in the top dense layers that were just trained and contained in the  will be overwritten, right?

On the other hand, I tried to directly use , but when I checked the layers in the , it showed:

    for i, layer in enumerate(parallel_model.layers):
       print(i, layer.name)

    (0, 'input_1')
    (1, 'lambda_1')
    (2, 'lambda_2')
    (3, 'model_1')
    (4, 'dense_3')

So what are the 'lambda_1', 'lambda_2' and 'model_1' layers and why it only shows 5 layers in the ?

More importantly, how to freeze the layers in the ?

  [1]: https://keras.io/applications/",,"[""I think you actually did freeze your `parallel_model` (PM) through your single-gpu model (SM). \r\n\r\nSo if you do anything w.r.t. this SM, it will be reflected in the corresponding PM. This means if you freeze `K` parameters in SM, you will also see `K` non-trainable parameters in PM.  \r\n\r\nMore specific, if you do something below\r\n```\r\nsingle_model = Model(inputs=base_model.input, outputs=predictions)\r\n\r\n# freeze/unfreeze weights in SM\r\nfor layer in single_model.layers:\r\n    layer.trainable = False\r\n\r\nparallel_model = multi_gpu_model(single_model, gpus=2)\r\nparallel_model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\r\n\r\nprint single_model.summary()\r\nprint parallel_model.summary()\r\n```\r\nyou will see both print out messages show the same number of non-trainable parameters.\r\n\r\nOnce again, you should only freeze `a layer` when it is a `keras layer` (just as SM in this case, it is actually `model`)\r\n"", 'This is correct. You can set `trainable` on layers from the original `model`, and that will be taken into account in `parallel_model`. \r\n\r\nHowever, you should always remember to `compile` `parallel_model` after changing any trainable value. Compiling is required for any changes to be taken into account.', ""Thanks, but let me make sure I understand correctly.\r\n\r\nLet's say, I first freeze some layers in the original `model`, then call `parallel_model = multi_gpu_model(model, gpus=2)`, `parallel_model.compile(...)` and `parallel_model.fit(...)`. I understand this because the statement `parallel_model = multi_gpu_model(model, gpus=2)` will copy the `trainable` setting from the original `model` to the `parallel_model`.\r\n\r\nBut after that, I want to freeze other layers in the `parallel_model`. So this time, I only need to set trainable on layers from the original `model` and compile the `parallel_model`, and I do NOT need to call `parallel_model = multi_gpu_model(model, gpus=2)` to freeze the layers in the `parallel_model`, is that correct?\r\n\r\nIf so, how is the `trainable` setting transferred from the original `model` to the `parallel_model` in this case?"", 'Yes, that\'s correct. As for ""transfer"", you must understand that they share\nthe same layers, so there is nothing to transfer.\n\nOn 26 January 2018 at 14:25, chao huang <notifications@github.com> wrote:\n\n> Thanks, but let me make sure I understand correctly.\n>\n> Let\'s say, I first freeze some layers in the original model, then call parallel_model\n> = multi_gpu_model(model, gpus=2), parallel_model.compile(...) and\n> parallel_model.fit(...). I understand this because the statement parallel_model\n> = multi_gpu_model(model, gpus=2) will copy the trainable setting\n> transferred from the original model to the parallel_model.\n>\n> But after that, I want to freeze other layers in the parallel_model. So\n> this time, I only need to set trainable on layers from the original model\n> and compile the parallel_model, and I do NOT need to call parallel_model\n> = multi_gpu_model(model, gpus=2) to freeze the layers in the\n> parallel_model, is that correct?\n>\n> If so, how is the trainable setting transferred from the original model\n> to the parallel_model in this case?\n>\n> —\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/keras-team/keras/issues/9198#issuecomment-360921418>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AArWbyzqPPqeCKpiTz0sPh1TvstGqY_pks5tOlDTgaJpZM4Ruaep>\n> .\n>\n']",[],"['multi_gpu_model', 'parallel_model = multi_gpu_model(model, gpus=2)', 'parallel_model', 'parallel_model', 'for layer in parallel_model.layers[:249]: layer.trainable = False', 'parallel_model', 'parallel_model', 'parallel_model']",0,0
171,keras,12782,closed,issue while using LSTM in functional mode,"
**System information**  
- Have I written custom code (as opposed to using example directory):  yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10 Pro
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  1.13.1
- Keras version:  2.2.4
- Python version: 3.7.3
- CUDA/cuDNN version:  NA
- GPU model and memory:  NA

**Describe the current behavior**  
When creating a LSTM layer using the functional moded below error is thrown


**Describe the expected behavior**  
LSTM layer is created successfully

**Code to reproduce the issue**  


**Error Log**  
  

**The below version of the code which uses the Sequential model runs successfully**



**Success log**

",stat:awaiting response type:support,"['I was able to execute both of the code snippets successfully in TF 1.13.1 using google colab. From the stack trace I suspect that the reported error arises from different line of code.', 'Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!']","[""\r\nimport keras\r\nfrom keras.layers import Input, LSTM, Dense\r\nfrom keras.models import Model\r\n\r\nimport numpy as np\r\n\r\n\r\ndef build_lstm(lstm_input, hidden_dim=256):\r\n    out = LSTM(hidden_dim, input_shape=(1, 129))(lstm_input)\r\n    return out\r\n\r\n\r\ndef test_lstm():\r\n    lstm_input = Input(shape=(129, ), name='lstm_input')\r\n    out = build_lstm(lstm_input)\r\n\r\n    predictions = Dense(256, activation='softmax')(out)\r\n    model = Model(inputs=lstm_input, outputs=predictions)\r\n    model.compile(loss='categorical_crossentropy',\r\n        optimizer='rmsprop',\r\n        metrics=['accuracy'])\r\n\r\n    model.summary()\r\n\r\n    return  \r\n"", '\r\n(NPI_Arch_Keras) C:\\workspaces\\NPL\\project\\NPI_Arch_Keras\\npi>python npi_lstm_test.py\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File ""npi_lstm_test.py"", line 35, in <module>\r\n    test_lstm()\r\n  File ""npi_lstm_test.py"", line 21, in test_lstm\r\n    out = build_lstm(lstm_input)\r\n  File ""npi_lstm_test.py"", line 15, in build_lstm\r\n    out = LSTM(hidden_dim, input_shape=(1, 129))(lstm_input)\r\n  File ""C:\\MyProgramFiles\\Anaconda3\\envs\\NPI_Arch_Keras\\lib\\site-packages\\keras\\layers\\recurrent.py"", line 532, in __call__\r\n    return super(RNN, self).__call__(inputs, **kwargs)\r\n  File ""C:\\MyProgramFiles\\Anaconda3\\envs\\NPI_Arch_Keras\\lib\\site-packages\\keras\\engine\\base_layer.py"", line 414, in __call__\r\n    self.assert_input_compatibility(inputs)\r\n  File ""C:\\MyProgramFiles\\Anaconda3\\envs\\NPI_Arch_Keras\\lib\\site-packages\\keras\\engine\\base_layer.py"", line 311, in assert_input_compatibility\r\n    str(K.ndim(x)))\r\nValueError: Input 0 is incompatible with layer lstm_1: expected ndim=3, found ndim=2\r\n', ""\r\nimport keras\r\nfrom keras.models import Sequential\r\nfrom keras.layers import LSTM, Dense\r\n\r\nimport numpy as np\r\n\r\n\r\ndef build_lstm(hidden_dim=256, input_shape=None):\r\n    model = Sequential()\r\n    model.add(LSTM(hidden_dim, return_sequences=True, input_shape=input_shape))\r\n    model.add(LSTM(hidden_dim))\r\n    return model\r\n\r\n\r\ndef test_lstm():\r\n    model = Sequential()\r\n    model.add(build_lstm(input_shape=(1, 129)))\r\n    model.add(Dense(256, activation='softmax'))\r\n\r\n    model.compile(loss='categorical_crossentropy',\r\n              optimizer='rmsprop',\r\n              metrics=['accuracy'])\r\n\r\n    model.summary()\r\n\r\n    return\r\n"", '\r\n(NPI_Arch_Keras) C:\\workspaces\\NPL\\project\\NPI_Arch_Keras\\npi>python npi_lstm_working.py\r\nUsing TensorFlow backend.\r\nWARNING:tensorflow:From C:\\MyProgramFiles\\Anaconda3\\envs\\NPI_Arch_Keras\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\nsequential_2 (Sequential)    (None, 256)               920576\r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 256)               65792\r\n=================================================================\r\nTotal params: 986,368\r\nTrainable params: 986,368\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n']","['ValueError: Input 0 is incompatible with layer lstm_1: expected ndim=3, found ndim=2']",0,0
172,keras,916,closed,test data used for validation,"https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py#L52

Might want to update your example ;-) 
",,"[""It's fine because the `validation_data` in this example is used strictly for real-time monitoring. It doesn't end up affecting the learned model in any way.\n"", 'Only true if the example is used to run once, and never touched again. If I were running this code doing ""real-time monitoring"" of the logs and then decide to change any of the hyperparams (the script even notes there\'s room for tuning), then that would be violating the sanctity of the test data. Overfitting to test data can happen in subtle ways.\n', ""@fchollet Is monitoring the validation loss to reduce the learning rate when it reaches a plateau not considered tweaking the parameters ? So it that case aren't we peeking into the test data (validation data is the case of keras) which would eventually lead to overfitting?""]",[],[],0,0
173,keras,8897,closed, 'ModuleNotFoundError : no module named tensorflow',"Hi
I have successfully installed theano, tensorflow and keras but I always get this error each time I try to import keras.

My OS is windows 10
I am now using python 3.5 (was 3.63 but had to reinstall anaconda with python 3.5 when troubleshooting)

Kindly help

Thanks
",,"['Hi! Does the error appear when typing only ""import tensorflow"" in the interpreter? If it does, then it\'s not really related to keras and you should try to reinstall Tensorflow from scratch (and open an issue in the Tensorflow repo if really necessary). Please tell us the result of just doing ""import tensorflow"", it should help. Thank you. ', ""Hi\r\nI have been able to run import keras, import tensorflow and import theano successfully.\r\nHowever, when I try to run 'from keras.model import Sequential' it threw this error:\r\n\r\nImportError: No module named 'keras.model'\r\n\r\nSame issues with 'from keras.layers import Dense'\r\n\r\nThanks once again\r\n"", 'For your first issue, this is because it\'s keras.models instead of\nkeras.model.\nFor the second one, it\'s a bit strange I must admit. Can you type ""from\nkeras.layers import Dense"" in the interpreter and give me the full\nstacktrace?\n\nOn Thu, 28 Dec 2017, 22:27 T-DS, <notifications@github.com> wrote:\n\n> Hi\n> I have been able to run import keras, import tensorflow and import theano\n> successfully.\n> However, when I try to run \'from keras.model import Sequential\' it threw\n> this error:\n>\n> ImportError: No module named \'keras.model\'\n>\n> Same issues with \'from keras.layers import Dense\'\n>\n> Thanks once again\n>\n> —\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/keras-team/keras/issues/8897#issuecomment-354288114>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AMS2K9b7XuWt5IHMcUXSDM7s2fuibmBPks5tE5cogaJpZM4RNugh>\n> .\n>\n', ""Hi\r\nThanks a lot for your response. I can confirm that the issue is now resolved. Used from from keras.models import Sequential' and it worked fine.\r\nAlso 'from keras.layers import Dense' worked fine.\r\n\r\nMany thanks"", 'Happy to help! Can you close this issue? It helps the other issues readers know what to prioritise. Thank you! ', 'I have same time with you . Can you tell me how resolved this issue. ', 'Thanks, I find my trouble , my project name is ""keras"".']",[],[],0,0
174,keras,9235,closed,Can't apply Inception on input tensor,"This code works in keras 2.1.2 but not on keras 2.1.3:



",,"['Which version of tensorflow are you using? We have noticed a problem with version 1.3 with keras 2.1.3. If you are using version 1.3, can you upgrade the tensorflow version and try running the model again?\r\n\r\nRelated issue:#9165', '@anj-s Indeed, I had this problem with TF version 1.3. With version 1.4 it works.', 'Closing this issue since we have a workaround. Thanks.']","['\r\nfrom keras.applications.inception_v3 import InceptionV3\r\nfrom keras.layers import Input\r\n\r\ninp_shape = (299, 299, 1)\r\ncnn = InceptionV3(input_shape=inp_shape, weights=None, classes=10)\r\ninp = Input(shape=inp_shape)\r\nout = cnn(inp)\r\n', '\r\nValueError: Variable batch_normalization_1/moving_mean/biased already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\r\n\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py"", line 1001, in moving_average_update\r\n    x, value, momentum, zero_debias=True)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/normalization.py"", line 185, in call\r\n    self.momentum),\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 617, in __call__\r\n    output = self.call(inputs, **kwargs)\r\n\r\n']",[],0,0
175,keras,3120,closed,Is it possible to set CPU / GPU usage without knowing the backend?,"In other words, I don't know if the Keras install will use Theano or TensorFlow, but I'd still like to have control over whether the CPU or GPU is being used. Is this possible? Could we add a flag to  for this purpose otherwise?
",stale,"['You could automatically check `~/.keras/keras.json` at the beginning of your script:\n- if the backend is Theano then with `os.environ` you can set `THEANO_FLAGS` to `device=gpu` (and/or  `openmp=True` for multi-CPU usage) and then `import theano`.\n- if the backend is TensorFlow then it will automatically use the GPU. If needed you could then try to prevent tf to use the GPU with something like [this](https://github.com/tensorflow/tensorflow/issues/754).\n', 'Yeah, sure, but it feels like that should be a function provided by Keras, especially as more backends are eventually added. User code should be backend agnostic.\n', 'I always get gpu errors like ""c:\\tf_jenkins\\home"" etc. and can\'t simply switch to cpu (using tensorflow).\r\nThere is only a CPU version of Tensorflow available for my environment (Win7/64)!\r\nI have NVIDIA GPU Computing installed, but it\'s not used (double trouble).', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],['~/.keras/keras.json'],0,0
176,keras,3457,closed,Loading saved model with custom loss,"when I load a model that was saved in hdf5 that was using a custom loss function called  I get the following exception

Even if I have defined the function in the current script it still raises this exeption.

Is this the normal behaviour? Do I have to install keras defining my custom loss in the source code?
",,"[""Sorry guys, just noticed there's a lot of already open issues on the matter.\n""]",[],"['dice_coef_loss', 'Exception: Invalid objective: dice_coef_loss']",0,0
177,keras,2712,closed,is model.fit() accumulative?,"Hi, I am new to keras and neural network, and I am wondering if model.fit() is accumulative?

So basically, I accidentally killed the program yesterday, but however I do save weights on every epoch. So I was wondering if I could load the last weight into the model and then call model.fit()?
",,"['Yes, you can.\nOn May 12, 2016 6:20 PM, ""flyingpoops"" notifications@github.com wrote:\n\n> Hi, I am new to keras and neural network, and I am wondering if\n> model.fit() is accumulative?\n> \n> So basically, I accidentally killed the program yesterday, but however I\n> do save weights on every epoch. So I was wondering if I could load the last\n> weight into the model and then call model.fit()?\n> \n> —\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly or view it on GitHub\n> https://github.com/fchollet/keras/issues/2712\n', 'OK, thanks. Just to want to validate one other thing. When fit() in keras/models.py is called, it returned with the fit() in keras/engine/training.py. Am I right?\n', '@fchollet \n', '@flyingpoops Yes, the `fit()` implementations in `keras/model.py` do some setting up and then call `fit()` in `keras/engine/training.py`.\n', 'I think now I have a better understanding of keras, thanks @jfsantos \n']",[],[],0,0
178,keras,3323,closed,"What does ""nb"" mean in Keras?","I see a lot of variables are named like ""nb_xx"", such as ""nb_classes"", ""nb_samples"", etc. So what does ""nb"" means here?
",,"['Number of?\n', 'Number. \n', 'Thanks! What a stupid question!\n']",[],[],0,0
179,keras,11358,closed,Error in load model,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",,"[""Closing as it's not possible to understand the bug or how to reproduce. Please open a new issue and add a gist to reproduce the problem. ""]",[],[],0,0
180,keras,8373,closed,Meeting some trouble when running examples/mnist_acgan.py(ACGAN model),"I ran this code several times on different PCs recent days, but the code was not able to correctly generate samples. All the samples that generated are all pure black, that, only a black image.

I ran the code for 50 epoches, on keras 2(recent updated) , both theano and tensorflow backend were tried. 

And I noticed that in the code, the set ""discriminator.trainable = False"" in done in line 160, but seems never set to be True in the ""for"" loop of training. So is the discriminator is initialized but never trained during the process?

![window](https://user-images.githubusercontent.com/15433614/32375122-5d0e4396-c098-11e7-97f2-48bb5e69d1d8.png)
",,[],[],[],0,0
181,keras,5360,closed,How to implement LSTM/GRU language model through Keras?,"
I want to implement LSTM/GRU language model by Keras. But it seems one challenge.
How to generate softmax over all the vocabularies? I.e. I cannot access weight in Embedding layer in either my own implemented loss function as well as function in Lambda layer.",stale,"[""I think I didn't understand this completely. You do know the number of words in your vocab beforehand right?\r\nSo why not define a dense layer like\r\n```python\r\nDense(num_vocab)(some_layer)\r\n```\r\nThis will generate softmax over all the words in the vocab. You can also define your loss function over this output. Please do correct me if I am wrong.\r\n\r\nCheers."", '@siddheshk \r\nActually, I want to say is I want to generate Softmax over embedding of vocabulary.\r\nSay, Input layer only can input the target word index target_i, and through the Embedding layer, I am able to get the embedding of the target_i, embed(target_i).\r\nBut I want to take the Softmax over all the embedding of words, say embed(target_i) / sum (embed(all words)). \r\nIn order to maximize the softmax, I need my own implemented loss function to access all the embedding of vocabulary (denominator of softmax). Is it feasible?\r\n\r\n', 'Could you learn the embeddings as a separate model (as a feature transformer) that becomes the input to the LSTM/GRU model, or do you need to train the embedding weights as well as part of the Language Model?', '@amy12xx \r\nI want to train the embedding through the LSTM/GRU. So the input of LSTM/GRU will be the embedding of a word and tune the embedding through the LSTM/GRU.']",[],[],0,0
182,keras,2145,closed,Include clipnorm/clipvalue in Optimizer.get_config output,"The Optimizer's class get_config method returns a dictionary with single key ""name"". However, Optimizer can also receive ""clipnorm"" and ""clipvalue"" keyword arguments. It would be nice to also add them to the dictionary returned by get_config if they were set.
- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I'd be happy to implement this feature.
",stale,[],[],[],0,0
183,keras,6156,closed,"I want to use seq2seq model for Q&A,but I don't know the output of the LSTM(decoder),is it a array of probability??","Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],0,0
184,keras,7089,closed,Autocancelation,"@fchollet can you enable autocancelation on Travis? Haven't tested it out myself but it should cancel redundant builds (for example, if you push twice to a pr). I don't think it is enabled now but I could be wrong.

https://blog.travis-ci.com/2017-03-22-introducing-auto-cancellation

Travis seems to be doing a lot of work recently, especially with the new backend. Could help to cut back a little bit.

Cheers",,"['Done, thanks for the heads up.']",[],[],0,0
185,keras,100,closed,Roadmap,"Here are the key features that we will have in Keras by the time it hits v1.0. 
- Visualization tools, possibly built on top of Bokeh. You should be able to see everything that's going on in your experiment, and maybe even manage your experiments from a GUI. Total visualization is key to doing good research.
- Easy support for Spearmint for hyperparameter search.
- Complete unit tests. The recent regularizers/constraints debacle that left Kaggle users confused highlights once more that reliable unit tests should be an absolute priority. We want to be able to develop quality code safely and with confidence. 
- Better convnet features, including FFT convolutions, and maybe new padding options. In the medium term we should look into incorporating support for Nervana Systems' fast convolution kernels. Let's stay state of the art ;-)
- Support for non-sequential models, by the way of:
  - [DONE] a Merge container that takes a list of Sequential models and turns them into a single output
  - a Fork container that replicates the output of a Sequential model to a list of Sequential models
    (both would be trainable end-to-end, of course).

If you see anything you would like to work on, please post here with your thoughts on how you would incorporate it into the current architecture / data structures. This is our opportunity to make a big contribution to the deep learning ecosystem! : )
",Enhancement stat:contributions welcome,"[""Sounds Great! \n\nYou have provided a high level direction. \nCouple of (minor) things I would add to the list:\n1. Interfacing with Caffe Models - loading, saving weights. \n2. Automatic Tensor Size Calculation. (even in the densely connected layer) \n3. Layer wise Learning Rates for fine tuning. \n4. Having a Model Zoo with various models trained on a wide number of Datasets would be great. \n\nMy thoughts on your ideas:\n1. GUI is a must as you have stated. I'm currently running a basic graphing and filter visualization code with keras + matplotlib , but thats very basic and a more robust approach is needed. Nvidia's DIGITS Library would give some good inspiration. It's runs on a local server,  Bokeh would give us that,  but its dependencies are enormous. But,  it's a really versatile library that can support all ranges of work done on keras. \n2. Adding native t-SNE somewhere in the loop would be a great addition. \n3. Building a better abstracted Layer Class would help both now and the future demands. Recently there was minor issue with having Dropout Layer as the 1st layer. If there is a chance to improve this,  it would be now. Once the core Layer Class is fixed and shipped, changing that will break a lot of things and create a lot of hassle.  So I would suggest a through debate on the core layer class,  so any future layers -  be it fork,  merge or whatever, that can be built.  \n"", ""> Interfacing with Caffe Models - loading, saving weights.\n\nDefinitely something that would be great to have. If you have experience with Caffe, is this something you would be interested in working on?\n\n> Automatic Tensor Size Calculation. (even in the densely connected layer)\n\nI've been thinking about something like this. It would dramatically change the API, but since it would make it simpler, it would be worth it.\n\n> Layer wise Learning Rates for fine tuning.\n\nI don't think we'll go in that direction, because it would clash too much with the modularity of the architecture. Optimizers, of which learning rates are one part, are supposed to be independent from the model you apply them to. Introducing layer parameters that would only be used by SGD... doesn't seem right. Also adaptative optimizers like Adam, Adagrad, Adadelta, do use layer-wise (even parameter-wise) learning rates, in a way.\n\n> Having a Model Zoo with various models trained on a wide number of Datasets would be great.\n\nAlso something that would be great to have : ) Having a set of automatically downloadable datasets (currently: IMDB, MNIST, Reuters, CIFAR10) is one step in that direction. Next we should definitely have a library of pre-trained models (code to build them + HDF5 weights) that perform well on these datasets.\n"", ""I'd like to mention adding hessian free optimizers. Keras looks awesome, but as someone who has read a bit about the state of the art in neural networks but has fairly little practical experience, the lack of a hessian-free optimizer put me off. I would definitely consider submitting a pull request.\n"", ""> Keras looks awesome, but as someone who has read a bit about the state of the art in neural networks but has fairly little practical experience, the lack of a hessian-free optimizer put me off.\n\nI'm quite sure hessian-free optimization is being used non-ironically by anyone in the NN community today. Turns out SGD with momentum and decay fits pretty much every use case, as long as the weights are properly initialized. HF is very 2010.\n\nBut if you want to add an HessianFree optimizer (there are several Theano implementations floating around GitHub), then go ahead : ) \n"", ""I can take up the Caffe Interfacing, but I can't guarantee any deadlines as I have a busy schedule. But, I think I can have a basic version within 3 weeks.\n"", ""Awesome! : ) Don't worry about deadlines, code quality is more important.\n"", ""> Better convnet features, including FFT convolutions, and maybe new padding options. In the medium term we should look into incorporating support for Nervana Systems' fast convolution kernels. Let's stay state of the art ;-)\n\nRegarding Nervana Kernels, I started an issue with the Theano group and here's their reply:\nhttps://github.com/Theano/Theano/issues/2919\n\nWhich refers this issue, with instructions to get Nervana Kernels to work with Theano:\nhttps://github.com/Theano/Theano/issues/2941\n\nSeeing that only Maxas class GPUs are supported, I think having another Conv/FFT backend would be better.\n"", 'Regarding the GUI I would recommend Tkinter which is part of the python standard library and allows matplotlib figures to be integrated into it.\nIt cannot be more minimalistic from external dependencies viewpoint. :)\n\nWhat kind of visualizations do you have in mind? I am thinking, especially for CNNs, maps of the learned filters and occlusion heatmaps like here http://cs231n.github.io/understanding-cnn/ which are good for understanding what image regions contribute how much for the (classification) task.\n\nGenerally I think it would be useful to be able to plot the training/validation loss over the epochs. It would be neat to be able to monitor these curves as the training runs. I am not 100% sure how to do it but using observable streams from https://github.com/ReactiveX/RxPY for it would definitely be an elegant solution.\n', ""Hey, thanks for the suggestions. Plotting basic quantities (loss, accuracy) over training will be the very first thing we'll add. Being able to visualize the learned weights, and being able to plot clusters of features or/and inputs would be neat as well. \n\nI was thinking of building the UI in JS/HTML5, and make it a completely separate app (that could potentially be re-used with a different library than Keras). This seems to offer maximum flexibility and would requires no unusual dependencies either.\n"", 'If you do not need it right away I can take up the task of basic plotting scripts. I would write them such that they could be used after fitting a model, using the output from the fit method.\n\nWhat do you think?\n', 'Sure, what do you have in mind exactly? \n', 'Just a simple util class which takes the output of the fit method as input. The class would then provide a method for plotting the loss/acc over the epochs and optionally saving the figure to a file. This would make matplotlib a dependency of the project though.\n\nI will create a pull request when i have the code done. :)\n', 'The advantage of a web-based UI is the ability to easily share results with\nothers. Tools such as Ipython notebook show the power of this approach. I\nalso support the modular approach of a separate library. If Keras is\ncapable of providing the appropriate callbacks and data structures to\nsupport plotting requests, that sounds appropriate. That kind of modularity\nwould (hopefully) allow easy integration into other kinds of GUI (e.g.\nstandalone) for people with that use case.\n\nOn 29 May 2015 at 03:46, François Chollet notifications@github.com wrote:\n\n> Hey, thanks for the suggestions. Plotting basic quantities (loss,\n> accuracy) over training will be the very first thing we\'ll add. Being able\n> to visualize the learned weights, and being able to plot clusters of\n> features or/and inputs would be neat as well.\n> \n> I was thinking of building the UI in JS/HTML5, and make it a completely\n> separate app (that could potentially be re-used with a different library\n> than Keras). This seems to offer maximum flexibility and would requires no\n> unusual dependencies either.\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/fchollet/keras/issues/100#issuecomment-106521999.\n\n## \n\n---\n\nTennessee Leeuwenburg\nhttp://myownhat.blogspot.com/\n""Don\'t believe everything you think""\n', 'Also, why was Bokeh chosen specifically? Seaborn is another strong alternative which I think would be worth thinking about.\n', ""> Also, why was Bokeh chosen specifically? Seaborn is another strong alternative which I think would be worth thinking about.\n\nI was thinking about Bokeh because it allows back-and-forth communication with Python, but really right now it's just one option among others. Thanks for suggesting Seaborn too!\n"", ""I'm also building visualization tools. Maybe you guys want to check it out. Here is an example to visualize cost function and weights being adapted with Bokeh: \nhttps://github.com/EderSantana/agnez/blob/master/examples/bokeh_plotting.py\n\nIt was using the old API (I'll be updating all my code to the new API soon). But it may work as a starting point.\n\nFor an MLP training on the MNIST, I had visualizations like this (sorry for the bad crop):\n![screen shot 2015-10-17 at 4 20 14 pm](https://cloud.githubusercontent.com/assets/3902480/10561025/0c197f46-74eb-11e5-8454-559426a63f32.png)\n\nand this live demo: http://agnez.herokuapp.com (this link may change subject or go down at one point)\n\nI'll build this in parallel to Keras to possibly serve other frameworks as well. Obviously it will serve Keras first. The License is compatible, so if anybody wants to adapt it for this repo, I may help.\n"", ""@EderSantana very cool stuff!\n\n> I'll build this in parallel to Keras to possibly serve other frameworks as well.\n\nThat's also what I had in mind: there is currently no web-based visualization tool for deep learning, and the community really needs one, whether for use with Caffe, Torch, Keras... And there is no reason to make it dependent on one DL framework or another, since it will communicate with models via an API. On the Keras side, integration is dead easy via callbacks.\n"", 'I recommend bokeh, manage plotting, gui, callbacks all in html 5 and in one package....and yes, I think a platform independent tool would be awesome. \n', 'TensorFlow port is really great idea. TF needs keras like API as much as Keras would need TF when ported!\n', ""@pranv I think that @fchollet hinted us many times that something was coming\n( e.g. https://github.com/fchollet/keras/issues/666#issuecomment-138946174 https://github.com/fchollet/keras/issues/793 )\n\nWe need to wrap up the force a bit and start abstracting the Keras API as soon as possible.\nHaving TensorFlow in mind because being the first framework to support it would be awesome since it have lot's of visibility right now.\n\nWe need to start thinking about that. A discussion thread should be opened here or on the google group to discuss how it should be achieved.\n"", '@dbonadiman Now that we have confirmation that TF will be the new backend for Keras, why dont we start a new issue and list out the basic TODO list?\n', 'Incidentally, @fchollet - I remember you once linked to some WIP web UI for graphing/visualization - is that an active project?\n', ""Are there any plans for MultiNEAT/HyperNEAT type support? I've loved Keras, and the ability to utilize NEAT functionality inline would be tremendously useful. \n"", 'I would suggest Hyperopt instead of Spearmint since Spearmint could have commercial licensing issues: ""Academic and Non-Commercial Research Use License"".\n\nHyperopt has a more ""open"" license: https://github.com/hyperopt/hyperopt/blob/master/LICENSE.txt\n\nNevertheless, I\'m not sure how it should be integrated, apart from just using Hyperopt on top of Keras (that\'s what I\'ve been doing).\n', 'Anyone have an update on visualization tools??\n', '@claymcleod I did something on a separate project: Agnez\nhttps://github.com/AgnezIO/agnez/tree/master/examples\nit can be used for visualizing Keras models. Check it out if it has something you need.\n', 'The playing with tensorflow visualization is under the apache license:\nhttps://github.com/tensorflow/playground\nsite: http://playground.tensorflow.org/\n', ""I think a nifty little feature that can be added (which doesn't require much modification) could be changing learning rate when you reach patience in Early stopping callback instead of stopping completely. It could passed as an optional flag that determines whether to stop training or run schedule function and set new learning rate for optimizer\n?\n"", 'http://aetros.com/trainer is now available for everyone, so you have basically a model designer and visualisation of your Keras model, also with insights during training (like accuracy, loss, convolutional layer, confusion matrix, etc.).\n']",[],[],0,0
186,keras,8227,closed,[API Inconsistency] GeneratorEnqueuer(random_seed),"Paramater to GeneratorEnqueuer is called 'random_seed', while everywhere else throughout Keras, it's called 'seed'. I can rename it, but should I provide backward compatibility or not?
https://github.com/fchollet/keras/blob/master/keras/utils/data_utils.py#L556",,"['BTW, random_seed parameter is not actually used in Keras codebase. Maybe better remove it altogether?', ""random_seed was introduced in PR https://github.com/fchollet/keras/pull/6891 4 months ago. It's clear that backward compatibility is not needed.""]",[],[],0,0
187,keras,2326,closed,`on_epoch_end` in Callback doesn't receive `acc` key in `logs` argument anymore?,"Hi,

I was using the following piece to report the training progress



This was working before, but after upgrading to latest , it looks  isn't supplied anymore. It gives a KeyError. (I suspect it's related with  being deprecated.)

It looks documentation in http://keras.io/callbacks/ should be updated with the keys as well. 

Thank you very much. 
",,"['Adding `metrics=[""accuracy""]`to `model.compile` solves this problem but the docs should reflect this IMO. \n', 'And so do they.\n']","['\nclass SlackerProgress(Callback):                                                                                            \n    def on_epoch_end(self, epoch, logs={}):                                                                                 \n        hostname = socket.gethostname()                                                                                     \n        msg = """"""End of Epoch {} in {}                                                                                      \n\n*loss:*  {:.2f}                                                                                                             \n*acc:* {:.2f}                                                                                                               \n*val_loss:* {:.2f}                                                                                                          \n*val_acc:* {:.2f}                                                                                                           \n\n"""""".format(epoch, hostname,                                                                                                 \n           logs[""loss""], logs[""acc""], logs[""val_loss""], logs[""val_acc""])                                                    \n        slacker_message(msg)                                                                                                \n\n']","['master', 'logs[""acc""]', 'show_accuracy']",0,0
188,keras,5291,closed,Text Generation With Kearas LSTM with varying time step,"I am quite new to Keras. What I am trying to accomplish is to train a word level text generation module.
To overcome the problem of varying length of sentences in my training data, I set my batch_size to 1 and trained the model with statefull = true. 
Now it will predict next word given a word. Then I tried to concatenate the generated word to the input to the predict function but in that case I get output shape as a array of predictions for each word in the input ( What I wished for was a single prediction for the next word, considering all the previous words)

I want to design something like if i give 
START_TOKEN
Generate say : What
then i concatenate it to START_TOKEN  to get ""START_TOKEN What"" (ie add a new row to input vector)
Then the next word generated should consider both START_TOKEN and word ""What"" to generate the next word and so on....

My current code:




Here the batch function generates a x and y arrays representing with shape (len_corpus, 1, vocabulary)
y is one word ahead of x 
like x= ""how are you....""
 then y =""are you.....

Thanks in advance",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs, but feel free to re-open it if needed.\n']",[],"['model.add(LSTM(100, return_sequences=False, unroll=True, stateful=True, input_shape=(1,len(itw)) , batch_input_shape=(1,1,len(itw)) ))\r\nmodel.add(Activation(\'tanh\'))\r\nmodel.add(Dense(len(itw)))\r\nmodel.add(Activation(\'softmax\'))\r\noptimizer = RMSprop(lr=0.01)\r\nmodel.compile(loss=""categorical_crossentropy"",optimizer=optimizer,metrics=[""accuracy""])\r\na,b =batch1()\r\nmodel.fit(a,b,batch_size=1,nb_epoch=10,verbose=1)']",0,0
189,keras,7548,closed,KerasRegressor/KerasClassifier get_params and set_params incompatible," returns  as a parameter name which  does not accept.

To reproduce:





Raises:


",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],"['get_params()', 'build_fn', 'set_params()', 'from keras.wrappers.scikit_learn import KerasClassifier', 'from keras.models import Sequential', 'm = KerasClassifier(Sequential())', 'm.set_params(**m.get_params())', 'ValueError: build_fn is not a legal parameter']",0,0
190,keras,6580,closed,Note about the categorical_crossentropy?,"Hi,

I'm not sure but the documentation about categorical_crossentropy seems to be not compatible with the tensorflow one.

I mean just had a look at the documentation about loss functions i. e.
https://keras.io/losses/#available-loss-functions

and at the end there is a note about the categorical_crossentropy.
There is such a sentence:

""Note: when using the categorical_crossentropy loss, your targets should be in categorical format (e.g. if you have 10 classes, the target for each sample should be a 10-dimensional vector that is all-zeros expect for a 1 at the index corresponding to the class of the sample).""

So I understand that the only possible target is e.g. [0, 0, 1, 0, 0]?

But in the tensorflow documentation about softmax_cross_entropy_with_logits  we have (https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)

""While the classes are mutually exclusive, their probabilities need not be. All that is required is that each row of labels is a valid probability distribution.""

which I understand means that such a target is possible [0.0, 0.75, 0.25, 0.0, 0.0]?

categorical_crossentropy is defined in https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/keras/python/keras/backend.py and indeed returns softmax_cross_entropy_with_logits.

Am I missing something (if yes then I'm really sorry)?

Best wishes
Dawid
",,"['From what I understand, if you pass `from_logits = False` (default behavior), then the only possible targets are of the form one-hot encoding i.e.  `[0, 0, 1, 0, 0]`. However, if you pass `from_logits = True`, then Keras just calls TensorFlow `softmax_cross_entropy()` function. There I think a target such as `[0.0, 0.75, 0.25, 0.0, 0.0]` is fine. However, I may be wrong since I did not read its C code to find out how `softmax_cross_entropy()` is implemented in TensorFlow.', '@parag2489 I guess your right, I have not noticed the `from_logits` flag but still I would say that the **Note** about categorical_crossentropy in \r\nhttps://keras.io/losses/#available-loss-functions\r\n\r\nis misleading because it does not mention `from_logits` (that is probably why I got confused).\r\n\r\n', '`from_logits` has nothing to do with targets and one-hot encoding. targets can be continuous in any case. It defines how the output (predictions) are interpreted:\r\n\r\n![image](https://cloud.githubusercontent.com/assets/1140359/25938149/91b0148e-362e-11e7-8df0-33d26b4aae4f.png)', 'Yes, I understand that. I had only one doubt. When `from_logits = False`, we should get minimum value when `pred == target`. When I recalculated it now, I did get the minimum value when `pred == target`. Maybe, I did some mistake when I calculated it earlier.', '@gokceneraslan you have used the following `target = [0.2, 0.2, 0.6]` and that is my point. In https://keras.io/losses/#available-loss-functions is is said that `target` should be \r\n\r\n> in categorical format (e.g. if you have 10 classes, the target for each sample should be a 10-dimensional vector that is all-zeros expect for a 1 at the index corresponding to the class of the sample)\r\n\r\nI think that this is missleading since from what you say (and what is written in the tensorflow docs) `target` can be any valid probability distribution.', ""It's just to tell people that instead of integer labels like [2, 3, 1], they should use [[0,0,1,0], [0,0,0,1], [0,1,0,0]]. If you look up the definition of cross-entropy, it's clear that both output and target are probability distributions.""]",[],[],0,0
191,keras,3806,closed,"Lstm input shape error,","Hello , i want to ask your opinion why this is happening. To explain myself until yesterday, my models as simple as they were they were working until today. my code is as follows :
The above code words for 100 or 200 embedding size but not for the above setup. It produces the following error.

The same error is produced and when i use cnn-lstm with word2vec vectors. The weird part is that the models were working yesterday for any value i had test them.
thanks,
nick
",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs, but feel free to re-open it if needed.\n']",[],"[""model = Sequential()\nmodel.add(Embedding(max_features, 350, input_length=maxlen))\nmodel.add(Bidirectional(LSTM(350)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n\nprint('Train...')\nmodel.fit(X_train, y_train,\n          batch_size=batch_size,\n          nb_epoch=10,\n          validation_data=[X_test, y_test])\n\nscore, acc = model.evaluate(X_test, y_test,\n                            batch_size=batch_size)"", 'Traceback (most recent call last):\n  File ""C:/Users/usera/PycharmProjects/Diplomatiki/create.py"", line 72, in <module>\n    validation_data=[X_test, y_test])\n  File ""C:\\Users\\usera\\Anaconda2\\lib\\site-packages\\keras\\models.py"", line 620, in fit\n    sample_weight=sample_weight)\n  File ""C:\\Users\\usera\\Anaconda2\\lib\\site-packages\\keras\\engine\\training.py"", line 1104, in fit\n    callback_metrics=callback_metrics)\n  File ""C:\\Users\\usera\\Anaconda2\\lib\\site-packages\\keras\\engine\\training.py"", line 822, in _fit_loop\n    outs = f(ins_batch)\n  File ""C:\\Users\\usera\\Anaconda2\\lib\\site-packages\\keras\\backend\\theano_backend.py"", line 672, in __call__\n    return self.function(*inputs)\n  File ""C:\\Users\\usera\\Anaconda2\\lib\\site-packages\\theano\\compile\\function_module.py"", line 871, in **call**\n    storage_map=getattr(self.fn, \'storage_map\', None))\n  File ""C:\\Users\\usera\\Anaconda2\\lib\\site-packages\\theano\\gof\\link.py"", line 314, in raise_with_op\n    reraise(exc_type, exc_value, exc_trace)\n  File ""C:\\Users\\usera\\Anaconda2\\lib\\site-packages\\theano\\compile\\function_module.py"", line 859, in **call**\n    outputs = self.fn()\nValueError: Input dimension mis-match. (input[2].shape[0] = 64, input[3].shape[0] = 1003)\nApply node that caused the error: Elemwise{Composite{Switch(i0, (i1 \\* i2 \\* i3), i2)}}[(0, 2)](InplaceDimShuffle{x,x}.0, TensorConstant{%281L, 1L%29 of 2.0}, Join.0, Elemwise{Composite{Cast{float32}%28LT%28i0, i1%29%29}}[%280, 0%29].0)\nToposort index: 208\nInputs types: [TensorType(uint8, (True, True)), TensorType(float32, (True, True)), TensorType(float32, matrix), TensorType(float32, matrix)]\nInputs shapes: [(1L, 1L), (1L, 1L), (64L, 700L), (1003L, 700L)]\nInputs strides: [(1L, 1L), (4L, 4L), (2800L, 4L), (2800L, 4L)]\nInputs values: [array([[1]], dtype=uint8), array([[ 2.]], dtype=float32), \'not shown\', \'not shown\']\nOutputs clients: [[Dot22(Elemwise{Composite{Switch(i0, (i1 \\* i2 \\* i3), i2)}}[(0, 2)].0, dense_1_W), InplaceDimShuffle{1,0}(Elemwise{Composite{Switch(i0, (i1 \\* i2 \\* i3), i2)}}[(0, 2)].0)]]\n']",0,0
192,keras,3647,closed,Theanio Flag Error when  Predict on CNN ,"## Error Message is below (Full Code  is attached)

Using Theano backend.
Using gpu device 0: GeForce GTX 1060 3GB (CNMeM is enabled with initial size: 81.0% of memory, cuDNN 5005)
(6L, 3L, 150L, 150L) <- this is data for predict

Traceback (most recent call last):
  File ""C:\Program Files (x86)\JetBrains\PyCharm Edu 2.0.4\helpers\pydev\pydevd.py"", line 2411, in <module>
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""C:\Program Files (x86)\JetBrains\PyCharm Edu 2.0.4\helpers\pydev\pydevd.py"", line 1802, in run
    launch(file, globals, locals)  # execute the script
  File ""C:/Users/byoru/PycharmProjects/MailLession/FirstNN/Canvas.py"", line 50, in <module>
    output = model.predict(testarrnp, batch_size=2,verbose=1)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\models.py"", line 664, in predict
    return self.model.predict(x, batch_size=batch_size, verbose=verbose)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\engine\training.py"", line 1180, in predict
    batch_size=batch_size, verbose=verbose)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\engine\training.py"", line 879, in _predict_loop
    batch_outs = f(ins_batch)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\backend\theano_backend.py"", line 655, in __call__
    return self.function(*inputs)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\theano\compile\function_module.py"", line 879, in **call**
    storage_map=getattr(self.fn, 'storage_map', None))
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\theano\gof\link.py"", line 325, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\theano\compile\function_module.py"", line 866, in **call**
    self.fn() if output_subset is None else\
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\theano\gof\op.py"", line 908, in rval
    r = p(n, [x[0] for x in i], o)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\theano\tensor\nnet\abstract_conv.py"", line 848, in perform
    conv_out = self.conv2d(img, kern, mode=""valid"", dilation=self.filter_dilation)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\theano\tensor\nnet\abstract_conv.py"", line 775, in conv2d
    dilated_kern[n, im0, ...],
IndexError: index 1 is out of bounds for axis 1 with size 1
Apply node that caused the error: AbstractConv2d{border_mode='valid', subsample=(1, 1), filter_flip=True, imshp=(None, None, None, None), kshp=(32, 3, 3, 3), filter_dilation=(1, 1)}(convolution2d_input_1, HostFromGpu.0)
Toposort index: 33
Inputs types: [TensorType(float32, 4D), TensorType(float32, 4D)]
Inputs shapes: [(2L, 3L, 150L, 150L), (32L, 1L, 3L, 3L)]
Inputs strides: [(270000L, 90000L, 600L, 4L), (36L, 36L, 12L, 4L)]
Inputs values: ['not shown', 'not shown']
Inputs type_num: [11, 11]
Outputs clients: [[Elemwise{add,no_inplace}(AbstractConv2d{border_mode='valid', subsample=(1, 1), filter_flip=True, imshp=(None, None, None, None), kshp=(32, 3, 3, 3), filter_dilation=(1, 1)}.0, Reshape{4}.0)]]

Backtrace when the node is created(use Theano flag traceback.limit=N to make it longer):
  File ""C:/Users/byoru/PycharmProjects/MailLession/FirstNN/Canvas.py"", line 11, in <module>
    model.add(Convolution2D(32, 3, 3, input_shape=(3, 150, 150)))
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\models.py"", line 275, in add
    layer.create_input_layer(batch_input_shape, input_dtype)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\engine\topology.py"", line 367, in create_input_layer
    self(x)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\engine\topology.py"", line 511, in **call**
    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\engine\topology.py"", line 569, in add_inbound_node
    Node.create_node(self, inbound_layers, node_indices, tensor_indices)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\engine\topology.py"", line 150, in create_node
    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\layers\convolutional.py"", line 353, in call
    filter_shape=self.W_shape)
  File ""c:\program files (x86)\microsoft visual studio 12.0\vc\theano\keras\keras\backend\theano_backend.py"", line 1073, in conv2d
    filter_shape=filter_shape)

Debugprint of the apply node: 
AbstractConv2d{border_mode='valid', subsample=(1, 1), filter_flip=True, imshp=(None, None, None, None), kshp=(32, 3, 3, 3), filter_dilation=(1, 1)} [id A] <TensorType(float32, 4D)> ''  
 |convolution2d_input_1 [id B] <TensorType(float32, 4D)>
 |HostFromGpu [id C] <TensorType(float32, 4D)> ''  
   |convolution2d_1_W [id D] <CudaNdarrayType(float32, 4D)>

Storage map footprint:
- dense_1_W, Shared Input, Shape: (18496, 64), ElemSize: 4 Byte(s), TotalSize: 4734976 Byte(s)
- convolution2d_input_1, Input, Shape: (2L, 3L, 150L, 150L), ElemSize: 4 Byte(s), TotalSize: 540000 Byte(s)
- <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (92160,), ElemSize: 4 Byte(s), TotalSize: 368640 Byte(s)
- <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (92160,), ElemSize: 4 Byte(s), TotalSize: 368640 Byte(s)
- <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (92160,), ElemSize: 4 Byte(s), TotalSize: 368640 Byte(s)
- convolution2d_3_W, Shared Input, Shape: (64, 32, 3, 3), ElemSize: 4 Byte(s), TotalSize: 73728 Byte(s)
- convolution2d_2_W, Shared Input, Shape: (32, 32, 3, 3), ElemSize: 4 Byte(s), TotalSize: 36864 Byte(s)
- HostFromGpu.0, Shape: (32L, 1L, 3L, 3L), ElemSize: 4 Byte(s), TotalSize: 1152 Byte(s)
- convolution2d_1_W, Shared Input, Shape: (32, 1, 3, 3), ElemSize: 4 Byte(s), TotalSize: 1152 Byte(s)
- convolution2d_3_b, Shared Input, Shape: (64,), ElemSize: 4 Byte(s), TotalSize: 256 Byte(s)
- dense_1_b, Shared Input, Shape: (64,), ElemSize: 4 Byte(s), TotalSize: 256 Byte(s)
- dense_2_W, Shared Input, Shape: (64, 1), ElemSize: 4 Byte(s), TotalSize: 256 Byte(s)
- convolution2d_1_b, Shared Input, Shape: (32,), ElemSize: 4 Byte(s), TotalSize: 128 Byte(s)
- convolution2d_2_b, Shared Input, Shape: (32,), ElemSize: 4 Byte(s), TotalSize: 128 Byte(s)
- TensorConstant{[ 1 32  1  1]}, Shape: (4L,), ElemSize: 4 Byte(s), TotalSize: 16 Byte(s)
- TensorConstant{[ 1 64  1  1]}, Shape: (4L,), ElemSize: 4 Byte(s), TotalSize: 16 Byte(s)
- TensorConstant{[ 1 32  1  1]}, Shape: (4L,), ElemSize: 4 Byte(s), TotalSize: 16 Byte(s)
- Constant{0}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)
- Constant{0}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)
- TensorConstant{0.5}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- dense_2_b, Shared Input, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)
- TensorConstant{0.5}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- DimShuffle{x,x,x,x}.0, Shape: (1L, 1L, 1L, 1L), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)
- TensorConstant{0.800000011921}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- TensorConstant{0.800000011921}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- TensorConstant{0.5}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- DimShuffle{x,x}.0, Shape: (1L, 1L), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)
- TensorConstant{0.5}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- TensorConstant{0.5}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- TensorConstant{1.0}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- TensorConstant{0.5}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- DimShuffle{x,x,x,x}.0, Shape: (1L, 1L, 1L, 1L), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)
- DimShuffle{x,x,x,x}.0, Shape: (1L, 1L, 1L, 1L), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)
- TensorConstant{0.800000011921}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- TensorConstant{0.800000011921}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- TensorConstant{0.0}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
- DimShuffle{x,x,x,x}.0, Shape: (1L, 1L, 1L, 1L), ElemSize: 1 Byte(s), TotalSize: 1 Byte(s)
- DimShuffle{x,x,x,x}.0, Shape: (1L, 1L, 1L, 1L), ElemSize: 1 Byte(s), TotalSize: 1 Byte(s)
- DimShuffle{x,x}.0, Shape: (1L, 1L), ElemSize: 1 Byte(s), TotalSize: 1 Byte(s)
- keras_learning_phase, Input, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)
  TotalSize: 6494952.0 Byte(s) 0.006 GB
  TotalSize inputs: 6493781.0 Byte(s) 0.006 GB

---

---------- Full Code ----------------------------
import numpy as np
import os
from PIL import Image
from keras.models import Sequential
from keras.layers import Convolution2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense

from keras.preprocessing.image import ImageDataGenerator

model = Sequential()
model.add(Convolution2D(32, 3, 3, input_shape=(3, 150, 150)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Convolution2D(32, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))

model.add(Convolution2D(64, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))

model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors
model.add(Dense(64))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(1))
model.add(Activation('sigmoid'))

model.load_weights('weight1.h5')

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])
path = 'test'
testarr = []

for item in os.listdir(path):
    imgpath = path + '\' + item
    img = Image.open(imgpath)
    resizedimg = img.resize((150,150),Image.ANTIALIAS)
    data = np.array( resizedimg )
    data2 = data.transpose(2,0,1)
    testarr.append(data2)
testarrnp = np.asarray(testarr)

print testarrnp.shape
output = model.predict(testarrnp, batch_size=2,verbose=1)
print output
",stale,"['I dont know what is wrong.. and how can i fix this...\n\npls help me\n', 'You probably have an error in your code/shapes. To help you find it, you\ncan use test_values in Theano:\n\nhttp://deeplearning.net/software/theano/tutorial/debug_faq.html?highlight=test%20value#using-test-values\n\nOn Wed, Aug 31, 2016 at 10:44 AM, idioluck notifications@github.com wrote:\n\n> I dont know what is wrong.. and how can i fix this...\n> \n> pls help me\n> \n> —\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/fchollet/keras/issues/3647#issuecomment-243787138,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AALC-1UmoW0BslCz5o-bSipeu8y-Y4Juks5qlZNTgaJpZM4JxsXq\n> .\n']",[],[],0,0
193,keras,6129,closed,Bug: error when using Activation('linear') as first layer,"Keras throws an error when the first layer is an  layer with  (this is a pretty unusual use case, but it should work). This seems to be because the definition of that layer is simply:

    def linear(x):
        return x

So the topology is not properly created.
Here's a simple script to reproduce the error:


	from keras.models import Sequential
	from keras.layers import Dense, Activation
	import numpy as np

	def build_net_bug(activation):
		model = Sequential()
		model.add(Activation(activation,input_shape=(12,)))
		model.add(Dense(2))
		model.compile(loss='categorical_crossentropy', optimizer='sgd')
		return model

	model = build_net_bug('linear')
	model.train_on_batch(np.zeros((32,12)),np.zeros((32,2)))
	print('Ok')

Producted the following error:

    /path/experimental/keras/keras/engine/topology.py:1516: UserWarning: Model inputs must come from a Keras Input layer, they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to ""sequential_1_model"" was not an Input tensor, it was generated by layer activation_1.
    Note that input tensors are instantiated via .
    The tensor that caused the issue was: activation_1_input:0
      str(x.name))
    Traceback (most recent call last):
      File ""test_keras_bug_2.py"", line 12, in <module>
        model = build_net_bug('linear')
      File ""test_keras_bug_2.py"", line 9, in build_net_bug
        model.compile(loss='categorical_crossentropy', optimizer='sgd')
      File ""/path/experimental/keras/keras/models.py"", line 761, in compile
        self.build()
      File ""/path/experimental/keras/keras/models.py"", line 520, in build
        name=self.name + '_model')
      File ""/path/experimental/keras/keras/legacy/interfaces.py"", line 88, in wrapper
        return func(*args, **kwargs)
      File ""/path/experimental/keras/keras/engine/topology.py"", line 1569, in __init__
        if layer.is_placeholder:
    AttributeError: 'Activation' object has no attribute 'is_placeholder'


Replacting  with any other activation fixes the issue.
Any suggestions on the best way to fix this (preferably without impacting the performance of  ? ",,"['We can fix it by adding an identity op to the backend. That\'s easy in TF,\nunsure if we can do it without generating a copy of the tensor data with\nTheano.\n\nOn 3 April 2017 at 19:28, Yann Henon <notifications@github.com> wrote:\n\n> Keras throws an error when the first layer is an Activation layer with\n> mode=\'linear\' (this is a pretty unusual use case, but it should work).\n> This seems to be because the definition of that layer is simply:\n>\n> def linear(x):\n>     return x\n>\n> So the topology is not properly created.\n> Here\'s a simple script to reproduce the error:\n>\n> from keras.models import Sequential\n> from keras.layers import Dense, Activation\n> import numpy as np\n>\n> def build_net_bug(activation):\n> \tmodel = Sequential()\n> \tmodel.add(Activation(activation,input_shape=(12,)))\n> \tmodel.add(Dense(2))\n> \tmodel.compile(loss=\'categorical_crossentropy\', optimizer=\'sgd\')\n> \treturn model\n>\n> model = build_net_bug(\'linear\')\n> model.train_on_batch(np.zeros((32,12)),np.zeros((32,2)))\n> print(\'Ok\')\n>\n> Producted the following error:\n>\n> /path/experimental/keras/keras/engine/topology.py:1516: UserWarning: Model inputs must come from a Keras Input layer, they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to ""sequential_1_model"" was not an Input tensor, it was generated by layer activation_1.\n> Note that input tensors are instantiated via `tensor = Input(shape)`.\n> The tensor that caused the issue was: activation_1_input:0\n>   str(x.name))\n> Traceback (most recent call last):\n>   File ""test_keras_bug_2.py"", line 12, in <module>\n>     model = build_net_bug(\'linear\')\n>   File ""test_keras_bug_2.py"", line 9, in build_net_bug\n>     model.compile(loss=\'categorical_crossentropy\', optimizer=\'sgd\')\n>   File ""/path/experimental/keras/keras/models.py"", line 761, in compile\n>     self.build()\n>   File ""/path/experimental/keras/keras/models.py"", line 520, in build\n>     name=self.name + \'_model\')\n>   File ""/path/experimental/keras/keras/legacy/interfaces.py"", line 88, in wrapper\n>     return func(*args, **kwargs)\n>   File ""/path/experimental/keras/keras/engine/topology.py"", line 1569, in __init__\n>     if layer.is_placeholder:\n> AttributeError: \'Activation\' object has no attribute \'is_placeholder\'\n>\n> Any suggestions on the best way to fix this (preferably without impacting\n> the performance of Activation(\'linear\') ?\n>\n> —\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/fchollet/keras/issues/6129>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AArWb2uGmOrjLleMx-wQ-JAV-jt_djt1ks5rsSwxgaJpZM4Mx2WQ>\n> .\n>\n', '@fchollet Why not just do `return x + 0.` or ` return 1. * x` ?', 'This would create a tensor copy at least in TF.\n\nOn Apr 3, 2017 11:51, ""Fariz Rahman"" <notifications@github.com> wrote:\n\n> @fchollet <https://github.com/fchollet> Why not just do return x + 0. or return\n> 1. * x ?\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/fchollet/keras/issues/6129#issuecomment-291238084>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AArWbybPVrlpu571veFecodVJ6iYUDxhks5rsT_LgaJpZM4Mx2WQ>\n> .\n>\n', 'I have the same bug on using Dropout as the first layer (keras 2 only)', 'I can confirm this bug also happens when using `Dropout(0)` or `Dropout(1)` as the first layer.\r\nAny layer that just passes the data through will create this issue. For example with a lambda layer:\r\n\r\n```\r\ndef no_op(x):\r\n\treturn x\r\n\r\ndef build_net_bug(activation):\r\n\tmodel = Sequential()\r\n\tmodel.add(Lambda(no_op,output_shape=(12,),input_shape=(12,)))\r\n```\r\n\r\nWill throw an error. I think simply adding an identity op is not quite sufficient in this case, and it may be necessary to make an appropriate adjustment to `Container(Layer)` `__init__` ?\r\n\r\n', 'Fixed in 17ef113ed76cd646622acd7eabeea473c5a59f86']",[],"['Activation', ""mode='linear'"", 'tensor = Input(shape)', ""'linear'"", ""Activation('linear')""]",0,0
194,keras,2554,closed,Multiple workers gone from fit_generator,"It was a very useful feature. Why was it removed? Could it be readded?
",,"[""I'd like to see it back too, and it wouldn't be hard with joblib.\n"", 'Maybe you could propose a pr? @roryhr  :smile: \n\n@fchollet, I guess it was removed because the implementation felt messy, rather than the feature not being wanted? I hope.\n', 'Duplicate issue\n']",[],[],0,0
195,keras,2028,closed,How to build autoencode on other layers?,"Hi,everybody. 
I have a convolution network, i want to use an autoencode above the convolution network out, in order to learn the condensed features.

the convolution part is below, how should i do next?
### covolution part

convolutionpart=Sequential()
convolutionpart.add(Convolution1d(100,3,input_shape=(trian[1].shape,300)))
convolutionpart.add(MaxPooling1D(pool_length=4))
### autoencoder here
",stale,[],[],[],0,0
196,keras,1614,closed,ImageDataGenerator with regression problem,"Dear all,

I would like to use ImageDataGenerator with a regression problem (detect facial key points - https://www.kaggle.com/c/facial-keypoints-detection). By looking at the code I've got the feeling that_ y_ value is not modified when images are transformed. 
Is that true ? 
If so are you planning to support a kind of regression mode in parallel to the classification one 
",stale,"['true\n', 'If you are interested I did some small modifications to handle that use case for rotation, shift and flipping. You can find modified preprocessing image here (https://github.com/gregLibert/keras/blob/master/keras/preprocessing/image.py) or I can pull it ...\n', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],0,0
197,keras,6213,closed,Concatenate Layer Errors,"Keras = 2.0.2
TensorFlow = 1.0.0
Python3
Mac OSX

Error:

Using tensorflow backend here is my network:

     convolutionInput = Input(shape=(1, maxRow, cols), name='convolutional_input')
     x = Conv2D(32, (3, cols), input_shape=(1, maxRow, cols), data_format='channels_first')(convolutionInput)
     x = MaxPooling2D(pool_size=(2, 1))(x)
     x = Dropout(.5)(x)
     convolutionOutput = Flatten()(x)
     additionalInput = Input(shape=(1,), name='additional_input')
     x = Concatenate([convolutionOutput, additionalInput], axis=1)
     x = Dense(64, activation='relu')(x)
     x = Dense(64, activation='relu')(x)
     finalOutput = Dense(2, activation='softmax')(x)
     convoNet = Model(inputs=[convolutionInput, additionalInput], outputs=finalOutput)
     convoNet.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])
     convoNet.fit(x={'convolutional_input': trainingSet[0], 'additional_input': trainingSet[1]}, y=trainLabels, epochs=20, batch_size=10)

And I get this error:

      x = Concatenate([convolutionOutput, additionalInput], axis=1)
     TypeError: __init__() got multiple values for argument 'axis'

I also tried not including the axis keyword argument at all and got this error:

     Traceback (most recent call last):
       File ""/Users/bl755p/Documents/WRT_NLP.py"", line 683, in <module>
         x = Dense(64, activation='relu')(x)
       File ""/Users/bl755p/anaconda/envs/ATT_NLP-Keras2/lib/python3.5/sitepackages/keras/engine/topology.py"", line 511, in __call__
         self.assert_input_compatibility(inputs)
       File ""/Users/bl755p/anaconda/envs/ATT_NLP-Keras2/lib/python3.5/site-packages/keras/engine/topology.py"", line 423, in assert_input_compatibility
         ndim = K.ndim(x)
       File ""/Users/bl755p/anaconda/envs/ATT_NLP-Keras2/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py"", line 437, in ndim
         dims = x.get_shape()._dims
     AttributeError: 'Concatenate' object has no attribute 'get_shape'
",,"['`Concatenate` -> `concatenate`\r\n', ""I changed it to this and it worked:\r\n\r\n     concat = Concatenate(axis=1)\r\n     x = concat([convolutionOutput, additionalInput])\r\n\r\nChanging to lowercase gave this error though:\r\n\r\n     NameError: name 'concatenate' is not defined\r\n\r\nI guess is there a different place I need to import that maybe?"", '`from keras.layers import concatenate`', '```python\r\nimport tensorflow as tf\r\n\r\nc1 = tf.constant([[1,2,3], [4,5,6]], dtype=tf.float32)\r\nc2 = tf.constant([[1,2,3], [4,5,6]], dtype=tf.float32)\r\nl1 = tf.keras.layers.Dense(10)(c1)\r\nl2 = tf.keras.layers.Dense(10)(c2)\r\nconcat = tf.keras.layers.Concatenate(axis=1)([l1, l2])\r\nout = tf.keras.layers.Dense(10)(concat)\r\n\r\nprint(""-""*30)\r\nprint(out)\r\nprint(out.shape)\r\n\r\n```', ""I ran into this too, even though I've used it before. @SpikingNeuron has it right - the key thing is that the merged layers are in the second function call, just like you would for other normal layers. Otherwise you get things like: \r\n```\r\n'Concatenate' object has no attribute 'shape'\r\n```\r\nHad me scratching my head for a bit."", 'The syntax is \r\n\r\n```python\r\nfrom tensorflow.keras.layers import Concatenate\r\n# prev_layer1 = Dense(...)\r\n# prev_layer2 = Dense(...)\r\nmerged = Concatenate()([prev_layer1, prev_layer2])\r\n```\r\n']",[],[],0,0
198,keras,7197,closed,Feature Wanted? Recurrent Additive Networks. I'd be happy to implement/PR but...,"A new paper from [Lee et al.](http://www.kentonl.com/pub/llz.2017.pdf) details a new, simplified recurrent neural network dubbed the ""Recurrent Additive Network."" It seems to do well on language modeling tasks when compared to the LSTM, despite using simpler computation with less parameters. This model would be easy to implement in Keras as a subclass of Recurrent, and I'd be happy to do it. However, my question is: is this something that would be pulled into the main repo? What is the standard policy for adding new sorts of RNNs?",,"['You can submit your PR to keras-contrib and then if people like it, it will get merged into Keras.', 'Cool! Just wanted to see if that was the main path still.', 'I think its now more interesting to focus on SRU (https://github.com/fchollet/keras/issues/7870#issuecomment-328642850) since it has the same philosophy, its more up to date, and it already takes RAN as a reference.']",[],[],0,0
199,keras,3311,closed,Error in deconv output shape estimation,"Here is a modified version of the  file. It does downsampling in the input and upsampling in the output it compiles and converges as expected but the estimated output shape is wrong it says  instead of . There may be an error in the deconv layer output shape estimation

I'll look into this now @yaringal , but if you already have the solution let me know.

Copy paste the code below to reproduce results

Lambda(sampling)([z_mean, z_log_var])
",stale,"['as a quick fix, we can just return `output_shape_` in the `get_output_shape_for_` function. But it would be nice if we use shape inference once and for all. You mentioned you wanted to help out @lukovkin ?\n', 'I\'ve made a PR on this to the @yaringal fork.\nIt seems to be working, but there\'s a Theano-specific issue there - output_shape becomes Theano expression, but Theano doesn\'t accept it as the input shape. Evaluation fails too.\nI was AFK today, will try to clarify tomorrow, but you can take a look at Deconv PR - my PR is mentioned there and I\'ve commented there on this issue.\nCode is not clean yet as soon as I was working to solve this issue.\n\n-----Original Message-----\nFrom: ""Eder Santana"" notifications@github.com\nSent: \u200e26.\u200e07.\u200e2016 1:16\nTo: ""fchollet/keras"" keras@noreply.github.com\nCc: ""Dmitry Lukovkin"" dmitry.lukovkin@gmail.com; ""Mention"" mention@noreply.github.com\nSubject: Re: [fchollet/keras] Error in deconv output shape estimation (#3311)\n\nas a quick fix, we can just return output_shape_ in the get_output_shape_for_ function. But it would be nice if we use shape inference once and for all. You mentioned you wanted to help out @lukovkin ?\n—\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n', 'deconv was merged here. you can work directly on master now\n']",[],"['variational_autoencoder_deconv.py', '1, 27, 27', '1, 28, 28', '', ' python\n\'\'\'This script demonstrates how to build a variational autoencoder with Keras and deconvolution layers.\n\nReference: ""Auto-Encoding Variational Bayes"" https://arxiv.org/abs/1312.6114\n\'\'\'\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom keras.layers import Input, Dense, Lambda, Flatten, Reshape\nfrom keras.layers import Convolution2D, Deconvolution2D, MaxPooling2D\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras import objectives\nfrom keras.datasets import mnist\n\n# input image dimensions\nimg_rows, img_cols, img_chns = 28, 28, 1\n# number of convolutional filters to use\nnb_filters = 32\n# convolution kernel size\nnb_conv = 3\n\nbatch_size = 16\noriginal_dim = (img_chns, img_rows, img_cols)\nlatent_dim = 2\nintermediate_dim = 128\nepsilon_std = 0.01\nnb_epoch = 5\n\n\nx = Input(batch_shape=(batch_size,) + original_dim)\nc = Convolution2D(nb_filters, nb_conv, nb_conv, subsample=(2, 2), border_mode=\'same\', activation=\'relu\')(x)\nf = Flatten()(c)\nh = Dense(intermediate_dim, activation=\'relu\')(f)\n\nz_mean = Dense(latent_dim)(h)\nz_log_var = Dense(latent_dim)(h)\n\n\ndef sampling(args):\n    z_mean, z_log_var = args\n    epsilon = K.random_normal(shape=(batch_size, latent_dim),\n                              mean=0., std=epsilon_std)\n    return z_mean + K.exp(z_log_var) * epsilon\n\n# note that ""output_shape"" isn\'t necessary with the TensorFlow backend\n# so you could write ', ""\nz = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n\n# we instantiate these layers separately so as to reuse them later\ndecoder_h = Dense(intermediate_dim, activation='relu')\ndecoder_f = Dense(nb_filters*img_rows*img_cols/4, activation='relu')\ndecoder_c = Reshape((nb_filters, img_rows/2, img_cols/2))\ndecoder_mean = Deconvolution2D(img_chns, nb_conv, nb_conv,\n                               (batch_size, img_chns, img_rows, img_cols),\n                               subsample=(2, 2),\n                               border_mode='same')\n\nh_decoded = decoder_h(z)\nf_decoded = decoder_f(h_decoded)\nc_decoded = decoder_c(f_decoded)\nx_decoded_mean = decoder_mean(c_decoded)\n\n\ndef vae_loss(x, x_decoded_mean):\n    # NOTE: binary_crossentropy expects a batch_size by dim for x and x_decoded_mean, so we MUST flatten these!\n    x = K.flatten(x)\n    x_decoded_mean = K.flatten(x_decoded_mean)\n    xent_loss = objectives.binary_crossentropy(x, x_decoded_mean)\n    kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n    return xent_loss + kl_loss\n\nvae = Model(x, x_decoded_mean)\nvae.compile(optimizer='rmsprop', loss=vae_loss)\nvae.summary()\n\n# train the VAE on MNIST digits\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nx_train = x_train.astype('float32')[:, None, :, :] / 255.\nx_test = x_test.astype('float32')[:, None, :, :] / 255.\n\nvae.fit(x_train, x_train,\n        shuffle=True,\n        nb_epoch=nb_epoch,\n        batch_size=batch_size,\n        validation_data=(x_test, x_test))\n\n\n# build a model to project inputs on the latent space\nencoder = Model(x, z_mean)\n\n# display a 2D plot of the digit classes in the latent space\nx_test_encoded = encoder.predict(x_test, batch_size=batch_size)\nplt.figure(figsize=(6, 6))\nplt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\nplt.colorbar()\nplt.show()\n\n# build a digit generator that can sample from the learned distribution\ndecoder_input = Input(shape=(latent_dim,))\n_h_decoded = decoder_h(decoder_input)\n_f_decoded = decoder_f(_h_decoded)\n_c_decoded = decoder_c(_f_decoded)\n_x_decoded_mean = decoder_mean(_c_decoded)\ngenerator = Model(decoder_input, _x_decoded_mean)\n\n# display a 2D manifold of the digits\nn = 15  # figure with 15x15 digits\ndigit_size = 28\nfigure = np.zeros((digit_size * n, digit_size * n))\n# we will sample n points within [-15, 15] standard deviations\ngrid_x = np.linspace(-15, 15, n)\ngrid_y = np.linspace(-15, 15, n)\n\nfor i, yi in enumerate(grid_x):\n    for j, xi in enumerate(grid_y):\n        z_sample = np.array([[xi, yi]])\n        x_decoded = generator.predict(z_sample)\n        digit = x_decoded[0].reshape(digit_size, digit_size)\n        figure[i * digit_size: (i + 1) * digit_size,\n               j * digit_size: (j + 1) * digit_size] = digit\n\nplt.figure(figsize=(10, 10))\nplt.imshow(figure)\nplt.show()\n"", '']",0,0
200,keras,2931,closed,Maximum Recursion Depth error in Model.fit,"I am running a deep net model on Keras code. The training model was running perfectly fne on image size 76_76, but it not working on image dimension 128_128.

I got the below error :

  <i>File ""/usr/local/lib/python2.7/dist-packages/theano/tensor/opt.py"", line 4262, in get_num_denum
    pairs = [self.get_num_denum(input2) for input2 in parent.inputs]
RuntimeError: maximum recursion depth exceeded

if isinstance(c, graph.Constant) and (len(c.clients) <= 1):
RuntimeError: ('maximum recursion depth exceeded while calling a Python object', 'Please, report this to theano-dev mailing list. As a temporary work around, you can raise Python stack limit with: import sys; sys.setrecursionlimit(10000)')</i>

I am using a GPU machine (NVIDIA GF110GL [Tesla M2075]) with fairly good configuration. I don't want to proceed with temporary work-around because in that case, the code is getting stuck before training. Kindly suggest some better way to handle this issue. 
",,"['The Keras build used is from the below URL : https://github.com/fchollet/keras/tree/d6e33ce5558169de2c85bad5360609323cdc44c5\n\nThe Model architecture can be found in the below gist.\n\nhttps://gist.github.com/nitish11/d1e90390a5cc203e9e7a493a9e1d85e9\n', 'the error massage gives clear solution\nwrite the following code at the beginning\n\n```\nimport sys\nsys.setrecursionlimit(10000)\n```\n', ""Hi @ymcui ,\nthanks for the reply. \n\nI have already tried this approach and I have already mentioned the code is getting stuck in training. \nIt's taking more time (waited for 30 minutes)  to start the training.\n"", 'The long traning time is due to the size of the graph. You can use the Theano flag optimizer=fast_compile to get something faster, but would slow down the run time.\n', 'I used following command to run the training and waited for an hour, the training is not getting started.\n\n<i>THEANO_FLAGS=mode=FAST_RUN,optimizer=fast_compile,device=gpu,floatX=float32  python model_training.py </i>\n', ""There is a new undocumented flag that can help speed up the compilation by\n~5-10%:\n\noptimizer_excluding=UnShapeOpt\n\nWe are actively working on making the optimizer in fast_compile faster. We\nshould have something merged next week for this in the new back-end. Follow\nthis PR to stay up to date: https://github.com/Theano/Theano/pull/4570/files\n\nAnother option for now is to pickle a compiled Theano function. When we\nunpickle it, we skip the longer part in this case.\n\nThe first time you compile it on one computer, the compilation can be\nslower due to the C compilation, but as we cache this, the recompilation\nwill be faster. This with the pickle should make something fast (but yes,\nthe pickle isn't super trivial to use)\n\nOn Thu, Jun 9, 2016 at 8:01 AM, Nitish Bhardwaj notifications@github.com\nwrote:\n\n> I used following command to run the training and waited for an hour, the\n> training is not getting started.\n> \n> *THEANO_FLAGS=mode=FAST_RUN,optimizer=fast_compile,device=gpu,floatX=float32\n> python model_training.py *\n> \n> —\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/fchollet/keras/issues/2931#issuecomment-224874630,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AALC-_PTXKnxcwWgz3Uo9B-X71z9lUW7ks5qKACFgaJpZM4IwpOp\n> .\n"", 'Thanks @nouiz. I will follow up the PR https://github.com/Theano/Theano/pull/4570/files \n']",[],[],0,0
201,keras,3809,closed,How does fit_generator works,"I'm not quite understand how fit_generator works. In particular what does mean parameter samples_per_epoch? As described [here](https://github.com/fchollet/keras/issues/1627) samples_per_epoch = batch_size \* numbers of batches and fit_generator will read exactly samples_per_epoch samples from generator. And there also said that we control batch size by our generator yieldings. But here begins what I'm really missing.

Let's assume that generator return batch of 5 single samples and we want to train model on 10 batches per epoch so we have training on 50 single samples per epoch. But if we set samples_per_epochs with given formula we get 50 samples taken by fit_generator and each sample is a BATCH yielded by generator so we get 5*50=250 single samples.  I see that we can get samples number in a batch by dividing input length by length of input shape's zero axis. Therefore we can calculate how much times we need to ask for a batch. But it seems too intricately for me.

Ok. Then let's suppose that the formula is wrong and in this case we set samples_per_epoch to our batch_size. It looks more confident for me. Then generator will take exactly sample_per_epoch batches from generator. Again batch size in units of single samples can be computed by dividing.

So the question is what is actually right?
",,[],[],[],0,0
202,keras,6205,closed,How use Transfer Learning on keras 2.0.2,"I want use keras develop a Face Recognition project, I think maybe need Transfer Learning and have some way can use Transfer Learning on keras 2.0.2
thank you",stale,"['Francois Chollet has written a blog post about transfer learning: [https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html). You may need to do some modifications because of API changes in Keras 2.0. \r\n\r\nThe Keras docs have some code examples how to use the models included in the release:\r\n[https://keras.io/applications](https://keras.io/applications)', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],0,0
203,keras,571,closed,How to save the loss and acc of each batch  to the file? And how to output the results of each layer?,"I am running the keras eample-imdb. I am a fresh man, leaning DL not long. I like the keras framework. It looks so easy. Besides the question of title, I also want to know how to save the output of each layer. Thanks!
",,['P.S  I used the imdb_cnn. \nExpect more eamples and study materials about text classfication(or sentiment classification) by keras. Thanks!\n'],[],[],0,0
204,keras,1787,closed,What happened to WordContextProduct?,"

This page now returns a 404: https://github.com/fchollet/keras/blob/master/examples/skipgram_word_embeddings.py

Was this code taken out of keras, or just moved somewhere else?

Thanks,

Zach

---

Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",,"['I believe it was removed as there is a dedicated module called Gensim that can replace it.\n', 'It was removed a while ago. As dandxy89 said, check gensim if you want to use word2vec. \n\nIf you want to achieve the same thing using keras, I believe you could do so using some of the core layers.\nThis is what get_output looked like in the old WordContextProduct\n\n```\ndef get_output(self, train=False):\n        X = self.get_input(train)\n        w = self.W_w[X[:, 0]]  # nb_samples, proj_dim\n        c = self.W_c[X[:, 1]]  # nb_samples, proj_dim\n\n        dot = T.sum(w * c, axis=1)\n        dot = theano.tensor.reshape(dot, (X.shape[0], 1))\n        return self.activation(dot)\n```\n\nSo, if I understand this right, you just do a dot product of two Embeddings, this should be possible using the `Merge` layer (please correct me if I am wrong)\n', 'Ahhh, ok.  So is the reccomended workflow now:\n1. Use gensim to find the word vectors\n2. Use an embedding layer in keras that uses those word vectors\n?\n\nThanks!\n', 'It would be nice to have some examples in the examples folder of using gensim + keras, to replace the old WordContextProduct examples.\n', ""If you need something like WordContextProduct for your models, it's easy to\nreimplement with a Graph (or just a Merge) and a dot merge.\n\nOn 22 February 2016 at 07:58, Zach Mayer notifications@github.com wrote:\n\n> It would be nice to have some examples in the examples folder of using\n> gensim + keras, to replace the old WordContextProduct examples.\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/fchollet/keras/issues/1787#issuecomment-187243392.\n"", 'Thanks for all the advice!\n', ""I'm trying something like this:\n\n``` python\nmodels = []\n\n# Word vectors\nmodel_word = Sequential()\nmodel_word.add(Embedding(1e4, 300, input_length=1))\nmodel_word.add(Reshape(dims=(300,)))\nmodels.append(model_word)\n\n# Context vectors\nmodel_context = Sequential()\nmodel_context.add(Embedding(1e4, 300, input_length=1))\nmodel_context.add(Reshape(dims=(300,)))\nmodels.append(model_context)\n\n# Combined model\nmodel = Sequential()\nmodel.add(Merge(models, mode='dot'))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\nmodel.compile(loss='mean_squared_error', optimizer=Adam(lr=0.001))\n```\n\nDoes that look reasonable?  And then as input, I need to provide 2 lists of indexes?\n"", 'See updated comment above.  Its for a word vector model with 1e4 words into an embedding with 300 dimensions.\n', 'Hi zachmayer,\n\n  Have you ran your code in the latest comments? Seems doesn\'t work for me, get the error: ""Exception: Input 0 is incompatible with layer dense_1: expected ndim=2, found ndim=5"".\n\nRegards\nJunwei Pan\n', ""Try it with modifying  the following line: \n`model.add(Merge(models, mode='dot', dot_axes=1))`\n"", 'Hi nooralahzadeh,\n\n  Seems that your solution does not work, after setting `dot_axes=1`, the output shape of the Merge layer is not correct, if the input is a (1,100) and (5,100) tensor respectively, then the output will become (100, 100) rather than the (1,5) which is the desired output. I have pasted the code under https://github.com/kemaswill/keras/blob/master/examples/word2vec.py\n\nRegards\nJunwei Pan\n', 'It should work in the original code from @zachmayer . your code is different because you get negative sample in once, but it should be pair in training time like (word1, negSample1) (word1,negSample2) ... based on the output from WordContextProduct . your input looks like (word, negSample, negSample,....,negSample) \n', '  Yes, it works by adding the `dot_axes=1` to the original code from @zachmayer . I just wonder why it does not work for my code which train multiple negative samples at the same time. Since I have implemented the word2vec in a similar way using Lua and Torch: https://github.com/kemaswill/word2vec_torch/blob/master/word2vec.lua. Thanks a lot, @nooralahzadeh \n', 'Problem resolved, I should use `dot_axes=[2,2]` instead. Thanks!\n']","[' python\nIn [1]: import keras\n\nIn [2]: keras.__version__\nOut[2]: \'0.3.2\'\n\nIn [3]: from keras.layers.embeddings import WordContextProduct\nUsing Theano backend.\n/usr/local/lib/python3.5/site-packages/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n  warnings.warn(""downsample module has been moved to the pool module."")\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\n<ipython-input-3-65e83b407b3e> in <module>()\n----> 1 from keras.layers.embeddings import WordContextProduct\n\nImportError: cannot import name \'WordContextProduct\'\n']",[],0,0
205,keras,3310,closed,DisconnectedInputError: When Popping Layers and Adding on Top,"Currently I'm trying to use multiple vgg16 models from the keras model zoo to classify videos.
At this stage I'm taking only a few frames from each video to classify the whole video.
What I do is I take a VGG model, popout the last two layers and merge a few of these models togethers.

But this method isn't working right now, when I try to train the network. I get an error from Theano about disconnected inputs, These inputs are apparently the inputs that I popped off the vgg model. So I think that might be the problem.  Also I don't get this error if I don't load the weights for the vgg model.

Here is a gist that reproduces this error
https://gist.github.com/m1sk/b8ed6d43a5ea86ae51f193d5fc2c01b3

Also any advice with a better way to build this network will be appreciated.

Specs:
Latest Keras and Theano (from git) - updated today
Running on Windows 8.1 with Anaconda

The full text of the error:

> ---
> 
> DisconnectedInputError                    Traceback (most recent call last)
> <ipython-input-22-77cc237b2259> in <module>()
>      79 y_train = np.random.randint(2, size=examples)
>      80 
> ---> 81 merged_model.fit(x_train,y_train)
> 
> N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\models.pyc in fit(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)
>     430                               shuffle=shuffle,
>     431                               class_weight=class_weight,
> --> 432                               sample_weight=sample_weight)
>     433 
>     434     def evaluate(self, x, y, batch_size=32, verbose=1,
> 
> N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\engine\training.pyc in fit(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)
>    1077         else:
>    1078             ins = x + y + sample_weights
> -> 1079         self._make_train_function()
>    1080         f = self.train_function
>    1081 
> 
> N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\engine\training.pyc in _make_train_function(self)
>     694             # get trainable weights
>     695             trainable_weights = collect_trainable_weights(self)
> --> 696             training_updates = self.optimizer.get_updates(trainable_weights, self.constraints, self.total_loss)
>     697             updates = self.updates + training_updates
>     698 
> 
> N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\optimizers.pyc in get_updates(self, params, constraints, loss)
>     119 
>     120     def get_updates(self, params, constraints, loss):
> --> 121         grads = self.get_gradients(loss, params)
>     122         lr = self.lr \* (1. / (1. + self.decay \* self.iterations))
>     123         self.updates = [K.update_add(self.iterations, 1)]
> 
> N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\optimizers.pyc in get_gradients(self, loss, params)
>      51 
>      52     def get_gradients(self, loss, params):
> ---> 53         grads = K.gradients(loss, params)
>      54         if hasattr(self, 'clipnorm') and self.clipnorm > 0:
>      55             norm = K.sqrt(sum([K.sum(K.square(g)) for g in grads]))
> 
> N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\backend\theano_backend.pyc in gradients(loss, variables)
>     655 
>     656 def gradients(loss, variables):
> --> 657     return T.grad(loss, variables)
>     658 
>     659 
> 
> N:\Programs\Anaconda\lib\site-packages\theano-0.9.0.dev2-py2.7.egg\theano\gradient.pyc in grad(cost, wrt, consider_constant, disconnected_inputs, add_names, known_grads, return_disconnected, null_gradients)
>     531         if elem not in var_to_app_to_idx and elem is not cost \
>     532                 and elem not in grad_dict:
> --> 533             handle_disconnected(elem)
>     534             grad_dict[elem] = disconnected_type()
>     535 
> 
> N:\Programs\Anaconda\lib\site-packages\theano-0.9.0.dev2-py2.7.egg\theano\gradient.pyc in handle_disconnected(var)
>     518             elif disconnected_inputs == 'raise':
>     519                 message = utils.get_variable_trace_string(var)
> --> 520                 raise DisconnectedInputError(message)
>     521             else:
>     522                 raise ValueError(""Invalid value for keyword ""
> 
> DisconnectedInputError:  
> Backtrace when that variable is created:
> 
>   File ""kerasmodelzoo\vgg16.py"", line 53, in model
>     vgg16_model.add(Dense(1000, activation='softmax'))
>   File ""N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\models.py"", line 146, in add
>     output_tensor = layer(self.outputs[0])
>   File ""N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\engine\topology.py"", line 458, in **call**
>     self.build(input_shapes[0])
>   File ""N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\layers\core.py"", line 604, in build
>     name='{}_W'.format(self.name))
>   File ""N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\initializations.py"", line 59, in glorot_uniform
>     return uniform(shape, s, name=name)
>   File ""N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\initializations.py"", line 32, in uniform
>     return K.random_uniform_variable(shape, -scale, scale, name=name)
>   File ""N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\backend\theano_backend.py"", line 111, in random_uniform_variable
>     dtype=dtype, name=name)
>   File ""N:\Programs\Anaconda\lib\site-packages\keras-1.0.6-py2.7.egg\keras\backend\theano_backend.py"", line 40, in variable
>     return theano.shared(value=value, name=name, strict=False)
",,"[""After trying a different solution for my problem, that is an RNN instead of merging the layer:\n\n```\n\nrnn = Sequential()\nrnn.add(TimeDistributed(get_vgg_model(),input_shape=(10,3,224,224)))\nprint rnn.layers[0].input_shape\nprint rnn.layers[-1].output_shape\nrnn.add(LSTM(10,activation='tanh'))\nrnn.add(Dense(1,activation='sigmoid'))\nrnn.compile(optimizer='sgd',\n              loss='mean_squared_error',\n              metrics=['accuracy'])\nexamples = 20\n\nx_train = np.random.rand(examples,10,3,224,224)\ny_train = np.random.randint(2, size=examples)\nrnn.fit(x_train,y_train)\n```\n\nI get the same exact error!\n\nSo this seems to be something general with the fact that I am popping out 2 layers and adding more on top.\n"", ""I found a way to train my model.\nSo what I do is I pop the last layers from the vgg model **after** connecting it to the next layer:\n\n```\nvgg_model = vgg16.model(weights=True,summary=False)\nrnn = Sequential()\nrnn.add(TimeDistributed(vgg_model, input_shape=(10, 3, 224, 224)))\nvgg_model.pop()\nvgg_model.pop()\nrnn.add(LSTM(10, activation='tanh'))\nrnn.add(Dense(1, activation='sigmoid'))\nrnn.compile(optimizer='sgd',\n            loss='mean_squared_error',\n            metrics=['accuracy'])\nrnn.fit(x_train,y_train, validation_split=0.16)\nprint rnn.evaluate(x_test,y_test)\n```\n\nI still believe this is an issue but if anyone else has it, you might be able to use this trick.\n"", 'Exactly, and if you `print rnn.summary()`, it will output one Model (for vgg_model) and 2 layers (LSTM and Dense). I am expecting that it will output all layers (including layers in vgg_model).\n', ""I'm seeing the same error - popping layers from the top of VGG16 will cause DisconnectedInputError for the original final layer of VGG16, even though these layers do not show up when printing model.summary().\n"", 'Actually, scratch that - `.pop()` works for me with the current Keras. The issue was that the code used an implementation preceding the builtin `.pop()` method which attempted to do the same thing, but did not properly reset the `model._flattened_layers` cache, therefore that list still contained the old layers. Changing the code to use the `.pop()` method instead fixed the issue.  Perhaps this report still helps someone migrating their code from older Keras versions.\n', 'Ok I tested it and `.pop()` seems to be working properly with the current version.\nThanks @pasky \n', ""@pasky \r\nI wanna use this pretrained vgg19.  \r\n\r\n>  x = Flatten(name='flatten')(x)\r\n        x = Dense(4096, activation='relu', name='fc1')(x)\r\n        x = Dense(4096, activation='relu', name='fc2')(x)\r\n        x = Dense(1000, activation='softmax', name='predictions')(x)\r\n model = Model(img_input, x)\r\n    return model\r\n\r\n i have only 8 labels to classlify. \r\n But it is used by the keras function API ,not the `Sequential` model.\r\n how can i use the `model.layers.pop()`\r\nlike this\r\n\r\n>  x = Dense(1000, activation='softmax', name='predictions')(x)\r\nmodel = Model(img_input, x)\r\nmodel.layers.pop()\r\n x = Dense(8, activation='softmax', name='predictions')(x)\r\n loading weight\r\nreturn model\r\n\r\nDo you give me some advices? Thanks""]",[],[],0,0
206,keras,4239,closed,Shape invariance error using `TimeDistributed` layers with TensorFlow,"The following snippet:

    from keras.layers import Input, LSTM, Dense
    from keras.layers.wrappers import TimeDistributed
    from keras.models import Model

    batch_size = 32
    time_steps = 10
    input_dim = 5
    num_classes = 3
    hidden_size = 16
    inputs = Input(batch_shape=(batch_size, time_steps, input_dim))
    y = LSTM(hidden_size, return_sequences=True)(inputs)
    y = TimeDistributed(Dense(num_classes, activation='softmax'))(y)  # Exception raised here
    model = Model(input=inputs, output=y)
    model.compile(optimizer='rmsprop')

Raises the following exception in the indicated line when using TensorFlow backend:

    ValueError: The shape for while_1/Merge_2:0 is not an invariant for the loop. It enters
    the loop with shape (10, 16), but has shape (32, 3) after one iteration. Provide shape
    invariants using either the  argument of tf.while_loop or set_shape()
    on the loop variables.

This seems to be a compatibility issue with TensorFlow 0.11:
* TensorFlow 0.10rc0 + Keras 1.1.0: Works
* TensorFlow 0.11rc0 + Keras 1.1.0: Fails
* TensorFlow 0.11rc1 + Keras 1.1.0: Fails

I have tested this with Python 3 on Debian Jessie and Ubuntu 14.04.5 LTS.",,['This does not seem to be happening anymore with Keras 1.1.2.'],[],['shape_invariants'],0,0
207,keras,1606,closed,Forward pass with dropout,"I'm looking for a way to do something that seems conceptually simple, but I haven't found a way to do it in Keras yet. I have a CNN with dropout applied in various places in the network. After training, I'd like to compute the forward pass on some data and have the dropout mask applied such that each time I compute the forward pass a different dropout mask is applied. 

If I understand things correctly, this is what is done during training. However, if I use the standard model.predict(X_test) after training, no dropout is applied and the outputs are multiplied by the dropout probability, yielding a non-stochastic set of predictions. 

Does anyone know how I can achieve this? 
",,"['Permanent dropout:\n\n``` python\nfrom keras.layers.core import Lambda\nfrom keras import backend as K\n\nmodel.add(Lambda(lambda x: K.dropout(x, level=0.5)))\n```\n', ""There is no elegant way of doing this with a 'normally' defined  model that at test time can be evaluated with and without the dropout mask ?  \n"", ""The simplest thing you can do is use your own dropout layer. Look at the\ncode for the existing dropout layer. Simply remove the part that makes\ndropout conditional on the learning phase.\n\nOn 12 June 2016 at 19:44, Christopher Bonnett notifications@github.com\nwrote:\n\n> There is no elegant way of doing this with a 'normally' defined model that\n> at test time can be evaluated with and without the dropout mask ?\n> \n> —\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> https://github.com/fchollet/keras/issues/1606#issuecomment-225479014,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AArWb0Sj_JsBRKt95J3vDQxkZzlWlMQjks5qLMQegaJpZM4HQGbd\n> .\n"", '@fchollet How do we generalize this answer to other forms of dropout, such as those in Embedding and RNN layers?\n', '```\r\nclass PermanentDropout(Dropout):\r\n    def __init__(self, rate, **kwargs):\r\n        super(PermanentDropout, self).__init__(rate, **kwargs)\r\n        self.uses_learning_phase = False\r\n\r\n    def call(self, x, mask=None):\r\n        if 0. < self.rate < 1.:\r\n            noise_shape = self._get_noise_shape(x)\r\n            x = K.dropout(x, self.rate, noise_shape)\r\n        return x\r\n```', '@fchollet I am interested in making a dropout layer that is static throughout the course of training and testing. Unlike normal dropout, I only want to sever a certain amount of random weights, not the entire node. Is creating a custom layer the easiest way to achieve this? \r\n\r\nHere is my current [progress](https://github.com/jordanott/No-Weight-Sharing/blob/master/CustomLayers/Biology.py). Any suggestions would help.', '@fchollet is there a way to do the same with spatial dropout?', 'Based on issue #9412, it appears that this ""permanent dropout"" feature has been (quietly) added into core Keras.\r\n\r\nPer @fchollet:\r\n\r\n> There is this feature in Keras: it\'s the training argument in the call of the Dropout layer.\r\n> \r\n> Here\'s a model with a Dense layer and a Dropout layer that runs both in training and testing:\r\n> \r\n> ```\r\n> import keras\r\n> \r\n> inputs = keras.Input(shape=(10,))\r\n> x = keras.layers.Dense(3)(inputs)\r\n> outputs = keras.layers.Dropout(0.5)(x, training=True)\r\n> \r\n> model = keras.Model(inputs, outputs)\r\n> ```\r\n\r\nOddly, the `training` argument doesn\'t currently appeared to be documented on [https://keras.io/layers/core/](https://keras.io/layers/core/), but you can see it in line 118 of the [source](https://github.com/keras-team/keras/blob/master/keras/layers/core.py#L80). ', 'For TF>=2 https://stackoverflow.com/a/56085099/10375049', 'How can I pass dropout rate during predict in Keras?\r\nfor example:\r\ndropout_rate = 0.1\r\nprediction = model.predict(x_test, dropout_rate)', ""> You can also change the dropout with a trained model:\r\n> \r\n> `f = K.function([model.layers[0].input, K.learning_phase()], [model.layers[-1].output])`\r\n> \r\n> In this way you don't have to train again the model!!!\r\n\r\nThis is throwing error.""]",[],[],0,0
208,keras,1434,closed,lstm_text_generation.py doesnt work,"when i try to run the code i my output is a pile of c++ code(1009 lines) in its first lines it says ""support code"" besides that i get another long error about g++ and gcc this is the error log(im using windows and i run the code in powershell) : 
[log.txt](https://github.com/fchollet/keras/files/83558/log.txt)

thank you.
",stale,[],[],[],0,0
209,keras,1907,closed,pad_sentences,"Hi,
I'm wondering where i can find an explanation of why we need to apply pad_sentences to text for the embedding layer and why the 2D transformation ? Thanks!
",,"[""The input to an Embedding layer is a 2D tensor of integers. The first dimension is the batch size, the second dimension is the sequence length. So that's why all your sequences have to have the same size, and the simplest way to achieve same-length sequences is to pad the shorter ones with zeros.\n"", 'Thanks for this straightforward explanation !\n']",[],[],0,0
210,keras,4919,closed,saving/loading models with frozen layers still not working with 1.2.0,"This issue has been raised before in #953, I believe the fix c4f3155d192935c7cb659cac6d38c76b15ec971e did not solve the issue. 
Here is a code to reproduce:


",,"['Cannot reproduce. Check that your version of Keras is the latest version\nfrom Github.\n\nOn 5 January 2017 at 01:30, timpx <notifications@github.com> wrote:\n\n> This issue has been raised before in #953\n> <https://github.com/fchollet/keras/issues/953>, I believe the fix c4f3155\n> <https://github.com/fchollet/keras/commit/c4f3155d192935c7cb659cac6d38c76b15ec971e>\n> did not solve the issue.\n> Here is a code to reproduce:\n>\n> from keras.models import Sequentialfrom keras.layers import Dense\n>\n> model = Sequential()\n> model.add(Dense(output_dim=64, input_dim=100))\n> model.compile(loss=\'categorical_crossentropy\', optimizer=\'sgd\', metrics=[\'accuracy\'])\n> model.save_weights(\'test\')\n>\n> model2 = Sequential()\n> model2.add(Dense(output_dim=64, input_dim=100, trainable=False))\n> model2.load_weights(\'test\') # get a ValueError ""You are trying to load a weight file containing 1 layers into a model with 0 layers.\n>\n> —\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/fchollet/keras/issues/4919>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AArWbyTNWLzn0phvl3QZsr941sTCy6GPks5rPDmZgaJpZM4LbOGX>\n> .\n>\n', ""ok thanks and sorry for that, it's indeed working with the latest github version.""]","['python\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense \r\n\r\nmodel = Sequential()\r\nmodel.add(Dense(output_dim=64, input_dim=100))\r\nmodel.compile(loss=\'categorical_crossentropy\', optimizer=\'sgd\', metrics=[\'accuracy\'])\r\nmodel.save_weights(\'test\')\r\n\r\nmodel2 = Sequential()\r\nmodel2.add(Dense(output_dim=64, input_dim=100, trainable=False))\r\nmodel2.load_weights(\'test\') # get a ValueError ""You are trying to load a weight file containing 1 layers into a model with 0 layers.\r\n']",[],0,0
211,keras,6631,closed,EntropySGD: gradient updates for Langevin dynamics,"Hi, 

I've been attempting to implement the EntropySGD in this paper (https://arxiv.org/pdf/1611.01838.pdf). However the inner loop of the optimizer requires L gradient updates to estimate the exponential decayed mus which are used to update the parameters on one batch of data. It looks like  'get_updates' is called from training.py. My question is:

1. Is there a way to update the network params, compute a new loss and get new gradients on the same batch of data in the optimizer class to run the L iterations for the Langevin dynamics?
2. If not, can this be implemented in other ways or can this be added as a future feature?

Thanks",,['Being implemented https://github.com/fchollet/keras/issues/6563'],[],[],0,0
212,keras,4857,closed,Implement general callback to perform an action after some epochs of no improvement on a monitored metric,"Currently, the callbacks  and  perform the same general function of carrying out some action after some number of epochs of no improvement on a monitored metric. These classes could be rewritten as subclasses of a more general class that monitors a quantity of interest and calls an  function once this quantity ceases to improve. This would make it easy to implement other callbacks that are dependent on training progress in the future.",stale,[],[],"['EarlyStopping', 'ReduceLROnPlateau', 'action()']",0,0
213,keras,6860,closed,Training LSTM with more .csv files,"Hi, I have more csv files, where each file contains a time-series for my forecasting problem.
The problem is thet whenever I fit the csv file for each file, the precedent fit will be,obviously, forget.
There is a method for don't forget it? I insert an image with my problem.
![problema](https://cloud.githubusercontent.com/assets/18617527/26780123/205af932-49e0-11e7-8e9b-e634c86f77c3.jpg)
My idea is using the LSTM, and for the testing I want to insert one single input for time, and predict the value",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],0,0
214,keras,379,closed,"merge_mode=""concat"" behaving unexpectedly? (with the GPU backend)","I'm building a network using the new Graph model which takes two sequences as inputs, feeds them through an RNN, concatenates the final output vectors, and passes that on to a dense layer. I expected that the input size of dense layer should be twice the output dimensionality of the RNNs but this doesn't seem to be working. 

Does anyone know what I may be doing wrong? 



The error that I get is:



It seems like the two RNN outputs are not actually getting merged into a 10 dimensional vector, is that expected?

_edit_: This error goes away (and the model trains successfully) when using the Theano CPU backend. 

_edit 2.0_: The error goes away even on the GPU if I switch the JZS1 RNN to an LSTM. 

_edit 3.0_ Actually, the error persists with LSTMs but only if I'm using floatX=32. Current hypothesis: merging of input variables doesn't get handled correctly when calling an external matrix multiply routine. 
",,"[""This is an old Theano bug. Just upgrade to the latest version of Theano (get it from Github) and you'll be all set.\n"", 'thanks \n']","['\ngraph = Graph()\ngraph.add_input(name=\'peptide\', ndim=3)\ngraph.add_input(name=\'mhc\', ndim=3)\n\n# RNN for peptide sequences\ngraph.add_node(JZS1(input_dim=20, output_dim=5), name=""peptide_rnn"", input=""peptide"")\n\n# RNN for MHC sequences\ngraph.add_node(JZS1(input_dim=20, output_dim=5), name=""mhc_rnn"", input=""mhc"")\n\n# concatenate last output of both RNNs and transform them into a lower dimensional space\ngraph.add_node(Dense(5 * 2, 16, activation=""relu""), name=""hidden"", merge_mode=""concat"", inputs=(""peptide_rnn"", ""mhc_rnn""))\n\n# output prediction of affinity\ngraph.add_node(Dense(16, 1, activation=""sigmoid""), name=""affinity"", input=""hidden"")\n\n# weird that I have to name the output twice\ngraph.add_output(name=\'affinity_output\', input=\'affinity\')\ngraph.compile(\'rmsprop\', {\'affinity_output\':\'mse\'})\n', '\nTraceback (most recent call last):\n  File ""train-jsz-rnn.py"", line 138, in <module>\n    history = graph.fit({\'peptide\':padded_peptides, \'mhc\':padded_mhc, \'affinity_output\':log_target_values}, nb_epoch=1)\n  File ""/home/ubuntu/keras/keras/models.py"", line 537, in fit\n    validation_split=validation_split, val_f=val_f, val_ins=val_ins, shuffle=shuffle, metrics=metrics)\n  File ""/home/ubuntu/keras/keras/models.py"", line 135, in _fit\n    outs = f(*ins_batch)\n  File ""/home/ubuntu/anaconda/lib/python2.7/site-packages/theano/compile/function_module.py"", line 588, in __call__\n    self.fn.thunks[self.fn.position_of_error])\n  File ""/home/ubuntu/anaconda/lib/python2.7/site-packages/theano/compile/function_module.py"", line 579, in __call__\n    outputs = self.fn()\nValueError: dimension mismatch in args to gemm (128,5)x(10,16)->(128,16)\nApply node that caused the error: GpuDot22(GpuJoin.0, hidden_W)\nInputs shapes: [(128, 5), (10, 16)]\nInputs strides: [(5, 1), (16, 1)]\nInputs types: [CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix)]\n']",[],0,0
215,keras,9116,closed,Multiple Inputs with Multiple Embeddings (Not working),"I am trying a simple model, which  consists of two separate inputs (each with two different embedded inputs). The problem is a toy scenario which forms the framework of a more complex system i need to create with multiple inputs and multple embeddings, however i need to get this basic system working. I have no problem using multiple embeddings in the one input case, it is only when i try using multiple inputs that i get the follow error:

> __main__:1: UserWarning: The  function is deprecated and will be removed after 08/2017. Use instead layers from , e.g. , , etc.
> /services/tools/anaconda3/4.0.0/lib/python3.5/site-packages/keras/legacy/layers.py:460: UserWarning: The  layer is deprecated and will be removed after 08/2017. Use instead layers from , e.g. , , etc.
>   name=name)
> Traceback (most recent call last):
> 
>   File ""<ipython-input-676-0f3f0da1e7a7>"", line 3, in <module>
>     model = Model(inputs=[[q1 ,q2],[q3 , q4]], outputs=MC_MergeOut)
> 
>   File ""/services/tools/anaconda3/4.0.0/lib/python3.5/site-packages/keras/legacy/interfaces.py"", line 88, in wrapper
>     return func(*args, **kwargs)
> 
>   File ""/services/tools/anaconda3/4.0.0/lib/python3.5/site-packages/keras/engine/topology.py"", line 1485, in __init__
>     inputs_set = set(self.inputs)
> 
> TypeError: unhashable type: 'list'

Has anyone tried multi-input and multiple embeddings for each input? Any help would be greatly appreciated as i'm on a time deadline. Thanks in advance

",,"['Hi @RA19,\r\n\r\nI am having the same issue, when merging 2 models of multiple inputs. May I ask how did you solve it?', '+1 What am I doing wrong?', ""I believe the problem is your inputs should not be a list of lists. You're treating the inputs as 2x2 but really you are just feeding in 4 inputs. so if you change inputs to:\r\ninputs=[q1,q2,q3,q4]\r\nand adjust your input matrix to reflect that it should work.""]","[""\r\n# Embedding Input1\r\nname='ques1'\r\nq1 = Input(shape=(1,), dtype='int32',name=name+'_in')\r\nq1_out = Embedding(input_dim=32, output_dim=10, input_length=1)(q1)\r\n# Embedding Input2\r\nname='ques2'\r\nq2  = Input(shape=(1,), dtype='int32',name=name+'_in')\r\nq2_out  = Embedding(input_dim=64, output_dim=10, input_length=1)(q2)\r\n# MODEL A\r\nA = merge([q1_out ,q2_out ], mode='concat',concat_axis=1)\r\nA = Dropout(0.05)(A)\r\nA = Dense(32,activation='relu', init='uniform')(A)\r\n\r\n# Embedding Input2\r\nname='ques3'\r\nq3 = Input(shape=(1,), dtype='int32',name=name+'_in')\r\nq3_out = Embedding(input_dim=48, output_dim=10, input_length=1)(q3)\r\n# Embedding Input3\r\nname='ques4'\r\nq4  = Input(shape=(1,), dtype='int32',name=name+'_in')\r\nq4_out  = Embedding(input_dim=12, output_dim=10, input_length=1)(q4)\r\n# MODEL B\r\nB  = merge([q3_out ,q4_out ], mode='concat',concat_axis=1)\r\nB  = Dropout(0.05)(B)\r\nB  = Dense(32,activation='relu', init='uniform')(B )\r\n\r\n\r\nMC_MergeOut = merge([A, B], mode='cos', dot_axes=1) # dot_axes\r\n\r\nmodel = Model(inputs=[[q1 ,q2],[q3 , q4]], outputs=MC_MergeOut)\r\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy')\r\n""]","['merge', 'keras.layers.merge', 'add', 'concatenate', 'Merge', 'keras.layers.merge', 'add', 'concatenate']",0,0
216,keras,1348,closed,Input shape error with ConvNet 1D,"Hi everyone,

I have a dataset with 24 inputs, and 1 output, which is categorical value.

So my X_train.shape is _(20000,24)_ and X_test.shape is _(5000,24)_ and the output should be only one 1 six values: a, b, c, d, e, f. So the Y_train.shape is _(20000,6)_.

Adapting from https://github.com/fchollet/keras/issues/579



but I met the problem



at line



Could you help to point out what I did wrong?

Thanks,
",stale,"['Hello,\n\nthe first argument for the `Dense` layer specifies the dimension of the output. The dimension of the input is inferred automatically, so try to replace\n\n```\nmodel.add(Dense(output_size, hidden_dims))\n```\n\nwith \n\n```\nmodel.add(Dense(hidden_dims))\n```\n\nand \n\n```\nmodel.add(Dense(hidden_dims, nb_classes))\n```\n\nwith \n\n```\nmodel.add(Dense(nb_classes))\n```\n', 'Thanks @jgc128 , but I have exactly the same error after modifying the code as suggested.\n', 'Hi, \n\nAfter changing the Convolution1D to (as said, I have 20000 train examples, and 5000 test examples):\n\n``` python\nmodel.add(Convolution1D(input_dim=embedding_dims,\n            nb_filter=nb_filters,\n            filter_length=filter_length,\n            border_mode=""valid"",\n            activation=""relu"",\n            subsample_length=1,\n            input_length=20000))\n```\n\nNow I can build the model, but when I do\n\n``` python\nmodel.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,show_accuracy=True, validation_data=(X_test, Y_test))\n```\n\nI have an error\n\n``` bash\nTypeError: (\'Bad input argument to theano function with name ""/Users/my_name/anaconda/lib/python2.7/site-packages/keras/backend/theano_backend.py:354""  at index 0(0-based)\', \'Wrong number of dimensions: expected 3, got 2 with shape (32, 24).\')\n```\n', 'From the doc for [convolution1d](http://keras.io/layers/convolutional/#convolution1d), I think your input should be a 3D tensor. Try with `model.fit(X_train[:,None,:], ...`\n', ""Couldn't you just apply a reshape?\nnewshape = (<# samples>, 1, <#dim of embedding>)\nX_train = np.reshape(train_vectors, newshape)\n"", 'Hi, @jgc128 . \nI met the same question as you described in 1D\nHow did you solve it?\n', 'Hi @breadada \n\nAre you sure you have the latest versions of Keras and Theano from GitHub?\n', ""Hi, @jgc128 \nyes, I update it this afternoon, it's 0.3.1 by pip update.\nI met several erros in my debug this evening, I will tell you tomorrow, it's 0:10 now.\ngood night:)\n"", 'Good night!\n', 'Hi, @jgc128\nNow my code can run without MaxPooling1D. Maybe this error is the last one.\nSee the details at #1457 \nMy data is (623, 24), label is 0/1\n', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']","[' python\nbatch_size = 32\nembedding_dims = 24\nnb_filters = 250\nfilter_length = 3\nhidden_dims = 250\nnb_epoch = 10\ntest_split=0.2\nseed=113\nnb_words=None\nskip_top=0\nmaxlen=200\nstart_char=1 \noov_char=2\nindex_from=3\nnb_classes = 6\n\nY_train = np_utils.to_categorical(Y_train, nb_classes)\nY_test = np_utils.to_categorical(Y_test, nb_classes)\n\n\nprint(\'Build model...\')\nmodel = Sequential()\n\n# we start off with an efficient embedding layer which maps\n# our vocab indices into embedding_dims dimensions\n#model.add(Embedding(max_features, embedding_dims))\n#model.add(Dropout(0.25))\n\n# we add a Convolution1D, which will learn nb_filters\n# word group filters of size filter_length:\nmodel.add(Convolution1D(input_dim=embedding_dims,\n            nb_filter=nb_filters,\n            filter_length=filter_length,\n            border_mode=""valid"",\n            activation=""relu"",\n            subsample_length=1))\n\n# we use standard max pooling (halving the output of the previous layer):\nmodel.add(MaxPooling1D(pool_length=2))\n\n# We flatten the output of the conv layer, so that we can add a vanilla dense layer:\nmodel.add(Flatten())\n\n# Computing the output shape of a conv layer can be tricky;\n# for a good tutorial, see: http://cs231n.github.io/convolutional-networks/\noutput_size = nb_filters * (((maxlen - filter_length) / 1) + 1) / 2\n\n# We add a vanilla hidden layer:\nmodel.add(Dense(output_size, hidden_dims))\nmodel.add(Dropout(0.25))\nmodel.add(Activation(\'relu\'))\n\n# We project onto a single unit output layer, and squash it with a sigmoid:\nmodel.add(Dense(hidden_dims, nb_classes))\nmodel.add(Activation(\'softmax\'))\n\n# model.add(Dense(128, 1, init=\'normal\'))\n# model.add(Activation(\'relu\'))\n\n# sgd = SGD(l2=0.0,lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n# model.compile(loss=\'categorical_crossentropy\', optimizer=sgd, class_mode=""categorical"")\n# model.compile(loss=\'categorical_crossentropy\', optimizer=\'sgd\', class_mode=""categorical"")\nmodel.compile(loss=\'categorical_crossentropy\', optimizer=\'rmsprop\')\nmodel.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,show_accuracy=True, validation_data=(X_test, Y_test))\nscore, acc = model.evaluate(X_test, Y_test, batch_size=batch_size, show_accuracy=True)\nprint(\'Test score:\', score)\nprint(\'Test accuracy:\', acc)\n', "" bash\nTypeError: unsupported operand type(s) for *: 'NoneType' and 'int'\n"", ' python\nmodel.add(Dense(output_size, hidden_dims))\n']",[],0,0
217,keras,1130,closed,Type error when stateful,"After the fix for issue  #1125, I'm trying



but I get this error



This is the traceback:



Theano is up-to-date. Am I missing anything?
",,"['Does the tensorflow backend work?\n', ""I can't test TensorFlow at the moment.\n"", 'Should be fixed now. Will require more unit tests. Let me know how it goes.\n', '@fchollet I just updated (as of a few minutes ago) and am still getting errors \n\n``` python\nmodel = Sequential()\nmodel.add(Embedding(tokens, 128))\nmodel.add(LSTM(1024, return_sequences=True, stateful=True, batch_input_shape=(batch_size,max_len-1, 128) ))\nmodel.add(Dropout(0.5))\nmodel.add(LSTM(1024, return_sequences=True, stateful=True, batch_input_shape=(batch_size,max_len-1, 1024) ))\nmodel.add(Dropout(0.5))\nmodel.add(LSTM(1024, return_sequences=True, stateful=True, batch_input_shape=(batch_size,max_len-1, 1024) ))\nmodel.add(Dropout(0.5))\nmodel.add(TimeDistributedDense(tokens))\nmodel.add(Activation(\'softmax\'))\n\nmodel.compile(loss=\'categorical_crossentropy\', optimizer=Adam(clipnorm=20))\n```\n\nGives the error \n\n```\n---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\n<ipython-input-8-893d09bc3dcd> in <module>()\n      1 model = Sequential()\n      2 model.add(Embedding(tokens, 128))\n----> 3 model.add(LSTM(1024, return_sequences=True, stateful=True, batch_input_shape=(batch_size,max_len-1, 128) ))\n      4 model.add(Dropout(0.5))\n      5 model.add(LSTM(1024, return_sequences=True, stateful=True, batch_input_shape=(batch_size,max_len-1, 1024) ))\n\n/usr/local/lib/python2.7/dist-packages/Keras-0.3.0-py2.7.egg/keras/layers/containers.pyc in add(self, layer)\n     30         self.layers.append(layer)\n     31         if len(self.layers) > 1:\n---> 32             self.layers[-1].set_previous(self.layers[-2])\n     33             if not hasattr(self.layers[0], \'input\'):\n     34                 self.set_input()\n\n/usr/local/lib/python2.7/dist-packages/Keras-0.3.0-py2.7.egg/keras/layers/core.pyc in set_previous(self, layer, connection_map)\n     42             assert self.supports_masked_input(), ""Cannot connect non-masking layer to layer with masked output""\n     43         self.previous = layer\n---> 44         self.build()\n     45 \n     46     def build(self):\n\n/usr/local/lib/python2.7/dist-packages/Keras-0.3.0-py2.7.egg/keras/layers/recurrent.pyc in build(self)\n    295                 raise Exception(\'If a RNN is stateful, a complete \' +\n    296                                 \'input_shape must be provided \' +\n--> 297                                 \'(including batch size).\')\n    298             self.states = [K.zeros((input_shape[0], self.output_dim)),\n    299                            K.zeros((input_shape[0], self.output_dim))]\n\nException: If a RNN is stateful, a complete input_shape must be provided (including batch size).\n```\n\nThis is the same problem I\'ve blocked on in issue #98  \n', ""@fchollet \nThe issue that I reported is fixed for me, so I'll close this issue. However there's another issue, so I'm going to open a new bug report.\n""]","['\nin_out_neurons = 1\nhidden_neurons = 10\nbatch_size = 10\n\nmodel = Sequential()\nmodel.add(GRU(hidden_neurons, batch_input_shape=(batch_size, 5, in_out_neurons), return_sequences=True, stateful=True))\n', '\nTypeError: data type not understood\n', '\nmodel.add(GRU(hidden_neurons, batch_input_shape=(batch_size, 5, in_out_neurons), return_sequences=True, stateful=True))\n\nrecurrent.py"", line 190, in __init__\nsuper(GRU, self).__init__(**kwargs)\n\nrecurrent.py"", line 25, in __init__\nsuper(Recurrent, self).__init__(**kwargs)\n\ncore.py"", line 30, in __init__\nself.set_input_shape(tuple(kwargs[\'batch_input_shape\']))\n\ncore.py"", line 95, in set_input_shape\nself.build()\n\nrecurrent.py"", line 219, in build\nself.states = [K.zeros(input_shape[0], self.output_dim)]\n\ntheano_backend.py"", line 80, in zeros\nreturn variable(np.zeros(shape), dtype, name)\n\ntheano_backend.py"", line 33, in variable\nvalue = np.asarray(value, dtype=dtype)\n\nnumeric.py"", line 474, in asarray\nreturn array(a, dtype, copy=False, order=order)\n']",[],0,0
218,keras,1587,closed,Out of memory issues (GPU),"I'm training a model with Theano/CUDA, and if I attempt to specify a large batch_size (1024 in my case), it reports an out of memory error, which is understandable. However, if I change it back to a size that previously worked (I'm doing it in a notebook), it will still be out of memory, as if it didn't attempt to free whatever it allocated for the previous attempt, so I'm forced to restart the Python process (and reload all data/recompile models).

I can provide model code if needed, its on a laptop that does not have internet access currently.
",stale,"[""It sounds like a problem with IPython Notebook, maybe it doesn't release allocated resources (memory). It happened to me before.\nI suggest you develop your model in a text editor or IDE, and run it through console.\n"", ""It is possible that cuda driver don't like this and stay in a bad state. It\nisn't able to recover from all type of error. I'm not sure which one it\nrecover correctly and which one it don't.\n\nCalling a_theano_function.free() could help free the memory, but as it was\nsaid, running it outside ipython notebook would fix those problems.\n\nOn Fri, Jan 29, 2016 at 7:17 AM, Marko Jocić notifications@github.com\nwrote:\n\n> It sounds like a problem with IPython Notebook, maybe it doesn't release\n> allocated resources (memory). It happened to me before.\n> I suggest you develop your model in a text editor or IDE, and run it\n> through console.\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/fchollet/keras/issues/1587#issuecomment-176726753.\n"", ""If I run it as a complete python program then it obviously wouldn't be a problem because the exception would terminate the python process, but that defeats the purpose of being able to keep training the model and checking the results without having to recompile it every time.\n\nI'm pretty sure running it in python shell would be the same as using the notebook, though.\n\nIt seems that it starts allocating large amounts of memory, but when it runs out it throws an exception and doesn't free the memory. I don't know if forcing garbage collection would help, but that theano free function looks like it would help, thanks.\n"", ""Hitting this as well. I've been playing with a trivial model in jupyter and I've observed that while iterating in jupyter my GPU used memory (as reported by nvidia-settings) steadily increases in ~200mb increments until finally theano generates an out of memory error.\n\nCalling `del model`, `gc.collect()` seems to have no effect on used GPU memory. Restarting the jupyter kernel resolves it, but is undesirable.\n"", 'Has anyone managed to find a way around this (i.e. without restarting the kernel) ?', '@MannyKayy deleting all IPython references to the model and calling the garbage collector works for me (using theano):\r\n```\r\n%xdel model\r\nimport gc\r\nfor i in range(3): gc.collect()\r\n```', ""having this issue myself with tensorflow, above^ method doesn't solve issue in notebook."", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', ""still no progress on this? I'm having the same issue...using tf and keras"", ""I have the same issue. Can't find any solution except to restart the notebook."", '+1. Struggling with this a lot.', 'Same for me. It would be great to not restart the notebook every time', 'Same here. Any solution better than restart the kernel of the notebook?', 'Why no one can solve this serious problem? My method is to shutdown the jupyter kernal everytime, too waste time.']",[],[],0,0
219,keras,1723,closed,"Keras for many number of classes, e.g., language modelling with vocabulary size 50,000.","Hi all,

First of all thanks for this wonderful framework and great work you guys have done.

I have been trying to use Keras to build a simple LSTM language model with a vocabulary of size 50,000. Considering one-hot representation of words, this translates into 50,000 classes. 
To avoid padding, I have been dividing my training data (~80 million sentences) into chunks where each chunk includes sentences of the same length. Model compiles well and training loss is decreasing but I have two main problems:

1) Using ""  "" function results in huge memory (and computation?) waste. I tried """" to avoid it but got a rank mismatch error from Theano. Is there any work around for this? 

2) I guess when the number of classes is huge the running time becomes very high. Is there any solution for this?
E.g., in my language model experiment, with just one LSTM layer and a Softmax layer on the top, 512 cells for LSTM, maximum sentence length of 15, vocabulary size 50,000 and mini-batch size = 1024 sentences, it takes 4 sec / mini-batch on GPU, which is too long. 
It also consumes up to 7 GB of GPU memory which I believe is because of  ""  "" function.

Thanks!
Hamid
",stale,"[""I think the traditional approach to this is to use hierarchical softmax. Don't believe this is implemented in Keras (yet?).\n"", '@neggert : I thought that it is the hierarchical softmax by default. I just found out there is Advanced Activation Layers in Keras but it does not include hierarchical softmax. Updated my Keras installation and checked the layers, was not there as well. Guess it is not implemented yet ...\n', '@xiaodonghe\n@zer0n\n', 'Someone has already implemented hierarchial softmax Theano, so it should just work with Keras.\n\nhttps://github.com/RobGrimm/hierarchical_softmax/blob/master/HierarchicalSoftmax.py\n', '@KeironO Could you elaborate on how to use this with Keras?  Do you just drop it in with a `Lambda` layer?\n', 'Yes.\n', 'There are weights inside HSM, Lambda layers are for stateless functions.\n', 'I will be rolling out a very tightly optimised softmax layer this weekend. So hang on.\n', '@farizrahman4u any news on the hierarchical softmax?\n', 'github.com/farizrahman4u/huffmax\n']",[],"['np_utils.to_categorical ()', ""K.mean(K.categorical_crossentropy(y_pred, K.cast(y_true.flatten(), dtype='int32')), axis=-1)"", 'np_utils.to_categorical ()']",0,0
220,keras,7927,closed,CNN to manage sequences of grayscale images like a video files,"Hello,

I'm in trouble because I'm not able to manage correctly the CNN and maybe the LSTM-RNN. 

I'm trying to solve the following problem. I've 20000 chunks of data that each of them are composed by a sequence of 20 images. The size of each image is 40x40. The images are in grayscale mode. For each chunk of data, we have two possible labels, 0 or 1. We have to classify each chunk of data in two possible sets: 0 or 1. As they are numpy arrays, the shape of each one is:
- Data shape: (20000, 20, 40, 40)
- Labels shape: (20000, 1)

Well, I want to train a CNN to predict if the label of a chunk of data is 0 or 1. Remember that each chunk of data is composed by 20 images of 40x40 in grayscale. And the most important thing is that the order of that 20 images matter. I mean, they have to be managed as a 'little' video file of 20 images. If we change the order of that 20 images in a chunk of data the label should be different.

I don't know if is mandatory to use a RNN. I've read some papers where the researchers have used CNN without LSTM and with LSTM for trying to solve similar problems. I think that it isn't mandatory but I think that I've to use the combination of CNN with TimeDistributed. Am I right?

Thank you so much in advance.

Kind regards,
Rubén



",,"[""I don't think this question should be a Keras Issue, as it has nothing to do with the development of Keras as a library.\r\n\r\nThat said, I think convolutional LSTM (which exists in Keras, although I am not sure about the implementation, see this Issue https://github.com/fchollet/keras/issues/7918). It works like a normal convolutional layer, allowing it to effectively learn spatiotemporal things. You can also use 3D convolutions, which answers the question of if it is mandatory to use an RNN (it is not), but RNNs are designed to model the learning of long temporal sequences, which sound like what you're describing. A common implementation is to first work with convolutional layers (that work frame-by-frame, using TimeDistributed) and then once you've scaled the spatial dimensions down to something very small, use a TimeDistributed Flatten layer, followed by a normal (non-spatial) LSTM. I think this way of thinking is a bit flawed, but depending on the data it might work well. This way of modelling assumes that you can get a compact representation of the video frames for each frame independently, and then see the sequence of that compact data and draw temporal conclusions from that. That works in some cases and not in others.\r\n\r\nHope this helps you somewhat."", 'Thank you for your reply @ahrnbom \r\n\r\nI have exposed my problem in a general way and not related with the Keras library due to my lack of my knowledge, sorry.\r\n\r\nTalking about the Keras API, I have the following problem. I\'m getting the following error:\r\n\r\nTraceback (most recent call last):\r\n`  File ""/home/rcc/.local/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py"", line 654, in _call_cpp_shape_fn_impl\r\n    input_tensors_as_shapes, status)\r\n  File ""/usr/lib/python3.5/contextlib.py"", line 66, in __exit__\r\n    next(self.gen)\r\n  File ""/home/rcc/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Negative dimension size caused by subtracting 3 from 1 for \'time_distributed_1/convolution\' (op: \'Conv2D\') with input shapes: [?,1,40,40], [3,3,40,40].\r\n`\r\n\r\n\r\nThis is my ""toying"" code:\r\n`import keras\r\nfrom keras.layers import Dense, Dropout, LSTM\r\nfrom keras.layers import Conv2D, Flatten\r\nfrom keras.models import Sequential\r\nfrom keras.layers.wrappers import TimeDistributed\r\nfrom keras.models import Model\r\nimport numpy as np\r\n\r\nnumber_of_samples = 2500\r\nnumber_of_test_samples = 2000\r\ntimesteps = 20\r\nframe_row = 40\r\nframe_col = 40\r\nchannels = 1\r\noutput_label_size = 1\r\nepochs = 1\r\nbatch_size = 32\r\n\r\ndata = np.random.random((number_of_samples, timesteps, channels, frame_row, frame_col))\r\nlabel = np.random.random((number_of_samples, timesteps, output_label_size))\r\n\r\nX_train = data[0:number_of_test_samples,:]\r\ny_train = label[0:number_of_test_samples]\r\nX_test = data[number_of_test_samples:,:]\r\ny_test = label[number_of_test_samples:,:]\r\n\r\ninput_shape = (timesteps, channels, frame_row, frame_col)\r\n\r\nprint(\'Input shape: \', input_shape)\r\nprint(\'X_train shape: \', X_train.shape)\r\nprint(\'y_train shape: \', y_train.shape)\r\nprint(\'X_test shape: \', X_test.shape)\r\nprint(\'y_test shape: \', y_test.shape)\r\n\r\nmodel = Sequential()\r\nmodel.add(TimeDistributed(Conv2D(40, (3, 3), activation=\'relu\'), input_shape=input_shape))\r\nmodel.add(TimeDistributed(Dropout(0.2)))\r\nmodel.add(TimeDistributed(Conv2D(20, (3, 3), activation=\'relu\')))\r\nmodel.add(TimeDistributed(Dropout(0.2)))\r\nmodel.add(TimeDistributed(Flatten()))\r\nmodel.add(LSTM(30, return_sequences = True))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(LSTM(15))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(Dense(output_label_size, init=\'uniform\'))\r\nmodel.compile(optimizer=\'adam\', loss=\'mse\')`', 'My point was that the Keras Issue tracker is meant to track issues with the Keras library itself. If you cannot get something up and running in Keras, that is not a problem with the Keras library, but rather a problem with your lack of information. That is something you should bring up somewhere else, like a forum. ']",[],[],0,0
221,keras,11989,closed,Pre-trained models don't work in float16 mode,"If you try to use the pre-trained models in float16 mode, they just don't load. I think fixing this can be useful, especially for object detection use cases.




",stat:awaiting tensorflower,"['Thought you might find this interesting @taehoonlee ', ""It's because ResNet50 contains BatchNormalization layers, which are not supported in float16 mode unfortunately. \r\nTo overcome this issue, either consider using a model without batchnormalization or write your own BatchNorm layer and change it in model's build function.\r\n\r\nIn my github page I shared a custom BatchNorm layer that support float16, you're welcome to pull it and use it in your project: [float_16_BatchNormalization](https://github.com/Golbstein/KerasExtras/blob/master/BN16.py)"", '@Golbstein Thanks ! Though it would be better if this is supported in http://github.com/keras-team/keras-applications as well, given half precision training is very important for many use cases.', '@rohit-gupta, What is your version of TensorFlow? The float16 for the `BatchNormalization` has been supported since TensorFlow 1.5.', '@taehoonlee I have Tensorflow 1.10 \r\n\r\nEdit: For precision:\r\n\r\n```\r\n>>> import tensorflow as tf\r\n>>> import keras\r\nUsing TensorFlow backend.\r\n>>> keras.__version__\r\n\'2.2.4\'\r\n>>> tf.__version__\r\n\'1.10.1\'\r\n>>> from keras import backend as K\r\n>>> K.set_floatx(\'float16\')\r\n>>> K.set_epsilon(1e-4)\r\n>>> from keras.applications.resnet50 import ResNet50\r\n>>> ResNet50(include_top=False, weights=None)\r\n\r\n2019-01-13 13:36:50.406461: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-01-13 13:36:53.004865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties:\r\nname: Tesla K40m major: 3 minor: 5 memoryClockRate(GHz): 0.745\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 11.17GiB freeMemory: 11.09GiB\r\n2019-01-13 13:36:53.004906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2019-01-13 13:36:53.248081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-01-13 13:36:53.248125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0\r\n2019-01-13 13:36:53.248132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N\r\n2019-01-13 13:36:53.248400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10747 MB memory) -> physical GPU (device: 0, name: Tesla K40m, pci bus id: 0000:04:00.0, compute capability: 3.5)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/deepromay/anaconda2/envs/tensorflow110/lib/python2.7/site-packages/keras/applications/__init__.py"", line 28, in wrapper\r\n    return base_fun(*args, **kwargs)\r\n  File ""/home/deepromay/anaconda2/envs/tensorflow110/lib/python2.7/site-packages/keras/applications/resnet50.py"", line 11, in ResNet50\r\n    return resnet50.ResNet50(*args, **kwargs)\r\n  File ""/home/deepromay/anaconda2/envs/tensorflow110/lib/python2.7/site-packages/keras_applications/resnet50.py"", line 231, in ResNet50\r\n    x = layers.BatchNormalization(axis=bn_axis, name=\'bn_conv1\')(x)\r\n  File ""/home/deepromay/anaconda2/envs/tensorflow110/lib/python2.7/site-packages/keras/engine/base_layer.py"", line 457, in __call__\r\n    output = self.call(inputs, **kwargs)\r\n  File ""/home/deepromay/anaconda2/envs/tensorflow110/lib/python2.7/site-packages/keras/layers/normalization.py"", line 185, in call\r\n    epsilon=self.epsilon)\r\n  File ""/home/deepromay/anaconda2/envs/tensorflow110/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py"", line 1864, in normalize_batch_in_training\r\n    epsilon=epsilon)\r\n  File ""/home/deepromay/anaconda2/envs/tensorflow110/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py"", line 1839, in _fused_normalize_batch_in_training\r\n    data_format=tf_data_format)\r\n  File ""/home/deepromay/anaconda2/envs/tensorflow110/lib/python2.7/site-packages/tensorflow/python/ops/nn_impl.py"", line 909, in fused_batch_norm\r\n    name=name)\r\n  File ""/home/deepromay/anaconda2/envs/tensorflow110/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 3836, in fused_batch_norm_v2\r\n    is_training=is_training, name=name)\r\n  File ""/home/deepromay/anaconda2/envs/tensorflow110/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 609, in _apply_op_helper\r\n    param_name=input_name)\r\n  File ""/home/deepromay/anaconda2/envs/tensorflow110/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 60, in _SatisfiesTypeConstraint\r\n    "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\nTypeError: Value passed to parameter \'scale\' has DataType float16 not in list of allowed values: float32\r\n\r\n```', '@rohit-gupta, The issue has been resolved by #11580. If you want to use the latest version, you can try `pip install -U git+https://github.com/keras-team/keras`.', '@taehoonlee Thanks !', 'Hello Iam using google colab (up to date) \r\n\r\nI cant use pre-trained inception with keras in float16 .... \r\n\r\nhere is the code : \r\n```\r\n\r\nfrom keras.applications.inception_v3 import InceptionV3\r\nfrom keras.layers import Input, Dense, GlobalAveragePooling2D\r\nfrom keras.models import Model\r\nfrom keras import backend as K\r\nimport tensorflow as tf\r\nfrom keras.optimizers import Adam\r\nfrom keras.utils import np_utils\r\nfrom keras.models import Sequential\r\nfrom keras.layers.convolutional import Conv2D \r\nfrom keras.layers.convolutional import AveragePooling2D, MaxPooling2D\r\nfrom keras.layers.core import Activation, Dropout\r\nfrom keras.layers.core import Flatten\r\nfrom keras.layers.core import Dense\r\nfrom keras.datasets import mnist\r\nfrom keras.utils import np_utils\r\nfrom keras.optimizers import SGD, RMSprop, Adam\r\nimport matplotlib.pyplot as plt\r\nfrom keras.callbacks import TensorBoard\r\nfrom keras.callbacks import EarlyStopping\r\nfrom keras.callbacks import ReduceLROnPlateau\r\nfrom keras.callbacks import ModelCheckpoint\r\nfrom keras.engine.topology import Layer,InputSpec\r\n\r\ndtype=\'float16\'\r\nK.set_floatx(dtype)\r\n\r\n# default is 1e-7 which is too small for float16.  Without adjusting the epsilon, we will get NaN predictions because of divide by zero problems\r\nK.set_epsilon(1e-4) \r\n\r\n# this could also be the output a different Keras model or layer\r\ninput_tensor = Input(shape=(512, 650, 3))  # this assumes K.image_data_format() == \'channels_last\'\r\n\r\nbase_model = InceptionV3(input_tensor=input_tensor, weights=\'imagenet\', include_top=False,pooling=max,classes=512)\r\n# add a global spatial average pooling layer\r\nx = base_model.output\r\nx = GlobalAveragePooling2D()(x)\r\n# let\'s add a fully-connected layer\r\nx = Dense(1024, activation=\'relu\')(x)\r\n# and a logistic layer -- let\'s say we have 200 classes\r\npredictions = Dense(5, activation=\'softmax\')(x)\r\n\r\n# this is the model we will train\r\nmodel = Model(inputs=base_model.input, outputs=predictions)\r\n```\r\n\r\n\r\nand the error : ```\r\n\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-1-494b27bb2539> in <module>()\r\n     31 input_tensor = Input(shape=(512, 650, 3))  # this assumes K.image_data_format() == \'channels_last\'\r\n     32 \r\n---> 33 base_model = InceptionV3(input_tensor=input_tensor, weights=\'imagenet\', include_top=False,pooling=max,classes=512)\r\n     34 # add a global spatial average pooling layer\r\n     35 x = base_model.output\r\n\r\n11 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _SatisfiesTypeConstraint(dtype, attr_def, param_name)\r\n     58           ""allowed values: %s"" %\r\n     59           (param_name, dtypes.as_dtype(dtype).name,\r\n---> 60            "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\n     61 \r\n     62 \r\n\r\nTypeError: Value passed to parameter \'scale\' has DataType float16 not in list of allowed values: float32\r\n```', 'Just to bump this: \r\n\r\nBatchNormalization might work now but LayerNormalization does not work in TF 2.1.0 with mixed precision.']","[""\r\nimport numpy as np\r\n\r\nfrom keras import backend as K\r\nfrom keras.applications.resnet50 import ResNet50\r\nfrom keras.applications.resnet50 import preprocess_input, decode_predictions\r\n\r\nK.set_floatx('float16')\r\n\r\nbackbone = ResNet50(include_top=False, weights='imagenet', pooling=None)\r\n"", '\r\nTraceback (most recent call last):\r\n  File ""retinanet.py"", line 11, in <module>\r\n    backbone = ResNet50(include_top=False, weights=\'imagenet\', pooling=None)\r\n  File ""/path/to/python/python2.7/site-packages/keras/applications/__init__.py"", line 28, in wrapper\r\n    return base_fun(*args, **kwargs)\r\n  File ""/path/to/python/python2.7/site-packages/keras/applications/resnet50.py"", line 11, in ResNet50\r\n    return resnet50.ResNet50(*args, **kwargs)\r\n  File ""/path/to/python/python2.7/site-packages/keras_applications/resnet50.py"", line 231, in ResNet50\r\n    x = layers.BatchNormalization(axis=bn_axis, name=\'bn_conv1\')(x)\r\n  File ""/path/to/python/python2.7/site-packages/keras/engine/base_layer.py"", line 457, in __call__\r\n    output = self.call(inputs, **kwargs)\r\n  File ""/path/to/python/python2.7/site-packages/keras/layers/normalization.py"", line 185, in call\r\n    epsilon=self.epsilon)\r\n  File ""/path/to/python/python2.7/site-packages/keras/backend/tensorflow_backend.py"", line 1864, in normalize_batch_in_training\r\n    epsilon=epsilon)\r\n  File ""/path/to/python/python2.7/site-packages/keras/backend/tensorflow_backend.py"", line 1839, in _fused_normalize_batch_in_training\r\n    data_format=tf_data_format)\r\n  File ""/path/to/python/python2.7/site-packages/tensorflow/python/ops/nn_impl.py"", line 909, in fused_batch_norm\r\n    name=name)\r\n  File ""/path/to/python/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 3836, in fused_batch_norm_v2\r\n    is_training=is_training, name=name)\r\n  File ""/path/to/python/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 609, in _apply_op_helper\r\n    param_name=input_name)\r\n  File ""/path/to/python/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 60, in _SatisfiesTypeConstraint\r\n    "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\nTypeError: Value passed to parameter \'scale\' has DataType float16 not in list of allowed values: float32\r\n']",[],0,0
222,keras,1573,closed,Regarding Class Balancing,"Hello,

I'm trying to add weights to the classes during training right before the final  layer, the output shape of the layer is  with one-hot-label of the same size, and I'm using  loss here. I added a class balancing layer before  which did:



but it didn't seem to help... any suggestions will be greatly appreciated!

Update: I've tried to use  in  and send class weight as dictionary: , but got the error: 
",,"['@tdhd just saw your post regarding this problem, could you shed some light on this one? Thanks!\n', '@wxs @fchollet any ideas?\n']","[' python\n...\nX = self.get_input(train)\nx_balanced = X\n# unique: label values in the training set, [0, 1, 2, ...] (1 x nb_class)\n# putting weight to each class\nfor i in range(len(self.unique)):\n    x_balanced = T.set_subtensor(x_balanced[:, self.unique[i]], x_balanced[:, self.unique[i]]*self.weight[i])\nreturn x_balanced\n...\n']","['softmax', '(batch_size, length, nb_class)', 'categorical_crossentropy', 'softmax', 'class_weight', 'fit', '{0:w1, 1:w2, ...}', 'Exception: class_weight not supported for 3+ dimensional targets.']",0,0
223,keras,4585,closed,generator in mnist_acgan example collapses to black images,"As the title states, when I run the mnist_acgan example the discriminator loss quickly goes to zero and the generator loss increases over time. The generated images at the end of each epoch are black. I'm digging into this myself, but GANs aren't my area so I expect progress to be slow. Any suggestions on where to look? Is anyone able to run this example without any problems?

Link to the example: https://github.com/fchollet/keras/blob/master/examples/mnist_acgan.py

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).",,"['I mentioned this problem before in the pull requests. The solution is easy. You can change the random seed and that works for me', ""Thank you for the suggestion. That didn't work for me unfortunately. I see you're using a different version of CUDA and cuDNN than I am. I'm using CUDA version 7.0 and cuDNN 5.0.4. I'm due for an update, so I'll report back when I test this again."", 'I think I upgraded the version and changed the seed to 1331. Then it works fine.', 'This is working after updating to CUDA 8.0. Thanks for the tips!', ""Hello!\r\n\r\nI have CUDA 8.0 and the latest versions of keras and tensorflow. However, the generator keeps creating black images. I tried changing the random seed and it didn't help.\r\n\r\nSystem overview:\r\nWindows 10,\r\nPython 3.5.2 :: Anaconda 4.2.0 (64-bit),\r\ntensorflow-gpu==0.12.1,\r\nKeras==1.2.2\r\n\r\nAny suggestions on how to make the example work?"", ""Last I checked, which was about 10 weeks ago, my PR to fix this issue was\nrejected and the fix put in by @fchollet did not fix the issue.  I gave up\nand moved to tensorflow.\n\nOn Mon, Feb 13, 2017 at 8:24 AM, sautin1 <notifications@github.com> wrote:\n\n> Hello!\n>\n> I have CUDA 8.0 and the latest versions of keras and tensorflow. However,\n> the generator keeps creating black images. I tried changing the random seed\n> and it didn't help.\n>\n> System overview:\n> Windows 10,\n> Python 3.5.2 :: Anaconda 4.2.0 (64-bit),\n> tensorflow-gpu==0.12.1,\n> Keras==1.2.1\n>\n> Any suggestions on how to make the example work?\n>\n> —\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/fchollet/keras/issues/4585#issuecomment-279391078>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ALarq48nBol4DtoygHo2HFcGGmlP8c8fks5rcFmFgaJpZM4LDb7L>\n> .\n>\n"", ""As I understand, you address to this PR: https://github.com/fchollet/keras/pull/4482\r\nAre you sure it has something to do with the issue being discussed? The mnist_acgan example doesn't seem to even call the function, which was corrected in your PR.\r\n\r\n> Last I checked, which was about 10 weeks ago, my PR to fix this issue was\r\nrejected and the fix put in by @fchollet did not fix the issue.  I gave up\r\nand moved to tensorflow."", '@sautin1   Yes, you are correct, this doesnt look to be the same issue.  This should be pretty easy to debug though.  You should remove layers in the generator 1 by 1 until they produce an output.  If you get to the a single layer and the image is still black, you can trace through the code in the debugger to find the offending line.', 'The examples also collaped into black images for me. I changed the learning rate from 0.0002 to 0.00005 and it converges now.', '\r\nTo change random seed and learning rate to 0.00005 do not work for me.\r\nthis is my training loss info, seems that d loss is too small which are not good:\r\n', '599/600 [============================>.] - ETA: 0s   \r\nTesting for epoch 1:\r\ncomponent              | loss | generation_loss | auxiliary_loss\r\n-----------------------------------------------------------------\r\ngenerator (train)      | 8.35 | 6.03            | 2.32 \r\ngenerator (test)       | 8.40 | 6.09            | 2.31 \r\ndiscriminator (train)  | 1.32 | 0.01            | 1.31 \r\ndiscriminator (test)   | 1.20 | 0.00            | 1.20 \r\nEpoch 2 of 50\r\n599/600 [============================>.] - ETA: 0s  \r\nTesting for epoch 2:\r\ncomponent              | loss | generation_loss | auxiliary_loss\r\n-----------------------------------------------------------------\r\ngenerator (train)      | 9.38 | 7.07            | 2.31 \r\ngenerator (test)       | 8.89 | 6.58            | 2.30 \r\ndiscriminator (train)  | 1.21 | 0.00            | 1.21 \r\ndiscriminator (test)   | 1.19 | 0.00            | 1.19 \r\nEpoch 3 of 50\r\n599/600 [============================>.] - ETA: 0s  \r\nTesting for epoch 3:\r\ncomponent              | loss | generation_loss | auxiliary_loss\r\n-----------------------------------------------------------------\r\ngenerator (train)      | 9.79 | 7.48            | 2.31 \r\ngenerator (test)       | 9.30 | 6.99            | 2.30 \r\ndiscriminator (train)  | 1.20 | 0.00            | 1.20 \r\ndiscriminator (test)   | 1.18 | 0.00            | 1.18 \r\nEpoch 4 of 50\r\n599/600 [============================>.] - ETA: 0s  \r\nTesting for epoch 4:\r\ncomponent              | loss | generation_loss | auxiliary_loss\r\n-----------------------------------------------------------------\r\ngenerator (train)      | 10.14 | 7.83            | 2.31 \r\ngenerator (test)       | 9.91 | 7.61            | 2.30 \r\ndiscriminator (train)  | 1.19 | 0.00            | 1.19 \r\ndiscriminator (test)   | 1.17 | 0.00            | 1.17 \r\nEpoch 5 of 50\r\n599/600 [============================>.] - ETA: 0s  \r\nTesting for epoch 5:\r\ncomponent              | loss | generation_loss | auxiliary_loss\r\n-----------------------------------------------------------------\r\ngenerator (train)      | 10.27 | 7.96            | 2.31 \r\ngenerator (test)       | 10.45 | 8.14            | 2.30 \r\ndiscriminator (train)  | 1.19 | 0.00            | 1.19 \r\ndiscriminator (test)   | 1.17 | 0.00            | 1.17 \r\nEpoch 6 of 50\r\n599/600 [============================>.] - ETA: 0s  \r\nTesting for epoch 6:\r\ncomponent              | loss | generation_loss | auxiliary_loss\r\n-----------------------------------------------------------------\r\ngenerator (train)      | 10.56 | 8.25            | 2.31 \r\ngenerator (test)       | 9.99 | 7.69            | 2.30 \r\ndiscriminator (train)  | 1.18 | 0.00            | 1.18 \r\ndiscriminator (test)   | 1.18 | 0.00            | 1.18 \r\n', 'I get the following output animation: ![mnist acgan git animation](https://github.com/s-macke/Kerasimo/blob/master/images/mnist_acgan.gif).\r\nI use my own script which is almost identical to the example except the learning rate: [mnist_acgan source](https://github.com/s-macke/Kerasimo/blob/master/models/mnist_acgan.py)\r\n\r\nI use the following libraries\r\n- Keras 2.0.2\r\n- tensorflow 1.0.0\r\n- cuda 8.0.61\r\n- numpy 1.12.1\r\n- cudnn 5.1.10\r\n\r\nI successfully run it on a GTX 1080 and on a GTX 960.', 'after updating learning rate to 0.00001, I got excpected result']",[],[],0,0
224,keras,2366,closed,ModelCheckpoint saves one extra layer of weights,"I'm using the ModelCheckpoint as described in keras.io with a sequential model.  When I save the model using the code described in the FAQ and then try to reload it based on the weights saved by ModelCheckpoint, it tells me:

model.load_weights(os.path.join('checkpoints')+'//weights.hdf5')
Traceback (most recent call last):

  File ""<ipython-input-15-ca39381e1009>"", line 1, in <module>
    model.load_weights(os.path.join('checkpoints')+'//weights.hdf5')

  File ""C:\WinPython\WinPython-64bit-3.4.4.1\python-3.4.4.amd64\lib\site-packages\keras\engine\topology.py"", line 2286, in load_weights
    str(len(flattened_layers)) + ' layers.')

**Exception: You are trying to load a weight file containing 31 layers into a model with 30 layers.**

Is there an error in the way the ModelCheckpoint saves weights, for instance including input layer weights of 1?
",,"[""So, I found a sort of fix.  If I save the model before I start training, it doesn't work, it gives me that error, but if I use the same code to save the model during a callback that runs on_train_begin, the model will have the same number of layers as the saved weights.  Is that intentional?  Is there a way to make it not require that?\n"", 'Update Keras.\nOn Apr 17, 2016 12:42 AM, ""penguin2121"" notifications@github.com wrote:\n\n> So, I found a sort of fix. If I save the model before I start training, it\n> doesn\'t work, it gives me that error, but if I use the same code to save\n> the model during a callback that runs on_train_begin, the model will have\n> the same number of layers as the saved weights. Is that intentional? Is\n> there a way to make it not require that?\n> \n> —\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly or view it on GitHub\n> https://github.com/fchollet/keras/issues/2366#issuecomment-210971441\n', 'Wow, that was fast. Fix works for me. (found this ticket via google search)\n', 'Which version will not have the issue?\r\nI mean, I am using 1.2.2, and updating to 2.0.x will generate tons of import errors...so I hope I can fix it in a lower version.']",[],[],0,0
225,keras,9262,closed,to_categorical() doesn't respect default float precision in Keras configuration file," doesn't respect the default float precision in Keras configuration file. In my case, the default is  while the actual is . Related StackOverflow [thread](https://stackoverflow.com/q/48486775/1348273).

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
~~~python
import numpy as np
from keras.utils import to_categorical
print(to_categorical(np.ones(2), 2).dtype)
~~~

",,[],[],"['keras.utils.to_categorical()', 'float32', 'float64']",0,0
226,keras,7355,closed,mnist_sklearn_wrapper.py,"I go this error  in mnist_sklearn_wrapper.py
",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],"['Traceback (most recent call last):\r\n  File ""mnist_sklearn_wrapper.py"", line 91, in <module>\r\n    validator.fit(x_train, y_train)\r\n  File ""D:\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py"", line 804, in fit\r\n    return self._fit(X, y, ParameterGrid(self.param_grid))\r\n  File ""D:\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py"", line 522, in _fit\r\n    self.scorer_ = check_scoring(self.estimator, scoring=self.scoring)\r\n  File ""D:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py"", line 238, in check_scoring\r\n    return get_scorer(scoring)\r\n  File ""D:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py"", line 197, in get_scorer\r\n    % (scoring, sorted(SCORERS.keys())))\r\nValueError: \'neg_log_loss\' is not a valid scoring value. Valid options are [\'accuracy\', \'adjusted_rand_score\', \'average_precision\', \'f1\', \'f1_macro\', \'f1_micro\', \'f1_samples\', \'f1_weighted\', \'log_loss\', \'mean_absolute_error\', \'mean_squared_error\', \'median_absolute_error\', \'precision\', \'precision_macro\', \'precision_micro\', \'precision_samples\', \'precision_weighted\', \'r2\', \'recall\', \'recall_macro\', \'recall_micro\', \'recall_samples\', \'recall_weighted\', \'roc_auc\']']",0,0
227,keras,4962,closed,How to add Attention on top of a Recurrent Layer (Text Classification),"I am doing text classification. Also I am using my pre-trained word embeddings and i have a  layer on top with a  at the end.

Pretty simple. Now I want to add attention to the model,  but i don't know how to do it.
 
My understanding is that i have to set  so as the attention layer will weigh each timestep accordingly. This way the LSTM will return a 3D Tensor, right?
After that what do i have to do?
Is there a way to easily implement a model with attention using Keras Layers or do i have to write my own custom layer?

If this can be done with the available Keras Layers, I would really appreciate an example.
",,"[""It's been a while since I've used attention, so take this with a grain of salt.\r\n\r\n`return_sequences` does not necessarily need to be `True` for attention to work; the underlying computation is the same, and this flag should be used only based on whether you need 1 output or an output for each timestep.\r\n\r\nAs for implementing attention in Keras.. There are two possible methods: a) add a hidden `Activation` layer for the softmax or b) change the recurrent unit to have a softmax.\r\n\r\nOn option a): this would apply attention to the output of the recurrent unit but *not* to the output/input passed to the next time step. I don't this is what is desired. In this case, the LSTM should have a squashing function applied, as LSTMs don't do too well with linear/relu style activation.\r\n\r\nOn option b): this would apply attention to the output of the recurrentcy, and also to the output/input passed to the next timestep. I *think* that this is what is desired, but I could be wrong. In this case, the linear output of the neurons would be squashed directly by the softmax; if you wish to apply a pre-squashing such as `sigmoid` or `tanh` before the softmax calculation, you would need a custom activation that does both in one step.\r\n\r\nI could draw a diagram if necessary, and I should probably read the activation papers again.."", ""@patyork Thanks for the reply. \r\nDo you have a good paper (or papers) in mind (for attention)? I am reading a lot about attention, and i want to try it out, because i really like the idea. But even though i think i understand the concept i don't have a clear understanding of how it works and how to implement it.\r\n\r\nIf it is possible i would like for someone to offer an example in Keras.\r\n\r\nPS. is this the correct place to ask such question or i should do it at https://groups.google.com/d/forum/keras-users?\r\n"", '@baziotis This area is supposed to be more for bugs as opposed to ""how to implement"" questions. I admit I don\'t often look at the google group, but that is a valid place to ask these questions, as well as on the Slack channel.\r\n\r\n[Bengio et. al](https://arxiv.org/pdf/1502.03044v3.pdf) has a pretty good paper on attention (soft attention is the softmax attention).\r\n\r\nAn example of method a) I described:\r\n```python\r\nvocab_size = embeddings.shape[0]\r\nembedding_size = embeddings.shape[1]\r\n\r\nmodel = Sequential()\r\n\r\nmodel.add(Embedding(\r\n        input_dim=vocab_size,\r\n        output_dim=embedding_size,\r\n        input_length=max_length,\r\n        trainable=False,\r\n        mask_zero=True,\r\n        weights=[embeddings]\r\n    ))\r\n\r\nmodel.add(LSTM(200, return_sequences=False))\r\nmodel.add(Activation(\'softmax\')) #this guy here\r\nmodel.add(Dropout(0.5))\r\n\r\nmodel.add(Dense(3, activation=\'softmax\', activity_regularizer=activity_l2(0.0001)))\r\n```\r\n\r\nexample b), with simple activation:\r\n```python\r\nvocab_size = embeddings.shape[0]\r\nembedding_size = embeddings.shape[1]\r\n\r\nmodel = Sequential()\r\n\r\nmodel.add(Embedding(\r\n        input_dim=vocab_size,\r\n        output_dim=embedding_size,\r\n        input_length=max_length,\r\n        trainable=False,\r\n        mask_zero=True,\r\n        weights=[embeddings]\r\n    ))\r\n\r\nmodel.add(LSTM(200, return_sequences=False, activation=\'softmax\'))\r\nmodel.add(Dropout(0.5))\r\n\r\nmodel.add(Dense(3, activation=\'softmax\', activity_regularizer=activity_l2(0.0001)))\r\n```\r\n\r\nexample b) with sigmoid and then softmax (non-working, but the idea):\r\n```python\r\nvocab_size = embeddings.shape[0]\r\nembedding_size = embeddings.shape[1]\r\n\r\ndef myAct(out):\r\n    return K.softmax(K.tanh(out))\r\n\r\nmodel = Sequential()\r\n\r\nmodel.add(Embedding(\r\n        input_dim=vocab_size,\r\n        output_dim=embedding_size,\r\n        input_length=max_length,\r\n        trainable=False,\r\n        mask_zero=True,\r\n        weights=[embeddings]\r\n    ))\r\n\r\nmodel.add(LSTM(200, return_sequences=False, activation=myAct))\r\nmodel.add(Dropout(0.5))\r\n\r\nmodel.add(Dense(3, activation=\'softmax\', activity_regularizer=activity_l2(0.0001)))\r\n```\r\n\r\nIn addition, I should say that my notes about whether a) or b) above is what you probably need are based on your example, where you want one output (making option b probably the correct way). Attention is often used in spaces like caption generation where there is more than 1 output such as setting `return_sequences=True`. For those cases, I think that option a) is the described usage, such that the recurrency keeps all the information passing forward, and it\'s just the higher layers that utilize the attention.', '@patyork Thanks for the examples and for the paper. I new that posting here would get more attention :P \r\n\r\nI will try them and post back.', ""@patyork, I'm sorry, but I don't see how this implements attention at all?\r\n\r\nFrom my understanding, the softmax in the Bengio et al. paper is not applied over the LSTM output, but over the output of an attention model, which is calculated from the LSTM's hidden state at a given timestep. The output of the softmax is then used to modify the LSTM's internal state. Essentially, attention is something that happens *within an LSTM* since it is both based on and modifies its internal states.\r\n\r\nI actually made my own attempt to create an attentional LSTM in Keras, based on the very same paper you cited, which I've shared here:\r\n\r\nhttps://gist.github.com/mbollmann/ccc735366221e4dba9f89d2aab86da1e\r\n\r\nThere are several different ways to incorporate attention into an LSTM, and I won't claim 100% correctness of my implementation (though I'd appreciate any hints if something seems terribly wrong!), but I'd be surprised if it was as simple as adding a softmax activation."", '@mbollmann You are correct that none of the solutions @patyork is what i want. i want to get a weight distribution (importance) for the outputs from each timestep of the RNN. Like in the paper: ""[Hierarchical Attention Networks for Document Classification](https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf)"" but in my case i want just the representation of a sentence. I am trying to implement this using the available keras layers. \r\n\r\nSimilar idea in this [paper](http://anthology.aclweb.org/P16-2034).', '@baziotis That indeed looks conceptually much simpler. I could just take a very short glance right now, but is there a specific point where you got stuck? ', '@mbollmann Please do if you can. \r\nI am trying to implement it right now and trying to understand the Keras API.\r\n\r\nI don\'t have a working solution but i think i should set  `return_sequences=True` in the RNN in order to get the intermediate outputs and `masking=False`.\r\nOn top of that i am thinking  i should put a `TimeDistributed(Dense(1))`  with a softmax activation. But i haven\'t figured out how to put everything together.\r\n\r\nAlso i think that putting `masking=False` won\'t affect the performance as the attention layer will assign the correct weights on the padded words. Am i right?\r\n\r\n\r\nEdit: to clarify i want to implement an attention mechanism like the one in [1].\r\n![attention mechanism](https://cloud.githubusercontent.com/assets/5629093/21811660/277a0ee6-d759-11e6-8d02-29ee327e9ff8.png)\r\n1.   Zhou, Peng, et al. ""Attention-based bidirectional long short-term memory networks for relation classification."" The 54th Annual Meeting of the Association for Computational Linguistics. 2016.', 'I tried this:\r\n```python\r\n_input = Input(shape=[max_length], dtype=\'int32\')\r\n\r\n    # get the embedding layer\r\n    embedded = embeddings_layer(embeddings=embeddings_matrix,\r\n                                trainable=False, masking=False, scale=False, normalize=False)(_input)\r\n\r\n    activations = LSTM(64, return_sequences=True)(embedded)\r\n\r\n    # attention\r\n    attention = TimeDistributed(Dense(1, activation=\'tanh\'))(activations) \r\n    attention = Flatten()(attention)\r\n    attention = Activation(\'softmax\')(attention)\r\n\r\n    activations = Merge([activations, attention], mode=\'mul\')\r\n\r\n    probabilities = Dense(3, activation=\'softmax\')(activations)\r\n\r\n    model = Model(input=_input, output=probabilities)\r\n    model.compile(optimizer=\'adam\', loss=\'categorical_crossentropy\', metrics=[])\r\n```\r\nand i get the following error:\r\n```\r\n  File ""...\\keras\\engine\\topology.py"", line 1170, in __init__\r\n    node_indices, tensor_indices)\r\n  File ""...\\keras\\engine\\topology.py"", line 1193, in _arguments_validation\r\n    layer_output_shape = layer.get_output_shape_at(node_indices[i])\r\nAttributeError: \'TensorVariable\' object has no attribute \'get_output_shape_at\'\r\n```', '@baziotis The cause of the error probably is that you need to use the `merge` function (lowercase), not the `Merge` layer (uppercase).\r\n\r\nApart from that, as far as I understood it:\r\n\r\nThe part with the `tanh` activation (Equation 5 in Yang et al., Equation 9 in Zhou et al.) comes before the multiplication with a trained context vector/parameter vector which reduces the dimensionality to ""one scalar per timestep"".  For Yang et al., that seems to be a Dense layer which doesn\'t yet reduce the dimensionality (though this is a little unclear to me), so I\'d expect `TimeDistributed(Dense(64, activation=\'tanh\'))`.  For Zhou et al., they just write ""tanh"", so you\'d probably not even need a Dense layer, just the tanh activation after the LSTM.\r\n\r\nFor the multiplication with a trained context vector/parameter vector, I believe (no longer -- see EDIT) this might be a simple `Dense(1)` in Keras, *without* the `TimeDistributed` wrapper, since we want to have individual weights for each timestep, but I\'m not totally sure about this and haven\'t tested it. I\'d imagine something like this, but take this with a grain of salt:\r\n\r\n```\r\n    # attention after Zhou et al.\r\n    attention = Activation(\'tanh\')(activations)    # Eq. 9\r\n    attention = Dense(1)(attention)                # Eq. 10\r\n    attention = Flatten()(attention)               # Eq. 10\r\n    attention = Activation(\'softmax\')(attention)   # Eq. 10\r\n    activations = merge([activations, attention], mode=\'mul\')  # Eq. 11\r\n```\r\n\r\n(EDIT: Nope, doesn\'t seem that way, they train a parameter vector with dimensionality of the embedding, not a matrix with a timestep dimension.)', 'My apologies; this would explain why I was not impressed with the results from my ""attention"" implementation.\r\n\r\nThere is an implementation [here ](https://github.com/fchollet/keras/issues/1472)that seems to be working for people.', ""@mbollmann you were right about the `merge`, it is different from `Merge` #2467.\r\n\r\nI think this is really close:\r\n\r\n```python\r\nunits = 64\r\nmax_length = 50\r\n\r\n_input = Input(shape=[max_length], dtype='int32')\r\n\r\n# get the embedding layer\r\nembedded = embeddings_layer(embeddings=embeddings_matrix,\r\n\t\t\t\t\t\t\ttrainable=False, masking=False, scale=False, normalize=False)(_input)\r\n\r\nactivations = LSTM(units, return_sequences=True)(embedded)\r\n\r\n# compute importance for each step\r\nattention = TimeDistributed(Dense(1, activation='tanh'))(activations) \r\nattention = Flatten()(attention)\r\nattention = Activation('softmax')(attention)\r\nattention = RepeatVector(units)(attention)\r\nattention = Permute([2, 1])(attention)\r\n\r\n# apply the attention\r\nsent_representation = merge([activations, attention], mode='mul')\r\nsent_representation = Lambda(lambda xin: K.sum(xin, axis=0))(sent_representation)\r\nsent_representation = Flatten()(sent_representation)\r\n\r\nprobabilities = Dense(3, activation='softmax')(sent_representation)\r\n\r\nmodel = Model(input=_input, output=probabilities)\r\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[])\r\n```\r\nbut i get an error because Lamda doesn't output the right dimensions. I should be getting [1,units] right?\r\nWhat am i doing wrong?\r\n\r\n---\r\n\r\n**Update**: i tried explicitly passing the `output_shape` for `Lambda` and the model compiles:\r\n```python\r\nsent_representation = merge([activations, attention], mode='mul')\r\nsent_representation = Lambda(lambda xin: K.sum(xin, axis=0), output_shape=(units, ))(sent_representation)\r\n# sent_representation = Flatten()(sent_representation)\r\n```\r\n\r\nbut now i get the following error:\r\n```\r\nValueError: Input dimension mis-match. (input[0].shape[0] = 128, input[1].shape[0] = 50)\r\nApply node that caused the error: Elemwise{Composite{(i0 * log(i1))}}(dense_2_target, Elemwise{Clip}[(0, 0)].0)\r\nToposort index: 155\r\nInputs types: [TensorType(float32, matrix), TensorType(float32, matrix)]\r\nInputs shapes: [(128, 3), (50, 3)]\r\nInputs strides: [(12, 4), (12, 4)]\r\nInputs values: ['not shown', 'not shown']\r\nOutputs clients: [[Sum{axis=[1], acc_dtype=float64}(Elemwise{Composite{(i0 * log(i1))}}.0)]]\r\n```\r\n"", ""Well i found out why it wasn't working. I was expecting the input to Lamda to be (max_length, units) but it was (None, max_length, units), so i just had to change the axis to 1. This now works.\r\n\r\n```python\r\nunits = 64\r\nmax_length = 50\r\nvocab_size = embeddings.shape[0]\r\nembedding_size = embeddings.shape[1]\r\n\r\n\r\n_input = Input(shape=[max_length], dtype='int32')\r\n\r\n# get the embedding layer\r\nembedded = Embedding(\r\n        input_dim=vocab_size,\r\n        output_dim=embedding_size,\r\n        input_length=max_length,\r\n        trainable=trainable,\r\n        mask_zero=masking,\r\n        weights=[embeddings]\r\n    )(_input)\r\n\r\nactivations = LSTM(units, return_sequences=True)(embedded)\r\n\r\n# compute importance for each step\r\nattention = TimeDistributed(Dense(1, activation='tanh'))(activations) \r\nattention = Flatten()(attention)\r\nattention = Activation('softmax')(attention)\r\nattention = RepeatVector(units)(attention)\r\nattention = Permute([2, 1])(attention)\r\n\r\n# apply the attention\r\nsent_representation = merge([activations, attention], mode='mul')\r\nsent_representation = Lambda(lambda xin: K.sum(xin, axis=1))(sent_representation)\r\n\r\nprobabilities = Dense(3, activation='softmax')(sent_representation)\r\n\r\nmodel = Model(input=_input, output=probabilities)\r\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[])\r\n```\r\n\r\nI would like if someone could verify that this implementation is correct."", ""@baziotis Looks good to me.  I re-read the description in Zhou et al. and the code looks like it does what they describe.  I no longer understand how what they're doing does anything useful, since the attention model *only* depends on the input and applies the same weights at every timestep, but ... that's probably just my insufficient understanding (I'm used to slightly different types of attention). :)"", ""@mbollmann i am confused about the same thing. can you give an example of the type of attention that you have in mind? I think that i have to put the word (embedding) in the calculation of the attention. \r\n\r\nFrom what i understand the Dense layer:\r\n1. assigns a _different_ weight (importance) to each timestep\r\n2. BUT the importance is static. Essentially this means that each word position in a sentence has different importance, but the importance comes from the position of the word and not the word itself.\r\n\r\nI plotted the weights of the `TimeDistributed(Dense(1, activation='tanh'))(activations)` in a heatmap:\r\n\r\n![att](https://cloud.githubusercontent.com/assets/5629093/21855113/e35b5eb8-d825-11e6-9fb2-202e922dbd0d.png)\r\nMy interpretation is that the positions with big weights play more important role, so the output of the RNN for those steps will have i bigger impact in the final representation of the sentence.\r\n\r\nThe problem is that this is _static_. If an _important_ word happens to occur in a position with a small weight then the representation of the sentence won't be good enough.\r\n\r\nI would like some feedback on this, and preferably a good paper with a better attention mechanism. "", ""@baziotis Are you sure you don't have it the wrong way around?\r\n\r\nThe Dense layer takes the output of the LSTM *at one timestep* and transforms it. The TimeDistributed wrapper applies the *same Dense layer* with the *same weights* to each timestep -- which means the output of the calculation **cannot** depend on the position/timestep since the Dense layer doesn't even know about it.\r\n\r\nSo my confusion seems to be of a different nature than yours. :)\r\n\r\n(In short: I don't see what calculating a softmax and multiplying the original vector by that gets you that a plain `TimeDistributed(Dense(...))` couldn't already learn.  However, I work on attentional models where the output is also a time-series, which means that I have multiple output timesteps for which the model should learn to attend to different input timesteps.  I think that's not directly comparable to your situation, since you only have one output.)"", ""@mbollmann I'm also a bit confused (but I have been from the get go). I think [this blog post](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/) is fairly informative, or at least has some decent pictures.\r\n\r\nSo, @baziotis is using time series with multiple output steps (LSTM, with `return_sequences=True`). The first dense layer is applying weights over *each individual time step output* from the LSTM, which I'm not sure is accomplishing the intended behavior of looking at all the past activations and assigning weights to those, as in this picture:\r\n![image](https://cloud.githubusercontent.com/assets/1304633/21856620/78c12b9e-d7d7-11e6-8171-ca43a2491ed8.png)\r\n\r\nI'm thinking the code above is just the line `at,T` feeding into the attention layer at each timestep. The fallout of this is that the attention is just determining which *activations* are important, not which *timesteps* are important."", '@mbollmann i thought that the TimeDistributed applies different weights to each timestep...\r\nIn that case everything is wrong.\r\nHow can i make it so i can apply different weights to each timestep?\r\nCan this be done with the available keras layers? Any hint?', ""TimeDistributed applies the same weight set across every timestep.\r\n\r\nYou'd need to setup a standard Dense layer as a matrix e.g. `Dense(20)` where 20 is the lookback length. You'd then feed examples of 20 timesteps to train. This is where I'm quite confused about implementing attention, as in theory it looks like this lookback is infinite, not fixed at a certain length."", ""Sorry for the miss-click. \r\nSo if i have inputs of constant length, lets say 50 then is this what i have to do?\r\n\r\n```python\r\nactivations = LSTM(units, return_sequences=True)(embedded)\r\n\r\n# compute importance for each step\r\nattention = Dense(50 , activation='tanh')(activations) \r\nattention = Flatten()(attention)\r\n```\r\n"", 'Actually, no, I think you would just remove the `TimeDistributed` wrapper and keep `Dense(1)` - I need to implement it real quick and check some shapes though.', ""So I guess that is what you are looking for.\r\n\r\n- 50 timesteps\r\n- Feeds into a regular `Dense(1)`, which provides separate weights for the 50 timesteps\r\n- Calculates attention and multiplies against the 50 timesteps to apply attention\r\n- Sums (this reduces the 50 timesteps to 1 output; this is where this attention implementation differs from what most of what I've read describes)\r\n- Dense layer that produces output of shape (None, 3)\r\n\r\n```python\r\n_input = Input(shape=[max_length], dtype='int32')\r\n\r\n# get the embedding layer\r\nembedded = Embedding(\r\n        input_dim=vocab_size,\r\n        output_dim=embedding_size,\r\n        input_length=max_length,\r\n        trainable=False,\r\n        mask_zero=False\r\n    )(_input)\r\n\r\nactivations = LSTM(units, return_sequences=True)(embedded)\r\n\r\n# compute importance for each step\r\nattention = Dense(1, activation='tanh')(activations)\r\nattention = Flatten()(attention)\r\nattention = Activation('softmax')(attention)\r\nattention = RepeatVector(units)(attention)\r\nattention = Permute([2, 1])(attention)\r\n\r\n\r\nsent_representation = merge([activations, attention], mode='mul')\r\nsent_representation = Lambda(lambda xin: K.sum(xin, axis=-2), output_shape=(units,))(sent_representation)\r\n\r\nprobabilities = Dense(3, activation='softmax')(sent_representation)\r\n```\r\n\r\nI think this (ugly) chart maps the above out pretty well; it's up to you to determine if it makes sense for what you are doing:\r\n![image](https://cloud.githubusercontent.com/assets/1304633/21859140/6217ec8a-d7e0-11e6-8bb8-22957ecb9536.png)\r\n"", '@patyork Thanks! I think this is what is described in the paper.\r\nWhat they are trying to do from what i understand is:  instead of using just the last output of the RNN, they use the weighted sum of all the intermediate outputs.\r\n\r\nI have a question about this line:\r\n```python\r\nsent_representation = Lambda(lambda xin: K.sum(xin, axis=-2), output_shape=(units,))(sent_representation)\r\n```\r\nWhy `axis=-2`. How does this sum the tensors? I am using axis=1.\r\n', ""continuing from my last comment, this is what is described in the blog post [that you mentioned](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/). See after the image that you posted...\r\n\r\n> ![](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/12/Screen-Shot-2015-12-30-at-1.16.08-PM-235x300.png) The y‘s are our translated words produced by the decoder, and the x‘s are our source sentence words. The above illustration uses a bidirectional recurrent network, but that’s not important and you can just ignore the inverse direction. _The important part is that each decoder output word y_t now depends on a weighted combination of all the input states, not just the last state._ The a‘s are weights that define in how much of each input state should be considered for each output. So, if a_{3,2} is a large number, this would mean that the decoder pays a lot of attention to the second state in the source sentence while producing the third word of the target sentence. The a's are typically normalized to sum to 1 (so they are a distribution over the input states).\r\n\r\nWhat different kind of attention do you have in mind? In the article attention is described in the context of machine translation. In my case (classification) i just want a better representation for the sentence."", 'Yeah, after thinking about this, it makes sense. The softmax multiplication will weight the timestep outputs (most will be near zero, some nearer to 1) and the so the sum of those will be close to the outputs of the ""near to 1"" timesteps - pretty clever.\r\n\r\nIn this case, `axis=-2` is equivalent to `axis=1`; I use the reverse indexing all the time, so that I never have to remember that Keras includes the batch_size (the `None` aspect) in those shapes. [You ran into this gotcha earlier](https://github.com/fchollet/keras/issues/4962#issuecomment-271710714); using the reverse indexing means I never have to think about that aspect - and you\'ll see that form of indexing throughout the actual Keras code for this reason.\r\n\r\nI just mean that implementation seems a little limiting - you have to set `T=50` or another limit; it can\'t be an infinite or undefined variable, which means you have to throw away the first `T-1` (49) outputs/training outputs. As that image leads me to believe, the T should be infinite/undefined/variable, Something like the `TimeDistributed` wrapper *could* provide. Perhaps this is a good thing, perhaps not - I haven\'t tried both ways (obviously).', 'Phew, a lot happened here, and I think I agree with most of what was written.  Using `Dense(1)` without the TimeDistributed wrapper was what I was already trying to argue for yesterday, some dozens posts above, so that does seem correct to me as well in this scenario.', '@mbollmann I read that - it seems like you talked yourself out of that at some point though, based on the edit. I was confusing/arguing with myself to no end throughout this entire issue as well.\r\n\r\nI learned quite a bit though, at least.', ""@patyork @mbollmann Thank you both! I learned a lot.\r\n\r\nBtw after runnng some tests, i am not impressed. I see no obvious improvement compared to the classic senario (using just the last timestep). But the idea is interesting...\r\n\r\n@patyork This may be stupid, but what do you mean by saying:\r\n> it can't be an infinite or undefined variable, which means you have to throw away the first T-1 (49) outputs/training outputs.\r\n\r\nWhy are they thrown? They are used in the weighted sum, aren't they? *\r\nI agree that this is limiting as it won't work with masking (series of varying length).\r\n\r\n*Do you mean the timesteps that are padded to keep a constant length?"", 'Sorry - hard concept to explain in words, so:\r\n![image](https://cloud.githubusercontent.com/assets/1304633/21860920/a6c62232-d7e7-11e6-9d5d-a8ef91d536b8.png)\r\n\r\n\r\nSay you have a sequence of 8 items; you want to apply attention with a lookback of 3; by default, you have to feed the first 3 items to get the first output, so you wind up with `length - T + 1` or `8-3+1=6` possible outputs (the blue arrows). Looking at the diagram, there would generally be 8 outputs/targets available, but 2 were dropped (the red arrows).\r\n\r\nThinking some more, I\'ve just realized the solution: add `T-1` padding inputs at the beginning, like so:\r\n![image](https://cloud.githubusercontent.com/assets/1304633/21861078/2f96d9ee-d7e8-11e6-9083-f7361751b83e.png)\r\n..so you\'ve now got the first output (dependent on the masked/padding input and the first input) and the second output (dependent on one pad, the first, and the second inputs) as they should be.\r\n\r\nThe only rub here is how to implement it: I don;t think the Lambda nor Flatten layers support masking, so those padding inputs would need to be ""neutral"" data as they can;t be masked out easily.\r\n\r\nHope that makes sense.\r\n\r\nEdit: to make clear: the colored boxes in the diagrams are the window of T items that produce an output. There are 6 boxes (8-3+1) in the first diagram which seems like you have to drop 2 (T-1) of the outputs. The second diagram has all 8 boxes/outputs with padding leading in.', 'If i understand correctly what you are saying, this applies only in the case of stateful RNNs.\r\nI see the problem and i haven\'t thought about it. The idea with the padded data is nice, the only problem is how you create/find/define this ""neutral"" data depending on the problem.\r\n\r\nIn my case i pass each observation (sentence) in one go so i don\'t have to deal with this problem.\r\n\r\n', 'I wouldn\'t say it applies on to stateful, it would apply to any sequence-to-sequence problem (which, as you say, isn\'t what your problem is) where the output sequence length is the same (or longer) as the input sequence length; perhaps frame classification problems or word-by-word translation problems (such as the example in that blog I linked to earlier).\r\n\r\nAnd you\'re right, ""neutral"" data is very problem specific; and I said ""neutral"" just for the fact that masking is not implemented for those two layers as far as I know, otherwise masking would definitely be the best.', 'You are right. \r\nThank you.\r\n\r\nI am closing this.', 'I think it should be better to keep this open for a while. \r\nMaybe someone else will have something interesting to add to the discussion...', '@baziotis @mbollmann  Thanks a lot for your clarification and complete discussion. I am also trying to implement attention. It is mostly same with @mbollmann but with different H matrix. Hope it will work. I will ask questions if I get stuck. ', ""Coming back to it... is there a way to make it work with masking?\r\nThe main problem is that the `attention = Dense(1, activation='tanh')(activations)` layer computes weights for every timestep even for the zero-padded and this results in _a lot_ of wasted time. Most of the times the length of the input series is around 10 but it can go as long as 70. So i have increased max_length in order to cover these long inputs.\r\n\r\nIs there a way to make it work using the existing layers, or with a simple workaround or this can only be done by implementing a custom Layer for attention?"", ""@baiziotis It should work with masking like every other layer, i.e. calculate everything, then apply the mask by multiplication, setting the masked timesteps to zero. I'm not sure what the alternative would be -- computations are usually performed in batches via matrix multiplication, which kind of requires padding to make all samples have the same shape. Or am I misunderstanding something?"", ""There are (were, as of a month ago) several layers that don't support Masking. I think Repeat Vector and Flatten were among those that don't support it.\r\n\r\nEven if Masking was supported, it would still calculate the N=70 timesteps each batch (even all of the 0's).\r\n\r\nI'm not sure exactly what your code is looking like now, but Recurrent networks allow you to pass any length sequence at any time; in keras, I think you have to set the input shape to (None, ...) instead of (70, ...) and you'll be able to feed it a batch with any length of timesteps. Then you can create a random batch, and then pad (or if it works, mask) the samples in the batch up to the length of the longest sequence in the batch, which is probably <70 long.\r\n\r\nex:\r\n```\r\nlen(batch[0]) = 14\r\nlen(batch[1]) = 21\r\nlen(batch[2]) = 10\r\n\r\nbatch[0] -> pad to length 21\r\nbatch[1] -> no padding needed\r\nbatch[2] -> pad to length 21\r\n```"", ""Sorry for the late reply... To be more specific, my problem is text (sentence) classification. The length of the sentences in my dataset follow a normal distribution and i have set the `max_length` to the length of the longest sentence. But this results in a lot of wasted computations as most of the sentences have around 20-30 words. \r\n\r\n@mbollmann  i though that what masking was doing was restricting the RNN from processing the padded timesteps. My problem was not about the performance but the efficiency of the model. The masked timesteps won't matter in the weighted sum (attention) anyway.\r\n\r\n@patyork You are correct that the problem with masking has to do with the `Flatten`, `Permute`, `Repeat` layers. \r\nAs you can see form all the code chunks that i have posted above, i nowhere explicitly define the input shape to the RNN. The problem begins with the embedding layer where i have to fill the `input_length` param.\r\n```python\r\n_embedding = Embedding(\r\n        input_dim=vocab_size,\r\n        output_dim=embedding_size,\r\n        input_length=max_length,\r\n        trainable=trainable,\r\n        mask_zero=masking,\r\n        weights=[embeddings]\r\n    )\r\n```\r\nBut as soon as i set masking to True i get an error that Flatten does not support masking. \r\nAlso i don't understand what input shape i have to set to `None`. To the Embedding?\r\n\r\nEdit: phrasing..."", ""I admit, I haven't used the Embedding layer at all. From the docs though:\r\n>input_length: Length of input sequences, when it is constant. This argument is required if you are going to connect  Flatten then Dense layers upstream (without it, the shape of the dense outputs cannot be computed).\r\n\r\nTheoretically, since it's an LSTM that follows, perhaps input_length could be None? This layer would generally output a shape (None, 70, embedding_size), but (None, None, embedding_size) is a valid shape for an LSTM to take I think. (None, None, ....) is the shape that the TimeDistributed wrapper gives us, batch_size=None and SequenceLength=None."", ""@patyork check this out https://github.com/fchollet/keras/issues/1047#issuecomment-158793786\r\nI can't have _actual_ variable length inputs. i have to use padding, in which case i have to use masking, which doesn't work with Flatter on Repeat."", 'Also from [Embedding](https://keras.io/layers/embeddings/) Layer doc:\r\n\r\n> mask_zero: Whether or not the input value 0 is a special ""padding""\r\n>           value that should be masked out.\r\n>           This is useful for [recurrent layers](recurrent.md) which may take\r\n>           variable length input. If this is `True` then _all subsequent layers\r\n>           in the model need to support masking or an exception will be raised_.\r\n>           If mask_zero is set to True, as a consequence, index 0 cannot be\r\n>           used in the vocabulary (input_dim should equal |vocabulary| + 2).\r\n\r\nUpdate: Just to make sure can someone please clarify something. In this simple case where we use masking:\r\n```python\r\nmodel = Sequential()\r\nmodel.add(embeddings_layer(masking=True))\r\nmodel.add(LSTM(128))\r\nmodel.add(Dense(3, activation=\'softmax\'))\r\nmodel.compile(optimizer=\'adam\', loss=\'categorical_crossentropy\')\r\n```\r\nis the RNN going to process the masked timesteps or not?', ""Yeah, I guess when you're using Embedding the input length can't be variable, and masking isn't supported, so you're back to just setting max_len=70 and padding."", ""Sorry, I'm confused again. (EDIT: resolved below. ) ~~@patyork, when you use `Dense(1)`, doesn't that reduce the full input sequence to a single number? I wouldn't be surprised if this didn't improve anything then, since you're effectively just multiplying everything by a scalar.  Shouldn't it be something like `Dense(max_length)` instead, since we want one number for each input timestep?~~"", ""My understanding is that `Dense(1)` is applied to each timestep independently squeezing the timestep vector to a single number with the tahn activation and then the softmax is applied to `max_length`x`Dense(1)` outputs.\r\nThis model:\r\n```python \r\nactivations = LSTM(64, return_sequences=True, consume_less='mem')(words)\r\nactivations_weights = Dense(1, activation='tanh')(activations)\r\nactivations_weights = Flatten()(activations_weights)\r\nactivations_weights = Activation('softmax')(activations_weights)\r\nactivations_weights = RepeatVector(64)(activations_weights)\r\nactivations_weights = Permute([2, 1])(activations_weights)\r\nactivations_weighted = merge([activations, activations_weights], mode='mul')\r\nsent_representation = Lambda(lambda x: K.sum(x, axis=-2), output_shape=(64,))(activations_weighted)\r\n\r\nprobabilities = Dense(classes)(sent_representation)\r\nprobabilities = Activation('softmax')(probabilities)\r\n``` \r\ngives this:\r\n```\r\n____________________________________________________________________________________________________\r\nlstm_1 (LSTM)                    (None, 50, 64)        67840       embedding_1[0][0]                \r\n____________________________________________________________________________________________________\r\ndense_1 (Dense)                  (None, 50, 1)         65          lstm_1[0][0]                     \r\n____________________________________________________________________________________________________\r\nflatten_1 (Flatten)              (None, 50)            0           dense_1[0][0]                    \r\n____________________________________________________________________________________________________\r\nactivation_1 (Activation)        (None, 50)            0           flatten_1[0][0]                  \r\n____________________________________________________________________________________________________\r\nrepeatvector_1 (RepeatVector)    (None, 64, 50)        0           activation_1[0][0]               \r\n____________________________________________________________________________________________________\r\npermute_1 (Permute)              (None, 50, 64)        0           repeatvector_1[0][0]             \r\n____________________________________________________________________________________________________\r\nmerge_1 (Merge)                  (None, 50, 64)        0           lstm_1[0][0]                     \r\n                                                                   permute_1[0][0]                  \r\n____________________________________________________________________________________________________\r\nlambda_1 (Lambda)                (None, 64)            0           merge_1[0][0]                    \r\n____________________________________________________________________________________________________\r\ndense_2 (Dense)                  (None, 3)             195         lambda_1[0][0]                   \r\n____________________________________________________________________________________________________\r\nactivation_2 (Activation)        (None, 3)             0           dense_2[0][0]                    \r\n```\r\nand changing to:\r\n```python \r\nactivations = LSTM(64, return_sequences=True, consume_less='mem')(words)\r\nactivations_weights = Dense(max_length, activation='tanh')(activations)\r\n...\r\n```\r\ngives the following error:\r\n```\r\nValueError: Only layers of same output shape can be merged using mul mode. Layer shapes: [(None, 50, 64), (None, 2500, 64)]\r\n```\r\n"", ""Sorry, my bad, the weekend made me forget what I'd only looked up last week in the Keras docs...\r\n\r\nI'm still not sure what your concern is regarding the masked timesteps, by the way.  My understanding is that masked timesteps should not affect the result of the computation, and everything else is an implementation detail... but not sure if that is what your questions refer to."", ""@mbollmann So this means that my understanding of how the dense connects to the timesteps is correct?\r\n\r\nRegarding the masking... my question was not about the correctness of the computations. it was about whether it was possible to avoid calculating weights for each of the padded inputs (words). Since i use the Embedding layer it seems that this cannot be done. \r\n\r\nMy last concern (at least for now...) has to do with the `TimeDistributed`. You made it clear before  that for my use case the correct use for the weight calculation is the Dense layer, but reading https://github.com/fchollet/keras/issues/1029 and https://groups.google.com/forum/#!topic/keras-users/suKYo6L1bSI and http://stackoverflow.com/questions/36812351/keras-attention-layer-over-lstm, where these guys use `TimeDistributed(Dense(1))` instead of `Dense(1)` confused me again.\r\n\r\nEdit: in https://github.com/fchollet/keras/issues/1029 they don't talk about attention, just about  TimeDistributedDense and  Dense in general. it's just that different things where written and made me have doubts. "", '@baziotis How the mask is handled (and therefore, whether masked timesteps are computed and then dropped, or not computed to begin with) depends on the implementation of each layer, and I\'m not totally sure how RNNs handle it (the code is in the `rnn` function of `theano_backend.py` and `tensorflow_backend.py`, respectively).  However, the computation of *n* padded sequences with equal length can be much faster than the computation of *n* sequences with differing lengths, and the result will be the same in any case, so I\'d say it\'s nothing to worry about.\r\n\r\nRegarding the `Dense` layer, after reading the docs once more and doing some experimentation of my own, I actually don\'t see how `Dense` and `TimeDistributed(Dense)` differ at all -- they appear to do the exact same thing:\r\n\r\n```\r\nfrom keras.layers import Input, Dense, TimeDistributed\r\nfrom keras.models import Model\r\nimport numpy as np\r\n\r\ninput_shape = (10, 128)\r\ninputs = Input(shape=input_shape)\r\nlayer = Dense(64)\r\nx = np.array([np.random.random(input_shape)])\r\n\r\n# without TimeDistributed\r\noutput = layer(inputs)\r\nmodel = Model(input=inputs, output=output)\r\nmodel.compile(optimizer=\'sgd\', loss=\'mse\')\r\ny1 = model.predict(x)\r\n\r\n# with TimeDistributed\r\noutput = TimeDistributed(layer)(inputs)\r\nmodel = Model(input=inputs, output=output)\r\nmodel.compile(optimizer=\'sgd\', loss=\'mse\')\r\ny2 = model.predict(x)\r\n\r\n# => ""True""\r\nprint(np.array_equal(y1, y2))\r\n```\r\n\r\nThis confuses me a lot.  Did the functionality of `Dense` change at some point?  Does it make a difference for training?', 'Dense began to handle the Time dimension about a month ago. TimeDistributedDense is now deprecated and TimeDistributed(Dense) unnecessary.\n\n-----Original Message-----\nFrom: ""Marcel Bollmann"" <notifications@github.com>\nSent: \u200e1/\u200e16/\u200e2017 8:21 AM\nTo: ""fchollet/keras"" <keras@noreply.github.com>\nCc: ""Pat York"" <pat.york@nevada.unr.edu>; ""Mention"" <mention@noreply.github.com>\nSubject: Re: [fchollet/keras] How to add Attention on top of a Recurrent Layer(Text Classification) (#4962)\n\n@baziotis How the mask is handled (and therefore, whether masked timesteps are computed and then dropped, or not computed to begin with) depends on the implementation of each layer, and I\'m not totally sure how RNNs handle it (the code is in the rnn function of theano_backend.py and tensorflow_backend.py, respectively). However, the computation of n padded sequences with equal length can be much faster than the computation of n sequences with differing lengths, and the result will be the same in any case, so I\'d say it\'s nothing to worry about.\nRegarding the Dense layer, after reading the docs once more and doing some experimentation of my own, I actually don\'t see how Dense and TimeDistributed(Dense) differ at all -- they appear to do the exact same thing:\nfrom keras.layers import Input, Dense, TimeDistributed\nfrom keras.models import Model\nimport numpy as np\n\ninput_shape = (10, 128)\ninputs = Input(shape=input_shape)\nlayer = Dense(64)\nx = np.array([np.random.random(input_shape)])\n\n# without TimeDistributed\noutput = layer(inputs)\nmodel = Model(input=inputs, output=output)\nmodel.compile(optimizer=\'sgd\', loss=\'mse\')\ny1 = model.predict(x)\n\n# with TimeDistributed\noutput = TimeDistributed(layer)(inputs)\nmodel = Model(input=inputs, output=output)\nmodel.compile(optimizer=\'sgd\', loss=\'mse\')\ny2 = model.predict(x)\n\n# => ""True""\nprint(np.array_equal(y1, y2))\nThis confuses me a lot. Did the functionality of Dense change at some point? Does it make a difference for training?\n—\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.', 'So by using Dense the _same_ or _different_ weights are applied to each timestep? See https://github.com/fchollet/keras/issues/1029#issuecomment-270289453.\r\n\r\nBecause if i am just scaling the timesteps there is no attension...', ""A quick check shows the same weight shapes are present in Dense() and TimeDistributed(Dense), meaning the same weights of shape (48, 1) are used at each time step. @mbollmann's checks above imply that as well.\r\n\r\nIf you want/think it is necessary, you'd need to apply a Flatten() before the Dense, I guess.\r\n\r\nI admit, I still don't think I fully understand attention, haha. I should have tried it back when we were first discussing it."", 'If i downgrade to a previous version is there a layer which will provide me with the desired functionality (Dense() or TimeDistributed(Dense))? ', 'Also: \r\n```python\r\nclass TimeDistributed(Wrapper):\r\n    """"""This wrapper allows to apply a layer to every\r\n    temporal slice of an input.\r\n\r\n    The input should be at least 3D,\r\n    and the dimension of index one will be considered to be\r\n    the temporal dimension.\r\n\r\n    Consider a batch of 32 samples, where each sample is a sequence of 10\r\n    vectors of 16 dimensions. The batch input shape of the layer is then `(32, 10, 16)`\r\n    (and the `input_shape`, not including the samples dimension, is `(10, 16)`).\r\n\r\n    --> You can then use `TimeDistributed` to apply a `Dense` layer to each of the 10 timesteps, _independently_:\r\n    ```python\r\n        # as the first layer in a model\r\n        model = Sequential()\r\n        model.add(TimeDistributed(Dense(8), input_shape=(10, 16)))\r\n        # now model.output_shape == (None, 10, 8)\r\n\r\n        # subsequent layers: no need for input_shape\r\n        model.add(TimeDistributed(Dense(32)))\r\n        # now model.output_shape == (None, 10, 32)\r\n...\r\n```\r\n\r\n@fchollet can you please clear the confusion? which one applies different weights to each timestep?', ""Thanks @patyork, I didn't know that! That basically renders everything I said above invalid...\r\n\r\n@baziotis You're definitely applying the same weights to each timestep, either way. That's what my code above demonstrates.\r\n\r\nIn that case, flattening the input before the Dense layer (as @patyork suggested) would be closer to what I had in mind -- that would make every index at every timestep have an individual weight:\r\n\r\n```\r\nactivations = LSTM(64, return_sequences=True)(words)\r\nactivations_weights = Flatten()(activations)\r\nactivations_weights = Dense(max_length, activation='tanh')(activations_weights)\r\nactivations_weights = Activation('softmax')(activations_weights)\r\nactivations_weights = RepeatVector(64)(activations_weights)\r\nactivations_weights = Permute([2, 1])(activations_weights)\r\n```\r\n\r\n(Untested though.)"", ""If you want to downgrade, you'd have to clone keras from some time [before this commit](https://github.com/fchollet/keras/commit/18e5b75f67ed640ff207ae52b425e9e3c0c293be) from Dec 19th and install.\r\n\r\n((Personally, I think it's doing what you want. The Dense layer outputs 70 items (one for each timestep, using the same weights); softmax is applied, making some timesteps more important, a bit of repeating/permuting and it applies the weights to the values from the activations. This is exactly what the other people were doing with the TimeDist wrapper on it.))"", ""> ((Personally, I think it's doing what you want. The Dense layer outputs 70 items (one for each timestep, using the same weights); softmax is applied, making some timesteps more important, a bit of repeating/permuting and it applies the weights to the values from the activations. This is exactly what the other people were doing with the TimeDist wrapper on it.))\r\n\r\nBut if the weighting of a timestep *only* depends on *that timestep's input* and nothing else, I still don't understand how this could learn anything that just another `Dense` activation (without all that softmax and multiplication in-between) couldn't.  It might be my insufficient understanding of (this particular type of) attention, but I guess that's unrelated to Keras now and we won't solve that particular problem here..."", ""1 - @patyork   So if i downgrade to 1.1.2 and then to this:\r\n```python\r\nactivations = LSTM(64, return_sequences=True, consume_less='mem')(words)\r\nactivations_weights = TimeDistributed(Dense(1, activation='tanh'))(activations) # TimeDistributed\r\nactivations_weights = Flatten()(activations_weights)\r\nactivations_weights = Activation('softmax')(activations_weights)\r\nactivations_weights = RepeatVector(64)(activations_weights)\r\nactivations_weights = Permute([2, 1])(activations_weights)\r\nactivations_weighted = merge([activations, activations_weights], mode='mul')\r\nsent_representation = Lambda(lambda x: K.sum(x, axis=-2), output_shape=(64,))(activations_weighted)\r\n```\r\nwill i have the desired behavior? \r\n\r\n2 - i don't understand why Flatten will do the same thing. It will just convert the 2D tensor with the timesteps to a big 1D tensor and then by applying the Dense i will have lost the difference between the timesteps.\r\nHere is what i mean:\r\n```\r\n____________________________________________________________________________________________________\r\nlstm_1 (LSTM)                    (None, 50, 64)        67840       embedding_1[0][0]                \r\n____________________________________________________________________________________________________\r\nflatten_1 (Flatten)              (None, 3200)          0           lstm_1[0][0]                     \r\n____________________________________________________________________________________________________\r\ndense_1 (Dense)                  (None, 50)            160050      flatten_1[0][0]                  \r\n____________________________________________________________________________________________________\r\nactivation_1 (Activation)        (None, 50)            0           dense_1[0][0]                    \r\n____________________________________________________________________________________________________\r\nrepeatvector_1 (RepeatVector)    (None, 64, 50)        0           activation_1[0][0]               \r\n____________________________________________________________________________________________________\r\npermute_1 (Permute)              (None, 50, 64)        0           repeatvector_1[0][0]             \r\n____________________________________________________________________________________________________\r\nmerge_1 (Merge)                  (None, 50, 64)        0           lstm_1[0][0]                     \r\n                                                                   permute_1[0][0]                  \r\n____________________________________________________________________________________________________\r\nlambda_1 (Lambda)                (None, 64)            0           merge_1[0][0]                    \r\n____________________________________________________________________________________________________\r\n```\r\n"", ""@baziotis\r\n\r\n1. No, `TimeDistributed(Dense)` hasn't changed and will do the same thing in 1.1.2.\r\n\r\n2. Yes, that's exactly what I was trying to achieve: having a different weight for each index at each timestep, which is exactly what happens if you flatten them into one big 1D tensor. Afterwards, we reshape the result to a 2D tensor again with essentially one weight per timestep. This is what I presumed the cited literature was doing, but I may be wrong (and in that case, confused, as I explained in my previous comment)."", ""It's just blind leading blind here. I haven't the foggiest about what the desired behavior is so I won't comment.\r\n\r\n- All I can say is Dense() == TimeDistributedDense() == TimeDistributed(Dense()) as of December 19th; read any code/discussions previous to that date very carefully.\r\n- You can downgrade Keras if you don't like the Dense behavior\r\n"", 'First of all, i am sorry if i wasn\'t clear and i thank you for helping me so far.\r\n\r\nWhat i want is just: \r\n1 - get a scalar weight for each timestep\r\n2 - do a weighted sum of the timesteps in order to get a ""better"" representation of the input (instead of just getting the last timestep of just a simple average of the timesteps)\r\n\r\nWithout saying anything else and confuse you, from what i just said, how would you do it?', ""@baziotis Totally depends on how that scalar weight should be calculated.\r\n\r\nIf the weight of one timestep should be calculated from only that same timestep's input -- your comment here https://github.com/fchollet/keras/issues/4962#issuecomment-272888992\r\n\r\nIf the weight of one timestep should be calculated from *all timesteps's* input -- my comment here:\r\nhttps://github.com/fchollet/keras/issues/4962#issuecomment-272919449\r\n\r\nThat's how I see it, and further than that, I agree that it's just blind leading blind now (and we'd need someone with experience with that particular type of attention to weigh in)."", '@mbollmann just to be on the same page. When you said a hundred posts back that you have in mind a different kind of attention did you have in mind the calculation on your comment here: https://github.com/fchollet/keras/issues/4962#issuecomment-272919449 ?', 'At least this way there is no ambiguity about what is going on... ', ""@baziotis No, I'm working on attention in an encoder/decoder architecture.  I have an encoder RNN that provides the activations (just as in your example), and a decoder RNN which calculates attention weights based on the input activations *and* its own current hidden state.  I guess it's not really applicable to your scenario."", ""@mbollmann  i imagine you mean something like [bahdanau et al. 2014](https://arxiv.org/pdf/1409.0473v7.pdf). What i want is up to a point what you do during the encoding phase. See equations (5) and (6).\r\n\r\n![bahdanau et al 2014](https://cloud.githubusercontent.com/assets/5629093/21995436/917dd696-dc2d-11e6-8417-b5218f122d89.PNG)\r\n\r\nThe context vector is the weighted sum of the hidden states (timesteps). right? How do you compute the weights for each h? Whether it is h or [h->;h<-] for the BRNN doesn't make a difference.\r\n"", 'I have mistaken the outputs for the hidden states... This one does a weighted sum of the hidden states, not the outputs. ', '@baziotis I compute the attention weights from a combination of the `h_{1..T}` and `s_{t-1}`, essentially following [this Xu et al. paper, Sec. 3.1.2](https://arxiv.org/pdf/1502.03044.pdf), implemented in this gist I also linked to a few hundred comments back: https://gist.github.com/mbollmann/ccc735366221e4dba9f89d2aab86da1e', ""@mbollmann thanks for the code!\r\n\r\n1) So keras doesn't offer the ability to get the intermediate hidden states instead of the outputs? I have to subclass / extend `Recurrent` or `LSTM`/`GRU` in order to do so?\r\n\r\n2) Also is this the distinction between soft and hard attention? Soft=using hidden states, Hard=using outputs?"", '@mbollmann I was looking at your other gists and i found [this](https://gist.github.com/mbollmann/29fd21931820c64095617125824ea246) :)))\r\nThis maybe exactly what i need. By reading the example i see [here](https://gist.github.com/mbollmann/29fd21931820c64095617125824ea246#file-hidden_state_lstm-py-L105) that you return the hidden states. \r\nThis means i can use them instead of the outputs for calculating the weights. Is that correct??', ""@baziotis Erm, no, there seems to be some kind of fundamental misunderstanding here.\r\n\r\n1. The point is that Keras processes input layer-by-layer, i.e. first all timesteps of the first layer, then all timesteps of the second, and so on... the point of my AttentionLSTM is that I want to calculate attention weights for *each timestep* in the *same* layer, based on the hidden state of *the very same* AttentionLSTM after each timestep. There's no way you could do this without writing a custom layer -- I can't perform the calculations in a separate layer before calling an LSTM since they're supposed to depend on the hidden states of that LSTM, and modify the behaviour of that very same LSTM.\r\n\r\n2. Not at all. Hard attention uses a probability distribution. I'm actually not too familiar with it, but there's an explanation in the Xu et al. paper and also on various deep learning blogs.\r\n\r\n3. Re the HiddenStateLSTM gist: The main point here is really to have inputs that set the hidden states. In Keras, the outputs of an LSTM *are* the hidden states -- there's no difference!"", '@mbollmann Thanks for clearing things.\r\n1) So this means `hidden states` == `outputs` == `return_sequences=True`?\r\n\r\nEdit: \r\n2) So in keras when we stack two RNNs the second RNN takes as `input` the `hidden` states of the previous RNN?\r\n\r\n3) From  [bahdanau et al. 2014](https://arxiv.org/pdf/1409.0473v7.pdf) in this image:\r\n![hidden](https://cloud.githubusercontent.com/assets/5629093/22015618/7e942f3e-dcab-11e6-959c-d9ecd92c5929.PNG)\r\nwhat keras returns with `return_sequences=True` is the `hi`s?', ""@baziotis Yes to all.\r\n\r\nI can really recommend studying the Keras code for these things, too; it's a little daunting at first but very insightful.  In this case, you can read it off the `step` functions of the SimpleRNN/GRU/LSTM layers, which return a tuple `(output, states)`, and all of them basically return the same thing for both."", '@mbollmann Will do. Thanks again!  :)', 'So i moved all the attention stuff in a Custom Layer..\r\n```python\r\nclass Attention(Layer):\r\n    def __init__(self, **kwargs):\r\n        """"""\r\n        Attention operation for temporal data.\r\n        # Input shape\r\n            3D tensor with shape: `(samples, steps, features)`.\r\n        # Output shape\r\n            2D tensor with shape: `(samples, features)`.\r\n        :param kwargs:\r\n        """"""\r\n        self.supports_masking = True\r\n        self.init = initializations.get(\'glorot_uniform\')\r\n        super(Attention, self).__init__(**kwargs)\r\n\r\n    def build(self, input_shape):\r\n        assert len(input_shape) == 3\r\n        self.W = self.init((input_shape[-1],), name=\'{}_W\'.format(self.name))\r\n        self.b = K.ones((input_shape[1],), name=\'{}_b\'.format(self.name))\r\n        self.trainable_weights = [self.W, self.b]\r\n\r\n        super(Attention, self).build(input_shape)\r\n\r\n    def call(self, x, mask=None):\r\n        eij = K.tanh(K.dot(x, self.W) + self.b)\r\n        ai = K.exp(eij)\r\n        weights = ai / K.sum(ai, axis=1).dimshuffle(0, \'x\')\r\n        weighted_input = x * weights.dimshuffle(0, 1, \'x\')\r\n        return weighted_input.sum(axis=1)\r\n\r\n    def get_output_shape_for(self, input_shape):\r\n        return input_shape[0], input_shape[-1]\r\n```\r\nHere is a super simple example:\r\n```python\r\n_input = Input(shape=[max_length], dtype=\'int32\')\r\nwords = embeddings_layer(max_length=max_length, embeddings=embeddings,\r\n                         trainable=False, masking=False, scale=False, normalize=False)(_input)\r\n\r\nactivations = LSTM(64, return_sequences=True, consume_less=\'mem\')(words)\r\nsentence = Attention()(activations)\r\n\r\nprobabilities = Dense(classes)(sentence)\r\nprobabilities = Activation(\'softmax\')(probabilities)\r\n\r\nmodel = Model(input=_input, output=probabilities)\r\nmodel.compile(optimizer=Adam(clipnorm=5.), loss=\'categorical_crossentropy\')\r\n```\r\nIt works but `masking=False`. I added `self.supports_masking = True` but when enabling masking in the Embedding Layer then the final Dense Layer given as error:\r\n```\r\nValueError: Layer dense_1 does not support masking, but was passed an input_mask: Elemwise{neq,no_inplace}.0\r\n```\r\nThere are almost no examples ([this doesn\'t really show much](https://keras.io/layers/writing-your-own-keras-layers/)) on creating Custom Layers and the few blog posts that i have found are too simplistic. Fortunately my layer doesn\'t have to do much. \r\n\r\nIn my case what do i have to do to support masking?', ""@baziotis But it's not your layer that's the problem, it's the Dense layer (according to the error message). Does it work with `TimeDistributed(Dense)`? If so, that would seem like an oversight when they adapted Dense for 3d inputs maybe..."", '@mbollmann Why use do i have to use `TimeDistributed(Dense)`? Look at the shape of the final Dense input. It is just an 1D Tensor.... The attention layer just compresses the timesteps to a single vector.\r\n```\r\nLayer (type)                     Output Shape          Param #     Connected to                     \r\n====================================================================================================\r\ninput_1 (InputLayer)             (None, 50)            0                                            \r\n____________________________________________________________________________________________________\r\nembedding_1 (Embedding)          (None, 50, 300)       150000900   input_1[0][0]                    \r\n____________________________________________________________________________________________________\r\nlstm_1 (LSTM)                    (None, 50, 100)       160400      embedding_1[0][0]                \r\n____________________________________________________________________________________________________\r\nattention_1 (Attention)          (None, 100)           150         lstm_1[0][0]                     \r\n____________________________________________________________________________________________________\r\ndense_1 (Dense)                  (None, 3)             303         attention_1[0][0]                \r\n____________________________________________________________________________________________________\r\nactivation_1 (Activation)        (None, 3)             0           dense_1[0][0]                    \r\n====================================================================================================\r\n```', ""Also i tried `masking=True`,  just the LSTM with `return_sequences=False` and then the final Dense and it works. So this means that my Layer should be doing something with the mask but i don't know what..."", ""@baziotis Sorry, my bad. I *think* your layer should probably discard the mask, since you can't logically have masked timesteps after squashing the input to 2d. Look into overriding `compute_mask()` of your layer to return `None`. I'm not 100% sure of all the consequences of this, though."", ""@mbollmann i was about to post that! Take a look in what i did.\r\n```python\r\nclass Attention(Layer):\r\n    def __init__(self, **kwargs):\r\n        self.supports_masking = True\r\n        self.init = initializations.get('glorot_uniform')\r\n        super(Attention, self).__init__(**kwargs)\r\n\r\n    def build(self, input_shape):\r\n        assert len(input_shape) == 3\r\n        self.W = self.init((input_shape[-1],), name='{}_W'.format(self.name))\r\n        self.b = K.ones((input_shape[1],), name='{}_b'.format(self.name))\r\n        self.trainable_weights = [self.W, self.b]\r\n\r\n        super(Attention, self).build(input_shape)\r\n\r\n    def compute_mask(self, input, input_mask=None):\r\n        return None\r\n\r\n    def call(self, x, mask=None):\r\n        eij = K.tanh(K.dot(x, self.W) + self.b)\r\n        ai = K.exp(eij)\r\n        weights = ai / K.sum(ai, axis=1).dimshuffle(0, 'x')\r\n        weighted_input = x * weights.dimshuffle(0, 1, 'x')\r\n        return weighted_input.sum(axis=1)\r\n\r\n    def get_output_shape_for(self, input_shape):\r\n        return input_shape[0], input_shape[-1]\r\n```\r\nI tried that and it works. But i need you to tell me your opinion. I read the [compute_mask(x, mask)](https://github.com/fchollet/keras/blob/master/keras/engine/topology.py#L650)  function of the Layer i override and it looks that what was happening, was that my layer was passing the mask to the Dense, which makes no sense in my case.\r\n\r\nIs this correct?\r\n\r\nAlso should i make any change in my call function?\r\n```python\r\n    def call(self, x, mask=None):\r\n        a = K.tanh(K.dot(x, self.W) + self.b)\r\n        ai = K.exp(K.dot(a, self.u))\r\n        weights = ai / K.sum(ai, axis=1).dimshuffle(0, 'x')\r\n        weighted_input = x * weights.dimshuffle(0, 1, 'x')\r\n        return weighted_input.sum(axis=1)\r\n```\r\nUpdate: is there a way to debug the call function in runtime? How can i tell what is the value of x and verify what i am doing is correct? I tried to set `device=cpu` and set a breakpoint but this doen't work (this must have to do with the function being symbolic - i don't know exactly how this works...). My main concern is if i have to change anything in the call(). Other than that, things look good and i think i am getting better results now...\r\n\r\n@patyork i would like to also hear your opinion."", ""@baziotis Yes to the `compute_mask()` thing, that's what I was getting at.\r\n\r\nAs to your `call()` function, I'm not sure what the point of the additional `K.dot(a, self.u)` operation is.  For a straight equivalence to your previous code, it shouldn't be there I think. But as I'm sure you know by now, there are many ways to approach the same basic idea... :)"", ""@mbollmann oops that is from another Layer :P (look at [Yang et al.](https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf)), i made i mistake... This is how it is:\r\n\r\n```python\r\n    def call(self, x, mask=None):\r\n        eij = K.tanh(K.dot(x, self.W) + self.b)\r\n        ai = K.exp(eij)\r\n        weights = ai / K.sum(ai, axis=1).dimshuffle(0, 'x')\r\n        weighted_input = x * weights.dimshuffle(0, 1, 'x')\r\n        return weighted_input.sum(axis=1)\r\n```\r\nMy concern is masking. If the masking thing is correct i think we are good.\r\n\r\nUpdate 1: Also regarding the zeroed x's how can i calculate the `K.dot(x, self.W)` only on the non zeroed x's?\r\nUpdate 2: What do you think of `K.dot(K.not_equal(x, 0), self.W)`? ~~Would that be correct?~~ it returns a bool thensor so it wouldn't but i think it would be better if somehow i calculated the product only on the non padded timesteps."", ""@baziotis curious - did you finally get this attention mechanism working on your classification task? I've been reading through but I haven't seen any one mention that it worked qualitatively :) only that it worked (as in the code ran!).\r\n\r\n/cc @patyork and @mbollmann - have you guys posted an attention layer anywhere as well?"", ""@viksit i have not settled to a final version of the layer (i am having some probelms with masking) but i'll tell you what i have observed so far: I see a clear but _not big_ improvement. \r\n\r\nThe most important thing i have observed is related to the series length. For small lengths no attention and attention give about the same results. But as i increase the length, the RNN without attention starts degrading (not able to remember very long term dependencies) while the RNN+Attention gives the same results. So this is a big plus i think, and especially in my case where i have sentences that go up to 50 words.  Basically this is the exact observation as in [Bahdanau et al.](https://arxiv.org/pdf/1409.0473v7.pdf) - see figure 2.\r\n\r\nMy concern now is if this is correct:\r\n```python\r\n    def call(self, x, mask=None):\r\n        eij = K.tanh(K.dot(x, self.W))\r\n        if self.bias:\r\n            eij += self.b\r\n\r\n        ai = K.exp(eij)\r\n        \r\n        # apply mask\r\n        if mask is not None:\r\n            ait = K.cast(mask, 'float32')\r\n            ait *= mask\r\n\r\n        weights = ai / K.sum(ai, axis=1).dimshuffle(0, 'x')\r\n        weighted_input = x * weights.dimshuffle(0, 1, 'x')\r\n        return weighted_input.sum(axis=1)\r\n```\r\nI tried applying the mask directly on x, which was what i thought i should do and i got a dimension mismatch error. I am confused about the dimensions. But this works and the networks trains fine.\r\nI see no obvious difference after including the mask in the calculation but maybe i am doing something wrong.\r\n\r\nI will post the 2 versions of my attention layer when i am finished."", '@baziotis `ait *= mask` just multiplies the mask with itself, no? I.e. your code is not actually using it anywhere? ', ""@mbollmann  you are right, this is so embarrassing...  \r\n\r\nI've been staring at the screen for hours and i missed it.  No wonder i was seeing no difference in the results.\r\nThis is what i was meaning to do:\r\n```python\r\n    def call(self, x, mask=None):\r\n        eij = K.tanh(K.dot(x, self.W))\r\n        if self.bias:\r\n            eij += self.b\r\n\r\n        ai = K.exp(eij)\r\n\r\n        # apply mask\r\n        if mask is not None:\r\n            mask = K.cast(mask, 'float32')\r\n            ai *= mask\r\n\r\n        weights = ai / K.sum(ai, axis=1).dimshuffle(0, 'x')\r\n        weighted_input = x * weights.dimshuffle(0, 1, 'x')\r\n        return weighted_input.sum(axis=1)\r\n```\r\nAnd what is happening is that after a given point i get NaN's in the loss.\r\n\r\nUpdate:\r\nAs it was pointed out [here](https://groups.google.com/d/msg/keras-users/IWK9opMFavQ/SOHrz77ZAQAJ) the problem with the NaN's has to do with the way i did the softmax calculation (with the exp). I replaced it with the K.softmax.\r\n```python\r\n    def call(self, x, mask=None):\r\n        eij = K.tanh(K.dot(x, self.W))\r\n        if self.bias:\r\n            eij += self.b\r\n\r\n        # apply mask\r\n        if mask is not None:\r\n            mask = K.cast(mask, 'float32')\r\n            eij *= mask\r\n\r\n        a = K.softmax(eij)\r\n        a = K.expand_dims(a)\r\n        weighted_input = x * a\r\n        return K.sum(weighted_input, axis=1)\r\n```\r\nThis is stable and the network trains. But this problem was caused only after i applied the mask. Is this the right way to apply the mask?"", 'Just an update. I have ended up with these two Layers:\r\nAttention: https://gist.github.com/cbaziotis/6428df359af27d58078ca5ed9792bd6d\r\nAttentionWithContext: https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2\r\n\r\nAny comments are welcome...\r\n', 'Hey @baziotis :) Thank you so much for your work. This is just what I was looking for. I have some trouble getting your layers to work on my machine. Using Theano they compile, but in my model and system using Theano as a backend is too slow.\r\n\r\nTrying to run it with Tensorflow results in the following crash:\r\n\r\n```\r\n  File ""~/.virtualenvs/master/lib/python3.5/site-packages/keras/models.py"", line 327, in add\r\n    output_tensor = layer(self.outputs[0])\r\n  File ""~/.virtualenvs/master/lib/python3.5/site-packages/keras/engine/topology.py"", line 569, in __call__\r\n    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)\r\n  File ""~/.virtualenvs/master/lib/python3.5/site-packages/keras/engine/topology.py"", line 632, in add_inbound_node\r\n    Node.create_node(self, inbound_layers, node_indices, tensor_indices)\r\n  File ""~/.virtualenvs/master/lib/python3.5/site-packages/keras/engine/topology.py"", line 164, in create_node\r\n    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))\r\n  File ""~/code/rorschach/prediction/layer/attention_layer.py"", line 66, in call\r\n    eij = K.tanh(K.dot(x, self.W))\r\n  File ""~/.virtualenvs/master/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py"", line 799, in dot\r\n    y_permute_dim = [y_permute_dim.pop(-2)] + y_permute_dim\r\nIndexError: pop index out of range\r\n```\r\n\r\nCurrently using Keras 1.2.0 and I\'ve tried both Tensorflow 0.11.0 and 0.12.1 without luck.', ""@OptimusCrime  I am using Theano as a backend and have not experienced any slowdowns. Are you sure that the reason for the slowdowns is the attention layers?\r\n\r\nBTW, in tensorflow if you are using `AttentionWithContext` Layer the dot doesn't work, as it is pointed [here](https://groups.google.com/d/msg/keras-users/IWK9opMFavQ/SOHrz77ZAQAJ), so what you have to do is:\r\n\r\nReplace this:\r\n```python\r\nuit = K.dot(x, self.W)\r\n\r\nif self.bias:\r\n    uit += self.b\r\n\r\nuit = K.tanh(uit)\r\nait = K.dot(uit, self.u) # replace this\r\n\r\na = K.exp(ait)\r\n```\r\nWith this:\r\n```python\r\nuit = K.dot(x, self.W)\r\n\r\nif self.bias:\r\n    uit += self.b\r\n\r\nuit = K.tanh(uit)\r\n\r\nmul_a = uit  * self.u # with this\r\nait = K.sum(mul_a, axis=2) # and this\r\n\r\na = K.exp(ait)\r\n```\r\n\r\nAlso please look at the updated gists as i have updated them with a fix."", 'Hello. Thanks for the code @cbaziotis.\r\nI was having the same problem and now it works with no errors. But there is still a problem with the output dimensions. I tried this:\r\n\r\n```\r\ninputs = [[[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[1,2,3],[4,5,6],[7,8,9],[10,11,12]],[[10,20,30],[40,50,60],[70,80,90],[100,110,120]]]\r\n\r\nhidden_size = 6\r\nsent_size = 4\r\ndoc_size = 3\r\n\r\nmodel = Sequential()\r\nmodel.add(LSTM(hidden_size,input_shape = (sent_size,doc_size),return_sequences = True))\r\nmodel.add(AttentionWithContext())\r\n\r\nprint ""First layer:""\r\nintermediate_layer_model = Model(input=model.input,output=model.layers[0].output)\r\nprint intermediate_layer_model.predict(inputs)\r\nprint """"\r\nprint ""Second layer:""\r\nintermediate_layer_model = Model(input=model.input,output=model.layers[1].output)\r\nprint intermediate_layer_model.predict(inputs)\r\n```\r\n\r\n\r\nand it is giving me this result:\r\n```\r\nFirst layer:\r\n[[[ 0.          0.          0.          0.          0.          0.        ]\r\n  [ 0.          0.          0.          0.          0.          0.        ]\r\n  [ 0.          0.          0.          0.          0.          0.        ]\r\n  [ 0.          0.          0.          0.          0.          0.        ]]\r\n\r\n [[ 0.04093511 -0.00982957 -0.          0.25834009 -0.39604828 -0.169927  ]\r\n  [ 0.         -0.         -0.          0.68305802 -0.73000526 -0.1271846 ]\r\n  [ 0.         -0.         -0.          0.79648596 -0.83882242 -0.        ]\r\n  [ 0.         -0.         -0.          0.79895407 -0.79928428 -0.        ]]\r\n\r\n [[ 0.          0.         -0.          0.23120573 -0.76159418 -0.32464135]\r\n  [ 0.          0.         -0.          0.76159418 -0.76159418 -0.        ]\r\n  [ 0.          0.         -0.          0.76159418 -0.76159418 -0.        ]\r\n  [ 0.          0.         -0.          0.76159418 -0.76159418 -0.        ]]]\r\n\r\nSecond layer:\r\n[[ 0.          0.          0.          0.          0.          0.        ]\r\n [ 0.00770082 -0.00184916  0.          0.66687739 -0.71645236 -0.06456213]\r\n [ 0.          0.          0.          0.68619043 -0.76159418 -0.04615331]]\r\n```\r\n\r\nShouldn\'t the Attention output have dimensions (samples, features), that this case should be (3,4)?\r\n\r\n\r\n', 'No. The attention layer all that does is to compute a weighted sum of the outputs of the RNN.\r\nIn your case for example:\r\n1. the first layer outputs 3 (4,6) tensors.\r\n2. The weighted sum of a (4,6) tensor will be a (1,6) tensor (a 6 dimensional vector). We compress each column _not_ each row.\r\n3. Then at the second layer you have a (3,6) tensor, which is correct.', ""Hi guys,\r\nI have the following model to correct input language sentence in a one-hot vector that is not in the standard English vocabulary. How can I introduces Attention mechanism to the model, so that the output will be the relevant information that will give the sentence a meaning\r\n\r\nhiddenStateSize = 256\r\nhiddenLayerSize = 256\r\nmodel = Sequential()\r\n# The output of the LSTM layer are the hidden states of the LSTM for every time step. \r\nmodel.add(GRU(hiddenStateSize, return_sequences = True, input_shape=(maxSequenceLength, len(char_2_id))))\r\nmodel.add(Dense(1, activation='tanh')\r\nmodel.add(Flatten())\r\nmodel.add(Activation('softmax'))\r\n###\r\nI got stuck from this moment\r\n###\r\nmodel.add(TimeDistributed(Dense(hiddenLayerSize)))\r\nmodel.add(TimeDistributed(Activation('relu'))) \r\nmodel.add(TimeDistributed(Dense(len(char_2_id))))  \r\nmodel.add(TimeDistributed(Activation('softmax')))\r\n# ----SGD-------\r\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\r\n# define model\r\n%time model.compile(loss='categorical_crossentropy', optimizer = sgd , metrics=['accuracy'])\r\n\r\n"", ""@cbaziotis I've been your `AttentionWithContext` code at https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2 \r\n\r\nFor some reason the output shape is wrong. See the `model.summary()` output below:\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\ntext_input (InputLayer)      (None, 100)               0\r\n_________________________________________________________________\r\nembedding_1 (Embedding)      (None, 100, 100)          2361000\r\n_________________________________________________________________\r\nmasking_1 (Masking)          (None, 100, 100)          0\r\n_________________________________________________________________\r\nbidirectional_1 (Bidirection (None, 100, 256)          175872\r\n_________________________________________________________________\r\nattention_with_context_1 (At (None, 100, 256)          66048\r\n_________________________________________________________________\r\noutput (Dense)               (None, 100, 34)           8738\r\n=================================================================\r\n```\r\n\r\nShouldn't `attention_with_context_1` have an output shape of `(None, 256)` as listed in your documentation for your function? It should output a 2D tensor of shape `(samples, features)`. The peculiar thing is when I retrieve the layer and get the output it shows the correct shape:\r\n```\r\n>>> att_layer.output\r\n<tf.Tensor 'attention_with_context_1/Sum_2:0' shape=(?, 256) dtype=float32>\r\n>>> # but this returns the wrong shape\r\n>>> att_layer.output_shape\r\n(None, 100, 256)\r\n```\r\nAny ideas?"", '@cbaziotis Found the issue. Turns out if you write a custom layer and it modifies the input shape, you need a `compute_output_shape` method. See [here](https://gist.github.com/nigeljyng/37552fb4869a5e81338f82b338a304d3) for a fork that now works.\r\n\r\n```\r\n>>> model.summary()\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\ntext_input (InputLayer)      (None, 100)               0\r\n_________________________________________________________________\r\nembedding_2 (Embedding)      (None, 100, 100)          2361000\r\n_________________________________________________________________\r\nmasking_4 (Masking)          (None, 100, 100)          0\r\n_________________________________________________________________\r\nbidirectional_5 (Bidirection (None, 100, 256)          175872\r\n_________________________________________________________________\r\nattention_with_context_4 (At (None, 256)               66048\r\n_________________________________________________________________\r\noutput (Dense)               (None, 34)                8738\r\n=================================================================\r\n```', 'OK I did not read the whole discussion, but Zhou, Peng, et al. says H is a matrix where every column has the dimensionality of a word vector. Why is that? I think it should be the units of LSTM layer, which can be chosen to be the same as word vector dimensionality, of course, but it does not have to be so?', 'Hey, have a look at this repo:\r\n\r\nhttps://github.com/philipperemy/keras-attention-mechanism\r\n\r\nIt shows how to build an attention module of top of a recurrent layer.\r\n\r\nThanks', '@philipperemy I tested your approach. Indeed you can learn an attention vector, but testing across a suite of contrived problems, I see the model is just as skillful as a plan Dense + LSTM combination. Attention is an optimization that should lift skill or decrease training time for the same skill. Perhaps you have an example where your approach is more skillful than a straight Dense + LSTM setup with the same resources?\r\n\r\n@cbaziotis After testing, I believe your attention method is something new/different inspired by Bahdanau, et al. [1]. It does not appear skillful on contrived problems either. Perhaps you have a good demonstration of where it does do well?\r\n\r\n@mbollmann is correct as far as I can tell. The attention approach of Bahdanau, et al. requires access to the decoder hidden state (decoder output) of the last time step in order to compute the current time step (s_i-1 in the paper). This is unavailable unless you write your own layer and access it.\r\n\r\n[1] https://arxiv.org/pdf/1409.0473.pdf', ""@jbrownlee \r\nWould it be possible to share some of these 'test' case contrived problems? It would be extremely helpful in terms of debugging and evaluating the efficacy of various attention implementations."", ""@cbaziotis , How will the above attention mechanism work for the imdb example in keras? The input size is (5000, 80) (#max_length=80) and output is (5000, ). This the model for training :\r\n```\r\n\tinput_ = Input(shape=(80,), dtype='float32')\r\n\tprint (input_.get_shape())                       #(?, 80)\r\n\tinput_embed = Embedding(max_features, 128 ,input_length=80)(input_)\r\n\tprint (input_embed.get_shape())                  #(?, 80, 128)\r\n\r\n\tactivations = LSTM(64, return_sequences=True)(input_embed)\r\n\tattention = TimeDistributed(Dense(1, activation='tanh'))(activations)\r\n\tattention = Flatten()(attention)\r\n\tattention = Activation('softmax')(attention)\r\n\tattention = RepeatVector(64)(attention)\r\n\tattention = Permute([2, 1])(attention)\t\r\n\tprint (activations.get_shape())                   #(?, ?, 64)\r\n\tprint (attention.get_shape())                     #(?, ?, 64)\r\n\r\n\tsent_representation = merge([activations, attention], mode='mul')\r\n\tsent_representation = Lambda(lambda x_train: K.sum(x_train, axis=1), output_shape=(5000,))(sent_representation)\r\n\tprint (sent_representation.get_shape())           #(?, 64)\r\n\tprobabilities = Dense(1, activation='softmax')(sent_representation)      #Expected (5000,)\r\n\tmodel = Model(inputs=input_, outputs=probabilities)\r\n\tmodel.summary()\r\n\r\n Error : ValueError: Dimensions must be equal, but are 64 and 5000 for 'dense_2/MatMul' (op: 'MatMul') with input shapes: [?,64], [5000,1]."", 'Hi, @cbaziotis  Thanks for your code.\r\nAs you did not conduct special treatment for the padded words, I am wondering if the attention mechanism will assign the correct weights (close to zero) on the padded words.', 'If you read carefully you will see that i have posted the updated versions of the layers. Here you go:\r\n- [AttentionWithContext.py](https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2)\r\n```python\r\nmodel.add(LSTM(64, return_sequences=True))\r\nmodel.add(AttentionWithContext())\r\n# next add a Dense layer (for classification/regression) or whatever...\r\n```\r\n- [Attention.py](https://gist.github.com/cbaziotis/6428df359af27d58078ca5ed9792bd6d)\r\n```python\r\nmodel.add(LSTM(64, return_sequences=True))\r\nmodel.add(Attention())\r\n# next add a Dense layer (for classification/regression) or whatever...\r\n```\r\n\r\nAnd as i say, the layers take into account the mask.\r\nEdit: also note that i have not tested them with Keras 2, but i imagine that you will need to make some minor syntactic changes.', 'does the attention+lstm improve the accuracy in text classification? In my dataset, I find that, there is  no difference with mean pooling + lstm.', ""@cbaziotis I have a query regarding the attention:\r\nactivations=LSTM(neu,activation='relu',return_sequences=True,return_state=True)(inputs)\r\nThis statement applies attention on output of LSTM. Does this imply on h (hidden state) where h=o_t (tanh(c_t))\r\n\r\nI read somewhere, that in\r\nactivations,hh,cc=LSTM(neu,activation='relu',return_sequences=True,return_state=True)(inputs)\r\n\r\nthat hh is hidden state and cc is the cell state. Are hh and cc the final hidden and cell states?\r\n\r\nAlso what is the difference between attention and attention with context\r\n\r\n"", '@Ravin0512 Any updates?', '@Ravin0512 i recently made this tool https://github.com/cbaziotis/neat-vision\r\n\r\nJust make sure to return the attention scores besides the final representation of the sentence from the attention layer.', ""@cbaziotis As per sharing the weights across time-steps, I think it is fine. Even Andrew Ng's Sequence Models course have shared weight implementation."", ""1. can one make the attentionmodel shorter by using the dot function of keras.laysers ?\r\n\r\ninputs=Input(shape=(input_len,))\r\nembedded=Embedding(input_dim, embedding_dim)(inputs)\r\nactivation=LSTM(hidden_dim, return_sequences=True)(embedded)\r\nattention=TimeDistributed(Dense(1,use_bias=False, activation='linear'))(activation)\r\nattention=Flatten()(attention)\r\nattention=Activation('softmax')(attention)\r\nrepresentation=dot([attention,activation],axes=1)\r\n\r\nisnt it the same as the Long Version \r\nattention=TimeDistributed(Dense(1,use_bias=False, activation='linear'))(activation)\r\nattention=Flatten()(attention)\r\nAttention=Activation('softmax')(attention)\r\nattention=RepeatVector(self.hidden_dim)(attention)\r\nattention=Permute([2,1])(attention)\r\nactivation=multiply([attention,activation])\r\nrepresentation=Lambda(lambda x: K.sum(x,axis=1))(activation)\r\n\r\nthe dot function contracts the Tensor at the axis=1 sum_t a_t*h_th= h_h\r\n\r\nthe dense layer for the activation shouldnt have a bias, since the weights accoording th zhou work only on the hidden components of the hidden states.  further more in zhous model a linear activation is enough\r\n\r\nas far as i understood the Attention-dense layer has to be time distributed. because the weights act on the hidden states components they have the same role mor or less as all matrices in the the recurrent layer which all share the weights over time.\r\n\r\nthe time dependence oft the activation factors rises from the the hidden state differences (components deiffer an therefore alpha(t)=softmax(w^T*h_t) differs, \r\n"", '@Ravin0512 I just found an ugly method. \r\nFirst you need to define a simple network structure before your attention layer (here the attention layer is the fourth layer). \r\n` sent_before_att = K.function([sent_model.layers[0].input, K.learning_phase()], [sent_model.layers[2].output])` \r\nAnd you then take out the attention layer weight.\r\n` sent_att_w = sent_model.layers[3].get_weights()` \r\nAnd use the sent_before_att function to get the vector after the layer before the attention layer.\r\n` sent_each_att = sent_before_att([sentence, 0])` \r\nIn addtion, you need to define a function to calculate the attention weights, here is the funtion named cal_att_weights, you can use numpy to realize the same thing you define the attention layer.\r\nFinally the sent_each_att is the attention weight you want.\r\n`sent_each_att = cal_att_weights(sent_each_att, sent_att_w)`\r\n\r\n\r\n\r\n', '@cbaziotis the best attention visualization tools I have ever seen 👍 ', ""i want to Regression output with Attention LSTM\r\n\r\nI tried this:\r\n`def Attention_LSTM(self):\r\n    \r\n        _input = Input(shape=(self.seq_length, self.feature_length,))\r\n\r\n        LSTM_layer = LSTM(self.n_hidden, return_sequences=True)(_input)\r\n\r\n        # Attention layer\r\n        attention = TimeDistributed(Dense(1, activation='tanh'))(LSTM_layer)\r\n        attention = Flatten()(attention)\r\n        attention = Activation('softmax')(attention)\r\n        attention = RepeatVector(self.n_hidden)(attention)\r\n        attention = Permute([2,1])(attention)\r\n\r\n        #sent_representation = merge([LSTM_layer, attention], mode='mul')\r\n        sent_representation = multiply([LSTM_layer, attention])\r\n        sent_representation = Lambda(lambda xin: K.sum(xin, axis=-1))(sent_representation)\r\n\r\n        probabilities = TimeDistributed(Dense(1, activation='sigmoid'))(sent_representation)\r\n        \r\n        model = Model(inputs=_input, outputs=probabilities)\r\n        return model`\r\n\r\n\r\nbut gives the following error:\r\n\r\n\r\n> `     assert len(input_shape) >= 3\r\nAssertionError\r\n\r\n`\r\n  \r\nmy understanding may be inadequate...\r\n\r\n"", ""Sorry, made an error, activaton and flatten had to be changed,  (first flatten and than activation('softmax') fixed it.\r\n\r\nI tested my Version, and it worked, so far as i could see\r\n\r\nhere is the graph of an example with 1 layer GRU und nextword prediction with attantion including shapes for clarification\r\nsequence length=20, \r\nhidden_dim=128, \r\nembedding_dim=32, \r\nvocabulary_size=397\r\n\r\n(for real language processing typical stacked lstms insteand of grus and higher hidden_dims and embedding_dims are used. ist only a toy example)\r\n\r\n![next_errlog_layr1_slen20_hdim128_edim32_attn1_graph](https://user-images.githubusercontent.com/40464312/42870163-9a0bd978-8a66-11e8-832f-6b61630957ea.png)\r\n\r\n"", ""Hi, @stevewyl -- what is inside that `cal_att_weights` call? \r\nI'm following this [post](https://stackoverflow.com/a/52166510/11234438) to detect the weights per word in an inputted test text. It implements the attentive layer from @cbaziotis and then tacks on that `cal_att_weights` method to inspect the weights per word. \r\nThe dimensions of the weight array I get back are correct, but the weights themselves are crazy small -- all of them hover around `0.0000009`.\r\nDoes this calculation step look correct to you?\r\n\r\n```\r\ndef cal_att_weights(output, att_w):\r\n    eij = np.tanh(np.dot(output[0], att_w[0]) + att_w[1])\r\n    eij = np.dot(eij, att_w[2])\r\n    eij = eij.reshape((eij.shape[0], eij.shape[1]))\r\n    ai = np.exp(eij)\r\n    weights = ai / np.sum(ai)\r\n    return weights```"", 'attention = Flatten()(attention)\r\nfor this line I am getting error:\r\nLayer flatten_4 does not support masking, but was passed an input_mask: Tensor(""time_distributed_6/Reshape_3:0"", shape=(None, None), dtype=bool)', ""Hello all , \r\n\r\nI am trying to use attention on top of a BiLSTM in tensorflow 2.\r\nAlso, I am using pretrained word embeddings. \r\n\r\nmy model is the following: \r\n\r\n\r\n```\r\nunits=250\r\nEMBEDDING_DIM=310\r\nMAX_LENGTH_PER_SENTENCE=65\r\nencoder_input = keras.Input(shape=(MAX_LENGTH_PER_SENTENCE))\r\nx =layers.Embedding(input_dim=len(embedding_matrix), output_dim=EMBEDDING_DIM, input_length=MAX_LENGTH_PER_SENTENCE,\r\n                              weights=[embedding_matrix],\r\n                              trainable=False)(encoder_input)\r\n                              \r\nactivations =layers.Bidirectional(tf.keras.layers.LSTM(units))(x)\r\nactivations = layers.Dropout(0.5)(activations)\r\n\r\nattention=layers.Dense(1, activation='tanh')(activations)\r\nattention=layers.Flatten()(attention)\r\nattention=layers.Activation('softmax')(attention)\r\nattention=layers.RepeatVector(units*2)(attention)\r\nattention=layers.Permute((2, 1))(attention)\r\n\r\nsent_representation = layers.Multiply()([activations, attention])\r\nsent_representation = layers.Lambda(lambda xin: K.sum(xin, axis=-2), output_shape=(units*2,))(sent_representation)\r\n\r\nsent_representation = layers.Dropout(0.5)(sent_representation)\r\n\r\nprobabilities = layers.Dense(4, activation='softmax')(sent_representation)\r\n\r\n\r\nencoder = keras.Model(inputs=[encoder_input], outputs=[probabilities],name='encoder')\r\nencoder.summary()\r\n```\r\n\r\nCould you please let me know if my implementation is correct ?\r\nWhat makes me worry is that the result with attention model do not have an improvement.\r\n\r\nThanks in advance.\r\n"", 'here a simple solution to add attention in your network\r\n\r\nhttps://stackoverflow.com/questions/62948332/how-to-add-attention-layer-to-a-bi-lstm/62949137#62949137', ""Hey everyone. I saw that everyone adds Dense( ) layer in their custom attention layer, which I think isn't needed. \r\n\r\n![image](https://user-images.githubusercontent.com/67724640/96329117-90af1000-1067-11eb-8a56-97448b8fe164.png)\r\n\r\nThis is an image from a tutorial [here](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3). Here, we are just multiplying 2 vectors and then doing several operations on these vectors only. So what is the need of Dense( ) layer. Is the tutorial on 'how does attention work' wrong?\r\n"", '> \r\n> \r\n> Hey, have a look at this repo:\r\n> \r\n> https://github.com/philipperemy/keras-attention-mechanism\r\n> \r\n> It shows how to build an attention module of top of a recurrent layer.\r\n> \r\n> Thanks\r\n\r\nThanks Philip. Your implementation is clean and easy to follow.', ""> Hey everyone. I saw that everyone adds Dense( ) layer in their custom attention layer, which I think isn't needed.\r\n> \r\n> ![image](https://user-images.githubusercontent.com/67724640/96329117-90af1000-1067-11eb-8a56-97448b8fe164.png)\r\n> \r\n> This is an image from a tutorial [here](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3). Here, we are just multiplying 2 vectors and then doing several operations on these vectors only. So what is the need of Dense( ) layer. Is the tutorial on 'how does attention work' wrong?\r\n\r\nSame question I have""]","[""\r\nvocab_size = embeddings.shape[0]\r\nembedding_size = embeddings.shape[1]\r\n\r\nmodel = Sequential()\r\n\r\nmodel.add(Embedding(\r\n        input_dim=vocab_size,\r\n        output_dim=embedding_size,\r\n        input_length=max_length,\r\n        trainable=False,\r\n        mask_zero=True,\r\n        weights=[embeddings]\r\n    ))\r\n\r\nmodel.add(LSTM(200, return_sequences=False))\r\nmodel.add(Dropout(0.5))\r\n\r\nmodel.add(Dense(3, activation='softmax', activity_regularizer=activity_l2(0.0001)))\r\n""]","['LSTM', 'softmax', 'return_sequences=True']",0,0
228,keras,3657,closed,AttributeError: 'ProgbarLogger' object has no attribute 'log_values',"Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [X] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [X] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [X] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I am performing batch learning and after a few batches I get this error from this line of code:



I have no idea why I get this error, It seems to happen randomly, can anyone point me in the right direction?

Here is the module of code that I am running:


",,"['Can you post a minimal, short, standalone script to reproduce your issue?\n', 'Thanks fchollet, I found the bug in my code.\nApparently this line was causing it:\n`(Xtrain, Ytrain) = loadImages(imagesAndClass[startingPoint:remainder])`\n\nIt should have been\n\n`(Xtrain, Ytrain) = loadImages(imagesAndClass[startingPoint:startingPoint+remainder])`\n\nIt was trying to return an array with an invalid range something like: 500000-4900\n', 'Hi,\r\n\r\nI\'ve the same error. I\'m following this **https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html** tutorial for **Using pre-trained word embeddings in a Keras model**.\r\n\r\n```\r\n  File ""/home/mrx/src/experpk/exper.pk/API/Product/management/commands/categorize.py"", line 130, in handle\r\n    model.fit(x_train, y_train, validation_data=(x_val, y_val), nb_epoch=2, batch_size=50, verbose=1)\r\n  File ""/home/mrx/src/experpk/venv-experpk/local/lib/python2.7/site-packages/keras/engine/training.py"", line 1192, in fit\r\n    initial_epoch=initial_epoch)\r\n  File ""/home/mrx/src/experpk/venv-experpk/local/lib/python2.7/site-packages/keras/engine/training.py"", line 912, in _fit_loop\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File ""/home/mrx/src/experpk/venv-experpk/local/lib/python2.7/site-packages/keras/callbacks.py"", line 76, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File ""/home/mrx/src/experpk/venv-experpk/local/lib/python2.7/site-packages/keras/callbacks.py"", line 265, in on_epoch_end\r\n    self.progbar.update(self.seen, self.log_values, force=True)\r\nAttributeError: \'ProgbarLogger\' object has no attribute \'log_values\'\r\n```\r\n\r\nRespective code is ;\r\n\r\n```\r\n        if self.verbose:\r\n            # here\r\n            self.progbar.update(self.seen, self.log_values, force=True)\r\n```\r\n\r\nCan you please help me.', ""For method fit() Keras should throw an exception when using validation_split and training set is ended up to be 0... Example: your last batch in epoch contains 1 sample and validation_split = 0.3 => training size = 0...\r\nWhen training set size is 0 you will see this exception...\r\nIf you look at callbacks.py in keras, class ProgbarLogger, log_values member is created ONLY when batch starts... So if you don't have data in the batch it gets to on_epoch_end with log_values undefined..."", ""It seems this is not solved yet. I uploaded Keras yesterday (2/6/2017) and the code is still raising this message: AttributeError: 'ProgbarLogger' object has no attribute 'log_values'"", '```\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Embedding, GRU, Dense\r\nimport numpy as np\r\n\r\n\r\ndef get_model():\r\n    model = Sequential()\r\n    model.add(Embedding(input_dim=27, output_dim=300))\r\n    model.add(GRU(256))\r\n    model.add(Dense(27, activation=\'softmax\'))\r\n    model.compile(loss=\'categorical_crossentropy\', optimizer=\'adam\')\r\n    return model\r\n\r\nmodel = get_model()\r\nX = np.zeros((0, 10), dtype=np.int)\r\ny = np.zeros((0, 27), dtype=np.bool)\r\nmodel.fit(X, y)\r\n```\r\n\r\nHere\'s a self-contained example that reproduces the error. @fchollet , can we reopen this?\r\n\r\nHere\'s the output I get\r\n\r\n```\r\n~ g$ python3 keras_error.py\r\nUsing TensorFlow backend.\r\nEpoch 1/10\r\nTraceback (most recent call last):\r\n  File ""keras_error.py"", line 17, in <module>\r\n    model.fit(X, y)\r\n  File ""/usr/local/lib/python3.6/site-packages/keras/models.py"", line 870, in fit\r\n    initial_epoch=initial_epoch)\r\n  File ""/usr/local/lib/python3.6/site-packages/keras/engine/training.py"", line 1507, in fit\r\n    initial_epoch=initial_epoch)\r\n  File ""/usr/local/lib/python3.6/site-packages/keras/engine/training.py"", line 1176, in _fit_loop\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File ""/usr/local/lib/python3.6/site-packages/keras/callbacks.py"", line 77, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File ""/usr/local/lib/python3.6/site-packages/keras/callbacks.py"", line 309, in on_epoch_end\r\n    self.progbar.update(self.seen, self.log_values, force=True)\r\nAttributeError: \'ProgbarLogger\' object has no attribute \'log_values\'\r\n```', ""I'm also getting the same error while training a four layer LSTM on small conversation corpus.epochs = 500\r\nepochs = 500\r\nfor i in range(10):\r\n    model.fit(vecX, vecY, epochs = epochs, validation_split = 0.2, verbose = 1)\r\n    model.save('LSTM' + str((epochs * i + 500)) + '.h5') \r\n    \r\nthis gives out\r\n\r\nTrain on 0 samples, validate on 1 samples\r\nEpoch 1/500\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-13-c99fc1b67032> in <module>()\r\n      1 epochs = 500\r\n      2 for i in range(10):\r\n----> 3     model.fit(vecX, vecY, epochs = epochs, validation_split = 0.2, verbose = 1)\r\n      4     model.save('LSTM' + str((epochs * i + 500)) + '.h5')\r\n\r\n~/.local/lib/python3.5/site-packages/keras/models.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\r\n    868                               class_weight=class_weight,\r\n    869                               sample_weight=sample_weight,\r\n--> 870                               initial_epoch=initial_epoch)\r\n    871 \r\n    872     def evaluate(self, x, y, batch_size=32, verbose=1,\r\n\r\n~/.local/lib/python3.5/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\r\n   1505                               val_f=val_f, val_ins=val_ins, shuffle=shuffle,\r\n   1506                               callback_metrics=callback_metrics,\r\n-> 1507                               initial_epoch=initial_epoch)\r\n   1508 \r\n   1509     def evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None):\r\n\r\n~/.local/lib/python3.5/site-packages/keras/engine/training.py in _fit_loop(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\r\n   1174                         for l, o in zip(out_labels, val_outs):\r\n   1175                             epoch_logs['val_' + l] = o\r\n-> 1176             callbacks.on_epoch_end(epoch, epoch_logs)\r\n   1177             if callback_model.stop_training:\r\n   1178                 break\r\n\r\n~/.local/lib/python3.5/site-packages/keras/callbacks.py in on_epoch_end(self, epoch, logs)\r\n     75         logs = logs or {}\r\n     76         for callback in self.callbacks:\r\n---> 77             callback.on_epoch_end(epoch, logs)\r\n     78 \r\n     79     def on_batch_begin(self, batch, logs=None):\r\n\r\n~/.local/lib/python3.5/site-packages/keras/callbacks.py in on_epoch_end(self, epoch, logs)\r\n    307                 self.log_values.append((k, logs[k]))\r\n    308         if self.verbose:\r\n--> 309             self.progbar.update(self.seen, self.log_values, force=True)\r\n    310 \r\n    311 \r\n\r\nAttributeError: 'ProgbarLogger' object has no attribute 'log_values'\r\n\r\n"", 'Same error, and I tried removing validation_split from my fit command, but no change.\r\n\r\n', ""Have you checked the dimensions for both ur x and y for train and test, I had a dimension issue with my y train. Generally the error arising if u tey to split data of incorrect size , say if it's 1, and u try to split it, it gives this error"", 'Ran into this issue today. saransh-mehta suggestion was the solution. Also ran into an issue where the number of samples for test/train split was not sufficient. train was 0 samples.', 'If this is a common issue, we need a clear error message to handle it. Please open a PR to add appropriate error handling.', 'Hi Guys,\r\nI also meet this issue, but it happened that the sample size is small, e.g., I used a sample size 2 and do model.fit it by split ratio 0.2 then get this error. but when I used sample size >1000, then it disappeared.\r\n\r\nhope this information may help.\r\n\r\nregards, Yifei\r\n', 'I also encountered this error today. Had to set `verbose=0 ` to fix.\r\nPlease reopen the issue. It needs to be fixed!\r\n', 'Encountered this error when my training set was an empty array.  Using Keras 2.0.9.\r\n\r\nA more descriptive error message might be helpful ', 'This happens if `steps_per_epoch` is 0. Make sure that your batch size is not greater than the dataset size to avoid it.', 'I got a similar issue when my training data set was empty after data cleaning and normalization. Might be worth checking you data set size before training.', ""I've hit this error in Keras 2.1.5 as well."", 'I also encountered this problem and I do not know how to solve it', 'maybe this issue occurrence due to less train sample.increase more train sample can solve it', 'Encountered this in 2.1.6 when supplying empty training set by mistake. IMHO the code should handle this gracefully.', '@kevkid @oswaldoludwig \r\nAttached is my gist. https://gist.github.com/Anoopparjanya/d921e3edf1ef9eeb8621ed6138f9557f/revisions\r\n\r\nIm getting the same error though i have installed updated versions f keras and Theano mentioned above.\r\n\r\nPlease help me out,how to overcome this issue?', 'Set verbose=0:\r\n\r\n    model.fit(X, Y, verbose=0)\r\n\r\nOr make sure your dataset is bigger than the batch size.', '将batch_size设置为较小的数字。当您的batch_size设置为比样本集大小更大的值时，会出现此错误。', ""(using TensorFlow `1.7.0`)\r\n\r\nI would like to propose a code change to `keras/callbacks.py`.  The current error is confusing and provides no clue as to what's going on.  I also believe that it wasn't designed to fail, but is just incorrect.  IF keras wants to let the user know that they have their training setup wrong, that's ok, but I don't think this is the mechanism to do that.\r\n\r\nThe `log_values` attribute is used on `on_epoch_end()`, yet is created in `on_batch_begin()`.  This seems wrong.\r\n\r\nInstead, place the initialization in `ProgbarLogger.__init__()` (while still leaving `on_batch_begin()` as is).\r\n\r\n    def __init__(self, count_mode='samples',\r\n                 stateful_metrics=None):\r\n        super(ProgbarLogger, self).__init__()\r\n        if count_mode == 'samples':\r\n            self.use_steps = False\r\n        elif count_mode == 'steps':\r\n            self.use_steps = True\r\n        else:\r\n            raise ValueError('Unknown `count_mode`: ' + str(count_mode))\r\n        if stateful_metrics:\r\n            self.stateful_metrics = set(stateful_metrics)\r\n        else:\r\n            self.stateful_metrics = set()\r\n        self.log_values = []  # <----- INSERTED HERE\r\n\r\n"", 'Please, see PR with fix above.\r\nIt fix this and some related bugs and contains tests for regressions and related bugs manifestation. Concerns behind change set are described as well.', ""I also got this error. Because the number of training samples is smaller than 'batch_size'.\r\nHope this message can help."", 'Same error with my execution:\r\n\r\n```\r\nfrom __future__ import print_function\r\nimport keras\r\nimport tensorflow as tf\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout, Activation, Flatten\r\nfrom keras.layers import Conv2D, MaxPooling2D\r\nimport os\r\n\r\nbatch_size = 32\r\nnum_classes = 10\r\nepochs = 10\r\nnum_predictions = 20\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), padding=\'same\', data_format=""channels_first""))\r\nmodel.add(Activation(\'relu\'))\r\nmodel.add(Conv2D(32, (3, 3), data_format=""channels_first""))\r\nmodel.add(Activation(\'relu\'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2), data_format=""channels_first""))\r\nmodel.add(Dropout(0.25))\r\n\r\nmodel.add(Conv2D(64, (3, 3), padding=\'same\'))\r\nmodel.add(Activation(\'relu\'))\r\nmodel.add(Conv2D(64, (3, 3), data_format=""channels_first""))\r\nmodel.add(Activation(\'relu\'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2), data_format=""channels_first""))\r\nmodel.add(Dropout(0.25))\r\n\r\nmodel.add(Flatten())\r\nmodel.add(Dense(512))\r\nmodel.add(Activation(\'relu\'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(num_classes))\r\nmodel.add(Activation(\'softmax\'))\r\n\r\nopt = keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)\r\n\r\nmodel.compile(loss=\'categorical_crossentropy\',\r\n              optimizer=opt,\r\n              metrics=[\'accuracy\'])\r\n\r\ntrain_datagen = ImageDataGenerator(rescale=1./255)\r\n\r\ntrain_generator = train_datagen.flow_from_directory(\r\n    \'data\',\r\n    target_size=(150,150),\r\n    batch_size=32,\r\n    class_mode=\'binary\')\r\n\r\nmodel.fit_generator(train_generator,\r\n                    verbose=2,\r\n                    epochs=epochs)\r\n```\r\n\r\nTraceback (most recent call last):\r\n  File ""sturdy_couscous_cnn.py"", line 62, in <module>\r\n    epochs=epochs)\r\n  File ""C:\\Users\\WINDOWS\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras-2.3.0-py3.6.egg\\keras\\legacy\\interfaces.py"", line 91, in wrapper\r\n    return func(*args, **kwargs)\r\n  File ""C:\\Users\\WINDOWS\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras-2.3.0-py3.6.egg\\keras\\engine\\training.py"", line 1732, in fit_generator\r\n    initial_epoch=initial_epoch)\r\n  File ""C:\\Users\\WINDOWS\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras-2.3.0-py3.6.egg\\keras\\engine\\training_generator.py"", line 260, in fit_generator\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File ""C:\\Users\\WINDOWS\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras-2.3.0-py3.6.egg\\keras\\callbacks\\callbacks.py"", line 152, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File ""C:\\Users\\WINDOWS\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras-2.3.0-py3.6.egg\\keras\\callbacks\\callbacks.py"", line 611, in on_epoch_end\r\n    self.progbar.update(self.seen, self.log_values)\r\nAttributeError: \'ProgbarLogger\' object has no attribute \'log_values\'', ""I had the same error for the same code that was running efficiently a day before, I only changed the data -I increased it though-\r\nmost of the solutions say that it's for the size of data or that the batch size might be greater than the train sample but I've got about 30k images with a split of 75-25 between train and validation, I tried to set (verbose=0) but it showed me another error: (list index out of range)"", 'My guess is that the issue is not in how big is your training set, but rather the last batch size. Example: if the last batch has 3 samples, that gives you 2 samples for training set and 0 for validation (casted to integer). This is when you would see this exception. So, increase the last batch to the size where validation samples # will be > 0. Really surprised that the issue is not fixed for such a long time...', ""I thought of this too, but actually it seemed that the problem was more trivial than so. \r\nThanks for your help.\r\nFor my problem, I was calling the name of the images differently, so when it was supposed to take these images and split them it couldn't find them for the different naming I called it with, so it wasn't a problem in the model rather a step before it. "", 'I used a fixed step_per_epoch and the issues was resolved. You can play around number like 100 etc. Thanks']","['\nmodel.fit(Xtrain, Ytrain, batch_size=128, nb_epoch=1,\n                  verbose=1,validation_split=0.01,\n                  callbacks=[ModelCheckpoint(weightStr, monitor=\'val_loss\', verbose=0, save_best_only=True, mode=\'auto\')])\n\nTraceback (most recent call last):\n\n  File ""<ipython-input-1-0ab90ed05873>"", line 321, in <module>\n    callbacks=[ModelCheckpoint(weightStr, monitor=\'val_loss\', verbose=0, save_best_only=True, mode=\'auto\')])\n\n  File ""/home/kevin/.local/lib/python2.7/site-packages/keras/models.py"", line 620, in fit\n    sample_weight=sample_weight)\n\n  File ""/home/kevin/.local/lib/python2.7/site-packages/keras/engine/training.py"", line 1104, in fit\n    callback_metrics=callback_metrics)\n\n  File ""/home/kevin/.local/lib/python2.7/site-packages/keras/engine/training.py"", line 842, in _fit_loop\n    callbacks.on_epoch_end(epoch, epoch_logs)\n\n  File ""/home/kevin/.local/lib/python2.7/site-packages/keras/callbacks.py"", line 40, in on_epoch_end\n    callback.on_epoch_end(epoch, logs)\n\n  File ""/home/kevin/.local/lib/python2.7/site-packages/keras/callbacks.py"", line 196, in on_epoch_end\n    self.progbar.update(self.seen, self.log_values, force=True)\n\nAttributeError: \'ProgbarLogger\' object has no attribute \'log_values\'\n\n', '\nfor e in range(numEpoch):\n    numOfImgToLoad = 50000#we can tune this\n    totalNumberOfImages = len(imagesAndClass)\n    runningTotal = 0\n    startingPoint = 0\n    endingPoint = numOfImgToLoad\n    while totalNumberOfImages > 0:\n        print ""StartingPoint: {}, endingPoint {}"".format(startingPoint, endingPoint)\n        totalNumberOfImages = totalNumberOfImages - numOfImgToLoad#subtract the number of images loaded into mem\n        if totalNumberOfImages < 0:\n            remainder = totalNumberOfImages + numOfImgToLoad\n            (Xtrain, Ytrain) = loadImages(imagesAndClass[startingPoint:remainder])\n            Xtrain = np.array(Xtrain).reshape(len(Xtrain), 1, 106, 106)#np.array(Xtrain).reshape(4415, 1, 106, 106)\n            runningTotal += remainder\n        else:\n            (Xtrain, Ytrain) = loadImages(imagesAndClass[startingPoint:endingPoint])\n            Xtrain = np.array(Xtrain).reshape(len(Xtrain), 1, 106, 106)\n            runningTotal += numOfImgToLoad\n            startingPoint = endingPoint+1\n            endingPoint = startingPoint + numOfImgToLoad - 1\n\n        Xtrain /= 255#change pixel value to between 0 and 1\n        Xtrain = Xtrain.astype(\'float32\')\n        Ytrain = np_utils.to_categorical(Ytrain, len(classes)+1)\n        Ytrain = np.array(Ytrain)\n        print ""Starting epoch {}"".format(e)\n        model.fit(Xtrain, Ytrain, batch_size=128, nb_epoch=1,\n                  verbose=1,validation_split=0.01,\n                  callbacks=[ModelCheckpoint(weightStr, monitor=\'val_loss\', verbose=0, save_best_only=True, mode=\'auto\')])\n                  #callbacks=[EarlyStopping(monitor=\'val_loss\', patience=2)])\n        #print ""Starting epoch {} on image {}"".format(e, runningTotal)\n        print ""Killing Xtrain and resetting""\n        del Xtrain\n        del Ytrain\n']",[],0,0
229,keras,6655,closed,Functional Preprocessing and Augmentation API,"Edit: A new example proposal + details is in https://github.com/fchollet/keras/pull/6891#issuecomment-307659460.

Discussion about a dense prediction API such as image segmentation in https://github.com/fchollet/keras/issues/6538 brought up the possibility of a functional preprocessing API, which could move preprocessing steps into the Keras backend APIs and generalize preprocessing to more network designs. [Dropout](https://github.com/fchollet/keras/blob/master/keras/layers/core.py#L72) provides a precedent for such augmentation layers.

If the layers could be designed much like dropout, and I would expect them to be configured so that the augmentation operations could be applied identically to one or more image inputs as well as one or more image label data, useful for dense prediction tasks.

This could have the advantages of being easy to use, easily applied consistently for arbitrary data inputs, and make it possible to use the [TF backend image augmentation APIs](https://www.tensorflow.org/api_guides/python/image) thus improving performance.

What would be the pros/cons and barriers to a Functional Preprocessing API?


Example usage, label augmentation optional for dense prediction tasks:

",stale,"[""Is there anything like the tf preprocessing for theano? Otherwise, I guess it's a deal-breaker for theano users."", ""@Dref360 I don't know, perhaps plain python APIs could be run on the theano side? They already have the sequential api you mentioned so adaptation may not be too hard, where would a feature request for theano be submitted?"", 'Edit: A new example proposal + details is in https://github.com/fchollet/keras/pull/6891#issuecomment-307659460.', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', ""If you're using the TensorFlow backend, you can just use input tensors by following the [mnist_tfrecord.py](https://github.com/fchollet/keras/blob/master/examples/mnist_tfrecord.py) example, and before passing your tensors to the Input layer apply TensorFlow preprocessing ops such as [tf.random_crop](https://www.tensorflow.org/api_docs/python/tf/random_crop). \r\n\r\nSince there is a way to accomplish this, I'm closing the issue.""]","[""python\r\nL = InputLabel(...)\r\ninput = Input(...)\r\n\r\n# augmentation\r\nx,L = Flip(axis=0)([input,L])\r\nx,L = Flip(axis=1)([x,L])\r\nx,L = Zoom(range=[0.5,2])([x,L])\r\nx,L = Transform(matrix=[1,0,0,0,1,0,0,0,1], interpolation='bilinear')([x,L])\r\nx,L = RandomCrop(fill_mode='constant', cval=0.)([x,L])\r\n\r\n# network definition\r\nx = Conv2D(inter_channel, (1, 1), kernel_initializer='he_uniform', padding='same', use_bias=False,\r\n                   kernel_regularizer=l2(weight_decay))(x)\r\n\r\n# ...snip...\r\nmodel = Model(inputs=input, outputs=x, labels=L)\r\nmodel.compile(Adam(), loss='categorical_crossentropy',metrics=['acc'])\r\n# ...snip...\r\nmodel.fit()\r\n""]",[],0,0
230,keras,1315,closed,Bug in /theano/sandbox/cuda/dnn.py,"/usr/bin/python2.7 /home/dell/DLTest/cifar_test/Residul/cifar_residul_net.py
Using Theano backend.
Using gpu device 1: GeForce GTX TITAN X (CNMeM is disabled)
('X_train shape:', (50000, 3, 32, 32))
(50000, 'train samples')
(10000, 'test samples')
Traceback (most recent call last):
  File ""/home/dell/DLTest/cifar_test/Residul/cifar_residul_net.py"", line 87, in <module>
    model.compile(loss='categorical_crossentropy', optimizer=sgd)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/models.py"", line 372, in compile
    self.y_train = self.get_output(train=True)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/containers.py"", line 73, in get_output
    return self.layers[-1].get_output(train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/core.py"", line 681, in get_output
    X = self.get_input(train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/core.py"", line 102, in get_input
    return self.previous.get_output(train=train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/core.py"", line 591, in get_output
    X = self.get_input(train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/core.py"", line 102, in get_input
    return self.previous.get_output(train=train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/containers.py"", line 216, in get_output
    return self.outputs[self.output_order[0]].get_output(train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/core.py"", line 389, in get_output
    s = self.layers[0].get_output(train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/convolutional.py"", line 212, in get_output
    X = self.get_input(train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/core.py"", line 102, in get_input
    return self.previous.get_output(train=train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/core.py"", line 98, in get_output
    return self.get_input(train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/core.py"", line 102, in get_input
    return self.previous.get_output(train=train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/containers.py"", line 216, in get_output
    return self.outputs[self.output_order[0]].get_output(train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/core.py"", line 389, in get_output
    s = self.layers[0].get_output(train)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/layers/convolutional.py"", line 215, in get_output
    dim_ordering=self.dim_ordering)
  File ""/home/dell/.local/lib/python2.7/site-packages/keras/backend/theano_backend.py"", line 543, in conv2d
    border_mode=(pad_x, pad_y))
  File ""/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/dnn.py"", line 1191, in dnn_conv
    conv_mode=conv_mode)(img.shape, kerns.shape)
  File ""/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/dnn.py"", line 251, in **init**
    border_mode = tuple(map(int, border_mode))
TypeError: int() argument must be a string or a number, not 'TensorVariable'

I got a bug after i delete /usr/local/cuda/lib64/libcudnn\*  and /usr/local/cuda/include/cudnn.h

Then add the same files of cudnn7-5v3 into the same path 
",stale,"['I sovled it by re-installing keras\n', 'I have the same problem after installing cuDNN 3 into CUDA7.5 directories.  If I rename usr/local/cuda/include/cudnn.h to some other name, it runs (CUDA works). \nI reinstalled Keras (v 0.3.0), but the problem persists. I know that CUDA is quirky to set up, so after copying the files I  tried this again: sudo ldconfig /usr/local/cuda-7.5/lib64, but no luck.\n\nMy theanorc file contains:\n[global]\nfloatX = float32\ndevice = gpu0\noptimizer = fast_run\noptimizer_including=cudnn\n\n[nvcc]\nfastmath = True\n\n[cuda]\nroot = /usr/local/cuda-7.5\n\n[lib]\ncnmem = 0.9\n\nI\'d be grateful for any advice.\nThanks!\n\n/usr/bin/python2.7 /home/virostatiq/PycharmProjects/imdb/net/cifar.py\nUsing Theano backend.\nUsing gpu device 0: GeForce GTX 980 Ti (CNMeM is enabled)\nX_train shape: (50000, 3, 32, 32)\n50000 train samples\n10000 test samples\nTraceback (most recent call last):\n  File ""/home/virostatiq/PycharmProjects/imdb/net/cifar.py"", line 70, in <module>\n    model.compile(loss=\'categorical_crossentropy\', optimizer=sgd)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 372, in compile\n    self.y_train = self.get_output(train=True)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/containers.py"", line 73, in get_output\n    return self.layers[-1].get_output(train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 512, in get_output\n    X = self.get_input(train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 102, in get_input\n    return self.previous.get_output(train=train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 681, in get_output\n    X = self.get_input(train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 102, in get_input\n    return self.previous.get_output(train=train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 488, in get_output\n    X = self.get_input(train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 102, in get_input\n    return self.previous.get_output(train=train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 512, in get_output\n    X = self.get_input(train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 102, in get_input\n    return self.previous.get_output(train=train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 681, in get_output\n    X = self.get_input(train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 102, in get_input\n    return self.previous.get_output(train=train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 591, in get_output\n    X = self.get_input(train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 102, in get_input\n    return self.previous.get_output(train=train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 488, in get_output\n    X = self.get_input(train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 102, in get_input\n    return self.previous.get_output(train=train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/convolutional.py"", line 323, in get_output\n    X = self.get_input(train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 102, in get_input\n    return self.previous.get_output(train=train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 512, in get_output\n    X = self.get_input(train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 102, in get_input\n    return self.previous.get_output(train=train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/convolutional.py"", line 212, in get_output\n    X = self.get_input(train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 102, in get_input\n    return self.previous.get_output(train=train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 512, in get_output\n    X = self.get_input(train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 102, in get_input\n    return self.previous.get_output(train=train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/convolutional.py"", line 212, in get_output\n    X = self.get_input(train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 102, in get_input\n    return self.previous.get_output(train=train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 488, in get_output\n    X = self.get_input(train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 102, in get_input\n    return self.previous.get_output(train=train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/convolutional.py"", line 323, in get_output\n    X = self.get_input(train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 102, in get_input\n    return self.previous.get_output(train=train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 512, in get_output\n    X = self.get_input(train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 102, in get_input\n    return self.previous.get_output(train=train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/convolutional.py"", line 212, in get_output\n    X = self.get_input(train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 102, in get_input\n    return self.previous.get_output(train=train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 512, in get_output\n    X = self.get_input(train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 102, in get_input\n    return self.previous.get_output(train=train)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/convolutional.py"", line 215, in get_output\n    dim_ordering=self.dim_ordering)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/theano_backend.py"", line 543, in conv2d\n    border_mode=(pad_x, pad_y))\n  File ""/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/dnn.py"", line 1192, in dnn_conv\n    conv_mode=conv_mode, precision=precision)(img.shape,\n  File ""/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/dnn.py"", line 260, in **init**\n    border_mode = tuple(map(int, border_mode))\nTypeError: int() argument must be a string or a number, not \'TensorVariable\'\n\nProcess finished with exit code 1\n', 'The way to fix this is to find in that line why border_mode isn\'t an int.\n\nUpdating Theano could help.\n\nOtherwise, I would need a way to reproduce it, I got such report outside of\nKeras about this, but I don\'t have a use case to investigate.\n\nAlso, check that this line in keras have pad_x and pad_y as an int. If not,\nthen the problem is in Keras as it should be an int.\n\nFile\n""/usr/local/lib/python2.7/dist-packages/keras/backend/theano_backend.py"",\nline 543, in conv2d\nborder_mode=(pad_x, pad_y))\n\nOn Thu, Dec 31, 2015 at 11:24 AM, solipsy notifications@github.com wrote:\n\n> I have the same problem after installing cuDNN 3 into CUDA7.5 directories.\n> If I rename usr/local/cuda/include/cudnn.h to some other name, it runs\n> (CUDA works).\n> I reinstalled Keras (v 0.3.0), but the problem persists. I know that CUDA\n> is quirky to set up, so after copying the files I tried this again: sudo\n> ldconfig /usr/local/cuda-7.5/lib64, but no luck.\n> \n> My theanorc file contains:\n> [global]\n> floatX = float32\n> device = gpu0\n> optimizer = fast_run\n> optimizer_including=cudnn\n> \n> [nvcc]\n> fastmath = True\n> \n> [cuda]\n> root = /usr/local/cuda-7.5\n> \n> [lib]\n> cnmem = 0.9\n> \n> I\'d be grateful for any advice.\n> Thanks!\n> \n> /usr/bin/python2.7 /home/virostatiq/PycharmProjects/imdb/net/cifar.py\n> Using Theano backend.\n> Using gpu device 0: GeForce GTX 980 Ti (CNMeM is enabled)\n> X_train shape: (50000, 3, 32, 32)\n> 50000 train samples\n> 10000 test samples\n> Traceback (most recent call last):\n> File ""/home/virostatiq/PycharmProjects/imdb/net/cifar.py"", line 70, in\n> model.compile(loss=\'categorical_crossentropy\', optimizer=sgd)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 372,\n> in compile\n> self.y_train = self.get_output(train=True)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/containers.py"",\n> line 73, in get_output\n> return self.layers[-1].get_output(train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 512, in get_output\n> X = self.get_input(train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 102, in get_input\n> return self.previous.get_output(train=train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 681, in get_output\n> X = self.get_input(train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 102, in get_input\n> return self.previous.get_output(train=train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 488, in get_output\n> X = self.get_input(train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 102, in get_input\n> return self.previous.get_output(train=train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 512, in get_output\n> X = self.get_input(train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 102, in get_input\n> return self.previous.get_output(train=train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 681, in get_output\n> X = self.get_input(train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 102, in get_input\n> return self.previous.get_output(train=train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 591, in get_output\n> X = self.get_input(train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 102, in get_input\n> return self.previous.get_output(train=train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 488, in get_output\n> X = self.get_input(train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 102, in get_input\n> return self.previous.get_output(train=train)\n> File\n> ""/usr/local/lib/python2.7/dist-packages/keras/layers/convolutional.py"",\n> line 323, in get_output\n> X = self.get_input(train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 102, in get_input\n> return self.previous.get_output(train=train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 512, in get_output\n> X = self.get_input(train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 102, in get_input\n> return self.previous.get_output(train=train)\n> File\n> ""/usr/local/lib/python2.7/dist-packages/keras/layers/convolutional.py"",\n> line 212, in get_output\n> X = self.get_input(train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 102, in get_input\n> return self.previous.get_output(train=train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 512, in get_output\n> X = self.get_input(train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 102, in get_input\n> return self.previous.get_output(train=train)\n> File\n> ""/usr/local/lib/python2.7/dist-packages/keras/layers/convolutional.py"",\n> line 212, in get_output\n> X = self.get_input(train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 102, in get_input\n> return self.previous.get_output(train=train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 488, in get_output\n> X = self.get_input(train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 102, in get_input\n> return self.previous.get_output(train=train)\n> File\n> ""/usr/local/lib/python2.7/dist-packages/keras/layers/convolutional.py"",\n> line 323, in get_output\n> X = self.get_input(train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 102, in get_input\n> return self.previous.get_output(train=train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 512, in get_output\n> X = self.get_input(train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 102, in get_input\n> return self.previous.get_output(train=train)\n> File\n> ""/usr/local/lib/python2.7/dist-packages/keras/layers/convolutional.py"",\n> line 212, in get_output\n> X = self.get_input(train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 102, in get_input\n> return self.previous.get_output(train=train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 512, in get_output\n> X = self.get_input(train)\n> File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line\n> 102, in get_input\n> return self.previous.get_output(train=train)\n> File\n> ""/usr/local/lib/python2.7/dist-packages/keras/layers/convolutional.py"",\n> line 215, in get_output\n> dim_ordering=self.dim_ordering)\n> File\n> ""/usr/local/lib/python2.7/dist-packages/keras/backend/theano_backend.py"",\n> line 543, in conv2d\n> border_mode=(pad_x, pad_y))\n> File ""/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/dnn.py"",\n> line 1192, in dnn_conv\n> conv_mode=conv_mode, precision=precision)(img.shape,\n> File ""/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/dnn.py"",\n> line 260, in _init_\n> border_mode = tuple(map(int, border_mode))\n> TypeError: int() argument must be a string or a number, not\n> \'TensorVariable\'\n> \n> Process finished with exit code 1\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/fchollet/keras/issues/1315#issuecomment-168216503.\n', 'I think this was fixed in keras. Can someone confirm and close this issue if so?\n', 'Hi,\n\ni updated Keras from master and the issue is indeed fixed. At least it worked for me. Thanks!\n\nMarko\n', '@nouiz, @solipsy and @meank, yes it has been fixed and tested with CUDA 7.5 and cuDNN v3 and v4 (#1383, 13379da and #1414).\n']",[],[],0,0
231,keras,741,closed,How to train a multi-label Classifier,"I need  train a multi-label softmax classifier, but there is  a lot of one-hot code labels in examples, so how to change code to do it?
",,"['Don\'t use softmax. Use sigmoid units in the output layer and then use ""binary_crossentrpy"" loss.\n', 'That works in my case. However `model.predict_classes` is not ""adapted"" for this. As an example for a sample from the test set, where target label is `1 0 1 0 0 0 0` (I have 7 in total, )\n`model.predict(tSets[1,:])`: 9.90e-01,   2.7e-07, 6.05e-13, 9.98e-01, 2.16e-05, 7.62e-05, 1.51e-04 (so that is correct), but\n`model.predict_classes(tSets[1,:])` gives just array([3]) (seems like it picks the highest value from `model.predict`. A quick fix might be `numpy.around` but maybe there is a more elegant solution?\n', 'Getting classes from .predict() is one line of numpy code really.\n', 'model.predict(blabla) > 0.5 ?\n', '@elanmart Hi, why do you think using softmax is not a good idea?\n\nDo you use a graph model, given we have multiple outputs?\n', 'my loss is not convergence @holderm @elanmart \n\n``` python\nmodel.predict(Y_train[1,:])\n```\n\nit shows [  0.00000000e+000   0.00000000e+000   0.00000000e+000   0.00000000e+000\n    0.00000000e+000]\nmy complete code:\n\n``` python\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nimport scipy.io\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\nfrom keras.layers.convolutional import Convolution2D, MaxPooling2D\nfrom keras.optimizers import SGD, Adadelta, Adagrad\nfrom keras.utils import np_utils, generic_utils\nfrom six.moves import range\n\nbatch_size = 100\nnb_classes = 5\nnb_epoch = 5\ndata_augmentation = True\n\nshapex, shapey = 64, 64\n\nnb_filters = [32, 64]\n\nnb_pool = [4, 3]\n\nnb_conv = [5, 4]\n\nimage_dimensions = 3\n\n\nmat = scipy.io.loadmat(\'E:\\scene.mat\')\n\nX_train = mat[\'x_train\']\nY_train = mat[\'y_train\']\nX_test =  mat[\'x_test\']\nY_test =  mat[\'y_test\']\nprint(X_train.shape)\nprint(X_test.shape)\n\nmodel = Sequential()\n\nmodel.add(Convolution2D(nb_filters[0], image_dimensions, nb_conv[0], nb_conv[0], border_mode=\'valid\'))\nmodel.add(Activation(\'relu\'))\n\nmodel.add(MaxPooling2D(poolsize=(nb_pool[0], nb_pool[0])))\nmodel.add(Dropout(0.25))\n\nmodel.add(Convolution2D(nb_filters[1], nb_filters[0], nb_conv[1], nb_conv[1], border_mode=\'valid\'))\nmodel.add(Activation(\'relu\'))\n\nmodel.add(MaxPooling2D(poolsize=(nb_pool[1], nb_pool[1])))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(nb_filters[-1] * (((shapex - nb_conv[0]+1)/ nb_pool[0] -nb_conv[1]+1)/ nb_pool[1]) * (((shapey -nb_conv[0]+1)/ nb_pool[0] -nb_conv[1]+1)/ nb_pool[1]), 512))\nmodel.add(Activation(\'relu\'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(512, nb_classes,init=\'uniform\'))\nmodel.add(Activation(\'sigmoid\'))\n\n\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss=\'binary_crossentropy\', optimizer=sgd) \n\nif not data_augmentation:\n    print(""Not using data augmentation or normalization"")\n\n    X_train = X_train.astype(""float32"")\n    X_test = X_test.astype(""float32"")\n    X_train /= 255\n    X_test /= 255\n    model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch)\n    score = model.evaluate(X_test, Y_test, batch_size=batch_size)\n    print(\'Test score:\', score)\n\nelse:\n    print(""Using real time data augmentation"")\n\n    # this will do preprocessing and realtime data augmentation\n    datagen = ImageDataGenerator(\n        featurewise_center=True,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=True,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n    datagen.fit(X_train)\n    model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch)\n    score = model.evaluate(X_test, Y_test, batch_size=batch_size)\n    print (model.predict(X_test[1,:]))\n\n\n```\n\ncould you help me to find out where it is wrong, thx !\n', ""@lemuriandezapada yeah, \n\n``` python\nlabels = np.zeros(preds.shape)\nlabels[preds>0.5] = 1\n```\n\n@arushi02 in softmax when increasing score for one label, all others are lowered (it's a probability distribution). You don't want that when you have multiple labels. \nNo, you don't need `Graph`\n\nHere's an example of one of my multilabel nets:\n\n``` python\n# Build a classifier optimized for maximizing f1_score (uses class_weights)\n\nclf = Sequential()\n\nclf.add(Dropout(0.3))\nclf.add(Dense(xt.shape[1], 1600, activation='relu'))\nclf.add(Dropout(0.6))\nclf.add(Dense(1600, 1200, activation='relu'))\nclf.add(Dropout(0.6))\nclf.add(Dense(1200, 800, activation='relu'))\nclf.add(Dropout(0.6))\nclf.add(Dense(800, yt.shape[1], activation='sigmoid'))\n\nclf.compile(optimizer=Adam(), loss='binary_crossentropy')\n\nclf.fit(xt, yt, batch_size=64, nb_epoch=300, validation_data=(xs, ys), class_weight=W, verbose=0)\n\npreds = clf.predict(xs)\n\npreds[preds>=0.5] = 1\npreds[preds<0.5] = 0\n\nprint f1_score(ys, preds, average='macro')\n```\n\n@xieximeng2008 What does it print during training?\n"", '@elanmart Using real time data augmentation\n\n``` python\nEpoch 0\n\n 100/1800 [>.............................] - ETA: 58s - loss: 8.1209\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 200/1800 [==>...........................] - ETA: 55s - loss: 6.7125\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 300/1800 [====>.........................] - ETA: 51s - loss: 6.2430\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 400/1800 [=====>........................] - ETA: 48s - loss: 6.0284\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 500/1800 [=======>......................] - ETA: 44s - loss: 6.1214\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 600/1800 [=========>....................] - ETA: 40s - loss: 5.9915\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 700/1800 [==========>...................] - ETA: 37s - loss: 5.8876\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 800/1800 [============>.................] - ETA: 33s - loss: 5.7681\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 900/1800 [==============>...............] - ETA: 30s - loss: 5.6844\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1000/1800 [===============>..............] - ETA: 27s - loss: 5.6092\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1100/1800 [=================>............] - ETA: 23s - loss: 5.5703\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1200/1800 [===================>..........] - ETA: 20s - loss: 5.5240\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1300/1800 [====================>.........] - ETA: 16s - loss: 5.4976\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1400/1800 [======================>.......] - ETA: 13s - loss: 5.4809\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1500/1800 [========================>.....] - ETA: 10s - loss: 5.4526\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1600/1800 [=========================>....] - ETA: 6s - loss: 5.4486 \x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1700/1800 [===========================>..] - ETA: 3s - loss: 5.4596\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1800/1800 [==============================] - 60s - loss: 5.4326    \nEpoch 1\n\n 100/1800 [>.............................] - ETA: 56s - loss: 5.1808\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 200/1800 [==>...........................] - ETA: 52s - loss: 5.0979\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 300/1800 [====>.........................] - ETA: 49s - loss: 5.1670\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 400/1800 [=====>........................] - ETA: 45s - loss: 5.2326\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 500/1800 [=======>......................] - ETA: 42s - loss: 5.2554\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 600/1800 [=========>....................] - ETA: 39s - loss: 5.2430\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 700/1800 [==========>...................] - ETA: 36s - loss: 5.2104\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 800/1800 [============>.................] - ETA: 33s - loss: 5.1912\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 900/1800 [==============>...............] - ETA: 29s - loss: 5.1716\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1000/1800 [===============>..............] - ETA: 26s - loss: 5.1559\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1100/1800 [=================>............] - ETA: 23s - loss: 5.1318\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1200/1800 [===================>..........] - ETA: 19s - loss: 5.1532\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1300/1800 [====================>.........] - ETA: 16s - loss: 5.1489\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1400/1800 [======================>.......] - ETA: 13s - loss: 5.1512\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1500/1800 [========================>.....] - ETA: 9s - loss: 5.1642 \x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1600/1800 [=========================>....] - ETA: 6s - loss: 5.1549\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1700/1800 [===========================>..] - ETA: 3s - loss: 5.1418\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1800/1800 [==============================] - 59s - loss: 5.1325    \nEpoch 2\n\n 100/1800 [>.............................] - ETA: 56s - loss: 5.2637\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 200/1800 [==>...........................] - ETA: 52s - loss: 5.1394\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 300/1800 [====>.........................] - ETA: 49s - loss: 5.1117\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 400/1800 [=====>........................] - ETA: 46s - loss: 5.0150\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 500/1800 [=======>......................] - ETA: 42s - loss: 5.0150\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 600/1800 [=========>....................] - ETA: 39s - loss: 4.9874\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 700/1800 [==========>...................] - ETA: 36s - loss: 5.0387\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 800/1800 [============>.................] - ETA: 32s - loss: 5.0565\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 900/1800 [==============>...............] - ETA: 29s - loss: 5.0565\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1000/1800 [===============>..............] - ETA: 26s - loss: 5.0813\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1100/1800 [=================>............] - ETA: 23s - loss: 5.0942\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1200/1800 [===================>..........] - ETA: 19s - loss: 5.0876\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1300/1800 [====================>.........] - ETA: 16s - loss: 5.1234\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1400/1800 [======================>.......] - ETA: 13s - loss: 5.1305\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1500/1800 [========================>.....] - ETA: 9s - loss: 5.1256 \x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1600/1800 [=========================>....] - ETA: 6s - loss: 5.1316\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1700/1800 [===========================>..] - ETA: 3s - loss: 5.1296\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1800/1800 [==============================] - 60s - loss: 5.1325    \nEpoch 3\n\n 100/1800 [>.............................] - ETA: 56s - loss: 4.7664\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 200/1800 [==>...........................] - ETA: 52s - loss: 5.0772\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 300/1800 [====>.........................] - ETA: 49s - loss: 5.1394\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 400/1800 [=====>........................] - ETA: 46s - loss: 5.1290\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 500/1800 [=======>......................] - ETA: 42s - loss: 5.1311\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 600/1800 [=========>....................] - ETA: 39s - loss: 5.1601\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 700/1800 [==========>...................] - ETA: 36s - loss: 5.1157\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 800/1800 [============>.................] - ETA: 33s - loss: 5.1497\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 900/1800 [==============>...............] - ETA: 29s - loss: 5.1716\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1000/1800 [===============>..............] - ETA: 26s - loss: 5.1891\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1100/1800 [=================>............] - ETA: 23s - loss: 5.1695\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1200/1800 [===================>..........] - ETA: 19s - loss: 5.1705\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1300/1800 [====================>.........] - ETA: 16s - loss: 5.1585\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1400/1800 [======================>.......] - ETA: 13s - loss: 5.1660\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1500/1800 [========================>.....] - ETA: 9s - loss: 5.1587 \x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1600/1800 [=========================>....] - ETA: 6s - loss: 5.1394\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1700/1800 [===========================>..] - ETA: 3s - loss: 5.1394\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1800/1800 [==============================] - 59s - loss: 5.1325    \nEpoch 4\n\n 100/1800 [>.............................] - ETA: 55s - loss: 5.1394\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 200/1800 [==>...........................] - ETA: 52s - loss: 5.1394\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 300/1800 [====>.........................] - ETA: 49s - loss: 5.1117\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 400/1800 [=====>........................] - ETA: 45s - loss: 5.1601\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 500/1800 [=======>......................] - ETA: 42s - loss: 5.1477\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 600/1800 [=========>....................] - ETA: 39s - loss: 5.1808\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 700/1800 [==========>...................] - ETA: 36s - loss: 5.1334\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 800/1800 [============>.................] - ETA: 32s - loss: 5.1290\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n 900/1800 [==============>...............] - ETA: 29s - loss: 5.1163\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1000/1800 [===============>..............] - ETA: 26s - loss: 5.1311\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1100/1800 [=================>............] - ETA: 23s - loss: 5.1431\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1200/1800 [===================>..........] - ETA: 19s - loss: 5.1394\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1300/1800 [====================>.........] - ETA: 16s - loss: 5.1298\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1400/1800 [======================>.......] - ETA: 13s - loss: 5.1423\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1500/1800 [========================>.....] - ETA: 9s - loss: 5.1338 \x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1600/1800 [=========================>....] - ETA: 6s - loss: 5.1161\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1700/1800 [===========================>..] - ETA: 3s - loss: 5.1174\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n1800/1800 [==============================] - 59s - loss: 5.1325    \n```\n\ntesting...\n\n``` python\n100/200 [==============>...............] - ETA: 1s\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n200/200 [==============================] - 2s     \n[[  0.00000000e+000   0.00000000e+000   0.00000000e+000   0.00000000e+000\n    0.00000000e+000]\n [  0.00000000e+000   0.00000000e+000   0.00000000e+000   0.00000000e+000\n    0.00000000e+000]\n [  0.00000000e+000   0.00000000e+000   0.00000000e+000   0.00000000e+000\n    0.00000000e+000]\n [  0.00000000e+000   0.00000000e+000   0.00000000e+000   0.00000000e+000\n    0.00000000e+000]\n[  1.22857558e-291   0.00000000e+000   3.11779756e-297   0.00000000e+000\n    0.00000000e+000]\n.........\n.........\n```\n\nalmost all outputs are zero or very very small float num\n', '@elanmart  I used your example ,but also have  above problems. dataset : X_train (1800,3,64,64),\nX_test(200,3,64,64)  Y_train(1800,5),Y_test(200,5)  \nI just change the code as you listed\n\n``` python\nmodel.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,validation_data = (X_test,Y_test),verbose = 0)\n    preds = model.predict(X_test)\n    preds[preds>= 0.5] = 1\n    preds[preds<0.5] = 0\n    print (preds)\n```\n\nThanks for helping me!\n', ""@xieximeng2008 I'd guess the problem is in your data, since the network worked well for me few days ago.\n"", '@elanmart \n\nSuppose I want to identify a house no 5436 from an image and I assume every image will have max 4 digits, so one image will be tagged with 4 one hot vectors like \n\n[(0000010000), (0000100000), (0001000000), (0000001000)] and I pass this as a 2D matrix then will it give me probabilities for each element? In this kind of tagging, I want every row to have one element which is most probable (following a probability distribution).\n', ""Does anyone know how to replace the default the validation score by the another scoring function printed at every epoch?   The scoring function for validation set should be similar to the one implemented for test set. Many thanks.\n\nclf.fit(xt, yt, batch_size=64, nb_epoch=300, validation_data=(xs, ys), class_weight=W, verbose=0)\npreds = clf.predict(xs)\npreds[preds>=0.5] = 1\npreds[preds<0.5] = 0\nprint f1_score(ys, preds, average='macro')\n"", '@elanmart \ni have image dataset, each having multiple label and y for particular image is [1,1,-1,-1,-1] where 1==class present and -1==class not present. my question is how to change y so that keras model will accept that y for trainning the data.\n', '@suraj-deshmukh ,Do you solve your problem how to load the multi-label data? How do you do it？ Do you share your code? Thanks.\n', '@alyato , Hi I solved my problem but I lost all my codes :( due to hdd failure. But as I said in previous comment my y/target was [1,1,-1,-1,-1] and I converted it into [1,1,0,0,0] where 1 == presence and 0 == absence for all images and passed that data to ConvNet having binary crossentropy as loss function and sigmoid as activation function for output layer. \n', ""@suraj-deshmukh ,Does i understand it like this.\nfor single label:(total 3)\n\n> `x`   `y`\n> `[1,2,3]`    `[0]`\n> `[4,5,6]`    `[1]`\n> `[7,8,9]`   `[2]`\n\nSo i  load the train_data and  train_label. The format of train_label is  [0,1,2].\n`train_label.shape is (3,)`\nBut for multi-label:(total 3)\n\n> `x`   `y`\n> `[1,2,3]`    `[0,2]`\n> `[4,5,6]`    `[1,2]`\n> `[7,8,9]`   `[0,1]`\n\nThen The format of train_label is [ [1,0,1],[0,1,1],[1,1,0] ]\n`train_label.shape is (3,3)`\n\nIs that right?\nIf it are right,i also have one question.\n\n> for single label,The format of train_label is  [0,1,2].And i need call the function (np_utils.to_categorical),converting it to the  one-hot format\n> \n> for multi-label ,The format of train_label is [ [1,0,1],[0,1,1],[1,1,0] ]\n> I don't call the function (np_utils.to_categorical)\n"", '@alyato  \nyes you are right\n', '@suraj-deshmukh ,Thanks for your answer. But i also have some questions.\n\n> preds[preds>=0.5] = 1\n> preds[preds<0.5] = 0\n1. how to set the Threshold,such as 0.5\n2. If i gets my predict_test_label,how can i compare it with the real_test_label.\n\n> the predict_test_label is \n> [[1,0,1],\n> [0,1,1],\n> [1,1,0]]\n> and the real_test_label is \n> [[1,0,0],\n> [1,0,1],\n> [1,1,0]]\n\nhow to measure my model is better or worse?\n', '@elanmart \n""in softmax when increasing score for one label, all others are lowered (it\'s a probability distribution). You don\'t want that when you have multiple labels.""\n\nI am kind of disagree with the conclusion. Maybe I am wrong. \nsoftmax is just to calculate a normalized exponential value (probability) for each node in the output layer. Assuming there are two target labels out of seven for example, the neural network tries to predict top two posterior probabilities in the specific nodes, and the two probs are definitely the same.\n', 'Hi, I\'m trying to classify an image with multiple digits. Say an image with ""123"" to output ""123"". There are up to 5 digits. \n\nI\'m stuck after I built the convolution layers. How do we output 5 digits each with 10 classes? Some suggested 5 independent fully connected layers after the final convolution layer. But how do we code this in Keras for the 5 independent FCs?\n', '@xieximeng2008 Did you ever find out why your network only returned values close to zero? I am in a similar situation where my network only returns zeroes. I am fine-tuning an InceptionV3 model. Loss function is binary_crossentropy, I am using sigmoid as activation for the final layer, and as an optimizer I use rmsprop.', '@xieximeng2008  check this https://suraj-deshmukh.github.io/Multi-Label-Image-Classification/', 'like this!  modify sgd to Adam， could dec loss! thank @elanmart \r\n@xieximeng2008 , i use this cnn same with you!\r\ncnn --- sigmoid binary_crossentropy adam, this is all!', 'This thread is really helpful!\r\nI have another question. What if my response data is partially missing, i.e. say I have five classes, and most of the data only have partial information on responses, e.g. [1,0,NaN,NaN,1]. \r\nI know I can build individual model for each class, but what if I want to build one single model?\r\n', '@michelleowen I am in no way an expert, but could it maybe work to set the NaN values to 0.5? This might not work in general, and it might be that this value should be tweaked dependent on the problem.', ""@janmatias Yes, I agree it is one workaround, but not perfect. I am thinking to modify the loss function, if the true response is NaN, then don't penalize it in the loss function. However, I am not quite sure which part of the keras code I should modify."", 'Awesome! I still have a question. If the dataset is quite imbalanced, i.e. samples in some categories are much more than others, how can I adopt class_weight to solve this to get a multi-label prediction? Can anybody answer me? @suraj-deshmukh @xieximeng2008 ', '@xieximeng2008  have you ever solve the problem?I have similar problem with you. I use sigmoid function as activiation function and my loss is binary cross entropy loss. As training, the loss did drop. But when feed an image into the network, the output probability is all zero. So weird,how could it happen?', '@vanpersie32 If you have a lot of labels (say 1000) and only 2 of the labels are 1s, the model is happy to assigned 0 to all labels to get a very low binary cross entropy as this is an average across all labels and 998 of 0 will mask the signal from the 2 labels you want to classify. I found this very annoying.', '@jerrypaytm In this case, you need to set sample weight for each sample. When you have 1000 labels, for a particular class, the data with other 999 labels are all negative samples. Then you have to punish hard when a positive sample is marked as negative', '@james97 Thanks! I will try that. It should also speed up the convergence as the signal from the class labels are diluted by the 0 zeros. \r\n\r\nI think most people with multi-label classification will face this issue. Unless there are half-one labels and half-zero labels in the target. Otherwise, the network will think it is doing great by just setting 0 for all labels.', ""@jerrypaytm You are welcome. The process will become a little bit complex here. Model will be compiled with sample_weight_mode='temporal', Y vector will be 3D instead of 2D because each it contains the results of multiple binary outputs instead of one softmax output. Anybody has an easier way?"", '@jerrypaytm I am not sure if you have a problem with skewed label distribution in the training data or with encoding labels as one hot vector and using binary cross entropy instead of categorical.', ""Would it maybe be an alternative to use a different loss function? Like this tensorflow loss function: https://www.tensorflow.org/api_docs/python/tf/nn/weighted_cross_entropy_with_logits\r\n\r\nThe keras [binary_crossentropy](https://github.com/fchollet/keras/blob/2.0.5/keras/backend/tensorflow_backend.py#L2792) loss uses the _sigmoid_cross_entropy_with_logits_ tensorflow function, and tensorflow _weighted_cross_entropy_with_logits_ is ...\r\n> like sigmoid_cross_entropy_with_logits() except that pos_weight, allows one to trade off recall and precision by up- or down-weighting the cost of a positive error relative to a negative error.\r\n\r\nIn the case where you just have a lot of labels and not very imbalanced training data maybe this could help? \r\n\r\nI haven't tried to implement a custom loss function in keras yet though, so I don't know how much effort this would be and if it works well - but if it is not too complicated it might be worth trying?!"", ""@djstrong The problem I'm trying to solve is a 2 out of 64 label classification. A very skewed dataset would force the network to learn the labels that are the majority in the training dataset but my observation is different. All sigmoids in the last layer are happy to produce a very low score. If you look at the math, it does make sense because 62/64 of labels in the target variable are 0. We need a way to penalize the 2 labels (in my case) that are 1 to have stronger signal so that the network takes them seriously. "", ""@tobigue This is the direction I'm moving toward right now. gradient descenting (hopefully). Thanks!"", ""@jerrypaytm You're welcome. I'd be interested if this worked for you and how one can use this tensorflow function in a keras model."", '@jerrypaytm one more thing I remembered - Keras 1.x had an [option to print precision, recall and fmeasure metrics](https://faroit.github.io/keras-docs/1.2.2/metrics/#precision) during training. I found this very helpful when using binary_crossentropy with multiple labels, as all the correctly predicted zeros push the accuracy metric immediately to a very unhelpful high value. I guess it should be still possible using a custom metric function in Keras 2.', ""@tobigue \r\nI am looking for something like this as well. My accuracy goes to 90% after one epoch because I have so many 0's in my label set.  Does anyone have a suggestion for this? Either a custom function or package update?  I am using Keras 2.0.2. "", '@iymitchell You can try updating your class weights.', 'I would suggest to use tanh instead of sigmoid. Tanh distributes values in range (-1; 1), sigmoid distributes in range (0, 1). For optimization point of view it is better when threshold centered around zero, rather than around 0.5 ', 'To summarize:\r\n\r\n* **Loss**: [`binary_crossentropy`](https://keras.io/losses/#binary_crossentropy)\r\n* **Output layer**: not softmax (e.g. sigmoid)\r\n\r\nFor predictions, you can use the pattern\r\n\r\n```\r\npreds = clf.predict(xs)\r\n\r\npreds[preds>=0.5] = 1\r\npreds[preds<0.5] = 0\r\n```', 'Hi @elanmart , I read your explanation about why softmax is not good and it makes perfect sens. But then is there any use case softmax is better then sigmoid + binary_crossentropy? It seems most classification use cases is label mutual exclusive. So it seems softmax is not that useful in most of classification problems? ', '@elanmart hi, I am using below code try to detecting multi-labels on pascal voc data, but the validation loss is increasing from the first round. Following the orignal code, I changed the last layer and use sigmoid and binary_crossentropy, wondering why the training loss is decreasing but why the validation loss is increasing and the accurency is decreasing.\r\n \r\n```\r\n\r\n# -*- coding: utf-8 -*-\r\nimport keras\r\nfrom keras.models import Sequential\r\nfrom keras.optimizers import SGD\r\nfrom keras.layers import Input, Dense, Convolution2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Dropout, Flatten, merge, Reshape, Activation\r\n\r\nfrom sklearn.metrics import log_loss\r\n\r\nfrom load_cifar10 import load_cifar10_data\r\nfrom load_pascal2012 import load_pascal2012_data\r\nimport matplotlib\r\nmatplotlib.use(\'Agg\')\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nfrom keras import backend as K\r\nK.set_image_dim_ordering(\'th\')\r\nimport sklearn.metrics as skm\r\n\r\ndef vgg16_model(img_rows, img_cols, channel=1, num_classes=None):\r\n    """"""VGG 16 Model for Keras\r\n\r\n    Model Schema is based on \r\n    https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3\r\n\r\n    ImageNet Pretrained Weights \r\n    https://drive.google.com/file/d/0Bz7KyqmuGsilT0J5dmRCM0ROVHc/view?usp=sharing\r\n\r\n    Parameters:\r\n      img_rows, img_cols - resolution of inputs\r\n      channel - 1 for grayscale, 3 for color \r\n      num_classes - number of categories for our classification task\r\n    """"""\r\n    model = Sequential()\r\n    model.add(ZeroPadding2D((1, 1), input_shape=(channel, img_rows, img_cols)))\r\n    model.add(Convolution2D(64, 3, 3, activation=\'relu\'))\r\n    model.add(ZeroPadding2D((1, 1)))\r\n    model.add(Convolution2D(64, 3, 3, activation=\'relu\'))\r\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\r\n\r\n    model.add(ZeroPadding2D((1, 1)))\r\n    model.add(Convolution2D(128, 3, 3, activation=\'relu\'))\r\n    model.add(ZeroPadding2D((1, 1)))\r\n    model.add(Convolution2D(128, 3, 3, activation=\'relu\'))\r\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\r\n\r\n    model.add(ZeroPadding2D((1, 1)))\r\n    model.add(Convolution2D(256, 3, 3, activation=\'relu\'))\r\n    model.add(ZeroPadding2D((1, 1)))\r\n    model.add(Convolution2D(256, 3, 3, activation=\'relu\'))\r\n    model.add(ZeroPadding2D((1, 1)))\r\n    model.add(Convolution2D(256, 3, 3, activation=\'relu\'))\r\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\r\n\r\n    model.add(ZeroPadding2D((1, 1)))\r\n    model.add(Convolution2D(512, 3, 3, activation=\'relu\'))\r\n    model.add(ZeroPadding2D((1, 1)))\r\n    model.add(Convolution2D(512, 3, 3, activation=\'relu\'))\r\n    model.add(ZeroPadding2D((1, 1)))\r\n    model.add(Convolution2D(512, 3, 3, activation=\'relu\'))\r\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\r\n\r\n    model.add(ZeroPadding2D((1, 1)))\r\n    model.add(Convolution2D(512, 3, 3, activation=\'relu\'))\r\n    model.add(ZeroPadding2D((1, 1)))\r\n    model.add(Convolution2D(512, 3, 3, activation=\'relu\'))\r\n    model.add(ZeroPadding2D((1, 1)))\r\n    model.add(Convolution2D(512, 3, 3, activation=\'relu\'))\r\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\r\n\r\n    # Add Fully Connected Layer\r\n    model.add(Flatten())\r\n    model.add(Dense(4096, activation=\'relu\'))\r\n    model.add(Dropout(0.5))\r\n    model.add(Dense(4096, activation=\'relu\'))\r\n    model.add(Dropout(0.5))\r\n    model.add(Dense(1000, activation=\'softmax\'))\r\n\r\n    # Loads ImageNet pre-trained data\r\n    model.load_weights(\'imagenet_models/vgg16_weights_th_dim_ordering_th_kernels.h5\')\r\n\r\n    # Truncate and replace softmax layer for transfer learning\r\n    model.layers.pop()\r\n    model.outputs = [model.layers[-1].output]\r\n    model.layers[-1].outbound_nodes = []\r\n    model.add(Dense(num_classes, activation=\'sigmoid\'))\r\n\r\n    # Uncomment below to set the first 10 layers to non-trainable (weights will not be updated)\r\n    #for layer in model.layers[:10]:\r\n    #    layer.trainable = False\r\n\r\n    # Learning rate is changed to 0.001\r\n    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\r\n    model.compile(optimizer=sgd,\r\n                  loss=\'binary_crossentropy\',\r\n                  metrics=[\'accuracy\'])\r\n\r\n    return model\r\n\r\nif __name__ == \'__main__\':\r\n\r\n    # Example to fine-tune on 3000 samples from Cifar10\r\n\r\n    img_rows, img_cols = 224, 224 # Resolution of inputs\r\n    channel = 3\r\n    num_classes = 20\r\n    batch_size = 16 \r\n    nb_epoch = 10\r\n\r\n    # Load Cifar10 data. Please implement your own load_data() module for your own dataset\r\n    # X_train, Y_train, X_valid, Y_valid = load_cifar10_data(img_rows, img_cols)\r\n    X_train, Y_train, X_valid, Y_valid = load_pascal2012_data(img_rows, img_cols)\r\n\r\n\r\n    # Load our model\r\n    model = vgg16_model(img_rows, img_cols, channel, num_classes)\r\n\r\n    # Start Fine-tuning\r\n    history = model.fit(X_train, Y_train,\r\n              batch_size=batch_size,\r\n              epochs=nb_epoch,\r\n              shuffle=True,\r\n              verbose=1,\r\n              validation_data=(X_valid, Y_valid),\r\n              )\r\n\r\n    # Make predictions\r\n    predictions_valid = model.predict(X_valid, batch_size=batch_size, verbose=1)\r\n\r\n\r\n    # Cross-entropy loss score\r\n    score = log_loss(Y_valid, predictions_valid)\r\n    print(score)\r\n```\r\n\r\n```\r\nEpoch 1/10\r\n3000/3000 [==============================] - 255s - loss: 0.2229 - acc: 0.9338 - val_loss: 0.3628 - val_acc: 0.9150\r\nEpoch 2/10\r\n3000/3000 [==============================] - 256s - loss: 0.1510 - acc: 0.9487 - val_loss: 0.4318 - val_acc: 0.9025\r\nEpoch 3/10\r\n3000/3000 [==============================] - 256s - loss: 0.1230 - acc: 0.9556 - val_loss: 0.4887 - val_acc: 0.8980\r\nEpoch 4/10\r\n3000/3000 [==============================] - 257s - loss: 0.1064 - acc: 0.9608 - val_loss: 0.5058 - val_acc: 0.8985\r\nEpoch 5/10\r\n3000/3000 [==============================] - 257s - loss: 0.0946 - acc: 0.9639 - val_loss: 0.5580 - val_acc: 0.8940\r\nEpoch 6/10\r\n3000/3000 [==============================] - 257s - loss: 0.0848 - acc: 0.9663 - val_loss: 0.5640 - val_acc: 0.8965\r\nEpoch 7/10\r\n3000/3000 [==============================] - 257s - loss: 0.0782 - acc: 0.9681 - val_loss: 0.5811 - val_acc: 0.8940\r\nEpoch 8/10\r\n3000/3000 [==============================] - 257s - loss: 0.0709 - acc: 0.9700 - val_loss: 0.6254 - val_acc: 0.8930\r\nEpoch 9/10\r\n3000/3000 [==============================] - 257s - loss: 0.0667 - acc: 0.9714 - val_loss: 0.6396 - val_acc: 0.8910\r\n```', ""@elanmart how would you update your model using an Embedding layer and multiple LSTM layers?\r\n\r\n\r\n```\r\n# Build a classifier optimized for maximizing f1_score (uses class_weights)\r\n\r\nclf = Sequential()\r\n\r\nclf.add(Dropout(0.3))\r\nclf.add(Dense(xt.shape[1], 1600, activation='relu'))\r\nclf.add(Dropout(0.6))\r\nclf.add(Dense(1600, 1200, activation='relu'))\r\nclf.add(Dropout(0.6))\r\nclf.add(Dense(1200, 800, activation='relu'))\r\nclf.add(Dropout(0.6))\r\nclf.add(Dense(800, yt.shape[1], activation='sigmoid'))\r\n\r\nclf.compile(optimizer=Adam(), loss='binary_crossentropy')\r\n\r\nclf.fit(xt, yt, batch_size=64, nb_epoch=300, validation_data=(xs, ys), class_weight=W, verbose=0)\r\n\r\npreds = clf.predict(xs)\r\n\r\npreds[preds>=0.5] = 1\r\npreds[preds<0.5] = 0\r\n\r\nprint f1_score(ys, preds, average='macro')\r\n```"", ""Agree on `binary_crossentropy` as a loss.\r\n\r\nIf multi-labels are sparse (i.e. many zeros and a few ones for each output) the network will reply with small values, and given a threshold of 0.5 as suggested above does not cut it.\r\n\r\nOne should manually find a threshold as suggested in the link by @suraj-deshmukh.\r\n\r\nI got a little improvement by using `tanh` as final layer activation function instead of the `sigmoid`, as suggested by @jurastm.\r\n\r\nStill, convergence is really slow and there should be a better solution.\r\nI was thinking about giving more weight to the ones via `class_weight` but can't understand how that works with multilabel output.\r\n\r\nHelp is appreciated :D"", ""i'm trying to solve a similar requirement. Classifier using data with many labels (more than 200), with all of the labels being binary (flags). And for most rows the values are 0.\r\n\r\nPlease help pieroit and I. I believe the solution lies somewhere in adjusting weights for the 1s.\r\n\r\nHow to adjust weights with class_weights for input data?"", '@pieroit @bryan831 you could try to give more weight to positive targets in the loss function.\r\n\r\nIf you use the tensorflow backend of keras you can use [`tf.nn.weighted_cross_entropy_with_logits`](https://www.tensorflow.org/api_docs/python/tf/nn/weighted_cross_entropy_with_logits) like this: https://stackoverflow.com/a/47313183/979377\r\n\r\nWould be interested to hear if this worked for you and how you set the POS_WEIGHT in relation to your number of classes!\r\n', ""Hi! I am facing a bit different problem in training multi-label classifier.\r\nI use  *sigmoid* and  *binary cross entropy* for training,\r\nhowever, the network's output got almost same values among images, like below.\r\nI have 200 classes, and now its output is not appropriate.\r\n~~~~\r\n    input_tensor = Input(shape=(img_rows, img_cols, n_channels))\r\n    vgg16 = VGG16(include_top=False, weights='imagenet', input_tensor=input_tensor)\r\n    top_model = Sequential()\r\n    top_model.add(Flatten(input_shape=vgg16.output_shape[1:]))\r\n    top_model.add(Dense(4096, activation='relu'))\r\n    top_model.add(Dropout(0.5))\r\n    top_model.add(Dense(4096, activation='relu'))\r\n    top_model.add(Dropout(0.5))\r\n    top_model.add(Dense(nb_classes, activation='sigmoid', init='glorot_uniform'))\r\n    model = Model(input=vgg16.input, output=top_model(vgg16.output))\r\n    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\r\n~~~~\r\n~~~~\r\nimage001:   [[0.94, 0.03, 0.01, 0.91, ... , 0.91]]\r\nimage002:    [[0.93, 0.02, 0.01, 0.93, ... , 0.93]]\r\nimage003:    [[0.91, 0.02, 0.01, 0.92, ... , 0.92]]\r\n~~~~\r\nPlease tell me how to deal with this problem. "", ""@pieroit @bryan831 \r\nI'm facing exactly the same issue as you do. I'm wondering did you use the method @tobigue suggested and how does that work? Could you show me how did you solve this problem? FYI I tried class_weight = {0:1, 1:20} but it did not work and error out, looks like it does not work for multi-dimensional output."", '@hpnhxxwn did you try out the code I posted in the stackoverflow answer? Should be easy for you to test with copy & paste if you use the tensorflow backend.', ""Instead of:\r\n\r\n```\r\npreds = clf.predict(xs)\r\npreds[preds>=0.5] = 1\r\npreds[preds<0.5] = 0\r\n```\r\n\r\nyou can just write:\r\n\r\n```\r\npreds = (clf.predict(xs) >= 0.5).astype(int)\r\n```\r\n\r\nWe threshold the probabilities to obtain a boolean vector which we in turn convert to integers. It's less imperative than two assignments. Possibly you could either keep the booleans without conversion."", 'I am facing the problem with input shape of model for categorical classifier\r\nx              y\r\n[1,2,3]    [0]\r\n[2,3,5]    [1]\r\n[2,1,6]    [2]\r\n[1,2,3]    [0]\r\n[2,3,5]    [0]\r\n[2,1,6]    [2]\r\nthen i changed the y label into categorical as\r\n[1,0,0]\r\n[0,1,0]\r\n[0,0,1]\r\n[1,0,0]\r\n[1,0,0]\r\n[0,0,1]\r\nand my x_train shape is (6000,3)\r\ny_train shape is (6000,3)\r\nx_test shape is (2000,3)\r\ny_test shape is (2000,3)\r\n\r\ni tried this model and getting value error\r\n\r\nmodel=sequential()\r\nmodel.add(Dense(1, input_shape(3,), activation=""softmax""))\r\nmodel.compile(Adam(lr=0.5), \'categorical_crossentropy\', metrics=[\'accuracy\'])\r\nmodel.fit(X_train,y_train,epochs=50, verbose=1)\r\n\r\nValue error: Error when checking target: expected dense_1 to have shape(None,1) but got array with shape (6000,3)\r\n\r\ni dont understand this error. help me to sort out this\r\n', ""facing the same problem as @vijaycol ... but in my case, it's about image segmentation. I am passing X_train of shape (209,256,256,3). i.e. 209 images,256x256 size of each and 3 channels. It's everywhere mentioned that Y_train should be one-hot encoder but this will give error while using one-hot encoder.What to do? Suggest any solution asap. \r\n\r\nThe above-discussed problem, I am facing in the following code.\r\n\r\nmodel=Sequential()\r\nmodel.add(Conv2D(32,kernel_size=(3,3),input_shape=(x,y,z),padding='same',data_format='channels_last',kernel_initializer='ones',bias_initializer='zeros'))\r\nmodel.add(Activation('relu'))\r\nmodel.compile(optimizer='sgd',loss='categorical_crossentropy')\r\nmodel.fit(X_trainY_train,epochs=10,batch_size=1)"", '@hpts23 Hey, did you solve your problem? I have exactly the same issue when using vgg16 for binary classification.', 'Hi, if you are doing multi-label classification, you need to use the\nmulti-label binarizer instead of one hot.\nᐧ\n\nOn Fri, Apr 27, 2018 at 1:57 AM, SpecKROELLchen <notifications@github.com>\nwrote:\n\n> @hpts23 <https://github.com/hpts23> Hey, did you solve your problem? I\n> have exactly the same issue when using vgg16 for binary classification.\n>\n> —\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/keras-team/keras/issues/741#issuecomment-384909768>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AX71c5F1t9jWIKqzH8OQDKn_bXS1KEV6ks5tst2UgaJpZM4GE5pO>\n> .\n>\n\n\n\n-- \nBest Regards,\n\nStephen Lizcano\n\n+49 176 4762 5344\n+1 (626) 695 4868\n', 'That is of course what i am already doing. I use binary_crossentropy with a sigmoid activation.\r\nBut my CNN still does not do what it should.\r\nI already posted here #10040, so i will explain my problem in more detail there.', 'Hi, I am trying to do a multi-label classification on an image dataset of size 2.2M. I have seen people often use flow_from_directory and flow to train the network in batches. I cannot go for flow from directory as it is a multi-label problem and for using flow I need to load all my data in an array.\r\n\r\nCan someone please suggest a better way of doing it?\r\n\r\nThanks! ', '@agupt013  Why you cannot use flow or flow_from_directory for multi-label ? Can you give a valid reason for it ?', 'Hi @mohapatras For each image I have 5 labels out of 20 classes. My understanding of flow_from_directory is that images are placed in a subdirectory of the respective class. I want to compute loss such that I pass all 5 labels and respective predictions to the loss function.', 'I have similar but slightly different problems. I have multi-labels in one sample. Of each label, the class  is mutually exclusive. So the target is more like concat( [0, 0, 1], [0, 1], [0, 0, 0, 1, 0] ) for one row. In  such a case, should I train 3 separate models using `softmax` for each label, or can I also use `sigmoid` with `binary_crossentropy` loss function in one model？', '@luoshao23 you can train a model with multiple outputs: https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models. If 3 models or 1 model is a better choice probably depends on the task.', '@tobigue Thank you for your answer! So does it mean it is not a good choice to model this problem with only one output that is concatenated together in one vector?', 'I need to classify attributes in a face like colour of eye, hair, skin; facial hair, lighting and so on. Each has few sub-categories in it. So should I directly apply sigmoid on all the labels or separately apply softmax on each subcategory like hair/eye colour etc?\r\nWhich one will be better in this case?\r\nOr should I combine both as some subclasses are binary?', 'I wrote an explanatory blog post about multi-label classification and there is also an example with keras. https://www.depends-on-the-definition.com/guide-to-multi-label-classification-with-neural-networks/', '@tsterbak great tutorial! Everybody in this thread should read it', 'Closing as this is resolved', 'I think this is still an unsolved query. And a lot of ppl are struggling to implement the multi-labeled models. Even after using sigmoid activation and binary cross-entropy the predicted probability distribution is almost near to zero for all the classes per samples. I think we really need to dig into the K.binarycross-entropy loss which in  return calls nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output).', '@srijandas07 have you tried to give more weight to positive targets in the loss function?\r\n\r\nIf you use the tensorflow backend of keras you can use [`tf.nn.weighted_cross_entropy_with_logits`](https://www.tensorflow.org/api_docs/python/tf/nn/weighted_cross_entropy_with_logits) like this: https://stackoverflow.com/a/47313183/979377', 'great! I will try to use this!!', ""@tobigue Even in  tf.nn.weighted_cross_entropy_with_logits sigmoid has been used.\r\nIn that case, I don't think we need to add sigmoid activation in the last dense layer for the multi-label classification task."", '@srijandas07 There are probably other activations and loss functions to explore, but using the sigmoid is to my best knowledge the standard for multi-label classification. The weighted version allows you to give a higher penalty when the classifier predicted a 0 while the target was a 1, which should improve the problem of getting predictions that are all close to zero.', '@tobigue I have removed the activation and used nn.binarycrossentropy with logits. And this seems to work. After going through the function, I could infer that they implicitly use sigmoid to compute the loss. ', '> @lemuriandezapada yeah,\r\n> \r\n> ```python\r\n> labels = np.zeros(preds.shape)\r\n> labels[preds>0.5] = 1\r\n> ```\r\n> \r\n> @arushi02 in softmax when increasing score for one label, all others are lowered (it\'s a probability distribution). You don\'t want that when you have multiple labels.\r\n> No, you don\'t need `Graph`\r\n> \r\n> Here\'s an example of one of my multilabel nets:\r\n> \r\n> ```python\r\n> # Build a classifier optimized for maximizing f1_score (uses class_weights)\r\n> \r\n> clf = Sequential()\r\n> \r\n> clf.add(Dropout(0.3))\r\n> clf.add(Dense(xt.shape[1], 1600, activation=\'relu\'))\r\n> clf.add(Dropout(0.6))\r\n> clf.add(Dense(1600, 1200, activation=\'relu\'))\r\n> clf.add(Dropout(0.6))\r\n> clf.add(Dense(1200, 800, activation=\'relu\'))\r\n> clf.add(Dropout(0.6))\r\n> clf.add(Dense(800, yt.shape[1], activation=\'sigmoid\'))\r\n> \r\n> clf.compile(optimizer=Adam(), loss=\'binary_crossentropy\')\r\n> \r\n> clf.fit(xt, yt, batch_size=64, nb_epoch=300, validation_data=(xs, ys), class_weight=W, verbose=0)\r\n> \r\n> preds = clf.predict(xs)\r\n> \r\n> preds[preds>=0.5] = 1\r\n> preds[preds<0.5] = 0\r\n> \r\n> print f1_score(ys, preds, average=\'macro\')\r\n> ```\r\n> \r\n> @xieximeng2008 What does it print during training?\r\n\r\nCan we apply different weights for different ""labels"" using this approach of binary cross-entropy? How is W structured here? @elanmart']",[],[],0,0
232,keras,13105,closed,Custom BatchNormalization layer in ResNet,"**System information**  
- Have I written custom code (as opposed to using example directory):  
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Arch Linux
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  1.13.1
- Keras version:  2.1.6
- Python version:  3.7
- CUDA/cuDNN version:  10
- GPU model and memory:  GTX 1070

Hello, I am using Faster-RCNN with ResNet50 as base network for object detection. It uses a custom BatchNormalization layer named 'FixedBatchNormalization' in ResNet. I want to know what's the difference between this custom layer and the official BatchNormalization layer in keras. I know that there is an ongoing issue with fine-tuning ResNet when freezing layers. So does the 'FixedBatchNormalization' in this repo solve that issue and does this anyway affect the training and evaluating?

And this layer works only with keras version 2.1.6.
Link to the Faster-RCNN repo:
[https://github.com/kbardool/keras-frcnn](url)

**FixedBatchNormalization code:**



",type:bug/performance,"[""Hi @datumbox, I saw your posts on Batchnormalization. But I couldn't get to any conclusion. How is this implementation different from yours?"", ""@datumbox @fchollet Can someone please tell me how to fine-tune ResNet50 here? I want to freeze all Conv blocks except the last one and training the remaining. What is the right way to freeze batchnorm layers so that my validation loss and accuracy improves?\r\n\r\n`x = BatchNormalization(axis=3, name=bn_name_base + '2a', trainable = False)(x)`\r\n(or)\r\n`x = BatchNormalization(axis=3, name=bn_name_base + '2a', trainable =False)(x, training = False)`\r\n\r\nWhich of the above implementation is correct?""]",[],"[""from keras.engine import Layer, InputSpec\r\nfrom keras import initializers, regularizers\r\nfrom keras import backend as K\r\n\r\n\r\nclass FixedBatchNormalization(Layer):\r\n\r\n    def __init__(self, epsilon=1e-3, axis=-1,\r\n                 weights=None, beta_init='zero', gamma_init='one',\r\n                 gamma_regularizer=None, beta_regularizer=None, **kwargs):\r\n\r\n        self.supports_masking = True\r\n        self.beta_init = initializers.get(beta_init)\r\n        self.gamma_init = initializers.get(gamma_init)\r\n        self.epsilon = epsilon\r\n        self.axis = axis\r\n        self.gamma_regularizer = regularizers.get(gamma_regularizer)\r\n        self.beta_regularizer = regularizers.get(beta_regularizer)\r\n        self.initial_weights = weights\r\n        super(FixedBatchNormalization, self).__init__(**kwargs)\r\n\r\n    def build(self, input_shape):\r\n        self.input_spec = [InputSpec(shape=input_shape)]\r\n        shape = (input_shape[self.axis],)\r\n\r\n        self.gamma = self.add_weight(shape,\r\n                                     initializer=self.gamma_init,\r\n                                     regularizer=self.gamma_regularizer,\r\n                                     name='{}_gamma'.format(self.name),\r\n                                     trainable=False)\r\n        self.beta = self.add_weight(shape,\r\n                                    initializer=self.beta_init,\r\n                                    regularizer=self.beta_regularizer,\r\n                                    name='{}_beta'.format(self.name),\r\n                                    trainable=False)\r\n        self.running_mean = self.add_weight(shape, initializer='zero',\r\n                                            name='{}_running_mean'.format(self.name),\r\n                                            trainable=False)\r\n        self.running_std = self.add_weight(shape, initializer='one',\r\n                                           name='{}_running_std'.format(self.name),\r\n                                           trainable=False)\r\n\r\n        if self.initial_weights is not None:\r\n            self.set_weights(self.initial_weights)\r\n            del self.initial_weights\r\n\r\n        self.built = True\r\n\r\n    def call(self, x, mask=None):\r\n\r\n        assert self.built, 'Layer must be built before being called'\r\n        input_shape = K.int_shape(x)\r\n\r\n        reduction_axes = list(range(len(input_shape)))\r\n        del reduction_axes[self.axis]\r\n        broadcast_shape = [1] * len(input_shape)\r\n        broadcast_shape[self.axis] = input_shape[self.axis]\r\n\r\n        if sorted(reduction_axes) == range(K.ndim(x))[:-1]:\r\n            x_normed = K.batch_normalization(\r\n                x, self.running_mean, self.running_std,\r\n                self.beta, self.gamma,\r\n                epsilon=self.epsilon)\r\n        else:\r\n            # need broadcasting\r\n            broadcast_running_mean = K.reshape(self.running_mean, broadcast_shape)\r\n            broadcast_running_std = K.reshape(self.running_std, broadcast_shape)\r\n            broadcast_beta = K.reshape(self.beta, broadcast_shape)\r\n            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\r\n            x_normed = K.batch_normalization(\r\n                x, broadcast_running_mean, broadcast_running_std,\r\n                broadcast_beta, broadcast_gamma,\r\n                epsilon=self.epsilon)\r\n\r\n        return x_normed\r\n\r\n    def get_config(self):\r\n        config = {'epsilon': self.epsilon,\r\n                  'axis': self.axis,\r\n                  'gamma_regularizer': self.gamma_regularizer.get_config() if self.gamma_regularizer else None,\r\n                  'beta_regularizer': self.beta_regularizer.get_config() if self.beta_regularizer else None}\r\n        base_config = super(FixedBatchNormalization, self).get_config()\r\n        return dict(list(base_config.items()) + list(config.items()))""]",0,0
233,keras,2466,closed,"error: Wrong number of dimensions: expected 4, got 3 with shape","I got this error


here is my code


as you can see when I printed trainX shape I got (28709L, 48L, 48L) so my data dim is correct
",,"['You should reshape your data into `(num_data, channel, row, col)`, in your case `(28709, 1, 48, 48)`\n', 'I got this error after reshape the data\n\n`Using Theano backend.\nUsing gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN 5004)\n(28709L, 1L, 48L, 48L)\n(3589L, 1L, 48L, 48L)\nTrain on 28709 samples, validate on 3589 samples\nEpoch 1/50\nTraceback (most recent call last):\n  File ""train.py"", line 171, in <module>\n    history = model.fit(trainX, trainY, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1,validation_data=(validX,vali\ndY), callbacks=[checkpointer])\n  File ""c:\\keras\\keras\\models.py"", line 402, in fit\n    sample_weight=sample_weight)\n  File ""c:\\keras\\keras\\engine\\training.py"", line 1036, in fit\n    callback_metrics=callback_metrics)\n  File ""c:\\keras\\keras\\engine\\training.py"", line 774, in _fit_loop\n    outs = f(ins_batch)\n  File ""c:\\keras\\keras\\backend\\theano_backend.py"", line 499, in __call__\n    return self.function(*inputs)\n  File ""c:\\theano\\theano\\compile\\function_module.py"", line 902, in **call**\n    storage_map=getattr(self.fn, \'storage_map\', None))\n  File ""c:\\theano\\theano\\gof\\link.py"", line 314, in raise_with_op\n    reraise(exc_type, exc_value, exc_trace)\n  File ""c:\\theano\\theano\\compile\\function_module.py"", line 889, in **call**\n    self.fn() if output_subset is None else\\\nRuntimeError: Could not set tensorNd descriptor: CUDNN_STATUS_BAD_PARAMdim=4\nApply node that caused the error: GpuDnnConv{algo=\'small\', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty\n.0, GpuDnnConvDesc{border_mode=\'valid\', subsample=(1, 1), conv_mode=\'conv\', precision=\'float32\'}.0, Constant{1.0}, Const\nant{0.0})\nToposort index: 277\nInputs types: [CudaNdarrayType(float32, 4D), CudaNdarrayType(float32, 4D), CudaNdarrayType(float32, 4D), <theano.gof.typ\ne.CDataType object at 0x0000000023F2A3C8>, Scalar(float32), Scalar(float32)]\nInputs shapes: [(128, 128, 2, 2), (256, 128, 3, 3), (128, 256, 0, 0), \'No shapes\', (), ()]\nInputs strides: [(512, 4, 2, 1), (1152, 9, 3, 1), (0, 0, 0, 1), \'No strides\', (), ()]\nInputs values: [\'not shown\', \'not shown\', CudaNdarray([]), <PyCObject object at 0x0000000026712A08>, 1.0, 0.0]\nInputs name: (\'image\', \'kernel\', \'output\', \'descriptor\', \'alpha\', \'beta\')\n\nOutputs clients: [[GpuElemwise{Add}[(0, 0)](GpuDnnConv{algo=\'small\', inplace=True}.0, GpuReshape{4}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This\ncan be done with by setting the Theano flag \'optimizer=fast_compile\'. If that does not work, Theano optimizations can be\n disabled with \'optimizer=None\'.\nHINT: Use the Theano flag \'exception_verbosity=high\' for a debugprint and storage map footprint of this apply node.`\n', ""Do you design your model by yourself? You input is too small to use this network. I can run it with 96x96 image without problem. In your case:\n48x48 -> 23x23 -> 9x9 -> 2x2 -> boom! \nYou should add `border_type='same'` for each your conv layers (VGG style).\n""]",[],"['(28709L, 48L, 48L)\n(3589L, 48L, 48L)\nTrain on 28709 samples, validate on 3589 samples\nEpoch 1/50\nTraceback (most recent call last):\n  File ""train.py"", line 171, in <module>\n    history = model.fit(trainX, trainY, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1,validation_data=(validX,vali\ndY), callbacks=[checkpointer])\n  File ""c:\\keras\\keras\\models.py"", line 402, in fit\n    sample_weight=sample_weight)\n  File ""c:\\keras\\keras\\engine\\training.py"", line 1036, in fit\n    callback_metrics=callback_metrics)\n  File ""c:\\keras\\keras\\engine\\training.py"", line 774, in _fit_loop\n    outs = f(ins_batch)\n  File ""c:\\keras\\keras\\backend\\theano_backend.py"", line 499, in __call__\n    return self.function(*inputs)\n  File ""c:\\theano\\theano\\compile\\function_module.py"", line 815, in __call__\n    allow_downcast=s.allow_downcast)\n  File ""c:\\theano\\theano\\tensor\\type.py"", line 178, in filter\n    data.shape))\nTypeError: (\'Bad input argument to theano function with name ""c:\\\\keras\\\\keras\\\\backend\\\\theano_backend.py:495""  at inde\nx 0(0-based)\', \'Wrong number of dimensions: expected 4, got 3 with shape (128L, 48L, 48L).\')', 'img_rows = img_cols = 48\nconv_input_shape=(1, img_rows, img_cols)\nconv_nb_row = 3\nconv_nb_col = 3\nconv_nb_pool = 2\ndense_out = 7\ndecay = l2(0.01)\n\nmodel = Sequential()\nmodel.add(Convolution2D(32,conv_nb_row,conv_nb_col,border_mode=\'same\',input_shape=conv_input_shape,W_regularizer=decay))\nmodel.add(Activation(\'relu\'))\nmodel.add(Convolution2D(32, conv_nb_row, conv_nb_row,W_regularizer=decay))\nmodel.add(Activation(\'relu\'))\nmodel.add(MaxPooling2D(pool_size=(conv_nb_pool, conv_nb_pool)))\nmodel.add(Dropout(0.1))\n\nmodel.add(Convolution2D(64, conv_nb_row, conv_nb_row,W_regularizer=decay))\nmodel.add(Activation(\'relu\'))\nmodel.add(Convolution2D(64, conv_nb_row, conv_nb_row,W_regularizer=decay))\nmodel.add(Activation(\'relu\'))\nmodel.add(MaxPooling2D(pool_size=(conv_nb_pool, conv_nb_pool)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Convolution2D(128, conv_nb_row, conv_nb_row,W_regularizer=decay))\nmodel.add(Activation(\'relu\'))\nmodel.add(Convolution2D(128, conv_nb_row, conv_nb_row,W_regularizer=decay))\nmodel.add(Activation(\'relu\'))\nmodel.add(MaxPooling2D(pool_size=(conv_nb_pool, conv_nb_pool)))\nmodel.add(Dropout(0.3))\n\nmodel.add(Convolution2D(256, conv_nb_row, conv_nb_row,W_regularizer=decay))\nmodel.add(Activation(\'relu\'))\nmodel.add(Convolution2D(256, conv_nb_row, conv_nb_row,W_regularizer=decay))\nmodel.add(Activation(\'relu\'))\nmodel.add(MaxPooling2D(pool_size=(conv_nb_pool, conv_nb_pool)))\nmodel.add(Dropout(0.4))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(dense_out,W_regularizer=decay))\nmodel.add(Activation(\'softmax\'))\nmodel.compile(loss=\'categorical_crossentropy\', optimizer=\'Adam\',metrics=[\'accuracy\'])\n\nbatch_size = 128\nnb_epoch = 50\n\ncheckpointer = ModelCheckpoint(filepath=""best.hdf5"",monitor=\'val_acc\')\ntrainX = np.load(\'fer2013\\xtrain.npy\')\ntrainY = np.load(\'fer2013\\ytrain.npy\')\ntrainX, trainY = normaliaze(trainX,trainY)\nprint trainX.shape\nvalidX = np.load(\'fer2013\\xvalid.npy\')\nvalidY = np.load(\'fer2013\\yvalid.npy\')\nvalidX, validY = normaliaze(validX, validY)\nprint validX.shape\n\nhistory = model.fit(trainX, trainY, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1,validation_data=(validX,validY), callbacks=[checkpointer])']",0,0
234,keras,4232,closed,Accessing the softmax output of previous RNN state,"The recurrence formula in my RNN is , where  is the classification output (Softmax output) of the RNN at time . How can I access this output at the next time step?
I guess I have to write a custom RNN, but I am a bit confused by all the recurrent functions in recurrent.py in Keras and I am not sure which parts should be modified. ",stale,"['Use recurrentshop.\n', '@farizrahman4u Thanks a lot for your response. \nI installed recurrentshop and tried to import it using  `from recurrentshop import *` , but I got the following error:\n\n`Using Theano backend.\nUsing gpu device 0: GeForce GTX 1080 (CNMeM is disabled, cuDNN 5005)\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""recurrentshop/__init__.py"", line 1, in <module>\n    from .engine import RNNCell, RecurrentContainer, weight\n  File ""recurrentshop/engine.py"", line 32, in <module>\n    _backend = getattr(K, K.backend() + \'_backend\')\nAttributeError: \'module\' object has no attribute \'backend\'`\n\nFurthermore, my problem with including the softmax output to the recurrent fiormula is that I don\'t know where to add the softmax activation, should it be within the `step` function in the custom RNN cell, or it still needs to be applied to the sequential model as a final layer?\nI\'d really appreciate it if you could add a small sample code to help me understand how to customize it.\n', ""Update keras. Just add a softmax activation to the recurrent container, with '`readout=True`. The output from the softmax activation will be added to the input in the next timestep. \n"", '@farizrahman4u Thanks for the tips. \nIn the step function, in order to extract the output `O(t-1)` vector from the input vector `x`, I tried to print the shape of `x`, but it is in form of a TensorVariable which gave me shape.0. So I am not sure how to obtain the recurred output classes. I need to have access to `O(t-1)` separately because I should apply a non-linear application to it and then use it in the recurrence formula. So in fact what I am after is `h(t) = tanh(W.x + U.h(t-1) + V.f(O(t-1)) + b`\nPlease bear with me as I am new to Keras. \n', 'The problem is that I do not want to directly merge `O(t-1)` with `x(t)`, but I want to have separate access to `O(t-1)` in the `build` function of `SimpleRNN` to process it and then pass it to the recurrence formulation.\n', '@farizrahman4u \nI wrote the following sample code:\n\n```\ndim = 20\nncls = 4\nts = 6\n\nrc = RecurrentContainer(readout=True, return_sequences=True)\nrc.add(SimpleRNNCell(10, input_dim=dim))\nrc.add(Dropout(0.5))\nrc.add((Dense(ncls)))\nrc.add(Activation(\'softmax\'))\n\na = Input((ts, dim))\nb = rc(a)\nmodel = Model(a, b)\nmodel.compile(loss=\'mse\', optimizer=\'sgd\')\n\nx = np.random.random((1000, ts, dim))\ny = np.random.random((1000, ts, ncls))\nmodel.fit(x, y, nb_epoch=5)\n```\n\nthat produces this error:\n\n```\nFile ""theano/scan_module/scan_perform.pyx"", line 397, in theano.scan_module.scan_perform.perform (/home/monaj/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-2.7.12-64/scan_perform/mod.cpp:4193)\nValueError: GpuElemwise. Input dimension mis-match. Input 1 (indices start at 0) has shape[1] == 4, but the output\'s size on that axis is 20.\nApply node that caused the error: GpuElemwise{add,no_inplace}(<CudaNdarrayType(float32, matrix)>, <CudaNdarrayType(float32, matrix)>)\nToposort index: 4\nInputs types: [CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix)]\nInputs shapes: [(32, 20), (32, 4)]\nInputs strides: [(120, 1), (4, 1)]\nInputs values: [\'not shown\', \'not shown\']\nOutputs clients: [[GpuDot22(GpuElemwise{add,no_inplace}.0, simplernncell_1_W_copy0[cuda])]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag \'optimizer=fast_compile\'. If that does not work, Theano optimizations can be disabled with \'optimizer=None\'.\nHINT: Use the Theano flag \'exception_verbosity=high\' for a debugprint and storage map footprint of this apply node.\nApply node that caused the error: forall_inplace,gpu,scan_fn&scan_fn}(Shape_i{1}.0, GpuSubtensor{int64:int64:int16}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, DeepCopyOp.0, GpuIncSubtensor{InplaceSet;:int64:}.0, Shape_i{1}.0, Shape_i{1}.0, simplernncell_1_W, simplernncell_1_U, dense_1_W, GpuDimShuffle{x,0}.0, GpuDimShuffle{x,0}.0)\nToposort index: 178\nInputs types: [TensorType(int64, scalar), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, 3D), CudaNdarrayType(float32, matrix), TensorType(int64, scalar), TensorType(int64, scalar), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, row), CudaNdarrayType(float32, row)]\nInputs shapes: [(), (6, 32, 20), (7, 32, 10), (7, 32, 4), (7, 32, 10), (7, 32, 4), (7, 92160), (), (), (20, 10), (10, 10), (10, 4), (1, 4), (1, 10)]\nInputs strides: [(), (20, 120, 1), (320, 10, 1), (128, 4, 1), (320, 10, 1), (128, 4, 1), (92160, 1), (), (), (10, 1), (10, 1), (4, 1), (0, 1), (0, 1)]\nInputs values: [array(6), \'not shown\', \'not shown\', \'not shown\', \'not shown\', \'not shown\', \'not shown\', array(6), array(6), \'not shown\', \'not shown\', \'not shown\', CudaNdarray([[ 0.  0.  0.  0.]]), \'not shown\']\nOutputs clients: [[GpuSubtensor{int64:int64:int64}(forall_inplace,gpu,scan_fn&scan_fn}.0, ScalarFromTensor.0, ScalarFromTensor.0, Constant{-1}), GpuSubtensor{int64:int64:int64}(forall_inplace,gpu,scan_fn&scan_fn}.0, ScalarFromTensor.0, ScalarFromTensor.0, Constant{-1})], [GpuSubtensor{int64:int64:int64}(forall_inplace,gpu,scan_fn&scan_fn}.1, ScalarFromTensor.0, ScalarFromTensor.0, Constant{-1})], [GpuSubtensor{int64:int64:int64}(forall_inplace,gpu,scan_fn&scan_fn}.2, ScalarFromTensor.0, ScalarFromTensor.0, Constant{-1}), GpuSubtensor{int64:int64:int64}(forall_inplace,gpu,scan_fn&scan_fn}.2, ScalarFromTensor.0, ScalarFromTensor.0, Constant{-1})], [GpuSubtensor{int64:int64:int64}(forall_inplace,gpu,scan_fn&scan_fn}.3, ScalarFromTensor.0, ScalarFromTensor.0, Constant{-1})], [GpuSubtensor{int64:int64:int64}(forall_inplace,gpu,scan_fn&scan_fn}.4, ScalarFromTensor.0, ScalarFromTensor.0, Constant{-1}), GpuSubtensor{int64}(forall_inplace,gpu,scan_fn&scan_fn}.4, ScalarFromTensor.0)], [GpuDimShuffle{1,0,2}(forall_inplace,gpu,scan_fn&scan_fn}.5)], [GpuDimShuffle{1,0,2}(forall_inplace,gpu,scan_fn&scan_fn}.6)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag \'optimizer=fast_compile\'. If that does not work, Theano optimizations can be disabled with \'optimizer=None\'.\nHINT: Use the Theano flag \'exception_verbosity=high\' for a debugprint and storage map footprint of this apply node.\n```\n\nCan you tell me what the problem could be?\nI suppose when I am using recurrentContainers, I don\'t need to use `TimeDistributed()`. Is that right?\n\nBesides, I still don\'t know how to extract the readout output from the `step` function in `recurrentshop`. I am really confused right now and I\'d appreciate if anyone could give me some clues.\n', 'Does it mean that the number of classes of the output and input_dim must be the same when we want to have `readout=True`?\n', ""Yes.\n\n``` python\n\nrc = RecurrentContainer(readout=True, return_sequences=True)\nrc.add(SimpleRNNCell(ncls, input_dim=dim))\nrc.add(Dropout(0.5))\nrc.add((Dense(ncls)))\nrc.add(Activation('softmax'))\n\n\nmodel = Sequential()\nmodel.add(TimeDistributed(Dense(ncls), input_shape=(nb_timesteps, input_dim))\nmodel.add(rc)\n```\n"", 'Also, if you need separate access to readout, you may write your own recurrent container (simply copy the existing one and change the readout part).\n', 'In my case, the number of output classes is different from the input_dim.\nSo I will try to modify the recurrent container for that purpose. Thanks!\n', '> In my case, the number of output classes is different from the input_dim.\n\nI know.. follow the method I posted above. Use a timedistributed dense layer to change your input dim to ncls.\n', 'Thanks. But do I necessarily have to do this dimension reduction (input_dim=4096D --> ncls=18 in my case)? \nI am trying to figure out where in `recurrentContainer`, the softmax on output is computed, so that I could directly pass it to the `step` function. I suppose once `readout` is captured here (line 250 engine.py):\n\n```\nif self.readout and i == 0:\n    readout = states[-1]\n```\n\nI can then skip `x += readout` and return `readout` along with `x, states` to the `step` function in `SimpleRNNCell`. Is that correct.\n', ""@farizrahman4u\nMy classification task is binary so by using your example code with TimeDistributed(Dense(ncls)) I would have to convert the embedding for each of my words down to 2 dimensions before feeding into the RNN, which would make it impossible to learn anything.\n\nWould it be possible to just use recurrentshop for the softmax layer with readout=True? For instance, would something along the lines of this work?\n\n```\nrc = RecurrentContainer(readout=True, return_sequences=True)\nrc.add(Dense(ncls))\nrc.add(Activation('softmax'))\n\nmodel = Sequential()\nmodel.add(Embedding(...))\nmodel.add(LSTM(1024, return_sequences=True))\nmodel.add(LSTM(ncls, return_sequences=True))\nmodel.add(rc)\n```\n"", '@monaj07 Did you end up finding a solution?\n', '@PiranjaF Not really, I gave up using recurrentshop. \nI am going to compute the softmax output of the previous time-step out(t-1) in the current time-step, by adding a single line to the step function, as follows:\n\n```\ndef step(self, x, states):\n    prev_output = states[0]\n    B_U = states[1]\n    B_W = states[2]\n\n    if self.consume_less == \'cpu\':\n        h = x\n    else:\n        h = K.dot(x * B_W, self.W) + self.b\n    output = self.activation(h + K.dot(prev_output * B_U, self.U))\n\n    prev_softmaxOut = self.activation_dense(K.dot(prev_output, self.W_dense) + self.b_dense)\n    return output, [output]\n```\n\nI need this output to use it as an extra factor for updating `h(t)`. However, when I introduce the weights `self.W_dense` and `self.b_dense` and add them to the trainable_weights by\n\n`self.trainable_weights += [self.W_dense, self.b_dense]`\n\nI get the following error:\n\n```\ntheano.gradient.DisconnectedInputError:  \nBacktrace when that variable is created:\n\n  File ""/home/monaj/Dropbox/codes/RNN/simplifiedReplicateRNN.py"", line 87, in <module>\n    RNN_out = rnnFunction(RNN_in)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 487, in __call__\n    self.build(input_shapes[0])\n  File ""/home/monaj/Dropbox/codes/RNN/ModifiedClasses.py"", line 60, in build\n    self.W_dense = self.init((self.output_dim, self.num_classes), name=\'{}_W_dense\'.format(self.name))\n  File ""/usr/local/lib/python2.7/dist-packages/keras/initializations.py"", line 59, in glorot_uniform\n    return uniform(shape, s, name=name)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/initializations.py"", line 32, in uniform\n    return K.random_uniform_variable(shape, -scale, scale, name=name)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/theano_backend.py"", line 141, in random_uniform_variable\n    dtype=dtype, name=name)\n  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/theano_backend.py"", line 66, in variable\n    return theano.shared(value=value, name=name, strict=False)\n```\n\nDo you think it makes sense at all to compute the softmax outputs this way? I could not come up with a straightforward approach to obtain these softmax outputs within the RNN loop.\n', ""``` python\ndef step(self, x, states):\n    prev_output = states[0]\n    prev_softmax_output = states[1]\n    B_U = states[1]\n    B_W = states[2]\n\n    if self.consume_less == 'cpu':\n        h = x\n    else:\n        h = K.dot(x * B_W, self.W) + self.b\n    output = self.activation(h + K.dot(prev_output * B_U, self.U))\n\n    softmax_output = self.activation_dense(K.dot(prev_output, self.W_dense) + self.b_dense)\n    return output, [output, softmax_output]\n```\n\nNote that this wouldn't work as expected for a stack.\n"", 'Also note that recurrentshop now supports readout input to arbitrary cells, and you can decide what is done with the readout, not just add/mul.\n1. Create your own cell class, inheriting from `RNNCell`. See `SimpleRNNCell` code for example.\n2. Define the step function for the cell as follows:\n\n``` python\ndef step(x, states, weights):\n    # x is a list; x = [input, readout]\n    readout = x[1]\n    x = x[0]\n    # Rest of your step function.......\n    return y, new_states\n```\n1. Create an object of the cell class. `cell=MyCellClass(output_dim=10)`\n2. Set attribute `receive_readout` to True for the object `cell.receive_readout=True`\n3. Create `RecurrentContainer` object with `readout=True`;Add your custom cells and other cells.\n', '@farizrahman4u  Thanks for your response. As you said it does not work on a batch and I still get the same error when I add `W_dense` and `b_dense` to the trainable_weights.\n', 'Its because you are not using the weights anywhere.\n', '@farizrahman4u Thanks. \nI implemented it using Theano, though it needed more codes.\n', ""Hey @monaj07 would you mind posting your solution? I'm also interested in your exact problem!"", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs, but feel free to re-open it if needed.\n']",[],"['h(t) = tanh(W.x(t) + U.h(t-1) + V.O(t-1) + b)', 'O(t-1)', 't-1']",0,0
235,keras,6168,closed,Simple stateful LSTM example,"Please consider this simple example
    
    nb_samples = 100000
    X = np.random.randn(nb_samples)
    Y = X[1:]
    X = X[:-1]
    X = X.reshape((len(Y), 1, 1))
    Y = Y.reshape((len(Y), 1))

So we have basically 

    Y[i] = X[i-1]


and the model is simply a lag operator. 

Now I try to learn this model with a stateful LSTM, by giving the pairs of values  one by one ()

    model = Sequential()
    model.add(LSTM(batch_input_shape=(1, 1, 1),
                   output_dim =10,
                   activation='tanh', stateful=True
                  )
            )
    model.add(Dense(output_dim=1, activation='linear'))
    model.compile(loss='mse', optimizer='adam')
    
    
    for epoch in range(10000):
        model.reset_states()
        train_loss = 0
        for i in range(Y_train.shape[0]):
            train_loss += model.train_on_batch(X_train[i:i+1],
                             Y_train[i:i+1],
                             )
        print '# epoch', epoch, '  loss ', train_loss/float(Y_train.shape[0])


but I am seeing a mean loss around 1, which is the standard deviation of my randomly generated data, so the model does not seem to learn.

Am I having something wrong?",,"[""All LSTMs are stateful. Keras `stateful` only means stateful between batches.\r\n\r\nThe problem is that gradient can't backprop between batches. So you are using state t at time t+1, but those are two separate batches and gradient can't flow back to the hidden representation at t.\r\n\r\nAn LSTM can only learn dependencies effectively within a batch. Stateful can hypothetically learn something between batches but don't depend on it. Batch t+1 will do the best it can do based on hidden t, but hidden t will not try to make a good representation.\r\n\r\nIf you modify your code to make batches of 2 instead of 1, it should start working.\r\n\r\nCheers"", 'Thanks Ben for this answer. Please bear with me on this example and correct my misunderstandings.\r\n\r\n1- I tried with a `batch_size = 2`, and even with a `batch_size = 20` and got the same result. Basically the model is not learning anything.\r\n\r\n2- My understanding is that whatever batch_size I put, I will not see that y(t) depends on x(t-1). Indeed, as in every batch, I am giving the pairs (x(t), y(t)), back propagation through time will only compute the derivative of the loss with respect to x(t). The batch size is there only to estimate that derivative on a large number of samples, then take the mean. So whatever batch_size I put, I will not see the dependence on x(t-1). The only way I can see it is to put nb_steps (the length of the window of x sequence) to a number greater than 1).\r\n\r\nSo the LSTM does not learn dependcies within a batch : every row of the batch is treated independently, and the longer the batch, the better is the derivative estimates. Dependencies seem learned within the window of xs given to the model (nb_steps in keras nomenclature, i.e. the second parameter in batch_input_shape = (..., ..., ...).\r\n\r\nWhat I hoped with my experiment was that the network will learn to simply put the input x[t] in its hidden state, then at t+1, return its hidden state as output y[t+1], and replaces the hidden state by x[t+1], and do like that recursively. Obviously, I can achieve this by using a stateless LSTM with nb_steps = 2 or larger, but wanted to have the result with a stateful one.\r\n\r\nIf I am right, it seems the LSTM or other recurrent neural network is able to learn a history as long as the input sequence (nb_steps in keras nomenclature). So if I am dealing with time series where y(t) depends on x(t-101) and it happens that the moving window of xs that I use as input to the model is of length 100 only, then the model will not learn anything.\r\n\r\nPlease correct me', ""Yup. I meant batches with depth 2 instead of 1. Just realized how vague I was.\r\n\r\nBackprop of an lstm is only within the batch and goes back the depth of the batch. If your depth is more than 1 it should work.\r\n\r\nWith depth one, the top level can be trained, but it won't backprop to the previous layers."", 'Coming back to the example of volvador:\r\n\r\n> So if I am dealing with time series where y(t) depends on x(t-101) and the batch-depth (seq. length) is 100\r\n\r\nIn this case the function will not be able to learn our example, right? \r\n\r\nSo `stateful=True` will just give me a light version of recurrence?', 'If your sequence length is 100 and the only dependency is 101 away, then then function will not learn that dependency.\r\n\r\nIf there is also another dependency 100 away, the model will learn to encode that dependency into the hidden state. If you run a stateful LSTM, the model will try to predict 101 from the hidden state at 100, which might still have some useful information.\r\n\r\nThe thing to keep in mind is that information travels through backprop, which is only working within a single batch.\r\n\r\nAlso, keep in mind these are largely hypothetical and an LSTM is probably not going to learn much with a depth of 100. It gets harder and harder to learn longer dependencies. LSTM will learn longer dependencies than simple RNNs but not infinite. You could read into highway networks or multi-timescale learning if 100 was not just a figure of speech.\r\n\r\nCheers', ""Thanks a lot, that makes it more understandable.\r\n\r\nMy dependencies are actually not that long (though not actually known), I just have the problem that my dimensionality is large and using a sliding window approach would significantly increase training time. I thought with a stateful RNN I could circumvent giving overlapping slices after slices to the network. Turns out I can't."", ""No free lunch. You should have sequences as long as whatever dependency you're trying to learn and at least a handful of sequences for batch. Good luck!\r\n\r\nCheers"", '@bstriner : I am confused by the use of term batch. Do you mean ""one sample in a batch"" when you say ""batch"" here.\r\n ""Backprop of an lstm is only within the batch and goes back the depth of the batch. If your depth is more than 1 it should work"". \r\n\r\nAs per my understanding BPTT is only done for an individual sample within a batch, rather than over the entire batch.  Each sample is formed by concatenating _l_ consecutive time steps together. Thus _l_ decides the length of BPTT.  And there is no way to backprop over different samples or preserve state from one sample to another in a batch. Am I correct?\r\n\r\nThanks ', ""Don't get too hung up on the language. In audio processing there are many samples per sequence. In text processing a sample is a sequence.\r\n\r\nI like to think of a batch as being made of samples. Each sample is independent and the independent gradients for samples are combined. If you backprop between samples, you kind of only have one sample.\r\n\r\nThen again, if you use something like batchnorm, there is a gradient between sample 0 at time 0 and sample `n` at time `t`. So maybe we should think of those samples as part of a meta-sample, because the gradients aren't independent, as they are in traditional batches.\r\n\r\n> And there is no way to backprop over different samples or preserve state from one sample to another in a batch. Am I correct?\r\n\r\nIf you could preserve state between samples in a batch then wouldn't you just have one sample?\r\n\r\nJust remember that a standard LSTM input should have 3 dimensions (batch, sequence, feature) and will iterate over the middle dimension in each batch.\r\n\r\nGood luck!"", 'Yes, I read too much and got all tied up. But this clears it up. Thanks!', 'Hi all, just to be completely clear, @bstriner when you say:\r\n""Just remember that a standard LSTM input should have 3 dimensions (batch, sequence, feature) and will iterate over the middle dimension in each batch.""\r\n\r\nis it also true when using the option **stateful = True** ?\r\n\r\nif it is the case I am not sure to understand, we propagate the state across batches AND doing backprop on elements within a batch that are supposed to be independent ?\r\n\r\nto quote fchollet in https://github.com/fchollet/keras/issues/98\r\n""\r\n\' or should it consider that the samples in a batch are independent, but that the next batch will provide the samples that come chronologically next (i.e. batch_2[i] is the successor to batch_1[i] for all i)?\'\r\n\r\nLet\'s go with this behavior, and let\'s implement it as an option in existing recurrent layers (statefulkeyword argument in constructor,\xa0False\xa0by default).I believe this can be easily achieved by simply storing the last output and last memory at the end of a batch (e.g. as class attributes of the layer?), then passing these as\xa0outputs_info\xa0in the scan loop of the next batch. Remarks, concerns? ""\r\n\r\n\r\nThank you for your time, I am a bit lost.\r\n\r\n', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', 'I want to use LSTM for building a one step ahead predictor; there are lot of examples online that use a predefined ""time window"" to train the stateless LSTM neural network, I will make an example to make it more clear:\r\n\r\ntime window size = 5\r\nDataset = [1,2,3,4,5,6,7,8,9]\r\nX_train = [[1,2,3,4,5],[2,3,4,5,6],[3,4,5,6,7],[4,5,6,7,8]]\r\nY_train = [     [6]       ,       [7]     ,      [8]      ,      [9]      ]\r\n\r\nNow with this data the network is trained and here comes into play the batch_size.\r\nLet\'s suppose batch_size=2.\r\nSo, before the weights in the neural network are updated 2 samples are showed to the NN, e.g.:\r\n\r\nX = [[1,2,3,4,5], [2,3,4,5,6]]\r\nY = [       [6]     ,        [7]     ]\r\n\r\nFrom what I have understood, in a stateless LSTM, what is happening is the following. Samples in the same batch (hence X[0] and X[1] in the example above) are processed in parallel. The initial state of the LSTM memory cell is initialized randomly for all the samples (X[0] and X[1]) in the batch, then for a single sample (e.g X[0]) the network is trained using BPTT. Hence,for example the LSTM memory cell value for the 2nd value of X[0] (which is 2), will use the value of the memory LSTM cell state computed with the 1st value of X[0] (which is 1) and so on until all the time_window_size number of values in X[0] are processed and the final output is computed. The latter will be compared against Y[0] etc.. Same is happening for X[1]. At the end of the batch, using the value computed with the 2 samples the weights are updated.\r\nAm I understanding this correctly? Is it correct to train an LSTM for time-series forecasting in this way?\r\nIf yes, then why in various papers is stated that: "" Because of this ability to learn long term correlations in a sequence, LSTM networks obviate the need for a pre-specified time window and are capable of accurately modelling complex multivariate sequences"" ?\r\nAny help appreciated.', 'I have a simple question. If I trained a stateful model and use a loop to perform predictions. Such as:\r\n`for data in data_set:\r\n    prediction = model.predict(X, batch_size)`\r\n\r\nWill the model be stateful between the iterations?\r\nI mean, when predict() is called, does the model maintain states from the end of previous prediction?', '@ylmeng I had the same question.  See [this](https://stackoverflow.com/questions/43336630/when-predicting-with-an-lstm-in-keras-is-the-hidden-state-still-adjusted).', '@shrikanth95 The answer is not clear and I am not clear about its meaning. Does it means the states are updated?', 'For a good example of stateful LSTMs (not keras though), see this relatively recent paper. Describes a process of using stateful LSTMs to go over the entire wikitext corpus while maintaining hidden state. Several key points about how you have to generate batches, reset state, vary sequence length, etc.\r\n\r\nTL;DR: Backprop does not work between batches. However, if you vary the batch boundaries, and randomly reset the hidden state, you can get reasonable generalization for longer sequences.\r\n\r\nhttps://arxiv.org/pdf/1708.02182.pdf\r\nhttps://github.com/salesforce/awd-lstm-lm\r\n', ""@mik1904 I've had the same question on my mind for hours now! Did you have any luck?\r\n@bstriner maybe you could advise? I feel like this could almost be a FAQ\r\n\r\n**Example:**\r\n`Dataset = [1,2,3,4,5,6,7,8,9]`\r\n`X_train = [[1,2,3,4,5],[2,3,4,5,6],[3,4,5,6,7],[4,5,6,7,8]]`\r\n`Y_train = [[6],[7],[8],[9]]`\r\n\r\nGiven time-series dataset (eg. forex), learn patterns in the data to enable future prediction that:\r\n`model.predict([20,21,22,23,24])` -> `[25]`\r\n\r\n**Objectives:**\r\n- Assess (theoretically) how shifting windows approach can be improved with batch size.\r\n- Assess how windows can be implemented in a stateful model.\r\n\r\n_I'm hoping this will help myself and others deciding between stateful vs stateless LSTMs and save time implementing window forecast models. You can tune hyperparams with an algorithm to assess the prediction performance but it would be beneficial to understand the theory first!_\r\n\r\n**Stateless LSTM with windows:**\r\n\r\nBatch size of 1 would mean a single window is fed in, and size 2 would mean two windows are passed in training before LSTM weights are updated.\r\n\r\nI was of the understanding [eg. from this article](https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/) (by @jbrownlee) that a batch size containing all the windows would learn the sequence best:\r\n> This suggests that if we had a batch size large enough to hold all input patterns and if all the input patterns were ordered sequentially, that the LSTM could use the context of the sequence within the batch to better learn the sequence.\r\n\r\n1. To take advantage of a larger batch size learning a longer sequence, should the training be set to `shuffle = False`? \r\nShuffling will randomise the windows inside each batch, therefore losing any long term sequence found by retaining state after sequential windows. \r\neg. Shuffle True: `[[4,5,6,7,8], [2,3,4,5,6]]` False: ` [[1,2,3,4,5],[2,3,4,5,6]]` (better because the LSTM continues from the first window's state)\r\n\r\n2. Or is the whole point of window training that the network only finds patterns in each independent window rather than an entire dataset?\r\nie. if you wanted to find a pattern in a longer sequence, should you instead increase the size of each window. If this is the case, how does a larger batch size help?\r\n\r\n**Stateful LSTM:**\r\n\r\nCould the problem be solved better by a stateful LSTM? The state is maintained after each batch and reset after each epoch, meaning the data must be fed sequentially. This would let a model learn the whole dataset.\r\n\r\nFor this to succeed, the number of windows must be a multiple of the batch size for training and prediction.\r\nEg. For a batch size of 2:\r\nTrain: `[[window 1][window 2], [window3][window4], ... ]`\r\nPredict: `[window10][window11] -> [x, y]`\r\n_where x is the prediction of value after window 10 (start of window 11), and y is the prediction we care about (into the future)_\r\n\r\n3. For a stateful LSTM, should the training data not be supplied in windows of 1 step?\r\nAs the input is sequential, would the model be expecting [1-5][6-10][11-15]... instead of the current overlapping windows.\r\n\r\n\r\nMany thanks in advance! 😄 "", ""@bstriner I didn't find anything related to stateful lstm in  https://arxiv.org/pdf/1708.02182.pdf paper and why it should be used in language model. Can you describe some points related to it in short or provide relevent link?""]",[],"['(x, y)', 'batch_size = 1']",0,0
236,keras,3421,closed,ImportError: cannot import name 'Deconvolution2D' (install from source),"Im getting this weird error, while other keras modules load without a problem.
Im just running: variational_autoencoder_deconv.py
Ideas?


",,"['Having same issue. Installed convolutional.py has no Deconvolution2D declaration. @varoudis how did you fix this?\n', 'Just fuxed it with:\n- sudo rm -rf Keras.egg-info/ build/\n- sudo pip3 uninstall keras\n- update Theano\n\nbefore build and install keras again.\n']","['\nroot@b667c5f44d6a:~/keras# ls\nCONTRIBUTING.md    Keras.egg-info  README.md  dist    docs      keras       setup.cfg  tests\nISSUE_TEMPLATE.md  LICENSE         build      docker  examples  pytest.ini  setup.py\nroot@b667c5f44d6a:~/keras# python3 setup.py install\nrunning install\nrunning bdist_egg\nrunning egg_info\nwriting Keras.egg-info/PKG-INFO\nwriting requirements to Keras.egg-info/requires.txt\nwriting dependency_links to Keras.egg-info/dependency_links.txt\nwriting top-level names to Keras.egg-info/top_level.txt\nreading manifest file \'Keras.egg-info/SOURCES.txt\'\nwriting manifest file \'Keras.egg-info/SOURCES.txt\'\ninstalling library code to build/bdist.linux-x86_64/egg\nrunning install_lib\nrunning build_py\ncreating build/bdist.linux-x86_64/egg\ncreating build/bdist.linux-x86_64/egg/keras\ncopying build/lib/keras/optimizers.py -> build/bdist.linux-x86_64/egg/keras\ncopying build/lib/keras/models.py -> build/bdist.linux-x86_64/egg/keras\ncreating build/bdist.linux-x86_64/egg/keras/datasets\ncopying build/lib/keras/datasets/cifar100.py -> build/bdist.linux-x86_64/egg/keras/datasets\ncopying build/lib/keras/datasets/__init__.py -> build/bdist.linux-x86_64/egg/keras/datasets\ncopying build/lib/keras/datasets/mnist.py -> build/bdist.linux-x86_64/egg/keras/datasets\ncopying build/lib/keras/datasets/data_utils.py -> build/bdist.linux-x86_64/egg/keras/datasets\ncopying build/lib/keras/datasets/cifar10.py -> build/bdist.linux-x86_64/egg/keras/datasets\ncopying build/lib/keras/datasets/cifar.py -> build/bdist.linux-x86_64/egg/keras/datasets\ncopying build/lib/keras/datasets/imdb.py -> build/bdist.linux-x86_64/egg/keras/datasets\ncopying build/lib/keras/datasets/reuters.py -> build/bdist.linux-x86_64/egg/keras/datasets\ncreating build/bdist.linux-x86_64/egg/keras/legacy\ncopying build/lib/keras/legacy/models.py -> build/bdist.linux-x86_64/egg/keras/legacy\ncopying build/lib/keras/legacy/__init__.py -> build/bdist.linux-x86_64/egg/keras/legacy\ncopying build/lib/keras/__init__.py -> build/bdist.linux-x86_64/egg/keras\ncreating build/bdist.linux-x86_64/egg/keras/layers\ncopying build/lib/keras/layers/__init__.py -> build/bdist.linux-x86_64/egg/keras/layers\ncopying build/lib/keras/layers/noise.py -> build/bdist.linux-x86_64/egg/keras/layers\ncopying build/lib/keras/layers/pooling.py -> build/bdist.linux-x86_64/egg/keras/layers\ncopying build/lib/keras/layers/embeddings.py -> build/bdist.linux-x86_64/egg/keras/layers\ncopying build/lib/keras/layers/advanced_activations.py -> build/bdist.linux-x86_64/egg/keras/layers\ncopying build/lib/keras/layers/local.py -> build/bdist.linux-x86_64/egg/keras/layers\ncopying build/lib/keras/layers/normalization.py -> build/bdist.linux-x86_64/egg/keras/layers\ncopying build/lib/keras/layers/wrappers.py -> build/bdist.linux-x86_64/egg/keras/layers\ncopying build/lib/keras/layers/recurrent.py -> build/bdist.linux-x86_64/egg/keras/layers\ncopying build/lib/keras/layers/convolutional.py -> build/bdist.linux-x86_64/egg/keras/layers\ncopying build/lib/keras/layers/core.py -> build/bdist.linux-x86_64/egg/keras/layers\ncreating build/bdist.linux-x86_64/egg/keras/engine\ncopying build/lib/keras/engine/topology.py -> build/bdist.linux-x86_64/egg/keras/engine\ncopying build/lib/keras/engine/__init__.py -> build/bdist.linux-x86_64/egg/keras/engine\ncopying build/lib/keras/engine/training.py -> build/bdist.linux-x86_64/egg/keras/engine\ncreating build/bdist.linux-x86_64/egg/keras/preprocessing\ncopying build/lib/keras/preprocessing/__init__.py -> build/bdist.linux-x86_64/egg/keras/preprocessing\ncopying build/lib/keras/preprocessing/text.py -> build/bdist.linux-x86_64/egg/keras/preprocessing\ncopying build/lib/keras/preprocessing/sequence.py -> build/bdist.linux-x86_64/egg/keras/preprocessing\ncopying build/lib/keras/preprocessing/image.py -> build/bdist.linux-x86_64/egg/keras/preprocessing\ncopying build/lib/keras/metrics.py -> build/bdist.linux-x86_64/egg/keras\ncreating build/bdist.linux-x86_64/egg/keras/wrappers\ncopying build/lib/keras/wrappers/__init__.py -> build/bdist.linux-x86_64/egg/keras/wrappers\ncopying build/lib/keras/wrappers/scikit_learn.py -> build/bdist.linux-x86_64/egg/keras/wrappers\ncreating build/bdist.linux-x86_64/egg/keras/utils\ncopying build/lib/keras/utils/test_utils.py -> build/bdist.linux-x86_64/egg/keras/utils                                                        [83/463]\ncopying build/lib/keras/utils/visualize_util.py -> build/bdist.linux-x86_64/egg/keras/utils\ncopying build/lib/keras/utils/__init__.py -> build/bdist.linux-x86_64/egg/keras/utils\ncopying build/lib/keras/utils/layer_utils.py -> build/bdist.linux-x86_64/egg/keras/utils\ncopying build/lib/keras/utils/io_utils.py -> build/bdist.linux-x86_64/egg/keras/utils\ncopying build/lib/keras/utils/data_utils.py -> build/bdist.linux-x86_64/egg/keras/utils\ncopying build/lib/keras/utils/generic_utils.py -> build/bdist.linux-x86_64/egg/keras/utils\ncopying build/lib/keras/utils/np_utils.py -> build/bdist.linux-x86_64/egg/keras/utils\ncopying build/lib/keras/activations.py -> build/bdist.linux-x86_64/egg/keras\ncopying build/lib/keras/objectives.py -> build/bdist.linux-x86_64/egg/keras\ncreating build/bdist.linux-x86_64/egg/keras/backend\ncopying build/lib/keras/backend/tensorflow_backend.py -> build/bdist.linux-x86_64/egg/keras/backend\ncopying build/lib/keras/backend/__init__.py -> build/bdist.linux-x86_64/egg/keras/backend\ncopying build/lib/keras/backend/common.py -> build/bdist.linux-x86_64/egg/keras/backend\ncopying build/lib/keras/backend/theano_backend.py -> build/bdist.linux-x86_64/egg/keras/backend\ncopying build/lib/keras/regularizers.py -> build/bdist.linux-x86_64/egg/keras\ncopying build/lib/keras/callbacks.py -> build/bdist.linux-x86_64/egg/keras\ncopying build/lib/keras/initializations.py -> build/bdist.linux-x86_64/egg/keras\ncopying build/lib/keras/constraints.py -> build/bdist.linux-x86_64/egg/keras\nbyte-compiling build/bdist.linux-x86_64/egg/keras/optimizers.py to optimizers.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/models.py to models.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/datasets/cifar100.py to cifar100.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/datasets/__init__.py to __init__.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/datasets/mnist.py to mnist.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/datasets/data_utils.py to data_utils.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/datasets/cifar10.py to cifar10.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/datasets/cifar.py to cifar.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/datasets/imdb.py to imdb.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/datasets/reuters.py to reuters.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/legacy/models.py to models.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/legacy/__init__.py to __init__.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/__init__.py to __init__.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/layers/__init__.py to __init__.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/layers/noise.py to noise.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/layers/pooling.py to pooling.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/layers/embeddings.py to embeddings.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/layers/advanced_activations.py to advanced_activations.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/layers/local.py to local.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/layers/normalization.py to normalization.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/layers/wrappers.py to wrappers.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/layers/recurrent.py to recurrent.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/layers/convolutional.py to convolutional.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/layers/core.py to core.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/engine/topology.py to topology.cpython-34.pyc                                                [40/463]\nbyte-compiling build/bdist.linux-x86_64/egg/keras/engine/__init__.py to __init__.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/engine/training.py to training.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/preprocessing/__init__.py to __init__.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/preprocessing/text.py to text.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/preprocessing/sequence.py to sequence.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/preprocessing/image.py to image.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/metrics.py to metrics.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/wrappers/__init__.py to __init__.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/wrappers/scikit_learn.py to scikit_learn.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/utils/test_utils.py to test_utils.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/utils/visualize_util.py to visualize_util.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/utils/__init__.py to __init__.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/utils/layer_utils.py to layer_utils.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/utils/io_utils.py to io_utils.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/utils/data_utils.py to data_utils.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/utils/generic_utils.py to generic_utils.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/utils/np_utils.py to np_utils.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/activations.py to activations.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/objectives.py to objectives.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/backend/tensorflow_backend.py to tensorflow_backend.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/backend/__init__.py to __init__.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/backend/common.py to common.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/backend/theano_backend.py to theano_backend.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/regularizers.py to regularizers.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/callbacks.py to callbacks.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/initializations.py to initializations.cpython-34.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/keras/constraints.py to constraints.cpython-34.pyc\ncreating build/bdist.linux-x86_64/egg/EGG-INFO\ncopying Keras.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\ncopying Keras.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\ncopying Keras.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\ncopying Keras.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\ncopying Keras.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\nzip_safe flag not set; analyzing archive contents...\nkeras.backend.__pycache__.theano_backend.cpython-34: module MAY be using inspect.stack\ncreating \'dist/Keras-1.0.6-py3.4.egg\' and adding \'build/bdist.linux-x86_64/egg\' to it\nremoving \'build/bdist.linux-x86_64/egg\' (and everything under it)\nProcessing Keras-1.0.6-py3.4.egg\nremoving \'/usr/local/lib/python3.4/dist-packages/Keras-1.0.6-py3.4.egg\' (and everything under it)\ncreating /usr/local/lib/python3.4/dist-packages/Keras-1.0.6-py3.4.egg\nExtracting Keras-1.0.6-py3.4.egg to /usr/local/lib/python3.4/dist-packages\nKeras 1.0.6 is already the active version in easy-install.pth\n\nInstalled /usr/local/lib/python3.4/dist-packages/Keras-1.0.6-py3.4.egg\nProcessing dependencies for Keras==1.0.6\nSearching for six==1.10.0\nBest match: six 1.10.0\nAdding six 1.10.0 to easy-install.pth file\n\nUsing /usr/local/lib/python3.4/dist-packages\nSearching for PyYAML==3.11\nBest match: PyYAML 3.11\nAdding PyYAML 3.11 to easy-install.pth file\n\nUsing /usr/local/lib/python3.4/dist-packages\nSearching for Theano==0.9.0.dev2\nBest match: Theano 0.9.0.dev2\nProcessing Theano-0.9.0.dev2-py3.4.egg\nTheano 0.9.0.dev2 is already the active version in easy-install.pth\nInstalling theano-test script to /usr/local/bin\nInstalling theano-cache script to /usr/local/bin\nInstalling theano-nose script to /usr/local/bin\n\nUsing /usr/local/lib/python3.4/dist-packages/Theano-0.9.0.dev2-py3.4.egg\nSearching for scipy==0.18.0\nBest match: scipy 0.18.0\nAdding scipy 0.18.0 to easy-install.pth file\n\nUsing /usr/local/lib/python3.4/dist-packages\nSearching for numpy==1.11.1\nBest match: bumpy 1.11.1\nAdding numpy 1.11.1 to easy-install.pth file\n\nUsing /usr/local/lib/python3.4/dist-packages\nFinished processing dependencies for Keras==1.0.6\n\n\n\nroot@b667c5f44d6a:~/keras# python3 /root/keras/examples/variational_autoencoder_deconv.py\nUsing Theano backend.\nUsing gpu device 0: Tesla K80 (CNMeM is enabled with initial size: 50.0% of memory, cuDNN 5005)\nTraceback (most recent call last):\n  File ""/root/keras/examples/variational_autoencoder_deconv.py"", line 9, in <module>\n    from keras.layers import Convolution2D, Deconvolution2D, MaxPooling2D\nImportError: cannot import name \'Deconvolution2D\'\nroot@b667c5f44d6a:~/keras#\n']",[],0,0
237,keras,11379,closed,ZeroPadding Dynamic for step> 1 and access the actual shape of a tensor of dimension None in Keras,"
I'm trying to implement a dynamic zero padding to keep the second dimension of a constant tensor after going through convolutional layers that have stride> 1, the input tensor has the following shape (batch_size, time_step, 50), I need the time step dimension not be changed by the convolutional layers. I tried to use 'same' padding, however when stride> 1, this does not work, so I created a custom layer for ZeroPadding, it is working for tensors with the shape (None, 100,50), (None, 120,50), (None, 60,50), but does not work for dynamic shapes of type (None, None, 50), I get the following error:



I've added my custom class to an imdb example, to make it easier to reproduce the errors. Change model.add(Embedding(max_features, embedding_dims, input_length = None)) to model.add(Embedding(max_features, embedding_dims, input_length = 400)) and dynamic padding will work, however it needs to work for dimension of type None. The code:

(left_pad, right_pad)(batch, axis_to_pad, features)(batch, padded_axis, features)

I searched and saw that I have to use K.shape (inputs), to get the correct shape at runtime, instead of None, but I could not make it work with Keras, could anyone help me?

If you have another solution to solve the problem of zero dynamic padding, it is very welcome.
The question is also in StackOverflow: [ZeroPadding Dynamic for step> 1 and access the actual shape of a tensor of dimension None in Keras](https://stackoverflow.com/questions/52788133/zeropadding-dynamic-for-step-1-and-access-the-actual-shape-of-a-tensor-of-dimen) 

Thank you in advance for your attention.


Thank you!

- [ V ] Check that you are up-to-date with the master branch of Keras. You can update with:


- [ V ] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ V ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",type:support,[],"['\r\nTraceback (most recent call last):\r\n  File ""keras-dinamic-padding-for-stride.py"", line 120, in <module>\r\n    model.add (ZeroPadding1D (dinamic_padding_stride = 2))\r\n  File ""/home/edresson/anaconda3/lib/python3.6/site-packages/keras/engine/sequential.py"", line 181, in add\r\n    output_tensor = layer (self.outputs [0])\r\n  File ""/home/edresson/anaconda3/lib/python3.6/site-packages/keras/engine/base_layer.py"", line 457, in __call__\r\n    output = self.call (inputs, ** kwargs)\r\n  File ""keras-dinamic-padding-for-stride.py"", line 67, in call\r\n    padding = int (inputs.shape [1] * self.dinamic_padding_stride)\r\nTypeError: __int__ returned non-int (type NoneType)\r\n']","['', '\r\nfrom __future__ import print_function\r\n\r\nfrom keras.preprocessing import sequence\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout, Activation\r\nfrom keras.layers import Embedding\r\nfrom keras.layers import Conv1D, GlobalMaxPooling1D,Lambda\r\nfrom keras.datasets import imdb\r\n\r\n\r\nfrom keras.engine.topology import Layer,InputSpec\r\nfrom keras.utils import conv_utils\r\nimport keras.backend as K\r\n\r\nclass ZeroPadding1D(Layer):\r\n    """"""Zero-padding layer for 1D input (e.g. temporal sequence).\r\n\r\n    # Arguments\r\n        padding: int, or tuple of int (length 2), or dictionary.\r\n            - If int:\r\n            How many zeros to add at the beginning and end of\r\n            the padding dimension (axis 1).\r\n            - If tuple of int (length 2):\r\n            How many zeros to add at the beginning and at the end of\r\n            the padding dimension (', "").\r\n         dinamic_padding_stride: int\r\n             - if used it totally ignores the padding parameter\r\n             - is used to maintain the size of an input when\r\n               passed through a convolutional layer, adding the\r\n               fill with zeros dynamically. Note: the posterior\r\n               convolutional layer should use padding = 'same'\r\n\r\n    # Input shape\r\n        3D tensor with shape "", '\r\n\r\n    # Output shape\r\n        3D tensor with shape ', '\r\n    """"""\r\n\r\n    def __init__(self, padding=1,dinamic_padding_stride=None, **kwargs):\r\n        super(ZeroPadding1D, self).__init__(**kwargs)\r\n        self.padding = conv_utils.normalize_tuple(padding, 2, \'padding\')\r\n        self.input_spec = InputSpec(ndim=3)\r\n        self.dinamic_padding_stride = dinamic_padding_stride\r\n\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        if input_shape[1] is not None:\r\n            if self.dinamic_padding_stride is not None :\r\n                padding =  input_shape[1] * self.dinamic_padding_stride - input_shape[1] \r\n                self.padding = (int(padding/2),int(padding/2))\r\n            length = input_shape[1] + self.padding[0] + self.padding[1]\r\n        else:\r\n            length = None\r\n        return (input_shape[0],\r\n                length,\r\n                input_shape[2])\r\n\r\n    def call(self, inputs):\r\n        if self.dinamic_padding_stride is not None:\r\n            padding = int(inputs.shape[1] * self.dinamic_padding_stride)\r\n            self.padding = (int(padding/2),int(padding/2)) \r\n        return K.temporal_padding(inputs, padding=self.padding)\r\n\r\n    def get_config(self):\r\n        config = {\'padding\': self.padding}\r\n        base_config = super(ZeroPadding1D, self).get_config()\r\n        return dict(list(base_config.items()) + list(config.items()))\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n# set parameters:\r\nmax_features = 5000\r\nmaxlen = 400\r\nbatch_size = 32\r\nembedding_dims = 50\r\nfilters = 250\r\nkernel_size = 3\r\nhidden_dims = 250\r\nepochs = 2\r\n\r\nprint(\'Loading data...\')\r\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\r\nprint(len(x_train), \'train sequences\')\r\nprint(len(x_test), \'test sequences\')\r\n\r\nprint(\'Pad sequences (samples x time)\')\r\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\r\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\r\nprint(\'x_train shape:\', x_train.shape)\r\nprint(\'x_test shape:\', x_test.shape)\r\n\r\nprint(\'Build model...\')\r\nmodel = Sequential()\r\n\r\n# we start off with an efficient embedding layer which maps\r\n# our vocab indices into embedding_dims dimensions\r\nmodel.add(Embedding(max_features,\r\n                    embedding_dims,\r\n                    input_length=None))\r\nmodel.add(Dropout(0.2))\r\n\r\n# we add a Convolution1D, which will learn filters\r\n# word group filters of size filter_length:\r\nmodel.add(Conv1D(filters,\r\n                 kernel_size,\r\n                 padding=\'same\',\r\n                 activation=\'relu\',\r\n                 strides=1))\r\nmodel.add(ZeroPadding1D(dinamic_padding_stride=2))\r\nmodel.add(Conv1D(filters,\r\n                 kernel_size,\r\n                 padding=\'same\',\r\n                 activation=\'relu\',\r\n                 strides=2))\r\n# we use max pooling:\r\nmodel.add(GlobalMaxPooling1D())\r\n\r\n# We add a vanilla hidden layer:\r\nmodel.add(Dense(hidden_dims))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(Activation(\'relu\'))\r\n\r\n# We project onto a single unit output layer, and squash it with a sigmoid:\r\nmodel.add(Dense(1))\r\nmodel.add(Activation(\'sigmoid\'))\r\n\r\nmodel.compile(loss=\'binary_crossentropy\',\r\n              optimizer=\'adam\',\r\n              metrics=[\'accuracy\']) \r\nmodel.summary()\r\nmodel.fit(x_train, y_train,\r\n          batch_size=batch_size,\r\n          epochs=epochs,\r\nvalidation_data=(x_test, y_test))\r\n', '', 'pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps']",0,0
238,keras,5326,closed,GPU runs out of memory for VGG19 & VGG16 but not for ResNet50,"Below I provide a toy example which throws an out-of-memory exception while training VGG19 or VGG16 models on GPU. The batch-size and the dataset used here are tiny and my graphics card should handle them. If I use a ResNet50 architecture instead, I get no error and I am able to use really big datasets & batch-sizes. Training the models on CPU works fine for all network architectures. 

The problem first appeared on a more complex pipeline that runs on p2.16xlarge AWS instances. I can reproduce the problem using Ubuntu 14.04, Keras 1.2.1 and Tensorflow 0.12.1. Can anyone reproduce the problem? Any thoughts?

For the shake of completeness I also uploaded the dataset [here](https://ufile.io/c3fbf).



Error message using VGG19:


Successful execution with ResNet50:
",,"[""Hello, \r\nThe out of memory error is caused by (which you don't add btw in case of resnet) : \r\n    model.add(Flatten(name='flatten'))\r\n    model.add(Dense(4096, activation='relu', name='fc1'))\r\n\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[25088,4096]\r\n\r\n25088 = 7 * 7 * 512 \r\n\r\nThis is a huge matrix (102760448 parameters) , and I remember having some problem in the past with VGG and memory consumption with smaller GPU."", '@unrealwill Thanks so much for your reply. So effectively what you say is that the model is too big to live in my GPU. Makes sense.\r\n\r\nI noticed that I get the same error in GPUs with more memory IF I have earlier initialised other models (in the same session). Possibly the were not garbage collected? Is there a way to ensure that a model defined earlier is removed completely from the GPU? Will a simple ""del model"" do the trick?', ""I have not tried del model. You can monitor GPU usage with nvidia-smi (if not using CNMeM). \r\nUsually I start a fresh python session. Maybe there is a trick do release the model, but I don't know it."", 'I tried del and forcing gc but does not work. I am going to close this ticket and it is not a bug and ask how to release memory of previous models on the different issue. \r\n\r\nThanks so much @unrealwill for your help. You were spot on.', '@datumbox: The `del` keyword in Python only releases main memory allocated by your Python script. In order to release the memory on the GPU, you could use `K.clear_session()` where `K` refers to your Keras backend.', ""@unrealwill Is there something fundamentally different in the way memory is implemented on Tensorflow vs Theano?  The Theano vgg16 model has no problem running on my 4GB graphics card wheras the TF model runs out of memory and I saw another thread talking about how it allocates 12GB of memory?  \r\n\r\nI understand they're different libraries and implemented entirely differently, but if this really is a memory issue why would one framework be able to handle it and the other not?  I'm not saying you're wrong, I'm just genuinely curious.  I haven't heard that Theano is that much more efficient that Tensorflow at memory and if that's the case it changes the choice of platform I'm going to focus on."", ""One dumb thing to check: if you have a run Ctl-Z'd in the background holding the GPU memory."", '@EvenOldridge Yes, Theano only reserved the amount of memory it needed for its variables, so running multiple Theano ""sessions"" in parallel was fine if your GPU had the RAM. Tensorflow greedily reserves all the RAM on all the GPU\'s when you start a session (check out nvidia-smi when you launch). That said, Theano is officially dying soon, and I\'ve actually seen pretty substantial performance increases by switching from it to TF (not to mention absurdly faster launch times due to no runtime compilation), so you\'re probably best sticking with TF and trying to work with its design decisions.\r\n\r\n@phobrain Good point, and I\'ve also been learning, if doing keras/TF in Jupyter, make sure to kill one notebook before trying to run another. I was getting some very strange, non-deterministic bugs when two notebooks were both alive and had TF running.', 'Is there a concrete answer to this question yet? @EvenOldridge @scnerd ']","[""python\r\nimport keras.applications\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Flatten, Dense, Input\r\nfrom keras.preprocessing import image\r\n\r\narchitecture = 'VGG19'\r\n\r\nmodel = Sequential()\r\nif architecture == 'VGG19': #This fails\r\n    print('Using VGG19')\r\n    model.add(keras.applications.vgg19.VGG19(weights='imagenet', include_top=False, input_tensor=Input(shape=(224, 224, 3), name='input')))\r\n    model.add(Flatten(name='flatten'))\r\n    model.add(Dense(4096, activation='relu', name='fc1'))\r\n    model.add(Dense(4096, activation='relu', name='fc2'))\r\n    model.add(Dense(2, activation='softmax', name='predictions'))\r\nelse: #This works\r\n    print('Using ResNet50')\r\n    model.add(keras.applications.resnet50.ResNet50(weights='imagenet', include_top=False, input_tensor=Input(shape=(224, 224, 3), name='input')))\r\n    model.add(Flatten(name='flatten'))\r\n    model.add(Dense(2, activation='softmax', name='predictions'))\r\n\r\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\r\ngenerator = image.ImageDataGenerator().flow_from_directory('./data/cifar2tiny-train', target_size=(224, 224), batch_size=4, class_mode='categorical', shuffle=True) #100 images in 2 classes\r\nmodel.fit_generator(generator, samples_per_epoch=100, nb_epoch=1)\r\n"", '\r\nUsing TensorFlow backend.\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\nUsing VGG19\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: Quadro K2200\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.124\r\npciBusID 0000:03:00.0\r\nTotal memory: 3.95GiB\r\nFree memory: 3.17GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x3ed1b30\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: \r\nname: Quadro K2200\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.124\r\npciBusID 0000:81:00.0\r\nTotal memory: 3.95GiB\r\nFree memory: 3.92GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro K2200, pci bus id: 0000:03:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Quadro K2200, pci bus id: 0000:81:00.0)\r\nFound 100 images belonging to 2 classes.\r\nEpoch 1/1\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.14GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 1.05GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.09GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 1.05GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.10GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 1.08GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 561.27MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.15GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 1.09GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 589.50MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (256): \tTotal Chunks: 2, Chunks in use: 0 512B allocated for chunks. 16B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (512): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1024): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4096): \tTotal Chunks: 1, Chunks in use: 0 7.5KiB allocated for chunks. 8B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (32768): \tTotal Chunks: 1, Chunks in use: 0 32.0KiB allocated for chunks. 32.0KiB client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (131072): \tTotal Chunks: 1, Chunks in use: 0 128.0KiB allocated for chunks. 16.0KiB client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (33554432): \tTotal Chunks: 1, Chunks in use: 0 49.73MiB allocated for chunks. 16.0KiB client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (67108864): \tTotal Chunks: 2, Chunks in use: 0 128.02MiB allocated for chunks. 128.00MiB client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (268435456): \tTotal Chunks: 1, Chunks in use: 0 366.36MiB allocated for chunks. 64.0KiB client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:660] Bin for 392.00MiB was 256.00MiB, Chunk State: \r\nI tensorflow/core/common_runtime/bfc_allocator.cc:666]   Size: 366.36MiB | Requested Size: 64.0KiB | in_use: 0, prev:   Size: 64.00MiB | Requested Size: 64.00MiB | in_use: 1\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305980000 of size 1280\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305980500 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305980600 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305980700 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305980800 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305980900 of size 512\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305980b00 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305980c00 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305980d00 of size 1024\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305981100 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305981200 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305981300 of size 2048\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305981b00 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305981c00 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305981d00 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305981e00 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305981f00 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305982000 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305982100 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305982200 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305982300 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305982400 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305982500 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305982600 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305982700 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305982800 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305982900 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305982a00 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305982b00 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305982c00 of size 512\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305982e00 of size 1024\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305983200 of size 3328\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305983f00 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305984000 of size 16384\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305988000 of size 6912\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305989b00 of size 32768\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305991b00 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305991c00 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305991d00 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305991e00 of size 16384\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305995e00 of size 16384\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305999e00 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305999f00 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230599a000 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230599a100 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230599a200 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230599a300 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230599a400 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230599a500 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230599a600 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230599a800 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230599a900 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230599aa00 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230599ab00 of size 6912\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230599c700 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230599e600 of size 512\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230599e800 of size 512\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230599ea00 of size 1024\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230599ee00 of size 1024\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230599f200 of size 1024\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230599f600 of size 1024\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230599fa00 of size 2048\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059a0200 of size 2048\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059a0a00 of size 2048\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059a1200 of size 2048\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059a1a00 of size 2048\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059a2200 of size 2048\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059a2a00 of size 2048\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059a3200 of size 2048\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059a3a00 of size 17920\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059a8000 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059a8100 of size 147456\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059cc100 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059cc200 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059cc300 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059cc400 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059cc500 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059cc600 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059cc700 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059cc800 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059cc900 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059cca00 of size 6912\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059ce500 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059ce600 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059ce700 of size 512\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059ce900 of size 512\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059ceb00 of size 1024\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059cef00 of size 1024\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059cf300 of size 1024\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059cf700 of size 1024\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059cfb00 of size 2048\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059d0300 of size 2048\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059d0b00 of size 2048\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059d1300 of size 2048\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059d1b00 of size 2048\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059d2300 of size 2048\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059d2b00 of size 2048\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059d3300 of size 2048\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059d3b00 of size 16384\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059d7b00 of size 16384\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059dbb00 of size 32768\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059e3b00 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059e3c00 of size 50432\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059f0100 of size 512\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23059f0300 of size 294912\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305a38300 of size 147456\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305a5c300 of size 147456\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305a80300 of size 512\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305a80500 of size 589824\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305b10500 of size 294912\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305b58500 of size 294912\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305ba0500 of size 1024\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305ba0900 of size 1179648\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305cc0900 of size 589824\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305d50900 of size 589824\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305de0900 of size 6912\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305de2400 of size 147456\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305e06400 of size 294912\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305e4e400 of size 589824\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2305ede400 of size 1320192\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2306020900 of size 2359296\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2306260900 of size 1024\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2306260d00 of size 1024\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2306261100 of size 1024\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2306261500 of size 2359296\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23064a1500 of size 1179648\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23065c1500 of size 1179648\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23066e1500 of size 2048\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23066e1d00 of size 9437184\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2306fe1d00 of size 9437184\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23078e1d00 of size 9437184\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23081e1d00 of size 9437184\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2308ae1d00 of size 4718592\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2308f61d00 of size 2359296\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23091a1d00 of size 2359296\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23093e1d00 of size 2359296\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2309621d00 of size 7077888\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2309ce1d00 of size 9437184\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230a5e1d00 of size 2048\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230a5e2500 of size 2048\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230a5e2d00 of size 2048\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230a5e3500 of size 2048\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230a5e3d00 of size 2048\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230a5e4500 of size 2048\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230a5e4d00 of size 2048\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230a5e5500 of size 4578048\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230aa43000 of size 9437184\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230b343000 of size 9437184\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230bc43000 of size 9437184\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230c543000 of size 411041792\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2324d43000 of size 67108864\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2328d43000 of size 67108864\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x232cd43000 of size 53788672\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x233008f000 of size 2359296\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23302cf000 of size 2359296\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x233050f000 of size 2359296\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x233074f000 of size 4718592\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2330bcf000 of size 9437184\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23314cf000 of size 9437184\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2331dcf000 of size 9437184\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23326cf000 of size 9437184\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2332fcf000 of size 9437184\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23338cf000 of size 9437184\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23341cf000 of size 9437184\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2338ad3000 of size 16384\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2338ad7000 of size 16384\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2338adb000 of size 12845056\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x233971b000 of size 25690112\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x233af9b000 of size 25690112\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x233c81b000 of size 6422528\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x233ce3b000 of size 12845056\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x233da7b000 of size 12845056\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x233e6bb000 of size 12845056\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x233f2fb000 of size 12845056\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x233ff3b000 of size 3211264\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x234024b000 of size 6422528\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x234086b000 of size 6422528\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2340e8b000 of size 6422528\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23414ab000 of size 6422528\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2341acb000 of size 1605632\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2341c53000 of size 1605632\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2341ddb000 of size 1605632\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2341f63000 of size 1605632\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23420eb000 of size 1605632\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2342273000 of size 401408\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23422d5000 of size 16384\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23422d9000 of size 49152\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23422ed000 of size 32768\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2342315000 of size 65536\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2342325000 of size 401408\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2345543000 of size 2359296\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2345783000 of size 4718592\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2345c03000 of size 9437184\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2346503000 of size 9437184\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2346e03000 of size 9437184\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2347703000 of size 9437184\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2348003000 of size 9437184\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2348903000 of size 9437184\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2349203000 of size 9437184\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2349b03000 of size 411041792\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2362303000 of size 67108864\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2366303000 of size 2408448\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x236654f000 of size 64700416\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x236a303000 of size 411041792\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x2382b03000 of size 411041792\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x239f303000 of size 67108864\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x23a3303000 of size 67108864\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x230599a700 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x230599c600 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x230599c800 of size 7680\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x2334acf000 of size 67125248\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x23422e5000 of size 32768\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x23422f5000 of size 131072\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x2342387000 of size 52150272\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x239b303000 of size 67108864\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x23a7303000 of size 384159744\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:693]      Summary of in-use Chunks by size: \r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 55 Chunks of size 256 totalling 13.8KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 8 Chunks of size 512 totalling 4.0KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 14 Chunks of size 1024 totalling 14.0KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1280 totalling 1.2KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 25 Chunks of size 2048 totalling 50.0KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 3328 totalling 3.2KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 4 Chunks of size 6912 totalling 27.0KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 8 Chunks of size 16384 totalling 128.0KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 17920 totalling 17.5KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 3 Chunks of size 32768 totalling 96.0KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 49152 totalling 48.0KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 50432 totalling 49.2KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 65536 totalling 64.0KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 4 Chunks of size 147456 totalling 576.0KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 4 Chunks of size 294912 totalling 1.12MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 401408 totalling 784.0KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 4 Chunks of size 589824 totalling 2.25MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 3 Chunks of size 1179648 totalling 3.38MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1320192 totalling 1.26MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 5 Chunks of size 1605632 totalling 7.66MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 9 Chunks of size 2359296 totalling 20.25MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 2408448 totalling 2.30MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 3211264 totalling 3.06MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 4578048 totalling 4.37MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 3 Chunks of size 4718592 totalling 13.50MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 5 Chunks of size 6422528 totalling 30.62MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 7077888 totalling 6.75MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 22 Chunks of size 9437184 totalling 198.00MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 5 Chunks of size 12845056 totalling 61.25MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 25690112 totalling 49.00MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 53788672 totalling 51.30MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 64700416 totalling 61.70MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 5 Chunks of size 67108864 totalling 320.00MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 4 Chunks of size 411041792 totalling 1.53GiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 2.35GiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats: \r\nLimit:                  3095265280\r\nInUse:                  2524549120\r\nMaxInUse:               3007407104\r\nNumAllocs:                     553\r\nMaxAllocSize:            585486336\r\n\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:274] **************************_******_************************************************_*****____________\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:275] Ran out of memory trying to allocate 392.00MiB.  See logs for memory state.\r\nW tensorflow/core/framework/op_kernel.cc:975] Resource exhausted: OOM when allocating tensor with shape[25088,4096]\r\nTraceback (most recent call last):\r\n  File ""./example.py"", line 24, in <module>\r\n    model.fit_generator(generator, samples_per_epoch=100, nb_epoch=1)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 935, in fit_generator\r\n    initial_epoch=initial_epoch)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 1553, in fit_generator\r\n    class_weight=class_weight)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 1316, in train_on_batch\r\n    outputs = self.train_function(ins)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py"", line 1900, in __call__\r\n    feed_dict=feed_dict)\r\n  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 766, in run\r\n    run_metadata_ptr)\r\n  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[25088,4096]\r\n\t [[Node: gradients/MatMul_grad/MatMul_1 = MatMul[T=DT_FLOAT, _class=[""loc:@MatMul""], transpose_a=true, transpose_b=false, _device=""/job:localhost/replica:0/task:0/gpu:0""](Reshape_16, gradients/add_16_grad/Reshape)]]\r\n\r\nCaused by op u\'gradients/MatMul_grad/MatMul_1\', defined at:\r\n  File ""./example.py"", line 24, in <module>\r\n    model.fit_generator(generator, samples_per_epoch=100, nb_epoch=1)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 935, in fit_generator\r\n    initial_epoch=initial_epoch)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 1450, in fit_generator\r\n    self._make_train_function()\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 761, in _make_train_function\r\n    self.total_loss)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/optimizers.py"", line 227, in get_updates\r\n    grads = self.get_gradients(loss, params)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/optimizers.py"", line 80, in get_gradients\r\n    grads = K.gradients(loss, params)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py"", line 1925, in gradients\r\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\r\n  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 482, in gradients\r\n    in_grads = grad_fn(op, *out_grads)\r\n  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_grad.py"", line 731, in _MatMulGrad\r\n    math_ops.matmul(op.inputs[0], grad, transpose_a=True))\r\n  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py"", line 1729, in matmul\r\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 1442, in _mat_mul\r\n    transpose_b=transpose_b, name=name)\r\n  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 759, in apply_op\r\n    op_def=op_def)\r\n  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2240, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1128, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\n...which was originally created as op u\'MatMul\', defined at:\r\n  File ""./example.py"", line 13, in <module>\r\n    model.add(Dense(4096, activation=\'relu\', name=\'fc1\'))\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 332, in add\r\n    output_tensor = layer(self.outputs[0])\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 572, in __call__\r\n    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 635, in add_inbound_node\r\n    Node.create_node(self, inbound_layers, node_indices, tensor_indices)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 166, in create_node\r\n    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 813, in call\r\n    output = K.dot(x, self.W)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py"", line 814, in dot\r\n    out = tf.matmul(x, y)\r\n  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py"", line 1729, in matmul\r\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py"", line 1442, in _mat_mul\r\n    transpose_b=transpose_b, name=name)\r\n  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 759, in apply_op\r\n    op_def=op_def)\r\n  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2240, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1128, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[25088,4096]\r\n\t [[Node: gradients/MatMul_grad/MatMul_1 = MatMul[T=DT_FLOAT, _class=[""loc:@MatMul""], transpose_a=true, transpose_b=false, _device=""/job:localhost/replica:0/task:0/gpu:0""](Reshape_16, gradients/add_16_grad/Reshape)]]\r\n\r\n\r\nProcess finished with exit code 1\r\n', '\r\nUsing TensorFlow backend.\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\nUsing ResNet50\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: Quadro K2200\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.124\r\npciBusID 0000:03:00.0\r\nTotal memory: 3.95GiB\r\nFree memory: 3.16GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x4444090\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: \r\nname: Quadro K2200\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.124\r\npciBusID 0000:81:00.0\r\nTotal memory: 3.95GiB\r\nFree memory: 3.92GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro K2200, pci bus id: 0000:03:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Quadro K2200, pci bus id: 0000:81:00.0)\r\nFound 100 images belonging to 2 classes.\r\nEpoch 1/1\r\n  4/100 [>.............................] - ETA: 308s - loss: 0.6807 - acc: 0.7500I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2668 get requests, put_count=2592 evicted_count=1000 eviction_rate=0.385802 and unsatisfied allocation rate=0.44078\r\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110\r\n 92/100 [==========================>...] - ETA: 1s - loss: 2.8755 - acc: 0.5326I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2696 get requests, put_count=2955 evicted_count=1000 eviction_rate=0.338409 and unsatisfied allocation rate=0.283383\r\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281\r\n100/100 [==============================] - 22s - loss: 2.6731 - acc: 0.5500    \r\n\r\nProcess finished with exit code 0\r\n']",[],0,0
239,keras,11311,closed,AttributeError when attempting to set batch_input_shape using setattr,"Keras 2.2.4; TensorFlow 1.11.0; Python 3.6.6.

I'm trying to set up a test harness for an LSTM RNN. As a part of this, I want to test the effectiveness of statefulness. Since the LSTM is the first layer and using statefulness requires using batch_input_shape attribute rather than the input_shape attribute, I am attempting to define the layer first, then set the correct attributes. When I run the function below, I get an AttributeError: ""'LSTM' object has no attribute 'dtype'."" I've traced this specifically to adding the batch_input_shape attribute.

",,"[""You shouldn't set attributes manually. This is not supported by keras. (a lot of stuff happens in the __init__()) "", 'Is there a reason why? This makes for less efficient code. ', ""It would make the keras codebase more complicated because it would mean that we need to store the input arguments somewhere, and only do all the logic later on. You can look into de codebase to see how layers are implemented, and you'll see it would be very verbose to defer the execution of the logic in a lazy way. \r\n\r\nYou can consider that the attributes are read-only in keras (it's also the case in many other python libraries). "", 'Understood - thank you for the explanation!']","[""\r\ndef build_lstm_layer(batch_size, stateful, units, X_shape):\r\n    lstm = keras.layers.LSTM(units)\r\n\r\n    if stateful:\r\n        setattr(lstm, 'stateful', stateful)\r\n        setattr(lstm, 'batch_input_shape', (batch_size, X_shape[1], X_shape[2]))\r\n\r\n    else:\r\n        setattr(lstm, 'input_shape', (X_shape[1], X_shape[2]))\r\n\r\n    return lstm\r\n""]",[],0,0
240,keras,459,closed,Problem with the sequence input to SimpleRNN,"I have a 2D array consisting of time (nsecs), latitude, longitude and velocity as features which should be the input to simpleRNN. Its shape is (33336,4) as (nb_samples, input_dim). But there's a 3D array (nb_samples, timesteps, input_dim) needed. Of course, I could simply use numpy.expand_dims() but then I would have something like (33336,1,4). But my time features are representing those timesteps. So now I'm confused how to actually create a decent 3D array with latitude, longitude and velocity and the time feature as required timesteps. I tried out different things like first creating a 3D array and then adding the data where its needed. But then I get some memory errors because of its size. Have you guys any ideas how I could create a reasonable input?
",,"[""nb_samples is number of samples/examples, so you should split your training data accordingly. \n\nIf 'data' is your data of shape (33336, 4), X denotes the input data and you use time windows of lenght 25, then X[0] = data[0:25], X[1] = data[1:26] etc.\n"", '#facepalm yes, of course. That makes sense. Thank you!\n']",[],[],0,0
241,keras,5425,closed,"LSTM different case of sequences, doubts in general and CNN+LSTM network for regression problem","Hello, thanks in advance for your help and for the developers of Keras!

I am working with LSTM networks, actually I am trying to create a CNN+LSTM network that takes as inputs images with 3 channels. I have been reading a lot, but I still have several doubts about how LSTM layers really work, because the results I am obtaining in my experiments are horrible, and this networks are told to give great results.
I have read #4149 and #2403 and I have clear my mind enough to know that I have to learn a lot.

First I will told you my task and then enumerate the doubts I have about recurrent layers.

My inputs are images with 3 channels, I have reshape my data set with the following code in order to have sequences in time:











And that gives me structures with the following shapes:






So if I am right, that means that I have nb_samples=2126 (number of samples), each sample is a sequence of length 2 and each element of that sequence is an image of 3 channels and dimensions 10x8, am I right?

My outputs is a matrix of dimensions , so each input image has associated 3 numbers as outputs. What I want is to feed my net with my input sequences of image so each sequence has the image for t-1 and t, I want my net to give me as output the 3 numbers associated with the image at time t. I have read a lot about problems where the sequences are t-1,t and the output is t+1, but I want the output that correspond to the last element of my input sequence.

Having this in mind, I don't know if that is what I am doing with this net:















So as long as I knew, with  I make sure that the convolutional layer is applayed to each element in the sequence separated. And I added  in the first LSTM layer to connect with the second LSTM layer. Finally the Dense layer is to obtain the 3 outputs I need.

And here comes my doubts (generals and about my problem):

1. With this network, am I calculating the output of the last element of the sequences or the output of the future instant t+1?

2. Are my data reshape right?

3. I don't really understand all the parameters the LSTM and recurrent layers have (I have read the keras documentation but it is not clear to me). Moreover, I don't understand the difference between the cases in this image:
![e4cdf91c-063f-11e6-8844-c89a9e134339](https://cloud.githubusercontent.com/assets/25105487/23065901/1db65978-f518-11e6-8915-2c57668e714e.png)

I don't understand the difference and I don't know how can I programm the layer to obtain the different cases.

4. I have read that is recommended to use  instead of  to connect the cnn layer with the lstm layer, but to me, the reshape is not working.

5. Am I using well the  layer? 
I have read this : [https://github.com/fchollet/keras/blob/master/examples/imdb_cnn_lstm.py](url)
I know is 1D instead of 2D, but in that example they don't use TimeDistributed nor  layer.


I think that's all for the moment. Sorry about the big post, and hope some of you could help me.",stale,"[""Hello,\r\n\r\n5 Yes\r\n\r\n4 You are using TimeDistributed(Flatten()), so this is correct.\r\n\r\n3 Currently your architecture is many-to-one. You seem to be on the right track. By playing a little with the flag return_sequence and layer Repeat you can probably get to comprehend the rest of the architectures on your own.\r\n\r\n2 Y_seq = Y[(seq_len-1):10*len(Y),:] is confusing use  Y_seq = Y[(seq_len-1):,:]\r\n\r\n1 You are calculating the output of the last element of the sequence. (More exactly you are calculating an output based on the information contained on all elements of your sequence)\r\n\r\nThe fact that your seq_length=2 is confusing because LSTM use all information before t, not just t-1 and t (which is true in your case because seq_length=2). If you just want to use only information t-1 and t then use masked convolutions.\r\n\r\nBecause your seq_length=2 you are not really using the power of LSTM, and you maybe missing the point.\r\nFinally many-to-one architecture are harder to train because that is fundamentally a harder problem, because the network doesn't know in what timestep is the relevant information, and the network has to save this relevant information in its internal state, so its not easy, specially with layer sizes so small.\r\n\r\n"", ""Hi, thanks for replying! \r\n\r\nAbout point 3, how do you know my problem is many-to-one? I think I would like to have a one-to-one network. Anyway I will check the Repeat layer you told me. \r\n\r\nIn the case I would need the many-to-one architecture, I should increase the seq_len and the numbers of blocks in the LSTM layer, is it?\r\nThe fact is that for my problem I can't have a seq_len too large, the ideal was to have seq_len=2, so you recommend me to use only convolutions layers? I was very stucked because I have read about the amazing results of the CNN+LSTM, so I thought it could work for my problem.\r\n\r\nThanks about point 2!"", 'Hello,\r\nYour problem take a sequence of seq_length (=2 =many) as input, and produce a single (=one) output. \r\nIf you want to have a one-to-one network, just concatenate( Merge(mode=""concat"") )  your input sequence into a vector of 2 times the dim.\r\n\r\nLayers should have usually between 100 and 1000 units. One rule of thumb is that the amount of memory that your network has (and therefore its representation power) is proportional to its number of parameters. (To count the number of parameters (use model.summary() )\r\n\r\nYou will have to find what works best for your problem, but LSTM usually take longer to train than CNN and the main feature which is ""allow sequence of arbitrary length"" is irrelevant in your case.\r\n\r\nAlso you don\'t seem to have a lot of data, so SVM would probably work better.\r\n', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs, but feel free to re-open it if needed.\n']",[],"[""data = scipy.io.loadmat('cnn_1p.mat')"", 'X = data_test[""imgL""]', 'Y = data_test[""target""]', 'X_seq = []', 'seq_len = 2', 'for i in range(len(X)-seq_len+1):', '    X_seq.append(X[i:i+seq_len,:,:,:])', 'X_seq = np.asarray(X_seq)', 'Y_seq = Y[(seq_len-1):10*len(Y),:]', 'In [173]: X_seq.shape', 'Out[173]: (2126, 2, 3, 10, 8)', 'In [174]: Y_seq.shape', 'Out[174]: (2126, 3)', '(nb_samples, 3)', 'model = Sequential()', ""model.add(TimeDistributed(Convolution2D(40, 3, 3, border_mode='valid', activation='relu'), input_shape=(seq_len,3, 10, 8)))"", 'model.add(TimeDistributed(Dropout(0.2)))', 'model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))', ""model.add(TimeDistributed(Convolution2D(20, 2, 2, border_mode='valid', activation='relu')))"", 'model.add(TimeDistributed(Dropout(0.2)))', 'model.add(TimeDistributed(Flatten()))', 'model.add(LSTM(30, return_sequences = True))', 'model.add(Dropout(0.2))', 'model.add(LSTM(15))', 'model.add(Dropout(0.2))', ""model.add(Dense(3, init='uniform'))"", ""model.compile(optimizer='adam', loss='mse')"", 'TimeDistributed', 'return_sequences = True', 'reshape', 'flatten()', 'TimeDistributed', 'flatten()']",0,0
242,keras,3089,closed,passing a fixed matrix into the custom loss,"For a given data $X \in N*M$ (N is the data size, M is the input dim), the reconstruction of autoencoder and the corresponding representation (the output of encoder) are $Y$ and $H$. I hope to enforce some constraints on $H$, and my loss function is: 

$(X-Y)^{2} + lambda*(H-CH)^{2}$, where C is a pre-defined matrix. My code is as follows:

X = Input(shape=(784,), name='X')
H = Dense(10, activation='tank')(X)
Y = Dense(784, activation='tanh')(H)
model = Model(input=X, output=Y)
model.compile(optimizer='sgd', loss=my_loss(C, H, 0.2)) #  C is the given matrix, whose shape is N*N, where N is the size of X.
model.fit(X, X, nb_epoch=100, batch_size=32)
# the custom loss function

def my_loss(C, H, lmd): #  C is given, whose shape is N*N, where N is the size of X.
    global_loss = K.mean(K.square(encoded - K.dot(C, encoded)), axis=-1)
    def loss(y_true, y_pred):
        local_loss = K.mean(K.square(y_true - y_pred), axis=-1)
        return local_loss + lmd \* global_loss
    return loss
## I got the following errors:

AssertionError: Theano Assert failed!
Apply node that caused the error: Assert{msg='Theano Assert failed!'}(Elemwise{Composite{(i0 \* (Abs(i1) + i2 + i3))}}[(0, 2)].0, Elemwise{eq,no_inplace}.0)
Toposort index: 46
Inputs types: [TensorType(float32, matrix), TensorType(int8, scalar)]
Inputs shapes: [(32, 10), ()]
Inputs strides: [(40, 4), ()]
Inputs values: ['not shown', array(0, dtype=int8)]
Outputs clients: [[Elemwise{sub,no_inplace}(Assert{msg='Theano Assert failed!'}.0, InplaceDimShuffle{x,x}.0)]]

HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.
## HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.

I guess that the obtained error may result from passing the fix-sized matrix C into my loss. Anybody can kindly help me to solve this issue? Thank you very much. 
",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],0,0
243,keras,7614,closed,Predicting N probabailities for each pixel in Keras,"I Am developing a **classification based-model** to predict 12 probability for each pixel in the image , I have built the architecture , but I am not sure whether I am right or not , I am a newbie in deep learning.


_The following is the function for my baseline architecture :_

   

I will explain my architecture , the input is about a 64 * 64 * 1 graysale image , followed by many convolution layers and then it is flattened then there is two different hidden layer U & V , U represents U Channel in the CIELUV color space and V represents V Channel in the same color space, then the U&V channels each of them is reshaped into 64 * 64 * 12 , lemme convert it to 4096 * 12 so , in the 4096 pixel each of them should have a 12 corresponding probabilities and the sum of the 12 probability should be = 1 but that doesn't happen the sum of the overall matrix ( 4096 * 12 ) = 1 , how can I do that make every 12 node have a sum of 1 probability ? **thanks in advance**











",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']","[""\r\n import keras\r\n    from keras.models import Model\r\n    from keras.layers import Flatten, Dense, Input, Reshape,concatenate\r\n    from keras.layers import Conv2D\r\n    from keras import regularizers \r\n    \r\n    Input_Img=Input(shape=(Img_Size,Img_Size,1),name='Main_Input')     \r\n    \r\n    #The First Conv Layer + BatchNormalization\r\n    X=Conv2D(filters=8,kernel_size=5,activation='relu',name='Conv1')(Input_Img)\r\n    #The Second Conv Layer + BatchNormalization\r\n    X=Conv2D(filters=16,kernel_size=5,activation='relu',name='Conv2')(X)\r\n     #The Third Conv Layer + BatchNormalization\r\n    X=Conv2D(filters=32,kernel_size=5,activation='relu',name='Convfsf3d')(X)\r\n \r\n    X=Flatten()(X)\r\n    X=Dense(units=8,activation='relu')(X)\r\n    U=Dense(units=49152,name='U_Nodes',activation='softmax')(X)\r\n    V=Dense(units=49152,name='V_Nodes',activation='softmax')(X)\r\n    \r\n    L_Reshape=Reshape(target_shape=(Img_Size*Img_Size,1))(Input_Img)\r\n    U_Reshape=Reshape(target_shape=(Img_Size*Img_Size,12))(U)\r\n    V_Reshape=Reshape(target_shape=(Img_Size*Img_Size,12))(V)\r\n    X=concatenate(inputs=[L_Reshape,U_Reshape,V_Reshape])\r\n    MyModel=Model(Input_Img,X)\r\n    MyModel.compile(optimizer='adam',loss=keras.losses.categorical_crossentropy,metrics=['accuracy'])\r\n    print(MyModel.summary())\r\n    return MyModel\r\n\r\n""]",[],0,0
244,keras,11108,closed,adding one filter to existing filter in auto encoder during learning,"Hi,
I have an auto-encoder and as we know, it has two parts, encoder and decoder. the output of my encoder part is a 28X28 image and I want to add another 28X28 image to it and send 28X28X2 filter to decoder part during learning. I want to know, is it possible or not? if yes, how? please guide me completely due to I am a beginner. I attached my code here too. I do not know using this ""merge_encoded_w=cv2.merge(encoded,w)"" for adding w to encoder output is true or not?thanks
channels_firstchannels_firstchannels_firstchannels_first",type:support,"[""Hello, I'm positive it must be possible, but I sadly don't have the time right now to take your code and implement a solution and debug it (I don't think the answer it trivial).\r\n\r\nCan you please ask this question on StackOverflow or in the keras users mailing list? You will have more success there and also this will be more adapted since GitHub issues are mainly for reporting bugs or asking new features.\r\n\r\nFeel free to put a link there to your question in StackOverflow or the keras users mailing list so that future readers can find it :)"", ""thank you very much. I asked in StackOverflow. but no one answers\xa0me and I really need the solution for this. I ask here due to I see some other similar questions and some people answer their questions. I hoped to find the solution here.\n--Maedeh Jamali\nPhD Candidate of Artificial Intelligence\xa0\nDepartment of Electrical & Computer Engineering,\nIsfahan University of Technology \n\n    On Sunday, September 9, 2018, 7:30:12 AM EDT, Gabriel de Marmiesse <notifications@github.com> wrote:  \n \n \nHello, I'm positive it must be possible, but I sadly don't have the time right now to take your code and implement a solution and debug it (I don't think the answer it trivial).\n\nCan you please ask this question on StackOverflow or in the keras users mailing list? You will have more success there and also this will be more adapted since GitHub issues are mainly for reporting bugs or asking new features.\n\nFeel free to put a link there to your question in StackOverflow or the keras users mailing list so that future readers can find it :)\n\n—\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n   ""]",[],"['', '\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Input, Dense, Dropout, Activation,UpSampling2D,Conv2D, MaxPooling2D, GaussianNoise\r\nfrom keras.models import Model\r\nfrom keras.optimizers import SGD\r\nfrom keras.datasets import mnist\r\nfrom keras import regularizers\r\nfrom keras import backend as K\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport cv2\r\nfrom time import time\r\nfrom keras.callbacks import TensorBoard\r\n# Embedding phase\r\n##encoder\r\nw=np.random.random((1, 28,28))\r\n\r\ninput_img = Input(shape=(28, 28, 1))  # adapt this if using ', "" image data format\r\n\r\nx = Conv2D(8, (5, 5), activation='relu', padding='same')(input_img)\r\n#x = MaxPooling2D((2, 2), padding='same')(x)\r\nx = Conv2D(4, (3, 3), activation='relu', padding='same')(x)\r\n#x = MaxPooling2D((2, 2), padding='same')(x)\r\nx = Conv2D(2, (3, 3), activation='relu', padding='same')(x)\r\nencoded = Conv2D(1, (3, 3), activation='relu', padding='same')(x)\r\nmerge_encoded_w=cv2.merge(encoded,w)\r\n#\r\n#decoder\r\n\r\nx = Conv2D(2, (5, 5), activation='relu', padding='same')(merge_encoded_w)\r\n#x = UpSampling2D((2, 2))(x)\r\nx = Conv2D(4, (3, 3), activation='relu', padding='same')(x)\r\n#x = UpSampling2D((2, 2))(x)\r\nx = Conv2D(8, (3, 3), activation='relu',padding='same')(x)\r\n#x = UpSampling2D((2, 2))(x)\r\ndecoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\r\n\r\n#Extraction phase\r\ndecodedWithNois=GaussianNoise(0.5)(decoded)\r\nx = Conv2D(8, (5, 5), activation='relu', padding='same')(decodedWithNois)\r\n#x = MaxPooling2D((2, 2), padding='same')(x)\r\nx = Conv2D(4, (3, 3), activation='relu', padding='same')(x)\r\n#x = MaxPooling2D((2, 2), padding='same')(x)\r\nfinal_image_watermark = Conv2D(2, (3, 3), activation='relu', padding='same')(x)\r\n\r\n\r\nautoencoder = Model([input_img,w], [decoded,final_image_watermark(2)])\r\nencoder=Model(input_img,encoded)\r\nautoencoder.compile(optimizer='adadelta', loss=['mean_squared_error','mean_squared_error'],metrics=['accuracy'])\r\n(x_train, _), (x_test, _) = mnist.load_data()\r\nx_validation=x_train[1:10000,:,:]\r\nx_train=x_train[10001:60000,:,:]\r\n#\r\nx_train = x_train.astype('float32') / 255.\r\nx_test = x_test.astype('float32') / 255.\r\nx_validation = x_validation.astype('float32') / 255.\r\nx_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using "", ' image data format\r\nx_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using ', ' image data format\r\nx_validation = np.reshape(x_validation, (len(x_validation), 28, 28, 1))  # adapt this if using ', "" image data format\r\nautoencoder.fit(x_train, x_train,\r\n                epochs=5,\r\n                batch_size=128,\r\n                shuffle=True,\r\n                validation_data=(x_validation, x_validation),\r\n                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])\r\n\r\ndecoded_imgs = autoencoder.predict(x_test)\r\nencoded_imgs=encoder.predict(x_test)\r\n"", '']",0,0
245,keras,8657,closed,Introduction of global metrics (precision and recall),,,"[""Keras computes batchwise metrics in averages.\r\n\r\nSo those can't be accurately calculated .\r\n\r\nSee my improvement suggestion:\r\nhttps://github.com/fchollet/keras/issues/8607"", 'I agree that global metrics like precision and recall are important to have. @Kritikalcoder or @brge17, would you like to work on it?\r\n\r\nThe general template would be callable metrics classes (similar to layers, but simpler) that maintain weights (a state containing the global metric value). Calling an instance updates the state and returns the current state value. Makes sense?', 'Makes sense, I will give it a shot and raise a PR.', ""Note that the main change will be to add support for such metrics in `training.py`. We'll need a special case for them because *they will have update ops* and these update ops need to be incorporated into the training function and evaluate function."", 'Can we, please, have confusion matrix ASAP (i.e. making sure this new mechanism supports this use case). In fact, maybe precision and recall, etc should be calculated from the confusion matrix for a binary classifier anyway to sync the code. AUC is also an interesting use case (code design wise), probably the same very design can work if and only if it works for precision-recall curve.\r\n\r\nIn fact, maybe we can just have a single object that accumulates `y_true, y_pred, etc`, then applies whatever function you want to them, which, most of the time will be one of\r\nhttp://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics', '@ozabluda \r\n\r\nThe metric class we are describing will wrap any arbitrary metric to make it a global metric.\r\n\r\nThis includes False Positives, False Negatives, True Positive, True Negative (and there rate counterparts)\r\n\r\nThink of it like, global_metric(my_custom_metric(y_pred, y_true))', 'These are the global metrics that I think should be introduced:\r\n1. Micro precision\r\n2. Micro recall\r\n3. Micro F-score\r\n4. Macro precision\r\n5. Macro recall\r\n6. Macro F-score\r\n7. Confusion matrix', ""For the reference, here is a list of TF metrics, in case we'll (almost certainly) have to wrap both np and TF ones:\r\nhttps://www.tensorflow.org/api_docs/python/tf/metrics\r\nhttps://www.tensorflow.org/api_docs/python/tf/contrib/metrics"", 'Maybe we should have metrics classes that look like:\r\n\r\n```python\r\n\r\nclass MyMetric(object):\r\n\r\n     def __init__(self, ...):\r\n          self.true_positives = K.variable(...)\r\n          ...\r\n\r\n     def __call__(self, y_true, y_pred):\r\n         # will be called in `compile`\r\n         self.updates = [...]   # update state\r\n         return metric_value_so_far\r\n\r\n     def reset_state(self):\r\n         # will be called at the beginning of an epoch\r\n         K.batch_set_value([self.true_positives, ...], [0, ...])\r\n\r\n    def get_config(self):\r\n        ...\r\n\r\n    @classmethod\r\n    def from_config(self, config):\r\n        ...\r\n```\r\n\r\n`reset_state` should also be called when resetting model state (`model.reset_state()`, if I recall correctly).\r\n\r\nDo we need anything else to make it work?\r\n          \r\n           \r\n', 'I like that design, I was just going to pass the custom metric to the constructor:\r\n\r\n```\r\nclass GlobalMetric(object):\r\n\r\n     def __init__(self, my_metric):\r\n          self.metric_func = my_metric ...\r\n\r\n# Define the metric outside the class\r\ndef my_metric(y_pred, y_true):\r\n\t...\r\n\treturn score\r\n```\r\n\r\nWith the idea GlobalMetric could wrap any metric', '> With the idea GlobalMetric could wrap any metric\r\n\r\nI believe that would not work in the general case. The nature of the state, how to update it, and how to go from predictions/labels plus state to the current value, are all potentially-custom logic.', ""How I saw global metric wrapper above is it would work for any metric, but potentially be very expensive to compute.\r\n\r\nIt would essentially cache the predictions in the state, and recompute the metric from scratch each time new predictions are available. That solves the most general case, but is not necessarily compute efficient for every case.\r\n\r\n> The nature of the state, how to update it, and how to go from predictions/labels plus state to the current value, are all potentially-custom logic.\r\n\r\nSo for thing like False Positives or False Negatives, etc... We could write highly efficient implementations, but for a user custom metric they would get an admittedly inefficient (potentially) implementation but one that would work.\r\n\r\nThe metric I care most about, AUC, I don't have a clever way to cache it in state off the top of my head. Perhaps, I could read the TensorFlow streaming AUC for inspiration."", '> It would essentially cache the predictions in the state, and recompute the metric from scratch each time new predictions are available. That solves the most general case, but is not necessarily compute efficient for every case.\r\n\r\nThat would be expensive, in particular it would consume a lot of memory for large datasets. The streaming approach is preferable -- we typically only need to keep around a few scalars updated at each iteration, for most metrics.\r\n', 'In this PR, should the scope be limited to showing how to implement a couple key metrics in the fashion shown above? This will give people a starting point for their own custom metrics and knock out a couple of popular requests.\r\n\r\nI was thinking {false positive, false negative, true positive, true negative, precision, recall, auc}.\r\n\r\nPoint taken on the compute end.', 'Yes, exactly. We just want to push out a template, and cover precision/recall since these are very common and useful.', 'all those ""key metrics"" can be extracted from the confusion matrix.  \r\n>Can we, please, have confusion matrix ASAP (i.e. making sure this new mechanism supports this use case). In fact, maybe precision and recall, etc should be calculated from the confusion matrix for a binary classifier anyway to sync the code. AUC is also an interesting use case (code design wise), probably the same very design can work if and only if it works for precision-recall curve.', 'I will probably code it up some time next week during the week.\r\n\r\nI already have False Positive, False Negative, True Positive, True Negative (rate*), precision, recall tensor operations coded up and tested as normal keras metrics (batch wise averages). I will just need to adapt it to have state as shown above.', 'Great. This is an important feature.', 'Having issues with reset_state (also reset_states) not doing anything on epoch end.  From a quick glance at the source, it looks like reset state only works for layers. Not quite sure the best way to do this neatly, open for suggestions.\r\n\r\nHere is a quick mock up that assumes one hot encoding:\r\n\r\n```\r\nfrom abc import abstractmethod\r\n\r\nfrom keras import backend as K\r\n\r\nclass GlobalMetric(object):\r\n\r\n    @abstractmethod\r\n    def __call__(self, y_true, y_pred):\r\n        raise NotImplementedError(""Method not implemented."")\r\n\r\n    @abstractmethod\r\n    def update_states(self):\r\n        raise NotImplementedError(""Method not implemented."")\r\n\r\n    @abstractmethod\r\n    def reset_states(self):\r\n        raise NotImplementedError(""Method not implemented."")\r\n\r\nclass TruePositives(GlobalMetric):\r\n\r\n    def __init__(self, threshold=None):\r\n\r\n        self.__name__ = ""true_positives""\r\n        if threshold is None:\r\n            self.threshold = K.variable(value=0.5)\r\n        else:\r\n            self.threshold = K.variable(value=threshold)\r\n        # tp = true positives\r\n        self.tp = K.variable(value=0.0)\r\n\r\n    def __call__(self, y_true, y_pred):\r\n        self.update_states(y_true, y_pred)\r\n        return self.tp\r\n\r\n    def reset_state(self):\r\n        self.tp = K.update(self.tp, K.variable(value=0.0))\r\n\r\n    def reset_states(self):\r\n        self.tp = K.update(self.tp, K.variable(value=0.0))\r\n\r\n    def update_states(self, y_true, y_pred):\r\n\r\n        # Slice the positive score\r\n        y_true = y_true[:, 1]\r\n        y_pred = y_pred[:, 1]\r\n\r\n        # Softmax -> probabilities\r\n        y_pred = K.cast(y_pred >= self.threshold, \'float32\')\r\n        # c = correct classifications\r\n        c = K.cast(K.equal(y_pred, y_true), \'float32\')\r\n        # tp_batch = number of true positives in a batch\r\n        tp_batch = K.sum(c * y_true)\r\n\r\n        self.tp = K.update_add(self.tp, tp)\r\n```', 'This requires a couple of ugly changes to the Base Logger and Progress Bar:\r\n\r\nBasically, if metric is a GlobalMetric (turn off batch averaging) else do what currently was happening.', ""See fork: https://github.com/brge17/keras/tree/brian-dev\r\n\r\nRemaining issues:\r\n1. Find the proper place to reset state (for {train, eval, test}) currently have a hack that doesn't really work.\r\n2. Find least pass of resistance to update BaseLogger, ProgBar. This involves disabling batch averaging selectively for ProgBar etc...\r\n\r\nSolved some issues some remain.\r\n  "", ""The most critical issue is that if reset_states() is called after model.compile it does nothing as it isn't added to the graph.\r\n\r\n So either reset_states() is called at the end of an epoch, and it updates some un-initialized tensor that isn't actually part of the global metric, or it is called every batch (and we are back where we started)"", ""If all metrics will be single numbers, would there be a simple way to make it work with TensorBoard? Part of what's really useful about the metrics argument in fit is that it shows up in the TB web interface."", 'It will work with TensorBoard, I have a private fork that is getting closer to functioning.\r\n\r\n-b', ""I'm not sure if this thread is still active, but I've implemented precision/recall using the stateful metrics PR given above following from BinaryTruePositives (https://github.com/keras-team/keras/blob/master/tests/keras/metrics_test.py).  Are we working on implementing such requested metrics? \r\n\r\nFor anyone that would like it, this is what it looks like so far. I was only able to get it to work by saving the confusion matrix within the class however. \r\n\r\nhttps://gist.github.com/vsocrates/7ff65268c2ed533a62f8f9f9d86786af"", 'I tried to raise it to master, but we argued about what the definition should be for multi-class. I went with one vs others.\r\n\r\nhttps://github.com/keras-team/keras/pull/9393', ""I've seen that progress has been made (#9200, #9253), but i'm not sure - is this feature usable now?"", '@GalAvineri yes', 'Here\'s a working example:\r\n\r\n\r\n```python\r\nimport keras\r\nfrom keras.layers import Input, Dense\r\nfrom keras.models import Model\r\n\r\nimport numpy as np\r\n\r\nclass BatchCounter(keras.layers.Layer):\r\n\r\n        def __init__(self, name=""batch_counter"", **kwargs):\r\n            super(BatchCounter, self).__init__(name=name, **kwargs)\r\n            self.stateful = True\r\n            self.batches = keras.backend.variable(value=0, dtype=""int32"")\r\n\r\n        def reset_states(self):\r\n            keras.backend.set_value(self.batches, 0)\r\n\r\n        def __call__(self, y_true, y_pred):\r\n            updates = [\r\n                keras.backend.update_add(\r\n                    self.batches, \r\n                    keras.backend.variable(value=1, dtype=""int32""))]\r\n            self.add_update(updates)\r\n            return self.batches\r\n\r\n# Dummy dataset\r\nX = np.ones((50, 1))\r\ny = np.zeros((50, 1))\r\n\r\n# Dummy model\r\ninputs = Input(shape=(1,))\r\noutputs = Dense(1)(inputs)\r\nmodel = Model(inputs=inputs, outputs=outputs)\r\nmodel.compile(loss=""mse"", optimizer=""adam"", metrics=[BatchCounter()])\r\n\r\nmodel.fit(X, y, batch_size=10, epochs=10, validation_data = (X, y))\r\n```', ""That's a great example! Thank you!\r\nIt also seems that most of  the metrics requests of @Kritikalcoder are filled by previous comments."", 'They are pretty easy to write, I have internal implementations at work that only work for the TensorFlow backend.\r\n\r\nYou can pretty easily translate any TensorFlow (streaming metric) to a StateFul Metric.', 'For the lazy amongst us, I’ll post the implementations iv\'e collected:\r\nThe implementation of recall and precision were made by @briannemsick and the f1 score was implemented by the \'keras_metrics\' library:\r\n\r\n```python\r\nfrom tensorflow.keras.layers import Layer\r\nimport tensorflow.keras.backend as K\r\nfrom operator import truediv\r\n\r\n\r\nclass Recall(Layer):\r\n    \'\'\'Compute recall over all batches.\r\n    # Arguments\r\n        name: String, name for the metric.\r\n        class_ind: Integer, class index.\r\n    \'\'\'\r\n    def __init__(self, name=\'recall\', class_ind=1):\r\n        super(Recall, self).__init__(name=name)\r\n        self.true_positives = K.variable(value=0, dtype=\'float32\')\r\n        self.total_positives = K.variable(value=0, dtype=\'float32\')\r\n        self.class_ind = class_ind\r\n\r\n    def reset_states(self):\r\n        K.set_value(self.true_positives, 0.0)\r\n        K.set_value(self.total_positives, 0.0)\r\n\r\n    def __call__(self, y_true, y_pred):\r\n        \'\'\'Update recall computation.\r\n        # Arguments\r\n            y_true: Tensor, batch_wise labels\r\n            y_pred: Tensor, batch_wise predictions\r\n        # Returns\r\n            Overall recall for the epoch at the completion of the batch.\r\n        \'\'\'\r\n        # Batch\r\n        y_true, y_pred = _slice_by_class(y_true, y_pred, self.class_ind)\r\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n        total_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\r\n        # Current\r\n        current_true_positives = self.true_positives * 1\r\n        current_total_positives = self.total_positives * 1\r\n        # Updates\r\n        updates = [K.update_add(self.true_positives, true_positives),\r\n                   K.update_add(self.total_positives, total_positives)]\r\n        self.add_update(updates, inputs=[y_true, y_pred])\r\n        # Compute recall\r\n        return (current_true_positives + true_positives) / \\\r\n               (current_total_positives + total_positives + K.epsilon())\r\n\r\n\r\nclass Precision(Layer):\r\n    \'\'\'Compute precision over all batches.\r\n    # Arguments\r\n        name: String, name for the metric.\r\n        class_ind: Integer, class index.\r\n    \'\'\'\r\n    def __init__(self, name=\'precision\', class_ind=1):\r\n        super(Precision, self).__init__(name=name)\r\n        self.true_positives = K.variable(value=0, dtype=\'float32\')\r\n        self.pred_positives = K.variable(value=0, dtype=\'float32\')\r\n        self.class_ind = class_ind\r\n\r\n    def reset_states(self):\r\n        K.set_value(self.true_positives, 0.0)\r\n        K.set_value(self.pred_positives, 0.0)\r\n\r\n    def __call__(self, y_true, y_pred):\r\n        \'\'\'Update precision computation.\r\n        # Arguments\r\n            y_true: Tensor, batch_wise labels\r\n            y_pred: Tensor, batch_wise predictions\r\n        # Returns\r\n            Overall precision for the epoch at the completion of the batch.\r\n        \'\'\'\r\n        # Batch\r\n        y_true, y_pred = _slice_by_class(y_true, y_pred, self.class_ind)\r\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n        pred_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\r\n        # Current\r\n        current_true_positives = self.true_positives * 1\r\n        current_pred_positives = self.pred_positives * 1\r\n        # Updates\r\n        updates = [K.update_add(self.true_positives, true_positives),\r\n                   K.update_add(self.pred_positives, pred_positives)]\r\n        self.add_update(updates, inputs=[y_true, y_pred])\r\n        # Compute recall\r\n        return (current_true_positives + true_positives) / \\\r\n               (current_pred_positives + pred_positives + K.epsilon())\r\n\r\n\r\nclass F1(Layer):\r\n    """"""Create a metric for the model\'s F1 score calculation.\r\n    The F1 score is the harmonic mean of precision and recall.\r\n    """"""\r\n\r\n    def __init__(self, name=\'f1\', class_ind=1):\r\n        super().__init__(name=name)\r\n        self.recall = Recall(class_ind=class_ind)\r\n        self.precision = Precision(class_ind=class_ind)\r\n        self.class_ind = class_ind\r\n\r\n    def reset_states(self):\r\n        """"""Reset the state of the metrics.""""""\r\n        self.precision.reset_states()\r\n        self.recall.reset_states()\r\n\r\n    def __call__(self, y_true, y_pred):\r\n        pr = self.precision(y_true, y_pred)\r\n        rec = self.recall(y_true, y_pred)\r\n        return 2 * truediv(pr * rec, pr + rec + K.epsilon())\r\n\r\n\r\ndef _slice_by_class(y_true, y_pred, class_ind):\r\n    \'\'\' Slice the batch predictions and labels with respect to a given class\r\n    that is encoded by a categorical or binary label.\r\n    #  Arguments:\r\n        y_true: Tensor, batch_wise labels.\r\n        y_pred: Tensor, batch_wise predictions.\r\n        class_ind: Integer, class index.\r\n    # Returns:\r\n        y_slice_true: Tensor, batch_wise label slice.\r\n        y_slice_pred: Tensor,  batch_wise predictions, slice.\r\n    \'\'\'\r\n    # Binary encoded\r\n    if y_pred.shape[-1] == 1:\r\n        y_slice_true, y_slice_pred = y_true, y_pred\r\n    # Categorical encoded\r\n    else:\r\n        y_slice_true, y_slice_pred = y_true[..., class_ind], y_pred[..., class_ind]\r\n    return y_slice_true, y_slice_pred\r\n```', '> They are pretty easy to write, I have internal implementations at work that only work for the TensorFlow backend.\r\n> \r\n> You can pretty easily translate any TensorFlow (streaming metric) to a StateFul Metric.\r\n\r\n@brge17 \r\nAre you suggesting wrapping the code as follows? In which case what should go in `reset_state`?\r\n```python\r\ndef __call__(self, y_true, y_pred):\r\n     metric_val, update_op = tf.contrib.metrics.some_streaming_op(y_true, y_pred)\r\n     self.add_update(update_op, inputs=[y_true, y_pred])\r\n     return metric_val\r\n```\r\n\r\nOr did you pull apart tensorflow\'s streaming metrics and place the relevant logic within a stateful subclass of `Layer` (i.e. ""translate"")?', 'Hi @ZackHodari . @brge17  was the github tag I used for open source contributions at my last job.\r\n\r\nI did the latter.\r\n\r\nAlso worth noting in the forthcoming TensorFlow 2.0.0 release, all of the streaming metrics will work with tf.keras:\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/ba7f78b9b0535f38e3e12dcb0b095fc7a6e7bca5#diff-09395f6eebca1f408ed2e91c63477511\r\n\r\nIf you have another metric in mind, I can help.', 'Glad to see that this issue has lead to the fruition of something useful. Thank you @briannemsick and everyone else here, for pursuing this. This is my first open source ""contribution"", if one may call it that. ', '> For the lazy amongst us, I’ll post the implementations iv\'e collected:\r\n> The implementation of recall and precision were made by @briannemsick and the f1 score was implemented by the \'keras_metrics\' library:\r\n> \r\n> ```python\r\n> from tensorflow.keras.layers import Layer\r\n> import tensorflow.keras.backend as K\r\n> from operator import truediv\r\n> \r\n> \r\n> class Recall(Layer):\r\n>     \'\'\'Compute recall over all batches.\r\n>     # Arguments\r\n>         name: String, name for the metric.\r\n>         class_ind: Integer, class index.\r\n>     \'\'\'\r\n>     def __init__(self, name=\'recall\', class_ind=1):\r\n>         super(Recall, self).__init__(name=name)\r\n>         self.true_positives = K.variable(value=0, dtype=\'float32\')\r\n>         self.total_positives = K.variable(value=0, dtype=\'float32\')\r\n>         self.class_ind = class_ind\r\n> \r\n>     def reset_states(self):\r\n>         K.set_value(self.true_positives, 0.0)\r\n>         K.set_value(self.total_positives, 0.0)\r\n> \r\n>     def __call__(self, y_true, y_pred):\r\n>         \'\'\'Update recall computation.\r\n>         # Arguments\r\n>             y_true: Tensor, batch_wise labels\r\n>             y_pred: Tensor, batch_wise predictions\r\n>         # Returns\r\n>             Overall recall for the epoch at the completion of the batch.\r\n>         \'\'\'\r\n>         # Batch\r\n>         y_true, y_pred = _slice_by_class(y_true, y_pred, self.class_ind)\r\n>         true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n>         total_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\r\n>         # Current\r\n>         current_true_positives = self.true_positives * 1\r\n>         current_total_positives = self.total_positives * 1\r\n>         # Updates\r\n>         updates = [K.update_add(self.true_positives, true_positives),\r\n>                    K.update_add(self.total_positives, total_positives)]\r\n>         self.add_update(updates, inputs=[y_true, y_pred])\r\n>         # Compute recall\r\n>         return (current_true_positives + true_positives) / \\\r\n>                (current_total_positives + total_positives + K.epsilon())\r\n> ```\r\n\r\nCould you give an example for how to use one of these metrics? Doing something like\r\n```\r\n    model.compile(loss=\'binary_crossentropy\',\r\n    \t      optimizer=tf.keras.optimizers.RMSprop(lr=args[""lr""], decay=args[""decay""]),\r\n    \t      metrics=[\'acc\', Precision(), Recall()])\r\n```\r\nunfortunately didn\'t work.']",[],[],0,0
246,keras,7393,closed,Error with Keras while running a Python script,"Hi, 
I am currently doing experiments on a dataset classifying text document using Embedding, Conv1D and Dense layers. 



Everything is ok, but while I am running the python script I obtain the following error related to native code in C/C++.



It's the first time that I got that error, I had problem before but due to incompatibility among different shapes, not because of the compile phase.

Can someone give me an hint on how to solve this problem?
Thanks",,"['Today I upgraded cudnn from version 5 to version 6 and theano from version 0.9.0-dev2 to version 0.9.0 and I\'m getting the same `mod.cu(77): error: identifier ""cudnnSetFilterNdDescriptor_v4"" is undefined` error message.\r\n\r\nAny ideas?\r\nThanks', 'Having the same issue `mod.cu(77): error: identifier ""cudnnSetFilterNdDescriptor_v4"" is undefined`. Coincidence that we\'re having the issue today and there are only like 3 Google results for this?', ""I'm currently hitting that issue too. As part of the fast.ai deep learning course there has been a lot of back and forth on the theano/cuDNN versions in this thread: http://forums.fast.ai/t/making-your-own-server/174/393\r\n\r\nI've not got it working yet, but am hopeful..."", 'I have jumped to this issue. And have just fixed it my downgrade to `cudnn` 5.1.5.\r\nPreviously, I was on latest `cudnn` on Arch Linux. Hope it helps someone', 'CamAnNguyen, how did you downgrade? In the package archive it starts only with version 6', '@snovik75  I installed through `anaconda`\r\n', ""@CamAnNguyen , what's your exact command to install cudnn 5.1.5 through anaconda ?"", '@huangyingw: you can check here: https://anaconda.org/search?q=cudnn . The official version from `anaconda` channel is now 6.0.21. For older version, you can choose any channel/owner. I used `conda install -c marta-sd cudnn`\r\n', '@CamAnNguyen thank you very much.', 'I am having the same issue. Hope it to be fixed soon. I have Ubuntu 16.04, Theano 0.9.0 and cuDNN 6021.', '@fazzolini  I have hitting the same problem and the same version of Ubuntu 16.04, Theano 0.9.0 and cuDNN 6021, have you solved this peoblem?']","['\r\nfrom __future__ import print_function\r\n\r\nimport time\r\nimport warnings\r\n\r\nimport numpy as np\r\nfrom keras.layers import Embedding, Dense, Dropout, GlobalAveragePooling1D, Conv1D, Conv2D, GlobalMaxPooling1D\r\nfrom keras.models import Sequential\r\nfrom keras.optimizers import RMSprop\r\nfrom keras.preprocessing import sequence\r\nfrom keras.preprocessing.text import Tokenizer\r\nfrom keras.utils import np_utils\r\nfrom sklearn import preprocessing\r\n\r\nwarnings.filterwarnings(""ignore"")\r\n\r\n\r\ndef load_data_from_file(filename):\r\n    file_to_read = open(filename, ""r"")\r\n    list_items = []\r\n    list_classes = []\r\n    lines = file_to_read.readlines()\r\n    for line in lines:\r\n        pattern = line.rsplit(\' \', 1)[0]\r\n        cls = line.rsplit(\' \', 1)[1]\r\n        list_items.append(pattern)\r\n        list_classes.append(cls.replace(""\\n"", """"))\r\n\r\n    seed = 11\r\n    np.random.seed(seed)\r\n    np.random.shuffle(list_items)\r\n    np.random.seed(seed)\r\n    np.random.shuffle(list_classes)\r\n\r\n    return list_items, list_classes\r\n\r\n\r\ndef convert_patterns_to_indices(count_vect, list_item_to_convert):\r\n    list_of_all_indices = []\r\n    for item in list_item_to_convert:\r\n        # for word in item.split("" ""):\r\n        transform = count_vect.transform(item.split("" ""))\r\n        if len(transform.indices) > 0:\r\n            list_of_all_indices.append(transform.indices)\r\n\r\n    return list_of_all_indices\r\n\r\n\r\ndef main():\r\n    start_time = time.time()\r\n\r\n    max_features = 2000\r\n    maxlen = 1000\r\n    batch_size = 64\r\n    embedding_dim = 500\r\n    epochs = 5\r\n\r\n    train_file = ""train-docs.csv""\r\n    test_file = ""test-docs.csv""\r\n    list_item_train, list_classes_train = load_data_from_file(train_file)\r\n    list_item_test, list_classes_test = load_data_from_file(test_file)\r\n\r\n    # count_vect = CountVectorizer()\r\n    # count_vect.fit(list_item_train)\r\n\r\n    tokenizer = Tokenizer(num_words=max_features, lower=True, split="" "")\r\n    tokenizer.fit_on_texts(list_item_train)\r\n    x_train = tokenizer.texts_to_sequences(list_item_train)\r\n\r\n    # tokenizer.fit_on_texts(list_item_test)\r\n    x_test = tokenizer.texts_to_sequences(list_item_test)\r\n\r\n    x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\r\n    x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\r\n\r\n    le = preprocessing.LabelEncoder()\r\n    le.fit(list_classes_train)\r\n    y_train = le.transform(list_classes_train)\r\n    y_test = le.transform(list_classes_test)\r\n    y_train = np_utils.to_categorical(y_train)\r\n    y_test = np_utils.to_categorical(y_test)\r\n\r\n    model = Sequential()\r\n    # layer = Embedding(max_features, output_dim=embedding_dim, input_length=maxlen)\r\n    model.add(Embedding(max_features, output_dim=embedding_dim, input_length=maxlen))\r\n    # we add a Convolution1D, which will learn filters\r\n    # word group filters of size filter_length:\r\n    model.add(Conv1D(filters=50, kernel_size=3, padding=\'valid\', activation=\'relu\', strides=1))\r\n    model.add(GlobalMaxPooling1D())\r\n    model.add(Dense(512, activation=\'relu\'))\r\n    model.add(Dropout(0.2))\r\n    model.add(Dense(512, activation=\'relu\'))\r\n    model.add(Dropout(0.2))\r\n    model.add(Dense(4, activation=\'sigmoid\'))\r\n\r\n    model.summary()\r\n\r\n    model.compile(loss=\'categorical_crossentropy\',\r\n                  optimizer=\'adam\',\r\n                  metrics=[\'accuracy\'])\r\n\r\n    print(\'Train...\')\r\n    model.fit(x_train, y_train, verbose=1, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test))\r\n    score, acc = model.evaluate(x_test, y_test)\r\n    print(\'Test score:\', score)\r\n    print(\'Test accuracy:\', acc)\r\n\r\n    elapsed_time = time.time() - start_time\r\n    print(""Elapsed time"", elapsed_time, ""seconds"")\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n\r\n', '\r\n/usr/bin/python2.7 /home/super/PycharmProjects/KERAS_tutorial/load_dataset.py\r\nUsing Theano backend.\r\nWARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\r\n https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\r\n\r\nUsing gpu device 0: GeForce GTX 1080 (CNMeM is disabled, cuDNN 6021)\r\n/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/__init__.py:631: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.1.\r\n  warnings.warn(warn)\r\nBuild model...\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nembedding_1 (Embedding)      (None, 1000, 500)         1000000   \r\n_________________________________________________________________\r\nconv1d_1 (Conv1D)            (None, 998, 50)           75050     \r\n_________________________________________________________________\r\nglobal_max_pooling1d_1 (Glob (None, 50)                0         \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 512)               26112     \r\n_________________________________________________________________\r\ndropout_1 (Dropout)          (None, 512)               0         \r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 512)               262656    \r\n_________________________________________________________________\r\ndropout_2 (Dropout)          (None, 512)               0         \r\n_________________________________________________________________\r\ndense_3 (Dense)              (None, 4)                 2052      \r\n=================================================================\r\nTotal params: 1,365,870\r\nTrainable params: 1,365,870\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nTrain...\r\n1 #include <Python.h>\r\n2 #include <iostream>\r\n3 #include ""theano_mod_helper.h""\r\n4 #include ""cuda_ndarray.cuh""\r\n5 #include <math.h>\r\n6 #include <numpy/arrayobject.h>\r\n7 #include <numpy/arrayscalars.h>\r\n8 #include ""cudnn.h""\r\n9 #include ""cudnn_helper.h""\r\n10 //////////////////////\r\n11 ////  Support Code\r\n12 //////////////////////\r\n13 \r\n14 void _capsule_destructor(PyObject *o) {\r\n15     void *d = PyCapsule_GetContext(o);\r\n16     void *p = PyCapsule_GetPointer(o, NULL);\r\n17     void (*f)(void *) = (void (*)(void *))d;\r\n18     if (f != NULL) f(p);\r\n19 }\r\n20 \r\n21 \r\n22 static cudnnHandle_t _handle = NULL;\r\n23 \r\n24 \r\n25 static int\r\n26 c_set_tensorNd(CudaNdarray *var, cudnnTensorDescriptor_t desc) {\r\n27 \r\n28   int dim = CudaNdarray_NDIM(var);\r\n29   int *strides = (int *)malloc(dim * sizeof(int));\r\n30   int default_str = 1;\r\n31   int return_value = 0;\r\n32   \r\n33   if (strides != NULL) {\r\n34     for (int i = dim-1; i >= 0; i--)\r\n35     {\r\n36       if (CudaNdarray_HOST_STRIDES(var)[i])\r\n37         strides[i] = CudaNdarray_HOST_STRIDES(var)[i];\r\n38       else\r\n39         strides[i] = default_str;\r\n40       default_str *= CudaNdarray_HOST_DIMS(var)[i];\r\n41     }\r\n42     \r\n43     cudnnStatus_t err = cudnnSetTensorNdDescriptor(desc, CUDNN_DATA_FLOAT, dim,\r\n44                                                    CudaNdarray_HOST_DIMS(var),\r\n45                                                    strides);\r\n46   \t \t\t\t\t\t\t\t\t\t\r\n47     \r\n48     if (err != CUDNN_STATUS_SUCCESS) {\r\n49       PyErr_Format(PyExc_RuntimeError,\r\n50 \t\t  ""Could not set tensorNd descriptor: %s""\r\n51 \t\t  ""dim=%d"",\r\n52 \t\t  cudnnGetErrorString(err), dim);\r\n53 \t\t  \r\n54 \t  return_value = -1;\r\n55     }\r\n56   } else {\r\n57     PyErr_Format(PyExc_MemoryError,\r\n58 \t\t""Could not allocate memory for strides array of size %d."",\r\n59 \t\tdim);\r\n60 \t\t\r\n61     return_value = -1;  \r\n62   }\r\n63     \r\n64   free(strides);\r\n65   return return_value;\r\n66 }\r\n67 \r\n68 \r\n69 static int\r\n70 c_set_filterNd(CudaNdarray *var, cudnnFilterDescriptor_t desc) {\r\n71   if (!CudaNdarray_is_c_contiguous(var)) {\r\n72     PyErr_SetString(PyExc_ValueError,\r\n73 \t\t    ""Only contiguous filters (kernels) are supported."");\r\n74     return -1;\r\n75   }\r\n76   int dim = CudaNdarray_NDIM(var);\r\n77   cudnnStatus_t err = cudnnSetFilterNdDescriptor_v4(desc,\r\n78                                                     CUDNN_DATA_FLOAT,\r\n79                                                     CUDNN_TENSOR_NCHW,\r\n80                                                     dim,\r\n81                                                     CudaNdarray_HOST_DIMS(var));\r\n82   if (err != CUDNN_STATUS_SUCCESS) {\r\n83     PyErr_Format(PyExc_RuntimeError,\r\n84 \t\t ""Could not set filter descriptor: %s.""\r\n85 \t\t "" dims= %d"",\r\n86 \t\t cudnnGetErrorString(err), dim);\r\n87     return -1;\r\n88   }\r\n89   return 0;\r\n90 }\r\n91 \r\n92 \r\n93 \r\n94     namespace {\r\n95     struct __struct_compiled_op_ea4e203b6529466794536f8a1bfa77ae {\r\n96         PyObject* __ERROR;\r\n97 \r\n98         PyObject* storage_V3;\r\n99 PyObject* storage_V5;\r\n100 PyObject* storage_V7;\r\n101 PyObject* storage_V9;\r\n102 PyObject* storage_V11;\r\n103 PyObject* storage_V13;\r\n104 PyObject* storage_V1;\r\n105         \r\n106 #define DTYPE_INPUT_0 npy_float32\r\n107 #define TYPENUM_INPUT_0 11\r\n108 #define ITEMSIZE_INPUT_0 4\r\n109 #define DTYPE_INPUT_1 npy_float32\r\n110 #define TYPENUM_INPUT_1 11\r\n111 #define ITEMSIZE_INPUT_1 4\r\n112 #define DTYPE_INPUT_2 npy_float32\r\n113 #define TYPENUM_INPUT_2 11\r\n114 #define ITEMSIZE_INPUT_2 4\r\n115 #define DTYPE_INPUT_4 npy_float32\r\n116 #define TYPENUM_INPUT_4 11\r\n117 #define ITEMSIZE_INPUT_4 4\r\n118 #define DTYPE_INPUT_5 npy_float32\r\n119 #define TYPENUM_INPUT_5 11\r\n120 #define ITEMSIZE_INPUT_5 4\r\n121 #define DTYPE_OUTPUT_0 npy_float32\r\n122 #define TYPENUM_OUTPUT_0 11\r\n123 #define ITEMSIZE_OUTPUT_0 4\r\n124 #define APPLY_SPECIFIC(str) str##_node_ea4e203b6529466794536f8a1bfa77ae_0\r\n125 #define CONV_ALGO CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM\r\n126 #define CHOOSE_ALGO 0\r\n127 #define CHOOSE_ALGO_ONCE 0\r\n128 #define CHOOSE_ALGO_TIME 0\r\n129 #define CONV_INPLACE 1\r\n130 \r\n131 cudnnTensorDescriptor_t APPLY_SPECIFIC(input);\r\n132 cudnnTensorDescriptor_t APPLY_SPECIFIC(output);\r\n133 cudnnFilterDescriptor_t APPLY_SPECIFIC(kerns);\r\n134 \r\n135 /* Keep track, from one execution to another, of the dimension of the data\r\n136 and the algorithms, if any, that were selected according to these dimensions\r\n137 and according to the amount of memory available at that time.\r\n138 \r\n139 Note : Implementation selection for backward convolution only exists starting\r\n140 at V3.\r\n141 */\r\n142 int APPLY_SPECIFIC(previous_input_shape)[5];\r\n143 int APPLY_SPECIFIC(previous_kerns_shape)[5];\r\n144 int APPLY_SPECIFIC(previous_output_shape)[5];\r\n145 bool APPLY_SPECIFIC(previous_algo_set);\r\n146 cudnnConvolutionFwdAlgo_t APPLY_SPECIFIC(previous_algo);\r\n147 cudnnConvolutionBwdFilterAlgo_t APPLY_SPECIFIC(previous_bwd_f_algo);\r\n148 cudnnConvolutionBwdDataAlgo_t APPLY_SPECIFIC(previous_bwd_d_algo);\r\n149 \r\n150 \r\n151 \r\n152 int\r\n153 APPLY_SPECIFIC(conv_fwd)(CudaNdarray *input, CudaNdarray *kerns,\r\n154                          CudaNdarray *om, cudnnConvolutionDescriptor_t desc,\r\n155                          float alpha, float beta, CudaNdarray **output) {\r\n156 \r\n157   cudnnStatus_t err = CUDNN_STATUS_SUCCESS;\r\n158   if (CudaNdarray_HOST_DIMS(input)[1] != CudaNdarray_HOST_DIMS(kerns)[1]) {\r\n159     PyErr_SetString(PyExc_ValueError,\r\n160                     ""GpuDnnConv images and kernel must have the same stack size\\n"");\r\n161     return 1;\r\n162   }\r\n163 \r\n164   int nb_dim = CudaNdarray_NDIM(input);\r\n165 \r\n166 #ifdef CONV_INPLACE\r\n167   Py_XDECREF(*output);\r\n168   *output = om;\r\n169   Py_INCREF(*output);\r\n170 #else\r\n171   if (CudaNdarray_prep_output(output, nb_dim, CudaNdarray_HOST_DIMS(om)) != 0)\r\n172     return 1;\r\n173   if (beta != 0.0 && CudaNdarray_CopyFromCudaNdarray(*output, om))\r\n174     return 1;\r\n175 #endif\r\n176 \r\n177   if (CudaNdarray_DIMS(input)[0] == 0 || CudaNdarray_DIMS(kerns)[0] == 0 || CudaNdarray_DIMS(kerns)[1] == 0) {\r\n178     cudaError_t err2 = cudaMemset((*output)->devdata, 0,\r\n179                                   CudaNdarray_SIZE(*output) * sizeof(real));\r\n180     if (err2 != cudaSuccess) {\r\n181       PyErr_Format(PyExc_RuntimeError,\r\n182                    ""GpuDnnConv could not fill the output with zeros: %s"",\r\n183                    cudaGetErrorString(err2));\r\n184       return 1;\r\n185     }\r\n186     return 0;\r\n187   }\r\n188 \r\n189   if (c_set_tensorNd(input, APPLY_SPECIFIC(input)) == -1)\r\n190     return 1;\r\n191   if (c_set_filterNd(kerns, APPLY_SPECIFIC(kerns)) == -1)\r\n192     return 1;\r\n193   if (c_set_tensorNd(*output, APPLY_SPECIFIC(output)) == -1)\r\n194     return 1;\r\n195 \r\n196   {\r\n197     size_t worksize;\r\n198     void *workspace;\r\n199     cudnnConvolutionFwdAlgo_t chosen_algo;\r\n200 \r\n201 \r\n202     if (CHOOSE_ALGO)\r\n203     {\r\n204 \r\n205       // A new convolution implementation should be selected, based either on\r\n206       // timing or heuristics if in one of the two following cases :\r\n207       // - The implementation should only be chosen during the first execution\r\n208       //   of an apply node and this is the first execution of the apply node.\r\n209       // - The implementation should be chosen as often as necessary and the\r\n210       //   shapes of the inputs differ from the last time an implementation\r\n211       //   was chosen.\r\n212       bool reuse_previous_algo;\r\n213       if (CHOOSE_ALGO_ONCE)\r\n214       {\r\n215         // Only choose a new implementation of none has been chosen before.\r\n216         reuse_previous_algo = APPLY_SPECIFIC(previous_algo_set);\r\n217       }\r\n218       else\r\n219       {\r\n220         // Reuse the previous implementation if the inputs and the kernels\r\n221         // have the same shapes as they had when the previous implementation\r\n222         // was selected\r\n223         bool same_shapes = true;\r\n224         for (int i = 0; (i < nb_dim) && same_shapes; i++)\r\n225         {\r\n226           same_shapes &= (CudaNdarray_HOST_DIMS(input)[i] ==\r\n227                           APPLY_SPECIFIC(previous_input_shape)[i]);\r\n228           same_shapes &= (CudaNdarray_HOST_DIMS(kerns)[i] ==\r\n229                           APPLY_SPECIFIC(previous_kerns_shape)[i]);\r\n230         }\r\n231         reuse_previous_algo = same_shapes;\r\n232       }\r\n233 \r\n234       // If the previously choosen implementation can\'t be reused, select a\r\n235       // new one based on the shapes of the current inputs\r\n236       if (!reuse_previous_algo)\r\n237       {\r\n238 \r\n239         // Obtain a convolution algorithm appropriate for the input and kernel\r\n240         // shapes. Either by choosing one according to heuristics or by making\r\n241         // cuDNN time every implementation and choose the best one.\r\n242         if (CHOOSE_ALGO_TIME)\r\n243         {\r\n244           // Time the different implementations to choose the best one\r\n245           int requestedCount = 1;\r\n246           int count;\r\n247           cudnnConvolutionFwdAlgoPerf_t choosen_algo_perf;\r\n248           err = cudnnFindConvolutionForwardAlgorithm(_handle,\r\n249                                                      APPLY_SPECIFIC(input),\r\n250                                                      APPLY_SPECIFIC(kerns),\r\n251                                                      desc,\r\n252                                                      APPLY_SPECIFIC(output),\r\n253                                                      requestedCount,\r\n254                                                      &count,\r\n255                                                      &choosen_algo_perf);\r\n256           if (err != CUDNN_STATUS_SUCCESS) {\r\n257             PyErr_Format(PyExc_RuntimeError,\r\n258                          ""GpuDnnConv: error selecting convolution algo: %s"",\r\n259                          cudnnGetErrorString(err));\r\n260             return 1;\r\n261           }\r\n262 \r\n263           chosen_algo = choosen_algo_perf.algo;\r\n264         }\r\n265         else\r\n266         {\r\n267           // The implementation should be chosen using heuristics based on the\r\n268           // input shapes and the amount of memory available.\r\n269 \r\n270           // Get the amount of available memory\r\n271           size_t free = 0, total = 0;\r\n272           cudaError_t err2 = cudaMemGetInfo(&free, &total);\r\n273           if (err2 != cudaSuccess){\r\n274             cudaGetLastError();\r\n275             fprintf(stderr,\r\n276                     ""Error when trying to find the memory information""\r\n277                     "" on the GPU: %s\\n"", cudaGetErrorString(err2));\r\n278             return 1;\r\n279           }\r\n280 \r\n281           // Use heuristics to choose the implementation\r\n282           err = cudnnGetConvolutionForwardAlgorithm(_handle,\r\n283                                                     APPLY_SPECIFIC(input),\r\n284                                                     APPLY_SPECIFIC(kerns),\r\n285                                                     desc,\r\n286                                                     APPLY_SPECIFIC(output),\r\n287                                                     CUDNN_CONVOLUTION_FWD_SPECIFY_WORKSPACE_LIMIT,\r\n288                                                     free,\r\n289                                                     &chosen_algo);\r\n290 \r\n291           if (err != CUDNN_STATUS_SUCCESS) {\r\n292             PyErr_Format(PyExc_RuntimeError,\r\n293                          ""GpuDnnConv: error selecting convolution algo: %s"",\r\n294                          cudnnGetErrorString(err));\r\n295             return 1;\r\n296           }\r\n297         }\r\n298 \r\n299         // Store the shapes of the inputs and kernels as well as the chosen\r\n300         // algorithm for future use.\r\n301         APPLY_SPECIFIC(previous_algo) = chosen_algo;\r\n302         APPLY_SPECIFIC(previous_algo_set) = true;\r\n303         for (int i = 0; i < nb_dim; i++)\r\n304         {\r\n305             APPLY_SPECIFIC(previous_input_shape)[i] =\r\n306                                             CudaNdarray_HOST_DIMS(input)[i];\r\n307             APPLY_SPECIFIC(previous_kerns_shape)[i] =\r\n308                                             CudaNdarray_HOST_DIMS(kerns)[i];\r\n309         }\r\n310       }\r\n311       else\r\n312       {\r\n313           // Reuse the previously chosen convolution implementation\r\n314           chosen_algo = APPLY_SPECIFIC(previous_algo);\r\n315       }\r\n316     }\r\n317     else\r\n318     {\r\n319       chosen_algo = CONV_ALGO;\r\n320     }\r\n321 \r\n322     if (0){\r\n323       char * a;\r\n324       switch(chosen_algo){\r\n325       case CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM:\r\n326 \ta = ""implicit gemm (0)"";\r\n327 \tbreak;\r\n328       case CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM:\r\n329 \ta = ""precomp gemm (1)"";\r\n330 \tbreak;\r\n331       case CUDNN_CONVOLUTION_FWD_ALGO_GEMM:\r\n332 \ta = ""gemm (2)"";\r\n333 \tbreak;\r\n334       case CUDNN_CONVOLUTION_FWD_ALGO_DIRECT:\r\n335 \ta = ""direct (3)"";\r\n336 \tbreak;\r\n337       case CUDNN_CONVOLUTION_FWD_ALGO_FFT:\r\n338 \ta = ""fft (4)"";\r\n339 \tbreak;\r\n340       case CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING:\r\n341 \ta = ""fft tiling (5)"";\r\n342 \tbreak;\r\n343 #if CUDNN_VERSION > 5000\r\n344       case CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD:\r\n345 \ta = ""winograd (6)"";\r\n346 \tbreak;\r\n347 #endif\r\n348       }\r\n349       printf(""GpuDNNConv: algo %s\\n"", a);\r\n350     }\r\n351 \r\n352     // The FFT implementation (only in V3 and onward) does not support strides,\r\n353     // 1x1 filters or inputs with a spatial dimension larger than 1024.\r\n354     // The tiled-FFT implementation (only in V4 onward) does not support\r\n355     // strides.\r\n356     // If the chosen implementation is FFT or tiled-FFT, validate that it can\r\n357     // be used on the current data and default on a safe implementation if it\r\n358     // can\'t.\r\n359     // Following code is 2d-specific, but it is fine as FFT and tiled-FFT are\r\n360     // defined only for 2d-filters\r\n361     if ((chosen_algo == CUDNN_CONVOLUTION_FWD_ALGO_FFT ||\r\n362          chosen_algo == CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING) && nb_dim == 4)\r\n363     {\r\n364 \r\n365       // Extract the properties of the convolution descriptor\r\n366       int nd;\r\n367       int pad[2];\r\n368       int stride[2];\r\n369       int upscale[2];\r\n370       cudnnConvolutionMode_t mode;\r\n371       cudnnDataType_t data_type;\r\n372       err = cudnnGetConvolutionNdDescriptor(desc, 2, &nd, pad, stride,\r\n373                                             upscale, &mode, &data_type);\r\n374 \r\n375       if (err != CUDNN_STATUS_SUCCESS) {\r\n376         PyErr_Format(PyExc_RuntimeError,\r\n377                      ""GpuDnnConv: error getting convolution properties: %s"",\r\n378                      cudnnGetErrorString(err));\r\n379         return 1;\r\n380       }\r\n381 \r\n382       // Extract the spatial size of the filters\r\n383       int filter_h = CudaNdarray_HOST_DIMS(kerns)[2];\r\n384       int filter_w = CudaNdarray_HOST_DIMS(kerns)[3];\r\n385 \r\n386       // Extract the spatial size of the input\r\n387       int input_h = CudaNdarray_HOST_DIMS(input)[2];\r\n388       int input_w = CudaNdarray_HOST_DIMS(input)[3];\r\n389 \r\n390       // Ensure that the selected implementation supports the requested\r\n391       // convolution. Fall back to a safe implementation otherwise.\r\n392       if (chosen_algo == CUDNN_CONVOLUTION_FWD_ALGO_FFT)\r\n393       {\r\n394         if (stride[0] != 1 || stride[1] != 1 || input_h > 1024 ||\r\n395             input_w > 1024 || (filter_h == 1 && filter_w == 1))\r\n396         {\r\n397           chosen_algo = CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM;\r\n398         }\r\n399       }\r\n400       else\r\n401       {\r\n402         // chosen_algo == CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING\r\n403         if (stride[0] != 1 || stride[1] != 1)\r\n404         {\r\n405           chosen_algo = CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM;\r\n406         }\r\n407       }\r\n408     }\r\n409 \r\n410     err = cudnnGetConvolutionForwardWorkspaceSize(_handle,\r\n411                                                   APPLY_SPECIFIC(input),\r\n412                                                   APPLY_SPECIFIC(kerns),\r\n413                                                   desc,\r\n414                                                   APPLY_SPECIFIC(output),\r\n415                                                   chosen_algo,\r\n416                                                   &worksize);\r\n417     if (err == CUDNN_STATUS_NOT_SUPPORTED) {\r\n418       // Fallback to none algo if not supported\r\n419       // TODO: Print a warning\r\n420       chosen_algo = CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM;\r\n421 \r\n422       err = cudnnGetConvolutionForwardWorkspaceSize(_handle,\r\n423                                                     APPLY_SPECIFIC(input),\r\n424                                                     APPLY_SPECIFIC(kerns),\r\n425                                                     desc,\r\n426                                                     APPLY_SPECIFIC(output),\r\n427                                                     chosen_algo,\r\n428                                                     &worksize);\r\n429     }\r\n430     if (err != CUDNN_STATUS_SUCCESS) {\r\n431       PyErr_Format(PyExc_RuntimeError,\r\n432                    ""GpuDnnConv: error getting worksize: %s"",\r\n433                    cudnnGetErrorString(err));\r\n434       return 1;\r\n435     }\r\n436     workspace = get_work_mem(worksize);\r\n437     if (workspace == NULL && worksize != 0)\r\n438       return 1;\r\n439 \r\n440     err = cudnnConvolutionForward(\r\n441       _handle,\r\n442       (void *)&alpha,\r\n443       APPLY_SPECIFIC(input), CudaNdarray_DEV_DATA(input),\r\n444       APPLY_SPECIFIC(kerns), CudaNdarray_DEV_DATA(kerns),\r\n445       desc,\r\n446       chosen_algo,\r\n447       workspace, worksize,\r\n448       (void *)&beta,\r\n449       APPLY_SPECIFIC(output), CudaNdarray_DEV_DATA(*output));\r\n450   }\r\n451   if (err != CUDNN_STATUS_SUCCESS) {\r\n452     PyErr_Format(PyExc_RuntimeError, ""GpuDnnConv: error doing operation: %s"",\r\n453 \t\t cudnnGetErrorString(err));\r\n454     return 1;\r\n455   }\r\n456   return 0;\r\n457 }\r\n458 \r\n459 #undef DTYPE_INPUT_0\r\n460 #undef TYPENUM_INPUT_0\r\n461 #undef ITEMSIZE_INPUT_0\r\n462 #undef DTYPE_INPUT_1\r\n463 #undef TYPENUM_INPUT_1\r\n464 #undef ITEMSIZE_INPUT_1\r\n465 #undef DTYPE_INPUT_2\r\n466 #undef TYPENUM_INPUT_2\r\n467 #undef ITEMSIZE_INPUT_2\r\n468 #undef DTYPE_INPUT_4\r\n469 #undef TYPENUM_INPUT_4\r\n470 #undef ITEMSIZE_INPUT_4\r\n471 #undef DTYPE_INPUT_5\r\n472 #undef TYPENUM_INPUT_5\r\n473 #undef ITEMSIZE_INPUT_5\r\n474 #undef DTYPE_OUTPUT_0\r\n475 #undef TYPENUM_OUTPUT_0\r\n476 #undef ITEMSIZE_OUTPUT_0\r\n477 #undef APPLY_SPECIFIC\r\n478 #undef CONV_ALGO\r\n479 #undef CHOOSE_ALGO\r\n480 #undef CHOOSE_ALGO_ONCE\r\n481 #undef CHOOSE_ALGO_TIME\r\n482 #undef CONV_INPLACE\r\n483 \r\n484         __struct_compiled_op_ea4e203b6529466794536f8a1bfa77ae() {\r\n485             // This is only somewhat safe because we:\r\n486             //  1) Are not a virtual class\r\n487             //  2) Do not use any virtual classes in the members\r\n488             //  3) Deal with mostly POD and pointers\r\n489 \r\n490             // If this changes, we would have to revise this, but for\r\n491             // now I am tired of chasing segfaults because\r\n492             // initialization code had an error and some pointer has\r\n493             // a junk value.\r\n494             memset(this, 0, sizeof(*this));\r\n495         }\r\n496         ~__struct_compiled_op_ea4e203b6529466794536f8a1bfa77ae(void) {\r\n497             cleanup();\r\n498         }\r\n499 \r\n500         int init(PyObject* __ERROR, PyObject* storage_V3, PyObject* storage_V5, PyObject* storage_V7, PyObject* storage_V9, PyObject* storage_V11, PyObject* storage_V13, PyObject* storage_V1) {\r\n501             Py_XINCREF(storage_V3);\r\n502 Py_XINCREF(storage_V5);\r\n503 Py_XINCREF(storage_V7);\r\n504 Py_XINCREF(storage_V9);\r\n505 Py_XINCREF(storage_V11);\r\n506 Py_XINCREF(storage_V13);\r\n507 Py_XINCREF(storage_V1);\r\n508             this->storage_V3 = storage_V3;\r\n509 this->storage_V5 = storage_V5;\r\n510 this->storage_V7 = storage_V7;\r\n511 this->storage_V9 = storage_V9;\r\n512 this->storage_V11 = storage_V11;\r\n513 this->storage_V13 = storage_V13;\r\n514 this->storage_V1 = storage_V1;\r\n515             \r\n516 \r\n517 \r\n518 \r\n519 \r\n520 \r\n521 \r\n522 \r\n523 \r\n524 #define DTYPE_INPUT_0 npy_float32\r\n525 #define TYPENUM_INPUT_0 11\r\n526 #define ITEMSIZE_INPUT_0 4\r\n527 #define DTYPE_INPUT_1 npy_float32\r\n528 #define TYPENUM_INPUT_1 11\r\n529 #define ITEMSIZE_INPUT_1 4\r\n530 #define DTYPE_INPUT_2 npy_float32\r\n531 #define TYPENUM_INPUT_2 11\r\n532 #define ITEMSIZE_INPUT_2 4\r\n533 #define DTYPE_INPUT_4 npy_float32\r\n534 #define TYPENUM_INPUT_4 11\r\n535 #define ITEMSIZE_INPUT_4 4\r\n536 #define DTYPE_INPUT_5 npy_float32\r\n537 #define TYPENUM_INPUT_5 11\r\n538 #define ITEMSIZE_INPUT_5 4\r\n539 #define DTYPE_OUTPUT_0 npy_float32\r\n540 #define TYPENUM_OUTPUT_0 11\r\n541 #define ITEMSIZE_OUTPUT_0 4\r\n542 #define APPLY_SPECIFIC(str) str##_node_ea4e203b6529466794536f8a1bfa77ae_0\r\n543 #define CONV_ALGO CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM\r\n544 #define CHOOSE_ALGO 0\r\n545 #define CHOOSE_ALGO_ONCE 0\r\n546 #define CHOOSE_ALGO_TIME 0\r\n547 #define CONV_INPLACE 1\r\n548 #define FAIL { \\\r\n549         if (!PyErr_Occurred()) { \\\r\n550             PyErr_SetString(PyExc_RuntimeError, \\\r\n551                 ""Unexpected error in an Op\'s C code. "" \\\r\n552                 ""No Python exception was set.""); \\\r\n553             } \\\r\n554         return 15; \\\r\n555 }\r\n556 \r\n557 \r\n558 cudnnStatus_t APPLY_SPECIFIC(err);\r\n559 APPLY_SPECIFIC(input) = NULL;\r\n560 APPLY_SPECIFIC(output) = NULL;\r\n561 APPLY_SPECIFIC(kerns) = NULL;\r\n562 if ((APPLY_SPECIFIC(err) = cudnnCreateTensorDescriptor(&APPLY_SPECIFIC(input))) != CUDNN_STATUS_SUCCESS) {\r\n563   PyErr_Format(PyExc_MemoryError, ""could not allocate tensor descriptor ""\r\n564 \t       ""(inp): %s"", cudnnGetErrorString(APPLY_SPECIFIC(err)));\r\n565   FAIL;\r\n566 }\r\n567 if ((APPLY_SPECIFIC(err) = cudnnCreateTensorDescriptor(&APPLY_SPECIFIC(output))) != CUDNN_STATUS_SUCCESS) {\r\n568   PyErr_Format(PyExc_MemoryError, ""could not allocate tensor descriptor ""\r\n569                ""(out): %s"", cudnnGetErrorString(APPLY_SPECIFIC(err)));\r\n570   FAIL;\r\n571 }\r\n572 if ((APPLY_SPECIFIC(err) = cudnnCreateFilterDescriptor(&APPLY_SPECIFIC(kerns))) != CUDNN_STATUS_SUCCESS) {\r\n573   PyErr_Format(PyExc_MemoryError, ""could not allocate filter descriptor: %s"",\r\n574 \t       cudnnGetErrorString(APPLY_SPECIFIC(err)));\r\n575   FAIL;\r\n576 }\r\n577 \r\n578 for (int i = 0; i < 5; i++)\r\n579 {\r\n580   APPLY_SPECIFIC(previous_input_shape)[i] = 0;\r\n581   APPLY_SPECIFIC(previous_kerns_shape)[i] = 0;\r\n582   APPLY_SPECIFIC(previous_output_shape)[i] = 0;\r\n583 }\r\n584 \r\n585 APPLY_SPECIFIC(previous_algo_set) = false;\r\n586 \r\n587 // Select default implementations for the case where the convolution\r\n588 // implementations should be selected based on the size of the data.\r\n589 APPLY_SPECIFIC(previous_algo) = CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM;\r\n590 APPLY_SPECIFIC(previous_bwd_f_algo) = CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0;\r\n591 APPLY_SPECIFIC(previous_bwd_d_algo) = CUDNN_CONVOLUTION_BWD_DATA_ALGO_0;\r\n592 \r\n593 \r\n594 #undef FAIL\r\n595 #undef DTYPE_INPUT_0\r\n596 #undef TYPENUM_INPUT_0\r\n597 #undef ITEMSIZE_INPUT_0\r\n598 #undef DTYPE_INPUT_1\r\n599 #undef TYPENUM_INPUT_1\r\n600 #undef ITEMSIZE_INPUT_1\r\n601 #undef DTYPE_INPUT_2\r\n602 #undef TYPENUM_INPUT_2\r\n\r\n603 #undef ITEMSIZE_INPUT_2\r\n604 #undef DTYPE_INPUT_4\r\n605 #undef TYPENUM_INPUT_4\r\n606 #undef ITEMSIZE_INPUT_4\r\n607 #undef DTYPE_INPUT_5\r\n608 #undef TYPENUM_INPUT_5\r\n609 #undef ITEMSIZE_INPUT_5\r\n610 #undef DTYPE_OUTPUT_0\r\n611 #undef TYPENUM_OUTPUT_0\r\n612 #undef ITEMSIZE_OUTPUT_0\r\n613 #undef APPLY_SPECIFIC\r\n614 #undef CONV_ALGO\r\n615 #undef CHOOSE_ALGO\r\n[\'nvcc\', \'-shared\', \'-O3\', \'-Xlinker\', \'-rpath,/usr/local/cuda-8.0/lib64\', \'-arch=sm_61\', \'-m64\', \'-Xcompiler\', \'-fno-math-errno,-Wno-unused-label,-Wno-unused-variable,-Wno-write-strings,-DCUDA_NDARRAY_CUH=c72d035fdf91890f3b36710688069b2e,-DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION,-fPIC,-fvisibility=hidden\', \'-Xlinker\', \'-rpath,/home/super/.theano/compiledir_Linux-4.4--generic-x86_64-with-LinuxMint-18.1-serena-x86_64-2.7.12-64/cuda_ndarray\', \'-I/home/super/.theano/compiledir_Linux-4.4--generic-x86_64-with-LinuxMint-18.1-serena-x86_64-2.7.12-64/cuda_ndarray\', \'-I/usr/local/cuda-8.0/include\', \'-I/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda\', \'-I/usr/local/lib/python2.7/dist-packages/numpy/core/include\', \'-I/usr/include/python2.7\', \'-I/usr/local/lib/python2.7/dist-packages/theano/gof\', \'-L/home/super/.theano/compiledir_Linux-4.4--generic-x86_64-with-LinuxMint-18.1-serena-x86_64-2.7.12-64/cuda_ndarray\', \'-L/usr/lib\', \'-o\', \'/home/super/.theano/compiledir_Linux-4.4--generic-x86_64-with-LinuxMint-18.1-serena-x86_64-2.7.12-64/tmpaFU_ee/ea4e203b6529466794536f8a1bfa77ae.so\', \'mod.cu\', \'-lcudart\', \'-lcublas\', \'-lcuda_ndarray\', \'-lcudnn\', \'-lpython2.7\']\r\n616 #undef CHOOSE_ALGO_ONCE\r\n617 #undef CHOOSE_ALGO_TIME\r\n618 #undef CONV_INPLACE\r\n619             this->__ERROR = __ERROR;\r\n620             return 0;\r\n621         }\r\n622         void cleanup(void) {\r\n623             __label_1:\r\n624 \r\n625 double __DUMMY_1;\r\n626 __label_3:\r\n627 \r\n628 double __DUMMY_3;\r\n629 __label_5:\r\n630 \r\n631 double __DUMMY_5;\r\n632 __label_7:\r\n633 \r\n634 double __DUMMY_7;\r\n635 __label_9:\r\n636 \r\n637 double __DUMMY_9;\r\n638 __label_11:\r\n639 \r\n640 double __DUMMY_11;\r\n641 __label_13:\r\n642 \r\n643 double __DUMMY_13;\r\n644 __label_16:\r\n645 \r\n646 #define DTYPE_INPUT_0 npy_float32\r\n647 #define TYPENUM_INPUT_0 11\r\n648 #define ITEMSIZE_INPUT_0 4\r\n649 #define DTYPE_INPUT_1 npy_float32\r\n650 #define TYPENUM_INPUT_1 11\r\n651 #define ITEMSIZE_INPUT_1 4\r\n652 #define DTYPE_INPUT_2 npy_float32\r\n653 #define TYPENUM_INPUT_2 11\r\n654 #define ITEMSIZE_INPUT_2 4\r\n655 #define DTYPE_INPUT_4 npy_float32\r\n656 #define TYPENUM_INPUT_4 11\r\n657 #define ITEMSIZE_INPUT_4 4\r\n658 #define DTYPE_INPUT_5 npy_float32\r\n659 #define TYPENUM_INPUT_5 11\r\n660 #define ITEMSIZE_INPUT_5 4\r\n661 #define DTYPE_OUTPUT_0 npy_float32\r\n662 #define TYPENUM_OUTPUT_0 11\r\n663 #define ITEMSIZE_OUTPUT_0 4\r\n664 #define APPLY_SPECIFIC(str) str##_node_ea4e203b6529466794536f8a1bfa77ae_0\r\n665 #define CONV_ALGO CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM\r\n666 #define CHOOSE_ALGO 0\r\n667 #define CHOOSE_ALGO_ONCE 0\r\n668 #define CHOOSE_ALGO_TIME 0\r\n669 #define CONV_INPLACE 1\r\n670 \r\n671 \r\n672 if (APPLY_SPECIFIC(input) != NULL)\r\n673   cudnnDestroyTensorDescriptor(APPLY_SPECIFIC(input));\r\n674 if (APPLY_SPECIFIC(output) != NULL)\r\n675   cudnnDestroyTensorDescriptor(APPLY_SPECIFIC(output));\r\n676 if (APPLY_SPECIFIC(kerns) != NULL)\r\n677   cudnnDestroyFilterDescriptor(APPLY_SPECIFIC(kerns));\r\n678 \r\n679 #undef DTYPE_INPUT_0\r\n680 #undef TYPENUM_INPUT_0\r\n681 #undef ITEMSIZE_INPUT_0\r\n682 #undef DTYPE_INPUT_1\r\n683 #undef TYPENUM_INPUT_1\r\n684 #undef ITEMSIZE_INPUT_1\r\n685 #undef DTYPE_INPUT_2\r\n686 #undef TYPENUM_INPUT_2\r\n687 #undef ITEMSIZE_INPUT_2\r\n688 #undef DTYPE_INPUT_4\r\n689 #undef TYPENUM_INPUT_4\r\n690 #undef ITEMSIZE_INPUT_4\r\n691 #undef DTYPE_INPUT_5\r\n692 #undef TYPENUM_INPUT_5\r\n693 #undef ITEMSIZE_INPUT_5\r\n694 #undef DTYPE_OUTPUT_0\r\n695 #undef TYPENUM_OUTPUT_0\r\n696 #undef ITEMSIZE_OUTPUT_0\r\n697 #undef APPLY_SPECIFIC\r\n698 #undef CONV_ALGO\r\n699 #undef CHOOSE_ALGO\r\n700 #undef CHOOSE_ALGO_ONCE\r\n701 #undef CHOOSE_ALGO_TIME\r\n702 #undef CONV_INPLACE\r\n703 double __DUMMY_16;\r\n704 \r\n705             Py_XDECREF(this->storage_V3);\r\n706 Py_XDECREF(this->storage_V5);\r\n707 Py_XDECREF(this->storage_V7);\r\n708 Py_XDECREF(this->storage_V9);\r\n709 Py_XDECREF(this->storage_V11);\r\n710 Py_XDECREF(this->storage_V13);\r\n711 Py_XDECREF(this->storage_V1);\r\n712         }\r\n713         int run(void) {\r\n714             int __failure = 0;\r\n715             \r\n716     PyObject* py_V1;\r\n717      CudaNdarray * V1;\r\n718     PyObject* py_V3;\r\n719      CudaNdarray * V3;\r\n720     PyObject* py_V5;\r\n721      CudaNdarray * V5;\r\n722     PyObject* py_V7;\r\n723      CudaNdarray * V7;\r\n724     PyObject* py_V9;\r\n725     \r\n726         cudnnConvolutionDescriptor_t V9;\r\n727         \r\n728     PyObject* py_V11;\r\n729     \r\n730                 typedef npy_float32 dtype_V11;\r\n731             \r\n732         npy_float32 V11;\r\n733         \r\n734     PyObject* py_V13;\r\n735     \r\n736                 typedef npy_float32 dtype_V13;\r\n737             \r\n738         npy_float32 V13;\r\n739         \r\n740 {\r\n741 \r\n742     py_V1 = PyList_GET_ITEM(storage_V1, 0);\r\n743     {Py_XINCREF(py_V1);}\r\n744     \r\n745         if (py_V1 == Py_None)\r\n746         {\r\n747             V1 = NULL;\r\n748         }\r\n749         else\r\n750         {\r\n751             \r\n752         assert(py_V1->ob_refcnt >= 2); // There should be at least one ref from the container object,\r\n753         // and one ref from the local scope.\r\n754 \r\n755         if (CudaNdarray_Check(py_V1))\r\n756         {\r\n757             //fprintf(stderr, ""c_extract CNDA object w refcnt %p %i\\n"", py_V1, (py_V1->ob_refcnt));\r\n758             V1 = (CudaNdarray*)py_V1;\r\n759             //std::cerr << ""c_extract "" << V1 << \'\\n\';\r\n760         \r\n761 \r\n762                 if (V1->nd != 4)\r\n763                 {\r\n764                     PyErr_Format(PyExc_RuntimeError,\r\n765                                  ""c_extract: Some CudaNdarray has rank %i, it was supposed to have rank 4"",\r\n766                                  V1->nd);\r\n767                     V1 = NULL;\r\n768                     {\r\n769         __failure = 2;\r\n770         if (!PyErr_Occurred()) {\r\n771             PyErr_SetString(PyExc_RuntimeError,\r\n772                 ""Unexpected error in an Op\'s C code. ""\r\n773                 ""No Python exception was set."");\r\n774             }\r\n775         goto __label_2;};\r\n776                 }\r\n777                 //std::cerr << ""c_extract "" << V1 << "" nd check passed\\n"";\r\n778             \r\n779 \r\n780                 assert(V1);\r\n781                 Py_INCREF(py_V1);\r\n782             }\r\n783             else if (py_V1 == Py_None)\r\n784             {\r\n785                 PyErr_SetString(PyExc_TypeError,\r\n786                                 ""expected a CudaNdarray, not None"");\r\n787                 V1 = NULL;\r\n788                 {\r\n789         __failure = 2;\r\n790         if (!PyErr_Occurred()) {\r\n791             PyErr_SetString(PyExc_RuntimeError,\r\n792                 ""Unexpected error in an Op\'s C code. ""\r\n793                 ""No Python exception was set."");\r\n794             }\r\n795         goto __label_2;};\r\n796             }\r\n797             else\r\n798             {\r\n799                 //fprintf(stderr, ""FAILING c_extract CNDA object w refcnt %p %i\\n"", py_V1, (py_V1->ob_refcnt));\r\n800                 PyErr_SetString(PyExc_TypeError, ""Argument not a CudaNdarray"");\r\n801                 V1 = NULL;\r\n802                 {\r\n803         __failure = 2;\r\n804         if (!PyErr_Occurred()) {\r\n805             PyErr_SetString(PyExc_RuntimeError,\r\n806                 ""Unexpected error in an Op\'s C code. ""\r\n807                 ""No Python exception was set."");\r\n808             }\r\n809         goto __label_2;};\r\n810             }\r\n811             //std::cerr << ""c_extract done "" << V1 << \'\\n\';\r\n812             \r\n813 \r\n814         }\r\n815         \r\n816 {\r\n817 \r\n818     py_V3 = PyList_GET_ITEM(storage_V3, 0);\r\n819     {Py_XINCREF(py_V3);}\r\n820     \r\n821         assert(py_V3->ob_refcnt >= 2); // There should be at least one ref from the container object,\r\n822         // and one ref from the local scope.\r\n823 \r\n824         if (CudaNdarray_Check(py_V3))\r\n825         {\r\n826             //fprintf(stderr, ""c_extract CNDA object w refcnt %p %i\\n"", py_V3, (py_V3->ob_refcnt));\r\n827             V3 = (CudaNdarray*)py_V3;\r\n828             //std::cerr << ""c_extract "" << V3 << \'\\n\';\r\n829         \r\n830 \r\n831                 if (V3->nd != 4)\r\n832                 {\r\n833                     PyErr_Format(PyExc_RuntimeError,\r\n834                                  ""c_extract: Some CudaNdarray has rank %i, it was supposed to have rank 4"",\r\n835                                  V3->nd);\r\n836                     V3 = NULL;\r\n837                     {\r\n838         __failure = 4;\r\n839         if (!PyErr_Occurred()) {\r\n840             PyErr_SetString(PyExc_RuntimeError,\r\n841                 ""Unexpected error in an Op\'s C code. ""\r\n842                 ""No Python exception was set."");\r\n843             }\r\n844         goto __label_4;};\r\n845                 }\r\n846                 //std::cerr << ""c_extract "" << V3 << "" nd check passed\\n"";\r\n847             \r\n848 \r\n849                 assert(V3);\r\n850                 Py_INCREF(py_V3);\r\n851             }\r\n852             else if (py_V3 == Py_None)\r\n853             {\r\n854                 PyErr_SetString(PyExc_TypeError,\r\n855                                 ""expected a CudaNdarray, not None"");\r\n856                 V3 = NULL;\r\n857                 {\r\n858         __failure = 4;\r\n859         if (!PyErr_Occurred()) {\r\n860             PyErr_SetString(PyExc_RuntimeError,\r\n861                 ""Unexpected error in an Op\'s C code. ""\r\n862                 ""No Python exception was set."");\r\n863             }\r\n864         goto __label_4;};\r\n865             }\r\n866             else\r\n867             {\r\n868                 //fprintf(stderr, ""FAILING c_extract CNDA object w refcnt %p %i\\n"", py_V3, (py_V3->ob_refcnt));\r\n869                 PyErr_SetString(PyExc_TypeError, ""Argument not a CudaNdarray"");\r\n870                 V3 = NULL;\r\n871                 {\r\n872         __failure = 4;\r\n873         if (!PyErr_Occurred()) {\r\n874             PyErr_SetString(PyExc_RuntimeError,\r\n875                 ""Unexpected error in an Op\'s C code. ""\r\n876                 ""No Python exception was set."");\r\n877             }\r\n878         goto __label_4;};\r\n879             }\r\n880             //std::cerr << ""c_extract done "" << V3 << \'\\n\';\r\n881             \r\n882 \r\n883 {\r\n884 \r\n885     py_V5 = PyList_GET_ITEM(storage_V5, 0);\r\n886     {Py_XINCREF(py_V5);}\r\n887     \r\n888         assert(py_V5->ob_refcnt >= 2); // There should be at least one ref from the container object,\r\n889         // and one ref from the local scope.\r\n890 \r\n891         if (CudaNdarray_Check(py_V5))\r\n892         {\r\n893             //fprintf(stderr, ""c_extract CNDA object w refcnt %p %i\\n"", py_V5, (py_V5->ob_refcnt));\r\n894             V5 = (CudaNdarray*)py_V5;\r\n895             //std::cerr << ""c_extract "" << V5 << \'\\n\';\r\n896         \r\n897 \r\n898                 if (V5->nd != 4)\r\n899                 {\r\n900                     PyErr_Format(PyExc_RuntimeError,\r\n901                                  ""c_extract: Some CudaNdarray has rank %i, it was supposed to have rank 4"",\r\n902                                  V5->nd);\r\n903                     V5 = NULL;\r\n904                     {\r\n905         __failure = 6;\r\n906         if (!PyErr_Occurred()) {\r\n907             PyErr_SetString(PyExc_RuntimeError,\r\n908                 ""Unexpected error in an Op\'s C code. ""\r\n909                 ""No Python exception was set."");\r\n910             }\r\n911         goto __label_6;};\r\n912                 }\r\n913                 //std::cerr << ""c_extract "" << V5 << "" nd check passed\\n"";\r\n914             \r\n915 \r\n916                 assert(V5);\r\n917                 Py_INCREF(py_V5);\r\n918             }\r\n919             else if (py_V5 == Py_None)\r\n920             {\r\n921                 PyErr_SetString(PyExc_TypeError,\r\n922                                 ""expected a CudaNdarray, not None"");\r\n923                 V5 = NULL;\r\n924                 {\r\n925         __failure = 6;\r\n926         if (!PyErr_Occurred()) {\r\n927             PyErr_SetString(PyExc_RuntimeError,\r\n928                 ""Unexpected error in an Op\'s C code. ""\r\n929                 ""No Python exception was set."");\r\n930             }\r\n931         goto __label_6;};\r\n932             }\r\n933             else\r\n934             {\r\n935                 //fprintf(stderr, ""FAILING c_extract CNDA object w refcnt %p %i\\n"", py_V5, (py_V5->ob_refcnt));\r\n936                 PyErr_SetString(PyExc_TypeError, ""Argument not a CudaNdarray"");\r\n937                 V5 = NULL;\r\n938                 {\r\n939         __failure = 6;\r\n940         if (!PyErr_Occurred()) {\r\n941             PyErr_SetString(PyExc_RuntimeError,\r\n942                 ""Unexpected error in an Op\'s C code. ""\r\n943                 ""No Python exception was set."");\r\n944             }\r\n945         goto __label_6;};\r\n946             }\r\n947             //std::cerr << ""c_extract done "" << V5 << \'\\n\';\r\n948             \r\n949 \r\n950 {\r\n951 \r\n952     py_V7 = PyList_GET_ITEM(storage_V7, 0);\r\n953     {Py_XINCREF(py_V7);}\r\n954     \r\n955         assert(py_V7->ob_refcnt >= 2); // There should be at least one ref from the container object,\r\n956         // and one ref from the local scope.\r\n957 \r\n958         if (CudaNdarray_Check(py_V7))\r\n959         {\r\n960             //fprintf(stderr, ""c_extract CNDA object w refcnt %p %i\\n"", py_V7, (py_V7->ob_refcnt));\r\n961             V7 = (CudaNdarray*)py_V7;\r\n962             //std::cerr << ""c_extract "" << V7 << \'\\n\';\r\n963         \r\n964 \r\n965                 if (V7->nd != 4)\r\n966                 {\r\n967                     PyErr_Format(PyExc_RuntimeError,\r\n968                                  ""c_extract: Some CudaNdarray has rank %i, it was supposed to have rank 4"",\r\n969                                  V7->nd);\r\n970                     V7 = NULL;\r\n971                     {\r\n972         __failure = 8;\r\n973         if (!PyErr_Occurred()) {\r\n974             PyErr_SetString(PyExc_RuntimeError,\r\n975                 ""Unexpected error in an Op\'s C code. ""\r\n976                 ""No Python exception was set."");\r\n977             }\r\n978         goto __label_8;};\r\n979                 }\r\n980                 //std::cerr << ""c_extract "" << V7 << "" nd check passed\\n"";\r\n981             \r\n982 \r\n983                 assert(V7);\r\n984                 Py_INCREF(py_V7);\r\n985             }\r\n986             else if (py_V7 == Py_None)\r\n987             {\r\n988                 PyErr_SetString(PyExc_TypeError,\r\n989                                 ""expected a CudaNdarray, not None"");\r\n990                 V7 = NULL;\r\n991                 {\r\n992         __failure = 8;\r\n993         if (!PyErr_Occurred()) {\r\n994             PyErr_SetString(PyExc_RuntimeError,\r\n995                 ""Unexpected error in an Op\'s C code. ""\r\n996                 ""No Python exception was set."");\r\n997             }\r\n998         goto __label_8;};\r\n999             }\r\n1000             else\r\n1001             {\r\n1002                 //fprintf(stderr, ""FAILING c_extract CNDA object w refcnt %p %i\\n"", py_V7, (py_V7->ob_refcnt));\r\n1003                 PyErr_SetString(PyExc_TypeError, ""Argument not a CudaNdarray"");\r\n1004                 V7 = NULL;\r\n1005                 {\r\n1006         __failure = 8;\r\n1007         if (!PyErr_Occurred()) {\r\n1008             PyErr_SetString(PyExc_RuntimeError,\r\n1009                 ""Unexpected error in an Op\'s C code. ""\r\n1010                 ""No Python exception was set."");\r\n1011             }\r\n1012         goto __label_8;};\r\n1013             }\r\n1014             //std::cerr << ""c_extract done "" << V7 << \'\\n\';\r\n1015             \r\n1016 \r\n1017 {\r\n1018 \r\n1019     py_V9 = PyList_GET_ITEM(storage_V9, 0);\r\n1020     {Py_XINCREF(py_V9);}\r\n1021     \r\n1022   V9 = (cudnnConvolutionDescriptor_t)PyCapsule_GetPointer(py_V9, NULL);\r\n1023   if (V9 == NULL) {\r\n1024         __failure = 10;\r\n1025         if (!PyErr_Occurred()) {\r\n1026             PyErr_SetString(PyExc_RuntimeError,\r\n1027                 ""Unexpected error in an Op\'s C code. ""\r\n1028                 ""No Python exception was set."");\r\n1029             }\r\n1030         goto __label_10;}\r\n1031         \r\n1032 {\r\n1033 \r\n1034     py_V11 = PyList_GET_ITEM(storage_V11, 0);\r\n1035     {Py_XINCREF(py_V11);}\r\n1036     \r\n1037             if (!PyObject_TypeCheck(py_V11, &PyFloat32ArrType_Type))\r\n1038             {\r\n1039                 PyErr_Format(PyExc_ValueError,\r\n1040                     ""Scalar check failed (npy_float32)"");\r\n1041                 {\r\n1042         __failure = 12;\r\n1043         if (!PyErr_Occurred()) {\r\n1044             PyErr_SetString(PyExc_RuntimeError,\r\n1045                 ""Unexpected error in an Op\'s C code. ""\r\n1046                 ""No Python exception was set."");\r\n1047             }\r\n1048         goto __label_12;}\r\n1049             }\r\n1050             \r\n1051         PyArray_ScalarAsCtype(py_V11, &V11);\r\n1052         \r\n1053 {\r\n1054 \r\n1055     py_V13 = PyList_GET_ITEM(storage_V13, 0);\r\n1056     {Py_XINCREF(py_V13);}\r\n1057     \r\n1058             if (!PyObject_TypeCheck(py_V13, &PyFloat32ArrType_Type))\r\n1059             {\r\n1060                 PyErr_Format(PyExc_ValueError,\r\n1061                     ""Scalar check failed (npy_float32)"");\r\n1062                 {\r\n1063         __failure = 14;\r\n1064         if (!PyErr_Occurred()) {\r\n1065             PyErr_SetString(PyExc_RuntimeError,\r\n1066                 ""Unexpected error in an Op\'s C code. ""\r\n1067                 ""No Python exception was set."");\r\n1068             }\r\n1069         goto __label_14;}\r\n1070             }\r\n1071             \r\n1072         PyArray_ScalarAsCtype(py_V13, &V13);\r\n1073         \r\n1074 {\r\n1075 // Op class GpuDnnConv\r\n1076 \r\n1077                 #define APPLY_SPECIFIC(str) str##_node_ea4e203b6529466794536f8a1bfa77ae_0\r\n1078 #define CONV_ALGO CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM\r\n1079 #define CHOOSE_ALGO 0\r\n1080 #define CHOOSE_ALGO_ONCE 0\r\n1081 #define CHOOSE_ALGO_TIME 0\r\n1082 #define CONV_INPLACE 1\r\n1083                 {\r\n1084                   if (APPLY_SPECIFIC(conv_fwd)(V3, V5, V7, V9, V11, V13, &V1) != 0) {\r\n1085                     {\r\n1086         __failure = 15;\r\n1087         if (!PyErr_Occurred()) {\r\n1088             PyErr_SetString(PyExc_RuntimeError,\r\n1089                 ""Unexpected error in an Op\'s C code. ""\r\n1090                 ""No Python exception was set."");\r\n1091             }\r\n1092         goto __label_15;}\r\n1093                   }\r\n1094                 }\r\n1095                 #undef APPLY_SPECIFIC\r\n1096 #undef CONV_ALGO\r\n1097 #undef CHOOSE_ALGO\r\n1098 #undef CHOOSE_ALGO_ONCE\r\n1099 #undef CHOOSE_ALGO_TIME\r\n1100 #undef CONV_INPLACE\r\n1101                 __label_15:\r\n1102 \r\n1103 double __DUMMY_15;\r\n1104 \r\n1105 }\r\n1106 __label_14:\r\n1107 \r\n1108     {Py_XDECREF(py_V13);}\r\n1109     \r\n1110 double __DUMMY_14;\r\n1111 \r\n1112 }\r\n1113 __label_12:\r\n1114 \r\n1115     {Py_XDECREF(py_V11);}\r\n1116     \r\n1117 double __DUMMY_12;\r\n1118 \r\n1119 }\r\n1120 __label_10:\r\n1121 \r\n1122     {Py_XDECREF(py_V9);}\r\n1123     \r\n1124 double __DUMMY_10;\r\n1125 \r\n1126 }\r\n1127 __label_8:\r\n1128 \r\n1129         //std::cerr << ""cleanup "" << py_V7 << "" "" << V7 << ""\\n"";\r\n1130         //fprintf(stderr, ""c_cleanup CNDA py_object w refcnt %p %i\\n"", py_V7, (py_V7->ob_refcnt));\r\n1131         if (V7)\r\n1132         {\r\n1133             //fprintf(stderr, ""c_cleanup CNDA cn_object w refcnt %p %i\\n"", V7, (V7->ob_refcnt));\r\n1134             Py_XDECREF(V7);\r\n1135         }\r\n1136         //std::cerr << ""cleanup done"" << py_V7 << ""\\n"";\r\n1137         \r\n1138     {Py_XDECREF(py_V7);}\r\n1139     \r\n1140 double __DUMMY_8;\r\n1141 \r\n1142 }\r\n1143 __label_6:\r\n1144 \r\n1145         //std::cerr << ""cleanup "" << py_V5 << "" "" << V5 << ""\\n"";\r\n1146         //fprintf(stderr, ""c_cleanup CNDA py_object w refcnt %p %i\\n"", py_V5, (py_V5->ob_refcnt));\r\n1147         if (V5)\r\n1148         {\r\n1149             //fprintf(stderr, ""c_cleanup CNDA cn_object w refcnt %p %i\\n"", V5, (V5->ob_refcnt));\r\n1150             Py_XDECREF(V5);\r\n1151         }\r\n1152         //std::cerr << ""cleanup done"" << py_V5 << ""\\n"";\r\n1153         \r\n1154     {Py_XDECREF(py_V5);}\r\n1155     \r\n1156 double __DUMMY_6;\r\n1157 \r\n1158 }\r\n1159 __label_4:\r\n1160 \r\n1161         //std::cerr << ""cleanup "" << py_V3 << "" "" << V3 << ""\\n"";\r\n1162         //fprintf(stderr, ""c_cleanup CNDA py_object w refcnt %p %i\\n"", py_V3, (py_V3->ob_refcnt));\r\n1163         if (V3)\r\n1164         {\r\n1165             //fprintf(stderr, ""c_cleanup CNDA cn_object w refcnt %p %i\\n"", V3, (V3->ob_refcnt));\r\n1166             Py_XDECREF(V3);\r\n1167         }\r\n1168         //std::cerr << ""cleanup done"" << py_V3 << ""\\n"";\r\n1169         \r\n1170     {Py_XDECREF(py_V3);}\r\n1171     \r\n1172 double __DUMMY_4;\r\n1173 \r\n1174 }\r\n1175 __label_2:\r\n1176 \r\n1177     if (!__failure) {\r\n1178       \r\n1179         //std::cerr << ""sync\\n"";\r\n1180         if (NULL == V1) {\r\n1181             // failure: sync None to storage\r\n1182             Py_XDECREF(py_V1);\r\n1183             py_V1 = Py_None;\r\n1184             Py_INCREF(py_V1);\r\n1185         }\r\n1186         else\r\n1187         {\r\n1188             if (py_V1 != (PyObject*)V1)\r\n1189             {\r\n1190                 Py_XDECREF(py_V1);\r\n1191                 py_V1 = (PyObject*)V1;\r\n1192                 Py_INCREF(py_V1);\r\n1193             }\r\n1194             assert(py_V1->ob_refcnt);\r\n1195         }\r\n1196         \r\n1197       PyObject* old = PyList_GET_ITEM(storage_V1, 0);\r\n1198       {Py_XINCREF(py_V1);}\r\n1199       PyList_SET_ITEM(storage_V1, 0, py_V1);\r\n1200       {Py_XDECREF(old);}\r\n1201     }\r\n1202     \r\n1203         //std::cerr << ""cleanup "" << py_V1 << "" "" << V1 << ""\\n"";\r\n1204         //fprintf(stderr, ""c_cleanup CNDA py_object w refcnt %p %i\\n"", py_V1, (py_V1->ob_refcnt));\r\n1205         if (V1)\r\n1206         {\r\n1207             //fprintf(stderr, ""c_cleanup CNDA cn_object w refcnt %p %i\\n"", V1, (V1->ob_refcnt));\r\n1208             Py_XDECREF(V1);\r\n1209         }\r\n1210         //std::cerr << ""cleanup done"" << py_V1 << ""\\n"";\r\n1211         \r\n1212     {Py_XDECREF(py_V1);}\r\n1213     \r\n1214 double __DUMMY_2;\r\n1215 \r\n1216 }\r\n1217 \r\n1218             \r\n1219         if (__failure) {\r\n1220             // When there is a failure, this code puts the exception\r\n1221             // in __ERROR.\r\n1222             PyObject* err_type = NULL;\r\n1223             PyObject* err_msg = NULL;\r\n1224             PyObject* err_traceback = NULL;\r\n1225             PyErr_Fetch(&err_type, &err_msg, &err_traceback);\r\n1226             if (!err_type) {err_type = Py_None;Py_INCREF(Py_None);}\r\n1227             if (!err_msg) {err_msg = Py_None; Py_INCREF(Py_None);}\r\n1228             if (!err_traceback) {err_traceback = Py_None; Py_INCREF(Py_None);}\r\n1229             PyObject* old_err_type = PyList_GET_ITEM(__ERROR, 0);\r\n1230             PyObject* old_err_msg = PyList_GET_ITEM(__ERROR, 1);\r\n1231             PyObject* old_err_traceback = PyList_GET_ITEM(__ERROR, 2);\r\n1232             PyList_SET_ITEM(__ERROR, 0, err_type);\r\n1233             PyList_SET_ITEM(__ERROR, 1, err_msg);\r\n1234             PyList_SET_ITEM(__ERROR, 2, err_traceback);\r\n1235             {Py_XDECREF(old_err_type);}\r\n1236             {Py_XDECREF(old_err_msg);}\r\n1237             {Py_XDECREF(old_err_traceback);}\r\n1238         }\r\n1239         // The failure code is returned to index what code block failed.\r\n1240         return __failure;\r\n1241         \r\n1242         }\r\n1243     };\r\n1244     }\r\n1245     \r\n1246 \r\n1247         static int __struct_compiled_op_ea4e203b6529466794536f8a1bfa77ae_executor(__struct_compiled_op_ea4e203b6529466794536f8a1bfa77ae* self) {\r\n1248             return self->run();\r\n1249         }\r\n1250 \r\n1251         static void __struct_compiled_op_ea4e203b6529466794536f8a1bfa77ae_destructor(void* executor, void* self) {\r\n1252             delete ((__struct_compiled_op_ea4e203b6529466794536f8a1bfa77ae*)self);\r\n1253         }\r\n1254         \r\n1255 //////////////////////\r\n1256 ////  Functions\r\n1257 //////////////////////\r\n1258 static PyObject * instantiate(PyObject * self, PyObject *argtuple) {\r\n1259   assert(PyTuple_Check(argtuple));\r\n1260   if (8 != PyTuple_Size(argtuple)){ \r\n1261      PyErr_Format(PyExc_TypeError, ""Wrong number of arguments, expected 8, got %i"", (int)PyTuple_Size(argtuple));\r\n1262      return NULL;\r\n1263   }\r\n1264   __struct_compiled_op_ea4e203b6529466794536f8a1bfa77ae* struct_ptr = new __struct_compiled_op_ea4e203b6529466794536f8a1bfa77ae();\r\n1265   if (struct_ptr->init( PyTuple_GET_ITEM(argtuple, 0),PyTuple_GET_ITEM(argtuple, 1),PyTuple_GET_ITEM(argtuple, 2),PyTuple_GET_ITEM(argtuple, 3),PyTuple_GET_ITEM(argtuple, 4),PyTuple_GET_ITEM(argtuple, 5),PyTuple_GET_ITEM(argtuple, 6),PyTuple_GET_ITEM(argtuple, 7) ) != 0) {\r\n1266     delete struct_ptr;\r\n1267     return NULL;\r\n1268   }\r\n1269   PyObject* thunk = PyCObject_FromVoidPtrAndDesc((void*)(&__struct_compiled_op_ea4e203b6529466794536f8a1bfa77ae_executor), struct_ptr, __struct_compiled_op_ea4e203b6529466794536f8a1bfa77ae_destructor);\r\n1270   return thunk; }\r\n1271 \r\n1272 //////////////////////\r\n1273 ////  Module init\r\n1274 //////////////////////\r\n1275 static PyMethodDef MyMethods[] = {\r\n1276 \t{""instantiate"", instantiate, METH_VARARGS, ""undocumented""} ,\r\n1277 \t{NULL, NULL, 0, NULL}\r\n1278 };\r\n1279 PyMODINIT_FUNC initea4e203b6529466794536f8a1bfa77ae(void){\r\n1280    import_array();\r\n1281    \r\n1282 \r\n1283 {\r\n1284   cudnnStatus_t err;\r\n1285   if ((err = cudnnCreate(&_handle)) != CUDNN_STATUS_SUCCESS) {\r\n1286     PyErr_Format(PyExc_RuntimeError, ""could not create cuDNN handle: %s"",\r\n1287 \t\t cudnnGetErrorString(err));\r\n1288 #if PY_MAJOR_VERSION >= 3\r\n1289     return NULL;\r\n1290 #else\r\n1291     return;\r\n1292 #endif\r\n1293   }\r\n1294 }\r\n1295 \r\n1296    (void) Py_InitModule(""ea4e203b6529466794536f8a1bfa77ae"", MyMethods);\r\n1297 }\r\n1298 \r\n===============================\r\nmod.cu(77): error: identifier ""cudnnSetFilterNdDescriptor_v4"" is undefined\r\nmod.cu(326): warning: conversion from a string literal to ""char *"" is deprecated\r\nmod.cu(329): warning: conversion from a string literal to ""char *"" is deprecated\r\nmod.cu(332): warning: conversion from a string literal to ""char *"" is deprecated\r\nmod.cu(335): warning: conversion from a string literal to ""char *"" is deprecated\r\nmod.cu(338): warning: conversion from a string literal to ""char *"" is deprecated\r\nmod.cu(341): warning: conversion from a string literal to ""char *"" is deprecated\r\nmod.cu(345): warning: conversion from a string literal to ""char *"" is deprecated\r\n1 error detected in the compilation of ""/tmp/tmpxft_000032de_00000000-9_mod.cpp1.ii"".\r\nTraceback (most recent call last):\r\n  File ""/home/super/PycharmProjects/KERAS_tutorial/load_dataset.py"", line 141, in <module>\r\n    main()\r\n  File ""/home/super/PycharmProjects/KERAS_tutorial/load_dataset.py"", line 131, in main\r\n    model.fit(x_train, y_train, verbose=1, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test))\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 863, in fit\r\n    initial_epoch=initial_epoch)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 1379, in fit\r\n    self._make_test_function()\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 959, in _make_test_function\r\n    **self._function_kwargs)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/theano_backend.py"", line 1206, in function\r\n    return Function(inputs, outputs, updates=updates, **kwargs)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/theano_backend.py"", line 1192, in __init__\r\n    **kwargs)\r\n  File ""/usr/local/lib/python2.7/dist-packages/theano/compile/function.py"", line 326, in function\r\n    output_keys=output_keys)\r\n  File ""/usr/local/lib/python2.7/dist-packages/theano/compile/pfunc.py"", line 486, in pfunc\r\n    output_keys=output_keys)\r\n  File ""/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py"", line 1795, in orig_function\r\n    defaults)\r\n  File ""/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py"", line 1661, in create\r\n    input_storage=input_storage_lists, storage_map=storage_map)\r\n  File ""/usr/local/lib/python2.7/dist-packages/theano/gof/link.py"", line 699, in make_thunk\r\n    storage_map=storage_map)[:3]\r\n  File ""/usr/local/lib/python2.7/dist-packages/theano/gof/vm.py"", line 1047, in make_all\r\n    impl=impl))\r\n  File ""/usr/local/lib/python2.7/dist-packages/theano/gof/op.py"", line 935, in make_thunk\r\n    no_recycling)\r\n  File ""/usr/local/lib/python2.7/dist-packages/theano/gof/op.py"", line 839, in make_c_thunk\r\n    output_storage=node_output_storage)\r\n  File ""/usr/local/lib/python2.7/dist-packages/theano/gof/cc.py"", line 1190, in make_thunk\r\n    keep_lock=keep_lock)\r\n  File ""/usr/local/lib/python2.7/dist-packages/theano/gof/cc.py"", line 1131, in __compile__\r\n    keep_lock=keep_lock)\r\n  File ""/usr/local/lib/python2.7/dist-packages/theano/gof/cc.py"", line 1586, in cthunk_factory\r\n    key=key, lnk=self, keep_lock=keep_lock)\r\n  File ""/usr/local/lib/python2.7/dist-packages/theano/gof/cmodule.py"", line 1159, in module_from_key\r\n    module = lnk.compile_cmodule(location)\r\n  File ""/usr/local/lib/python2.7/dist-packages/theano/gof/cc.py"", line 1489, in compile_cmodule\r\n    preargs=preargs)\r\n  File ""/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/nvcc_compiler.py"", line 405, in compile_str\r\n    \'for cmd\', \' \'.join(cmd))\r\nException: (\'The following error happened while compiling the node\', GpuDnnConv{algo=\'small\', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty.0, GpuDnnConvDesc{border_mode=\'valid\', subsample=(1, 1), conv_mode=\'conv\', precision=\'float32\'}.0, Constant{1.0}, Constant{0.0}), \'\\n\', \'nvcc return status\', 2, \'for cmd\', \'nvcc -shared -O3 -Xlinker -rpath,/usr/local/cuda-8.0/lib64 -arch=sm_61 -m64 -Xcompiler -fno-math-errno,-Wno-unused-label,-Wno-unused-variable,-Wno-write-strings,-DCUDA_NDARRAY_CUH=c72d035fdf91890f3b36710688069b2e,-DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION,-fPIC,-fvisibility=hidden -Xlinker -rpath,/home/super/.theano/compiledir_Linux-4.4--generic-x86_64-with-LinuxMint-18.1-serena-x86_64-2.7.12-64/cuda_ndarray -I/home/super/.theano/compiledir_Linux-4.4--generic-x86_64-with-LinuxMint-18.1-serena-x86_64-2.7.12-64/cuda_ndarray -I/usr/local/cuda-8.0/include -I/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda -I/usr/local/lib/python2.7/dist-packages/numpy/core/include -I/usr/include/python2.7 -I/usr/local/lib/python2.7/dist-packages/theano/gof -L/home/super/.theano/compiledir_Linux-4.4--generic-x86_64-with-LinuxMint-18.1-serena-x86_64-2.7.12-64/cuda_ndarray -L/usr/lib -o /home/super/.theano/compiledir_Linux-4.4--generic-x86_64-with-LinuxMint-18.1-serena-x86_64-2.7.12-64/tmpaFU_ee/ea4e203b6529466794536f8a1bfa77ae.so mod.cu -lcudart -lcublas -lcuda_ndarray -lcudnn -lpython2.7\', ""[GpuDnnConv{algo=\'small\', inplace=True}(<CudaNdarrayType(float32, (False, False, False, True))>, <CudaNdarrayType(float32, (False, False, False, True))>, <CudaNdarrayType(float32, 4D)>, <CDataType{cudnnConvolutionDescriptor_t}>, Constant{1.0}, Constant{0.0})]"")\r\n\r\nProcess finished with exit code 1\r\n\r\n']",[],0,0
247,keras,4746,closed,How can i use keras optimizer for backprop-ing on my own loss functions,"I am working on guided backprop for activation maximization. Instead of implementing rmsprop, Adam etc., I want to reuse optimizers defined in keras.
",stale type:support,"['You should check out the optimizer API as defined in `keras/optimizers.py`.', 'I did. \r\nnormally i would compute grads as\r\ngrads_fn = K.gradients(loss_fn, input_tensor)[0]\r\nloss_grads_fn =K.function([input_tensor], [loss_fn, grads_fn])\r\n\r\nMy backprop would be:\r\nloss, grads = loss_grads_fn([numpy_array])\r\nnumpy_array -= grads * lr\r\n\r\nget_gradients (https://github.com/fchollet/keras/blob/master/keras/optimizers.py#L61) seems to be called by get_updates() in Adam. Do i just call get_updates() once to build the update function? I am not sure how to use that function either. Specifically, I am confused about parts that are building a function vs functions where i could pass my numpy array to compute updates.', 'This is the relevant portion: https://github.com/raghakot/keras-vis/blob/master/vis/optimizer.py#L163\r\nInstead of rolling my custom rmsprop. It would be nicer if I used keras optimizers. Would appreciate if you could look through that code and advise. It is a keras visualization library :)', ""You can use Keras optimizers outside of Keras if you really can't do whatever you're doing within Keras.\r\n\r\nYes, it is important to call get_updates() once and only once and hang on to the returned updates. For example, the Adam optimizer locally creates momentum variables in the get_updates() function. Calling get_updates() multiple times for the same set of parameters will cause chaos.\r\n\r\nIf you have some custom loss function and a list of shared variables:\r\n```\r\nupdates = opt.get_updates(params, constraints, loss)\r\nfun = K.function([input],[], updates=updates)\r\n```\r\n\r\nYou're better off doing backprop on the GPU instead of back-and-forth with numpy. Store your weights as GPU variables and update them with functions. When you need the weights in numpy, use get_value and set_value.\r\n\r\nCheers,\r\nBen"", 'Thanks. the `input` (model.input) has shape (?, channels, rows, cols). When i try to create the update function using:\r\n```python\r\nupdates = opt.get_updates([input], [], [loss_fn])\r\n```\r\nit complains about None. Any ideas on how to handle that?', 'Please always post a stack trace or something if you have specific issues.\r\n\r\nI put together a Gist showing how to use Keras optimizers. It should teach you the basic style of how everything goes together.\r\n\r\nhttps://gist.github.com/bstriner/e1e011652b297d13b3ac3f99fd11b2bc\r\n\r\nThe standard in Keras is that model parameters are variables that live on the GPU and inputs and targets are placeholders that get passed in for each batch. \r\n\r\nA training function is created with inputs: batch inputs, batch targets; and outputs: loss, accuracy, other metrics. The function also performs updates on the model parameters on the GPU each time it is executed.\r\n\r\nTo train, you just pass batch inputs and batch targets to the training function and print out the current loss.\r\n\r\nAt the end, if you want to get the trained parameters, use K.get_value.\r\n\r\n```\r\nfrom keras.optimizers import Adam\r\nfrom keras import backend as K\r\nfrom keras.datasets import mnist\r\nfrom keras.utils.np_utils import to_categorical\r\nfrom keras.metrics import categorical_accuracy\r\nimport numpy as np\r\n\r\n# inputs and targets are placeholders\r\nx = K.placeholder(name=""x"", shape=(None, 28*28))\r\nytrue = K.placeholder(name=""y"", shape=(None, 10))\r\n\r\n# model parameters are variables\r\nW = K.variable(np.random.random((28*28,10)).astype(np.float32))\r\nb = K.variable(np.random.random((10,)).astype(np.float32))\r\nparams = [W, b]\r\n\r\n# single layer model: softmax(xW+b) \r\nypred = K.softmax(K.dot(x,W)+b)\r\n\r\n# categorical cross entropy loss\r\nloss = K.mean(K.categorical_crossentropy(ytrue, ypred),axis=None)\r\n\r\n# categorical accuracy\r\naccuracy = categorical_accuracy(ytrue, ypred)\r\n\r\n# Train function\r\nopt = Adam()\r\nupdates = opt.get_updates(params, [], loss)\r\ntrain = K.function([x, ytrue],[loss, accuracy],updates=updates)\r\n\r\n# Train the network\r\n((xtrain, ytrain),(xtest, ytest)) = mnist.load_data()\r\nxtrain = xtrain.reshape((-1, 28*28)) # flatten input image\r\nytrain = to_categorical(ytrain, 10)\r\nfor epoch in range(500):\r\n\tloss, accuracy = train([xtrain, ytrain])\r\n\tprint(""Epoch: {}, Loss: {}, Accuracy: {}"".format(epoch, loss, accuracy))\r\n\r\n\r\n```\r\n', ""Thanks. The example and gist are awesome. You should perhaps add or reference it somewhere in keras docs/examples for others.\r\n\r\nHere is a minimal example of whats happening in my case. \r\n\r\n```python\r\nfrom keras import backend as K\r\nfrom keras.optimizers import Adam\r\n\r\nx = K.placeholder(shape=(None, 224, 224, 3))\r\nopt = Adam()\r\n\r\n# Some contrived example\r\nloss = K.square(x)\r\n\r\nupdates = opt.get_updates([x], [], [loss])\r\niterate = K.function([x], [], updates=updates)\r\n```\r\n\r\nThis will give me `TypeError: int() argument must be a string or a number, not 'NoneType'` because x has `None` for batch dimension.\r\n\r\nAlso, how do i added a placeholder on top of `model.input`? Basically, i am trying to add a proxy input placeholder on top of the pretrained keras model so that i can perform certain input transformations of the GPU before feeding it into the model.input. I tried:\r\n\r\n```python\r\nproxy = K.placeholder(shape=K.int_shape(model.input))\r\n# This was my futile attempt to connect to existing model graph\r\nproxy = model.input + K.variable(0.)\r\n```"", ""@bstriner I am new to Keras, in your example how I can modify it to get the model's parameters if I have a loaded network (e.g. VGG16) through `load_model()` ? Thanks"", '@mongoose54 kind of unrelated to the OP. If you have a model you can inspect `model.layers` `model.layers[2].kernel` etc. You can also just `model.weights` to get all the weights.\r\n\r\nThat will give you the tensor variable which gives you the variable name. You can get the actual value of the variable with `import keras.backend as K; value = K.get_value(my_variable)`.\r\n\r\nCheers', ""@bstriner Sorry for placing it here. \r\n\r\nHowever I have a question related to this topic:\r\n\r\nLet's say I have the losses explicitly defined in a numpy array: `losses = [0.23 0.432 2.23 ...]` . How can I backpropagate them to update the network's parameters? "", '@bstriner thx for such an example but i have a weird problem.\r\n\r\nthe only reasonable difference with your example is:\r\nupdates = self.opt.get_updates(model.trainable_weights, [], loss_out)\r\n\r\nmodel is actually learning, loss is going down, val accuracy increasing (actually up to 100 in some iterations), i can save and load the model etc.\r\n\r\nbut something wrong with LR not changing.\r\n\r\n(i have changed these values just to see the change more easily, but no luck)\r\n\r\nself.opt = SGD(lr=1.0, decay= 1e-3, momentum=0.5, nesterov=False)\r\n\r\nK.get_value(m.opt.lr) => outputting always 1.0 in each loop, it doesnt change.\r\n(each call to ""get_updates"" (train step, not test) should change it, however it doesnt)\r\n\r\nany ideas anyone?\r\n\r\nedit: just added opt.lr to outputs directly, still no change.\r\n\r\nedit2: adding ""self.lr = lr""\r\n""after"" the following statement in get_updates fixes this issue.\r\nif self.initial_decay > 0:\r\nlr = lr * (1. / (1. + self.decay * K.cast(self.iterations,K.dtype(self.decay))))\r\n\r\nedit3: since i use tf as backend, probably it works ok as it builds up a graph, but some dependencies might not work as expected since opt.lr is not updated correctly.\r\n\r\nIs this a bug? \r\n\r\nwhat do i miss here?\r\n', ""Hi @bstriner, small question for you. Suppose I add another output head to your nn above, then what would need further adjustment?\r\n\r\nIt's just that I have a very similar nn, but as soon as I add an extra head (output) to it, then I get the `An operation has `None` for gradient. Please make sure ...` error.\r\n\r\nEverything is working fine before adding the extra output.\r\n\r\nWorking code:\r\n```\r\nclass NN():\r\n...\r\ndef _build_nn(self):\r\n        inputs = Input(shape=(self.obs_size,))\r\n        x = Dense(units=self.hidden_units, activation='relu', use_bias=True, name='l2')(inputs)\r\n        x = Dense(units=self.hidden_units, activation='relu', use_bias=True, name='l3')(x)\r\n        actions_probs = Dense(units=self.n_actions, activation='softmax', use_bias=False, name='actions_probs')(x)\r\n        self.nn = Model(inputs=inputs, outputs=[**actions_probs**])\r\n\r\n    def _build_train(self):\r\n        **actions_probs** = self.nn.output\r\n        actions_1hot = K.placeholder(shape=(None, self.n_actions), name='actions_1hot')\r\n        actions_scales = K.placeholder(shape=(None,), name='actions_scales')\r\n        actions_probs = K.sum(actions_probs * actions_1hot, axis=1)\r\n        log_actions_probs = K.log(actions_probs)\r\n        policy_loss = -1 * actions_scales * log_actions_probs\r\n        policy_loss = K.mean(policy_loss)\r\n        entropy = K.mean(-(actions_probs * log_actions_probs))\r\n        entropy_loss = -ENTROPY_BETA * entropy\r\n        loss = policy_loss + entropy_loss\r\n        optim = SGD(lr=LR, decay=1e-6, momentum=0.9, nesterov=True)\r\n        updates = optim.get_updates(params=self.nn.trainable_weights, loss=loss)\r\n        self.custom_train = K.function(inputs=[self.nn.input, actions_1hot, actions_scales], outputs=[loss], updates=updates)\r\n```\r\n\r\nNot-working code:\r\n```\r\ndef _build_nn(self):\r\n        inputs = Input(shape=(self.obs_size,))\r\n        x = Dense(units=self.hidden_units, activation='relu', use_bias=True, name='l2')(inputs)\r\n        x = Dense(units=self.hidden_units, activation='relu', use_bias=True, name='l3')(x)\r\n        actions_probs = Dense(units=self.n_actions, activation='softmax', use_bias=False, name='actions_probs')(x)\r\n        **extra_head** = Dense(units=1, activation='linear', use_bias=False, name='extra_head')(x)\r\n        self.nn = Model(inputs=inputs, outputs=[**actions_probs, extra_head**])\r\n\r\n    def _build_train(self):\r\n        **actions_probs, _** = self.nn.output\r\n        actions_1hot = K.placeholder(shape=(None, self.n_actions), name='actions_1hot')\r\n        actions_scales = K.placeholder(shape=(None,), name='actions_scales')\r\n        actions_probs = K.sum(actions_probs * actions_1hot, axis=1)\r\n        log_actions_probs = K.log(actions_probs)\r\n        policy_loss = -1 * actions_scales * log_actions_probs\r\n        policy_loss = K.mean(policy_loss)\r\n        entropy = K.mean(-(actions_probs * log_actions_probs))\r\n        entropy_loss = -ENTROPY_BETA * entropy\r\n        loss = policy_loss + entropy_loss\r\n        optim = SGD(lr=LR, decay=1e-6, momentum=0.9, nesterov=True)\r\n        updates = optim.get_updates(params=self.nn.trainable_weights, loss=loss)\r\n        self.custom_train = K.function(inputs=[self.nn.input, actions_1hot, actions_scales], outputs=[loss], updates=updates)\r\n```\r\nAditional info: When python vs code debugging, I can see the contents of loss being (correctly?) constructed/passed in, but I can't see as well inside params... \r\nAny ideas?"", 'Good morning @bstriner , re-reading my own question, maybe I should feed the ""_"" inside the call to K.function, as now it needs 2 different _y_true\'s_, and `actions_1hot` is just 1 of the outputs... ']",[],[],0,0
248,keras,13016,closed,something wrong with U-NET,"<em>Please make sure that this is a Bug or a Feature Request and provide all applicable information asked by the template.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.</em>  

**System information**  
- Have I written custom code (as opposed to using example directory):  
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  
- TensorFlow backend (yes / no):  
- TensorFlow version:  
- Keras version:  
- Python version:  
- CUDA/cuDNN version:  
- GPU model and memory:  

You can obtain the TensorFlow version with:  
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""  
You can obtain the Keras version with:  
python -c 'import keras as k; print(k.__version__)'  

**Describe the current behavior**  

**Describe the expected behavior**  

**Code to reproduce the issue**  
Provide a reproducible test case that is the bare minimum necessary to generate the problem.  

**Other info / logs**  
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.  
",type:support,"['I\'m confused that when I wanna  train my model, which its shape of input is (256,256,4)，and the final predicted class is 16 and my program is wrong .\r\n```\r\n#!usr/bin/env python\r\n#-*- coding:utf-8 _*-\r\n#@author:mqray\r\n#@file: Unet.py\r\n#@time: 2019/6/23 21:06\r\nimport PIL\r\nfrom PIL import Image\r\nimport matplotlib.pyplot as plt\r\nfrom libtiff import TIFF\r\nfrom libtiff import TIFFfile, TIFFimage\r\nfrom scipy.misc import imresize\r\nimport numpy as np\r\nimport glob\r\nimport cv2\r\nimport os\r\nimport math\r\nimport skimage.io as io\r\nimport skimage.transform as trans\r\nfrom keras.models import *\r\nfrom keras.layers import *\r\nfrom keras.optimizers import *\r\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras import backend as K\r\nfrom iou import iou\r\n\r\n# %matplotlib inline\r\n\r\ndef UNet(shape=(256, 256, 4)):\r\n    # Left side of the U-Net\r\n    inputs = Input(shape)\r\n    #    in_shape = inputs.shape\r\n    #    print(in_shape)\r\n    conv1 = Conv2D(64, 3, activation=\'relu\', padding=\'same\', kernel_initializer=\'random_normal\')(inputs)\r\n    conv1 = Conv2D(64, 3, activation=\'relu\', padding=\'same\', kernel_initializer=\'random_normal\')(conv1)\r\n    conv1 = BatchNormalization()(conv1)\r\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\r\n    conv2 = Conv2D(128, 3, activation=\'relu\', padding=\'same\', kernel_initializer=\'random_normal\')(pool1)\r\n    conv2 = Conv2D(128, 3, activation=\'relu\', padding=\'same\', kernel_initializer=\'random_normal\')(conv2)\r\n    conv2 = BatchNormalization()(conv2)\r\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\r\n    conv3 = Conv2D(256, 3, activation=\'relu\', padding=\'same\', kernel_initializer=\'random_normal\')(pool2)\r\n    conv3 = Conv2D(256, 3, activation=\'relu\', padding=\'same\', kernel_initializer=\'random_normal\')(conv3)\r\n    conv3 = BatchNormalization()(conv3)\r\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\r\n    conv4 = Conv2D(512, 3, activation=\'relu\', padding=\'same\', kernel_initializer=\'random_normal\')(pool3)\r\n    conv4 = Conv2D(512, 3, activation=\'relu\', padding=\'same\', kernel_initializer=\'random_normal\')(conv4)\r\n    conv4 = BatchNormalization()(conv4)\r\n    drop4 = Dropout(0.5)(conv4)\r\n    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\r\n\r\n    # Bottom of the U-Net\r\n    conv5 = Conv2D(1024, 3, activation=\'relu\', padding=\'same\', kernel_initializer=\'random_normal\')(pool4)\r\n    conv5 = Conv2D(1024, 3, activation=\'relu\', padding=\'same\', kernel_initializer=\'random_normal\')(conv5)\r\n    conv5 = BatchNormalization()(conv5)\r\n    drop5 = Dropout(0.5)(conv5)\r\n\r\n    # Upsampling Starts, right side of the U-Net\r\n    up6 = Conv2D(512, 2, activation=\'relu\', padding=\'same\', kernel_initializer=\'random_normal\')(\r\n        UpSampling2D(size=(2, 2))(drop5))\r\n    merge6 = concatenate([drop4, up6], axis=3)\r\n    conv6 = Conv2D(512, 3, activation=\'relu\', padding=\'same\', kernel_initializer=\'random_normal\')(merge6)\r\n    conv6 = Conv2D(512, 3, activation=\'relu\', padding=\'same\', kernel_initializer=\'random_normal\')(conv6)\r\n    conv6 = BatchNormalization()(conv6)\r\n\r\n    up7 = Conv2D(256, 2, activation=\'relu\', padding=\'same\', kernel_initializer=\'random_normal\')(\r\n        UpSampling2D(size=(2, 2))(conv6))\r\n    merge7 = concatenate([conv3, up7], axis=3)\r\n    conv7 = Conv2D(256, 3, activation=\'relu\', padding=\'same\', kernel_initializer=\'random_normal\')(merge7)\r\n    conv7 = Conv2D(256, 3, activation=\'relu\', padding=\'same\', kernel_initializer=\'random_normal\')(conv7)\r\n    conv7 = BatchNormalization()(conv7)\r\n\r\n    up8 = Conv2D(128, 2, activation=\'relu\', padding=\'same\', kernel_initializer=\'random_normal\')(\r\n        UpSampling2D(size=(2, 2))(conv7))\r\n    merge8 = concatenate([conv2, up8], axis=3)\r\n    conv8 = Conv2D(128, 3, activation=\'relu\', padding=\'same\', kernel_initializer=\'random_normal\')(merge8)\r\n    conv8 = Conv2D(128, 3, activation=\'relu\', padding=\'same\', kernel_initializer=\'random_normal\')(conv8)\r\n    conv8 = BatchNormalization()(conv8)\r\n\r\n    up9 = Conv2D(64, 2, activation=\'relu\', padding=\'same\', kernel_initializer=\'random_normal\')(\r\n        UpSampling2D(size=(2, 2))(conv8))\r\n    merge9 = concatenate([conv1, up9], axis=3)\r\n    conv9 = Conv2D(64, 3, activation=\'relu\', padding=\'same\', kernel_initializer=\'random_normal\')(merge9)\r\n    conv9 = Conv2D(64, 3, activation=\'relu\', padding=\'same\', kernel_initializer=\'random_normal\')(conv9)\r\n    conv9 = Conv2D(16, 3, activation=\'relu\', padding=\'same\', kernel_initializer=\'random_normal\')(conv9)\r\n    conv9 = BatchNormalization()(conv9)\r\n\r\n    # Output layer of the U-Net with a softmax activation\r\n    conv10 = Conv2D(16, 1, activation=\'softmax\')(conv9)\r\n\r\n    model = Model(input=inputs, output=conv10)\r\n\r\n    model.compile(optimizer=Adam(lr=0.000001), loss=\'categorical_crossentropy\', metrics=[\'accuracy\'])\r\n\r\n    model.summary()\r\n\r\n    # filelist_modelweights = sorted(glob.glob(\'*.h5\'), key=numericalSort)\r\n\r\n    # if \'model_nocropping.h5\' in filelist_modelweights:\r\n    #   model.load_weights(\'model_nocropping.h5\')\r\n    return model\r\n\r\n\r\n```\r\nand the output of the console is :\r\n```\r\nD:\\anaconda3\\python.exe E:/PycharmProjects/ImageProcessing/mfunc.py\r\nUsing TensorFlow backend.\r\n(10, 256, 256, 4) (10, 256, 256, 3)\r\nWARNING:tensorflow:From D:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n2019-06-26 22:24:18.712315: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2019-06-26 22:24:18.726009: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\r\nWARNING:tensorflow:From D:\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\r\nE:\\PycharmProjects\\ImageProcessing\\Unet.py:91: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(""in..., outputs=Tensor(""co...)`\r\n  model = Model(input=inputs, output=conv10)\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_1 (InputLayer)            (None, 256, 256, 4)  0                                            \r\n__________________________________________________________________________________________________\r\nconv2d_1 (Conv2D)               (None, 256, 256, 64) 2368        input_1[0][0]                    \r\n__________________________________________________________________________________________________\r\nconv2d_2 (Conv2D)               (None, 256, 256, 64) 36928       conv2d_1[0][0]                   \r\n__________________________________________________________________________________________________\r\nbatch_normalization_1 (BatchNor (None, 256, 256, 64) 256         conv2d_2[0][0]                   \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_1 (MaxPooling2D)  (None, 128, 128, 64) 0           batch_normalization_1[0][0]      \r\n__________________________________________________________________________________________________\r\nconv2d_3 (Conv2D)               (None, 128, 128, 128 73856       max_pooling2d_1[0][0]            \r\n__________________________________________________________________________________________________\r\nconv2d_4 (Conv2D)               (None, 128, 128, 128 147584      conv2d_3[0][0]                   \r\n__________________________________________________________________________________________________\r\nbatch_normalization_2 (BatchNor (None, 128, 128, 128 512         conv2d_4[0][0]                   \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_2 (MaxPooling2D)  (None, 64, 64, 128)  0           batch_normalization_2[0][0]      \r\n__________________________________________________________________________________________________\r\nconv2d_5 (Conv2D)               (None, 64, 64, 256)  295168      max_pooling2d_2[0][0]            \r\n__________________________________________________________________________________________________\r\nconv2d_6 (Conv2D)               (None, 64, 64, 256)  590080      conv2d_5[0][0]                   \r\n__________________________________________________________________________________________________\r\nbatch_normalization_3 (BatchNor (None, 64, 64, 256)  1024        conv2d_6[0][0]                   \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 256)  0           batch_normalization_3[0][0]      \r\n__________________________________________________________________________________________________\r\nconv2d_7 (Conv2D)               (None, 32, 32, 512)  1180160     max_pooling2d_3[0][0]            \r\n__________________________________________________________________________________________________\r\nconv2d_8 (Conv2D)               (None, 32, 32, 512)  2359808     conv2d_7[0][0]                   \r\n__________________________________________________________________________________________________\r\nbatch_normalization_4 (BatchNor (None, 32, 32, 512)  2048        conv2d_8[0][0]                   \r\n__________________________________________________________________________________________________\r\ndropout_1 (Dropout)             (None, 32, 32, 512)  0           batch_normalization_4[0][0]      \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_4 (MaxPooling2D)  (None, 16, 16, 512)  0           dropout_1[0][0]                  \r\n__________________________________________________________________________________________________\r\nconv2d_9 (Conv2D)               (None, 16, 16, 1024) 4719616     max_pooling2d_4[0][0]            \r\n__________________________________________________________________________________________________\r\nconv2d_10 (Conv2D)              (None, 16, 16, 1024) 9438208     conv2d_9[0][0]                   \r\n__________________________________________________________________________________________________\r\nbatch_normalization_5 (BatchNor (None, 16, 16, 1024) 4096        conv2d_10[0][0]                  \r\n__________________________________________________________________________________________________\r\ndropout_2 (Dropout)             (None, 16, 16, 1024) 0           batch_normalization_5[0][0]      \r\n__________________________________________________________________________________________________\r\nup_sampling2d_1 (UpSampling2D)  (None, 32, 32, 1024) 0           dropout_2[0][0]                  \r\n__________________________________________________________________________________________________\r\nconv2d_11 (Conv2D)              (None, 32, 32, 512)  2097664     up_sampling2d_1[0][0]            \r\n__________________________________________________________________________________________________\r\nconcatenate_1 (Concatenate)     (None, 32, 32, 1024) 0           dropout_1[0][0]                  \r\n                                                                 conv2d_11[0][0]                  \r\n__________________________________________________________________________________________________\r\nconv2d_12 (Conv2D)              (None, 32, 32, 512)  4719104     concatenate_1[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_13 (Conv2D)              (None, 32, 32, 512)  2359808     conv2d_12[0][0]                  \r\n__________________________________________________________________________________________________\r\nbatch_normalization_6 (BatchNor (None, 32, 32, 512)  2048        conv2d_13[0][0]                  \r\n__________________________________________________________________________________________________\r\nup_sampling2d_2 (UpSampling2D)  (None, 64, 64, 512)  0           batch_normalization_6[0][0]      \r\n__________________________________________________________________________________________________\r\nconv2d_14 (Conv2D)              (None, 64, 64, 256)  524544      up_sampling2d_2[0][0]            \r\n__________________________________________________________________________________________________\r\nconcatenate_2 (Concatenate)     (None, 64, 64, 512)  0           batch_normalization_3[0][0]      \r\n                                                                 conv2d_14[0][0]                  \r\n__________________________________________________________________________________________________\r\nconv2d_15 (Conv2D)              (None, 64, 64, 256)  1179904     concatenate_2[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_16 (Conv2D)              (None, 64, 64, 256)  590080      conv2d_15[0][0]                  \r\n__________________________________________________________________________________________________\r\nbatch_normalization_7 (BatchNor (None, 64, 64, 256)  1024        conv2d_16[0][0]                  \r\n__________________________________________________________________________________________________\r\nup_sampling2d_3 (UpSampling2D)  (None, 128, 128, 256 0           batch_normalization_7[0][0]      \r\n__________________________________________________________________________________________________\r\nconv2d_17 (Conv2D)              (None, 128, 128, 128 131200      up_sampling2d_3[0][0]            \r\n__________________________________________________________________________________________________\r\nconcatenate_3 (Concatenate)     (None, 128, 128, 256 0           batch_normalization_2[0][0]      \r\n                                                                 conv2d_17[0][0]                  \r\n__________________________________________________________________________________________________\r\nconv2d_18 (Conv2D)              (None, 128, 128, 128 295040      concatenate_3[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_19 (Conv2D)              (None, 128, 128, 128 147584      conv2d_18[0][0]                  \r\n__________________________________________________________________________________________________\r\nbatch_normalization_8 (BatchNor (None, 128, 128, 128 512         conv2d_19[0][0]                  \r\n__________________________________________________________________________________________________\r\nup_sampling2d_4 (UpSampling2D)  (None, 256, 256, 128 0           batch_normalization_8[0][0]      \r\n__________________________________________________________________________________________________\r\nconv2d_20 (Conv2D)              (None, 256, 256, 64) 32832       up_sampling2d_4[0][0]            \r\n__________________________________________________________________________________________________\r\nconcatenate_4 (Concatenate)     (None, 256, 256, 128 0           batch_normalization_1[0][0]      \r\n                                                                 conv2d_20[0][0]                  \r\n__________________________________________________________________________________________________\r\nconv2d_21 (Conv2D)              (None, 256, 256, 64) 73792       concatenate_4[0][0]              \r\n__________________________________________________________________________________________________\r\nconv2d_22 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_21[0][0]                  \r\n__________________________________________________________________________________________________\r\nconv2d_23 (Conv2D)              (None, 256, 256, 16) 9232        conv2d_22[0][0]                  \r\n__________________________________________________________________________________________________\r\nbatch_normalization_9 (BatchNor (None, 256, 256, 16) 64          conv2d_23[0][0]                  \r\n__________________________________________________________________________________________________\r\nconv2d_24 (Conv2D)              (None, 256, 256, 16) 272         batch_normalization_9[0][0]      \r\n==================================================================================================\r\nTotal params: 31,053,344\r\nTrainable params: 31,047,552\r\nNon-trainable params: 5,792\r\n__________________________________________________________________________________________________\r\nTraceback (most recent call last):\r\n  File ""E:/PycharmProjects/ImageProcessing/mfunc.py"", line 78, in <module>\r\n    hist = model.fit(trainx[:6],trainy[:6],epochs=5,validation_data=(trainx[6:9],trainy[6:9]),batch_size = 64,verbose=1)\r\n  File ""D:\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py"", line 952, in fit\r\n    batch_size=batch_size)\r\n  File ""D:\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py"", line 789, in _standardize_user_data\r\n    exception_prefix=\'target\')\r\n  File ""D:\\anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py"", line 138, in standardize_input_data\r\n    str(data_shape))\r\nValueError: Error when checking target: expected conv2d_24 to have shape (256, 256, 16) but got array with shape (256, 256, 3)\r\n\r\nProcess finished with exit code 1\r\n\r\n```\r\nI don\'t know how to fix this trouble, hope some help!!!  Thank you!', 'Well, looks like your labels are not consisting of 16 classes, but of 3. Your training data is supposed to be an input output pair of shape:\r\n\r\n(10, 256, 256, 4) (10, 256, 256, 16)\r\n\r\nHowever, you are using: \r\n(10, 256, 256, 4) (10, 256, 256, 3)']",[],[],0,0
249,keras,1732,closed,How to optimize AUC directly？,"I have defined a custom objective which can be used to optimize auc directly, but the roc_auc_score() function is from sklearn which need to feed numpy array as args. But both the y_true and y_pred are tensor variable：



the function above gives an error: 
**theano.gof.fg.MissingInputError: ('Undeclared input',<TensorType(float32, matrix)>)**
",,"[""You need a keras(theano/tensorflow) function in order to compute some objective\nThis gets compiled and run on the gpu(if available)\nYou can't just use an external function, you'd have to reimplement roc_auc_score yourself\n"", 'Also, the AUC of the ROC is not a differentiable objective...\n', 'The short answer is, you can\'t. You should optimize a different ""proxy"" objective, such a crossentropy.\n', ""I've experimented with adding a rank-based loss to crossentropy with mixed results.\n\nExample:\nhttps://gist.github.com/jerheff/8cf06fe1df0695806456\n"", ""AUC is not differentiable, but it's equivalent to the expected probability that a classifier will correctly rank a random positive and random negative example.\n\nIf your classifier outputs probabilities (or something you can treat as probabilities), you could try this as a proxy loss:\n\n1 - E[f(pos)] E[1 - f(neg)]\n\nThe first expectation is over positive labels, the second over negative.  This can be computed for minibatches, as long as you can ensure there's always a mix of positive and negative labels in training.\n\nNB: I'm not making any arguments about whether this proxy is _good_ for optimization, just that it could be seen as an estimator of the AUC, under the predictions-are-class-probabilities assumption.\n"", ""For optimizing AUC indirectly, see this paper by my colleague Elad:\nhttps://arxiv.org/abs/1608.04802\n\nOn 19 November 2016 at 09:07, Thouis (Ray) Jones notifications@github.com\nwrote:\n\n> AUC is not differentiable, but it's equivalent to the expected probability\n> that a classifier will correctly rank a random positive and random negative\n> example.\n> \n> If your classifier outputs probabilities (or something you can treat as\n> probabilities), you could try this as a proxy loss:\n> \n> 1 - E[f(pos)] E[1 - f(neg)]\n> \n> The first expectation is over positive labels, the second over negative.\n> This can be computed for minibatches, as long as you can ensure there's\n> always a mix of positive and negative labels in training.\n> \n> NB: I'm not making any arguments about whether this proxy is _good_ for\n> optimization, just that it could be seen as an estimator of the AUC, under\n> the predictions-are-class-probabilities assumption.\n> \n> —\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> https://github.com/fchollet/keras/issues/1732#issuecomment-261725828,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AArWb5Q2OMoGv7w3nV9lTAPuCoW1D2Eqks5q_yzngaJpZM4HaQf0\n> .\n"", ""@fchollet  Now that TF has some support to calculating AUC*, would it be feasible to implement a custom Keras metric (not loss function) that  supports this? I've been trying but I get an uninitialized tensor error.  Not sure if I'm overlooking something\r\n\r\n* https://www.tensorflow.org/api_docs/python/tf/contrib/metrics/streaming_auc"", 'any news?\r\n', 'Tflearn provides the option of [optimising ROC AUC directly](http://tflearn.org/objectives/#roc-auc-score) using an approximation suggested in this [paper](https://pdfs.semanticscholar.org/df27/dde10589455d290eeee6d0ae6ceeb83d0c6b.pdf).\r\n\r\nIf this is something people think it would be worth adding, I would be happy to give it a go. \r\n', 'you may try `pairwise ranking loss`. As `Minimizing ranking error is equivalent to maximizing AUC (area under ROC curve).`\r\n\r\n`', '@jerheff What is the ranking function you implemented call?', '> you may try `pairwise ranking loss`. As `Minimizing ranking error is equivalent to maximizing AUC (area under ROC curve).`\r\n> \r\n> `\r\nCan you kindly point out the source of this claim? Thanks.', ""> I've experimented with adding a rank-based loss to crossentropy with mixed results.\r\n> \r\n> Example:\r\n> https://gist.github.com/jerheff/8cf06fe1df0695806456\r\n\r\n\r\n\r\n> I've experimented with adding a rank-based loss to crossentropy with mixed results.\r\n> \r\n> Example:\r\n> https://gist.github.com/jerheff/8cf06fe1df0695806456\r\n\r\n@jerheff  This implementation is based on which paper? This one? https://arxiv.org/abs/1608.04802?"", ""I use R for tensorflow,\r\nit's possible to optimize on AUC?"", '> For optimizing AUC indirectly, see this paper by my colleague Elad:\r\n> https://arxiv.org/abs/1608.04802\r\n\r\n@fchollet  Are you aware of a demo/code where this idea is implemented \r\nand that could be re-used or adapted? Thanks!\r\n', 'I found out where the code associated with https://arxiv.org/abs/1608.04802\r\nis located. It is now here:\r\n\r\nhttps://github.com/tensorflow/models/tree/archive/research/global_objectives\r\n\r\nThis can is not compatible with TF2. However, it can be made compatible with\r\na couple of small changes:\r\n\r\n```\r\n*** loss_layers_example_orig.py\t2020-08-27 18:01:12.096020097 +0200\r\n--- loss_layers_example.py\t2020-08-27 17:35:05.029580911 +0200\r\n***************\r\n*** 25,30 ****\r\n--- 25,33 ----\r\n  import numpy as np\r\n  from sklearn.metrics import precision_score\r\n  import tensorflow as tf\r\n+ tf.compat.v1.disable_eager_execution()\r\n+ \r\n+ \r\n  from global_objectives import loss_layers\r\n  \r\n  # When optimizing using global_objectives, if set to True then the saddle point\r\n***************\r\n*** 176,182 ****\r\n          _, loss_value, step = sess.run([dual_update_op, loss, global_step])\r\n  \r\n        if use_global_objectives:\r\n!         go_outputs = sess.run(other_outputs.values())\r\n  \r\n        if step % checkpoint_step == 0:\r\n          precision = precision_at_recall(\r\n--- 179,186 ----\r\n          _, loss_value, step = sess.run([dual_update_op, loss, global_step])\r\n  \r\n        if use_global_objectives:\r\n!           #go_outputs = sess.run(other_outputs.values())\r\n!           go_outputs = sess.run(list(other_outputs.values()))\r\n  \r\n        if step % checkpoint_step == 0:\r\n          precision = precision_at_recall(\r\n```\r\n', '@fchollet the link () you provided is broken, can you provide the paper name?\r\n-----\r\nFor optimizing AUC indirectly, see this paper by my colleague Elad:\r\nhttps://arxiv.org/abs/1608.04802', '  @ieipi the paper name : Scalable Learning of Non-Decomposable Objectives']","[' python\ndef auc_obj(y_true, y_pred):\n    y_true = K.eval(y_true)\n    y_pred = K.eval(y_pred)\n    return K.variable(1.-roc_auc_score(y_true,y_pred))\n']",[],0,0
250,keras,12897,closed,Run Default code for imdb_cnn.py it gives pickle-errors,"<em>Please make sure that this is a Bug or a Feature Request and provide all applicable information asked by the template.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.</em>  

**System information**  
- Have I written custom code (as opposed to using example directory):  
>> I used the default code exactly as written.

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  
Microsoft Windows 10 Enterprise

- TensorFlow backend (yes / no): 
yes
 
- TensorFlow version:  
1.13.1

- Keras version:  
2.2.4

- Python version:  
Python 3.7.3

- CUDA/cuDNN version:  
None.  I'm using cpu-only vanilla keras installed by Anaconda. 

- GPU model and memory:  
NVidia Quadra P1000


**Describe the current behavior**  
It crashes when I hit F5 in Spyder.  It talks about pickle error.



**Describe the expected behavior**  

I would expect it to get to line 29 and print something about the training sequence count.

Line 28, where it is choking, is 


It doesn't get to print ""train sequences"" after attempting to load data, so it isn't getting past line 29.  

When I re-select line 28, and hit F9 (run highlighted) it gives the same error again.

**Code to reproduce the issue**  
You already have it.  It was literally copy-paste-run-crash

**Other info / logs**  
Traceback is included in what it did.
",stat:awaiting response type:bug/performance,['This is [bug](https://github.com/numpy/numpy/pull/13359) with numpy 1.16.3 where allow_pickle=False. For the time being can you please downgrade your numpy version to 1.16.2 . This should execute the script successfully.\r\n``` pip install numpy==1.16.2```'],"['\r\nusing TensorFlow backend.\r\nLoading data...\r\nTraceback (most recent call last):\r\n\r\n  File ""<ipython-input-1-4a5d5f4a1034>"", line 1, in <module>\r\n    runfile(\'C:/work/imdb_cnn.py\', wdir=\'c:/work\')\r\n\r\n  File ""C:\\ProgramData\\Anaconda3\\envs\\Step2\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py"", line 827, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File ""C:\\ProgramData\\Anaconda3\\envs\\Step2\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py"", line 110, in execfile\r\n    exec(compile(f.read(), filename, \'exec\'), namespace)\r\n\r\n  File ""C:/work/imdb_cnn.py"", line 28, in <module>\r\n    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\r\n\r\n  File ""C:\\ProgramData\\Anaconda3\\envs\\Step2\\lib\\site-packages\\keras\\datasets\\imdb.py"", line 59, in load_data\r\n    x_train, labels_train = f[\'x_train\'], f[\'y_train\']\r\n\r\n  File ""C:\\ProgramData\\Anaconda3\\envs\\Step2\\lib\\site-packages\\numpy\\lib\\npyio.py"", line 262, in __getitem__\r\n    pickle_kwargs=self.pickle_kwargs)\r\n\r\n  File ""C:\\ProgramData\\Anaconda3\\envs\\Step2\\lib\\site-packages\\numpy\\lib\\format.py"", line 692, in read_array\r\n    raise ValueError(""Object arrays cannot be loaded when ""\r\n\r\nValueError: Object arrays cannot be loaded when allow_pickle=False\r\n']","['(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)']",0,0
251,keras,1990,closed,CPU Error causing machine to shut down,"Has anyone else ever had their machine throw a CPU error and shut off while fitting a model on the GPU?

This has happened to me ~10 times, both on my Macbook Pro (GeForce GT 750M) and on a Dell server (Tesla M2070-Q).
",,"['Hardware proplem or overheat could cause that. On the server, bad power\nsupply can cause that. We had ""certified"" workstation do that due to bad\npower supply.\n\nCan you lower the bach-size significatively? it would be less efficient so,\nwould heat less. Try on a 3rd computer with different configurartion?\n\nOn Wed, Mar 16, 2016 at 11:12 AM, bkj notifications@github.com wrote:\n\n> Has anyone else ever had their machine throw a CPU error and shut off\n> while fitting a model on the GPU?\n> \n> This has happened to me ~10 times, both on my Macbook Pro (GeForce GT\n> 750M) and on a Dell server (Tesla M2070-Q).\n> \n> —\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly or view it on GitHub\n> https://github.com/fchollet/keras/issues/1990\n', 'This was more than likely due to overheating.\n']",[],[],0,0
252,keras,8757,closed,Support for Multitable input from hdf,"Hello,

I would like to ask for support in HDF5Matrix to select data from multiple tables at the same time.
This is mainly due to constrains on the width of tables in hdf.
(Storing waveforms  in my case with roughly 1k data points each so I have to split them into multiple tables)

Example:
Hdf file:
/data/table1
/data/table2

m = Model()
a = HDF5Matrix(""file.hdf5"", ""/data/table1"")
b = HDF5Matrix(""file.hdf5"", ""/data/table2"")
a.fit(a+b)


",,[],[],[],0,0
253,keras,8816,closed,How to merge LSTM and Input layer?,"I tried merging an Input Layer and stacked CNNs and it worked:



But I couldn't achive combining an Input Layer and LSTM, it gives dimension mismatch error in the Merge step, what can I do to match shapes?



Thanks!",,"[""Hi, I think I can help you. \r\nIt's very difficult to guess where the error comes from without the stacktrace. When asking for help in general on the Internet, please copy-past it.\r\nThank you. "", '@hkmztrk You are trying to merge a 3D array with a 2D one. LSTM with return_sequences=True produces 3D output, while model1 produces a 2D output. In your first example, you escaped this problem by introducing a GlobalMaxPooling1D layer, which took as input a 3D array and converted it into a 2D one.']","[""\r\n    combined = Sequential()\r\n    model1 = Sequential()\r\n    model1.add(Activation('linear', input_shape=(SEQ_LEN,)))\r\n\r\n\r\n    model2 = Sequential()\r\n    model2.add(Embedding(input_dim=EMBEDDING_DIMS+1, output_dim=128, input_length=MAX_LEN)) \r\n    model2.add(Conv1D(filters=NUM_FILTERS, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1))\r\n    model2.add(Conv1D(filters=NUM_FILTERS*2, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1))\r\n    model2.add(Conv1D(filters=NUM_FILTERS*3, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1))\r\n    model2.add(GlobalMaxPooling1D())\r\n\r\n\r\n    combined.add(Merge([model2, model1], mode='concat', concat_axis=1))\r\n\r\n    # Fully connected \r\n    combined.add(Dense(1024, activation='relu'))\r\n    combined.add(Dropout(0.1))\r\n\r\n    combined.add(Dense(1, kernel_initializer='normal'))\r\n"", ""\r\n\r\n    combined = Sequential()\r\n    model1 = Sequential()\r\n    model1.add(Activation('linear', input_shape=(SEQ_LEN,)))\r\n\r\n\r\n    model2 = Sequential()\r\n    model2.add(Embedding(input_dim=EMBEDDING_DIMS+1, output_dim=128,  input_length=MAX_LEN, mask_zero=True))\r\n    model2.add(LSTM(100,  return_sequences=True)) #stateful=True\r\n    model2.add(LSTM(50,  return_sequences=True))\r\n    model2.add(TimeDistributed(Dense(EMBEDDING_DIMS, activation='linear')))\r\n    combined.add(Merge([model1, model2], mode='concat', concat_axis=1))\r\n\r\n    # Fully connected \r\n    combined.add(Dense(1024, activation='relu'))\r\n    combined.add(Dropout(0.1))\r\n    combined.add(Dense(1, kernel_initializer='normal'))\r\n""]",[],0,0
254,keras,10875,closed,Multi-thread online training for multiple copies of a model,"Hello.
My task is to create cetrain amount of copies of the same network and run each in a separate thread, where they are waiting for data given batch at a time and once they received the data they should make a training step. I am using Keras + Tensorflow on GPU.

First problem that I ran into that simply creating a model in one thread and then trying to train it in another has to be handled with setting a graph and a session, otherwise I was getting an exception of trying to work with nodes from one graph summing up to nodes of another.
When I created separate sessions and graphs for each of the nodes and was specifying it with session.as_default and graph.as_default it was working, but the more processes I am running, the more TF sessions are created and they are completely slowing down the calculations. So I returned to the idea of having one session only, that I am getting via keras.backend.get_session() and using everywhere together with graph of this session.
But here the behavior becomes completely random. It might run without errors (for 1 and 2 workers), it might give an error of initialization


 or 



depending on how fast it starts the threads.

Network creation:



Then the update step looks like this:



What is the correct solution for using sessions and graphs in this situation?
",,['Do you sovle your problem？ @link-er '],"['\r\nlib/python3.5/site-packages/tensorflow/python/ops/variables.py"", line 322, in _init_from_args\r\n    ""initializer."" % name)\r\nValueError: Initializer for variable dense_4/BiasAdd/training/SGD/Assign_11/training/SGD/gradients/Fill/dense_9/kernel/ is from inside a control-flow construct, such as a loop or conditional. When creating a variable inside a loop or conditional, use a lambda as the initializer.\r\n\r\n', '\r\nFile ""/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3665, in colocate_with\r\n    self._colocation_stack.pop()\r\nIndexError: pop from empty list\r\n', '\r\nimport tensorflow as tf\r\nfrom keras.models import Model\r\nfrom keras.layers import Input, Dense, Dropout, Conv2D, MaxPooling2D, Flatten\r\nfrom keras.initializers import glorot_uniform\r\nfrom keras import optimizers\r\nimport numpy as np\r\nfrom keras import backend as K\r\n\r\n_SESSION = K.get_session()\r\n\r\nclass LearnerFactory():\r\n    def getKerasNN(self, updateRule, learningRate, lossFunction, batchSize):\r\n        with _SESSION.as_default():\r\n            with _SESSION.graph.as_default():\r\n                # network creation\r\n', '\r\n        with self._session.as_default():\r\n            with self._session.graph.as_default():\r\n                metrics = self._core.train_on_batch(np.asarray([record[0] for record in data]), np.asarray([record[1] for record in data]))\r\n']",[],0,0
255,keras,7695,closed,forward states of bidirectional lstm are not masked,"Hi Folks,

I am using Keras and TF.
I am passing padded sequences as input (pad value=-1) and masking the input with a  layer with mask value set to -1.0. However, when I collect the output of  after the BiLSTM layer, I see the forward states are non-zero at the masked positions.



Here is the output:



Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']","['Masking', 'x', 'python\r\n    from keras.layers import Embedding, LSTM, Dense, Input, Masking\r\n    from keras.layers.wrappers import Bidirectional\r\n    from keras.models import Model\r\n    import numpy as np\r\n    import tensorflow as tf\r\n    vec = np.random.randn(3, 5)\r\n    inp = Input((3,))\r\n    x = Masking(mask_value=-1.0)(inp)\r\n    x = Embedding(3, 5, weights=[vec], input_length=3, trainable=False)(x)\r\n    x = Bidirectional(LSTM(10, return_sequences=True))(x)\r\n    sess = tf.Session()\r\n    sess.run(tf.global_variables_initializer())\r\n    print(sess.run(x, {inp: [[0, 2, -1], [1, -1, -1]]}))\r\n', 'python\r\n    [[[ -4.62276675e-03  -4.01115604e-03   5.02156140e-03   1.97147974e-03\r\n         7.38522829e-03   5.62763307e-03   2.18000403e-03   8.19381850e-04\r\n         7.11255067e-04  -5.42447111e-03   4.71341610e-03  -9.23852995e-03\r\n         8.90769251e-03   5.24031650e-03   5.27720852e-03   5.26314508e-03\r\n         6.20147912e-03   3.62612633e-03   4.85892594e-03  -2.66220560e-03]\r\n      [ -6.73649739e-03  -2.59472057e-04   5.75539097e-03   6.66894065e-03\r\n         1.10127367e-02   2.46753707e-03  -2.99500511e-03  -3.73128545e-03\r\n        -5.83201367e-03  -4.31951787e-03   1.44616829e-03  -6.58686040e-03\r\n         4.14082780e-03   1.14090310e-03  -8.29242985e-04   5.53416228e-03\r\n        -4.11105895e-04   2.87892064e-03   3.62366205e-04  -7.94248248e-04]\r\n      [ -5.54567296e-03   1.15430041e-03   3.27830086e-03   4.12886823e-03\r\n         6.78183092e-03   1.79559551e-03  -1.80174352e-03  -3.33251758e-03\r\n        -5.29490225e-03  -3.05411895e-03   0.00000000e+00   0.00000000e+00\r\n         0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\r\n         0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]\r\n    \r\n     [[ -4.98074247e-03   9.01466759e-04   3.40987043e-03  -3.25349579e-03\r\n         9.21981584e-04   5.99770434e-03   1.67222356e-03   2.20844080e-03\r\n         4.45439760e-03  -3.40889138e-03   7.48059654e-04  -7.22813362e-04\r\n        -8.83788511e-04  -6.78786746e-05   2.53343279e-03   6.05521607e-04\r\n        -1.31173420e-03   2.08991882e-03  -1.15431065e-03   2.35650165e-04]\r\n      [ -3.34386993e-03   1.24489667e-03   1.97105715e-03  -2.06982507e-03\r\n         9.56661941e-04   4.27589752e-03   9.54369374e-04   1.84580882e-03\r\n         2.93672620e-03  -2.59263976e-03   0.00000000e+00   0.00000000e+00\r\n         0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\r\n         0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\r\n      [ -2.26694648e-03   1.35568588e-03   1.10661483e-03  -1.33866596e-03\r\n         8.82549793e-04   3.03406548e-03   4.88151883e-04   1.53438631e-03\r\n         1.89515646e-03  -2.01789290e-03   0.00000000e+00   0.00000000e+00\r\n         0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\r\n         0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]]\r\n']",[],0,0
256,keras,7987,closed,Is it possible to use image_ocr example for multiple line image? ,"The image_ocr code that is available in the example section, is it possible to use that code for multiple lines? I mean is this possible to use the code for a single printed document? ",,"['I have the same question, did you use this example on multiple lines text? Thanks.', '@minyil What i did is, used ocropy for segmentation of multiline. Then use the individual images for prediction of each line. See the documentation of OCROPY library in Python. ']",[],[],0,0
257,keras,11539,closed,lstm input_shape override input from previous layer?,"I'm confused by the behavior when I feed the output of, say, an embedding layer to an lstm layer, but also specify the input shape for the lstm layer.

Example code:





Here the output of the embedding layer has shape (None, None, 10). Then I specify the input shape of lstm to be (None, 32, 1), which does not match with the embedding layer's output. The model still compiles without problem.

What is the behavior here? Will  override the previous input shape, or the opposite?",type:support,[],[],"['embedding_input_dim = 500', 'model = Sequential()', 'model.add(Embedding(embedding_input_dim, 10))', 'model.add( LSTM(20, input_shape = (32,1)))', 'input_shape = (32,1)']",0,0
258,keras,11090,closed,Specify a different version of Keras in Docker Install ,"Is there a way to quickly change out the version of Keras in the ""Dockerfile""? 

Specifically, when using Keras docker, I  and add the command say to specify a version of Cuda. How do I change the version of keras in this?

Also is it possible to make sure that dependencies are the correct version too? For example if I use keras version 1.2 it would be nice for it to automatically pick the right theano, and thus pygpu, etc. for that version.

Thanks
",type:support,[],[],"['make notebook', 'CUDA_VERSION=8.0 ']",0,0
259,keras,8367,closed,Binary Cross Entropy loss function is not working with python 3.6,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",,"['\r\nimport keras\r\nfrom keras.models import Sequential\r\nfrom keras.layers.core import Dense\r\n\r\nclassifier = Sequential()\r\n\r\nclassifier.add(Dense(activation=""relu"", kernel_initializer=""random_uniform"", units=6, input_dim=11))\r\n\r\n\r\nclassifier.add(Dense(activation=""relu"", kernel_initializer=""random_uniform"", units=6))\r\nclassifier.add(Dense(activation=""sigmoid"", kernel_initializer=""random_uniform"", units=1))\r\nclassifier.compile(optimizer = \'adam\', loss = \'binary_crossentropy\', metrics = [\'accuracy\'])\r\n\r\nclassifier.fit(X_train, y_train, batch_size = 10, epochs = 100)\r\n\r\ny_pred = classifier.predict(X_test)\r\ny_pred = (y_pred > 0.5)\r\n\r\nfrom sklearn.metrics import confusion_matrix\r\ncm = confusion_matrix(y_test, y_pred)', 'Hello Team,\r\n\r\nHere is the version of python I am using in my MAC. \r\nblr-mpuvk:~ vphillip$ python --version\r\nPython 3.6.3 :: Anaconda custom (64-bit)\r\nblr-mpuvk:~ vphillip$ conda --version\r\nconda 4.3.29\r\n\r\nName: Keras\r\n\r\nVersion: 2.0.8\r\n\r\nSummary: Deep Learning for Python\r\n\r\nHome-page: https://github.com/fchollet/keras\r\n\r\nAuthor: Francois Chollet\r\n\r\nAuthor-email: francois.chollet@gmail.com\r\n\r\nLicense: MIT\r\n\r\nThanks\r\nVeneel']",[],[],0,0
260,keras,12679,closed,Feature Request: Accept None as target size for ImageDataGenerator,"I came across this article: https://www.fast.ai/2018/04/30/dawnbench-fastai/#imagenet and I wanted to try implementing dynamic image sizes using ImageDataGenerator and keras applications models (those with global pooling layer). This is to allow the model to accept any input shape for progressively increasing input shape during training, as well as training a variety of image sizes without having to resize the input. However, there are a few problems I encountered.

First, keras.preprocessing.ImageDataGenerator does not accept None or (None, None) as target size. When target_size=(None, None):

When target_size=None:


Also, while most keras applications models seem to accept (None, None, 3) as input shape, it doesn't seem to work with NASNet and ResNeXt. Here is the error when I get for ResNeXt for example:


Dynamic input shape may also break other models due to some implementation detail depending on input shape. For example, in a keras-contrib's implementation of ResNet, the stride value for Conv2D requires this calculation which will break if input shape is (None, None, 3)


It would really be great if ImageDataGenerator can support None type for target size for those fully convolutional networks and image models with global pooling which do not require a fixed input size.

Edit: I just realised that the problem is much more difficult than I initially thought since a numpy array representing a batch of images needs to have a fixed size",type:feature,"['how to  get the number of epochs during training.', ""@Sole1Memory ImageDataGenerator has a total_batches_seen attribute you could use by dividing that with the number of batches per epoch. You could also try using model.history.epoch. If you were using model.fit_generator you might have to use a callback:\r\n```python\r\nclass EpochPrinter(keras.callbacks.Callback):\r\n    def on_epoch_begin(self, epoch, logs=None):\r\n        print(f'Epoch: {epoch}')\r\n\r\nepoch_printer = EpochPrinter()\r\n# fit_generator(callbacks=[epoch_printer...\r\n```\r\n\r\nI'm still trying to figure a way to make use of the epoch number to progressively increase image size during training. One possible way might be to loop the training manually:\r\n```python\r\n# train_gen = image_datagen.flow_from_directory...\r\nfor epoch in range(num_epochs):\r\n    for batch in range(len(train_gen)):\r\n        image_batch, y = train_gen.next()\r\n        # do some manual image resizing\r\n        model.train_on_batch(resized_image_batch, y)\r\n```\r\n\r\nBut it would be nice if it could be accomplished with a callback to model.fit_generator or implemented in ImageDataGenerator itself""]","['python\r\nkeras_preprocessing/image/iterator.py"", line 218, in _get_batches_of_transformed_samples\r\n    batch_x = np.zeros((len(index_array),) + self.image_shape, dtype=self.dtype)\r\nTypeError: \'NoneType\' object cannot be interpreted as an integer\r\n', 'python\r\nkeras_preprocessing/image/iterator.py"", line 169, in set_processing_attrs\r\n    self.target_size = tuple(target_size)\r\nTypeError: \'NoneType\' object is not iterable\r\n', 'python\r\n  File ""keras/applications/__init__.py"", line 28, in wrapper\r\n    return base_fun(*args, **kwargs)\r\n  File ""keras/applications/resnext.py"", line 19, in ResNeXt101\r\n    return resnext.ResNeXt101(*args, **kwargs)\r\n  File ""keras_applications/resnet_common.py"", line 575, in ResNeXt101\r\n  File ""keras_applications/resnet_common.py"", line 373, in ResNet\r\n  File ""keras_applications/resnet_common.py"", line 566, in stack_fn\r\n  File ""keras_applications/resnet_common.py"", line 264, in stack3\r\n  File ""keras_applications/resnet_common.py"", line 231, in block3\r\n  File ""kera/engine/base_layer.py"", line 457, in __call__\r\n    output = self.call(inputs, **kwargs)\r\n  File ""keras/layers/core.py"", line 402, in call\r\n    return K.reshape(inputs, (K.shape(inputs)[0],) + self.target_shape)\r\n  File ""keras/backend/tensorflow_backend.py"", line 2167, in reshape\r\n    return tf.reshape(x, shape)\r\n  File ""tensorflow/python/ops/gen_array_ops.py"", line 7179, in reshape\r\n    ""Reshape"", tensor=tensor, shape=shape, name=name)\r\n  File ""tensorflow/python/framework/op_def_library.py"", line 529, in _apply_op_helper\r\n    (input_name, err))\r\nValueError: Tried to convert \'shape\' to a tensor and failed. Error: None values not supported.\r\n', 'python\r\nstride_width = int(round(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS]))\r\n stride_height = int(round(input_shape[COL_AXIS] / residual_shape[COL_AXIS]))\r\n']",[],0,0
261,keras,12240,closed,K.ctc_decode() beam search: tf.sparse_to_dense is deprecated," with beam search warns about deprecated TF function:

tf.sparse.SparseTensortf.sparse.to_dense

The solution is to change  to . It will not work in some earlier versions of TensorFlow (but I assume they're not supported).

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
 ()

- [x] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup). (1.12.0)

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

",type:bug/performance,[],"[""\r\n$ py.test -s -vv 'tests/keras/backend/backend_test.py::TestBackend::()::test_ctc_decode_beam_search'\r\n""]","['K.ctc_decode()', '', '\r\nWARNING:tensorflow:From keras/backend/tensorflow_backend.py:4553: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCreate a ', ' and use ', ' instead.\r\n', '', 'tf.sparse_to_dense()', 'tf.sparse.to_dense()', 'pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps', 'master']",0,0
262,keras,11024,closed,multi_gpu_model InvalidArgumentError on seq2seq model,"Hi,

I'm running a seq2seq model using GRU's on 8 GPUs, using fit_generator and this is the error I'm having:
InvalidArgumentError: Incompatible shapes: [128,100] vs. [1024,100]
	 [[Node: replica_0/model_1/gru_1/while/add = Add[T=DT_FLOAT, _class=[""loc:@train.../Reshape_1""], _device=""/job:localhost/replica:0/task:0/device:GPU:0""](replica_0/model_1/gru_1/while/BiasAdd, replica_0/model_1/gru_1/while/MatMul_3)]]
	 [[Node: replica_0/model_1/gru_2/while/Identity/_967 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_7766_replica_0/model_1/gru_2/while/Identity"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^_cloopreplica_0/model_1/gru_2/while/TensorArrayReadV3_1/_133)]]

keras was installed from master directly and updated to latest tensorflow-gpu from pip.

the generator is designed to yield a batch of 1024 so each GPU would work on a batch of 128.
if I reduce for example the number of GPUs to 4, I would have the following error:
InvalidArgumentError: Incompatible shapes: [256,100] vs. [1024,100]

for going over similar issues I've found the following issue #9449 and the merge request #10845 saying the issue is fixed. I've installed keras from source and I still have the issue.

Is it possible the issue is not fixed for GRUs?

this is my model definition:
word_input = Input(shape=(word_dim,))
decoder_inputs = Input(shape=(None,))
decoder_embed = Embedding(input_dim=num_tokens, output_dim=word_dim)
decoder_gru = GRU(word_dim, return_sequences=True, return_state=True)
decoder_dense = Dense(num_tokens, activation='softmax')

embedded = decoder_embed(decoder_inputs)
gru_output, state_h = decoder_gru(embedded, initial_state=word_input)
decoder_outputs = decoder_dense(gru_output)

model = Model([word_input, decoder_inputs], decoder_outputs)

rmsprop = optimizers.RMSprop(lr=0.001)

parallel_model = multi_gpu_model(model, gpus=8)
parallel_model.compile(loss='categorical_crossentropy',optimizer=rmsprop,metrics=['acc'])

filename = 'model.h5'
checkpoint = ModelCheckpoint(filename, monitor='loss', verbose=1, save_best_only=True, mode='min')

parallel_model.fit_generator(data_generator.generate(), training_size//batch_size*num_steps, num_epochs, callbacks=[checkpoint], verbose=1)

Thanks!


Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).",,"[""@andreynikk I'm sure this is fixed for all ```RNN``` layers including ```GRU```. Please check your environment and the keras version you run, if there are multiple keras installed, it may run into other versions. You can try ```pip uninstall keras``` to uninstall it and then use ```python setup.py install``` to install the master version. If there is still errors, please let me know."", ""Hi @yanboliang \r\n\r\nI did what you requested just to be on the safe side:\r\nprint(tf.__version__)\r\nprint(keras.__version__)\r\n1.10.1\r\n2.2.2\r\n\r\nIts either the issue was not fixed on GRUs or the way I combine multi_gpu_model with a generator (through fit_generator) is wrong.\r\nthe main hint on the issue would be in the error output:\r\nInvalidArgumentError: Incompatible shapes: [32,100] vs. [256,100]\r\n\r\nsome more context: my generator yields batches of [256,100] (tested and verified), and from official documentation multi_gpu_model would split the provided batch to the number of GPUs, so that for a batch of [256,100] I get [32,100] batch size for each GPU, and this is the exact error I'm having.\r\nso could be that it is unable to split properly the incoming batch from a generator to sub-batches that will accompany each GPU?\r\n\r\nmy model definitions:\r\nrmsprop = optimizers.RMSprop(lr=1)\r\nparallel_model = multi_gpu_model(model, gpus=8)\r\nparallel_model.compile(loss='categorical_crossentropy',optimizer=rmsprop,metrics=['acc'])\r\ncheckpoint = ModelCheckpoint(filename, monitor='loss', verbose=1, save_best_only=True, mode='min')\r\nparallel_model.fit_generator(data_generator.generate(), training_size//batch_size, num_epochs,\r\n                    callbacks=[checkpoint], verbose=1)\r\n\r\nIf you need any more information please let me know.\r\n"", 'I have the same problem, my model is working well on CPUs but raises the same error you mentioned above, error with GPUs.\r\nAny help, please?', ""@andreynikk Can you share the minimal code snippet which can reproduce this error? I still can't reproduce this issue on my environment which has two GPUs. If you can share the code, I can help to debug.""]",[],[],0,0
263,keras,10786,closed,Train and test data that has different channel size,"Hi!

Is there a way I can use pre-trained weights from a model that takes input_shape of (batch_size, 50, 50, 4) to be initialized for another model that takes input_shape of  (batch_size, 50, 50, 1)? Output_shape has size of (50, 50, 1).

AND

Is there a way I can train a model that takes input_shape of (batch_size, 50, 50, 4) and test on data that has input_shape of  (batch_size, 50, 50, 1)? Output_shape has size of (50, 50, 1).

Thank you!",,"['Here is not a place for help and this is not a bug or feature.\r\nPlease do not post here this type of questions but use stackoverflow.\r\nIn your question, the short answer is no.\r\nThe long one is:\r\nYou have 50x4 weights and you want to just use 50x1, from simple maths that is impossible.\r\nYou can sellect just one of the 4 channels (by using individual weight extraction) and use it but the results may not make any sense.\r\nPlease close the issue by pressing the close button under the reply/comment section. ', '@01bui Maybe you can change your second model to take as input (50, 50, 4) as well and add an extra layer `Conv2D(filters=1, kernel_size=(1,1))` that learns to map the 4 input features into 1 new feature. You would also have to change the layer after this one because it has learned to process 4 input variables instead of 1.']",[],[],0,0
264,keras,8072,closed,LSTM equation in Keras,"I confuse with Lstm in Keras. I try to map Keras code to LSTM equation. I found this weights parameter in Keras code. 

![image](https://user-images.githubusercontent.com/10016227/31258172-999520a6-aa67-11e7-9fc4-ffaa6d646994.png)

I think that It's a weight from current weight of cell and recurrent weight of cell.

In the call function in LSTM Keras code, I found the statement for calculate input ,forget ,cell and output term.

![image](https://user-images.githubusercontent.com/10016227/31258223-e7627d42-aa67-11e7-8810-723bd7a05391.png)


In input term of LSTM equation have W_ci * C_t-1 but I can't find in Keras code.

![image](https://user-images.githubusercontent.com/10016227/31258220-e31a4d32-aa67-11e7-8139-ae91968239ec.png)


Could please anyone explains the LSTM equation and Keras code.",,"['Where to you take your LSTM equations from ? \r\nOn Wikipedia about original LSTM : \r\n![image](https://user-images.githubusercontent.com/11304248/31311358-24fc6db8-abab-11e7-8c90-3709e1c70b66.png)\r\n\r\nYour equation may refer to the convolutional LSTM\r\n', 'In previous Keras versions, the following link was part of the LSTM layer description:\r\n\r\n""Long-Short Term Memory unit - Hochreiter 1997. For a step-by-step description of the algorithm, see [this tutorial] (http://deeplearning.net/tutorial/lstm.html).""\r\n\r\nIt is explained there that this change was performed for computational efficiency.']",[],[],0,0
265,keras,11471,closed,"Access or Define ""classes"" attribute for custom generator","I am using a custom generator to get the data from my paths and it seems to be working fine. However, one issue I am facing with using a custom generator is that, unlike the default generators of Keras, I cannot really use attributes. What I am trying here for example is getting a report and confusion matrix on my testing set, however, I cannot find a way to input the classes from my generator. Here is the script:



This works well with a default generator, but in this case my testgenerator is defined by a custom generator and inside the custom generator I am using 2 default generators to read 2 streams of data. So obviously I get the error  So my question is, is there a way to define the attributes for my custom generator or is there another way to feed my labels to the confusion matrix without using attribute? Or can new feature be added to deal with this kind of situation ?

I tried: this instead:

> cm = confusion_matrix(np.argmax(testgenerator[1], axis=1), y_pred)

But it gives me  while it is supposed to yield a tuple of ([1,9,1024], [1,9,4])",type:support,[],"[""\r\n#Confusion Matrix and Classification Report\r\nY_pred = Bi_LSTM.predict_generator(testgenerator, 120)\r\ny_pred = np.argmax(Y_pred, axis=1)\r\nprint('Confusion Matrix')\r\n\r\ncm = confusion_matrix(testgenerator.classes, y_pred)\r\nprint(cm)\r\nprint('Classification Report')\r\ntarget_names = ['Bark', 'Jump', 'Stand','Walk']\r\n\r\nprint(classification_report(testgenerator.classes, y_pred, target_names=target_names))\r\nplot_confusion_matrix_two(cm, target_names, title='Confusion Matrix',cmap=None,normalize=False)\r\n""]","[""AttributeError: 'generator' object has no attribute 'classes'."", ""TypeError: 'generator' object is not subscriptable""]",0,0
266,keras,11618,closed,Failure in saving weights of a model that has a sub-model that shares weights of another model,"- [x] Check that you are up-to-date with the master branch of Keras. You can update with:


- [x] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

https://gist.github.com/DSamuylov/1f3d42478be4e277f776783f215816cf

The code is not actually that long:

modelmodel_with_2_inputsmodel_finalmodel_final

I also added a print statement inside  method to print  and , and here there is the output:

modelmodel_with_2_inputsmodel_final
Am I wrong somewhere or there is a bug?",type:bug/performance,"[""I'm facing the same issue with TF Hub xfer learning https://www.tensorflow.org/tutorials/images/hub_with_keras""]",['\r\n\r\nThe error is the following:\r\n\r\n'],"['pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps', '', 'python\r\nimport keras\r\n# The same happens with:\r\n# import tensorflow.keras as keras\r\n\r\nkeras.backend.clear_session()\r\n\r\n# Model that we are goint to share.\r\nx = keras.layers.Input(shape=[None, None, 3])\r\ny = keras.layers.Conv2D(filters=4, kernel_size=(3, 3))(x)\r\nmodel = keras.models.Model(inputs=x, outputs=y)\r\n\r\nprint(""# Save ', ':"")\r\nmodel.save_weights(filepath=""test.h5"")\r\n# IT WORKS!\r\n\r\n# We importe this model as a part of a more complex model. Moreover we share its weights among multiple inputs:\r\nx1 = keras.layers.Input(shape=[None, None, 3])\r\ny1 = model(x1)\r\nmodel_to_be_shared = keras.models.Model(inputs=x1, outputs=y1)\r\n\r\nx2 = keras.layers.Input(shape=[None, None, 3])\r\ny2 = model_to_be_shared(x2)\r\n\r\nmodel_with_2_inputs = keras.models.Model(inputs = [x1, x2], outputs=keras.layers.Add()([y1, y2]))\r\n\r\nprint(""# Save ', ':"")\r\nmodel_with_2_inputs.save_weights(filepath=""test.h5"")\r\n# IT WORKS!\r\n\r\n# Finally, we wrap the model as a part of a more complex model:\r\nx1 = keras.layers.Input(shape=[None, None, 3])\r\nx2 = keras.layers.Input(shape=[None, None, 3])\r\ny = model_with_2_inputs([x1, x2])\r\nmodel_final = keras.models.Model(inputs=[x1, x2], outputs=y)\r\n\r\nprint(""# Save ', ':"")\r\nmodel_final.save_weights(filepath=""test.h5"")\r\n# IT FAILS!\r\n\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-35-35af618a87ac> in <module>\r\n     34 \r\n     35 print(""# Save ', ':"")\r\n---> 36 model_final.save_weights(filepath=""test.h5"")\r\n     37 # IT FAILS!\r\n\r\n~/anaconda3/lib/python3.6/site-packages/keras/engine/network.py in save_weights(self, filepath, overwrite)\r\n   1119                 return\r\n   1120         with h5py.File(filepath, \'w\') as f:\r\n-> 1121             saving.save_weights_to_hdf5_group(f, self.layers)\r\n   1122             f.flush()\r\n   1123 \r\n\r\n~/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py in save_weights_to_hdf5_group(f, layers)\r\n    581         for name, val in zip(weight_names, weight_values):\r\n    582             param_dset = g.create_dataset(name, val.shape,\r\n--> 583                                           dtype=val.dtype)\r\n    584             if not val.shape:\r\n    585                 # scalar\r\n\r\n~/anaconda3/lib/python3.6/site-packages/h5py/_hl/group.py in create_dataset(self, name, shape, dtype, data, **kwds)\r\n    117             dset = dataset.Dataset(dsid)\r\n    118             if name is not None:\r\n--> 119                 self[name] = dset\r\n    120             return dset\r\n    121 \r\n\r\n~/anaconda3/lib/python3.6/site-packages/h5py/_hl/group.py in __setitem__(self, name, obj)\r\n    285 \r\n    286             if isinstance(obj, HLObject):\r\n--> 287                 h5o.link(obj.id, self.id, name, lcpl=lcpl, lapl=self._lapl)\r\n    288 \r\n    289             elif isinstance(obj, SoftLink):\r\n\r\nh5py/_objects.pyx in h5py._objects.with_phil.wrapper()\r\n\r\nh5py/_objects.pyx in h5py._objects.with_phil.wrapper()\r\n\r\nh5py/h5o.pyx in h5py.h5o.link()\r\n\r\nRuntimeError: Unable to create link (name already exists)\r\n\r\n', '', 'save_weights_to_hdf5_group', 'layer', 'weight_names', '', '\r\n# Save ', "":\r\ninput_1: []\r\nconv2d: [b'conv2d/kernel:0', b'conv2d/bias:0']\r\n# Save "", "":\r\ninput_2: []\r\ninput_3: []\r\nmodel: [b'conv2d/kernel:0', b'conv2d/bias:0']\r\nmodel_1: [b'conv2d/kernel:0', b'conv2d/bias:0']\r\nadd: []\r\n# Save "", "":\r\ninput_4: []\r\ninput_5: []\r\nmodel_2: [b'conv2d/kernel:0', b'conv2d/bias:0', b'conv2d/kernel:0', b'conv2d/bias:0']\r\n"", '']",0,0
267,keras,8331,closed,StopIteration: Tensor [...] is not an element of this graph.,"I have recently started getting  errors when running my training  (it's a bit confusing because because this wasn't happening before and I'm not sure what code introduced the issue).

I believe it's a thread related issue (I'm using the model that I am training from inside my training data generator). I was wondering if there's a way to use the model I'm training from inside my generator while avoiding this issue.

Full stack trace:



Related: https://github.com/fchollet/keras/issues/6462 https://github.com/fchollet/keras/issues/5511 https://github.com/fchollet/keras/issues/2397",,['Calling `model._make_predict_function()` after loading/creating the model seemed to solve this issue.'],"['python\r\nEpoch 1/30\r\nTraceback (most recent call last):\r\n  File ""/home/ubuntu/.pyenv/versions/dir/lib/python3.6/site-packages/keras/utils/data_utils.py"", line 505, in get\r\n    inputs = self.queue.get(block=True).get()\r\n  File ""/home/ubuntu/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/pool.py"", line 644, in get\r\n    raise self._value\r\n  File ""/home/ubuntu/.pyenv/versions/3.6.2/lib/python3.6/multiprocessing/pool.py"", line 119, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File ""/home/ubuntu/.pyenv/versions/dir/lib/python3.6/site-packages/keras/utils/data_utils.py"", line 371, in get_index\r\n    return ds[i]\r\n  File ""/home/ubuntu/code/dir/dir/datagen.py"", line 148, in __getitem__\r\n    negatives = self.generate_negatives(anchors, paths)\r\n  File ""/home/ubuntu/code/dir/dir/datagen.py"", line 127, in generate_negatives\r\n    return self.generate_hard_negatives(images, paths)\r\n  File ""/home/ubuntu/code/dir/dir/datagen.py"", line 103, in generate_hard_negatives\r\n    vectors = self.model.predict(images)\r\n  File ""/home/ubuntu/.pyenv/versions/dir/lib/python3.6/site-packages/keras/engine/training.py"", line 1710, in predict\r\n    self._make_predict_function()\r\n  File ""/home/ubuntu/.pyenv/versions/dir/lib/python3.6/site-packages/keras/engine/training.py"", line 999, in _make_predict_function\r\n    **kwargs)\r\n  File ""/home/ubuntu/.pyenv/versions/dir/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 2297, in function\r\n    return Function(inputs, outputs, updates=updates, **kwargs)\r\n  File ""/home/ubuntu/.pyenv/versions/dir/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 2246, in __init__\r\n    with tf.control_dependencies(self.outputs):\r\n  File ""/home/ubuntu/.pyenv/versions/dir/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3936, in control_dependencies\r\n    return get_default_graph().control_dependencies(control_inputs)\r\n  File ""/home/ubuntu/.pyenv/versions/dir/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3665, in control_dependencies\r\n    c = self.as_graph_element(c)\r\n  File ""/home/ubuntu/.pyenv/versions/dir/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2708, in as_graph_element\r\n    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\n  File ""/home/ubuntu/.pyenv/versions/dir/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2787, in _as_graph_element_locked\r\n    raise ValueError(""Tensor %s is not an element of this graph."" % obj)\r\nValueError: Tensor Tensor(""l2_norm/l2_normalize:0"", shape=(?, 2048), dtype=float32) is not an element of this graph.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""./scripts/train"", line 197, in <module>\r\n    train(**vars(parser.parse_args()))\r\n  File ""./scripts/train"", line 117, in train\r\n    initial_epoch=initial_epoch,\r\n  File ""/home/ubuntu/.pyenv/versions/dir/lib/python3.6/site-packages/keras/legacy/interfaces.py"", line 87, in wrapper\r\n    return func(*args, **kwargs)\r\n  File ""/home/ubuntu/.pyenv/versions/dir/lib/python3.6/site-packages/keras/engine/training.py"", line 2011, in fit_generator\r\n    generator_output = next(output_generator)\r\n  File ""/home/ubuntu/.pyenv/versions/dir/lib/python3.6/site-packages/keras/utils/data_utils.py"", line 510, in get\r\n    raise StopIteration(e)\r\nStopIteration: Tensor Tensor(""l2_norm/l2_normalize:0"", shape=(?, 2048), dtype=float32) is not an element of this graph.\r\n']","['StopIteration: Tensor Tensor(""l2_norm/l2_normalize:0"", shape=(?, 2048), dtype=float32) is not an element of this graph', 'fit_generator']",0,0
268,keras,10284,closed,Support file objects in ModelCheckpoint and Model.save,"[all boxes checked, latest version of Keras etc.]

I want to run keras in a production system where files are stored in a distributed filesystem, and it's quite limiting that you can only pass a filename (string) to ModelCheckpoint and Model.save.

Can we add an option to use a file object instead of passing a filename? This would be uesful for cases where you want to save checkpoints to some cloud storage, or want to handle them in memory (pass a StringIO object).",,[],[],[],0,0
269,keras,13335,closed,Side effect of tf.global_variables_initializer on evaluation of output model,"**System information**  
- Have I written custom code that you can find here :




- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  1.8.0
- Keras version:  2.1.5
- Python version:  3.6

When I use sess.run to obtain the output of a model (here the first layer of a pretrained VGG19), I obtain a different output that the one obtained by using the predict function of the model. I am looking at an inference output. There is no training at all. All the weights are freeze and not trainable.

Moreover each time, I run tf.global_variables_initializer, I will obtain a different result whereas there is no stochastic element in the code.

You can find a minimal code above.

",,[],[],"[""\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras.preprocessing import image as kp_image\r\nfrom tensorflow.python.keras import models \r\nimport numpy as np\r\n\r\ndef minimalCase_forIssue():\r\n    max_dim = 224\r\n    path_to_img =  'Q23898.jpg' # Need to change with the name of your image\r\n    img = tf.keras.preprocessing.image.load_img(\r\n        path_to_img,\r\n        grayscale=False,\r\n        target_size=(max_dim, max_dim),\r\n        interpolation='nearest')\r\n    img = kp_image.img_to_array(img)\r\n    img = np.expand_dims(img, axis=0) # Should be replace by expand_dims in tf\r\n    img = tf.keras.applications.vgg19.preprocess_input(img)\r\n    img_tensor = tf.convert_to_tensor(img)\r\n    \r\n    vgg = tf.keras.applications.vgg19.VGG19(include_top=False, weights='imagenet')\r\n    vgg.trainable = False\r\n    output = vgg.get_layer('block1_conv1').output\r\n    model = models.Model(vgg.input, output)\r\n    sess = tf.Session()\r\n    \r\n    for i in range(2):\r\n        sess.run(tf.global_variables_initializer())\r\n        evaluation_by_sess = sess.run(model(img_tensor))\r\n        print('With sess.run :',evaluation_by_sess[0,0,0,:])\r\n        evaluation_by_predict = model.predict(img,batch_size=1)\r\n        print('With .predict() :',evaluation_by_predict[0,0,0,:])\r\n    sess.close()\r\n    \r\nif __name__ == '__main__':\r\n    minimalCase_forIssue()\r\n\r\n""]",0,0
270,keras,13562,closed,How to set cpu core number in tensorflow 2.0,"I want to limit the cpu usage of keras training program. I found this kaggle document(https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/43383):



But with tensorflow 2.0, the ConfigProto and Session are deprecated. ",type:support,[],"[""\r\nfrom keras import backend as K\r\nimport tensorflow as tf\r\n\r\nconfig = tf.ConfigProto(intra_op_parallelism_threads=args.jobs, \\ \r\n                        inter_op_parallelism_threads=args.jobs, \\\r\n                        allow_soft_placement=True, \\\r\n                        device_count = {'CPU': args.jobs})\r\nsession = tf.Session(config=config)\r\nK.set_session(session)\r\n""]",[],0,0
271,keras,9407,closed,Replacing tensor values on specific indices ,"I am building my own objective function:

> def my_loss(y_true, y_pred):
>     y_true = tf.reshape(y_true,[-1,256*256,14])     #shape: (num_images, pixels, channels)
>     sum = K.sum(y_true,axis=1)
>     indices = K.tf.where(K.equal(sum, 0))   #Get all the indices where sum of y_true is 0 for each image/channel
>     y_true[ind] = y_pred[ind]     
>     return keras.losses.binary_crossentropy(y_true, y_pred)

What I am not able to figure out how to assign those  of y_true with the values of the same  of y_pred. Basically I cannot do this:  

Any help would be very much appreciated.",,"[""I also tried this simpler version :\r\n\r\n```\r\ndef my_loss(y_true, y_pred):\r\n    sum = K.sum(y_true,axis=1) # No need to reshape because shape: (num_images, pixels, channels). So do the summation on axis 1 where the pixels are\r\n    y_true = K.switch(K.equal(sum, 0), y_pred, y_true) #Check if sum is 0 replace these elements with y_pred\r\n    return keras.losses.binary_crossentropy(y_true, y_pred)\r\n```\r\n\r\nThe model compiles but it gives me an error when running the experiment:\r\n\r\n`InvalidArgumentError (see above for traceback): Inputs to operation loss/activation_19_loss/Select_1 of type Select must have the same size and shape.  Input 0: [4,917504,14] != input 1: [4,65536,14]`\r\n\r\nI don't get where the `917504` came from. I think it came from `K.equal(sum, 0)`\r\n""]",[],"['ind', 'ind', 'y_true[ind] = y_pred[ind] ']",0,0
272,keras,10538,closed,fit_generator with gpu cache,"hi guys, i'm using a autoencoder, and i batch with 128 samples. could be possible send more data to gpu and execute batch faster? i think i~m with some bottleneck at copy, i see dedicated gpu memory changing from 8GB to 0GB at windows, many many times, and gpu% (smi) stay at ~30% and memory copy ~5%

the idea is something like 

multi_gpu_model.fit_generator(
        dae_generator(),
        steps_per_epoch=ceil(len(df)/(batch_size*2)),
        cache_batchs=100, # for example send 100x data
        epochs=1000, 
        verbose=2)


i'm using tensorflow backend",,[],[],[],0,0
273,keras,9566,closed,Keras 2.0.8 model.fit() out of memory,"Hi, dear all,

I know 2.0.8 is a little bit old, however, I observe an issue with 2.0.8 compared with 2.0.4. I don't if it is fixed in the latest version, if not, hope it will give you some information for next updates.

When I run the code with 2.0.8, it will pop out the error message like below:


Traceback (most recent call last):
  File ""/home/fxt120230/Study/MSP/Research/VisVAD/Samsung_VAD/Fusion_Part/ETE_VAD/vadete_run.py"", line 316, in <module>
    run_ete_exp(para_dict)
  File ""/home/fxt120230/Study/MSP/Research/VisVAD/Samsung_VAD/Fusion_Part/ETE_VAD/vadete_run.py"", line 112, in run_ete_exp
    validation_data=([aud_valid,vid_valid],valid_truth_mem,valid_weight),sample_weight=train_weight,verbose=verbose)
  File ""build/bdist.linux-x86_64/egg/keras/engine/training.py"", line 1601, in fit
  File ""build/bdist.linux-x86_64/egg/keras/engine/training.py"", line 1183, in _fit_loop
  File ""build/bdist.linux-x86_64/egg/keras/backend/theano_backend.py"", line 1223, in __call__
  File ""/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py"", line 917, in __call__
    storage_map=getattr(self.fn, 'storage_map', None))
  File ""/usr/local/lib/python2.7/dist-packages/theano/gof/link.py"", line 325, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File ""/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py"", line 903, in __call__
    self.fn() if output_subset is None else\
  File ""pygpu/gpuarray.pyx"", line 693, in pygpu.gpuarray.pygpu_empty (pygpu/gpuarray.c:9893)
  File ""pygpu/gpuarray.pyx"", line 301, in pygpu.gpuarray.array_empty (pygpu/gpuarray.c:5694)
pygpu.gpuarray.GpuArrayException: cuMemAlloc: CUDA_ERROR_OUT_OF_MEMORY: out of memory
Apply node that caused the error: GpuAllocEmpty{dtype='int32', context_name=None}(Elemwise{Composite{(Switch(LT(maximum(i0, i1), i2), (maximum(i0, i1) + i3), (maximum(i0, i1) - i4)) + i5)}}.0, Shape_i{0}.0, Shape_i{1}.0)
Toposort index: 1656
Inputs types: [TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar)]
Inputs shapes: [(), (), ()]
Inputs strides: [(), (), ()]
Inputs values: [array(2722), array(15360), array(6)]
Outputs clients: [[GpuIncSubtensor{InplaceSet;:int64:}(GpuAllocEmpty{dtype='int32', context_name=None}.0, Rebroadcast{0}.0, Constant{1})]]

HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.
HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.


The nework parameters are listed in the following table:


__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 2721, 5)      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 2721, 23)     0                                            
__________________________________________________________________________________________________
masking_1 (Masking)             (None, 2721, 5)      0           input_1[0][0]                    
__________________________________________________________________________________________________
masking_2 (Masking)             (None, 2721, 23)     0           input_2[0][0]                    
__________________________________________________________________________________________________
time_distributed_1 (TimeDistrib (None, 2721, 16)     288         masking_1[0][0]                  
__________________________________________________________________________________________________
time_distributed_3 (TimeDistrib (None, 2721, 64)     4608        masking_2[0][0]                  
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 2721, 16)     0           time_distributed_1[0][0]         
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 2721, 64)     0           time_distributed_3[0][0]         
__________________________________________________________________________________________________
time_distributed_2 (TimeDistrib (None, 2721, 16)     816         dropout_1[0][0]                  
__________________________________________________________________________________________________
time_distributed_4 (TimeDistrib (None, 2721, 64)     12480       dropout_4[0][0]                  
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 2721, 16)     0           time_distributed_2[0][0]         
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 2721, 64)     0           time_distributed_4[0][0]         
__________________________________________________________________________________________________
lstm_1 (LSTM)                   (None, 2721, 16)     2112        dropout_2[0][0]                  
__________________________________________________________________________________________________
lstm_3 (LSTM)                   (None, 2721, 64)     33024       dropout_5[0][0]                  
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 2721, 16)     0           lstm_1[0][0]                     
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 2721, 64)     0           lstm_3[0][0]                     
__________________________________________________________________________________________________
bidirectional_1 (Bidirectional) (None, 2721, 32)     4224        dropout_3[0][0]                  
__________________________________________________________________________________________________
bidirectional_2 (Bidirectional) (None, 2721, 128)    66048       dropout_6[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 2721, 160)    0           bidirectional_1[0][0]            
                                                                 bidirectional_2[0][0]            
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 2721, 160)    0           concatenate_1[0][0]              
__________________________________________________________________________________________________
bidirectional_3 (Bidirectional) (None, 2721, 256)    295936      dropout_7[0][0]                  
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 2721, 256)    0           bidirectional_3[0][0]            
__________________________________________________________________________________________________
bidirectional_4 (Bidirectional) (None, 2721, 256)    394240      dropout_8[0][0]                  
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 2721, 256)    0           bidirectional_4[0][0]            
__________________________________________________________________________________________________
time_distributed_5 (TimeDistrib (None, 2721, 128)    98688       dropout_9[0][0]                  
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 2721, 128)    0           time_distributed_5[0][0]         
__________________________________________________________________________________________________
time_distributed_6 (TimeDistrib (None, 2721, 2)      258         dropout_10[0][0]                 
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 2721, 2)      0           time_distributed_6[0][0]         
==================================================================================================
Total params: 912,722
Trainable params: 912,722
Non-trainable params: 0
__________________________________________________________________________________________________


I am using theano 1.0.1. The GPU setting is printed below:
Using Theano backend.
Using cuDNN version 5110 on context None
Mapped name None to device cuda: GeForce GTX 1070 (0000:04:00.0)

When I switch it back to 2.0.4 without other change, the program can run successfully. Could anyone tell the reason and how to fix it?

Thank you very much!",,[],[],[],0,0
274,keras,8546,closed,"OSError: Unable to open file (truncated file: eof = 98304, sblock->base_addr = 0, stored_eoa = 102853048) removing the model from keras/model didn't solve","I am not sure how to fix this error. Can you please guide? found the code from @flyyufelix on 
https://github.com/flyyufelix/cnn_finetune/blob/master/resnet_50.py




I get this error:
Conv2DConv2D(512, (3, 3), name=""res5b_branch2b"", padding=""same"")Conv2DConv2D(2048, (1, 1), name=""res5b_branch2c"")Conv2DConv2D(512, (1, 1), name=""res5c_branch2a"")Conv2DConv2D(512, (3, 3), name=""res5c_branch2b"", padding=""same"")Conv2DConv2D(2048, (1, 1), name=""res5c_branch2c"")Conv2DConv2D(64, (7, 7), name=""conv1"", strides=(2, 2))Conv2DConv2D(64, (1, 1), name=""res2a_branch2a"", strides=(1, 1))Conv2DConv2D(64, (3, 3), name=""res2a_branch2b"", padding=""same"")Conv2DConv2D(256, (1, 1), name=""res2a_branch2c"")Conv2DConv2D(256, (1, 1), name=""res2a_branch1"", strides=(1, 1))mergekeras.layers.mergeaddconcatenateMergekeras.layers.mergeaddconcatenateConv2DConv2D(64, (1, 1), name=""res2b_branch2a"")Conv2DConv2D(64, (3, 3), name=""res2b_branch2b"", padding=""same"")Conv2DConv2D(256, (1, 1), name=""res2b_branch2c"")mergekeras.layers.mergeaddconcatenateConv2DConv2D(64, (1, 1), name=""res2c_branch2a"")Conv2DConv2D(64, (3, 3), name=""res2c_branch2b"", padding=""same"")Conv2DConv2D(256, (1, 1), name=""res2c_branch2c"")Conv2DConv2D(128, (1, 1), name=""res3a_branch2a"", strides=(2, 2))Conv2DConv2D(128, (3, 3), name=""res3a_branch2b"", padding=""same"")Conv2DConv2D(512, (1, 1), name=""res3a_branch2c"")Conv2DConv2D(512, (1, 1), name=""res3a_branch1"", strides=(2, 2))Conv2DConv2D(128, (1, 1), name=""res3b_branch2a"")Conv2DConv2D(128, (3, 3), name=""res3b_branch2b"", padding=""same"")Conv2DConv2D(512, (1, 1), name=""res3b_branch2c"")Conv2DConv2D(128, (1, 1), name=""res3c_branch2a"")Conv2DConv2D(128, (3, 3), name=""res3c_branch2b"", padding=""same"")Conv2DConv2D(512, (1, 1), name=""res3c_branch2c"")Conv2DConv2D(128, (1, 1), name=""res3d_branch2a"")Conv2DConv2D(128, (3, 3), name=""res3d_branch2b"", padding=""same"")Conv2DConv2D(512, (1, 1), name=""res3d_branch2c"")Conv2DConv2D(256, (1, 1), name=""res4a_branch2a"", strides=(2, 2))Conv2DConv2D(256, (3, 3), name=""res4a_branch2b"", padding=""same"")Conv2DConv2D(1024, (1, 1), name=""res4a_branch2c"")Conv2DConv2D(1024, (1, 1), name=""res4a_branch1"", strides=(2, 2))Conv2DConv2D(256, (1, 1), name=""res4b_branch2a"")Conv2DConv2D(256, (3, 3), name=""res4b_branch2b"", padding=""same"")Conv2DConv2D(1024, (1, 1), name=""res4b_branch2c"")Conv2DConv2D(256, (1, 1), name=""res4c_branch2a"")Conv2DConv2D(256, (3, 3), name=""res4c_branch2b"", padding=""same"")Conv2DConv2D(1024, (1, 1), name=""res4c_branch2c"")Conv2DConv2D(256, (1, 1), name=""res4d_branch2a"")Conv2DConv2D(256, (3, 3), name=""res4d_branch2b"", padding=""same"")Conv2DConv2D(1024, (1, 1), name=""res4d_branch2c"")Conv2DConv2D(256, (1, 1), name=""res4e_branch2a"")Conv2DConv2D(256, (3, 3), name=""res4e_branch2b"", padding=""same"")Conv2DConv2D(1024, (1, 1), name=""res4e_branch2c"")Conv2DConv2D(256, (1, 1), name=""res4f_branch2a"")Conv2DConv2D(256, (3, 3), name=""res4f_branch2b"", padding=""same"")Conv2DConv2D(1024, (1, 1), name=""res4f_branch2c"")Conv2DConv2D(512, (1, 1), name=""res5a_branch2a"", strides=(2, 2))Conv2DConv2D(512, (3, 3), name=""res5a_branch2b"", padding=""same"")Conv2DConv2D(2048, (1, 1), name=""res5a_branch2c"")Conv2DConv2D(2048, (1, 1), name=""res5a_branch1"", strides=(2, 2))Conv2DConv2D(512, (1, 1), name=""res5b_branch2a"")Conv2DConv2D(512, (3, 3), name=""res5b_branch2b"", padding=""same"")Conv2DConv2D(2048, (1, 1), name=""res5b_branch2c"")Conv2DConv2D(512, (1, 1), name=""res5c_branch2a"")Conv2DConv2D(512, (3, 3), name=""res5c_branch2b"", padding=""same"")Conv2DConv2D(2048, (1, 1), name=""res5c_branch2c"")

",,[],"['\r\nfrom keras.models import Sequential\r\nfrom keras.optimizers import SGD\r\nfrom keras.layers import Input, Dense, Convolution2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Dropout, Flatten, merge, Reshape, Activation\r\nfrom keras.layers.normalization import BatchNormalization\r\nfrom keras.models import Model\r\nfrom keras import backend as K\r\n\r\nfrom sklearn.metrics import log_loss\r\n\r\nfrom load_cifar10 import load_cifar10_data\r\n\r\ndef identity_block(input_tensor, kernel_size, filters, stage, block):\r\n    """"""\r\n    The identity_block is the block that has no conv layer at shortcut\r\n    Arguments\r\n        input_tensor: input tensor\r\n        kernel_size: defualt 3, the kernel size of middle conv layer at main path\r\n        filters: list of integers, the nb_filters of 3 conv layer at main path\r\n        stage: integer, current stage label, used for generating layer names\r\n        block: \'a\',\'b\'..., current block label, used for generating layer names\r\n    """"""\r\n\r\n    nb_filter1, nb_filter2, nb_filter3 = filters\r\n    conv_name_base = \'res\' + str(stage) + block + \'_branch\'\r\n    bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\r\n\r\n    x = Convolution2D(nb_filter1, 1, 1, name=conv_name_base + \'2a\')(input_tensor)\r\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + \'2a\')(x)\r\n    x = Activation(\'relu\')(x)\r\n\r\n    x = Convolution2D(nb_filter2, kernel_size, kernel_size,\r\n                      border_mode=\'same\', name=conv_name_base + \'2b\')(x)\r\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + \'2b\')(x)\r\n    x = Activation(\'relu\')(x)\r\n\r\n    x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + \'2c\')(x)\r\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + \'2c\')(x)\r\n\r\n    x = merge([x, input_tensor], mode=\'sum\')\r\n    x = Activation(\'relu\')(x)\r\n    return x\r\n\r\ndef conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\r\n    """"""\r\n    conv_block is the block that has a conv layer at shortcut\r\n    # Arguments\r\n        input_tensor: input tensor\r\n        kernel_size: defualt 3, the kernel size of middle conv layer at main path\r\n        filters: list of integers, the nb_filters of 3 conv layer at main path\r\n        stage: integer, current stage label, used for generating layer names\r\n        block: \'a\',\'b\'..., current block label, used for generating layer names\r\n    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\r\n    And the shortcut should have subsample=(2,2) as well\r\n    """"""\r\n\r\n    nb_filter1, nb_filter2, nb_filter3 = filters\r\n    conv_name_base = \'res\' + str(stage) + block + \'_branch\'\r\n    bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\r\n\r\n    x = Convolution2D(nb_filter1, 1, 1, subsample=strides,\r\n                      name=conv_name_base + \'2a\')(input_tensor)\r\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + \'2a\')(x)\r\n    x = Activation(\'relu\')(x)\r\n\r\n    x = Convolution2D(nb_filter2, kernel_size, kernel_size, border_mode=\'same\',\r\n                      name=conv_name_base + \'2b\')(x)\r\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + \'2b\')(x)\r\n    x = Activation(\'relu\')(x)\r\n\r\n    x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + \'2c\')(x)\r\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + \'2c\')(x)\r\n\r\n    shortcut = Convolution2D(nb_filter3, 1, 1, subsample=strides,\r\n                             name=conv_name_base + \'1\')(input_tensor)\r\n    shortcut = BatchNormalization(axis=bn_axis, name=bn_name_base + \'1\')(shortcut)\r\n\r\n    x = merge([x, shortcut], mode=\'sum\')\r\n    x = Activation(\'relu\')(x)\r\n    return x\r\n\r\ndef resnet50_model(img_rows, img_cols, color_type=1, num_classes=None):\r\n    """"""\r\n    Resnet 50 Model for Keras\r\n\r\n    Model Schema is based on \r\n    https://github.com/fchollet/deep-learning-models/blob/master/resnet50.py\r\n\r\n    ImageNet Pretrained Weights \r\n    https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_th_dim_ordering_th_kernels.h5\r\n\r\n    Parameters:\r\n      img_rows, img_cols - resolution of inputs\r\n      channel - 1 for grayscale, 3 for color \r\n      num_classes - number of class labels for our classification task\r\n    """"""\r\n\r\n    # Handle Dimension Ordering for different backends\r\n    global bn_axis\r\n    if K.image_dim_ordering() == \'tf\':\r\n      bn_axis = 3\r\n      img_input = Input(shape=(img_rows, img_cols, color_type))\r\n    else:\r\n      bn_axis = 1\r\n      img_input = Input(shape=(color_type, img_rows, img_cols))\r\n\r\n    x = ZeroPadding2D((3, 3))(img_input)\r\n    x = Convolution2D(64, 7, 7, subsample=(2, 2), name=\'conv1\')(x)\r\n    x = BatchNormalization(axis=bn_axis, name=\'bn_conv1\')(x)\r\n    x = Activation(\'relu\')(x)\r\n    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\r\n\r\n    x = conv_block(x, 3, [64, 64, 256], stage=2, block=\'a\', strides=(1, 1))\r\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block=\'b\')\r\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block=\'c\')\r\n\r\n    x = conv_block(x, 3, [128, 128, 512], stage=3, block=\'a\')\r\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block=\'b\')\r\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block=\'c\')\r\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block=\'d\')\r\n\r\n    x = conv_block(x, 3, [256, 256, 1024], stage=4, block=\'a\')\r\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\'b\')\r\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\'c\')\r\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\'d\')\r\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\'e\')\r\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\'f\')\r\n\r\n    x = conv_block(x, 3, [512, 512, 2048], stage=5, block=\'a\')\r\n    x = identity_block(x, 3, [512, 512, 2048], stage=5, block=\'b\')\r\n    x = identity_block(x, 3, [512, 512, 2048], stage=5, block=\'c\')\r\n\r\n    # Fully Connected Softmax Layer\r\n    x_fc = AveragePooling2D((7, 7), name=\'avg_pool\')(x)\r\n    x_fc = Flatten()(x_fc)\r\n    x_fc = Dense(1000, activation=\'softmax\', name=\'fc1000\')(x_fc)\r\n\r\n    # Create model\r\n    model = Model(img_input, x_fc)\r\n\r\n    # Load ImageNet pre-trained data \r\n    if K.image_dim_ordering() == \'th\':\r\n      # Use pre-trained weights for Theano backend\r\n      weights_path = \'imagenet_models/resnet50_weights_th_dim_ordering_th_kernels.h5\'\r\n    else:\r\n      # Use pre-trained weights for Tensorflow backend\r\n      weights_path = \'imagenet_models/resnet50_weights_tf_dim_ordering_tf_kernels.h5\'\r\n\r\n    model.load_weights(weights_path)\r\n\r\n    # Truncate and replace softmax layer for transfer learning\r\n    # Cannot use model.layers.pop() since model is not of Sequential() type\r\n    # The method below works since pre-trained weights are stored in layers but not in the model\r\n    x_newfc = AveragePooling2D((7, 7), name=\'avg_pool\')(x)\r\n    x_newfc = Flatten()(x_newfc)\r\n    x_newfc = Dense(num_classes, activation=\'softmax\', name=\'fc10\')(x_newfc)\r\n\r\n    # Create another model with our customized softmax\r\n    model = Model(img_input, x_newfc)\r\n\r\n    # Learning rate is changed to 0.001\r\n    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\r\n    model.compile(optimizer=sgd, loss=\'categorical_crossentropy\', metrics=[\'accuracy\'])\r\n  \r\n    return model\r\n\r\nif __name__ == \'__main__\':\r\n\r\n    # Example to fine-tune on 3000 samples from Cifar10\r\n\r\n    img_rows, img_cols = 224, 224 # Resolution of inputs\r\n    channel = 3\r\n    num_classes = 10 \r\n    batch_size = 16 \r\n    nb_epoch = 10\r\n\r\n    # Load Cifar10 data. Please implement your own load_data() module for your own dataset\r\n    X_train, Y_train, X_valid, Y_valid = load_cifar10_data(img_rows, img_cols)\r\n\r\n    # Load our model\r\n    model = resnet50_model(img_rows, img_cols, channel, num_classes)\r\n\r\n    # Start Fine-tuning\r\n    model.fit(X_train, Y_train,\r\n              batch_size=batch_size,\r\n              nb_epoch=nb_epoch,\r\n              shuffle=True,\r\n              verbose=1,\r\n              validation_data=(X_valid, Y_valid),\r\n              )\r\n\r\n    # Make predictions\r\n    predictions_valid = model.predict(X_valid, batch_size=batch_size, verbose=1)\r\n    print(predictions)\r\n    # Cross-entropy loss score\r\n    score = log_loss(Y_valid, predictions_valid)\r\n    print(score)\r\n', ""\r\n\r\nI used the suggestion here, deleted the model and ran the code again. Now the model doesn't get downloaded and I also get the same error:\r\n\r\nhttps://github.com/fchollet/keras/issues/6221#issuecomment-326733884\r\n\r\n""]","['', '\r\nresnet_50.py:32: UserWarning: Update your ', ' call to the Keras 2 API: ', ""\r\n  border_mode='same', name=conv_name_base + '2b')(x)\r\nresnet_50.py:36: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c')(x)\r\nresnet_50.py:27: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter1, 1, 1, name=conv_name_base + '2a')(input_tensor)\r\nresnet_50.py:32: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  border_mode='same', name=conv_name_base + '2b')(x)\r\nresnet_50.py:36: UserWarning: Update your "", ' call to the Keras 2 API: ', '\r\n  x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + \'2c\')(x)\r\nTraceback (most recent call last):\r\n  File ""resnet_50.py"", line 180, in <module>\r\n    model = resnet50_model(img_rows, img_cols, channel, num_classes)\r\n  File ""resnet_50.py"", line 148, in resnet50_model\r\n    model.load_weights(weights_path)\r\n  File ""/usr4/cs542/jalal/.conda/envs/tensorflow/lib/python3.6/site-packages/keras/engine/topology.py"", line 2616, in load_weights\r\n    f = h5py.File(filepath, mode=\'r\')\r\n  File ""/usr4/cs542/jalal/.conda/envs/tensorflow/lib/python3.6/site-packages/h5py/_hl/files.py"", line 269, in __init__\r\n    fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)\r\n  File ""/usr4/cs542/jalal/.conda/envs/tensorflow/lib/python3.6/site-packages/h5py/_hl/files.py"", line 99, in make_fid\r\n    fid = h5f.open(name, flags, fapl=fapl)\r\n  File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper\r\n  File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper\r\n  File ""h5py/h5f.pyx"", line 78, in h5py.h5f.open\r\nOSError: Unable to open file (truncated file: eof = 98304, sblock->base_addr = 0, stored_eoa = 102853048)\r\n\r\n\r\n(tensorflow) [jalal@scc-c08 test_mona]$ cd ~/.keras/models\r\n(tensorflow) [jalal@scc-c08 models]$ ls\r\n(tensorflow) [jalal@scc-c08 models]$ cd_ml \r\n(tensorflow) [jalal@scc-c08 MHRN]$ cd test_mona/\r\n(tensorflow) [jalal@scc-c08 test_mona]$ ls\r\n__pycache__  imagenet_models  load_cifar10.py  resnet_50.py\r\n(tensorflow) [jalal@scc-c08 test_mona]$ python resnet_50.py \r\nUsing TensorFlow backend.\r\n2017-11-20 21:00:18.497760: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn\'t compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-11-20 21:00:18.497784: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn\'t compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-11-20 21:00:18.497790: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn\'t compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-11-20 21:00:18.497794: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn\'t compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-11-20 21:00:18.497798: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn\'t compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-11-20 21:00:18.759714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: \r\nname: Tesla P100-PCIE-12GB\r\nmajor: 6 minor: 0 memoryClockRate (GHz) 1.3285\r\npciBusID 0000:02:00.0\r\nTotal memory: 11.94GiB\r\nFree memory: 11.65GiB\r\n2017-11-20 21:00:18.759743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 \r\n2017-11-20 21:00:18.759750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y \r\n2017-11-20 21:00:18.759759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-12GB, pci bus id: 0000:02:00.0)\r\nresnet_50.py:107: UserWarning: Update your ', ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(64, 7, 7, subsample=(2, 2), name='conv1')(x)\r\nresnet_50.py:61: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  name=conv_name_base + '2a')(input_tensor)\r\nresnet_50.py:66: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  name=conv_name_base + '2b')(x)\r\nresnet_50.py:70: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c')(x)\r\nresnet_50.py:74: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  name=conv_name_base + '1')(input_tensor)\r\nresnet_50.py:77: UserWarning: The "", ' function is deprecated and will be removed after 08/2017. Use instead layers from ', ', e.g. ', ', ', "", etc.\r\n  x = merge([x, shortcut], mode='sum')\r\n/usr4/cs542/jalal/.conda/envs/tensorflow/lib/python3.6/site-packages/keras/legacy/layers.py:458: UserWarning: The "", ' layer is deprecated and will be removed after 08/2017. Use instead layers from ', ', e.g. ', ', ', ', etc.\r\n  name=name)\r\nresnet_50.py:27: UserWarning: Update your ', ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter1, 1, 1, name=conv_name_base + '2a')(input_tensor)\r\nresnet_50.py:32: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  border_mode='same', name=conv_name_base + '2b')(x)\r\nresnet_50.py:36: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c')(x)\r\nresnet_50.py:39: UserWarning: The "", ' function is deprecated and will be removed after 08/2017. Use instead layers from ', ', e.g. ', ', ', "", etc.\r\n  x = merge([x, input_tensor], mode='sum')\r\nresnet_50.py:27: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter1, 1, 1, name=conv_name_base + '2a')(input_tensor)\r\nresnet_50.py:32: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  border_mode='same', name=conv_name_base + '2b')(x)\r\nresnet_50.py:36: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c')(x)\r\nresnet_50.py:61: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  name=conv_name_base + '2a')(input_tensor)\r\nresnet_50.py:66: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  name=conv_name_base + '2b')(x)\r\nresnet_50.py:70: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c')(x)\r\nresnet_50.py:74: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  name=conv_name_base + '1')(input_tensor)\r\nresnet_50.py:27: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter1, 1, 1, name=conv_name_base + '2a')(input_tensor)\r\nresnet_50.py:32: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  border_mode='same', name=conv_name_base + '2b')(x)\r\nresnet_50.py:36: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c')(x)\r\nresnet_50.py:27: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter1, 1, 1, name=conv_name_base + '2a')(input_tensor)\r\nresnet_50.py:32: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  border_mode='same', name=conv_name_base + '2b')(x)\r\nresnet_50.py:36: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c')(x)\r\nresnet_50.py:27: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter1, 1, 1, name=conv_name_base + '2a')(input_tensor)\r\nresnet_50.py:32: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  border_mode='same', name=conv_name_base + '2b')(x)\r\nresnet_50.py:36: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c')(x)\r\nresnet_50.py:61: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  name=conv_name_base + '2a')(input_tensor)\r\nresnet_50.py:66: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  name=conv_name_base + '2b')(x)\r\nresnet_50.py:70: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c')(x)\r\nresnet_50.py:74: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  name=conv_name_base + '1')(input_tensor)\r\nresnet_50.py:27: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter1, 1, 1, name=conv_name_base + '2a')(input_tensor)\r\nresnet_50.py:32: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  border_mode='same', name=conv_name_base + '2b')(x)\r\nresnet_50.py:36: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c')(x)\r\nresnet_50.py:27: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter1, 1, 1, name=conv_name_base + '2a')(input_tensor)\r\nresnet_50.py:32: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  border_mode='same', name=conv_name_base + '2b')(x)\r\nresnet_50.py:36: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c')(x)\r\nresnet_50.py:27: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter1, 1, 1, name=conv_name_base + '2a')(input_tensor)\r\nresnet_50.py:32: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  border_mode='same', name=conv_name_base + '2b')(x)\r\nresnet_50.py:36: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c')(x)\r\nresnet_50.py:27: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter1, 1, 1, name=conv_name_base + '2a')(input_tensor)\r\nresnet_50.py:32: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  border_mode='same', name=conv_name_base + '2b')(x)\r\nresnet_50.py:36: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c')(x)\r\nresnet_50.py:27: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter1, 1, 1, name=conv_name_base + '2a')(input_tensor)\r\nresnet_50.py:32: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  border_mode='same', name=conv_name_base + '2b')(x)\r\nresnet_50.py:36: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c')(x)\r\nresnet_50.py:61: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  name=conv_name_base + '2a')(input_tensor)\r\nresnet_50.py:66: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  name=conv_name_base + '2b')(x)\r\nresnet_50.py:70: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c')(x)\r\nresnet_50.py:74: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  name=conv_name_base + '1')(input_tensor)\r\nresnet_50.py:27: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter1, 1, 1, name=conv_name_base + '2a')(input_tensor)\r\nresnet_50.py:32: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  border_mode='same', name=conv_name_base + '2b')(x)\r\nresnet_50.py:36: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c')(x)\r\nresnet_50.py:27: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  x = Convolution2D(nb_filter1, 1, 1, name=conv_name_base + '2a')(input_tensor)\r\nresnet_50.py:32: UserWarning: Update your "", ' call to the Keras 2 API: ', ""\r\n  border_mode='same', name=conv_name_base + '2b')(x)\r\nresnet_50.py:36: UserWarning: Update your "", ' call to the Keras 2 API: ', '\r\n  x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + \'2c\')(x)\r\nTraceback (most recent call last):\r\n  File ""resnet_50.py"", line 180, in <module>\r\n    model = resnet50_model(img_rows, img_cols, channel, num_classes)\r\n  File ""resnet_50.py"", line 148, in resnet50_model\r\n    model.load_weights(weights_path)\r\n  File ""/usr4/cs542/jalal/.conda/envs/tensorflow/lib/python3.6/site-packages/keras/engine/topology.py"", line 2616, in load_weights\r\n    f = h5py.File(filepath, mode=\'r\')\r\n  File ""/usr4/cs542/jalal/.conda/envs/tensorflow/lib/python3.6/site-packages/h5py/_hl/files.py"", line 269, in __init__\r\n    fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)\r\n  File ""/usr4/cs542/jalal/.conda/envs/tensorflow/lib/python3.6/site-packages/h5py/_hl/files.py"", line 99, in make_fid\r\n    fid = h5f.open(name, flags, fapl=fapl)\r\n  File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper\r\n  File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper\r\n  File ""h5py/h5f.pyx"", line 78, in h5py.h5f.open\r\nOSError: Unable to open file (truncated file: eof = 98304, sblock->base_addr = 0, stored_eoa = 102853048)\r\n(tensorflow) [jalal@scc-c08 test_mona]$ ls ~/.keras/models/\r\n(tensorflow) [jalal@scc-c08 test_mona]$ \r\n\r\n', '']",0,0
275,keras,8450,closed,merge layer documentation and architecture  ,"I built a project using (Merge, merge) layer once, and use share layer in another. The problem I can't find any documentation that explains how merge or share layer works like convolution or max-pooling layers.
I will be grateful if anyone can direct me or suggest some paper to me that helps me to understand how these layers work ???",,"['From the existing documentation and by reading the source code: Merge will take two input tensor and will combine them according to the defined operation, the operation may be of a mathematical nature such as addition, subtraction, getting the maximum or minimum value or can simply concatenate the data by agglomerating the two input tensor in a single one i.e.:  two array of dimensions [2,2] will become [4,2] or [2,4] depending on the axis of concatenation.\r\n\r\nAlso note that every mathematical operation is performed element-wise.']",[],[],0,0
276,keras,10821,closed,Could we add an option to model.fit() to keep out N mini-batches per epoch for more robust training?,"I've been thinking about the extra robustness one gets during training by randomizing what data from the training set goes into each mini-batches, but leaving some percentage of them out during each fit. I believe this may be possible at present by setting  and manually entering  to be less than the actual number that will be used. 

However, I think this is a useful enough feature that it could deserve its own option. Perhaps we could call it  which automatically sets , checks to be sure that mini-batches are being used, and either correctly sets , or ignores thae last  of the mini-batches after each reshuffle by adding a couple lines of extra code to the fitting routine.
",,[],[],"['shuffle=True', 'steps_per_epoch', 'mutate_percentage', 'shuffle=True', 'steps_per_epoch', 'mutate_percentage']",0,0
277,keras,6368,closed,Issue with passing a callback to keras classifier wrapper,"I'm trying to adopt a drop based learning rate decay strategy to my categorical data classification task.
Everything goes fine until I pass the callback to the KerasClassifier wrapper. 

# Drop-Based Learning Rate Decay
import numpy
from pandas import read_csv
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from keras.utils import np_utils
from pandas import read_csv
import numpy
import math
from sklearn.preprocessing import LabelEncoder
from keras.callbacks import LearningRateScheduler

# learning rate schedule
def step_decay(epoch):
    initial_lrate = 0.2
    drop = 0.5
    epochs_drop = 10.0
    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))
    return lrate

# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)

dataframe = read_csv(""Book1.csv"", header=None)
dataset = dataframe.values
X = dataset[:,0:15].astype(float)
Y = dataset[:,15]
#One hot encoding or creating dummy variables from a categorical variable (class)
# encode class values as integers
encoder = LabelEncoder()
encoder.fit(Y)
encoded_Y = encoder.transform(Y)
# convert integers to dummy variables (i.e. one hot encoded)
dummy_y = np_utils.to_categorical(encoded_Y)

# create model
def baseline_model():
    # create model
    model = Sequential()
    model.add(Dense(50, input_dim=15, kernel_initializer='normal', activation='relu'))
    model.add(Dense(3, kernel_initializer='normal', activation='sigmoid'))
    sgd = SGD(lr=0.0, momentum=0.9, decay=0, nesterov=False)
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    return model
    
#learning schedule callback
lrate = LearningRateScheduler(step_decay)
callbacks_list = [lrate]
estimators = []
estimators.append(('MinMaxScale', MinMaxScaler()))
estimators.append(('standardize', StandardScaler()))
estimators.append(('mlp', KerasClassifier(build_fn=baseline_model, epochs=100,
batch_size=5, callbacks=[lrate], verbose=1)))
pipeline = Pipeline(estimators)
kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=seed)
results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)
print(""Standardized: %.2f%% (%.2f%%)"" % (results.mean()*100, results.std()*100))
When I run the code, exactly after passing the callback to the kerasclassifier wrapper (at estimators.append(('mlp', KerasClassifier(build_fn=baseline_model, epochs=100,
batch_size=5, callbacks=[lrate], verbose=1))), I get the error:
""Cannot clone object <keras.wrappers.scikit_learn.KerasClassifier object at 0x0000000042582AC8>, as the constructor does not seem to set parameter callbacks""
I would like to get this code corrected with the right way of passing callbacks to the wrapper. thanks in advance.
 ",,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', 'I have the same problem, someone can help us?? ', 'I also have the same problem, I think this happens since fit accepts callbacks but predict does not accept this parameter, we can add a check in predict parameter in KerasClassifier. This may fix it.', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', 'I\'m facing a similar issue with getting callbacks to work inside a Pipeline. I suspect it has something to do with the fact that somehow the code calls the set_model() method of the Callback class to associate it with the Sequential model. When the model is embedded in a pipeline, it doesn\'t associate the model correctly. I tried digging through the code for the Callback class and the various other callback classes, but couldn\'t figure it out.\r\n\r\nIf you press tab after ""your_callback_instance"". it should show you what attributes have been set for the callback instance.', 'I had a related problem with getting `callbacks` to work with `GridSearchCV`. I ended up modifying `BaseWrapper` as explained here https://github.com/keras-team/keras/issues/4278#issuecomment-258922449', ""@KPLauritzen I also tried to modify BaseWrapper class as mentioned in #4278. However I am getting an error which is as follows:-\r\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'int'\r\nDid you also encountered this error as well? Let me know.\r\n\r\nThanks!"", ""@kishoreiitd No.\r\n\r\nI had to stop using this approach for unrelated reasons, but when I used it I didn't see that error."", ""@KPLauritzen Thanks a lot for your quick reply, I encountered this error when I inserted EarlyStopping to GridSearchCV using this fit_params={'callbacks': [EarlyStopping]}). However when I inserted using fit_params={'callbacks': [EarlyStopping()]}), it gives me an error which is:-\r\n'EarlyStopping' object is not callable\r\n\r\nAny help in this case would be highly appreciated.\r\nThanks again!"", ""@kishoreiitd I don't think I can help you with this. Sorry""]",[],[],0,0
278,keras,12585,closed,Tensorboard AttributeError: 'Model' object has no attribute '_eval_function',"Please make sure that the boxes below are checked before you submit your issue.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:


- [x] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",stat:awaiting tensorflower type:support,"['Environment: Windows 8 x64, Python 3.6.7, Keras 2.2.4, tensorflow 1.13.1 (CPU version), tensorboard 1.13.1, jupyter 1.0.0, jupyter-client 5.2.3, jupyter-console 6.0.0, jupyter-core 4.4.0, notebook 5.7.0\r\n\r\nI\'m working with Jupyter notebook. Everything works ok except one little problem.\r\nTraining that includes tensorboard callback fails (tensorboard web server starts ok).\r\nI have extensively searched for the issue and anything similar to it, but haven\'t found anything that would help so far.\r\n\r\nBelow is python code that prepares and starts training, including problematic callback.\r\nAfter that is error message.\r\n\r\ncode -------------------------------------------\r\npoc = dt.datetime.now()\r\ncheckpointer = ModelCheckpoint(filepath=best_net, save_best_only=True, monitor=""val_loss"")\r\nmonitor = ker.callbacks.EarlyStopping(monitor=\'val_loss\', patience=300, mode=\'auto\', verbose=True)\r\ntensorboardCb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\r\nhistory = NN_model.fit(features_train_list, labels_train, batch_size=4096, epochs=5000, \r\n                       validation_split=0.1,\r\n                       shuffle=False, verbose=0,\r\n                       callbacks=[checkpointer,monitor])\r\nkraj = dt.datetime.now()\r\ntraj = kraj - poc\r\nprint (""%d sekundi"" % traj.seconds)\r\n\r\nerror --------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-23-ba785c5243d1> in <module>\r\n     18                        validation_split=0.1,\r\n     19                        shuffle=False, verbose=0,\r\n---> 20                        callbacks=[checkpointer,monitor, tensorboardCb])\r\n     21 kraj = dt.datetime.now()\r\n     22 traj = kraj - poc\r\n\r\nc:\\users\\username\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\r\n   1037                                         initial_epoch=initial_epoch,\r\n   1038                                         steps_per_epoch=steps_per_epoch,\r\n-> 1039                                         validation_steps=validation_steps)\r\n   1040 \r\n   1041     def evaluate(self, x=None, y=None,\r\n\r\nc:\\users\\username\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training_arrays.py in fit_loop(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\r\n    144         for m in model.stateful_metric_functions:\r\n    145             m.reset_states()\r\n--> 146         callbacks.on_epoch_begin(epoch)\r\n    147         epoch_logs = {}\r\n    148         if steps_per_epoch is not None:\r\n\r\nc:\\users\\username\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\callbacks.py in on_epoch_begin(self, epoch, logs)\r\n     63         logs = logs or {}\r\n     64         for callback in self.callbacks:\r\n---> 65             callback.on_epoch_begin(epoch, logs)\r\n     66         self._delta_t_batch = 0.\r\n     67         self._delta_ts_batch_begin = deque([], maxlen=self.queue_length)\r\n\r\nc:\\users\\username\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py in on_epoch_begin(self, epoch, logs)\r\n   1139       # pylint: disable=protected-access\r\n   1140       # add the histogram summary op if it should run this epoch\r\n-> 1141       if self.merged not in self.model._eval_function.fetches:\r\n   1142         self.model._eval_function.fetches.append(self.merged)\r\n   1143         self.model._eval_function.fetch_callbacks[\r\n\r\nAttributeError: \'Model\' object has no attribute \'_eval_function\'\r\n', 'I tried this: removed all known traces of python installation and installed it from scratch.\r\nBut I had no luck, error message is the same.\r\nRemark: python version now is 3.7.3, Keras 2.2.4, tensorflow and tensorboard 1.13.1, jupyter 1.0.0, jupyter-client 5.2.4, jupyter-console 6.0.0, jupyter-core 4.4.0, jupyter-tensorboard 0.1.9, notebook 5.7.8', ""I just updated TensorFlow to 1.13.1, ran a script that was working completely fine before and got this error message as well.\r\n\r\nAfter messing around with my code I realized that my Tensorboard callback was causing the problem:\r\n\r\nTensorBoard(\r\n        log_dir=log_dir,\r\n        histogram_freq=0,\r\n        batch_size=10,\r\n        write_graph=False,\r\n        write_grads=True,\r\n        write_images=False)\r\n\r\nWhen histogram_freq was set to 1 I was getting the error you described.\r\nI had been trying to plot gradient histograms but was getting an error saying I need validation data to plot histograms, which I had passed as an argument to the model.fit() function and was working just fine otherwise. I decided to update TensorFlow as another user was having a similar problem and updating fixed it for them. However, for me I just got the error message you describe instead of the one about there needing to be validation data to plot histograms. Still not sure how to get around this issue as I would really like to be able to see those histograms but hopefully this can help the developers in fixing this. \r\n\r\nEdit: error message:  if self.merged not in self.model._eval_function.fetches:\r\nAttributeError: 'Sequential' object has no attribute '_eval_function'"", '@prekratko @ggaugler Is this still an issue?', 'Yes, it is.', 'I found the same behavior with tensorflow-1.13.1. If however I convert to 2.0-alpha the code works as expected.', ""I also confirm that by using TF 2.0 I don't get that error and I can view histogram, distrbution, images.\r\nIt seems that we will have to wait for 2.0 release."", ""I'm having this issue as well. Is there a workaround for using TF 1.13 or 1.14? I'm getting that `AttributeError: 'NoneType' object has no attribute 'fetches'`. I'm using tf datasets as my input to keras `fit`."", 'Have you try to use keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1) instead of  tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1) ?', ""I've switched away from using keras fit to my own training loop, so it's not easy for me to test that at this point."", 'I was having the same issue (tensorflow 1.13.1, Keras 2.2.4) and when using callback from keras instead of tensorflow the code works as expected.', 'I have this same error using\r\npython3.5\r\nKeras==2.3.1\r\ntensorflow==2.1.0\r\ntensorflow-gpu==2.1.0\r\ntensorboard==2.1.0\r\ntensorflow-datasets==1.3.0\r\ntensorflow-estimator==2.1.0\r\n\r\n', 'I have this same error using\r\npython3.6.8\r\nKeras==2.3.1\r\ntensorflow==2.1.0\r\ntensorflow-gpu==2.1.0', ""I have this same error.\r\nI changed 'from tensorflow.keras.callbacks import TensorBoard' to 'from keras.callbacks import TensorBoard'.\r\nand solve this problem.\r\nI hope it can help you.""]",[],['pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps'],0,0
279,keras,13522,closed,How to use KerasClassifier with fit_generator method?,"I want to perform Hyperparameter Optimization on my Keras Model. The problem is the dataset is quite big, normally in training I use fit_generator to load the data in batch from disk, but the common package like SKlearn Gridsearch, etc. only support fit method. How to use GridSearchCV to implement fit_generator() method? Is it possible to use Keras's scikit-learn API together with fit_generator() method? Thank you.",type:support,"[""@wangyexiang at the moment these wrappers don't support using the `.fit_generator` with the principal reason being the standard usage of sklearn estimators is to give them a `X, y` pair to fit and a single `X` to predict on. I am looking into making a general version which would allow for augmentation and the usage of a generator, what use case are you looking at?"", '@kmader Dear sir, thank you for your reply. I have given up using this wrapper, and finally I choose kerastuner instead. If you could make a general wrapper, that will be pretty useful for performing hyperparameter optimization with a big dataset. GridSearchCV can allow you to set `n_job=-1`, this can accelerate the optimization process with parallel searching.', '\r\n[keras_batch_classifier.zip](https://github.com/keras-team/keras/files/4453458/keras_batch_classifier.zip)\r\n']",[],[],0,0
280,keras,11416,closed,Two models with cross loss function,"Hi, 
I'm working on a heteroscedactic neural network. 
What I want is to get two neural networks and the loss function of each NN depends on the another NN. 
Here a short example : 



Is there a way to do so? Either by define in a correct way my loss functions or by define in another way my NN? 

Thank's for your help
Charles",type:support,[],"[""\r\n   def define_NN():\r\n        init=keras.initializers.RandomNormal(mean=0.0, stddev=1., seed=None)\r\n        input_tensor=(Input(shape=(X.shape[1],)))\r\n        denseA=Dense(15,activation='tanh')(input_tensor)\r\n        denseB=Dense(7,activation='tanh')(denseA)\r\n        output_tensor=Dense(1,activation='linear')(denseB)\r\n        return Model(inputs=input_tensor,outputs=output_tensor)\r\n\r\n\r\n    def customLossSigma(modelMu,x):\r\n        def neg_log_likelihood_Sigma(y_true, y_pred):\r\n            mu=modelMu(x)\r\n            inter=(mu-y_true)/K.clip(y_pred,K.epsilon(),None)\r\n            val=K.log(K.clip(K.square(y_pred),K.epsilon(),None))+K.square(inter)\r\n            return val\r\n        return neg_log_likelihood_Sigma\r\n\r\n    def customLossMu(modelSigma,x):\r\n        def neg_log_likelihood_Mu(y_true, y_pred):\r\n            sigma=modelSigma(x)\r\n            inter=(y_pred-y_true)/K.clip(sigma,K.epsilon(),None)\r\n            val=K.log(K.clip(K.square(sigma),K.epsilon(),None))+K.square(inter)\r\n            return val\r\n        return neg_log_likelihood_Mu\r\n\r\n    Adam=keras.optimizers.Adam(lr=learning_rate)\r\n\r\n    modelMu=define_NN()\r\n    modelSigma=define_NN()\r\n\r\n    modelSigma.compile(loss=customLossSigma(modelMu,modelSigma.input), optimizer=Adam)\r\n    modelMu.compile(loss=customLossMu(modelSigma,modelMu.input), optimizer=Adam)\r\n\r\n    for ii in range(3):\r\n        modelMu.fit(X_train, y_train, epochs=nepochs, batch_size=15,verbose=verbose,validation_data=(X_test, y_test))\r\n        modelSigma.fit(X_train, y_train, epochs=nepochs, batch_size=15,verbose=verbose,validation_data=(X_test, y_test))\r\n""]",[],0,0
281,keras,9758,closed,zoom_range in ImageDataGenerator uses different scales for x and y,"I noticed that ImageDataGenerator, when setting the parameter zoom_range, uses a different random transformation for the horizontal and vertical axis, which produces images with a random aspect ratio.

I checked the source code ([here](https://github.com/keras-team/keras/blob/master/keras/preprocessing/image.py#L853)) and the zoom transformation is indeed random for the horizontal and vertical axis. I think this is counterintuitive because I'm expecting a ""zoom"" transformation to simply magnify (or unmagnify) the input image. Instead, the output of zoom_range are images which are randomly stretched horizontally or vertically.

I suggest introducing an additional parameter like a boolean ""zoom_keep_ar"" (aspect ratio) which if True, keeps the aspect ratio constant (simple magnification, zx=zy=random), if False it also randomly change the aspect ratio (zx=rand, zy=rand).",,"['I agree. I just had the same problem. I expected the images to be only zoomed-in/zoomed-out versions of the original images, but instead the aspect ratio is manipulated.', 'Just a note, for people looking for the code. It has been moved [here](https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/image/image_data_generator.py).', 'the problem still not fix ? I was shocked when I saw the result \r\n06/17/2019', 'Still not fixed in keras-gpu 2.3.1. My sample image of a circle is changed into a oval.', 'Yes, the offending lines are 801-807:\r\n```python\r\n        if self.zoom_range[0] == 1 and self.zoom_range[1] == 1:\r\n            zx, zy = 1, 1\r\n        else:\r\n            zx, zy = np.random.uniform(\r\n                self.zoom_range[0],\r\n                self.zoom_range[1],\r\n                2)\r\n```\r\nAny easy suggestion to work around this?', 'Also the documentation is a bit misleading, a zoom >1 minifies the resulted image, not magnifies it as I assumed. It can matter a lot when the zoom cuts off part of your images needed for classification. Same for shift ops, I suppose.']",[],[],0,0
282,keras,10890,closed,Add Dice Loss (and Intersection Over Union),"I propose that Dice Score/Loss (also known as F1-score or Sorensen score) is added as a metric and loss function as these are very commonly used in image segmentation or bounding box problems. Within the medical community, this is an incredibly important function, although I have seen it in other areas like [astronomy](https://github.com/jakeret/tf_unet).

Additionally, the Intersection Over Union (IoU) (also known as Jaccard Index) is another important metric/loss for these same classes of problem. While the Dice and IoU are very similar functions, the Dice Score weights true positives (the intersection) more heavily than false positives and false negatives than IoU (which gives a more even weighting to TP, FP, & FN). In cases where false positives and false negatives can be very detrimental, the IoU will produce a better result than Dice. Andrew Ng (Stanford Prof, Google Brain co-founder, Coursera founder) even devotes an [entire video](https://www.coursera.org/lecture/convolutional-neural-networks/intersection-over-union-p9gxz) to IoU in his convolutional neural networks (CNN's) course as the metric to use to determine if your bounding box predictions are working.

According to Pull #7032 which sought to add Dice, the end result was the request was closed and that it will only be added if the community continues to bring it up and express interest in adding Dice ([See Comment](https://github.com/keras-team/keras/pull/7032#issuecomment-311459497) by @fchollet ).

As time has passed and interest in CNN's has [skyrocketed](https://trends.google.com/trends/explore?cat=174&date=all&geo=US&q=convolutional%20neural%20network), I suggest that we reconsider adding Dice and IoU as they are becoming more and more common place. Dice has been proposed or mentioned in issues and pull requests multiple times (#292, #369, #2115, #2994, #3442, #3457, #3611, #3653, #3977, #5916, #6933, #7032, #8961, #9154, #9275, #9395, #9444, #9671, #10783). Additionally, IoU has also been mentioned a number of times in this repository (#2016, #2185, #6467, #6538, #8225, #8643, #8669, #9367, #10104, #10602, #10783). Keep in mind that those references are only with in _this_ repository... there are plenty of other Github repositories that use Dice and/or IoU as a loss function.

Furthermore, the research community has used Dice or IoU in numerous papers that make use of CNN's. Here are a few that each have over 100 citations according to Google Scholar, though many more exist... [1](https://ieeexplore.ieee.org/abstract/document/7785132/), [2](https://ieeexplore.ieee.org/abstract/document/7444155/), [3](http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection), [4](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Erhan_Scalable_Object_Detection_2014_CVPR_paper.pdf), [5](https://www.cv-foundation.org/openaccess/content_cvpr_workshops_2015/W01/html/Brebisson_Deep_Neural_Networks_2015_CVPR_paper.html), [6](https://www.sciencedirect.com/science/article/pii/S1053811914010660), [7](https://link.springer.com/chapter/10.1007/978-3-319-46723-8_48), [8](https://www.computer.org/csdl/proceedings/icip/1994/6952/02/00413580.pdf), [9](https://link.springer.com/article/10.1007/s11548-016-1501-5), [10](https://ieeexplore.ieee.org/abstract/document/7482843/) (sorry ahead of time if they are behind a paywall).

I personally have been working in medical research with a U-Net for image segmentation and have found that training the model with binary cross entropy as my loss function at first and then switching to dice loss for additional training has significantly improved my performance over using only binary cross entropy. I have been using a custom loss to use Dice loss, however, it would be great to see an official version of this supported by Keras.

Given that over a year has past since PR #7032, would the Keras team reconsider implementing an official version of Dice and IoU loss functions?",type:feature,"['Any updates on this?', ""For anyone interested in this, I've implemented an IoU metric for evaluating semantic segmentation results [here](https://gist.github.com/Kautenja/69d306c587ccdf464c45d28c1545e580). I'd be happy to merge it into the Keras codebase."", 'I can also confirm that these metrics are important to segmentation tasks in the biometric domain. It would be really great to have an official implementation available. ', 'I can also confirm the need of these metrics and loss functions in the field of medical image segmentation, that often deals with data with great class imbalances. An official implementation would be deeply appreciated! ', ""After working with the I/U metrics for some time with Keras, I noticed unexpected values compared to reported baselines. After reviewing how metrics are calculated in Keras, I discovered that they are calculated batch-wise and averaged together. This is contrary to how I/U is typically calculated for semantic segmentation tasks, i.e., over an entire subset of data. Batch-wise I/U may be useful in certain contexts and applications, so a simple I/U metric built in may still be worth considering. However, correct semantic segmentation evaluation necessitates a novel solution. \r\n\r\nPotential solutions I've considered to correctly calculate I/U dataset-wise:\r\n\r\n1.  set batch size equal to size of dataset\r\n    -   terrible space complexity, infeasible for large datasets\r\n    -   restricts training configurations\r\n2.  aggregating a global confusion matrix to calculate I/U from\r\n    -   memory efficient, just needs a single CxC matrix to add to\r\n    -   hard to implement (requires a complementary callback for advanced functionality)\r\n        -   how to allocate the CxC matrix on training start?\r\n        -   how to reset to zero between epochs?\r\n        -   how to reset to zero for a validation/evaluation set?\r\n        -   how to access effectively to calculate I/U?\r\n3.  write external methods to evaluate models using above technique, but with NumPy\r\n    -   memory efficient, just needs a single CxC matrix to add to    \r\n    -   easy to implement\r\n    -   does not integrate with Keras model fitting (training / validation)\r\n    -   no GPU acceleration for calculating confusion matrices batch-wise\r\n4.  write a callback to calculate I/U for a validation dataset\r\n    -   easy to implement\r\n    -   integrates with Keras well (i.e., free GPU acceleration)\r\n    -   contradicts some standard Keras API (e.g., validation data args in fit, fit_generator)\r\n        -   doesn't integrate with other metrics baked into the model\r\n    -   no output for training or evaluation metrics\r\n    -   have to reimplement accuracy metrics in NumPy to evaluate (easy with confusion matrix)\r\n\r\nI ended up using tek 3 to implement SegNet and Tiramisu in Keras as it's the simplest solution and I only really needed test time I/U metrics anyway as loss and accuracy are enough for the training process. Anyone interested can find the evaluation code [here](https://github.com/Kautenja/neural-semantic-segmentation/blob/master/src/evaluate.py) and the individual metrics that module uses [here](https://github.com/Kautenja/neural-semantic-segmentation/blob/master/src/metrics/evaluation_metrics.py). \r\n\r\nEDIT: clarify meaning and some grammar"", ""I would also really want this; I'm going crazy implementing it. I'll just spam my question here as well:\r\n\r\nI am trying to perform semantic segmentation in TensorFlow 1.10 with eager execution with the [generalized dice loss](https://arxiv.org/pdf/1707.03237.pdf) function:\r\n\r\n```\r\ndef generalized_dice_loss(onehots_true, logits):\r\n    onehots_true, logits = mask(onehots_true, logits) # Not all of my pixels contain ground truth, and I filter those pixels out, which results in shape [num_gt_pixels, num_classes]-shaped labels and logits.\r\n    probabilities = tf.nn.softmax(logits)\r\n    weights = 1.0 / (tf.reduce_sum(onehots_true, axis=0)**2)\r\n    weights = tf.clip_by_value(weights, 1e-17, 1.0 - 1e-7) # Is this the correct way of dealing with inf values (the results of zero divisions)?\r\n    numerator = tf.reduce_sum(onehots_true * probabilities, axis=0)\r\n    numerator = tf.reduce_sum(weights * numerator)\r\n\r\n    denominator = tf.reduce_sum(onehots_true + probabilities, axis=0)\r\n    denominator = tf.reduce_sum(weights * denominator)\r\n\r\n    loss = 1.0 - 2.0 * (numerator + 1e-17) / (denominator + 1e-17)\r\n    return loss\r\n```\r\n\r\nHowever, I am struggling to get any meaningful loss which isn't always 1. What am I doing wrong here?\r\n\r\nAfter the initial weights (one for each class) are calculated, they contain many `inf`'s from zero divisions, as typically only a small subset of all classes is present in a sample image. Therefore, I clip the weights to the range [1e-17, 1-1e-17] (is this a good idea?), after which they look like this:\r\n\r\n```\r\ntf.Tensor(\r\n[4.89021e-05 2.21410e-10 5.43187e-11 1.00000e+00 1.00000e+00 4.23855e-07\r\n 5.87461e-09 3.13044e-09 2.95369e-07 1.00000e+00 1.00000e+00 2.22499e-05\r\n 1.00000e+00 1.73611e-03 9.47212e-10 1.12608e-05 2.77563e-09 1.00926e-08\r\n 7.74787e-10 1.00000e+00 1.34570e-07], shape=(21,), dtype=float32)\r\n```\r\n\r\nwhich seems fine to me, though they are pretty small. The numerators (`tf.reduce_sum(onehots_true * probabilities, axis=0)`, prior to their weighting) look like this:\r\n\r\n```\r\ntf.Tensor(\r\n[3.42069e+01 0.00000e+00 9.43506e+03 7.88478e+01 1.50554e-02 0.00000e+00\r\n 1.22765e+01 4.36149e-01 1.75026e+02 0.00000e+00 2.33183e+02 1.81064e-01\r\n 0.00000e+00 1.60128e+02 1.48867e+04 0.00000e+00 3.87697e+00 4.49753e+02\r\n 5.87062e+01 0.00000e+00 0.00000e+00], shape=(21,), dtype=float32)\r\ntf.Tensor(1.0, shape=(), dtype=float32)\r\n```\r\n\r\nwhich also looks reasonable, since they're basically the labels' respective sizes times the network's certainty about them (which is likely low in the beginning of training). The denominators (`tf.reduce_sum(onehots_true + probabilities, axis=0)`, prior to weighting) also look fine:\r\n\r\n```\r\ntf.Tensor(\r\n[ 14053.483   25004.557  250343.36    66548.234    6653.863    3470.502\r\n   5318.3926 164206.19    19914.338    1951.0701   3559.3235   7248.4717\r\n   5984.786    7902.9004 133984.66    41497.473   25010.273   22232.062\r\n  26451.926   66250.39     6497.735 ], shape=(21,), dtype=float32)\r\n```\r\n\r\nThese are large, but that is to be expected since the class probabilities of a pixel sum to 1, and therefore the sum of these denominators should more or less equal the amount of pixels with ground truth.\r\n\r\nHowever, summing the numerators gives a very small sum (~0.001, though occasionally it's in a single digit range) while the denominator sums to very large values. This results in my final loss being exclusively 1, or something really close to that. Does anyone know how I can mitigate this effect and obtain stable gradients?"", 'Would it be possible to request this as a loss in [`tensorflow/addons`](https://www.tensorflow.org/addons/api_docs/python/tfa/losses), rather than Keras?']",[],[],0,0
283,keras,6501,closed,Share stateful LSTM layers,"Hi,

if I want to reuse stateful LSTM layers in a separate model like in the code below (very simplified version)

I get the following error when calling otherModel.predict(...)


whereas if I set stateful to False, everything works.
ubuntu 16.04, python 3.5.2, tensorflow 1.1.0",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', 'I have the same issue, did you find why ?', ""@ThomasJanssoone no, but I haven't looked at it for a while.."", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', ""I have the same issue. When you would remove the `stateful=True` (so when the model is not stateful anymore) the problem disappears. Any new insights, @ThomasJanssoone, @cinhcet?\r\n\r\nEdit : I now see that @cinhcet originally already mentioned that the problem disappears when the model is made not stateful.\r\n\r\nEdit : I'm on Keras 2.1.2."", 'me too having the same issue @visionscaper . Any updates!!', 'No @DeepWolf90, also checkout issue #9084. Any recognition for that issue too? Please comment there if you do. ', 'I too have this issue attempting to share layers of a stateful model. Even though the placeholder for the batched input is not used in the new model, it is still expected that the placeholder is filled regardless of its use. Filling it with an arbitrary value and including it as another `Input` to the new model resolves the issue, but leads to very convoluted code. ']","[""python\r\nfrom keras.layers import Input, LSTM, Dense, Reshape\r\nfrom keras.models import Model\r\n\r\nimport numpy as np\r\n\r\nbatch_size = 1\r\nN = 10\r\nM = 2\r\n\r\ninput = Input(batch_shape=(batch_size,N,))\r\n\r\nlstm = Reshape((1,N))(input)\r\nlstm = LSTM(100, stateful=True, return_sequences=False, name='lstm')(lstm)\r\noutput = Dense(M, name='lstmDense')(lstm)\r\n\r\nmodel = Model(inputs=input, outputs=output)\r\n\r\nprint(model.predict(np.array([[10,10,10,10,10,10,10,10,10,10]])))\r\n\r\ninput2 = Input(batch_shape=(batch_size,N,))\r\nlayer = model.get_layer('lstmDense')(model.get_layer('lstm')(Reshape((1,N))(input2)))\r\notherModel = Model(input2, layer)\r\n\r\nprint(model.predict(np.array([[10,10,10,10,10,10,10,10,10,10]])))\r\nprint(otherModel.predict(np.array([[10,10,10,10,10,10,10,10,10,10]])))\r\n"", '\r\n2017-05-04 12:51:43.601816: W tensorflow/core/framework/op_kernel.cc:1152] Invalid argument: You must feed a value for placeholder tensor \'input_1\' with dtype float and shape [1,10]\r\n\t [[Node: input_1 = Placeholder[dtype=DT_FLOAT, shape=[1,10], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]\r\nTraceback (most recent call last):\r\n  File ""/home/userName/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1039, in _do_call\r\n    return fn(*args)\r\n  File ""/home/userName/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1021, in _run_fn\r\n    status, run_metadata)\r\n  File ""/usr/lib/python3.5/contextlib.py"", line 66, in __exit__\r\n    next(self.gen)\r\n  File ""/home/userName/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor \'input_1\' with dtype float and shape [1,10]\r\n\t [[Node: input_1 = Placeholder[dtype=DT_FLOAT, shape=[1,10], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""/home/userName/git/kerasTests/testBug.py"", line 25, in <module>\r\n    print(otherModel.predict(np.array([[10,10,10,10,10,10,10,10,10,10]])))\r\n  File ""/home/userName/.local/lib/python3.5/site-packages/keras/engine/training.py"", line 1573, in predict\r\n    batch_size=batch_size, verbose=verbose)\r\n  File ""/home/userName/.local/lib/python3.5/site-packages/keras/engine/training.py"", line 1203, in _predict_loop\r\n    batch_outs = f(ins_batch)\r\n  File ""/home/userName/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py"", line 2103, in __call__\r\n    feed_dict=feed_dict)\r\n  File ""/home/userName/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 778, in run\r\n    run_metadata_ptr)\r\n  File ""/home/userName/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 982, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File ""/home/userName/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1032, in _do_run\r\n    target_list, options, run_metadata)\r\n  File ""/home/userName/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1052, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor \'input_1\' with dtype float and shape [1,10]\r\n\t [[Node: input_1 = Placeholder[dtype=DT_FLOAT, shape=[1,10], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]\r\n\r\nCaused by op \'input_1\', defined at:\r\n  File ""/home/userName/git/kerasTests/testBug.py"", line 10, in <module>\r\n    input = Input(batch_shape=(batch_size,N,))\r\n  File ""/home/userName/.local/lib/python3.5/site-packages/keras/engine/topology.py"", line 1392, in Input\r\n    input_tensor=tensor)\r\n  File ""/home/userName/.local/lib/python3.5/site-packages/keras/engine/topology.py"", line 1303, in __init__\r\n    name=self.name)\r\n  File ""/home/userName/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py"", line 351, in placeholder\r\n    x = tf.placeholder(dtype, shape=shape, name=name)\r\n  File ""/home/userName/.local/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py"", line 1507, in placeholder\r\n    name=name)\r\n  File ""/home/userName/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 1997, in _placeholder\r\n    name=name)\r\n  File ""/home/userName/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 768, in apply_op\r\n    op_def=op_def)\r\n  File ""/home/userName/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2336, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File ""/home/userName/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1228, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor \'input_1\' with dtype float and shape [1,10]\r\n\t [[Node: input_1 = Placeholder[dtype=DT_FLOAT, shape=[1,10], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]\r\n']",[],0,0
284,keras,11750,closed,Predicting on real test set gives only very high probability for 1 for a very unbalanced data,"Hi guys, 
Excuse me for this question without much details, because I'm bound with time in order to post my results on a challenge platform, anyway I try to sum up as much as I can. I have a multivariate time-series, that I trained using an RNN, there are periods and repeating time indexes, from 2013-01 to 2016-09, steps are months, by repeating, I mean various subsets ordered from January to December, many times for the same year, for hundreds of times, and I am predicting the next year knowing other features. I trained using LSTM, on 3 years, and trying to predict also repeating time-series for the year 2017. I used fixed batch size, and one last layer for binary target value so I used such a basic neural network:



The batch size is 12 ( I chose) for 12 months, the target in train is very unbalanced with



the problem is that predicting on test like



is very frustrating before posting my results as all values of probability are above 0.5 and near 1 that means no probability for any entry to be zero.

what could be wrong !!?

Many thanks !! 
Hope to hear from you as soon as possible in order to tweak my model and get more coherent outputs ! :) :)",type:support,"[""I added \r\n```\r\nfrom sklearn.utils import class_weight\r\nclass_weights = class_weight.compute_class_weight('balanced',\r\n                                                 np.unique(train_y),\r\n                                                 train_y) \r\n```\r\nand passed class_weights to fit method, still zero probability values are under 0.5.""]","[""\r\nmodel = Sequential()\r\nmodel.add(LSTM(10, batch_input_shape=(12, train_X.shape[1], train_X.shape[2]), return_sequences=True, stateful=True, activation='sigmoid', inner_activation='hard_sigmoid'))\r\nmodel.add(Dropout(0.2))\r\n# model.add(LSTM(70))\r\n# model.add(Dropout(0.3))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(1, activation='sigmoid'))\r\nmodel.compile(loss='binary_crossentropy',\r\noptimizer='rmsprop',\r\nmetrics=['accuracy'])\r\n"", '\r\npd.value_counts(test_y)\r\n1.0 163781\r\n0.0 5551\r\ndtype: int64\r\n', '\r\nres = model.predict(t_e_s_t, batch_size=12)\r\n\r\nres\r\narray([[0.9633749 ],\r\n[0.79078996],\r\n[0.99266464],\r\n...,\r\n[0.9891131 ],\r\n[0.7582535 ],\r\n[0.95778626]], dtype=float32)\r\n']",[],0,0
285,keras,12871,closed,Keras Issue concatenate single input instead of multiple,"I am trying similar things, but I am getting stuck with the input of the model.

 


I first trained 3 different models:

- model 1 = 3 labels (8,9,10)
- model 2 = 2 labels (30,31)
- model 3 = 4 labels (80,81,82,83)

I would like to combine these 3 models into a final model with 1 output containing 9 labels (8,9,10,30,31,80,81,82,83). The final input needs to get 1 input image instead of 3 images. But I still get stuck. I am building a kind of hierarchy here to improve the accuracy.",,[],[],"['  \r\n\r\n     def generate_generator_multiple(generator,dir1, dir2,dir3, batch_size, img_size):\r\n          genX1 = generator.flow_from_directory(dir1,\r\n                                          target_size = (img_size,img_size),\r\n                                          class_mode = \'categorical\',\r\n                                          batch_size = batch_size,\r\n                                          shuffle=False, \r\n                                          seed=7,subset=\'training\')\r\n\r\n          genX2 = generator.flow_from_directory(dir2,\r\n                                          target_size = (img_size,img_size),\r\n                                          class_mode = \'categorical\',\r\n                                          batch_size = batch_size,\r\n                                          shuffle=False, \r\n                                          seed=7,subset=\'training\')\r\n\r\n          genX3 = generator.flow_from_directory(dir3,\r\n                                      target_size = (img_size,img_size),\r\n                                      class_mode = \'categorical\',\r\n                                      batch_size = batch_size,\r\n                                      shuffle=False, \r\n                                      seed=7,subset=\'training\')\r\n           while True:\r\n                  X1i = genX1.next()\r\n                  X2i = genX2.next()\r\n                  X3i = genX3.next()\r\n           yield [X1i[0], X2i[0],X3i[0]], X3i[1]  #Yield both images and their mutual label\r\n\r\n    train_datagen = ImageDataGenerator(validation_split=VAL_SPLIT,rescale=1./255)\r\n    inputgenerator=generate_generator_multiple(generator=train_datagen,\r\n                                           dir1=\'mergedata\',\r\n                                           dir2=\'mergedata\',\r\n                                           dir3=\'mergedata\',\r\n                                           batch_size=BATCH_SIZE,\r\n                                           img_size=IMG_SIZE)   \r\n\r\n    modelA = load_model(\'/content/8-base_model_1.h5\')\r\n    modelA.load_weights(\'/content/8-weights.002-0.991-0.406.hdf5\') \r\n    \r\n    modelB = load_model(\'/content/30-base_model_1.h5\')\r\n    modelB.load_weights(\'/content/30-weights.010-0.499-0.374.hdf5\')\r\n    \r\n    modelC = load_model(\'/content/80-base_model_1.h5\')\r\n    modelC.load_weights(\'/content/80-weights.002-1.064-0.305.hdf5\')\r\n    for layer in modelA.layers:\r\n        layer.trainable = False\r\n        layer.name = layer.name + str(""_1"")\r\n      \r\n    for layer in modelB.layers:\r\n        layer.trainable = False\r\n        layer.name = layer.name + str(""_2"")\r\n        \r\n    for layer in modelC.layers:\r\n        layer.trainable = False\r\n        layer.name = layer.name + str(""_3"")\r\n\r\n    mod_1 = concatenate([modelA.layers[-1].output,  modelB.layers[-1].output,modelC.layers[-1].output])     \r\n    mod_1 = Dropout(0.2)(mod_1)\r\n    mod_1 = Dense(15, activation=\'relu\')(mod_1)\r\n    predictions = Dense(9, activation=\'softmax\', name=\'pred_age\')(mod_1)\r\n    \r\n    top_model = Model(inputs=[modelA.input, modelB.input, modelC.input], outputs=predictions)\r\n\r\n    opt = get_optimizer(\'adam\', lr_rate)\r\n    top_model.compile(loss=[\'categorical_crossentropy\'],\r\n              optimizer=opt,\r\n              metrics=[\'accuracy\',\'mae\'])\r\n\r\n    model.fit_generator(inputgenerator,\r\n                               epochs=EPOCHS,\r\n                               steps_per_epoch = STEPS_PER_EPOCH,\r\n                               validation_data=validgenerator,\r\n                               validation_steps = VALIDATION_STEPS,\r\n                               verbose=1,\r\n                               callbacks=callbacks)']",0,0
286,keras,13218,closed,TypeError: Not JSON Serializable: <module 'tensorflow' from '/path/python/v3-5.1.0/lib/python3.6/site-packages/tensorflow/__init__.py'>,"**System information**  
- Have I written custom code (as opposed to using example directory):  yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux centos 7
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  v1.12.1-8795-ga675686 1.15.0-dev20190814 (tf-nightly dist)
- Keras version:  2.2.4
- Python version:  3.6
- CUDA/cuDNN version:  10.2
- GPU model and memory:  1080ti _ 11GB

**Describe the current behavior**

Save model returns a ""Not Jason serialisable"" error similar, but not exactly the same, as some of the other issues posted here, which are supposedly resolved by a tf-nightly install. I cannot provide the one-hot-encoded input data.

**Describe the expected behavior** 

Save the best model's weights to a hdf5 file.

**Code to reproduce the issue**  



**Other info / logs**  

TypeError                                 Traceback (most recent call last)
<ipython-input-22-4d0435c2448d> in <module>()
      1 get_ipython().run_line_magic('time', '')
----> 2 CNN_1D_model = keras_1D_CNN()
      3 CNN_history_dict = CNN_1D_model[1].history
      4 CNN_history_dict.keys()
      5 print(""\n%s: %.2f%%"" % (CNN_1D_model[0].metrics_names[1], CNN_1D_model[2][1]*100))

<ipython-input-21-2bf565bb8ec7> in keras_1D_CNN()
    100               epochs=1000,
    101               batch_size=2048,
--> 102               validation_split=0.2, shuffle=True, callbacks=[tensorboard, earlystopper, checkpoint], verbose=0)
    103 #     CNN_history = model.fit(partial_seq_train,
    104 #               partial_label_train,

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
   1037                                         initial_epoch=initial_epoch,
   1038                                         steps_per_epoch=steps_per_epoch,
-> 1039                                         validation_steps=validation_steps)
   1040 
   1041     def evaluate(self, x=None, y=None,

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/site-packages/keras/engine/training_arrays.py in fit_loop(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)
    215                         for l, o in zip(out_labels, val_outs):
    216                             epoch_logs['val_' + l] = o
--> 217         callbacks.on_epoch_end(epoch, epoch_logs)
    218         if callback_model.stop_training:
    219             break

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/site-packages/keras/callbacks.py in on_epoch_end(self, epoch, logs)
     77         logs = logs or {}
     78         for callback in self.callbacks:
---> 79             callback.on_epoch_end(epoch, logs)
     80 
     81     def on_batch_begin(self, batch, logs=None):

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/site-packages/keras/callbacks.py in on_epoch_end(self, epoch, logs)
    444                             self.model.save_weights(filepath, overwrite=True)
    445                         else:
--> 446                             self.model.save(filepath, overwrite=True)
    447                     else:
    448                         if self.verbose > 0:

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/site-packages/keras/engine/network.py in save(self, filepath, overwrite, include_optimizer)
   1088             raise NotImplementedError
   1089         from ..models import save_model
-> 1090         save_model(self, filepath, overwrite, include_optimizer)
   1091 
   1092     def save_weights(self, filepath, overwrite=True):

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/site-packages/keras/engine/saving.py in save_model(model, filepath, overwrite, include_optimizer)
    380 
    381     try:
--> 382         _serialize_model(model, f, include_optimizer)
    383     finally:
    384         if opened_new_file:

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/site-packages/keras/engine/saving.py in _serialize_model(model, f, include_optimizer)
     82     model_config['class_name'] = model.__class__.__name__
     83     model_config['config'] = model.get_config()
---> 84     model_config = json.dumps(model_config, default=get_json_type)
     85     model_config = model_config.encode('utf-8')
     86     f['model_config'] = model_config

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/json/__init__.py in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)
    236         check_circular=check_circular, allow_nan=allow_nan, indent=indent,
    237         separators=separators, default=default, sort_keys=sort_keys,
--> 238         **kw).encode(obj)
    239 
    240 

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/json/encoder.py in encode(self, o)
    197         # exceptions aren't as detailed.  The list call should be roughly
    198         # equivalent to the PySequence_Fast that ''.join() would do.
--> 199         chunks = self.iterencode(o, _one_shot=True)
    200         if not isinstance(chunks, (list, tuple)):
    201             chunks = list(chunks)

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/json/encoder.py in iterencode(self, o, _one_shot)
    255                 self.key_separator, self.item_separator, self.sort_keys,
    256                 self.skipkeys, _one_shot)
--> 257         return _iterencode(o, 0)
    258 
    259 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,

/d/harpy1/s/python/v3-5.1.0/lib/python3.6/site-packages/keras/engine/saving.py in get_json_type(obj)
     72             return obj.__name__
     73 
---> 74         raise TypeError('Not JSON Serializable: %s' % (obj,))
     75 
     76     from .. import __version__ as keras_version

TypeError: Not JSON Serializable: <module 'tensorflow' from '/d/harpy1/s/python/v3-5.1.0/lib/python3.6/site-packages/tensorflow/__init__.py'>
",type:bug/performance,[],"['\r\nfrom keras.utils import plot_model\r\nfrom IPython.display import SVG\r\nfrom keras.utils.vis_utils import model_to_dot\r\nfrom keras.layers import LSTM\r\nfrom keras.layers import TimeDistributed\r\nfrom keras.layers import Bidirectional\r\nimport h5py\r\nimport logging\r\nimport time\r\n\r\nts = time.time()\r\ntime = datetime.datetime.fromtimestamp(ts).strftime(""%H:%M:%S"")\r\ndate = datetime.datetime.fromtimestamp(ts).strftime(""%Y-%m-%d"")\r\nlog_dir =""/d/as2/u/mp002/chain_pairing/logs/{}/server_0"".format(date)\r\n\r\nif not os.path.isdir(log_dir):\r\n    os.makedirs(log_dir)\r\n\r\n#Visualize Model\r\ndef visualize_model(model):\r\n    SVG(model_to_dot(model).create(prog=""dot"", format=""svg""))\r\n    return plot_model(model, show_shapes=True, show_layer_names=True, \r\n                      to_file=os.path.join(log_dir, ""network_architecture_{}.svg"".format(time)))\r\n\r\n\r\ndef keras_1D_CNN():\r\n\r\n    model = models.Sequential()\r\n    model.add(Convolution1D(input_shape=(253,21), \r\n                            kernel_initializer=""he_uniform"", \r\n                            bias_initializer=initializers.Constant(0.1), \r\n                            nb_filter=256,\r\n                            filter_length=2,\r\n                            padding=""same"",\r\n                            activation=""relu"", \r\n                            dilation_rate=2, \r\n                            name=\'conv-1D_1\'))\r\n    model.add(BatchNormalization(name=\'BatchNorm_1\'))\r\n    model.add(MaxPooling1D(pool_size=2, strides=1, name=\'MaxPool1D_1\'))\r\n    \r\n    model.add(layers.Conv1D(128, 3, activation=\'relu\', padding=""same"", \r\n                            kernel_initializer=""he_uniform"", \r\n                            bias_initializer=initializers.Constant(0.1), \r\n                            dilation_rate=3, \r\n                            name=\'conv-1D_2\'))\r\n    model.add(BatchNormalization(name=\'BatchNorm_2\'))\r\n    model.add(MaxPooling1D(pool_size=3, strides=1, name=\'MaxPool1D_2\'))\r\n    \r\n    model.add(layers.Conv1D(64, 3, activation=\'relu\', padding=""same"", \r\n                            kernel_initializer=""he_uniform"", \r\n                            bias_initializer=initializers.Constant(0.1), \r\n                            dilation_rate=4, \r\n                            name=\'conv-1D_3\'))\r\n    model.add(BatchNormalization(name=\'BatchNorm_3\'))\r\n    model.add(MaxPooling1D(pool_size=4, strides=1, name=\'MaxPool1D_3\'))\r\n\r\n    model.add(Bidirectional(LSTM(1024, return_sequences=True, dropout=0.5,\r\n                             recurrent_dropout=0.2), input_shape=(241, 1)))\r\n    model.add(TimeDistributed(Dense(128, activation=\'relu\'), name=\'Dense_1\'))\r\n    model.add(TimeDistributed(Dropout(0.4, name=\'TimeDistributed-Dropout_1 = 0.4\')))\r\n    model.add(TimeDistributed(Dense(64, activation=\'relu\'), name=\'Dense_2\'))\r\n    model.add(Flatten(name=\'Flatten\'))\r\n    model.add(Dropout(0.5, name=\'Dropout_1\'))\r\n    model.add(Dense(64, activation=\'relu\', \r\n                    kernel_initializer=""he_uniform"", \r\n                    bias_initializer=initializers.Constant(0.1), \r\n                    name=\'Dense_3\'))\r\n    model.add(Dropout(0.5, name=\'Dropout_2\'))\r\n    \r\n    model.add(Dense(1, activation=\'sigmoid\', name=\'Sigmoid-layer\'))\r\n    \r\n    visualize_model(model)\r\n    model = multi_gpu_model(model, gpus=8)\r\n\r\n    opt = keras.optimizers.Adadelta()\r\n    tensorboard = TensorBoard(log_dir=os.path.join(log_dir, ""TensorBoard_{}"".format(time)), \r\n                              write_graph=True, write_grads=True, batch_size=2048)\r\n    model.compile(optimizer=opt,\r\n                  loss=\'binary_crossentropy\',\r\n                  metrics=[\'accuracy\'])\r\n    \r\n    with open(os.path.join(log_dir, \'model_summary{}.txt\'.format(time)),\'w\') as fh:\r\n        model.summary(print_fn=lambda x: fh.write(x + \'\\n\'))\r\n    checkpoint_filepath = os.path.join(log_dir, ""best-model-{}"".format(time) + ""_{epoch:02d}-{val_loss:.2f}.hdf5"")\r\n    checkpoint = ModelCheckpoint(checkpoint_filepath, monitor=\'val_loss\', \r\n                                   verbose=1, save_best_only=True, mode=""min"")\r\n    earlystopper = EarlyStopping(monitor=\'val_loss\', patience=100, verbose=1)\r\n    CNN_history = model.fit(partial_seq_train, \r\n              partial_label_train, \r\n              epochs=1000, \r\n              batch_size=2048, \r\n              validation_split=0.2, shuffle=True, callbacks=[tensorboard, earlystopper, checkpoint])\r\n\r\n    return (model, CNN_history, results)\r\nCNN_1D_model = keras_1D_CNN()\r\n']",[],0,0
287,keras,7783,closed,bug in concatenation layer,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

running this:
	resnet = resnet.get_layer('activation_49').output
	flatten = Flatten()(resnet)
	seed = Input(shape=(7,))

	merged = concatenate([flatten, seed], axis=-1)

results in : zero-dimensional arrays cannot be concatenated

However, running this:
	resnet = resnet.get_layer('activation_49').output
	flatten = Flatten()(resnet)
	seed = Input(shape=(7,))

	merged = merge([flatten, seed], mode='concat', concat_axis=-1)

compiles well without errors! Bug?",stale,"['Could you give an executable (self-contained) code?', 'Are you unable to reproduce the behavior?', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', 'I had the same problem..it turned out that there was some trouble recognizing concatenate to be associated with keras for some reason..\r\n\r\ndoing this solved the issue for me\r\n\r\n`from keras.layers.merge import concatenate as cnct`\r\n`merged = cnct([flatten,seed])`']",[],[],0,0
288,keras,10661,closed,Incorrect state dimensions for GRU when using CNTK backend,"When running the following script



using TensorFlow backend:


using CNTK backend:


with CNTK Keras outputs incorrectl tensor for state.",,[],"[""\r\nfrom keras.layers import GRU, Input\r\n\r\n// 10x100 - embedded sequence of 10 tokens\r\ngru_input = Input(shape=(10, 100), dtype='float32')\r\nsequence, state = GRU(64, return_sequences=True, return_state=True)(gru_input)\r\n"", ""\r\nsequence\r\nOut[2]: <tf.Tensor 'gru_1/transpose_1:0' shape=(?, ?, 64) dtype=float32>\r\n\r\nstate\r\nOut[3]: <tf.Tensor 'gru_1/while/Exit_2:0' shape=(?, 64) dtype=float32>\r\n"", ""\r\nsequence\r\nOut[2]: Composite(Reshape): Input('input_1', [#], [10 x 100]) -> Output('Reshape328_Output_0', [#], [10 x 64])\r\n\r\nstate\r\nOut[3]: Composite(Reshape): Input('input_1', [#], [10 x 100]) -> Output('Reshape331_Output_0', [#], [10 x 64])\r\n""]",[],0,0
289,keras,7462,closed,metrics calculation with callbacks when using fit_generator with generators for both training and validation,"If generator is used for the parameter validation_data when calling fit generator, how do you calculate custom metrics at the end of each epoch? Just wanted to try and understand how others are going about this problem.

Here's the code I use:

model.fit_generator(generate_data_from_file('data/0.1-percent/training-data.tsv', binarizer, batch_size),
        validation_data=generate_data_from_file('data/0.1-percent/validation-data.tsv', binarizer, batch_size),
        steps_per_epoch=math.ceil(1.0 * 109527 / batch_size),
        validation_steps=math.ceil(1.0 * 13692 / batch_size),
        epochs=10,
        verbose=1,
        class_weight=class_weights,
        max_queue_size=2)",stale,"['I did this:\r\n\r\n```\r\nhistory = CumulativeHistory()\r\ncallbacks = [history]\r\nfrom keras import backend as K\r\nif K.backend() == \'tensorflow\':\r\n  board = keras.callbacks.TensorBoard(log_dir=f""{self.prefix_folder_logs}{time()}"",\r\n                                    histogram_freq=1, write_graph=True, write_images=True)\r\n  callbacks.append(board)\r\nmetric_to_compare = \'val_euclidean_distance\'\r\nprint(""Begin of training model..."")\r\nfor i in range(MAX_NUM_EPOCHS):\r\n  model.fit_generator(self.train_inputs, steps_per_epoch=self.train_inputs.steps_per_epoch(),\r\n                      validation_data=test_input_sequence, validation_steps=steps_test,\r\n                      max_queue_size=self.train_inputs.workers, epochs=i+1, initial_epoch=i,\r\n                      workers=self.train_inputs.workers, use_multiprocessing=True,\r\n                      callbacks = callbacks)\r\n  try:\r\n    metrics_diff = history.history[metric_to_compare][i] - min(history.history[metric_to_compare][:i])\r\n  except:\r\n    metrics_diff = -1\r\n  if metrics_diff < 0:\r\n    self._save_models(i)\r\n    self.data_processor = None  # Empty memory\r\n    best_epoch = i\r\n    num_worse_epochs = 0\r\n  elif metrics_diff > 0:\r\n    num_worse_epochs += 1\r\n    if num_worse_epochs >= PATIENCE:\r\n      print(""Ran out of patience. Stopping training."")\r\n      break\r\nprint(""End of training model."")\r\n\r\n\r\nclass CumulativeHistory(History):\r\n    \'\'\'\r\n    History does not allow resume history. This class does.\r\n    \'\'\'\r\n    def on_train_begin(self, logs=None):\r\n        if not hasattr(self, \'epoch\'):\r\n            super(CumulativeHistory, self).on_train_begin(logs)\r\n```', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', ""I have some questions about it.\r\nI am now  using fit_generator with generators for both training and validation.\r\nI've wrote a class named My_callback which is inherited from keras.callbacks.Callback, in the class,  I define several metrics like loss,val_loss,acc,val_acc. \r\nBut when run the model.fit_generator code, only loss and acc can be computed, there will be a error printed  KeyError:'val_loss'\r\n\r\nBecause my data is so large, so I split the data into 32 files, each file has 10000 samples.\r\n\r\n**my simplified code is:**\r\n\r\n    def batch_iter(xpath,ypath,batch_size,num_epochs,shuffle=True):\r\n        num_file = len(xpath)\r\n        for epoch in range(num_epochs):\r\n            for i in range(num_file):\r\n                X = np.load(xpath[i])[:,:,:,np.newaxis]\r\n                Y = np.load(ypath[i])\r\n                data_size = len(X)\r\n                num_batches_per_file = int((len(X)-1)/batch_size)+1\r\n                if shuffle:\r\n                    shuffle_indices = np.random.permutation(np.arange(data_size))\r\n                    shuffle_X = X[shuffle_indices]\r\n                    shuffle_Y = Y[shuffle_indices]\r\n                else:\r\n                    shuffled_X = X\r\n                    shuffle_Y = Y\r\n                for batch_num in range(num_batches_per_file):\r\n                    start_index = batch_num * batch_size\r\n                    end_index = min((batch_num+1)*batch_size,data_size)\r\n                    x_batch = shuffle_X[start_index:end_index]\r\n                    y_batch = shuffle_Y[start_index:end_index]\r\n                    yield (x_batch,y_batch)\r\n\r\n    def top_2_accuracy(y_true,y_pred):\r\n        return keras.metrics.sparse_top_k_categorical_accuracy(y_true,y_pred,k=2)\r\n\r\n    def top_3_accuracy(y_true,y_pred):\r\n        return keras.metrics.sparse_top_k_categorical_accuracy(y_true,y_pred,k=3)\r\n\r\n    def cnn_model():\r\n        model = Sequential()\r\n        model.add(Conv2D(filters=32,kernel_size=(3,100),padding='same',input_shape=X_train.shape[1:],))\r\n        model.add(Activation('relu'))\r\n        model.add(MaxPool2D(pool_size=(25,1),strides=None))\r\n        model.add(Flatten())\r\n        model.add(Dense(units=128,activation='relu'))\r\n        model.add(Dropout(0.25))\r\n        model.add(Dense(units=Y_trian.shape[1],activation='sigmoid'))\r\n        model.summary\r\n        opt = optimizers.Adam()\r\n        model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy',top_2_accuracy,top_3_accuracy,'sparse_top_k_categorical_accuracy'])\r\n        return model\r\n\r\n    class My_Callback(keras.callbacks.Callback):\r\n        def __init__(self,batch_interval):\r\n            self.metrics = metrics\r\n            self.batch_interval = batch_interval\r\n            self.loss = []\r\n            self.val_loss = []\r\n\r\n        def on_train_begin(self,log={}):\r\n            self.loss = []\r\n            self.val_loss = []\r\n            self.batch_number = 0\r\n            self.epoch_number = 0\r\n\r\n        def on_epoch_begin(self,logs={}):\r\n            return\r\n\r\n        def on_batch_end(self,batch,logs={}):\r\n            self['batch_number'] +=1\r\n            if self.batch_number % batch_interval == 0:\r\n                self.epoch_number +=1        \r\n                self.loss.append(logs['loss'])\r\n                self.loss.append(logs['val_loss'])\r\n            return\r\n\r\n    x_path_train,y_path_train,x_path_val,y_path_val,x_path_test,y_path_test = train_val_test_split(x_path,y_path,1,1,1)\r\n    model = cnn_model()\r\n\r\n    history = My_Callback(batch_interval=10000*len(x_path_train)/16)\r\n    model.fit_generator(batch_iter(x_path_train,y_path_train,16,10),\r\n                        steps_per_epoch = 10000*10*len(x_path_train)/16,\r\n                        validation_data = batch_iter(x_path_val,y_path_val,16,10,shuffle=False),\r\n                        validation_stes = 10000*10*len(x_path_val)/16,\r\n                        callbacks = [history])\r\n""]",[],"['', '']",0,0
290,keras,12798,closed,Enable Slack Github Integration and Notifications,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I want to the Slack Github integration to notify my team of activity in , but I received an error:
 


To reproduce:

1. Add GitHub app to your Slack workspace
2. In a channel, run 
3. See error 
",,[],"[""\r\nEither the app isn't installed on your repository or the repository does not exist. Install it to proceed.\r\n""]","['keras-team/keras', '/github subscribe keras-team/keras']",0,0
291,keras,13031,closed,Error using 3D inputs with Keras to Scikit-Learn Wrapper in Adaboost,"Hi
I have used Keras wrappers for Scikit to implement Adaboost for single input Keras models. This solution works fine for Dense layers, but not for 3D layers. The moment I try to use any 3D layers such as LSTM or SimpleRNN, the first model training runs ok but immediately thereafter I get the following error message.
ValueError: Input 0 of layer simple_rnn_1 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 3]
(To recreate this error, please install scikit-nightly build 0.22.dev0. The stable 0.21 release does not allow the Adaboost class fit method to take 3D arrays required by LSTM.)



**System information**  
- Have I written custom code (as opposed to using example directory):  This is custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  1.14.0
- Keras version:  2.2.4-tf
- Python version:  3.7
- CUDA/cuDNN version:  None
- GPU model and memory:  NA

I have tried to use a 2D input layer with Scikit stable version 0.21 and then using the Keras Reshape layer to convert to 3D input. Unfortunately, after the initial round of training, the following error message is shown. So the 2 error messages occur at the same point in the training. First all epochs finish successfully, then just when you expect the second round of training to start, this error message pops up.
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 150 values, but the requested shape has 2200
         [[{{node reshape_1/Reshape}}]] [Op:__inference_keras_scratch_graph_4029]

Logs are attached for both errors.
[tensorflow_log_RESHAPE.txt](https://github.com/keras-team/keras/files/3342360/tensorflow_log_RESHAPE.txt)
[tensorflow_log_3D_input.txt](https://github.com/keras-team/keras/files/3342361/tensorflow_log_3D_input.txt)

Any help would be highly appreciated.

Thanks

Best Regards,

Adeel
",,[],"['\r\n        def simple_model():                                           \r\n            #model.add(Reshape((X_train.shape[1],1), input_shape=(X_train.shape[1],)))\r\n            model.add(tf.keras.layers.Input(shape=(X_train.shape[1],1)))\r\n             model.add(tf.keras.layers.SimpleRNN(12, activation=\'relu\'))   \r\n            model.add(Dense(64, activation=\'relu\'))\r\n            model.add(tf.keras.layers.Dropout(0.2))\r\n            model.add(tf.keras.layers.Flatten())\r\n            model.add(tf.keras.layers.Dense(3, activation=\'softmax\'))\r\n            model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.categorical_crossentropy, metrics=[tf.keras.metrics.categorical_accuracy])\r\n            return model\r\n       \r\n        class KC2(tf.keras.wrappers.scikit_learn.KerasClassifier):\r\n            def fit(self, x,y, sample_weight, **kwargs):\r\n                kwargs[""sample_weight""] = sample_weight\r\n                return tf.keras.wrappers.scikit_learn.KerasClassifier.fit(self,x,y,**kwargs)\r\n        \r\n        ann_estimator = KC2(build_fn= simple_model, epochs=3, batch_size=50, verbose=1)\r\n        boosted_ann = AdaBoostClassifier(base_estimator= ann_estimator, learning_rate=1, n_estimators=50)\r\n        boosted_ann.fit(X_train, y_train3) # scale your training data \r\n']",[],0,0
292,keras,11833,closed,ValueError: setting an array element with a sequence.,"hello , 
i am using keras and tensorflow to implement CNN nets for edge detection , when i tried to run the code ,  this error have occurred . so I would like that you help me to solve it. thanks in advance.
here is the code : 


[from keras.models import Sequential
from keras.layers import (Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D)
from keras.utils import np_utils
from keras import backend as K
K.set_image_dim_ordering('th')
import json, pylab
import cv2
import numpy as np


np.set_printoptions(threshold=np.nan)



# some model and data processing constants
batch_size = 128
nb_classes = 2
nb_epoch = 7

# input image dimensions
img_rows, img_cols = 48, 72

# number of convolutional filters to use
nb_filters = 32
# size of pooling area for max pooling
nb_pool = 5
# convolution kernel size
nb_conv = 3

# architecture
model = Sequential()
model.add(Conv2D(nb_filters, (nb_conv, nb_conv),
                        padding='valid',
                        input_shape=(1, img_rows, img_cols)))
model.add(Activation('relu'))
model.add(Conv2D(nb_filters, (nb_conv, nb_conv)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(128))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(nb_classes))
model.add(Activation('softmax'))
model.compile(loss='sparse_categorical_crossentropy',
              optimizer='adadelta',
              metrics=['accuracy'])




train_imgs=(cv2.imread('1.jpg',0))
ground_truth_train=(cv2.imread('1edges.jpg',0))


test_imgs=(cv2.imread('2.jpg'),0)
ground_truth_test=(cv2.imread('2edges.jpg',0))





print('Preparing images...')

X_train = np.array(train_imgs)
X_test = np.array(ground_truth_train)
y_train = np.array(test_imgs)
y_test = np.array(ground_truth_test)
# prepare the data

X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)
X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)



X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255
print('X_train shape:', X_train.shape)
print(X_train.shape, 'train samples')
print(X_test.shape, 'test samples')

# convert class vectors to binary class matrices
# convert class vectors to binary class matrices

from keras.utils import to_categorical 
y_train = np.divide(y_train,255)
y_test = np.divide(y_test,255)
Y_train = to_categorical(y_train,nb_classes)
y_test = to_categorical(y_test,nb_classes)



# train it plz
print('Training model...')


model.fit(X_train, Y_train ,
          batch_size=batch_size,
          nb_epoch=nb_epoch, verbose=1,
          validation_data=(X_test,y_test) )

# let's dump the model
print('Saving model...')
saved_model = model.to_json()
with open('CNN_architecture.json', 'w') as outfile:
    json.dump(saved_model, outfile)
model.save_weights('CNN_weights.h5')
](url)",To investigate,"['Can you provide the stacktrace and format your code snippet? Thanks! ', ""![image](https://user-images.githubusercontent.com/44825125/49735542-6c8f8480-fcc2-11e8-8466-d14cf2d3e632.png)\r\ni think , the error occurs in these steps,  also it was showing me this  Y_train = to_categorical(y_train,nb_classes) ,  y = np.array(y, dtype ='int')"", ""still i didn't solve that problem , i would like that you help me \r\n""]",[],[],0,0
293,keras,11172,closed,Recurrent Attention API for keras,"This issue is opened to host a discussion about the recurrent attention API for keras. 
Related issues:

#11142.
#8296.
#7633.
",type:feature,"[""From @fchollet in #11142:\r\n\r\n> I don't think this PR is a good fit to implement attention models at this point.\r\n\r\n>I believe the Model subclassing API would make this use case far easier.\r\n\r\n>We should come up with side by side code examples of similar models to figure out whether that is the case (the code doesn't have to work, it would just show the workflow with each API, the API in this PR and the model subclassing API).\r\n\r\n> Possibly, but first we should make a decision: what's the best way to implement attention models in the Keras API today, and would this PR provide a better API?\r\n\r\n>To make this decision we need to write quick code examples showing how to build common attention models 1) with the model subclassing API, 2) with the API introduced by #8296, and figure out what looks best."", ""From @farizrahman4u in #11142.\r\n\r\n>Shouldn't we first see what an attention model written in Keras (as of now) looks like? Sometimes you can get away with a well documented example (like @fchollet did with seq2seq) instead of making huge changes to the code base to fit niche cases. I would suggest writing an example using a simpler attention model (not MixGaussian.. it has way too much internal knobs) using existing Keras features and see where it goes."", 'Great to see this moving 👍I\'m happy cont. the work as well.\r\n\r\nThis was the result of my ""requirements analysis"" (based on common use-case/authoritative papers) as of the [API doc](https://docs.google.com/document/d/1psFXnmMlSTg5JapgZKz26ag-zBu3ERrxkKoEzNpzl4w/edit?usp=sharing):\r\n> 1. The attention mechanism should be implemented separately from the core cell so that it can be reused with any standard keras RNN Cells.\r\n> 2. Besides the attended, the attention mechanism should have access to the core cell’s states as well as the input at each timestep for computing the attention representation at that timestep.\r\n> 3. The attention mechanism should be allowed to be recurrent, i.e. “have state(s) on its own” that is forwarded to the next attention computation.\r\n> 4. One should (optionally) be able to get the sequence of attention representations from each timestep as output, for use in later stacked (recurrent) layers.\r\n> 5. It should be easily configurable whether the attention is applied “before or after” (in sequence/time dimension) the core recurrent transformation at each timestep.\r\n> 6. Masking should be supported and have the same behaviour as for regular keras recurrent layers.\r\n> 7. The attended should be allowed to consist of multiple inputs (tensors) of different shape.\r\n> 8. It should require minimal efforts for users to write custom attention mechanisms that fulfil requirement 1 - 7.\r\n\r\nThese should be useful as a reference when discussing tradeoffs and priorities.', 'Are we talking about Attention only in recurrent models?', 'Should we consider only the more common dot-product attention or everything else with an overridable method that computes weights?', 'I think it is important to not be limited to rnn design. I.e. See https://openreview.net/forum?id=HyGBdo0qFm', ""@douglas125 \r\n> Should we consider only the more common dot-product attention or everything else with an overridable method that computes weights?\r\n\r\nThe latter is what's achieved by [this method](https://github.com/keras-team/keras/pull/8296/files#diff-9c4188ddc4dd173d80f64feed5b89412R125) of the suggested base class for _recurrent_ attention mechanisms, as explained [here](https://github.com/keras-team/keras/pull/8296/files#diff-9c4188ddc4dd173d80f64feed5b89412R22).\r\n"", '@bhack \r\n> I think it is important to not be limited to rnn design. I.e. See https://openreview.net/forum?id=HyGBdo0qFm\r\n\r\nGenerally, I agree. The different approach taken in e.g. [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) is of great importance and should be considered for any seq2seq problem. However (!) I\'d claim that any feedforward architecture can already be supported by the existing Keras API. You might have to write several custom layers but feedforward attention can be implemented by reusing, and without duplicating, major parts of the existing API (along the lines of @farizrahman4u commet cited [above](https://github.com/keras-team/keras/issues/11172#issuecomment-422329888))\r\n\r\nRecurrent attention is different; if you want to implement it in a new layer/model you need to reimplement the majority of the RNN logic _or wrap the cell_ as is suggested in the previous [PR](https://github.com/keras-team/keras/pull/8296).\r\n\r\nThat said, it might still make sense to add layers or models to the API for non-recurrent attention. But I think it is still high prio to support _recurrent_ attention as was initially in the ""request for contributions"".\r\n', 'In a sense, I think recurrent attention ""is also supported"" since we added [support for constants](https://github.com/keras-team/keras/pull/7980) in the RNN - you can quite easily write your own cell wrapper. To me, It\'s just a question of how standardized and simple you want to make this and if/what ready (cell-wrapper) attention mechanisms should be added to the API.\r\n\r\nThe main current limitation with this approach is that there is no option to return ""state sequences"" from the RNN, which is required to feed the attention encoding from on layer to subsequent layers (see point [3) here](https://github.com/keras-team/keras/pull/8296#issue-149413767)).  ', 'Ok. So usage workflow examples have been requested. Since the heading of this and all preceding issues/PRs has been *recurrent* attention I\'ll focus on this and repeat/clarify the workflow of the only concrete suggestion so far. I\'ll use the architecture in [this paper](https://arxiv.org/pdf/1308.0850.pdf) for handwriting synthesis as the use-case (but the workflow would be the same for e.g. this kind [image captioning](https://arxiv.org/pdf/1502.03044.pdf))\r\n```\r\nxy = Input((None, 2))  # coordinates of handwriting trajectory\r\ntext = Input((None, n_characters))  # sequence of characters to be synthesized\r\n\r\ncell = MixtureOfGaussian1DAttention(LSTMCell(64), components=3, heads=2)\r\nattention_lstm = RNN(cell, return_sequences=True)\r\nh = attention_lstm(xy, constants=text)\r\nxy_pred = TimeDistributed(Dense(2, activation=None))(h)\r\n# MoG output is used instead of basic regression in the original paper, but this is has nothing to do with the attention mechanism and is left out for brevity.\r\n\r\nmodel = Model(inputs=[xy, text], outputs=xy_pred)\r\nmodel.compile(optimizer=\'adam\', loss=\'mse\')\r\nmodel.fit([xy_data[:, :-1], text_data], xy_data[:, 1:])\r\n```\r\nI think that this workflow is perfectly aligned with [Keras guiding principles](https://github.com/keras-team/keras#guiding-principles). Note that no modification of existing classes is required, we\'ve just defined a new `RNNCell`. For the sake of modularity it reuses/wraps the `LSTMCell`, which can be replaced by e.g. the `GRUCell`. This use-case and workflow were one of the main drivers for breaking out the `RNNCell` and adding support for `constants` in the `RNN`.\r\n\r\nI honestly can\'t come up with a reasonable alternative based on the Model subclassing API for this use case. I guess these are other options:\r\n\r\n```\r\nattention_lstm = MixtureOfGaussian1DAttentionRNN(\r\n    cell=LSTMCell(64),\r\n    components=3,\r\n    heads=2,\r\n    return_sequences=True\r\n)\r\nh = attention_lstm(xy, attended=text)\r\n```\r\nHere, `MixtureOfGaussian1DAttentionRNN` would be a standalone layer where the attention mechanism has been coupled tightly with the general `RNN` logic (why I think it is a bad option).\r\n\r\nAlternatively something like:\r\n```\r\nattention_lstm = AttentionRNN(\r\n    cell=LSTMCell(64),\r\n    attention_mechanism=MixtureOfGaussian1DAttention(components=3, heads=2),\r\n    components=3,\r\n    heads=2,\r\n    return_sequences=True  # here return_state_sequences could be supported or other\r\n)\r\nh = attention_lstm(xy, attended=text)\r\n```\r\nWhere both the core cell and and attention mechanism are injected into an new class that connects them. But this requires new interfaces both for the `attention_mechanism` and for the `AttentionRNN` which seems completely unnecessary since the existing `RNNCell` interacface already supports attentive cells (thanks to addition of `constants`).\r\n\r\n**Bottom Line**\r\n1) Is this use-case at all of relevans!?\r\n2) Is there any problem with the suggested approach? If so, please be concrete and/or provide an alternative!\r\n3) If we can agree that _recurrent_ attention is already supported by using an appropriate (attentive) `RNNCell` - do you feel that there is a need to add ""standard"" attentive cells, such as `MixtureOfGaussian1DAttention` in this example, to the official keras API?\r\n\r\n@fchollet @farizrahman4u @gabrieldemarmiesse ', '* Thinking in terms of reducing the number of mental models (cognitive load), I vote for the first one.\r\n\r\n* Even in that case, I think the ""MixtureOfGaussian1DAttention"" wrapper is too niche to make its way into the core api (I might be wrong). \r\n\r\n* We definitely need a standard way to do attention in Keras. I think we should come up with an end to end example, which includes the attention wrapper definition that users can easily extend. For simplicity, use the simplest attention mechanism possible.(`MixtureOfGaussian1DAttention` has too many moving parts).\r\n', 'I have a feeling that some uncertainty comes from the (motivated!) fuzz about non-recurrent attention mechanisms. If we were to add support for the transformer architecture in [Attention is All you Need](https://arxiv.org/pdf/1706.03762.pdf) (or the recent [BERT](https://arxiv.org/pdf/1810.04805.pdf)), I definitely think that Model subclassing API is a good place to start - because there are many intricate parts that should be combined the right way. It would look something like:\r\n```\r\ninput_sequence = Input((None, 1))\r\ntarget_sequence_tm1 = Input((None, 1))\r\ntransformer = Tranformer(\r\n    input_tokens=n_input_tokens,  # must be provided if embeddings created internally\r\n    target_tokens=n_target_tokens,  # must be provided if embeddings created internally\r\n    units_model=512,\r\n    units_ff=2048,\r\n    units_keys=64,\r\n    units_values=64,\r\n    layers=6,\r\n    heads=3\r\n)\r\ntarget_sequence_pred = transformer([input_sequence, target_sequence_tm1])\r\n```\r\nWhere `Tranformer` would subclass `Model`. It could also be motivated to expose some of the internals as separate sub-models or layers, which raises the question if the same ""Attention layers"" can be reused both in the feedforward and recurrent setting. This would be possible (as per my previous suggestion) by defining cell transformations using the functional API, e.g.:\r\n```\r\n# define complete attentive cell using functional API\r\nunits = 32\r\nxy_t = Input((2,))\r\ntext = Input((None, n_characters))  # note that this is a sequence\r\nh_tm1 = Input((units,))\r\nc_tm1 = Input((units,))\r\nh_att_t = AttentionMechanism(inputs=concatenate([xy_t, h_tm1]), attended=text)\r\nx_t = concatenate([xy_t, h_att_t])\r\nh_t, c_t = LSTMCell(units)(inputs=x_t, states=[h_tm1, c_tm1])\r\ncell = CellModel(  # creates a valid cell implementation based on functional definition\r\n    inputs=xy_t,\r\n    outputs=h_t,\r\n    input_states=[h_tm1, c_tm1],\r\n    output_states=[h_t, c_t],\r\n    constants=text\r\n)\r\n```\r\nBut we should probably avoid considering this for now and make as few additions as possible. This is why [it was decided](https://github.com/keras-team/keras/pull/8296#issuecomment-344007047) to not add the `RNNAttentionCell` base class to the API. ...In the end, the only API addition of the previous [PR](https://github.com/keras-team/keras/pull/8296) was _one first attentive_ [_RNNCell_](https://github.com/keras-team/keras/pull/8296/files#diff-9c4188ddc4dd173d80f64feed5b89412R370).', 'Thanks @farizrahman4u, makes sense. So you think something like the `RNNAttentionCell` base class _should_ be added to the API, to simplify custom implementations? Please let me know if you have suggestions for a simpler, yet relevant, first attention mechanism and corresponding use-case.', 'I think [this version of machine translation](https://arxiv.org/pdf/1409.0473.pdf) (Bengio 2016) would serve as a good end-to-end example. Keras implementation of the paper would look like this:\r\n```\r\ninput_sentence = Input((None,))  # sequence of word idxs language A\r\ntarget_sentence = Input((None,))  # sequence of word idxs language B\r\ninput_embeddings = Embedding(n_input_tokens, 620)(input_sentence)\r\ntarget_embeddings = Embedding(n_target_tokens, 620)(target_sentence)\r\ninput_encoding = Bidirectional(LSTM(1000))(input_embeddings)\r\nh = RNN(\r\n    DenseAnnotationAttention(LSTMCell(1000)),\r\n    return_sequences=True\r\n)(target_embeddings, constants=input_encoding)\r\ntarget_sentence_pred = TimeDistributed(Dense(n_target_tokens, activation=None))(h)\r\n# NOTE the paper uses ""deep output (Pascanu et al., 2014) with a single maxout hidden layer""\r\n```\r\nWhere `DenseAnnotationAttention` subclasses `RNNAttentionCell`. It computes the attention weight for each `input_encoding_t` just by using a single hidden layer MLP that takes the `h_lstm_t` and `input_encoding_t` as input - and is thus considerable simpler than the MoG1DAttention.\r\n\r\nSounds good? @farizrahman4u @fchollet If so I\'ll implement the attention mechanism and end-to-end example.', 'Fair enough. I think you can write the whole thing in the example (including the RNNAttentionCell class), submit a PRs, and discuss with @fchollet on what parts can moved into Keras API and what should stay in the example.', 'As per @farizrahman4u suggestion above, please see https://github.com/keras-team/keras/pull/11421 @fchollet ', ""@andhus thanks for making the PR #11421!\r\n\r\nAs a Keras fan eagering to implement attention mechanism, \r\ndo you think it's okay for me to start using the `recurrent_attention_machine_translation.py` you provide as a example, or should I wait until the PR is merged?\r\n\r\nThanks in advance."", 'Attention for Dense Networks on Keras RFC:\r\nhttps://github.com/tensorflow/community/pull/54\r\n', 'Is there an update on this? The RFC has been approved for over a year']",[],[],0,0
294,keras,12838,closed,Documentation Required GAN,"<em>Please make sure that this is a Bug or a Feature Request and provide all applicable information asked by the template.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.</em>  

**System information**  
- Have I written custom code (as opposed to using example directory):  
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  latest update
- Keras version:  up to date
- Python version:  up to date
- CUDA/cuDNN version:  -
- GPU model and memory: - 

The keras.io has no documentation on the Generative Adversial Network and No examples too, I guess there should be an section for this too",type:docs,"['Agreed. Feel free to make a PR.', 'Thanks @hr21 for raising this issue.\r\nCan we expect another example for GAN along with common MNIST, Fashion MNIST datasets?\r\nI feel that would be a benefit for all.']",[],[],0,0
295,keras,12944,closed,Request for Mixed-precision Training,"Relating to #12249

I am training an autoencoder. The feature vectors extracted from the hidden layers in the middle have to be stored in uint8 as well and utilised to infer the decoder later. Furthermore,  my output from the autoencoder, that is, the reconstructed input, has to compare with the original input in a uint8 precision only. But other values (weights, in/outputs in other layers etc.) can be float32 or float16. 

So has Keras supported such a mixed-precision training? If yes, may I know where the doc is? If not, can it be added? That is, allowing programmers to specify precision for each layer?

Thanks",,[],[],[],0,0
296,keras,10424,closed,Feature: cosine distance with negative sampling,"I am trying to train a model that uses cosine distance with negative samples. That is, the loss function is based on multiple values instead of a single y_true to y_pred comparison.

I noticed two things. One, there is no cosine distance with negative samples function for loss, as can be found in [CNTK (cosine_distance_with_negative_samples)](https://docs.microsoft.com/en-us/python/api/cntk.losses?view=cntk-py-2.5.1). Two, a loss function, including custom functions, must conform to the signature of LOSS(y_true, y_pred). I do not see any reason for this, as other libraries (such as Theano, CNTK and tensorflow) are able to use a loss function with multiple arguments.

I was wondering what was holding the implementation of these features back. If it is just because it is not a priority, I may go ahead and try to implement it myself.",,[],[],[],0,0
297,keras,7857,closed,ReshapeBatch (aka reshape_with_batch) makes CNTK model unable to load from C++,"I have a model which runs some 2D convolutions on an image, and then turns that image into a sequence, and runs some GRU layers from left to right on vertical slices of the image. I can train the model, but when I try and load it from C++, using CNTK, it fails, because the user-defined function reshape_with_batch is defined in Keras python code.

I'm using this for OCR, and here is the model:

",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']","[""python\r\ndef make_input_shape(width, height):\r\n    if K.image_data_format() == 'channels_first':\r\n        return Channels, width, height\r\n    else:\r\n        return width, height, Channels\r\n\r\nimg_input = Input(name='img_input', shape=make_input_shape(width, height), dtype='float32')\r\n\r\nb = img_input\r\nfor i in range(0, 4)\r\n    b = Conv2D(32, (5, 5), activation='relu', padding='same')(b)\r\n    b = Dropout(0.25)(b)\r\nb = Reshape(target_shape=(width, height * 32), name='reshape')(b)\r\n\r\n# Two layers of bidirectional GRUs\r\ngru_1 = GRU(rnn_size, return_sequences=True, name='gru1')(b)\r\ngru_1b = GRU(rnn_size, return_sequences=True, go_backwards=True, name='gru1_b')(b)\r\nb = keras.layers.concatenate([gru_1, gru_1b])\r\ngru_2 = GRU(rnn_size, return_sequences=True, name='gru2')(b)\r\ngru_2b = GRU(rnn_size, return_sequences=True, go_backwards=True, name='gru2_b')(b)\r\nb = keras.layers.concatenate([gru_2, gru_2b])\r\n\r\n# transform RNN output to character activations. I don't know how, but keras knows here that\r\n# num_classes applies only to the inner-most dimension. It leaves the outer dimension (width) alone.\r\nb = Dense(num_classes, name='y_pred', activation='softmax')(b)\r\ny_pred = b\r\nseq_model = Model(inputs=img_input, outputs=y_pred)\r\n""]",[],0,0
298,keras,10330,closed,Call model error,"Hi, I had some problems：
       The following error occurs when I call the model，But not when you're training.

def exponent_neg_manhattan_distance(left, right):
    """"""Helper function for the similarity estimate of the LSTMs outputs""""""
    return K.exp(-K.sum(K.abs(left - right), axis=1, keepdims=True))
--------------------------------------------------------------------------------------------------
 # Calculates the distance as defined by the MaLSTM model
    if distanceFunc == 'exponent_neg_manhattan_distance':
        malstm_distance = Merge(mode=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),
                               output_shape=lambda x: (x[0][0], 1))([left_output, right_output])
--------------------------------------------------------------------------------------------------
ERROR：
/Users/bai/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1269: UserWarning: The  layer is deprecated and will be removed after 08/2017. Use instead layers from , e.g. , , etc.
  return cls(**config)
Traceback (most recent call last):
  File ""/Users/bai/pyproject/cikmAnalytiCup/cikm/train_model.py"", line 264, in <module>
    test()
  File ""/Users/bai/pyproject/cikmAnalytiCup/cikm/train_model.py"", line 249, in test
    model = model_from_json(json_string)
  File ""/Users/bai/anaconda3/lib/python3.6/site-packages/keras/models.py"", line 349, in model_from_json
    return layer_module.deserialize(config, custom_objects=custom_objects)
  File ""/Users/bai/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py"", line 55, in deserialize
    printable_module_name='layer')
  File ""/Users/bai/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py"", line 143, in deserialize_keras_object
    list(custom_objects.items())))
  File ""/Users/bai/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py"", line 2517, in from_config
    process_node(layer, node_data)
  File ""/Users/bai/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py"", line 2476, in process_node
    layer(input_tensors, **kwargs)
  File ""/Users/bai/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py"", line 617, in __call__
    output = self.call(inputs, **kwargs)
  File ""/Users/bai/anaconda3/lib/python3.6/site-packages/keras/legacy/layers.py"", line 208, in call
    return self.mode(inputs, **arguments)
  File ""/Users/bai/pyproject/cikmAnalytiCup/cikm/train_model.py"", line 156, in <lambda>
    malstm_distance = Merge(mode=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),
NameError: name 'exponent_neg_manhattan_distance' is not defined

I hope to get some help",,[],[],"['Merge', 'keras.layers.merge', 'add', 'concatenate']",0,0
299,keras,12945,closed,model.evaluate_generator may give wrong results when using multiprocessing.,"When evaluating same data with different parameters, it came with different results.


I guess this is related to the warning
use_multiprocessing=Truekeras.utils.Sequence class.
test_data_flowflow_from_directory` which I believe most people would use.

So is it more reasonable to raise an Error than give a warning to protect users from getting wrong results?",,[],"['bash\r\nmodel.evaluate_generator(test_data_flow, use_multiprocessing=True, workers=3, steps=int(test_data_flow.samples/test_data_flow.batch_size))\r\n# [0.11300370331468847, 1.0] \r\n\r\nmodel.evaluate_generator(test_data_flow, steps=int(test_data_flow.samples/test_data_flow.batch_size), workers=3)\r\n# [0.3496840471564107, 0.8672641373341027]\r\n\r\nmodel.evaluate_generator(test_data_flow, steps=int(test_data_flow.samples/test_data_flow.batch_size))\r\n# [0.3496840471564107, 0.8672641373341027]\r\n']","['', 'bash\r\nUserWarning: Using a generator with ', ' and multiple workers may duplicate your data. Please consider using the', '', '\r\n\r\nHowever, ', ' is from ']",0,0
300,keras,13088,closed,Initialize LSTM initial state manually,"I want to initialize the initial state of an LSTM layer with the final hidden state of another LSTM layer. Basically, I want to implement the _conditional encoding_ as explained in this paper https://www.aclweb.org/anthology/D16-1084 

![image](https://user-images.githubusercontent.com/32245327/60956724-fec33e00-a320-11e9-8ccd-6c8564c626ac.png)

How do I access the final hidden state of one LSTM layer and use it as the initial state of another LSTM layer?
Thanks",type:support,"['Try to look at source code of LSTMCell, LSTM, RNN classes', 'Hi. Could you be able to implement LSTM with conditional encoding? I have the same question on how to initialize the second LSTM using the last hidden state of the first LSTM. ', ""I'll share a snippet of my code. The `article_lstm` is initialized with the final states of `headline_lstm`.  \r\n\r\n```\r\nheadline_input = Input(shape=(25,1024,),name='headline_input')\r\nheadline_lstm = LSTM(64,name='headline_lstm',return_state=True)\r\nheadline_output,state_h,state_c = headline_lstm(headline_input)\r\nfinal_headline_states = [state_h,state_c]\r\n\r\narticle_input = Input(shape=(350,1024,),name='article_input')\r\narticle_lstm = LSTM(64,name='article_lstm',return_state=True)\r\narticle_outputs,_,_ = article_lstm(article_input,initial_state=final_headline_states)\r\ndense = Dense(100,activation='relu',name='dense')(article_outputs)\r\noutput = Dense(4,activation='softmax',name='output')(dense)\r\n\r\nmodel_cond = Model(inputs=[headline_input,article_input],outputs=output)\r\nmodel_cond.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\r\nmodel_cond.fit([y,body_text],Y,epochs=1)\r\n```\r\nHope this helps. "", ""May I ask how did you feed the sentences to the first layer (headline_input = Input(shape=(25,1024,),name='headline_input'))? Did you have an embedding layer before this line? "", 'Yes. I actually calculated contextual embedding for my text and saved it to disk and then directly fed them to the LSTM. The` (25,1024 )` shape corresponds to sequence-length and the embedding/feature vector respectively. If I remember correctly, these are either extracted from ELMo or BERT.', 'Thank you. Did you possibly release the code? I am working on this for a while and it would be really helpful to look at your code. ', ""The code has not been released yet. You can mail me at kushalj001@gmail.com. I'll share some files with you."", 'Thanks. I just emailed you. ']",[],[],0,0
301,keras,12981,closed,Conv3D dilation increases batch size?,"**System information**  
- Have I written custom code (as opposed to using example directory):  No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  1.9.0
- Keras version:  2.2.4
- Python version:  3.5.2
- CUDA/cuDNN version: 9.0  
- GPU model and memory:  NVIDIA P6000

**Describe the current behavior**  

I'm trying to use dilated 3D convolutions. However, when I declare one, like

I run out of memory. It says it cannot allocate a tensor of .

**Describe the expected behavior**  

Given that the batch size is set to 64, where does that 512 come from?
",stat:awaiting response type:support,"['@HansLeonardVanBrueggemann Can you provide a standalone code to reproduce the issue?  Can you check by lowering the number of filters. Thanks!', ""I can't provide the data, but the network looks like this\r\n\r\n```\r\n    dropout, noise = 0.2, 0.02\r\n\r\n    inflow = keras.layers.Input(shape=(48, 48, 48, 1), name='input')\r\n    x = keras.layers.GaussianNoise(stddev=noise, name='input_noise')(inflow)\r\n\r\n    x = keras.layers.Conv3D(32, 3, dilation_rate=2, padding='same')(x)\r\n    x = keras.layers.LeakyReLU()(x)\r\n    x = keras.layers.Dropout(rate=dropout)(x)\r\n    x= keras.layers.AveragePooling3D(pool_size=2, strides=2)(x) \r\n\r\n    x = keras.layers.Conv3D(64, 3, dilation_rate=2, padding='same')(x)\r\n    x = keras.layers.LeakyReLU()(x)\r\n    x = keras.layers.Dropout(rate=dropout)(x)\r\n    x= keras.layers.AveragePooling3D(pool_size=2, strides=2)(x) \r\n\r\n    x = keras.layers.Conv3D(128, 3, dilation_rate=2, padding='same')(x)\r\n    x = keras.layers.LeakyReLU()(x)\r\n    x = keras.layers.Dropout(rate=dropout)(x)\r\n    x= keras.layers.GlobalAveragePooling3D()(x) \r\n\r\n    x = keras.layers.Dense(1, activation='sigmoid')(x)\r\n\r\n    model = keras.models.Model(inflow, x)\r\n```\r\n\r\ninput tensors are sized [64, 48, 48, 48, 1]""]",[],"[""x = keras.layers.Conv3D(32, 3, dilation_rate=2, padding='valid', name='conv0')(x)"", '[512, 32, 22, 22, 22]']",0,0
302,keras,9552,closed,"Why are arguments missing from Keras documentation, e.g. GRU() and LSTM()?","Hi All:

An earlier submission of similar topic disappeared!
So i am doing this twice.
When I looked at code for section 
""6.3-advanced-usage-of-recurrent-neural-networks""
of the book, input arguments like input_shape for 
layers.GRU() is missing from keras documentation: 

https://keras.io/layers/recurrent/#gru
https://keras.io/layers/recurrent/#lstm

i also looked a recurrent,py and it does 
not have any local variable for input_shape 
for classes recurrant and GRU.

What did I missed?
All this missing documentation makes learning very difficult.

Where can I locate all the input arguments 
for a particular class?



",,[],[],[],0,0
303,keras,12690,closed,Custom regularizer does not remain the same object,"Short version: Custom regularizer object seems to be reconstructed with each batch.

Full version:
I'm trying to create a custom Keras regularizer that uses the distance of the layer's weights from it's original weights, but what I used doesn't work and I get a zero difference at all times.

This is the regularizer code:



(I'm using tensorflow as the backend)

After playing with this class a bit, I noted something strange: It's as if the regularizer object is being created over and over again in the training with each batch, which does explain why I'm getting zeros.
I got to this conclusion by changing the class to -



And seeing that the loss does in fact suffer the penalty that follows from _ugly_check being 1 throughout the training.

I would expect the regularizer object to remain the same one throughout training. Is this a bug or am I not understanding the usage of custom regularizers correctly?",,[],"['python\r\nclass NormReg():\r\n    def __init__(self, coeff):\r\n        self._coeff = coeff\r\n        self._original_weights = None\r\n\r\n    def _norm(self, weight_matrix):\r\n        return K.sum(K.square(weight_matrix))\r\n\r\n    def __call__(self, weight_matrix):\r\n        if self._original_weights is None:\r\n            self._original_weights = weight_matrix\r\n\r\n        diff_matrix = weight_matrix - self._original_weights\r\n        return self._coeff * self._norm(diff_matrix)\r\n', 'python\r\nclass NormReg():\r\n    def __init__(self, coeff):\r\n        self._ugly_check = 1\r\n        self._coeff = coeff\r\n        self._original_weights = None\r\n\r\n    def _norm(self, weight_matrix):\r\n        return K.sum(K.square(weight_matrix))\r\n\r\n    def __call__(self, weight_matrix):\r\n        if self._original_weights is None:\r\n            self._original_weights = weight_matrix\r\n        if self._ugly_check == 1:\r\n            self._ugly_check = 0\r\n            return 10000\r\n        diff_matrix = weight_matrix - self._original_weights\r\n        return self._coeff * self._norm(diff_matrix)\r\n']",[],0,0
304,keras,12830,closed,Using advanced activation functions,"Quick question regarding use of advanced activation functions. Currently working with some RNN's on some regression problem. Let's say for the sake of question that my model looks something like the following:

model.add(LSTM())
model.add(ELU())
...

Now my question refers to that use of ELU activation function, by default the LSTM has 'tanh' set as an activation function, does adding ELU on top of that makes the data flow through LSTM -> Tanh -> ELU?

Regards, ",,[],[],[],0,0
305,keras,9076,closed,Rough estimate of number of parameters and LSTM architecture..,"Greetings all,

The problem that I am working is a binary classification. I have around 150 sequences, where each sequence has 130000 timesteps, where each timestep has 2 features, shape=(1,130000,2). Each of these sequences is labelled as ""1"" or ""0"".  I want to train an lstm network, where at the end, giving it a sequence of 130000 timesteps with 2 features each, will predict ""1"" or ""0"". Hence the problem as I have it in my mind is ""many to one"" right? 

1)If I keep for training 120 of these sequences and the rest 30 for validation, the number of parameters of the model, should be smth like 120x130000x2=31,200,000 parameters?
For example an LSTM layer with ~4000 units?

2)The network should be stateful? or not

The model as I am thinking it is smth like:



I tried for one epoch to fit it, but the memory gets quite high! Any recommendations for the whole problem approach?",,"['@xrtc21 The LSTM parameters are 4 * (input_dim * output_dim + output_dim * output_dim + output_dim). They do not depend on the number of training samples or the sequence length. In your case they would be 4 * (2 * 4000 + 4000 * 4000 + 4000) = 64,048,000.\r\nThe feed-forward (Dense) parameters are input_dim * output_dim + output_dim = 4000 * 1 + 1 = 4,001.\r\n\r\nAlso, I would advise you to re-think the 130,000 sequence length number. See if you can get it down to a few hundreds at most, not in the thousands.', 'What you mean is to have more samples instead and less timesteps for each one?\r\n\r\n1. Should I train then the model in batches of 32,64 after this transform? If my dataset has shape X_train.shape=(thousands, hundreds,2)\r\n2. Regarding the stateful status?\r\n3. Regarding the architecture of the model in the approach you mention. In order to decide how many lstm units I need which approach should I follow?\r\n\r\nThanks @tomastheod-ITI \r\n', '@xrtc21 Yes, more samples of shorter sequences. Unfortunately, there is no general answer to your questions. You can choose ""reasonable"" starting values for your parameters and experiment; one variable at a time.\r\nFor example, start with a batch size of 64, stateful = False and 100 LSTM neurons. Choose a sequence length that makes sense for your problem. Trying to predict the next word in a sentence based on the previous 20 words is reasonable; based on the previous 2000 words, not so much.']","[""\r\nmodel = Sequential()\r\nmodel.add(LSTM(units=4000,batch_input_shape=(1, 130000, 2),return_sequences=False,stateful=True, kernel_initializer='RandomNormal'))\r\nmodel.add(Dense(1, activation='sigmoid'))\r\nmodel.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\r\n\r\nhistory = model.fit(X_train, Y_train,validation_data=(X_test, Y_test), shuffle=False, epochs=5, batch_size=1, verbose=1)\r\n""]",[],0,0
306,keras,9526,closed,Variational Autoencoder Custom Loss function,"In running the variational_autoencoder.py file in keras/examples/, the following error is encountered - 
TypeError: compile() missing 1 required positional argument: 'loss'

On adding the vae_loss as the loss parameter in compile (i.e., vae.compile(optimizer='rmsprop'), loss=vae_loss) instead of vae.add_loss(vae_loss)), the following error is encountered - 

TypeError: Using a  as a Python  is not allowed. Use  instead of  to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.

I think the problem is that we need to define a custom loss function for Keras in terms of a Python function that acts on y_pred and y_true, and this is not being done here.",,['simply use `compile(loss=None)`'],[],"['tf.Tensor', 'bool', 'if t is not None:', 'if t:']",0,0
307,keras,10591,closed,Differences in weight data structures for the Sequential and functional API?,"I just had an interesting situation where I built the same network architecture using the Sequential and the functional api, however if I train the one and transfer the weights to the other like so:
model2.set_weight(model1.get_weight())
and I don't get the same outputs.
I used the following code to construct the models:

# Using Sequential
model1 = Sequential()
model1.add(GRU(15, activation=""relu"", input_shape=(1052,12), return_sequences=True))
model1.add(Dense(1))

# Using the functional API
inlayer= Input(shape=(1052,12))
hidden= GRU(15, activation=""relu"", return_sequences=True)(inlayer)
outlayer = Dense(1)(hidden) 
model2 = Model(inputs=inlayer, outputs=outlayer)

It is easy to get around this by sticking to the functional API, but why would this situation occur, and is it as intended?",,"['Have you compiled your model after setting the weights?\r\nThis works for me.\r\n```python\r\nimport numpy as np\r\n\r\nfrom keras import Sequential, Input, Model\r\nfrom keras.layers import GRU, Dense\r\n\r\nmodel1 = Sequential()\r\nmodel1.add(GRU(15, activation=""relu"", input_shape=(1052, 12), return_sequences=True))\r\nmodel1.add(Dense(1))\r\nmodel1.compile(\'sgd\', \'mse\')\r\nmodel1.fit(np.random.normal(size=[40, 1052, 12]), np.random.normal(size=[40, 1052, 1]), batch_size=10, epochs=1, verbose=0)\r\n\r\ndummy = np.random.normal(size=[10, 1052, 12])\r\nmodel1_pred = model1.predict_on_batch(dummy)\r\n\r\ninlayer = Input(shape=(1052, 12))\r\nhidden = GRU(15, activation=""relu"", return_sequences=True)(inlayer)\r\noutlayer = Dense(1)(hidden)\r\nmodel2 = Model(inputs=inlayer, outputs=outlayer)\r\nmodel2.set_weights(model1.get_weights())\r\nmodel2.compile(\'sgd\', \'mse\')\r\nmodel2_pred = model2.predict_on_batch(dummy)\r\n\r\nnp.testing.assert_allclose(model1_pred, model2_pred)\r\n```']",[],[],0,0
308,keras,7244,closed,Masking with BiDirectional LSTMs not working?,"Hi,

I'm trying to implement a nonlinear TimeSeries Prediction similar to this publication: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5336098/
Though I'm not interested in using a CNN as the first layer. I have a dataset of this shape:
, which I padded with 0s to have this shape: . This works just fine with normal LSTMs.

For Bidirectional LSTM I implemented my model like this:

Unfortunately I'm getting weird results with this model which look like the model is partly ignoring the masking.

The problem with BiDirectional LSTMs and masking seems to be that by reading that data backwards, masking is once in the front (where it should be) and once in the back (where it shouldn't). Therefore I'm getting weird results. I found this old [blog post](http://dirko.github.io/Bidirectional-LSTMs-with-Keras/) which handles a similar problem though it using the old API. 

Do I need to provide distinct datasets with the padding in the correct place for each layer of the Bi-LSTM? And how would I feed that data into the model? Or should I leave the padding out completely and create batches of same size? Or even use ? 

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

Cheers,

hfjn
",type:support,"[""I have the same problem. Did you find out what it was? I tried setting the batch size to 1 but the test set accuracy was pretty bad compared to training on the padded data (didn't try hyperparameter tuning, e.g. learning rate, so it may be my mistake).\r\n\r\nOne aspect I was wondering about: You said you would expect the masking to be in the front, not in the back. Since I don't know the internals about what Keras/TF does with masking exactly I can't be sure but it seemed a bit counterintuitive to me. Sequence data normally has its padding at the end, so I would expect the masking to be at the end as well, not in front. But still, is it correct to have masking at the end in one case and in front in another case?\r\n\r\nAnother issue I am unsure about with Bidirectional LSTMs with masking is whether the reverse function is working correctly. I looked into the code and it seems like the reverse function simply mirrors the data, which moves the padding from the back to the front as you wrote. However, I know that there is a TF function that does intelligent reversal which mirrors the data itself but keeps the padding at the end. However this function is not used by Keras, so I am wondering whether naive reverse with masking does the right thing."", '@Cerno-b Unfortunately, I never solved this problem. As I was working under strict time constraints (Did this for master thesis) I instead implemented a Deep LSTM for my problem. However, I agree that the reversal of padded strings in Keras is likely the origin of this problem and a way of ""intelligently"" reversing them would likely solve it.', 'is this issue solved? it sounds to be very serious as Bidirectional layer is commonly used.']","['\r\nmodel = Sequential()\r\nmodel.add(Masking(mask_value=0., input_shape=(None, X.shape[2])))\r\nmodel.add(Bidirectional(LSTM(1000, return_sequences=True)))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Bidirectional(LSTM(1000, return_sequences=True)))\r\nmodel.add(TimeDistributed(Dense(1000, activation=""linear"")))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(TimeDistributed(Dense(1, activation=""linear"")))\r\n\r\nmodel.compile(loss=""mse"", optimizer=""adam"", metrics=[""mae""])\r\n']","['(249, None, 55)', '(249, 543, 55)', 'batch_size=1']",0,0
309,keras,8365,closed,Issue with Generator: Trying to call index_generator from next function but getting the error :  Traceback (most recent call last): generator_ouput=next(output_generator) StopIteration.,"It would be great if someone can suggest me how I can fix this stop iteration problem. Based on the property of the generator whenever we call this function, once we’ve exhausted all of the yields within index_generator() running next() again results in a StopIteration error and in my case once reaches to the last batch of the directory iterator, it will stop working. This issue is not allowing me to compile my model.








Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
### 

",,"[""@coder1508 if you update the ticket with a snippet that reproduces the problem I'll be happy to have a look."", 'This is a bug of keras 2.0.9, when i change keras back to 2.0.8, using generator to generate data for training is OK.', ""Off the top of my head, there are two main changes on the generators on 2.0.9 that could affect you:\r\n\r\n1) The [Iterator class](https://github.com/fchollet/keras/blob/master/keras/preprocessing/image.py#L717) inherits from the `Sequence` instead of `object`. If you have custom generator that inherits from this class you might need to update it (hard to provide directions without more info). Some minor changes exist also on the `fit_generator()` method which uses different Enqueuers depending on the type of generator that you provide.\r\n2) The second is related to the [GeneratorEnqueuer class](https://github.com/fchollet/keras/blob/master/keras/utils/data_utils.py) which now supports finite generators and throw `StopIterator` exception if the undelying iterator is finite and is exhausted. Note that the original API on 2.0.8 required you to pass always an infinite generator (else the implementation would break), so the update is supposed to allow you to use also finite ones in case you are doing destributed predictions using Spark. The update should not have any effects on infinite generators.\r\n\r\nUnfortunately without a snippet that reproduces the problem it is very hard to tell which one is bitting you. If you provide a snippet, I'm happy to take a look."", 'Hi, @datumbox , I think I meet the same problem when I use keras 2.0.9.  I\'m trying a Keras implementation of FCN, here is the link [https://github.com/aurora95/Keras-FCN](url). In this repository, the author uses a DataGenerator class to load image and label. This part can be find in file *utils/SegDataGenator.py* . \r\nWhen I try to use this code, I get the following error:\r\n\r\n> lr: 0.010000\r\nEpoch 1/250\r\nTraceback (most recent call last):\r\n  File ""train.py"", line 246, in <module>\r\n    label_suffix=label_suffix, ignore_label=ignore_label, label_cval=label_cval)\r\n  File ""train.py"", line 163, in train\r\n    class_weight=class_weight\r\n  File ""/home/zxd/.local/lib/python2.7/site-packages/keras/legacy/interfaces.py"", line 87, in wrapper\r\n    return func(*args, **kwargs)\r\n  File ""/home/zxd/.local/lib/python2.7/site-packages/keras/engine/training.py"", line 2046, in fit_generator\r\n    generator_output = next(output_generator)\r\n  File ""/home/zxd/.local/lib/python2.7/site-packages/keras/utils/data_utils.py"", line 518, in get\r\n    raise StopIteration(e)\r\nStopIteration\r\n\r\nThe error message is not very detailed, but that all what I get. The error occurs when keras begins to use `model.fit_generator(...)` function in `trian.py`. \r\nBut it works well when I use keras version 2.0.5.\r\nThis may be an other reference: [https://stackoverflow.com/questions/46302911/what-raises-stopiteration-in-mine-keras-model-fit-generator](url). The quizzer seems to meet the same problem. I don\'t know what version of keras he uses ...\r\n\r\nHope that I have provided enough information for you ...', ""Hey @anguszxd, \r\n\r\nUnfortunately it is not feasible for me to read the entire code base of the other project and debug it. Please note that if this a problem related to another lib, it might be worth raising the issue on that project.\r\n\r\nIf you manage to reproduce the problem in a short, pure-keras snippet I'll be happy to debug it for you."", ""BTW with a very quick look on the lib it seems that your problem is caused by the 1st point that I suggested. The [SegDirectoryIterator](https://github.com/aurora95/Keras-FCN/blob/master/utils/SegDataGenerator.py#L82) inherits from `Iterator` which now extends `Sequence`. I believe the problem will be resolved if the SegDataGenerator class is updated to either use Keras' new generator logic or stop inheriting from Iterator."", 'Now by this [PR](https://github.com/aurora95/Keras-FCN/commit/c464c0f240c9750ddea33780ac021001497fbe81) solves this problem at  [aurora95/Keras-FCN](https://github.com/aurora95/Keras-FCN)', 'I use Keras 2.1.2 and I counter the same error ""StopIteration"", and after I change the value of generator\'s steps it work. I hope that can help.', 'I also encounter the same issue, @smallYoki  How to change steps_per_epoch?\r\n@coder1508 Did you fix this issue or find any workaround?', ""You can try set the value of 'steps_per_epoch' = numberofallsamples // mini_batchsize. Is better to make sure the number of all samples is integral multiple of batch size. Else, delete samples to make sure it."", 'I have the same error too(StopIteration in the next function of fit_generator). I have tried all the issues proposed, however i still get the error. I am using keras 2.1.4. any solutions ?\r\nThanks', '  File ""/usr/local/lib/python3.4/site-packages/keras/utils/data_utils.py"", line 564, in get\r\n    inputs = self.queue.get(block=True).get()\r\n  File ""/usr/local/lib/python3.4/multiprocessing/pool.py"", line 599, in get\r\n    raise self._value\r\n  File ""/usr/local/lib/python3.4/multiprocessing/pool.py"", line 119, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File ""/usr/local/lib/python3.4/site-packages/keras/utils/data_utils.py"", line 390, in get_index\r\n    return _SHARED_SEQUENCES[uid][i]\r\n  File ""/usr/local/lib/python3.4/site-packages/keras/preprocessing/image.py"", line 799, in __getitem__\r\n    return self._get_batches_of_transformed_samples(index_array)\r\n  File ""/usr/local/lib/python3.4/site-packages/keras/preprocessing/image.py"", line 845, in _get_batches_of_transformed_samples\r\n    raise NotImplementedError\r\nNotImplementedError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File ""cnn.py"", line 176, in <module>\r\n    main(sys.argv)\r\n  File ""cnn.py"", line 172, in main\r\n    _main()\r\n  File ""cnn.py"", line 161, in _main\r\n    trainModel(train_generator, val_generator, model, initial_epoch)\r\n  File ""cnn.py"", line 89, in trainModel\r\n    initial_epoch=initial_epoch)\r\n  File ""/usr/local/lib/python3.4/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper\r\n    return func(*args, **kwargs)\r\n  File ""/usr/local/lib/python3.4/site-packages/keras/engine/training.py"", line 2217, in fit_generator\r\n    generator_output = next(output_generator)\r\n  File ""/usr/local/lib/python3.4/site-packages/keras/utils/data_utils.py"", line 570, in get\r\n    six.raise_from(StopIteration(e), e)\r\n  File ""<string>"", line 3, in raise_from\r\nStopIteration\r\n']",[],[],0,0
310,keras,8407,closed,Error when using dropout and initial_state in gru layer,"my code is:

The error shown is: 

Removing dropout or initial_state argument seems to solve the problem but you can't use them both
Any help on this issue ?",,"[""get the same issue, when implements seq2seq,\r\n```\r\n        decoder_outputs, decoder_state_h, decoder_state_c = LSTM(self.lstm_unit, name='decoder', return_sequences=True,\r\n                                                                 return_state=True,\r\n                                                                 dropout=self.drop_out)(d, initial_state=encoder_states)\r\n```\r\n"", 'face the same issue, as far as I know this should be a bug.\r\nTensorflow version: 1.3\r\nKeras version: 2.1.2', 'As a simple workaround, go ahead to pip package source and make the change:\r\n \r\n## In file:       _xxx/site-packages/keras/layers/recurrent.py_\r\n\r\n## In method:\r\n\r\n-  `_generate_recurrent_dropout_mask`\r\n- `_generate_dropout_mask`\r\n\r\nInsert the code into the first line of the methods:\r\n```\r\nif type(inputs) is list:\r\n            inputs = inputs[0]\r\n```\r\nWhen you specify initial_state, the actual input becomes (`samples`, `state_1`, `state_2`....), but we only need the first element representing train data here.', 'Thank you very much @LitleCarl . The above worked for me.\r\n\r\nAs a note for others there is set of those methods for each recurrent model so make sure you augment all the ones you need or all of them.', 'I face the same problem when I use lstm...\r\n```\r\ndecoder_fw_0 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.2)\r\ndecoder_fw_0_outputs, _, _ = decoder_fw_0(decoder_inputs, initial_state=encoder_f_states_0)\r\n```\r\n\r\nError message is as below:\r\n```\r\n File ""seq2seq_ntm_bi_2.py"", line 154, in <module>\r\n    decoder_fw_0_outputs, _, _ = decoder_fw_0(decoder_inputs, initial_state=encoder_f_states_0)\r\n  File ""/Users/yudelin/.pyenv/versions/keras2/lib/python2.7/site-packages/keras/layers/recurrent.py"", line 518, in __call__\r\n    output = super(RNN, self).__call__(full_input, **kwargs)\r\n  File ""/Users/yudelin/.pyenv/versions/keras2/lib/python2.7/site-packages/keras/engine/topology.py"", line 603, in __call__\r\n    output = self.call(inputs, **kwargs)\r\n  File ""/Users/yudelin/.pyenv/versions/keras2/lib/python2.7/site-packages/keras/layers/recurrent.py"", line 2019, in call\r\n    self.cell._generate_recurrent_dropout_mask(inputs, training=training)\r\n  File ""/Users/yudelin/.pyenv/versions/keras2/lib/python2.7/site-packages/keras/layers/recurrent.py"", line 1781, in _generate_recurrent_dropout_mask\r\n    ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\r\n```', 'this happens to GRU with linux keras=2.1.2, tf-gpu=1.4.0\r\nwhile on a windows laptop all worked fine.\r\n\r\n', 'Hi @LitleCarl, how can i change the keras source code? \r\nI tried but when I ran my code again (after restarting my kernel) it seemed to me that my changes were inefficient.']",[],"['_costs_embd = TimeDistributed(\r\n        Dense(costs_embd_size, kernel_initializer=RandomUniform(-0.08, 0.08)))(costs)\r\n    _search_1 = GRU(h_search, return_sequences=True, kernel_initializer=RandomUniform(-0.08, 0.08),\r\n                    dropout=dropout)(_costs_embd, initial_state=[state_h])', 'Traceback (most recent call last):\r\n  File ""main.py"", line 21, in <module>\r\n    dropout=0.2)\r\n  File ""/home/hazem/PycharmProjects/optimizer-memory-network/optimzernet.py"", line 50, in build_agent\r\n    _costs_embd, initial_state=[state_h])\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/recurrent.py"", line 519, in __call__\r\n    output = super(RNN, self).__call__(full_input, **kwargs)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 603, in __call__\r\n    output = self.call(inputs, **kwargs)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/recurrent.py"", line 1503, in call\r\n    self.cell._generate_dropout_mask(inputs, training=training)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/recurrent.py"", line 1268, in _generate_dropout_mask\r\n    ones = K.ones_like(K.squeeze(inputs[:, 0:1, :], axis=1))\r\nTypeError: list indices must be integers, not tuple\r\n']",0,0
311,keras,7217,closed,Can fit_generator() declare an epoch by the whole data in a generator (finite number of data) instead of steps_per_epoch?," does a good job to pre-load data batches, avoid the problem of memory insufficiency, and fit for continuously available data. I was wondering, when passing a generator with finite number of outputs, is it possible to add an option to  so that it can behavior like the  function which declares all the data as an epoch instead of using ? Because I have no idea if the whole data are seen total epochs or something like 5.6 epochs when I use , and if I use train_on_batch, I need a thread to pre-load the batches because the preprocessing costs time.

Thanks.",stale,"['I think that yes!\r\nSee: https://github.com/fchollet/keras/issues/7462', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],"['fit_generator()', 'fit_generator()', 'fit()', 'steps_per_epoch', 'fit_generator()']",0,0
312,keras,11335,closed,Keras / LSTM model saving and loading produces inconsistent results,"I have seen several discussion on this topic in various forums. However, none of them seem to address the central issue I have. Here goes:

**Setup:**
- Working on a LSTM model. 
- It is a sentiment analysis application.
- I save / load the model per this FAQ (https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model)
- It works if I do it from the *same context*. That is same program / .py file: Train / Save / Delete model / Load / Predict works.

**Problem**: 
The following does not work.
- Program 1: Train the model. Save the model.
- Program 2: Load the model. Do prediction.

The predictions are completely off when compared to a single context run.
- There is a similar issue - that had some discussion and then it was marked as stale. https://github.com/keras-team/keras/issues/4904
- Another work around seems here is here: https://github.com/keras-team/keras/issues/7632

**Question**:
- Do we have to store internal states in a some other pickle format - for LSTM cases?
- Is there another recommended way of what I am trying to attempt?

**Output of **:




",To investigate,"['It seems to be related to `return_state` option and my particular use. I am using the sequential API. Perhaps because I am using `binary_crossentropy` as a loss function, the following message was triggered after I set `return_state = True` : `All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.`\r\n\r\nExact code - with return_state removed:\r\n```\r\nembedding_vecor_length = 64\r\nmodel = Sequential()\r\nmodel.add(Embedding(ms.vocab_size, embedding_vecor_length, input_length=ms.max_review_length))\r\nif ms.conv == \'y\':\r\n    model.add(Conv1D(filters=32, kernel_size=3, padding=\'same\', activation=\'relu\'))\r\n    model.add(MaxPooling1D(pool_size=2))\r\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\r\nmodel.add(Dense(1, activation=\'sigmoid\'))\r\nmodel.compile(loss=\'binary_crossentropy\', optimizer=\'adam\', metrics=[\'accuracy\'])\r\nprint(model.summary())\r\nmodel.fit(X_train, y_train, epochs=ms.num_epoch, batch_size=64)\r\n# Final evaluation of the model\r\nscores = model.evaluate(X_test, y_test, verbose=0)\r\nprint(""Accuracy: %.2f%%"" % (scores[1]*100))\r\n\r\n```\r\nWhen I debug above in PyCharm, there are no values stored in `mode.layers[1].states`. If it were, perhaps it would make its way into the hd5 file - or - I perhaps I could glean it / pickle it and do the reverse on prediction run.\r\n\r\nGoing to move my application to the functional API and try it next.']",['\r\nabsl-py==0.4.1\r\nastor==0.7.1\r\nbleach==1.5.0\r\ncycler==0.10.0\r\ngast==0.2.0\r\ngrpcio==1.14.2\r\nh5py==2.8.0\r\nhtml5lib==0.9999999\r\nKeras==2.2.2\r\nKeras-Applications==1.0.4\r\nKeras-Preprocessing==1.0.2\r\nMarkdown==2.6.11\r\nmatplotlib==2.1.1\r\nnumpy==1.14.5\r\npandas==0.22.0\r\nprotobuf==3.6.1\r\npyparsing==2.2.0\r\npython-dateutil==2.6.1\r\npytz==2017.3\r\nPyYAML==3.13\r\nscikit-learn==0.19.1\r\nscipy==1.1.0\r\nsix==1.11.0\r\nsklearn==0.0\r\ntensorboard==1.10.0\r\ntensorflow==1.10.1\r\ntensorflow-tensorboard==0.1.8\r\ntermcolor==1.1.0\r\nWerkzeug==0.14.1\r\n'],['pip freeze'],0,0
313,keras,9224,closed,pandas HDFStore conflicts with keras?!,"Hello, When i use **pandas HDFStore** with keras, Python program collapsed.



The code is OK if i comment the line .

-----------------------
System: Win10 x64
Pandas: 0.20.3
keras: 2.1.3
",,['Can you show the error message?'],"[""\r\nimport pandas as pd\r\nprint(pd.__version__)\r\ndf = pd.DataFrame({'a': [1, 2, 3]})\r\ndf.to_hdf('data.h5', 'table')\r\nprint(df)\r\n\r\nimport keras\r\nprint(keras.__version__)\r\n""]","[""df.to_hdf('data.h5', 'table')""]",0,0
314,keras,13658,closed,EarlyStopping with restore_best_weights=True makes TypeError: 'NoneType' object is not subscriptable,"EarlyStopping with restore_best_weights=True makes TypeError: 'NoneType' object is not subscriptable.

When restore_best_weights=False no exception.

**System information**  
 
- Keras version:  2.3.1
- Python version:  3.6

Stack trace:
",type:support,"['i have same problem', 'Same problem running on Google Colab', 'Found the ""problem"". \r\nIt makes sense it is None because it found no better model than the baseline in my case. \r\nI got rid of ""baseline=1.0"" and now it works.', '@iirekm Moving this issue to closed status as there has been no recent activity, in case you still face the error please create a new issue.Thanks!']","['\r\n  File ""/.../.venv/lib/python3.6/site-packages/keras/engine/training.py"", line 1239, in fit\r\n    validation_freq=validation_freq)\r\n  File ""/.../.venv/lib/python3.6/site-packages/keras/engine/training_arrays.py"", line 216, in fit_loop\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File ""/.../.venv/lib/python3.6/site-packages/keras/callbacks/callbacks.py"", line 152, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File ""/.../.venv/lib/python3.6/site-packages/keras/callbacks/callbacks.py"", line 834, in on_epoch_end\r\n    self.model.set_weights(self.best_weights)\r\n  File ""/.../.venv/lib/python3.6/site-packages/keras/engine/network.py"", line 516, in set_weights\r\n    own_weights = weights[:num_param]\r\nTypeError: \'NoneType\' object is not subscriptable\r\n\r\nProcess finished with exit code 1\r\n\r\n']",[],0,0
315,keras,9468,closed,Reducing Memory Usage of Char-Level Text Generation by LSTMs,"First off, these examples are wonderful.  That being said, I have a question/improvement to the [char-level text generation example](https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py).

I was trying this example out with a corpus of my own and kept running to MemoryError when allocating the training set and labels.  I noticed that both use a 1-hot encoding to indicate the activated character. 

**My question is**:  Why not bake this embedding into the model with an identity matrix before the first LSTM (basically just a character embedding layer that returns the 1-hot) and using a sparse categorical loss evaluation?  

This reduces the total in-memory usage by a factor of the number of unique characters.  Seemed to work for me when I tried it out.  

**Note**:  I was trying much longer input sequences (>100) which was why I was using a lot of memory.

Open to thoughts, issues, etc.",,"[""yeah，you're right!""]",[],[],0,0
316,keras,13344,closed,"ValueError: Error when checking target: expected dense_3 to have shape (15,) but got array with shape (1,)","import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from keras.layers import Dense,Flatten, Conv2D
from keras.layers import MaxPooling2D, Dropout
from keras.utils import np_utils, print_summary
import tensorflow as tf
from keras.models import Sequential
from keras.callbacks import ModelCheckpoint
import pickle
from keras.callbacks import TensorBoard

def keras_model(image_x, image_y):
    num_of_classes = 15
    model = Sequential()
    model.add(Conv2D(32, (5, 5), input_shape=(image_x,image_y,1), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))
    model.add(Conv2D(64, (5, 5), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))

    model.add(Flatten())
    model.add(Dense(512, activation='relu'))
    model.add(Dropout(0.6))
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.6))
    model.add(Dense(num_of_classes, activation='softmax'))

    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    filepath = ""QuickDraw.h5""
    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')
    callbacks_list = [checkpoint]

    return model, callbacks_list


def loadFromPickle():
    with open(""features"", ""rb"") as f:
        features = np.array(pickle.load(f))
    with open(""labels"", ""rb"") as f:
        labels = np.array(pickle.load(f))

    return features, labels


def augmentData(features, labels):
    features = np.append(features, features[:, :, ::-1], axis=0)
    labels = np.append(labels, -labels, axis=0)
    return features, labels


def prepress_labels(labels):
    labels = np_utils.to_categorical(labels)
    return labels


def main():
    features, labels = loadFromPickle()
    # features, labels = augmentData(features, labels)
    features, labels = shuffle(features, labels)
    labels=prepress_labels(labels)
    train_x, test_x, train_y, test_y = train_test_split(features, labels, random_state=0,
                                                        test_size=0.1)
    train_x = train_x.reshape(train_x.shape[0], 28, 28, 1)
    test_x = test_x.reshape(test_x.shape[0], 28, 28, 1)
    model, callbacks_list = keras_model(28,28)
    print_summary(model)
    model.fit(train_x, train_y, validation_data=(test_x, test_y), epochs=3, batch_size=64,
              callbacks=[TensorBoard(log_dir=""QuickDraw"")])
    model.save('QuickDraw.h5')


main()
",type:support,[],[],[],0,0
317,keras,11195,closed,Inception module Example in kears Doucumentation dont understand,"code bellow from keras Doucumentation Example:

from keras.layers import Conv2D, MaxPooling2D, Input

input_img = Input(shape=(256, 256, 3))

tower_1 = Conv2D(64, (1, 1), padding='same', activation='relu')(input_img)
tower_1 = Conv2D(64, (3, 3), padding='same', activation='relu')(tower_1)

tower_2 = Conv2D(64, (1, 1), padding='same', activation='relu')(input_img)
tower_2 = Conv2D(64, (5, 5), padding='same', activation='relu')(tower_2)

tower_3 = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(input_img)
tower_3 = Conv2D(64, (1, 1), padding='same', activation='relu')(tower_3)

output = keras.layers.concatenate([tower_1, tower_2, tower_3], axis=1)      #axis = 1, output shape is [?, 768, 256, 64]? I think axis should be -1, and output shape is [None, 256 ,256, 192].",To investigate,"['The output is of shape (?, 768, 256, 64)', '> The output is of shape (?, 768, 256, 64)\r\n\r\nOh! you are right, Why not add channels?', '> The output is of shape (?, 768, 256, 64)\r\n\r\nDepthConcat used in Inception paper.', ""I think it should be 'axis = -1'.""]",[],[],0,0
318,keras,11288,closed,Python sometimes does not exit when using multiprocessing=True in fit_generator,"Hi, 

I've implemented my own generator as sequence object. If I'm using it without multi processing, it works fine. Most of the times it also does the job when using multiprocessing=True (fit_generator).

However,  SOMETIMES (unfortunately not reproducible) when using multiprocessing=True, the program does not exit after the training finished. It reaches the last line (print out the time needed for training) but then does not exit. I've to kill it with ctrl + c. Most of the times, some processes are still running and tensorflow still occupies my GPU memory. I've to kill the processes then manually by kill -s 9 id .

I think it might be a problem with my generator:
https://gist.github.com/thorstenwagner/8033f43b99d1d3a1a6a31b054d91e7fc

However, I cannot nail it down to a specific line. I did my best to make it multi threading save.

I observed this problem with Keras 2.2.0, 2.2.2 and 2.2.3. I've tested tensorflow-gpu 1.8.0 and tensorflow-gpu 1.10.1 as backend. I had this problem with python 2 and with python 3.6.

Any ideas what might be problem?

Best,
Thorsten
",To investigate,"['We have a flaky multiprocessing test with a sequence. It might be related to what you are talking about.', 'Could you send a link to this unit test?', ""I'll see if I can find the unit test and also the failing build. "", ""https://travis-ci.org/keras-team/keras/jobs/424773817\r\n\r\nThis is one (of too many) flaky test that we have. After seeing it, it seems that the test isn't using multiprocessing. So it might be unrelated to your issue. \r\n\r\nDo you also have this issue with multithreading and multiple workers?"", 'I just use the fit_generator method and set number of workers to the number of cores and set multiprocessing to true. How can I use multi threading instead of multiple processes?', 'If you use a number of workers superior to 1 and use multiprocessing=False, it uses multithreading (from what I remember from the code).', ""I've now activated it and so far I could not observe hanging. However, I loose a lot of performance ( 0.580ms per step instead of 0.380 ms per step)."", ""Useful information to have. Multi-threading is generally slower for this kind of workflow (because of the GIL if you want to dig into that). Thanks for trying. Since it's not easily reproducible, and we dont know where the issue occurs, I don't think we can do anything anytime soon. I'll keep the issue open in case we stumble upon something similar. Thanks for the report and trying multi-threading."", '@gabrieldemarmiesse \r\nIf I ctrl+c the process I get following exception:\r\n```\r\nError in atexit._run_exitfuncs:\r\nTraceback (most recent call last):\r\n  File ""/home/twagner/Applications/miniconda3/envs/cryolo_python36/lib/python3.6/multiprocessing/util.py"", line 262, in _run_finalizers\r\n    finalizer()\r\n  File ""/home/twagner/Applications/miniconda3/envs/cryolo_python36/lib/python3.6/multiprocessing/util.py"", line 186, in __call__\r\n    res = self._callback(*self._args, **self._kwargs)\r\n  File ""/home/twagner/Applications/miniconda3/envs/cryolo_python36/lib/python3.6/multiprocessing/pool.py"", line 593, in _terminate_pool\r\n    task_handler.join()\r\n  File ""/home/twagner/Applications/miniconda3/envs/cryolo_python36/lib/python3.6/threading.py"", line 1056, in join\r\n    self._wait_for_tstate_lock()\r\n  File ""/home/twagner/Applications/miniconda3/envs/cryolo_python36/lib/python3.6/threading.py"", line 1072, in _wait_for_tstate_lock\r\n    elif lock.acquire(block, timeout):\r\nKeyboardInterrupt\r\n\r\n```\r\nMight the problem be connected to this issue?\r\nhttps://bugs.python.org/issue29309\r\n\r\nBut the issue is only about pressing twice CTRL+C... so I\'m not sure.\r\n\r\nAs keras is also using sleep here:\r\nhttps://github.com/keras-team/keras/blob/f2b261bc2555773bd88cbbeda976f98e244d02c1/keras/utils/data_utils.py#L553', 'We recently manage to get a traceback for our timeout in travis, maybe it\'s connected. See this build result:\r\n\r\n```\r\n________________________________ test_warnings _________________________________\r\n[gw1] linux -- Python 3.6.6 /home/travis/miniconda/envs/test-environment/bin/python\r\n    @pytest.mark.skipif(sys.version_info < (3,),\r\n                        reason=\'Cannot catch warnings in python 2\')\r\n    def test_warnings():\r\n        a = Input(shape=(3,), name=\'input_a\')\r\n        b = Input(shape=(3,), name=\'input_b\')\r\n    \r\n        a_2 = Dense(4, name=\'dense_1\')(a)\r\n        dp = Dropout(0.5, name=\'dropout\')\r\n        b_2 = dp(b)\r\n    \r\n        model = Model([a, b], [a_2, b_2])\r\n    \r\n        optimizer = \'rmsprop\'\r\n        loss = \'mse\'\r\n        loss_weights = [1., 0.5]\r\n        model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,\r\n                      sample_weight_mode=None)\r\n    \r\n        @threadsafe_generator\r\n        def gen_data(batch_sz):\r\n            while True:\r\n                yield ([np.random.random((batch_sz, 3)),\r\n                        np.random.random((batch_sz, 3))],\r\n                       [np.random.random((batch_sz, 4)),\r\n                        np.random.random((batch_sz, 3))])\r\n    \r\n        with pytest.warns(Warning) as w:\r\n            out = model.fit_generator(gen_data(4),\r\n                                      steps_per_epoch=10,\r\n                                      use_multiprocessing=True,\r\n                                      workers=2)\r\n        warning_raised = any([\'Sequence\' in str(w_.message) for w_ in w])\r\n        assert warning_raised, \'No warning raised when using generator with processes.\'\r\n    \r\n        with pytest.warns(None) as w:\r\n            out = model.fit_generator(RandomSequence(3),\r\n                                      steps_per_epoch=4,\r\n                                      use_multiprocessing=True,\r\n>                                     workers=2)\r\ntests/keras/engine/test_training.py:612: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\nkeras/legacy/interfaces.py:91: in wrapper\r\n    return func(*args, **kwargs)\r\nkeras/engine/training.py:1418: in fit_generator\r\n    initial_epoch=initial_epoch)\r\nkeras/engine/training_generator.py:179: in fit_generator\r\n    generator_output = next(output_generator)\r\nkeras/utils/data_utils.py:595: in get\r\n    inputs = self.queue.get(block=True).get()\r\n../../../miniconda/envs/test-environment/lib/python3.6/multiprocessing/pool.py:638: in get\r\n    self.wait(timeout)\r\n../../../miniconda/envs/test-environment/lib/python3.6/multiprocessing/pool.py:635: in wait\r\n    self._event.wait(timeout)\r\n../../../miniconda/envs/test-environment/lib/python3.6/threading.py:551: in wait\r\n    signaled = self._cond.wait(timeout)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\nself = <Condition(<unlocked _thread.lock object at 0x7fc34c3c7378>, 0)>\r\ntimeout = None\r\n    def wait(self, timeout=None):\r\n        """"""Wait until notified or until a timeout occurs.\r\n    \r\n            If the calling thread has not acquired the lock when this method is\r\n            called, a RuntimeError is raised.\r\n    \r\n            This method releases the underlying lock, and then blocks until it is\r\n            awakened by a notify() or notify_all() call for the same condition\r\n            variable in another thread, or until the optional timeout occurs. Once\r\n            awakened or timed out, it re-acquires the lock and returns.\r\n    \r\n            When the timeout argument is present and not None, it should be a\r\n            floating point number specifying a timeout for the operation in seconds\r\n            (or fractions thereof).\r\n    \r\n            When the underlying lock is an RLock, it is not released using its\r\n            release() method, since this may not actually unlock the lock when it\r\n            was acquired multiple times recursively. Instead, an internal interface\r\n            of the RLock class is used, which really unlocks it even when it has\r\n            been recursively acquired several times. Another internal interface is\r\n            then used to restore the recursion level when the lock is reacquired.\r\n    \r\n            """"""\r\n        if not self._is_owned():\r\n            raise RuntimeError(""cannot wait on un-acquired lock"")\r\n        waiter = _allocate_lock()\r\n        waiter.acquire()\r\n        self._waiters.append(waiter)\r\n        saved_state = self._release_save()\r\n        gotit = False\r\n        try:    # restore state no matter what (e.g., KeyboardInterrupt)\r\n            if timeout is None:\r\n>               waiter.acquire()\r\nE               Failed: Timeout >720.0s\r\n../../../miniconda/envs/test-environment/lib/python3.6/threading.py:295: Failed\r\n```\r\n\r\nWe obtained this build at this PR:  #11407\r\n\r\nIf the two issues are linked then we\'ll kill two birds with one stone.', 'I\'m getting this error intermittently when using multithreading for data generation:\r\n\r\n(function calls to own code omitted from backtrace)\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""/usr/local/miniconda3/envs/astro/lib/python3.6/runpy.py"", line 193, in _run_module_as_main\r\n    ""__main__"", mod_spec)\r\n  File ""/usr/local/miniconda3/envs/astro/lib/python3.6/runpy.py"", line 85, in _run_code\r\n    exec(code, run_globals)\r\n...\r\n  File ""/usr/local/miniconda3/envs/astro/lib/python3.6/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper\r\n    return func(*args, **kwargs)\r\n  File ""/usr/local/miniconda3/envs/astro/lib/python3.6/site-packages/keras/engine/training.py"", line 1418, in fit_generator\r\n    initial_epoch=initial_epoch)\r\n  File ""/usr/local/miniconda3/envs/astro/lib/python3.6/site-packages/keras/engine/training_generator.py"", line 262, in fit_generator\r\n    val_enqueuer.stop()\r\n  File ""/usr/local/miniconda3/envs/astro/lib/python3.6/site-packages/keras/utils/data_utils.py"", line 496, in stop\r\n    self.run_thread.join(timeout)\r\n  File ""/usr/local/miniconda3/envs/astro/lib/python3.6/threading.py"", line 1056, in join\r\n    self._wait_for_tstate_lock()\r\n  File ""/usr/local/miniconda3/envs/astro/lib/python3.6/threading.py"", line 1072, in _wait_for_tstate_lock\r\n    elif lock.acquire(block, timeout):\r\nKeyboardInterrupt\r\n```']",[],[],0,0
319,keras,12483,closed,AssertionError: Could not compute output Tensor when using multi_gpu_model(),"First of all, huge thanks for your effort.

I have 2 submodels (, ) out of which I form my full  using  by stacking them logically in ""series"". By this I mean that  accepts the output of plus an extra input tensor and the output of is the output of my full . The full is created **successfully** and I am also able to use .  

However, I want to parallelize the training of by running it on 2 GPUs, thus I use  which fails with the error:

> AssertionError: Could not compute output Tensor(""model_2/Dense_Decoder/truediv:0"", shape=(?, 33, 22), dtype=float32)

I have tried parallelizing the two submodels individually using  and , yet **both succeed**. The problem appears **only** with the full model. 

I am using **Tensorflow 1.12.0** and **Keras 2.2.4**. A snippet that demonstrates the problem (at least on my machine) is:



I believe this problem might be similar to #9599 but I can be mistaken. ",type:support,"['I found the solution to my problem, which I am not sure how to justify for.\r\n\r\nThe problem relies on `x.insert(0, decoder_inputs)` which I substituted with `x = [decoder_inputs] + x`. Both seem to result in the same list of tensors, however `multi_gpu_model` complains in the first case.']","['\r\nfrom keras.layers import Input, Dense,TimeDistributed, BatchNormalization\r\nfrom keras.layers import CuDNNLSTM as LSTM\r\nfrom keras.models import Model\r\nfrom keras.utils import multi_gpu_model\r\n\r\n\r\ndec_layers = 2\r\ncodelayer_dim = 11\r\nbn_momentum = 0.9\r\nlstm_dim = 128\r\ntd_dense_dim = 0\r\noutput_dims = 22\r\ndec_input_shape = [33, 44]\r\n\r\n\r\n# MODEL 1\r\nlatent_input = Input(shape=(codelayer_dim,), name=""Latent_Input"")\r\n\r\n# Initialize list of state tensors for the decoder\r\ndecoder_state_list = []\r\n\r\nfor dec_layer in range(dec_layers):\r\n    # The tensors for the initial states of the decoder\r\n    name = ""Dense_h_"" + str(dec_layer)\r\n    h_decoder = Dense(lstm_dim, activation=""relu"", name=name)(latent_input)\r\n\r\n    name = ""BN_h_"" + str(dec_layer)\r\n    decoder_state_list.append(BatchNormalization(momentum=bn_momentum, name=name)(h_decoder))\r\n\r\n    name = ""Dense_c_"" + str(dec_layer)\r\n    c_decoder = Dense(lstm_dim, activation=""relu"", name=name)(latent_input)\r\n\r\n    name = ""BN_c_"" + str(dec_layer)\r\n    decoder_state_list.append(BatchNormalization(momentum=bn_momentum, name=name)(c_decoder))\r\n\r\n# Define model_1\r\nmodel_1 = Model(latent_input, decoder_state_list)\r\n\r\n\r\n# MODEL 2\r\ninputs = []\r\n\r\ndecoder_inputs = Input(shape=dec_input_shape, name=""Decoder_Inputs"")\r\ninputs.append(decoder_inputs)\r\n\r\nxo = decoder_inputs\r\n\r\nfor dec_layer in range(dec_layers):\r\n    name = ""Decoder_State_h_"" + str(dec_layer)\r\n    state_h = Input(shape=[lstm_dim], name=name)\r\n    inputs.append(state_h)\r\n\r\n    name = ""Decoder_State_c_"" + str(dec_layer)\r\n    state_c = Input(shape=[lstm_dim], name=name)\r\n    inputs.append(state_c)\r\n\r\n    # RNN layer\r\n    decoder_lstm = LSTM(lstm_dim,\r\n                   return_sequences=True,\r\n                   name=""Decoder_LSTM_"" + str(dec_layer))\r\n\r\n    xo = decoder_lstm(xo, initial_state=[state_h, state_c])\r\n    xo = BatchNormalization(momentum=bn_momentum, name=""BN_Decoder_"" + str(dec_layer))(xo)\r\n    if td_dense_dim > 0: # Squeeze LSTM interconnections using Dense layers\r\n        xo = TimeDistributed(Dense(td_dense_dim), name=""Time_Distributed_"" + str(dec_layer))(xo)\r\n\r\n# Final Dense layer to return probabilities\r\noutputs = Dense(output_dims, activation=\'softmax\', name=""Dense_Decoder"")(xo)\r\n\r\n# Define model_2\r\nmodel_2 = Model(inputs=inputs, outputs=[outputs])\r\n\r\n\r\n# FULL MODEL\r\nlatent_input = Input(shape=(codelayer_dim,), name=""Latent_Input"")\r\ndecoder_inputs = Input(shape=dec_input_shape, name=""Decoder_Inputs"")\r\n\r\n# Stack the two models\r\n# Propagate tensors through 1st model\r\nx = model_1(latent_input)\r\n# Insert decoder_inputs as the first input of the 2nd model\r\nx.insert(0, decoder_inputs)\r\n# Propagate tensors through 2nd model\r\nx = model_2(x)\r\n\r\n# Define full model\r\nmodel = Model(inputs=[latent_input, decoder_inputs], outputs=[x])\r\n\r\n# Parallelize the model\r\nparallel_model = multi_gpu_model(model, gpus=2)\r\nparallel_model.summary()\r\n']","['model_1', 'model_2', 'model', 'keras.models.Model()', 'model_2 ', 'model_1 ', 'model_2 ', 'model', 'model ', 'compile/train/predict', 'model ', 'multi_gpu_model()', 'multi_gpu_model(model_1, gpus=2)', 'multi_gpu_model(model_1, gpus=2)']",0,0
320,keras,13115,closed,Feature Request: built-in ResizeBilinear Layer,"I'm requesting a built-in Keras layer for resizing tensors.

Today:
I have to implement this layer either as a Lambda or a custom layer, both of which simply wrap . This causes huge headaches for exporting to CoreML and TF-Lite. Existing layers like  and  do not meet my needs, since I am downsampling to a fixed output size, so scaling by some factor does not work.

CoreML has a built-in  layer, but it is difficult to connect my custom Keras layer with that CoreML built-in.

Ideal solution:
There is a built-in Keras layer, I ask the CoreML team to use it, they implement a new layer conversion mapping, and I convert my model without digging into the guts of both Keras and CoreML.

",stat:awaiting tensorflower type:feature,['[`tf.keras.layers.experimental.preprocessing.Resizing`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Resizing) sounds like what you need. Looks like it was added in late 2019.'],[],"['tensorflow.compat.v1.image.resize_bilinear', 'Upsampling2d', 'Reshape', 'resizeBilinear']",0,0
321,keras,12596,closed," Error when checking input: expected conv2d_21_input to have 4 dimensions, but got array with shape (1, 1, 224, 224, 3)","    
    from keras.models import load_model
    #model = load_model('atrs')
     (notSanta, santa) = model.predict(r)[0]

i got shape of ry as (1, 1, 224, 224, 3) inspite of adding one dimension it gave me two dimensions in case you wanted to know shape of r it is (400, 650, 3)


",type:support,[],[],"['import cv2\r\n    from keras.preprocessing import image\r\n    r = cv2.imread(\'C:\\\\...\\\\modi.jpg\',1)\r\n    orig = r.copy()\r\n    r = cv2.resize(r, (224, 224))\r\n    r = r.astype(""float"") / 255.0\r\n    r = image.img_to_array(r)\r\n    ry= np.expand_dims(k, axis=1)']",0,0
322,keras,13375,closed,"Video classification using stateful LSTM , validation accuracy doesn't improve","I am trying to classify some video into 3 different classes. Each video has different length of frame. The training data has the shape of (104, None, 528) where: 

 - 104 = Number of videos
 - None = number of frames for each video which are different
 - 528 = Number of features for each frame

As the sequence of frames for each video is long I am using ""stateful LSTM"" to manage the length of sequences. I have defined my model same as below:

    def LSTM_Model():
    
    model = Sequential()
    model.add(LSTM(units = 256, input_shape=(None, 528),\
                           return_sequences=False, stateful=True, batch_size = 1))
    model.add(Dropout(0.4))
    model.add(Dense(3, activation='softmax'))
    opt = keras.optimizers.SGD(lr=0.00005, decay = 1e-6, momentum=0.9, nesterov=True)
    model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])
    model.summary()
    
    return model

 Then I trained and tested the model:

   

    def train_model(X, y, X_test, y_test, model):
        np.random.seed(200)
        epochs = 100
        maxlen = 500
    
        
        for epoch in range(epochs):
            
            mean_tr_loss, mean_tr_acc =[],[]
            print('Epoch: ', epoch + 1 )
            
            for sbj in range(X.shape[0]):
                
                video = X[sbj]
                y_sbj = y[sbj,:]
                y_new = y_sbj
                nb_frame = video.shape[0]
    
                for count in range(nb_frame // maxlen +1):
                    
                    if count == nb_frame // maxlen :
                        seq = video[count*maxlen + count:, :]
    
                    else:
                        seq = video[count*maxlen+count : (count+1)*maxlen+count, :]
                        seq = np.expand_dims(seq, axis=0)
                        
          #   ''' Using train_on_batch '''
                        
                        tr_loss, tr_acc = model.train_on_batch(seq, np.array([y_new])) 
                        mean_tr_loss.append(tr_loss)
                        mean_tr_acc.append(tr_acc)
   
    
                print('Training on subject', sbj+1, 'done' )
                model.reset_states() 
    

            print('accuracy training = {}'.format(np.mean(mean_tr_acc)))
            print('loss training = {}'.format(np.mean(mean_tr_loss)))
            print('___________________________________')
            

        
            print('Testing....')
            

            mean_te_loss, mean_te_acc =[],[]
        
            for sbj_test in range(X_test.shape[0]):
                
                video_test = X_test[sbj_test]
                y_new_test = y_test[sbj_test]
                nb_frame_test = video_test.shape[0]
                
                for i in range(nb_frame_test // maxlen + 1):
                    
                    if i == nb_frame_test // maxlen :
                        seq_test = video_test[i*maxlen + i:, :]
                    else:
                        seq_test = video_test[i*maxlen+i : (i+1)*maxlen+i, :]
                        seq_test = np.expand_dims(seq_test, axis=0)
                        te_loss, te_acc = model.test_on_batch(seq_test, np.array([y_new_test])) 
                        mean_te_loss.append(te_loss)
                        mean_te_acc.append(te_acc)                
                print('Testing on subject', sbj_test+1, 'done' )
                model.reset_states()   
                 
    

            print('accuracy testing = {}'.format(np.mean(mean_te_acc)))
            print('loss testing = {}'.format(np.mean(mean_te_loss)))

 
        

In the above code I considered each video separately and then each video was divided to different frame sequences with length 500 (except last sequence frame for each video because the length of frames are not divisible by 500). The training accuracy and test accuracy are same as below.

    Epoch1 : accuracy training = 0.3694     accuracy testing = 0.3927
             loss training = 1.146          loss testing = 1.109
    Epoch2 : accuracy training = 0.4423     accuracy testing = 0.4048
             loss training = 1.053          loss testing = 1.109
    Epoch3 : accuracy training = 0.5017     accuracy testing = 0.4236
             loss training = 0.994          loss testing = 1.115
    Epoch4 : accuracy training = 0.5491     accuracy testing = 0.4099
             loss training = 0.94           loss testing = 1.124
    Epoch5: accuracy training = 0.5612      accuracy testing = 0.4013
            loss training = 0.924           loss testing = 1.128
    Epoch6 : accuracy training = 0.6142     accuracy testing = 0.4113
             loss training = 0.859          loss testing = 1.137
    Epoch7 : accuracy training = 0.6263     accuracy testing = 0.4116
             loss training = 0.824          loss testing = 1.142
    Epoch8 : accuracy training = 0.6659     accuracy testing = 0.415
             loss training = 0.775          loss testing = 1.152

 

After 100 epochs training accuracy increases while testing accuracy doesn't improve. If the case is ""overfitting"" adding dropout layer should help which didn't. So, I am confused about the cause. 

Any idea or suggestion would be appreciated.  
  
",type:support,"['hello，recently i am use the lstm to do something ,\r\n\r\nwould you like to communication with me about the lstm work?']",[],[],0,0
323,keras,10903,closed,"ValueError: Input 0 is incompatible with layer conv2d_1: expeced ndim=4, found ndim=5","Hi,
I am trying to pass a RGB image from a simulator into my custom neural network. At the source of the RGB generation (simulator), the dimension of RGB image is (3,144,256). 

This is how i construct the neural network:




Now, my rbg_shape is (1, 3, 144, 256).

This is the error that i get:
    _rgb_model.add(Conv2D(96, (11, 11), strides=(3, 3), padding='valid', activation='relu', input_shape=rgb_kshape, data_format = ""channels_first""))
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/sequential.py"", line 166, in add
    layer(x)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/base_layer.py"", line 414, in __call__
    self.assert_input_compatibility(inputs)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/base_layer.py"", line 311, in assert_input_compatibility
    str(K.ndim(x)))
ValueError: Input 0 is incompatible with layer conv2d_1: expected ndim=4, found ndim=5_

Why is keras complaining that the expected dimension is 5 when my actual dimension is 4?

Thanks",,"['@reinforcelearn Can you check the input_shape in ```rgb_model.add(Conv2D(96, (11, 11), strides=(3, 3), padding=\'valid\', activation=\'relu\', input_shape=rgb_kshape, data_format = ""channels_first""))```? Keras layers\' ```input_shape``` doesn\'t include batch dimension, so ```len(input_shape)``` should be 3 for ```Conv2D```, it seams your ```input_shape``` is 4.', 'I was receiving the same error with my data but I solved it. I used and will suggest, layer by layer debugging. I was miscalculating some dimensions and hence the kernel sizes that I was using were leading up to this error. You might need to modify the kernel sizes and strides. Here is how I went about it.\r\nI had N samples and each sample had the shape (7, 7, 2) where 2 is the number of channels. In your case, it would be (2, 7, 7). I commented out the rest of the model except the first layer, and printed the model summary which you can see as `print(rgb_model.summary())`. When you do this and repeat similar debugging by uncommenting one layer each time, you will be able to see the output size in the summary. It will help you with comparing your theoretical architecture with the actual code. \r\nI also realized that I was using the wrong arguments in `MaxPool2D()`. I was confusing between the `pool_size` and the `strides`.']","['\r\nrgb_model = Sequential()\r\nrgb = env.shape() // this is (3, 144, 256)\r\nrgb_shape = (1,) + rgb\r\nrgb_model.add(Conv2D(96, (11, 11), strides=(3, 3), padding=\'valid\', activation=\'relu\', input_shape=rgb_shape, data_format = ""channels_first""))\r\n']",[],0,0
324,keras,9845,closed,keras trained model using different tf device,"Hello,

I am using a trained model form keras. this model doesn't fit in to the GPU memory so I want to divide it into CPU and GPU and maybe GPU on different machines. 
SO if I want to get all tf.variable on my CPU and the calculation on GPU how can I do this because there is some docs that show we can place the tf.variable on CPU and the calculation on a GPU. (I am taking about a pre-trained keras model).

If I want to separate the layers of the pre-trained model  into different GPU on different machines. is it correct to use with  and then add the layers to my sequence model like this:



Thank you very much.
",,[],"['\r\nvgg16_model = keras.applications.vgg16.VGG16()\r\n\r\ncnnModel = Sequential()\r\n\r\n\r\nfor layer in vgg16_model.layers[0:13]:\r\n    cnnModel.add(layer)\r\nfor layer in vgg16_model.layers[14:16]:\r\n    cnnModel.add(layer)\r\nfor layer in vgg16_model.layers[17:21]:\r\n    cnnModel.add(layer)\r\n\r\nwith tf.device(........):\r\n   cnnModel.add(Dense(2048, name=""compress_1""))\r\n   cnnModel.add(Dense(1024, name=""compress_2""))\r\n   cnnModel.add(Dense(512, name=""compress_3""))\r\n\r\nwith tf.device(........):\r\n    model = Sequential()\r\n    model.add(keras.layers.TimeDistributed(cnnModel,input_shape=(10,224,224,3),name=""CNN_Model""))\r\n\r\nwith tf.device(........):\r\n    model.add(keras.layers.LSTM(256,name=""lstm1"",return_sequences=True))\r\n    model.add(keras.layers.LSTM(128,name=""lstm2"",return_sequences=True))\r\n    model.add(keras.layers.Flatten())\r\n    model.add(keras.layers.Dense(528))\r\n    model.add(keras.layers.Dense(64))\r\n    model.add(keras.layers.Dense(39,activation=\'sigmoid\'))\r\n']",['tf.device'],0,0
325,keras,10577,closed,Failing to load h5 model using tf-gpu as backend for keras?,"Traceback (most recent call last):
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1322, in _do_call
    return fn(*args)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1305, in _run_fn
    self._extend_graph()
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1340, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was re
gistered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU],
Registered kernels:
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]

         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_FLOAT, direction=""un
idirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode=
""gru"", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Expa
ndDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""Z:\trader_connect.py"", line 157, in <module>
    tick()
  File ""Z:\trader_connect.py"", line 74, in tick
    model1 = keras.models.load_model('Z:\\Productionmodel.h5')
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 264, in load_model
    load_weights_from_hdf5_group(f['model_weights'], model.layers)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 929, in load_weights_from_hdf5_group
    K.batch_set_value(weight_value_tuples)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\ba
ckend\tensorflow_backend.py"", line 2435, in batch_set_value
    get_session().run(assign_ops, feed_dict=feed_dict)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\ba
ckend\tensorflow_backend.py"", line 196, in get_session
    [tf.is_variable_initialized(v) for v in candidate_vars])
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 900, in run
    run_metadata_ptr)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1316, in _do_run
    run_metadata)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was re
gistered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU],
Registered kernels:
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]

         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_FLOAT, direction=""un
idirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode=
""gru"", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Expa
ndDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]

Caused by op 'bidirectional_1/CudnnRNN_1', defined at:
  File ""Z:\trader_connect.py"", line 157, in <module>
    tick()
  File ""Z:\trader_connect.py"", line 74, in tick
    model1 = keras.models.load_model('Z:\\Productionmodel.h5')
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 261, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 335, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\__init__.py"", line 55, in deserialize
    printable_module_name='layer')
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\ut
ils\generic_utils.py"", line 145, in deserialize_keras_object
    list(custom_objects.items())))
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\sequential.py"", line 293, in from_config
    model.add(layer)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\sequential.py"", line 166, in add
    layer(x)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\wrappers.py"", line 426, in __call__
    return super(Bidirectional, self).__call__(inputs, **kwargs)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\base_layer.py"", line 460, in __call__
    output = self.call(inputs, **kwargs)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\wrappers.py"", line 505, in call
    y_rev = self.backward_layer.call(inputs, **kwargs)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\cudnn_recurrent.py"", line 90, in call
    output, states = self._process_batch(inputs, initial_state)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\cudnn_recurrent.py"", line 297, in _process_batch
    is_training=True)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\contrib\cudnn_rnn\python\ops\cudnn_rnn_ops.py"", line 1623, in __call__
    seed=self._seed)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\contrib\cudnn_rnn\python\ops\cudnn_rnn_ops.py"", line 1012, in _cudnn_rnn_no_i
nput_c
    direction, dropout, seed, name)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\contrib\cudnn_rnn\python\ops\cudnn_rnn_ops.py"", line 926, in _cudnn_rnn
    name=name)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\ops\gen_cudnn_rnn_ops.py"", line 143, in cudnn_rnn
    is_training=is_training, name=name)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\framework\ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\framework\ops.py"", line 1718, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-
access

InvalidArgumentError (see above for traceback): No OpKernel was registered to su
pport Op 'CudnnRNN' with these attrs.  Registered devices: [CPU], Registered ker
nels:
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]

         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_FLOAT, direction=""un
idirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode=
""gru"", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Expa
ndDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]


(tensorflow-gpu) C:\Users\xion>python Z:\trader_connect.py --csv Y:\EURUSD,5.c
sv
Using TensorFlow backend.
2018-07-01 20:58:02.203507: I T:\src\github\tensorflow\tensorflow\core\common_ru
ntime\gpu\gpu_device.cc:1356] Found device 0 with properties:
name: GeForce GT 530 major: 2 minor: 1 memoryClockRate(GHz): 1.399
pciBusID: 0000:01:00.0
totalMemory: 2.00GiB freeMemory: 1.87GiB
2018-07-01 20:58:02.204507: I T:\src\github\tensorflow\tensorflow\core\common_ru
ntime\gpu\gpu_device.cc:1406] Ignoring visible gpu device (device: 0, name: GeFo
rce GT 530, pci bus id: 0000:01:00.0, compute capability: 2.1) with Cuda compute
 capability 2.1. The minimum required Cuda capability is 3.0.
2018-07-01 20:58:02.204507: I T:\src\github\tensorflow\tensorflow\core\common_ru
ntime\gpu\gpu_device.cc:923] Device interconnect StreamExecutor with strength 1
edge matrix:
2018-07-01 20:58:02.204507: I T:\src\github\tensorflow\tensorflow\core\common_ru
ntime\gpu\gpu_device.cc:929]      0
2018-07-01 20:58:02.204507: I T:\src\github\tensorflow\tensorflow\core\common_ru
ntime\gpu\gpu_device.cc:942] 0:   N
Traceback (most recent call last):
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1322, in _do_call
    return fn(*args)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1305, in _run_fn
    self._extend_graph()
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1340, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was re
gistered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU],
Registered kernels:
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]

         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_DOUBLE, direction=""u
nidirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode
=""gru"", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Exp
andDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""Z:\trader_connect.py"", line 157, in <module>
    tick()
  File ""Z:\trader_connect.py"", line 74, in tick
    model1 = keras.models.load_model('Z:\\Productionmodel.h5')
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 264, in load_model
    load_weights_from_hdf5_group(f['model_weights'], model.layers)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 929, in load_weights_from_hdf5_group
    K.batch_set_value(weight_value_tuples)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\ba
ckend\tensorflow_backend.py"", line 2435, in batch_set_value
    get_session().run(assign_ops, feed_dict=feed_dict)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\ba
ckend\tensorflow_backend.py"", line 196, in get_session
    [tf.is_variable_initialized(v) for v in candidate_vars])
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 900, in run
    run_metadata_ptr)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1316, in _do_run
    run_metadata)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\client\session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was re
gistered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU],
Registered kernels:
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]

         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_DOUBLE, direction=""u
nidirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode
=""gru"", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Exp
andDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]

Caused by op 'bidirectional_1/CudnnRNN_1', defined at:
  File ""Z:\trader_connect.py"", line 157, in <module>
    tick()
  File ""Z:\trader_connect.py"", line 74, in tick
    model1 = keras.models.load_model('Z:\\Productionmodel.h5')
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 261, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\saving.py"", line 335, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\__init__.py"", line 55, in deserialize
    printable_module_name='layer')
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\ut
ils\generic_utils.py"", line 145, in deserialize_keras_object
    list(custom_objects.items())))
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\sequential.py"", line 293, in from_config
    model.add(layer)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\sequential.py"", line 166, in add
    layer(x)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\wrappers.py"", line 426, in __call__
    return super(Bidirectional, self).__call__(inputs, **kwargs)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\en
gine\base_layer.py"", line 460, in __call__
    output = self.call(inputs, **kwargs)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\wrappers.py"", line 505, in call
    y_rev = self.backward_layer.call(inputs, **kwargs)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\cudnn_recurrent.py"", line 90, in call
    output, states = self._process_batch(inputs, initial_state)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\keras\la
yers\cudnn_recurrent.py"", line 297, in _process_batch
    is_training=True)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\contrib\cudnn_rnn\python\ops\cudnn_rnn_ops.py"", line 1623, in __call__
    seed=self._seed)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\contrib\cudnn_rnn\python\ops\cudnn_rnn_ops.py"", line 1012, in _cudnn_rnn_no_i
nput_c
    direction, dropout, seed, name)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\contrib\cudnn_rnn\python\ops\cudnn_rnn_ops.py"", line 926, in _cudnn_rnn
    name=name)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\ops\gen_cudnn_rnn_ops.py"", line 143, in cudnn_rnn
    is_training=is_training, name=name)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\framework\ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""C:\Users\xion\Anaconda2\envs\tensorflow-gpu\lib\site-packages\tensorfl
ow\python\framework\ops.py"", line 1718, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-
access

InvalidArgumentError (see above for traceback): No OpKernel was registered to su
pport Op 'CudnnRNN' with these attrs.  Registered devices: [CPU], Registered ker
nels:
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_DOUBLE]

         [[Node: bidirectional_1/CudnnRNN_1 = CudnnRNN[T=DT_DOUBLE, direction=""u
nidirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode
=""gru"", seed=87654321, seed2=0](bidirectional_1/transpose_2, bidirectional_1/Exp
andDims_3, bidirectional_1/Const_1, bidirectional_1/concat_1)]]








I've tried--fresh reinstall, change float

Any fixes? No idea why Im getting this. P.S: I trained the model on a titan v and I am trying to now open it on a computer with a GeForce GT 530 Gpu.",,"['I came across the same error now as well (keras==2.2.4, tensorflow-gpu==1.10.0, libcudnn7==7.4.2.24-1+cuda10.0) when loading a model trained with CuDNNGRU later without GPU available. Actually, last year or so I was writing support to load weights from CuDNNGRU to plain CuDNN-compatible GRU. It seems it is for some reason not applied in this case (`load_model()` instead of `load_weights()`).']",[],[],0,0
326,keras,12504,closed,Any way for Keras officially supporting Colab TPUs?,"Various blog posts, stackoverflow answers and tweets say like it's very easy to convert a Keras model to Tensorflow Keras model which can be used in Tensorflow TPU wrapper to get the model support TPU. But that's not very easy..

What about custom layers? How can I convert custom layers to get trained on TPUs? Or what about when I used Keras backend functions? Surely things are messing up and I am getting nothing but errors.

Do you have any plans on releasing Keras with TPU support? If yes, then can you give an estimated date till which users should wait?",,"['yeah is there any way to use a keras model and tpu opposed to using tensorflow?', 'Shift to tf.keras as Keras will no longer be maintained after the next release.']",[],[],0,0
327,keras,12370,closed,Feature request: Multi-class average recall metric,"Hi! Glad to see that Keras supported stateful metrics. I have been using multi-class average recall in most of my research projects in the past. I used to calculate them by callbacks, now I can finally add a metric.

I have posted my implementation at this [gist](https://gist.github.com/HawkinsZhao/766305acfb0141b70370e5dcd9415eb6). Also, I have found that Keras is going to update the basic metrics APIs #12149. How do you think about my work? And should I submit it as a PR at this time?

The multi-class average recall is a widely used metric in affective computing and many research problems. Besides, there is a lot of discussion about it #9393. I think it could certainly benefit lots of people.",,[],[],[],0,0
328,keras,10074,closed,'You must feed a value for placeholder tensor' when using Tensorboard callback with submodels,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [X] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

Trying to train a model that is composed of submodels, and using the Tesnorboard callback.
If histogram_freq is set, there's an error about missing input to the submodel

using tensorflow 1.8 (same error also in older versions)

sample code that reproduces the error:



error:



The reason of using submodels is the need to also run them separately ",,"['Your model has 2 inputs and you feed only one.', ""@Dref360 it's not 2 inputs - one input to the model, and inputs to the sub models.\r\n\r\nalso, calling fit with histogram_freq=0, and then calling predict works well\r\n\r\n```\r\nm.fit(x, y, callbacks=[keras.callbacks.TensorBoard(log_dir='/tmp', histogram_freq=0)], epochs=10, validation_data=(x, y))\r\n\r\nprint(m.predict(x))\r\n\r\n```"", 'I have a very similar issue:\r\n```\r\ni1 = Input()\r\nm1 = Model(inputs=i1, ...)\r\no = m1(i1)\r\ni2 = Input()\r\ni3 = Input()\r\nm2 = Model(inputs=[i2,i3],...)\r\no2 = m2([i2, o])\r\n\r\nfinal = Model(inputs=[i1, i2], outputs=o2)\r\n```\r\nThrows an error that `i3` placeholder is not specified even though that input is set by `o` from `o=m1(i1)` when calling `final.predict([x1,x2])`.\r\n', 'same here', 'I had the same problem, that only appeared when using TensorBoard callback and `histogram_freq` other than 0. Clearing tensorflow session right before creating the model fixed it:\r\n```\r\nimport keras.backend as K\r\nK.clear_session()\r\n```\r\n\r\nFrom this [issue](https://github.com/keras-team/keras/issues/4417#issuecomment-384502287)', 'I am having the same problem here. Calling `K.clear_session()` is not helping on my case. Any workaround?', '> I am having the same problem here. Calling `K.clear_session()` is not helping on my case. Any workaround?\r\n\r\nDid you try to call K.clear_session() before you build the model?', 'I have the same issue. I call K.clear_session() right before creating the model. :O Any news? \r\n', 'Same issue here. Calling `K.clear_session()` before creating the model is not helping on my case. Any news?', 'I have the same issue. the clear_session call does not work.', ""Just FYI, for `tf.data.Dataset` users, call `tf.keras.backend.clear_session()` before creating Dataset will do the trick.\r\n\r\nI can't imagine I encountered the same issue after 3 years later https://github.com/keras-team/keras/issues/4417."", 'I get ```AssertionError: Do not use tf.reset_default_graph()\r\nto clear nested graphs. If you need a cleared graph, exit the nesting and create a new graph.``` when using clear session', 'Same problem here. Clearing the session as proposed by @pdpino fixed for me.', 'Same! Clearing before AND before creating tf.dataset with tf.keras not working and same error.', 'I have the same issue. Below is my solution.\r\nWhy do we get this error when there are submodel in  our model? \r\nfor exmaple, below is a model which has a submodel.\r\n```\r\ndef shared_model(): \r\n    input = keras.Input(shape=(self.input_dim), dtype=tf.float32, name=""shared_input"")`\r\n    # model layers\r\n    ......\r\n    ......\r\n    share_out = .....\r\n    shared_model = keras.Model([input], share_out, name=""out_model"")\r\n    return shared_model\r\n\r\ninput1 = keras.Input(shape=(self.input_dim), dtype=tf.float32, name=""input1"")\r\ninput2= keras.Input(shape=(self.input_dim), dtype=tf.float32, name=""input2"")\r\nsub_model = shared_model()\r\nout1 =  sub_model([input1])\r\nout2 = sub_model([input2])\r\nout = keras.layers.concatenate([out1, out2], axis=1)\r\nout = .....\r\nmodel = Model(.....)\r\nmodel.compile(.....)\r\ntensorboard =  keras.callbacks.TensorBoard(histogram_freq=1,........)\r\nmodel.fit(...., callbacks=[ tensorboard])```\r\n```\r\nWhen training the model with the above code, get error:\r\n    \'You must feed a value for placeholder tensor  shared_input\'.\r\nwhy shared_input ? Let\'s see keras.callbacks.TensorBoard source code:\r\n```\r\n if self.histogram_freq and self.merged is None:\r\n      for layer in self.model.layers:\r\n        for weight in layer.weights:\r\n          mapped_weight_name = weight.name.replace(\':\', \'_\')\r\n          tf_summary.histogram(mapped_weight_name, weight)\r\n          if self.write_images:\r\n            w_img = array_ops.squeeze(weight)\r\n            shape = K.int_shape(w_img)\r\n            if len(shape) == 2:  # dense layer kernel case\r\n              if shape[0] > shape[1]:\r\n                w_img = array_ops.transpose(w_img)\r\n                shape = K.int_shape(w_img)\r\n              w_img = array_ops.reshape(w_img, [1, shape[0], shape[1], 1])\r\n            elif len(shape) == 3:  # convnet case\r\n              if K.image_data_format() == \'channels_last\':\r\n                # switch to channels_first to display\r\n                # every kernel as a separate image\r\n                w_img = array_ops.transpose(w_img, perm=[2, 0, 1])\r\n                shape = K.int_shape(w_img)\r\n              w_img = array_ops.reshape(w_img,\r\n                                        [shape[0], shape[1], shape[2], 1])\r\n            elif len(shape) == 1:  # bias case\r\n              w_img = array_ops.reshape(w_img, [1, shape[0], 1, 1])\r\n            else:\r\n              # not possible to handle 3D convnets etc.\r\n              continue\r\n\r\n            shape = K.int_shape(w_img)\r\n            assert len(shape) == 4 and shape[-1] in [1, 3, 4]\r\n            tf_summary.image(mapped_weight_name, w_img)\r\n```\r\nwhen histogram_freq=1,   layer \'out_model\' in self.model.layers  is a submodel which has its  own input  \'shared_input\'.  tf_summary.histogram(mapped_weight_name, weight) add this op to tf.Session() to run.\r\nbut submodel is a independent model , not in the tensorflow default graph. So we need feed value   for shared_input.  When modifying the code:\r\n```\r\n  for layer in self.model.layers:\r\n        if layer.name == \'out_model\':\r\n              continue\r\n        for weight in layer.weights:\r\n          mapped_weight_name = weight.name.replace(\':\', \'_\')\r\n          tf_summary.histogram(mapped_weight_name, weight)\r\n```\r\nWith above code, we don\'t add submodel to session.run. It works fine. But it does\'t add histograms for weights and bias.\r\nHow  can we add histograms for these ?  It‘s clear now. It works if we get weights, bias and so on from tensorflow default graph， not keras\'s layers. In tensorflow, tf.trainable_variables() get all trainable variables besides weights and bias. When modifying the code:\r\n ```\r\n        if self.histogram_freq and self.merged is None:\r\n            for weight in tf.trainable_variables():\r\n                mapped_weight_name = weight.name.replace(\':\', \'_\')\r\n                tf_summary.histogram(mapped_weight_name, weight)\r\n                if self.write_images:\r\n                    w_img = array_ops.squeeze(weight)\r\n                    shape = K.backend.int_shape(w_img)\r\n                    if len(shape) == 2:  # dense layer kernel case\r\n                        if shape[0] > shape[1]:\r\n                            w_img = array_ops.transpose(w_img)\r\n                            shape = K.backend.int_shape(w_img)\r\n                        w_img = array_ops.reshape(w_img, [1, shape[0], shape[1], 1])\r\n                    elif len(shape) == 3:  # convnet case\r\n                        if K.image_data_format() == \'channels_last\':\r\n                            # switch to channels_first to display\r\n                            # every kernel as a separate image\r\n                            w_img = array_ops.transpose(w_img, perm=[2, 0, 1])\r\n                            shape = K.backend.int_shape(w_img)\r\n                        w_img = array_ops.reshape(w_img,\r\n                                                  [shape[0], shape[1], shape[2], 1])\r\n                    elif len(shape) == 1:  # bias case\r\n                        w_img = array_ops.reshape(w_img, [1, shape[0], 1, 1])\r\n                    else:\r\n                        # not possible to handle 3D convnets etc.\r\n                        continue\r\n    \r\n                    shape = K.backend.int_shape(w_img)\r\n                    assert len(shape) == 4 and shape[-1] in [1, 3, 4]\r\n                    tf_summary.image(mapped_weight_name, w_img)\r\n ```\r\nUsing the above code will not report error any more. \r\nYou can write yourself custom callback with a little different with keras.callback.Tensorboard in set_model method. Alternately, you can inherit keras.callback.Tensorboard and rewrite set_model method.', '@ophiry , were u able to resolve this issue?', ""@tiantengfei you'd probably want to be using `model.trainable_variables` instead of `tf.trainable_variables()` so that it is scoped by the model, right? This looks like a simple enough fix. It looks like it's still like that in tfv2 as well"", ""sess=tf.Session()\r\nsaver = tf.train.import_meta_graph('/content/checkpoint_dir/MyModel_all.meta')\r\nsaver.restore(sess, tf.train.latest_checkpoint('./checkpoint_dir'))\r\nw1 = graph.get_tensor_by_name('x1_all:0')\r\n\r\nw1 shape is <tf.Tensor 'x1_all:0' shape=(?, 1) dtype=float32>\r\nx = np.linspace(0,3,100).reshape(-1,1)\r\nfeed_dict ={w1:x)\r\n\r\nbut when the feed_dict shape is <100,1>，turns a error? \r\n\r\nthe error is \r\nYou must feed a value for placeholder tensor 'x1' with dtype float and shape [?,1]"", ""I think it's row by column so your second value is the X which is 1.. the\nfirst value is Y which is your variable\n\nOn Thu, 19 Dec 2019, 1:52 pm fantasyjie, <notifications@github.com> wrote:\n\n> sess=tf.Session()\n> saver =\n> tf.train.import_meta_graph('/content/checkpoint_dir/MyModel_all.meta')\n> saver.restore(sess, tf.train.latest_checkpoint('./checkpoint_dir'))\n> w1 = graph.get_tensor_by_name('x1_all:0')\n>\n> w1 shape is <tf.Tensor 'x1_all:0' shape=(?, 1) dtype=float32>\n> x = np.linspace(0,3,100).reshape(-1,1)\n> feed_dict ={w1:x)\n>\n> but when the feed_dict shape is <100,1>，turns a error?\n>\n> the error is\n> You must feed a value for placeholder tensor 'x1' with dtype float and\n> shape [?,1]\n>\n> —\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/keras-team/keras/issues/10074?email_source=notifications&email_token=AA6XBOMPHEFHEPN6OVLE3CLQZN4BLA5CNFSM4E5T24KKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHJVLDA#issuecomment-567498124>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AA6XBOLFT4QNTJQ3JVESSSDQZN4BLANCNFSM4E5T24KA>\n> .\n>\n"", ""> I think it's row by column so your second value is the X which is 1.. the first value is Y which is your variable\r\n> […](#)\r\n> On Thu, 19 Dec 2019, 1:52 pm fantasyjie, ***@***.***> wrote: sess=tf.Session() saver = tf.train.import_meta_graph('/content/checkpoint_dir/MyModel_all.meta') saver.restore(sess, tf.train.latest_checkpoint('./checkpoint_dir')) w1 = graph.get_tensor_by_name('x1_all:0') w1 shape is <tf.Tensor 'x1_all:0' shape=(?, 1) dtype=float32> x = np.linspace(0,3,100).reshape(-1,1) feed_dict ={w1:x) but when the feed_dict shape is <100,1>，turns a error? the error is You must feed a value for placeholder tensor 'x1' with dtype float and shape [?,1] — You are receiving this because you commented. Reply to this email directly, view it on GitHub <#10074?email_source=notifications&email_token=AA6XBOMPHEFHEPN6OVLE3CLQZN4BLA5CNFSM4E5T24KKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHJVLDA#issuecomment-567498124>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AA6XBOLFT4QNTJQ3JVESSSDQZN4BLANCNFSM4E5T24KA> .\r\n\r\nthank you very much,you are right ,i had a stupid mistake,thanks a lot."", 'Fantastic, Glad I could help.\r\n\r\n I’ve made that mistake myself a few times. I wasn’t entirely sure if my answer was right so I’m glad it helped you 😊 ']","[""\r\nimport tensorflow\r\nkeras = tensorflow.keras\r\nimport numpy\r\n\r\n\r\nINPUT_LEN = 10\r\nN = 100\r\n\r\n\r\ndef get_sub_model(in_dim, out_dim):\r\n    input_layer = keras.layers.Input(shape=(in_dim,))\r\n    dense=keras.layers.Dense(out_dim)\r\n    return keras.models.Model(inputs=input_layer, outputs=dense(input_layer))\r\n\r\n\r\ninput_layer = keras.layers.Input(shape=(INPUT_LEN,), name='model_input')\r\nsub_model1 = get_sub_model(INPUT_LEN, 5)\r\nsub_model2 = get_sub_model(5, 1)\r\nm = keras.models.Model(inputs=[input_layer], outputs=sub_model2(sub_model1(input_layer)))\r\nm.compile(loss='binary_crossentropy', optimizer='adam')\r\n\r\nx = numpy.random.rand(N, INPUT_LEN)\r\ny = numpy.random.randint(0, 2, N)\r\n\r\nm.fit(x, y, callbacks=[keras.callbacks.TensorBoard(log_dir='/tmp', histogram_freq=1)], epochs=10, validation_data=(x,y))\r\n"", '\r\nTraceback (most recent call last):\r\n  File ""/Users/ophir/dev/ophir/tensorboard_issue.py"", line 25, in <module>\r\n    m.fit(x, y, callbacks=[keras.callbacks.TensorBoard(log_dir=\'/tmp\', histogram_freq=1)], epochs=10, validation_data=(x,y))\r\n  File ""/Users/ophir/miniconda3/envs/p3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/training.py"", line 1216, in fit\r\n    validation_steps=validation_steps)\r\n  File ""/Users/ophir/miniconda3/envs/p3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/training_arrays.py"", line 269, in fit_loop\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File ""/Users/ophir/miniconda3/envs/p3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/callbacks.py"", line 95, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File ""/Users/ophir/miniconda3/envs/p3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/callbacks.py"", line 799, in on_epoch_end\r\n    result = self.sess.run([self.merged], feed_dict=feed_dict)\r\n  File ""/Users/ophir/miniconda3/envs/p3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 900, in run\r\n    run_metadata_ptr)\r\n  File ""/Users/ophir/miniconda3/envs/p3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1135, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File ""/Users/ophir/miniconda3/envs/p3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run\r\n    run_metadata)\r\n  File ""/Users/ophir/miniconda3/envs/p3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor \'input_1\' with dtype float and shape [?,10]\r\n\t [[Node: input_1 = Placeholder[dtype=DT_FLOAT, shape=[?,10], _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]\r\n']",[],0,0
329,keras,9455,closed,BatchNormalization calculates batch mean and variance even in inference mode,"I'm not completely confident in the issue title, but I find it plausible and would love it if someone from the keras community helped verify/fix the issue.

Here's the steps to reproduce:
1. Build any model with one or more batch norm layers.  In my case it was a functional API model, although I doubt that is necessary to reproduce.
2. run model.predict() (wrap it in a timer of some sort)
3. At normalization.py:174 (as of master) add 
4. run model.predict()
5. observe that the model runs significantly faster.  For my model I get about a 3X speed increase.

",,"[""Investigating a bit more, I think I understand what's going on.  At the end of `BatchNormalization.call` we return a `K.in_training_phase` which basically does a `tf.cond()` on the training phase to use either the training mode batch norm or the inference batch norm.  Tensorflow executed both branches, so even though the calculation is correct, we end up calculating the mean and variance even in inference mode.\r\n\r\nDoes that make sense?  Any thoughts on a solution?"", ""This is as designed (by TF). So I don't think we can avoid it.\r\nRef : https://github.com/tensorflow/tensorflow/issues/10800"", ""I think I agree that it can't be avoided from a tensorflow perspective if tf.cond is used (although I'm not an expert on TF graphs, so who knows?)  But regardless, there's already logic in BatchNorm to short circuit the moments calculation if you pass training=False as an argument.  That's pretty awkward, however.  In my case I end up doing something along the lines of `\r\nprev = BatchNormalization()(prev, training=training)\r\n`\r\nwhere training is a global I manage.  It seems like there's got to be a better way, and if there's not, at the very least I would expect the documentation to be modified to warn the users about the overhead in inference mode and the way to avoid it.\r\n""]",[],['training=True'],0,0
330,keras,11703,closed,train.fit epoch1 uses -inf,"There seems to be an issue with model.fit in relation to loading previous weights.
The problems basically as seen in the logging  :
 
Epoch 00001: val_acc improved from -inf to 0.95089, saving model to \Chicken-weights.best.hdf5

Every fit starts with ""-inf"",  negative infinitive thus ignoring the last loaded weights results from previous runs. And so that will overwrite any best trainings from previous runs. if epoch 1 is worse it would still overwrite past saved results. due to ""-inf"", fit cannot improve from past runs.








",type:feature,"['I believe the issue is complicated since the `History` object is not saved with the model. We would need to rerun the model on the validation set before starting the training again, which might not be desirable by some users.\r\nPlus the model has no knowledge that it was loaded rather than trained directly.\r\n\r\nA simple fix would be to change `filepath` in your example to avoid overwriting. Supporting your use case would be extremely difficult.', ""You got me wrong, the only problem i see here, that it starts with -inf\r\nIt's not the history i'm after, (in fact wouldn't want that), for distributed computing, and early stopping I need to be able to tell what the last best result was and not let it start all over again as it currently does. (not overwriting previous hdf5 with bad results !!). Having an options to give it a starting score to test against is all that is missing currently. \r\nIn my view it wouldnt be hard to code either, (to pass a value for -inf instead) but i'm not owner of this source code.""]","[""\r\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\r\ncallbacks_list = [checkpoint]\r\nmodel.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=ep, batch_size=32, callbacks=callbacks_list, verbose=2)\r\n""]",[],0,0
331,keras,12956,closed,Memory leak in Keras: Functions Clone_model() and Set_Weights() become slower and slower when used in loop,"### Configuration

| Library                        | Version  |
| ------------------------------ | -------- |
| pyhton                         | 3.6.8    |
| GCC                            | 7.3.0    |
| tensorflow-base/tensorflow-gpu | 1.13.1   |
| keras-gpu/keras-base           | 2.2.4    |
| theano                         | 1.0.4    |
| cudnn                          | 7.6.0    |
| cudatoolkit                    | 10.0.130 |

Machine Configuration：

Ubuntu 18.04.2 LTS

Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz 128GB RAM

CUDA Version: 10.2 

Double GeForce GTX 1080ti 

---

### Description

I use  to clone the resnet50 model and use   to set the weights for each layer. But I find that the time consumption for each iteration becomes much larger as time goes by. At the first iteration, the clone stage takes about 4 seconds and the setting-weights stage takes about 3 seconds but those turn to be 34 seconds and 4 minutes at the 50th iteration. 

Is there a problem with my usage? Or would there be a memory leak in keras?

I hope to hear from you soon. Thanks in advance.

The code is shown as bellow:



Here is partial result:

![image](https://user-images.githubusercontent.com/17698785/59478187-019a5280-8e8b-11e9-8901-3e01a0b9daa4.png)

![image](https://user-images.githubusercontent.com/17698785/59478193-08c16080-8e8b-11e9-9e83-a595c0fe2e35.png)
",,"['Keras may leak memory。 load_model,memory continues to grow']","['python\r\nimport keras\r\nimport sys\r\nimport datetime\r\n\r\ndef get_HH_mm_ss(td):\r\n    days,seconds = td.days,td.seconds\r\n    hours = days * 24 + seconds // 3600\r\n    minutes = (seconds % 3600)//60\r\n    secs = seconds % 60\r\n    return hours,minutes,secs\r\n\r\n\r\nloop_iter = int(sys.argv[1])\r\norigin_resnet = keras.applications.resnet50.ResNet50(include_top=True, weights=\'imagenet\', input_tensor=None, input_shape=None, pooling=None, classes=1000)\r\n\r\nfor i in range(loop_iter):\r\n    print(""Iteration {}"".format(i))\r\n    s = datetime.datetime.now()\r\n    copy_model = keras.models.clone_model(origin_resnet)\r\n    e1 = datetime.datetime.now()\r\n    copy_model.set_weights(origin_resnet.get_weights())\r\n    e2 = datetime.datetime.now()\r\n    td_copy = e1 - s\r\n    td_set_weights = e2 - e1\r\n    h1, m1, s1 = get_HH_mm_ss(td_copy)\r\n    print(""Copy model: Time used: {} hour,{} min,{} sec"".format(h1, m1, s1))\r\n    h2, m2, s2 = get_HH_mm_ss(td_set_weights)\r\n    print(""Set weights: Time used: {} hour,{} min,{} sec"".format(h2, m2, s2))\r\n\r\n']","['keras.models.clone_model()', 'set_weights()']",0,0
332,keras,10882,closed,"""UnboundLocalError: local variable 'x' referenced before assignment"" while using load_model","
this is my code


package versions : 

- Keras==2.2.0
- Keras-Applications==1.0.2
- Keras-Preprocessing==1.0.1
- tensorflow-gpu==1.9.0",,"['error log\r\n```\r\nTraceback (most recent call last):\r\n  File ""concat.py"", line 58, in <module>\r\n    x = keras.models.load_model(\'hybrid.h5\')\r\n  File ""/home/jediyoda/.virtualenvs/py36/lib/python3.6/site-packages/keras/engine/saving.py"", line 261, in load_model\r\n    model = model_from_config(model_config, custom_objects=custom_objects)\r\n  File ""/home/jediyoda/.virtualenvs/py36/lib/python3.6/site-packages/keras/engine/saving.py"", line 335, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File ""/home/jediyoda/.virtualenvs/py36/lib/python3.6/site-packages/keras/layers/__init__.py"", line 55, in deserialize\r\n    printable_module_name=\'layer\')\r\n  File ""/home/jediyoda/.virtualenvs/py36/lib/python3.6/site-packages/keras/utils/generic_utils.py"", line 145, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File ""/home/jediyoda/.virtualenvs/py36/lib/python3.6/site-packages/keras/engine/network.py"", line 1036, in from_config\r\n    process_layer(layer_data)\r\n  File ""/home/jediyoda/.virtualenvs/py36/lib/python3.6/site-packages/keras/engine/network.py"", line 1022, in process_layer\r\n    custom_objects=custom_objects)\r\n  File ""/home/jediyoda/.virtualenvs/py36/lib/python3.6/site-packages/keras/layers/__init__.py"", line 55, in deserialize\r\n    printable_module_name=\'layer\')\r\n  File ""/home/jediyoda/.virtualenvs/py36/lib/python3.6/site-packages/keras/utils/generic_utils.py"", line 145, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File ""/home/jediyoda/.virtualenvs/py36/lib/python3.6/site-packages/keras/engine/network.py"", line 1063, in from_config\r\n    return cls(inputs=input_tensors, outputs=output_tensors, name=name)\r\n  File ""/home/jediyoda/.virtualenvs/py36/lib/python3.6/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper\r\n    return func(*args, **kwargs)\r\n  File ""/home/jediyoda/.virtualenvs/py36/lib/python3.6/site-packages/keras/engine/network.py"", line 91, in __init__\r\n    self._init_graph_network(*args, **kwargs)\r\n  File ""/home/jediyoda/.virtualenvs/py36/lib/python3.6/site-packages/keras/engine/network.py"", line 165, in _init_graph_network\r\n    layer, node_index, tensor_index = x._keras_history\r\nUnboundLocalError: local variable \'x\' referenced before assignment\r\n```', 'update - \r\nsame error when using \r\n```Model.from_config()``` \r\nand \r\n```model_from_json()```\r\n\r\n\r\n', ""you have a list outputs after going over all attributes. I don't understand the need for concatenation after that. Also, pop() method of keras is buggy, do:\r\n```\r\n# ... Load pre-trained VGG16 model\r\n\r\nmodel.layers.pop() # Get rid of the classification layer\r\nmodel.layers.pop() # Get rid of the dropout layer\r\nmodel.outputs = [model.layers[-1].output]\r\nmodel.layers[-1].outbound_nodes = []\r\n```\r\nI am also little uncomfortable the way you have defined things here,\r\n```\r\nmodel = model(_in)\r\nmodel = keras.models.Model(_in, model)\r\noutputs.append(model.output)\r\nprint('processed', attribute)\r\n```\r\nuse  different variables for different use, although I don't see the error in this."", 'Yea i agree i could define things in a better way. \nAnd about the concatenation having a single output tensor is easier to export for tensorflow serving. I could jus slice the result and get the respective predictions for each attribute \n', 'any update on this? I face similar issue when trying to merge clone of the same model into a single bigger model. it works but after saving and loading it I see the same error']","[""\r\nimport os\r\nimport sys\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\nimport tensorflow as tf\r\nimport keras\r\nfrom keras import backend as K\r\n\r\nconfig = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 12} ) \r\nsess = tf.Session(config=config) \r\nK.set_session(sess)\r\n\r\nattributes = ['a1', 'a2', 'a3', 'a4', 'a5']\r\n\r\n\r\nimg_height, img_width = 224, 224\r\n_in = keras.layers.Input(shape=(img_height, img_width, 3), name='img_input')\r\n\r\noutputs = []\r\n\r\nfor attribute in attributes:\r\n    model = keras.models.load_model(f'models/{attribute}/xception/model.h5')\r\n    model.name = attribute\r\n    for layer in model.layers:\r\n        layer.name = attribute+layer.name\r\n    model.layers.pop(0)\r\n    model = model(_in)\r\n    model = keras.models.Model(_in, model)\r\n    outputs.append(model.output)\r\n    print('processed', attribute)\r\n    \r\n\r\n_out = keras.layers.concatenate(outputs)\r\nhybrid_model = keras.models.Model(_in, _out)\r\nhybrid_model.name = 'hybrid'\r\nkeras.utils.vis_utils.plot_model(hybrid_model, 'hybrid.png')\r\n\r\nhybrid_model.save('hybrid.h5')\r\n\r\nprint(hybrid_model.output)\r\nx = keras.models.load_model('hybrid.h5')\r\n\r\n""]",[],0,0
333,keras,5014,closed,Error loading a saved model?,"There is my model:


 I fitted the model.However when I load it from a .h5 file,i got an error.


I guess it may caused by the complicacy of my model?",,"['Hi! I encountered a similar problem and got a workaround by saving the model architecture and weights separately, then loading the model from that. Could be a quick fix while this issue is not resolved. ', ""I had same problems with save_model + load_model on trained model with Adam optimizer on Theano.\r\nKeras 1.1.1\r\nThe problem was that the optimizer weights were not loaded in the same order so there was an assertion on weight shapes. I could solve locally this issue modifying Adam code to set names for each weights it used so that weights matched between save and load.\r\nIn optimizers.py :\r\n```\r\n#        self.iterations = K.variable(0)\r\n#TS 20170208 bugfix : save_model + load_model not working after training\r\n        self.iterations = K.variable(0, name='Adam_iter')\r\n\r\n```\r\n\r\n```\r\n#TS 20170208 bugfix : save_model + load_model not working after training\r\n#        ms = [K.zeros(shape) for shape in shapes]\r\n        ms = [K.zeros(shape,name='Adam_p_ms_'+ str(p.name)) for p,shape in zip(params,shapes)]\r\n#        vs = [K.zeros(shape) for shape in shapes]\r\n        vs = [K.zeros(shape,name='Adam_p_vs_'+ str(p.name)) for p,shape in zip(params,shapes)]\r\n\r\n```\r\n\r\nI haven't seen yet if there is a naming convention that should be used and if this is the correct fix, probably all optimizers need it.\r\n"", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs, but feel free to re-open it if needed.\n', ""Reproduced with Keras 2.0.5 on Tensorflow with the Adam optimizer. I'm using an Inception V4 model."", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', 'Still a problem.', 'Still a problem to me with Keras on Tensorflow 1.8 when trying this tutorial https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/03C_Keras_API.ipynb \r\n\r\nThis happens when I tried to save and load the Sequential Model in the tutorial. However no error when saving the Functional Model.']","[""\r\ndata_dim = 25*88\r\ntimesteps = 40\r\n\r\nsecen = Sequential()\r\nsecen.add(Convolution2D(32, 3, 3, activation='relu', border_mode='same', input_shape=(1, 25, 88)))\r\nsecen.add(MaxPooling2D((2, 2)))\r\nsecen.add(Dropout(0.25))\r\nsecen.add(Convolution2D(32, 3, 3, activation='relu', border_mode='same'))\r\nsecen.add(MaxPooling2D((2, 2)))\r\nsecen.add(Dropout(0.25))\r\nsecen.add(Convolution2D(32, 3, 3, activation='relu', border_mode='same'))\r\nsecen.add(MaxPooling2D((2, 2)))\r\nsecen.add(Dropout(0.25))\r\nsecen.add(Flatten())\r\n\r\nsecde = Sequential()\r\nsecde.add(Convolution2D(32, 3, 3,border_mode='same', input_shape=(1, 25, 11)))\r\nsecde.add(Activation('tanh'))\r\nsecde.add(UpSampling2D(size=(1, 2)))\r\nsecde.add(Convolution2D(32, 3, 3, border_mode='same'))\r\nsecde.add(Activation('tanh'))\r\nsecde.add(UpSampling2D(size=(1, 2)))\r\nsecde.add(Convolution2D(32, 3, 3, border_mode='same'))\r\nsecde.add(Activation('tanh'))\r\nsecde.add(UpSampling2D(size=(1, 2)))\r\nsecde.add(Convolution2D(1, 3, 3, border_mode='same'))\r\nsecde.add(Activation('sigmoid'))\r\n\r\nmusic_input = Input(shape=(timesteps, 1, 25, 88))\r\n\r\nencoded_frame_sequence = TimeDistributed(secen)(music_input)\r\nencoded_music = LSTM(data_dim/8,return_sequences=True)(encoded_frame_sequence)\r\nencoded_music=Dropout(0.25)(encoded_music)\r\nencoded_music=Reshape((timesteps, 1, 25,11 ))(encoded_music)\r\ndecode_music=TimeDistributed(secde)(encoded_music)\r\n\r\nmodel=Model(input=music_input,output=decode_music)\r\n\r\nmodel.compile(optimizer='rmsprop',\r\n              loss='binary_crossentropy')\r\n"", '\r\nmodel = load_model(\'midinet_0110.h5\')\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""C:\\Anaconda2\\Lib\\site-packages\\keras\\models.py"", line 179, in load_model\r\n    model.optimizer.set_weights(optimizer_weight_values)\r\n  File ""C:\\Anaconda2\\Lib\\site-packages\\keras\\optimizers.py"", line 90, in set_weights\r\n    \'provided weight shape \' + str(w.shape))\r\nValueError: Optimizer weight shape (275L, 275L) not compatible with provided weight shape (1056, 275)\r\n']",[],0,0
334,keras,8423,closed,Use of Dense layer in imdb_cnn sample,"Here is the summary of the model in imdb_cnn sample, can anyone tell me what is the purpose of dense_6  layer?

embedding_6 (Embedding)      (None, 400, 50)           250000    
dropout_5 (Dropout)          (None, 400, 50)           0         
conv1d_4 (Conv1D)            (None, 398, 250)          37750     
global_max_pooling1d_5 (Glob (None, 250)               0         
**dense_6 (Dense)              (None, 250)               62750**     
dropout_6 (Dropout)          (None, 250)               0         
activation_5 (Activation)    (None, 250)               0         
dense_7 (Dense)              (None, 1)                 251       
activation_6 (Activation)    (None, 1)                 0         

Total params: 350,751
Trainable params: 350,751
Non-trainable params: 0
",,[],[],[],0,0
335,keras,11493,closed,Introduction of Sobolev training capability in Keras,"Hi,

This is a feature request that I think could be interesting for several users and I wish to share it, as I am not able to implement it.
In the field of scientific computing (especially Computational Fluid Dynamic), there are more and more attempts to use neural network to speed up expensive iterative computations. It can be done in two ways:

- by improving the quality of the initialisation the computation with a neural network

- for computation based on Jacobian evaluation (such as Newton-Raphson), by estimating the Jacobian with a neural network.

In such cases, it is quite common to have access to the derivatives of the function one want to learn, therefore another kind of training can be used : Sobolev training : https://arxiv.org/abs/1706.04859

In such a training, the cost function is not only dependant of the output values of the neural network and the ground truth, but a term depending of the derivatives of the outputs with regards to the inputs is added. This requires to provide also the ground truth derivatives to the learning process.

It would be great to have to possibility to provide the derivatives values in Keras with a call like :



 being the matrices of the derivatives of the outputs with regards to the inputs.

The quality of the training is improved, as in addition of the knowledge of the output value, one use also the available knowledge of the derivative of the outputs with regards to the inputs. This would require to compute also for the learning process the derivative of the output of the network with regards to its inputs, in order to be able to do the gradient based update.

Once the learning process is achieved, both outputs values and derivatives values would have been learn in a single neural networks and it accessing to the values of the derivatives would be adding value to the information gathered by such a training, that means being able to call:



Best regards,",type:feature,[],[],"['model.fit(x_train, (y_train, dy_train), epochs=5, batch_size=32, sobolev_coef=0.1)', 'dy_train ', 'model.predict_derivatives(x_test, batch_size=128)']",0,0
336,keras,13098,closed,About the reloading problem of input layers assigned by tensor,"1. Problem description

When I design input layers via assigning tensors including constants, variables and etc. to avoid direct input, in not reloading the model, training and predicting are valid. However, after saving the model and reloading it, input layers via assigning tensors are loaded as direct input layers without constant and variable tensors. I want to load an originally designed model with designing loss.

2. Code example

input1 = Input(shape=(10, 1))
input2 = Input(tensor=K.random_normal_variable((10, 1), 0, 1))
input3 = Input(tensor=K.random_normal((10, 1)))

x = Lambda(lambda x: x[0] + x[1] + x[2])([input1, input2, input3])
model = Model(inputs=[input1, input2, input3], outputs=[x])

model.inputs
>> [<tf.Tensor 'input_45:0' shape=(?, 10, 1) dtype=float32>,
 <tf.Variable 'Variable_40:0' shape=(10, 1) dtype=float32_ref>,
 <tf.Tensor 'random_normal_22:0' shape=(10, 1) dtype=float32>]

model.save('example_model.h5')
r_model = load_model('example_model.h5')
r_model.inputs
>>[<tf.Tensor 'input_45_1:0' shape=(?, 10, 1) dtype=float32>,
 <tf.Tensor 'input_46:0' shape=(10, 1) dtype=float32>,
 <tf.Tensor 'input_47:0' shape=(10, 1) dtype=float32>]",type:support,[],[],[],0,0
337,keras,10387,closed,validation st,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

Hello. I have a problem with memory error.

I wrote next script



After that, if I set  any value for steps_per_epoch and validation_steps, I get messages next type:

> 2018-06-09 23:59:16.234583: W tensorflow/core/framework/allocator.cc:101] Allocation of 1507688448 exceeds 10% of system memory.

I have not found any information about this problem. Please, help me.

",,[],"['\r\nclass AutoencoderCreator:\r\n    def __init__(self, size_input_image, size_hidden_layer, seed):\r\n        self.size_input_image = size_input_image\r\n        self.size_hidden_layer = size_hidden_layer\r\n        self.seed = seed\r\n\r\n    def __create_model(self):\r\n        in_sh = (self.size_input_image[0], self.size_input_image[1], 1)\r\n        hd_sh = self.size_hidden_layer\r\n\r\n        np.random.seed(self.seed)\r\n\r\n        input_img = Input(shape=in_sh)\r\n\r\n        x = Conv2D(128, (7, 7), activation=\'relu\', padding=\'same\')(input_img)\r\n        x = MaxPooling2D((2, 2), padding=\'same\')(x)\r\n        x = Conv2D(32, (2, 2), activation=\'relu\', padding=\'same\')(x)\r\n        x = MaxPooling2D((2, 2), padding=\'same\')(x)\r\n        encoded = Conv2D(1, (hd_sh[0], hd_sh[1]), activation=\'relu\', padding=\'same\')(x)\r\n\r\n        input_encoded = Input(shape=(hd_sh[0], hd_sh[1], 1))\r\n        x = Conv2D(32, hd_sh, activation=\'relu\', padding=\'same\')(input_encoded)\r\n        x = UpSampling2D((2, 2))(x)\r\n        x = Conv2D(128, (2, 2), activation=\'relu\', padding=\'same\')(x)\r\n        x = UpSampling2D((2, 2))(x)\r\n        decoded = Conv2D(1, (7, 7), activation=\'sigmoid\', padding=\'same\')(x)\r\n\r\n        self.encoder = Model(input_img, encoded, name=""encoder"")\r\n        decoder = Model(input_encoded, decoded, name=""decoder"")\r\n        self.autoencoder = Model(input_img, decoder(self.encoder(input_img)), name=""autoencoder"")\r\n\r\n    def __call__(self):\r\n        self.__create_model()\r\n        self.autoencoder.compile(optimizer=\'adam\', loss=\'binary_crossentropy\')\r\n        return self.autoencoder\r\n\r\n\r\nx_train = np.array(x).astype(\'float32\') / 255.\r\nx_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\r\n\r\nautoencoder = AutoencoderCreator(size_input_image=input_size, size_hidden_layer=hidden_layer, seed=seed)\r\nautoencoder().fit(\r\n    x_train, x_train, epochs=epochs, validation_split=test_size, verbose=1,\r\n    steps_per_epoch=batch_size, validation_steps=batch_size\r\n)\r\n']",[],0,0
338,keras,12269,closed,Masking layer conflict with Embedding layer,"Tensorflow version = 1.5
keras version = 2.2.4
Everything is OK when I train the model on GPU machine, but something happens when training or predicting model on CPU machine.
It seems that Embedding Layser cannot recognize the masking value -1.
Here are my codes:

And I preprocess the list of word intergers with padding and filling with -1.


Here is the error:


Once again, everything is fine on my GPU machine. So what's wrong with the same code running on the CPU machine ? ",type:bug/performance wontfix,"['I seems that you version of tensorflow is quite old. Maybe updating would do the trick?', 'My bad it seems related to the embedding not being able to handle something else than positive numbers. Can you shift everything and use 0 for masking instead of -1?']","[""\r\ndef make_model():\r\n    title_ids = Input(shape=(MAXLEN_CONTENT, ), name = 'title_content_ids')\r\n    title_ids_mask = Masking(mask_value=-1, name='mask')(title_ids) # (?, 3075)\r\n    embedding = Embedding(len(tok.word_index) + 1, 300,  trainable=True)\r\n\r\n    title_embed = embedding(title_ids_mask) # (?, 44, 300)\r\n    title_encode = encode_rcnn(title_embed) # (?, 856)\r\n\r\n    mlp0 = Dense(512,activation='relu',name='mlp0')(title_encode)\r\n    mlp1 = Dense(256,activation='relu',name='mlp1')(mlp0)\r\n    score = Dense(max(cate_label.values())+1 ,activation='softmax', name='mlp2')(mlp1) \r\n    model = Model( title_ids, score)\r\n    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy'] ) \r\n    return model\r\n"", '\r\nCaused by op u\'embedding_1/embedding_lookup\', defined at:\r\n  File ""cate_class.py"", line 281, in <module>\r\n    model = make_model()\r\n  File ""cate_class.py"", line 208, in make_model\r\n    title_embed = embedding(title_ids_mask) # (?, 44, 300)\r\n  File ""/data01/yanan/envgpu/local/lib/python2.7/site-packages/keras/engine/base_layer.py"", line 457, in __call__\r\n    output = self.call(inputs, **kwargs)\r\n  File ""/data01/yanan/envgpu/local/lib/python2.7/site-packages/keras/layers/embeddings.py"", line 141, in call\r\n    out = K.gather(self.embeddings, inputs)\r\n  File ""/data01/yanan/envgpu/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py"", line 1228, in gather\r\n    return tf.nn.embedding_lookup(reference, indices)\r\n  File ""/data01/yanan/envgpu/local/lib/python2.7/site-packages/tensorflow/python/ops/embedding_ops.py"", line 325, in embedding_lookup\r\n    transform_fn=None)\r\n  File ""/data01/yanan/envgpu/local/lib/python2.7/site-packages/tensorflow/python/ops/embedding_ops.py"", line 150, in _embedding_lookup_and_transform\r\n    result = _clip(_gather(params[0], ids, name=name), ids, max_norm)\r\n  File ""/data01/yanan/envgpu/local/lib/python2.7/site-packages/tensorflow/python/ops/embedding_ops.py"", line 54, in _gather\r\n    return array_ops.gather(params, ids, name=name)\r\n  File ""/data01/yanan/envgpu/local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 2585, in gather\r\n    params, indices, validate_indices=validate_indices, name=name)\r\n  File ""/data01/yanan/envgpu/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 1864, in gather\r\n    validate_indices=validate_indices, name=name)\r\n  File ""/data01/yanan/envgpu/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File ""/data01/yanan/envgpu/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3160, in create_op\r\n    op_def=op_def)\r\n  File ""/data01/yanan/envgpu/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1625, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): indices[117,157] = -1 is not in [0, 875812)\r\n\t [[Node: embedding_1/embedding_lookup = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[""loc:@embedding_1/embeddings""], validate_indices=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](embedding_1/embeddings/read, embedding_1/Cast)]]\r\n']","[""title_content_pad_array = sequence.pad_sequences(title_content_ids_list, maxlen=MAXLEN_CONTENT, padding='post', truncating='post',value=-1)""]",0,0
339,keras,12524,closed,Neural Networks with no input (dummy) to optimise transformed output?,"Hello, 

I would like to create either a) a deep dense neural net that has n outputs with no activation function (for example, 3 numbers output) or b) an RNN that internally performs the same based on observed effects. So, I want to say, feed it stationary values of X=1 Y=1 but reconsider what the loss value is for the three inputs, in order to train to optimise my function

My issue is that I need to transform the output before the loss is given, eg:


Where: 

- Loss is the _true loss_ of the network
- my_function accepts the three outputs of the network at each step (a, b and c), rounded to integers
- my_function returns a float between 0.0 and 100.0 where 0.0 is the worst and 100.0 is the best possible value

Is it possible to get the network to learn from the above my_function as true loss? I've been trying for days to implement it. I can return the value of my_function() from my code, but when I input this code inside a custom loss function it just executes once at the start (TF Graphs).

Is there any way I can replace my loss with the above function using a Lambda layer, custom loss function or even a callback? I'm not sure how to do this, or if it's even possible (?)

This question seems to be aiming for a somewhat similar goal: https://github.com/keras-team/keras/issues/2691
but unfortunately I'm not quite good enough at Keras (yet!) to apply it to my problem

Help would be much appreciated,
Thanks

PS: I know a genetic or PSO would probably be better but my project is to show the effects of each method with results on multiple problems",,[],[],"['loss my_function=(a, b, c)']",0,0
340,keras,13637,closed,"""classes"" not working on flow_from_dataframe?","the  parameter works on  but not ?

testing code:


And below is my result on Keras==2.3.1 and Keras-Preprocessing==1.1.0:
![image](https://user-images.githubusercontent.com/16003088/70897626-f8eb2a80-202d-11ea-9898-1320dcad52a1.png)



",type:bug/performance,"[""I think the problem is from [here](https://github.com/keras-team/keras-preprocessing/blob/9a836c25177e1be5940e1b2ab19fdb383225c32a/keras_preprocessing/image/dataframe_iterator.py#L243)\r\n\r\nSet function sort 'classes'\r\nTo solve it, use OrderedDict\r\n\r\nhere is pull request for it\r\nhttps://github.com/keras-team/keras-preprocessing/pull/270"", 'Also facing the same issue. Will this be resolved soon? Is there any workaround that I can do for it to work on my end in the meantime?', '@qinst64 Moving this issue to closed status as there has been no recent activity.In case you still face the error please create a new issue,we will get you the right help.Thanks!']","['python\r\nimport keras\r\nfrom keras.preprocessing import image\r\nimport pandas as pd\r\n\r\ngen = image.ImageDataGenerator()\r\n\r\nf = gen.flow_from_directory(\'data\', classes=[\'A\', \'B\'])\r\nprint(f.class_indices)\r\nf = gen.flow_from_directory(\'data\', classes=[\'B\', \'A\'])\r\nprint(f.class_indices)\r\n\r\n# with ""data/A.jpg"" and ""data/B.jpg"" on disk\r\ndf = pd.DataFrame([[\'data/A.jpg\', \'A\'],[\'data/B.jpg\', \'B\']], columns=[\'filename\', \'class\'])\r\ndisplay(df)\r\nf = gen.flow_from_dataframe(df, classes=[\'A\', \'B\'])\r\nprint(f.class_indices)\r\nf = gen.flow_from_dataframe(df, classes=[\'B\', \'A\'])\r\nprint(f.class_indices)\r\n']","['classes', 'flow_from_directory', 'flow_from_dataframe']",0,0
341,keras,8315,closed,Categorical time sequence LSTM,"Hey there,

i converted my MLP Network to an LSTM Network to check if i can get a better accuracy taking
the time into consideration. I have sound signals that are split into multiple frames (87 Frames each sound signal) and 39 Features generated from those Frames (Mel Frequency Cepstrum Coefficients).
In my MLP Network i had a input dimension of 39 Feautres and in the LSTM Network i had to reshape
this vector to 3D Tensor and decided to have those 87 frames as timesteps. (It might make more sense to put this value higher).

So from  i reshaped the vector to  .
my output vector  also had to be reshaped from  to 
and i had to add the  Wrapper around my  output layer because
of the many-to-many (categories) nature of my classification problem.

here is the split and reshape part:

code
the creation of the model:


 depending
on the timesteps the dimensionalty of the 3D Tensor changes.

the keras function to categorical changes my 3D tensor back to 2D Tensor, so i have to reshape again.
Is there another way how tho accomplish this?

Greetings
",,"['Did you figure this out?', ""here is the working version:\r\n```\r\nnum_classes = 3\r\nn_features = 39\r\n\r\ntimesteps = 87 # amount of timeperiods of the audiosignal defined by the short time fourier transformation\r\n\r\nfeatures = features.reshape((int(features.shape[0]/timesteps), timesteps, 39)) # 2D -> 3D Vektor\r\nlabels = labels.reshape((int(labels.shape[0]/timesteps), timesteps, 1)) # 2D -> 3D Vektor\r\n\r\ntrain_test_split = np.random.rand(len(features)) < 0.70\r\n\r\nx_train = features[train_test_split]\r\ny_train = labels[train_test_split]\r\nx_test = features[~train_test_split]\r\ny_test = labels[~train_test_split]\r\n\r\ny_train = keras.utils.to_categorical(y_train, num_classes)\r\ny_test = keras.utils.to_categorical(y_test, num_classes)\r\n\r\ny_train = y_train.reshape((int(y_train.shape[0]/timesteps), timesteps, num_classes))\r\ny_test = y_test.reshape((int(y_test.shape[0]/timesteps), timesteps, num_classes))\r\n```\r\n\r\n```\r\n# create model\r\nmodel = Sequential()\r\nmodel.add(LSTM(max_features, input_shape=(timesteps, n_features), return_sequences=True))\r\nmodel.add(TimeDistributed(Dense(num_classes, activation='softmax')))\r\n# Compile model\r\noptimizer = keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\r\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\r\n```\r\n\r\nhope this helps :)""]","['\r\nnum_classes = 3\r\n\r\ntimesteps = 87\r\nn_features = 39\r\n\r\nfeatures = features.reshape((int(features.shape[0]/timesteps), timesteps, 39))\r\nlabels = labels.reshape((int(labels.shape[0]/timesteps), timesteps, 1))\r\n\r\ntrain_test_split = np.random.rand(len(features)) < 0.50\r\n\r\nx_train = features[train_test_split]\r\ny_train = labels[train_test_split]\r\nx_test = features[~train_test_split]\r\ny_test = labels[~train_test_split]\r\n\r\ny_train = keras.utils.to_categorical(y_train, num_classes)\r\ny_test = keras.utils.to_categorical(y_test, num_classes)\r\n\r\ny_train = y_train.reshape((int(y_train.shape[0]/timesteps), timesteps, num_classes))\r\ny_test = y_test.reshape((int(y_test.shape[0]/timesteps), timesteps, num_classes))\r\n\r\n', ""\r\nmodel = Sequential()\r\nmodel.add(LSTM(max_features, input_shape=(timesteps, n_features), return_sequences=True))\r\nmodel.add(TimeDistributed(Dense(num_classes, activation='softmax')))\r\n""]","['(215499, 39)', '(2477, 87, 39)', 'y', '(215499,)', '(2477, 87, 1)', 'TimeDistribution', 'Dense()', 'features = features.reshape((int(features.shape[0]/timesteps), timesteps, 39))']",0,0
342,keras,9622,closed,questions about resnet50,"The official document of resnet50 describes that it needs the size of picture must be 197*197 at least. However, every row of my data talks about a sample and the feature of it is 737. Then, how can I use the model of resnet50 in my object? Thank you ~",,[],[],[],0,0
343,keras,11097,closed,Usefulness of Phased LSTM-CNN,"Hi all,

in the last weeks I was successful in setting up a Convolutional LSTM using the layers provided by keras (https://github.com/keras-team/keras/blob/master/keras/layers/convolutional_recurrent.py).

For another task in the past I did some experiments with Phased LSTM (https://arxiv.org/abs/1610.09513), which also has been implemented in keras and Tensorflow.

Now, I have these things on my mind:

1. The Phased LSTM model seems really useful for many purposes. Yet, it didn't get really popular over the last years, and I'm wondering about potential reasons. Do I miss any strong counter-arguments, or have there been successful alternative approaches to tackle the same issues (like input from different sources and irregular sampling intervals)?

2. In practice, these two methods should be compatible, right? A ""time gate"" could also be included in an LSTM-CNN setting, analogous to its application in a classical LSTM. Yet, since I couldn't find any implementation, I wonder if there might be any hurdles I'm not aware of.

3. If the first points do not disqualify the idea already, feel free to get in contact with me if you would like to cooperate on its implementation.

Thanks!
",type:feature,[],[],[],0,0
344,keras,10496,closed,How to set the keras jason file for theano and tensorflow backends respectively,"I have anaconda 2 with and a created  environment. The python2.7 works with  and , while the  works with  and . They both share the sameJason file. Everytime when I use  with either  or  backend, I needs to modify the Jason file accordingly. 

I am wondering if there are simpler ways to set up  file (e.g., create two different jason files) to deal with such the situation?",,[],[],"['python2.7 ', 'python3.5', 'keras', 'theano', 'python 3.5', 'keras', 'tensorflow', ' keras ', 'keras', 'theano', 'tensorflow', 'keras']",0,0
345,keras,8224,closed,A bug in loading model when using GRU with initial_state?,"following  https://github.com/fchollet/keras/issues/6142, I defined a model with initial_state as a parameter. The model can be trained successfully.
But when I try to evaluate the model and load model definition form json file, the following error has appeared.


code snippet of defining model:


code snippet of loading model definiation from json file:



Thanks in advance for your kind help.
",,[],[],"['ValueError: Layer gru_1 expects 1 inputs, but it received 2 input tensors. Input received: [/image_input, /initial_state]', ' GRU(opt.n_rnn)(image_input, initial_state=initial_state_input)', 'from keras.models import model_from_json', 'model = model_from_json(open(json_model_definition_path).read())']",0,0
346,keras,12785,closed,"ValueError: Variable kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope?","I'm currently working on an LSTM based research problem. However, while using the RNNs in keras, as I will demonstrate below, I'm running into the above-mentioned error.

I use TF version 1.12.0 and keras 2.2.4.

This seems to work with cells like LSTMCell but fails to work with UGRNNCell.. No idea how to fix this issue.


And this is my model:


When run this is the error obtained:



This results in the above error but works seamlessly and without any error when the UGRNNCell is replaced by an LSTMCell.
Any help would be appreciated.
",stat:awaiting tensorflower type:support,"[""I'm having the same error"", ""So the issue is that the contrib UGRNNCell has been quite out of date, and was using the latest api to create its weights. we probably won't update the contrib code anymore since contrib is going away in TF 2.0. Having said that I could move the implementation of UGRNNCell to tensorflow/addons, which is a new repository for extended tf features.\r\n\r\nWill do this in a week or two."", 'Any solution for this?']","['python\r\ncell1=tf.contrib.rnn.UGRNNCell(64)\r\ncell2=tf.contrib.rnn.UGRNNCell(64)\r\ncell3=tf.contrib.rnn.UGRNNCell(64)\r\ncell4=tf.contrib.rnn.UGRNNCell(64)\r\n', ""\r\nmodel = Sequential()\r\n\r\nmodel.add(RNN(cell1, input_shape=(train_X.shape[1:]),return_sequences=True))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Dropout(0.5))\r\n\r\nmodel.add(RNN(cell2,return_sequences=True))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Dropout(0.5))\r\n\r\nmodel.add(RNN(cell3, return_sequences=True))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Dropout(0.5))\r\n\r\nmodel.add(RNN(cell4,return_sequences=False))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Dropout(0.5))\r\n\r\nmodel.add(Dense(128, activation='relu'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(Dropout(0.5))\r\n\r\nmodel.add(Dense(1, activation='sigmoid'))\r\n\r\n"", 'python\r\nTraceback (most recent call last):\r\n  File ""fastcell.py"", line 43, in <module>\r\n    model.add(RNN(cell2,return_sequences=True))\r\n  File ""d:\\Users\\prane\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\base.py"", line 474, in _method_wrapper\r\n    method(self, *args, **kwargs)\r\n  File ""d:\\Users\\prane\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py"", line 175, in add\r\n    output_tensor = layer(self.outputs[0])\r\n  File ""d:\\Users\\prane\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py"", line 619, in __call__\r\n    return super(RNN, self).__call__(inputs, **kwargs)\r\n  File ""d:\\Users\\prane\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py"", line 757, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File ""d:\\Users\\prane\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py"", line 750, in call\r\n    input_length=timesteps)\r\n  File ""d:\\Users\\prane\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py"", line 3201, in rnn\r\n    outputs, _ = step_function(inputs[0], initial_states + constants)\r\n  File ""d:\\Users\\prane\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py"", line 737, in step\r\n    output, new_states = self.cell.call(inputs, states, **kwargs)\r\n  File ""d:\\Users\\prane\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\rnn_cell.py"", line 1683, in call\r\n    self._linear = _Linear(cell_inputs, 2 * self._num_units, True)\r\n  File ""d:\\Users\\prane\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\core_rnn_cell.py"", line 99, in __init__\r\n    initializer=kernel_initializer)\r\n  File ""d:\\Users\\prane\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py"", line 1487, in get_variable\r\n    aggregation=aggregation)\r\n  File ""d:\\Users\\prane\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py"", line 1237, in get_variable\r\n    aggregation=aggregation)\r\n  File ""d:\\Users\\prane\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py"", line 540, in get_variable\r\n    aggregation=aggregation)\r\n  File ""d:\\Users\\prane\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py"", line 492, in _true_getter\r\n    aggregation=aggregation)\r\n  File ""d:\\Users\\prane\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py"", line 861, in _get_single_variable\r\n    name, """".join(traceback.format_list(tb))))\r\nValueError: Variable kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\r\n\r\n  File ""d:\\Users\\prane\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py"", line 1770, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n  File ""d:\\Users\\prane\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py"", line 3274, in create_op\r\n    op_def=op_def)\r\n  File ""d:\\Users\\prane\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py"", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n']",[],0,0
347,keras,8562,closed,CNN Regression with VGG-16 predicts the same value regardless of the input image. ,"So just a set up of the problem I am trying to solve. I have around 200k 64x64x3 RGB images of patches of terrain that a robot drove over. Each patch has a corresponding label of what the roughness of that image patch is. The roughness values range from 0-160. The data was collected with the robot driving at varying speeds, hence the range of the roughness values. My aim is to be able to predict the roughness of a patch. I am using the VGG-16 network, with the last layer modified to do regression. My batch size is 1024, the loss is mean sqaured error, the optimize is rmsprop. The network is shown below. My problem is that after training, the network predicts the exact same value for each test image. Another point to note is that the training loss is always higher than the validation loss which is odd. Lastly I tried with other optimizers such as SGD and Adam, as well as varying batch sizes. Right now I am trying to train the network from scratch but it does not seem too promising. I am not sure what is going wrong here, and I would really appreciate any help I can get. Thanks




    if input_tensor is None:
        img_input = Input(shape=input_shape)
    else:
        if not K.is_keras_tensor(input_tensor):
            img_input = Input(tensor=input_tensor, shape=input_shape)
        else:
            img_input = input_tensor
    # Block 1
    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)
    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)

    # Block 2
    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)
    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)

    # Block 3
    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)
    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)
    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)

    # Block 4
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)

    # Block 5
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)

    x = Flatten(name='flatten')(x)
    x = Dense(4096, activation='relu', name='fc1')(x)
    x = Dense(4096, activation='relu', name='fc2')(x)
    x = Dense(1,name='regression_dense')(x)  ` ",,"['Dear,\r\n\r\nI have the same question,\r\nCould U share your project ,please?\r\nI want to learn from you.\r\n\r\nThx']",[],"['    input_shape = _obtain_input_shape(input_shape,\r\n                                      default_size=224,\r\n                                      min_size=48,\r\n                                      data_format=K.image_data_format(),\r\n                                      require_flatten=include_top,\r\n                                      weights=weights)']",0,0
348,keras,13492,closed,tf.keras.layers.Concatenate(axis=axis)(conc_layer) not working,"**System information**  
- Have I written custom code (as opposed to using example directory):  no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  2.0.0
- Keras version:  2.2.4-tf
- Python version: 3.6
- CUDA/cuDNN version:  10.1
- GPU model and memory:  Titan GTX 12Gb

**Describe the current behavior** 
//layer_type is in this case BatchNorm+ELU+Conv2D
 def dens_block(layer_num, layer_type,**kwargs):
	conc_layer = []
	def block(x):
		conc_layer.append(x)
		for i in range(layer_num):
		     x = layer_type(32, 2, strides=(1, 1), padding='same', **kwargs)(x)
		     conc_layer.append(x)
                     x = tf.keras.layers.Concatenate(axis=axis)(**list**(conc_layer))
		return x
	return block

**Describe the expected behavior**  
without the **list** in  tf.keras.layers.Concatenate the code not work

**Code to reproduce the issue**  
import tensorflow as tf
import netron


def bn_elu_conv(filters, kernel_size, strides=(1, 1), padding='valid', **kwargs):
	def layer(input_tensor):
		with tf.keras.backend.name_scope('bn_elu_conv'):
			x = tf.keras.layers.BatchNormalization()(input_tensor)
			x = tf.keras.layers.ELU()(x)
			x = tf.keras.layers.Conv2D(filters, kernel_size, strides=strides, padding=padding, **kwargs)(x)
		return x

	return layer


def conv_bn_elu(filters, kernel_size, strides=(1, 1), padding='valid', **kwargs):
	def layer(input_tensor):
		with tf.keras.backend.name_scope('conv_bn_elu'):
			x = tf.keras.layers.Conv2D(filters, kernel_size, strides=strides, padding=padding, **kwargs)(input_tensor)
			x = tf.keras.layers.BatchNormalization()(x)
			x = tf.keras.layers.ELU()(x)
		return x

	return layer


def dens_block(layer_num, filters, kernels_sizes, strides=(1, 1), padding='same', layer_type='bn_elu_conv', axis=-1, **kwargs):
	layer_type = _get_layerTyp(layer_type)
	filters = _check_list(filters, layer_num)
	kernels_sizes = _check_list(kernels_sizes, layer_num)
	conc_layer = []

	def block(x):
		conc_layer.append(x)
		for i in range(layer_num):
			x = layer_type(filters[i], kernels_sizes[i], strides=strides, padding=padding, **kwargs)(x)
			conc_layer.append(x)
			x = tf.keras.layers.Concatenate(axis=axis)(conc_layer)
			# x = tf.keras.layers.Concatenate(axis=axis)(list(conc_layer) # is working

		return x

	return block


def res_block(layer_num, filters, kernels_sizes, strides=(1, 1), padding='same', layer_type='bn_elu_conv', conv_block=False, **kwargs):
	layer_type = _get_layerTyp(layer_type)
	filters = _check_list(filters, layer_num)
	kernels_sizes = _check_list(kernels_sizes, layer_num)

	def block(x):
		input_tensor = x
		for i in range(layer_num):
			x = layer_type(filters[i], kernels_sizes[i], strides=strides, padding=padding, **kwargs)(x)
		if conv_block:
			input_tensor = layer_type(filters[-1], kernels_sizes[-1], strides=strides, padding=padding, )(input_tensor)
		x = tf.keras.layers.Add()([x, input_tensor])
		return x

	return block


def _get_layerTyp(layer_typ):
	if layer_typ is 'bn_elu_conv':
		return bn_elu_conv
	if layer_typ is 'conv_bn_elu':
		return conv_bn_elu
	else:
		raise ValueError('layer_typ have to be \'bn_elu_conv\' or \'conv_bn_elu\'')


def _check_list(x, list_len):
	if isinstance(x, list):
		if len(x) is list_len:
			return x
		else:
			raise TypeError('x have to be the same length as list_len')
	else:
		return _to_list(x, list_len)


def _to_list(x, list_len):
	new_x = []
	for _ in range(list_len):
		new_x.append(x)
	return new_x


if __name__ == '__main__':
	input_tensor = tf.keras.layers.Input(shape=(256, 256, 3))
	x = bn_elu_conv(filters=3, kernel_size=2)(input_tensor)
	for _ in range(4):
		x = dens_block(layer_num=4, filters=32, kernels_sizes=2)(x)
	model = tf.keras.models.Model(input_tensor, x)
	# model.compile(loss='binary_crossentropy', metrics=['accuracy'])
	model.summary()
	model.save('test.h5')
	netron.start('test.h5')


**Other info / logs**  
ValueError: Graph disconnected: cannot obtain value for tensor Tensor(""conv2d_2/Identity:0"", shape=(None, 255, 255, 32), dtype=float32) at layer ""concatenate"". The following previous layers were accessed without issue: ['input_1', 'batch_normalization', 'elu', 'conv2d', 'batch_normalization_1', 'elu_1', 'conv2d_1']
",,"[""Tensorflow evalutates tensors lazily, so when you pass conc_layer in tf.keras.layers.Concatenate(), it is still not evaluated until the model is defined in tf.keras.models.Model, when you define the model in the end, it passes the whole conc_layer array(the final one with all the layers) to each and every Concatenate object due to lazy evaluation and gives the error as not all layers are connected to every concatenate layer, on the other hand when you use list(conc_layer) or conc_layer[:] it copies the array into the function hence all Concatenate functions don't get the same array.""]",[],[],0,0
349,keras,8661,closed,"Unable to open file (Unable to open file: name = 'apnamodel.best.hdf5', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)","I am trying to save best weights of Convoluted NN using Keras, and want to evaluate model based on best weights. But the file for saving best weight does not create.  I have given absolute path, changed the name of file but nothing worked. Although the file is not created in the working directory, error occurs at line ""model.load_weights(""apnamodel.best.hdf5"")""

Here is the code

model = Sequential() 
model.add(Conv2D(32, 2, strides=1, activation='relu', input_shape = input_shape)) 
model.add(MaxPooling2D(pool_size=2))
model.add(Flatten())
model.add(Dense(50, activation='relu'))
model.add(Dense(10, activation= ""softmax""))
nadam  =  optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)
model.compile(loss='categorical_crossentropy', optimizer=nadam, metrics=['accuracy'])

# checkpoint

filepath=""apnamodel.best.hdf5""
checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')
callbacks_list = [checkpoint]

earlystop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=3, verbose=0, mode='auto')
callbacks_list = [earlystop]

history=model.fit(train_data, label_train_tranformed, epochs=5, batch_size=1000, validation_data=(val_data, label_val_tranformed), callbacks=callbacks_list)
score = model.evaluate(test_data, label_test_tranformed, batch_size=100)
print(""score1 :""+ str(score[0]))
print(""score2 :""+ str(score[0]))

model = Sequential() 
model.add(Conv2D(32, 2, strides=1, activation='relu', input_shape = input_shape)) 
model.add(MaxPooling2D(pool_size=2))
model.add(Flatten())
model.add(Dense(50, activation='relu'))
model.add(Dense(10, activation= ""softmax""))
model.load_weights(""apnamodel.best.hdf5"")
nadam  =  optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)
model.compile(loss='categorical_crossentropy', optimizer=nadam, metrics=['accuracy'])
scores = model.evaluate(val_data, label_val_tranformed)
print (""Scores ""+str(scores))",,[],[],[],0,0
350,keras,8555,closed,File path check for /examples/deep_dream.py,"Running the script with  checks the output result prefix only at the end. It'd be appropriate to do a simple check for the validity of the paths in the beginning because otherwise, any error would only come up after all the processing is done.

<img width=""928"" alt=""screen shot 2017-11-22 at 10 38 20 am"" src=""https://user-images.githubusercontent.com/20726434/33110940-4fa3615e-cf71-11e7-8c21-9df69e2c82c3.png"">
",,"[""may be you should check the file 'image.png' is exist or not"", '@x19990416 the file gets created by the code, and the mistake I did was naming my path wrong. It works fine otherwise. What I meant was to have a check of the validity of the paths given as arguments before doing all the processing. That would save a re-run of the entire code just because the output path has been written wrong. ']",[],['python deep_dream.py <inputpath> <result_prefix>'],0,0
351,keras,13573,closed,"LSTM Error when checking target: expected to have 2 dimensions, but got array with 3D shape","I am working on an LSTM project. 

Let's I have 100 input data. My input layer uses a slinding windows equal to 10.

Here's my LSTM model : 

    inputs = tf.keras.Input(shape=(10,100))
    LSTM_1 = layers.LSTM(100, stateful=False, return_sequences=True)(inputs)
    FC_1 = layers.Dense(100)(LSTM_1)
    LSTM_2 = layers.LSTM(100, stateful=False, return_sequences=False)(FC_1)
    label = layers.Dense(5, activation='softmax')(LSTM_2)

Here's the model summary.

    Layer (type)                 Output Shape              Param #   
    =================================================================
    input (InputLayer)        (None, 10, 100)           0         
    _________________________________________________________________
    lstm_1 (LSTM)             (None, 10, 100)           80400     
    _________________________________________________________________
    dense (Dense)             (None, 10, 100)           10100     
    _________________________________________________________________
    lstm_2 (LSTM)             (None, 100)               80400     
    _________________________________________________________________
    label (Dense)             (None, 5)                1111      
    =================================================================
    Total params: 172,011
    Trainable params: 172,011
    Non-trainable params: 0
    _________________________________________________________________
    None

Below is the code to load data.

    h5_x = 'X.h5'
    h5_Y = 'Y.h5' 
    
    x_data = HDF5Matrix(h5_x, start=0, end=data_length)
    x_data = np.reshape(x_data,(data_length, 5))       
    
    y_label = HDF5Matrix(h5_Y, start=0, end=data_length) 
    y_label = np.reshape(y_label,(data_length, 5))

And let's say I have a function that splits data to sequences of 10 and feed them to the model.

My question is when I set ***return_sequences*** to ***True***, my code works fine feeding the model with sequences of 10 slices but the output layer is of a shape **(None, 10, 5)** which I don't want it this way. 

So when I set it to **False**, the output layer is of a shape **(None, 5)**, but I get the error message : Error when checking target: expected to have 2 dimensions, but got array with shape (x, x, x). 

I know it's just a shape problem but how can I solve this ? What do I need to reshape ?
",type:support,[],[],[],0,0
352,keras,4278,closed,How to pass callbacks to scikit_learn wrappers (e.g. KerasClassifier),"I want to use  and  callbacks with the  scikit_learn wrapper. Normally, when not using scikit_learn wrappers, I pass the callbacks to the fit function [as outlined in the documentation](https://keras.io/callbacks/#usage-of-callbacks). However, when using scikit_learn wrappers, this function is a method of . The documentation mentions that [ can contain arguments to the the fit method](https://keras.io/scikit-learn-api/#wrappers-for-the-scikit-learn-api) (among others) but I am unable to figure out how to use  to pass callbacks to the fit function inside the  class.

My code looks like this (excluding code for loading data into  and  for brevity):



Does anyone know how I should use  and  with this setup?

",,"[""I believe it's like this\n\n`classifier = KerasClassifier(build_fn=DNN, nb_epoch=32, batch_size=8, callbacks=[your_callback], verbose=1)`\n\naccording to [documentation](https://keras.io/scikit-learn-api/)\n"", ""When I attempted adding the kwarg to the KerasClassifier during a grid search as @Hudler instructs, it resulted in the error documented here #4081 . I have no idea how to solve this from within Keras, but I was able pass the callback to the fit_params kwarg of GridSearchCV.  It looks like cross_val_score also accepts the fit_params kwarg. \n\n@kaspermarstal Can you try changing this: \n`results = cross_val_score(classifier, x, encoded_y, cv=kfold, verbose=1)` \ninto this: \n`results = cross_val_score(classifier, x, encoded_y, cv=kfold, verbose=1, fit_params={'callbacks': [EarlyStopping(), TensorBoard()]})`\n\nn.b. Don't forget to add your own parameters for both EarlyStopping and TensorBoard unless you are using the defaults.\n"", '@jfr311 Sadly there is another problem. This creates one instance of the Callback which is passed to all fit() calls. I tried using ModelCheckpoint in GridSearchCV and I can see that in first epoch model is not saved because it compares its error against error reached in previous split. \n', ""Hmm, it seems the best solution overall would be to edit the KerasClassifier such that its constructor sets the parameter 'callbacks' so that each fit call receives it's own instance of Callback, but this may introduce other problems or may not produce the expected results due to its interaction with scikit-learn's clone method.\n\nIn the mean time, I suppose you could look into the ModelCheckpoint code and override the method 'on_train_begin' or 'on_train_end' to ensure a new file is created or that the previous results are remove from the instance after being saved to file. There would have to be some logic for creating new file names as to not overwrite the previous results.\n"", 'Thank you for your inputs! @jr311 passing callbacks for fit_params of GridSearchCV indeed does the trick! However the method @Hudler suggested seems more appropriate to me and this is also perhaps the intended interface since it is in the documentation. Is the project interested in a patch @fchollet? \n', ""Well both options suffer from the same problem. You won't be able to make use of callback in cv in most cases, unless you tweak your keras.\n"", ""I'm simply using the first listed method in the documentation: \n\n> Values passed to the dictionary arguments of fit, predict, predict_proba, and score methods\n\nGridSearchCV's fit_params argument is used to pass a dictionary to the fit method of the base estimator, the KerasClassifier in this case.\n\nUpon further investigation it looks like when the callback is passed to sk_params and then the estimator is cloned by GridSearchCV, two different instances of the callback are created. Thus the sanity check in clone fails when comparing that the original callback instance and the cloned callback instance (line 117 in sklearn\\base.py).\n\nIn an attempt to allow callbacks to be passed to sk_params. I added callbacks to the constructor of the BaseWrapper as so:\n\n`def __init__(self, build_fn=None, callbacks=[], **sk_params):`\n  `self.build_fn = build_fn`\n  `self.callbacks = callbacks`\n  `self.sk_params = sk_params`\n  `self.sk_params['callbacks'] = callbacks\n`   self.check_params(sk_params)`\n\nThen I added self.callbacks to the get_params method of the same class:\n\n`def get_params(self, deep=True):`\n  `res = copy.deepcopy(self.sk_params)`\n  `res.update({'build_fn': self.build_fn, 'callbacks': self.callbacks})`\n  `return res`\n\nAfter testing with my model using callbacks and not using callbacks, no more errors arise and my callbacks function as intended. I don't think this deserves a PR simply because it is already possible to pass in callbacks and the unexpected behavior of ModelCheckpoint is due to that callback's implementation.\n\n@Hudler, I believe this change allows different instances of ModelCheckpoint to be passed to each fit call, but since each instance would have the same file name for saving the checkpoints, it may end up overwriting the file at the begin of the following fit call.\n"", 'Does someone solve this in order to `EarlyStopping` (and possibly other callbacks) work with `GridSearchCV` with cross-validation? I think that the solution would be to somehow compute the `val_loss` on test data from cv.   So `GridSearchCV` must pass test data from cv to `model.fit(validation_data=(X_test, y_test))`. I am just looking at `GridSearchCV` source and it\'s horrible (functional style there is very chaotic for me) :smile:\r\n\r\nEDIT: So I did some research and here is the possible solution. All the magic seems to be there:\r\nhttps://github.com/scikit-learn/scikit-learn/blob/master/sklearn/model_selection/_search.py#L557\r\n\r\n```\r\nout = Parallel(\r\n            n_jobs=self.n_jobs, verbose=self.verbose,\r\n            pre_dispatch=pre_dispatch\r\n        )(delayed(_fit_and_score)(clone(base_estimator), X, y, self.scorer_,\r\n                                  train, test, self.verbose, parameters,\r\n                                  fit_params=self.fit_params,\r\n                                  return_train_score=self.return_train_score,\r\n                                  return_n_test_samples=True,\r\n                                  return_times=True, return_parameters=True,\r\n                                  error_score=self.error_score)\r\n          for parameters in parameter_iterable\r\n          for train, test in cv_iter)\r\n```\r\n\r\n:see_no_evil: This is the code doing the actual fitting of parameters on train-test split from cv.\r\n`parameters` pass the fitting params to our Keras model (both params for `build_fn` defined by you and for actual `fit()` method like `nb_epoch`). So you can also pass validation data there! Just change `parameters` to\r\n\r\n`dict(parameters, **{""validation_data"": (X[test], y[test])})`\r\n\r\nI am using SciPy\'s sparse matrices for RNN so I must also convert them to NumPy arrays (for me, `X` is NumPy array of SciPy`s sparse matrices):\r\n\r\n`dict(parameters, **{""validation_data"": (np.array([x.toarray() for x in X[test]]), y[test])})`\r\n\r\nHope it helps!', 'Also, some methods in scikit-learn do not implement the `fit_params` attribute, for example `learning_curve` ', 'I gave the callback to the gridsearch fit_params parameter which passes the callback to the fit function of the model:\r\n\r\n`filepath = ""weights-improvement-{epoch:02d}-{acc:.2f}.hdf5""`\r\n`checkpoint = ModelCheckpoint(filepath, monitor=\'acc\', verbose=1, save_best_only=True, mode=\'max\')`\r\n`grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=kfold, fit_params=dict(callbacks=callbacks_list))`\r\n\r\nand it saves the weights from the checkpoint without any problem.\r\nCan anyone else verify that it works ?', ""Hi @anselal, \r\nyour solution works for me but I had to use as monitor the 'loss' value instead of the 'val_loss' one."", '@paolof89 Great. The monitor parameter depends on your problem. For me it was a classification problem so I was monitoring the accuracy.', 'HI @anselal, I have tried your solution and it gave me `ValueError: fit_params is not a legal parameter`. Ended up seeing in the [code](https://github.com/scikit-learn/scikit-learn/blob/02a31f0a891bdf7edc8fb087e11f74ad7a597b8a/sklearn/model_selection/_search.py#L808)\r\n\r\n```\r\n    fit_params : dict, optional\r\n        Parameters to pass to the fit method.\r\n        .. deprecated:: 0.19\r\n           ``fit_params`` as a constructor argument was deprecated in version\r\n           0.19 and will be removed in version 0.21. Pass fit parameters to\r\n           the ``fit`` method instead.\r\n```', '@edumucelli It seems that you are using scikit version 0.19 which is still in development. The `fit_params` parameter will be indeed deprecated. I was using version 0.18. You can either downgrade your scikit version to 0.18 or you can pass the `callback_list` directly to the `fit` method as indicated in the documentation\r\n\r\n> fit_params : dict, optional\r\n        Parameters to pass to the fit method.\r\n        .. deprecated:: 0.19\r\n           ``fit_params`` as a constructor argument was deprecated in version\r\n           0.19 and will be removed in version 0.21. Pass fit parameters to\r\n           the ``fit`` method instead.', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', 'Anyone ??', 'Works nicely with Scikit-learn 0.19 and Keras 2.0.5 with TF backend.\r\nThanks for the report anselal', 'I am glad I could help you out guys !!!', 'Hmmm what about FeatureUnions, though?\r\nI\'ve had this discussion here, about not using fit_params to send anything ""downstream"":\r\nhttps://github.com/scikit-learn/scikit-learn/issues/2034 (it\'s pretty lengthy, scroll down to see the relevant part).\r\n\r\nFrom what I can tell, the situation is as follows: As described in this thread, I can\'t send callbacks through the constructor, as then the object can\'t be cloned. However, I can\'t send callbacks through fit_params either (in case of pipelines with feature_unions) due to what was written in the link above.\r\n\r\nCan someone confirm my observation (or provide a refutation or an alternative)? :)', ""@jfr311 Thanks for providing such a detailed explanation, However even after modifying base wrapper class, still I am getting an error. The error which I have received is as follows:-\r\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'int'\r\nCan anybody confirm if they have received it?\r\n\r\nThanks in advance!"", ""I have sort of the same issue (caused by the same reasons). \r\nI have been trying to plot the learning_curves of a CNN (which never accepted the fit_params argument in the first place), with a varying number of epochs - depending on the # of training examples (which is important, as training the same # epochs with 1/5 of the data will greatly overfit).\r\n\r\nI am trying to use EarlyStopping as a callback, but I get the following error:\r\n```\r\nmodel = KerasClassifier(build_fn=create_model , batch_size=batch_size, epochs=epochs, verbose=1, callbacks = [EarlyStopping(patience=0)] )\r\nlearning_curve(model, ...)\r\n>>> Cannot clone object <keras.wrappers.scikit_learn.KerasClassifier object at 0x7f129fae8390>, as the constructor does not seem to set parameter callbacks\r\n```\r\n\r\nWhat confuses me is that from the documentation of the [wrapper](https://keras.io/scikit-learn-api/), I can see that the params should be passed into the .fit method, so I can't see why it's not working.\r\n\r\n@anselal I am not sure if I understood your solution correctly, but how can you pass directly arguments in the fit method without using 'fit_params' or passing them in the `KerasClassifier`?\r\n\r\nThanks!"", '@littlewine it is been a year since I suggested this solution and the APIs have changed since then.\r\n\r\nAs I quoted  `fit_params` is deprecated since scikit v0.19 and will be removed in version 0.21. Pass fit parameters to the ``fit`` method instead.\r\n\r\nI was using Gridsearch and version 0.18 of scikit so I passed it to `fit_params`.\r\n\r\nthe KerasClassifier wrapper suggests that you pass it to `**sk_params`\r\n\r\nFrom the wrapper documentation:\r\n\r\n> Values passed to the **dictionary** arguments of fit, predict, predict_proba, and score methods\r\n\r\nSo fit=dict(callbacks=callbacks_list)\r\nSo I guess you can try something like this:\r\n\r\n`model = KerasClassifier(build_fn=create_model , batch_size=batch_size, epochs=epochs, verbose=1, fit = dict(callbacks=[EarlyStopping(patience=0)]) )`', ""Hi,\r\n\r\n@littlewine try to update your Keras module. your method worked for me. Example below:\r\n\r\n`model = KerasRegressor(build_fn=build_model, epochs=500, verbose=0, callbacks=[EarlyStopping(monitor='val_loss', patience=20)])`\r\n\r\nIf I understand correctly, in the [Source code](https://github.com/keras-team/keras/blob/88af7d0c97497b5c3a198ee9416b2accfbc72c36/keras/wrappers/scikit_learn.py#L43), it states that you can pass the arguments either directly to the `fit`, `predict`, `predict_proba`, and `score` methods or to the `KerasClassifier` / `KerasRegressor` constructor.\r\n\r\nHope it helps."", 'If we pass checkpoint as callback to gridSearchCV.fit function, then we can save the accuracy/loss only on the training set and not for the validation sets as is possible in keras Sequential model.fit.\r\nTypically we are more concerned with the monitoring of validation set, hence this is not very useful. Any alternatives?', ""Struggling with this as well. The following syntax works for me:\r\n\r\n```python\r\nearly_stopper= EarlyStopping(monitor='loss')\r\nestimator= KerasClassifier(build_fn=build_model, epochs=500, batch_size=10, verbose=0)\r\nresults= cross_val_score(estimator, x, y, cv=foldsList, n_jobs=-1,\r\n                         fit_params={'callbacks': [early_stopper]})\r\n```"", ""@kakoli I also have the same problem. My current workaround is: with some sacrifices on the training set, you can set the `validation_split` when fitting the model. In that case, you will make it to internally split the training set into train and validation and get the validation loss back into your callbacks. \r\n\r\nHere is the code:\r\n```python\r\nmodel = KerasClassifier(build_fn=create_model, epochs=100, verbose=1, validation_split=.2)\r\ngrid_search = GridSearchCV(model, para_grid, n_jobs=-1, cv=5, refit='accuracy')\r\ngrid_search.fit(X, y, callbacks=EarlyStopping(monitor='val_loss', patience=3))\r\n```""]","['\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout\r\nfrom keras.wrappers.scikit_learn import KerasClassifier\r\nfrom keras.layers import BatchNormalization\r\nfrom keras.callbacks import EarlyStopping, TensorBoard\r\n\r\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\r\n\r\n# model architecture\r\ndef DNN():\r\n    model = Sequential()\r\n    model.add(Dense(512, input_dim=x.shape[1], init=\'normal\', activation=\'relu\'))\r\n    model.add(BatchNormalization())\r\n    model.add(Dropout(0.5))\r\n    model.add(Dense(32, init=\'normal\', activation=\'relu\'))\r\n    model.add(BatchNormalization())\r\n    model.add(Dropout(0.5))\r\n    model.add(Dense(1, init=\'normal\', activation=\'sigmoid\'))\r\n    model.compile(loss=\'binary_crossentropy\', optimizer=\'adagrad\', metrics=[\'accuracy\'])\r\n    return model\r\n\r\n# fix random seed for reproducibility\r\nseed = 8\r\n\r\nclassifier = KerasClassifier(build_fn=DNN, nb_epoch=32, batch_size=8, verbose=1)\r\nkfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=seed)\r\nresults = cross_val_score(classifier, x, encoded_y, cv=kfold, verbose=1)\r\nprint(""Result: %.2f%% (%.2f%%)"" % (results.mean()*100, results.std()*100))\r\n']","['EarlyStopping', 'TensorBoard', 'KerasClassifier', 'KerasClassifier', 'sk_params', 'sk_params', 'KerasClassifier', 'x', 'encoded_y', 'EarlyStopping', 'TensorBoard']",0,0
353,keras,12777,closed,"How to get the ""None"" dimension of a tensor in the run time from a keras model?","I have a different set of ROI boxes for each of my input images. My batch_size=1 and I defined my **input images as (none, none, 3)** and **input boxes as (none,4)** to handle variations, but for ROI pooling layer I need to know how many ROI boxes should get extracted. **I want to know the first dimension in (none,4) in run time to pass it to my ROI pooling layer**. K.shape and K.int_shape are not working as one of them get the tensor and the other None.

![model1](https://user-images.githubusercontent.com/30056321/57056497-f1c70600-6c70-11e9-92ac-84e971a45063.png)
",,[],"[""\r\nin_img = Input(shape=(None, None, 3))\r\nin_roi = Input(shape=(None, 4)) \r\n\r\n# Block 1\r\nx = Conv2D(64, (5, 5), activation='relu', padding='same', name='block1_conv1')(in_img)\r\nx = Conv2D(64, (5, 5), activation='relu', padding='same', name='block1_conv2')(x)\r\nx = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\r\n\r\n# Block 2\r\nx = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\r\nx = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\r\nx = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\r\n\r\n\r\nout_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([x, in_roi])\r\n\r\nout = TimeDistributed(Flatten(name='flatten'))(out_roi_pool)\r\nout = TimeDistributed(Dense(2048, activation='relu', name='fc1'))(out)\r\nout = TimeDistributed(Dropout(.4 , name ='dropout'))(out)\r\nout = TimeDistributed(Dense(1024, activation='relu', name='fc2'))(out)\r\n\r\nout_class = TimeDistributed(Dense(nb_classes, activation='softmax', kernel_initializer='zero'), name='dense_class_{}'.format(nb_classes))(out)\r\nmodel = Model(inputs=[in_img,in_roi], outputs=out_class)\r\n""]",[],0,0
354,keras,9786,closed,Can we use existing layers in our custom keras layers,"Is there any easy interface where we can use Dense, Conv2d etc in our custom layer classes along with other trainable variables. This is something that can be easily achieved in pytorch.
",,['U could maybe use Wrapper on top of it.\r\n'],[],[],0,0
355,keras,8130,closed,how to use fit_generator with multiple image inputs ,"Hello, I'm trying to use a model with paired input images through (in their own similar directory trees), augmented through **ImageDataGenerator** using also **flow_from_directory** (so the method infers the labels by the folder structure). I'm getting an error because keras can't handle it in this way.

How can I combine the generators (using **flow_from_directory**) to be accepted by fit_generator?

Here is a sample code


Model definition ***






The error I get is the following:

TypeError: Error when checking model input: data should be a Numpy array, or list/dict of Numpy arrays. Found: <keras.preprocessing.image.DirectoryIterator object at 0x7f824c5080f0>...


",,"['[https://github.com/fchollet/keras/issues/3386](url)\r\nthis might help.\r\nI think you need a generator that yields something of the form ([x1, x2], y). ', ""Thanks. Here is how I solved it following some ideas from *issue 3386*, maybe someone might find it useful\r\n\r\n```python\r\n\r\n\r\ninput_imgen = ImageDataGenerator(rescale = 1./255, \r\n                                   shear_range = 0.2, \r\n                                   zoom_range = 0.2,\r\n                                   rotation_range=5.,\r\n                                   horizontal_flip = True)\r\n\r\ntest_imgen = ImageDataGenerator(rescale = 1./255)\r\n\r\n\r\n\r\ndef generate_generator_multiple(generator,dir1, dir2, batch_size, img_height,img_width):\r\n    genX1 = generator.flow_from_directory(dir1,\r\n                                          target_size = (img_height,img_width),\r\n                                          class_mode = 'categorical',\r\n                                          batch_size = batch_size,\r\n                                          shuffle=False, \r\n                                          seed=7)\r\n    \r\n    genX2 = generator.flow_from_directory(dir2,\r\n                                          target_size = (img_height,img_width),\r\n                                          class_mode = 'categorical',\r\n                                          batch_size = batch_size,\r\n                                          shuffle=False, \r\n                                          seed=7)\r\n    while True:\r\n            X1i = genX1.next()\r\n            X2i = genX2.next()\r\n            yield [X1i[0], X2i[0]], X2i[1]  #Yield both images and their mutual label\r\n            \r\n            \r\ninputgenerator=generate_generator_multiple(generator=input_imgen,\r\n                                           dir1=train_dir_1,\r\n                                           dir2=train_dir_2,\r\n                                           batch_size=batch_size,\r\n                                           img_height=img_height,\r\n                                           img_width=img_height)       \r\n     \r\ntestgenerator=generate_generator_multiple(test_imgen,\r\n                                          dir1=train_dir_1,\r\n                                          dir2=train_dir_2,\r\n                                          batch_size=batch_size,\r\n                                          img_height=img_height,\r\n                                          img_width=img_height)              \r\n          \r\n history=model.fit_generator(inputgenerator,\r\n                        steps_per_epoch=trainsetsize/batch_size,\r\n                        epochs = epochs,\r\n                        validation_data = testgenerator,\r\n                        validation_steps = testsetsize/batch_size,\r\n                        use_multiprocessing=True,\r\n                        shuffle=False)"", ""That was great. How would you find the class_indices for the inputgenerator and testgenerator? I'm not able to use this syntax: inputgenerator.class_indices() here since now, the generator works with multiple inputs."", '@aendrs I would like to ask how to give the two inputs same pre-processing everytime? As in the function of input_generator,  input_imgen is called twice, for each time randomly selected preprocessing steps are applied on the input.', '@aendrs @raaju-shiv @wangqianwen0418 @laukun @bmabey Could anyone of you guys kindly help me solve this problem https://github.com/keras-team/keras/issues/10499. I tried implementing the same generator as in this post but i dont seem  to figure out where is my mistake. any help is very much appreciated.', '@laukun I would suggest write your own data generator like in this example:\r\n```python\r\nclass DataGenerator(keras.utils.Sequence):\r\n    """"""Generates data for Keras.""""""\r\n    def __init__(self, img_files, clinical_info, labels, ave=None, std=None, batch_size=32, dim=(300, 300), n_channels=3,\r\n                 n_classes=2, shuffle=True):\r\n        """"""Initialization.\r\n        \r\n        Args:\r\n            img_files: A list of path to image files.\r\n            clinical_info: A dictionary of corresponding clinical variables.\r\n            labels: A dictionary of corresponding labels.\r\n        """"""\r\n        self.img_files = img_files\r\n        self.clinical_info = clinical_info\r\n        self.labels = labels\r\n        self.batch_size = batch_size\r\n        self.dim = dim\r\n        if ave is None:\r\n            self.ave = np.zeros(n_channels)\r\n        else:\r\n            self.ave = ave\r\n        if std is None:\r\n            self.std = np.zeros(n_channels) + 1\r\n        else:\r\n            self.std = std\r\n        \r\n        self.n_channels = n_channels\r\n        self.n_classes = n_classes\r\n        self.shuffle = shuffle\r\n        self.on_epoch_end()\r\n\r\n    def __len__(self):\r\n        """"""Denotes the number of batches per epoch.""""""\r\n        return int(np.floor(len(self.img_files) / self.batch_size))\r\n\r\n    def __getitem__(self, index):\r\n        """"""Generate one batch of data.""""""\r\n        # Generate indexes of the batch\r\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\r\n\r\n        # Find list of IDs\r\n        img_files_temp = [self.img_files[k] for k in indexes]\r\n\r\n        # Generate data\r\n        X, y = self.__data_generation(img_files_temp)\r\n\r\n        return X, y\r\n\r\n    def on_epoch_end(self):\r\n        """"""Updates indexes after each epoch.""""""\r\n        self.indexes = np.arange(len(self.img_files))\r\n        if self.shuffle == True:\r\n            np.random.shuffle(self.indexes)\r\n\r\n    def __data_generation(self, img_files_temp):\r\n        """"""Generates data containing batch_size samples.""""""\r\n        # X : (n_samples, *dim, n_channels)\r\n        # X = [np.empty((self.batch_size, self.dim[0], self.dim[1], self.n_channels))]\r\n        X_img = []\r\n        X_clinical = []\r\n        y = np.empty((self.batch_size), dtype=int)\r\n\r\n        # Generate data\r\n        for i, img_file in enumerate(img_files_temp):\r\n            # Read image\r\n            img = skimage.io.imread(img_file)\r\n            \r\n            # Resize\r\n            img = skimage.transform.resize(img, output_shape=self.dim, mode=\'constant\', preserve_range=True)\r\n            \r\n            # Normalization\r\n            for ch in range(self.n_channels):\r\n                img[:, :, ch] = (img[:, :, ch] - self.ave[ch])/self.std[ch]\r\n            \r\n            if self.shuffle:\r\n                # Some image augmentation codes\r\n                ###### You can put your preprocessing codes here. #####\r\n\r\n            X_img.append(img)\r\n            X_clinical.append(self.clinical_info[img_file])\r\n            y[i] = self.labels[img_file]\r\n        X = [np.array(X_img), np.array(X_clinical)]\r\n        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)\r\n```\r\n\r\nAnd to call the generator:\r\n\r\n```\r\ntrain_datagen = DataGenerator(img_files=img_files, clinical_info=clinical_info, labels=labels, ave=ave, std=std, batch_size=batch_size)\r\nval_datagen = DataGenerator(img_files=img_files, clinical_info=clinical_info, labels=labels, ave=ave, std=std, batch_size=batch_size)\r\nhist = model.fit_generator(train_datagen, \r\n                           steps_per_epoch=len(img_files) / batch_size, \r\n                           epochs=num_epochs, \r\n                           verbose=1, \r\n                           validation_data=val_datagen, \r\n                           validation_steps=1)\r\n```\r\n\r\nThe different thing I\'m doing here is using two different kinds of input, an image and a numpy array. I saved image paths as a list, and created two dictionaries whose key words are the image paths. When you want to do the preprocessing, you can easily apply them right after reading the image using the image paths.\r\n\r\nBut this way, the speed will be definitely slower than loading data directly from .npy', 'I\'m trying to train a two streams CNN architecture, but unfortunately I got an unexpected result whitch is almost the same as one of the two streams (I have already trained every one separately):\r\nhere is my code, please tell me what I\'m doing wrong :\r\n@aendrs @sdw95927  @wangkechn \r\n`\r\n## Arch\r\n    def fusionNet(nb_classes, inputs=(3, 224, 224)):\r\n        input_img = Input(shape=inputs, name=\'RGB_input\')\r\n        conv1 = Conv2D(96, (7, 7), activation=""relu"", name=""conv1"", strides=(2, 2), padding=""same"", \r\n        ## ....\r\n        ## ....\r\n        ## ....\r\n        firstCNN_output = BatchNormalization(name=\'firstCNN_output_BN\')(last_dropout)\r\n        ##\r\n        print(firstCNN_output._keras_shape) # I got here (None, 512, 13, 13)\r\n\r\n\r\n\r\n        input_img2 = Input(shape=inputs, name=\'depth_input\')\r\n        conv12 = Conv2D(96, (7, 7), activation=""relu"", name=""conv12"", strides=(2, 2), padding=""same"",             kernel_initializer=""glorot_uniform"")(input_img2)\r\n       #....\r\n       #....\r\n       #...\r\n    ## BN\r\n        SecondCNN_output = BatchNormalization(name=\'SecondCNN_output_BN2\')(last_dropout2)\r\n        print(SecondCNN_output._keras_shape) # I got here (None, 512, 13, 13)\r\n        #     \r\n\r\n        merge2CNN = merge([firstCNN_output, SecondCNN_output], mode=\'concat\', concat_axis=1)   \r\n        ### got here (None, 1024, 13, 13) \r\n        conv10 = Conv2D(\r\n        nb_classes, (1, 1),init=\'glorot_uniform\',\r\n        border_mode=\'valid\', name=\'conv10\')(merge2CNN)\r\n       avgpool10 = AveragePooling2D((13, 13), name=\'avgpool10\')(conv10)\r\n\r\n        flatten = Flatten(name=\'flatten\')(avgpool10)\r\n       softmax = Activation(""softmax"", name=\'softmax\')(flatten)\r\n\r\n        return Model(inputs=[input_img,input_img2], output=softmax)\r\n`\r\n## Training\r\n    sn = fusionNet(nb_classes=24)\r\n    sgd = SGD(lr=0.001, decay=0.0002, momentum=0.9, nesterov=True)\r\n\r\n    sn.compile( optimizer=sgd, loss=\'categorical_crossentropy\', metrics=[\'accuracy\')\r\n\r\n    train_datagen = ImageDataGenerator(\r\n        rescale=1./255,\r\n        zoom_range=0.01,\r\n        rotation_range=2.,\r\n        featurewise_center=True,\r\n        featurewise_std_normalization=True,\r\n    )\r\n    #train_datagen = ImageDataGenerator(rescale=1./255)\r\n\r\n    test_datagen = ImageDataGenerator(rescale=1./255)\r\n\r\n\r\n    def generate_generator_multiple(generator,dir1, dir2, batch_size, img_height,img_width):\r\n    genX1 = generator.flow_from_directory(dir1,\r\n                                      target_size = (img_height,img_width),\r\n                                      class_mode = \'categorical\',\r\n                                      batch_size = batch_size,\r\n                                      shuffle=False)\r\n\r\n    genX2 = generator.flow_from_directory(dir2,\r\n                                      target_size = (img_height, img_width),\r\n                                      class_mode = \'categorical\',\r\n                                      batch_size = batch_size)\r\n    while True:\r\n        X1i = genX1.next()\r\n        X2i = genX2.next()\r\n        yield [X1i[0], X2i[0]], X2i[1]  #Yield both images and their mutual label\r\n        \r\n        \r\n    train_generator=generate_generator_multiple(generator=train_datagen,\r\n                                       dir1=train_data_dir,\r\n                                       dir2=train_data_dir_colored,\r\n                                       batch_size=32,\r\n                                       img_height=size,\r\n                                       img_width=size)       \r\n \r\n    validation_generator=generate_generator_multiple(test_datagen,\r\n                                      dir1=validation_data_dir,\r\n                                      dir2=validation_data_dir_colored,\r\n                                      batch_size=batch_size,\r\n                                      img_height=size,\r\n                                      img_width=size)              \r\n\r\n\r\n\r\n    tensorboardcb = keras.callbacks.TensorBoard(log_dir=\'/workspace/sqn_224/fusion\', histogram_freq=0,         batch_size=32, write_graph=True)\r\n    sn.fit_generator(\r\n            train_generator,\r\n        steps_per_epoch=nb_train_samples//32,\r\n        nb_epoch=nb_epoch,\r\n        validation_data=validation_generator,\r\n        validation_steps = nb_validation_samples//32, \r\n        callbacks=[tensorboardcb])', '@scstu I would like to merge the input data first, which might be a better idea.', '@scstu that can be done in different ways. but keep in mind the number of learned parameters differ depending on the way you merge the input images. so if youre merging in a multispectral-like shape (for 2xRGB images you can merge in two ways as far as i know. so it would be something like: HxWx6 presuming that your concatenating the images, or it could be HxWx3 if youre finding a transformation of the 2 RGB images such as multiplication, add, sub etc..) in either case be careful with the input size for your model and remember that the number of the learned parameters will differ. to know whats better you can only try and compare the results. it also will affect the extracted feature by the convolution differently. i worked on a similar problem and i used multiple branch CNN and 2 input images where each images passes a branch from the CNN and then the features extracted by the two CNN branches are merged in a merging layer before passing it to the top layers.', '@aendrs thank you very much on your algorithm. He is fantastic.', '@aendrs it was really good work to generate multi-streams input.  thanks\r\n', '@aendrs could you please share your portion of code for concatenation and inputs manipulation\r\n\r\n', 'I have a doubt. I will train input sets on the same network, for example, model1 receives input X1 (three folders containing classes and each class has the training, validation, and test data) and model2 receives input X2 (three folders containing three classes and each class has the training, validation and test data). Then I will concatenate a convolution of model X1 with one of model X2. So I have two input data, two validation data and two test data. My question is about the following command: steps_per_epoch = nb_train_samples // batchsize. I would like to know if my nb_train_samples is the sum of only the training_class1_X1 + training_class2_X1 + training_class3_X1 or if nb_train_samples is the sum of (training_class1_X1 + training_class2_X1 + training_class3_X1) + (training_class1_X2 + training_class2_X2 + training_class3_X2).', '> @scstu that can be done in different ways. but keep in mind the number of learned parameters differ depending on the way you merge the input images. so if youre merging in a multispectral-like shape (for 2xRGB images you can merge in two ways as far as i know. so it would be something like: HxWx6 presuming that your concatenating the images, or it could be HxWx3 if youre finding a transformation of the 2 RGB images such as multiplication, add, sub etc..) in either case be careful with the input size for your model and remember that the number of the learned parameters will differ. to know whats better you can only try and compare the results. it also will affect the extracted feature by the convolution differently. i worked on a similar problem and i used multiple branch CNN and 2 input images where each images passes a branch from the CNN and then the features extracted by the two CNN branches are merged in a merging layer before passing it to the top layers.\r\n\r\n@MjdMahasneh I believe your comment is interesting. Could you help me with a doubt? I will train input sets on the same network, for example, model1 receives input X1 (three folders containing classes and each class has the training, validation, and test data) and model2 receives input X2 (three folders containing three classes and each class has the training, validation and test data). Then I will concatenate a convolution of model X1 with one of model X2. So I have two input data, two validation data and two test data. My question is about the following command: steps_per_epoch = nb_train_samples // batchsize. I would like to know if my nb_train_samples is the sum of only the training_class1_X1 + training_class2_X1 + training_class3_X1 or if nb_train_samples is the sum of (training_class1_X1 + training_class2_X1 + training_class3_X1) + (training_class1_X2 + training_class2_X2 + training_class3_X2).', '> I\'m trying to train a two streams CNN architecture, but unfortunately I got an unexpected result whitch is almost the same as one of the two streams (I have already trained every one separately):\r\n> here is my code, please tell me what I\'m doing wrong :\r\n> @aendrs @sdw95927 @wangkechn\r\n> `\r\n> \r\n> ## Arch\r\n> ```\r\n> def fusionNet(nb_classes, inputs=(3, 224, 224)):\r\n>     input_img = Input(shape=inputs, name=\'RGB_input\')\r\n>     conv1 = Conv2D(96, (7, 7), activation=""relu"", name=""conv1"", strides=(2, 2), padding=""same"", \r\n>     ## ....\r\n>     ## ....\r\n>     ## ....\r\n>     firstCNN_output = BatchNormalization(name=\'firstCNN_output_BN\')(last_dropout)\r\n>     ##\r\n>     print(firstCNN_output._keras_shape) # I got here (None, 512, 13, 13)\r\n> \r\n> \r\n> \r\n>     input_img2 = Input(shape=inputs, name=\'depth_input\')\r\n>     conv12 = Conv2D(96, (7, 7), activation=""relu"", name=""conv12"", strides=(2, 2), padding=""same"",             kernel_initializer=""glorot_uniform"")(input_img2)\r\n>    #....\r\n>    #....\r\n>    #...\r\n> ## BN\r\n>     SecondCNN_output = BatchNormalization(name=\'SecondCNN_output_BN2\')(last_dropout2)\r\n>     print(SecondCNN_output._keras_shape) # I got here (None, 512, 13, 13)\r\n>     #     \r\n> \r\n>     merge2CNN = merge([firstCNN_output, SecondCNN_output], mode=\'concat\', concat_axis=1)   \r\n>     ### got here (None, 1024, 13, 13) \r\n>     conv10 = Conv2D(\r\n>     nb_classes, (1, 1),init=\'glorot_uniform\',\r\n>     border_mode=\'valid\', name=\'conv10\')(merge2CNN)\r\n>    avgpool10 = AveragePooling2D((13, 13), name=\'avgpool10\')(conv10)\r\n> \r\n>     flatten = Flatten(name=\'flatten\')(avgpool10)\r\n>    softmax = Activation(""softmax"", name=\'softmax\')(flatten)\r\n> \r\n>     return Model(inputs=[input_img,input_img2], output=softmax)\r\n> ```\r\n> `\r\n> \r\n> ## Training\r\n> ```\r\n> sn = fusionNet(nb_classes=24)\r\n> sgd = SGD(lr=0.001, decay=0.0002, momentum=0.9, nesterov=True)\r\n> \r\n> sn.compile( optimizer=sgd, loss=\'categorical_crossentropy\', metrics=[\'accuracy\')\r\n> \r\n> train_datagen = ImageDataGenerator(\r\n>     rescale=1./255,\r\n>     zoom_range=0.01,\r\n>     rotation_range=2.,\r\n>     featurewise_center=True,\r\n>     featurewise_std_normalization=True,\r\n> )\r\n> #train_datagen = ImageDataGenerator(rescale=1./255)\r\n> \r\n> test_datagen = ImageDataGenerator(rescale=1./255)\r\n> \r\n> \r\n> def generate_generator_multiple(generator,dir1, dir2, batch_size, img_height,img_width):\r\n> genX1 = generator.flow_from_directory(dir1,\r\n>                                   target_size = (img_height,img_width),\r\n>                                   class_mode = \'categorical\',\r\n>                                   batch_size = batch_size,\r\n>                                   shuffle=False)\r\n> \r\n> genX2 = generator.flow_from_directory(dir2,\r\n>                                   target_size = (img_height, img_width),\r\n>                                   class_mode = \'categorical\',\r\n>                                   batch_size = batch_size)\r\n> while True:\r\n>     X1i = genX1.next()\r\n>     X2i = genX2.next()\r\n>     yield [X1i[0], X2i[0]], X2i[1]  #Yield both images and their mutual label\r\n>     \r\n>     \r\n> train_generator=generate_generator_multiple(generator=train_datagen,\r\n>                                    dir1=train_data_dir,\r\n>                                    dir2=train_data_dir_colored,\r\n>                                    batch_size=32,\r\n>                                    img_height=size,\r\n>                                    img_width=size)       \r\n> \r\n> validation_generator=generate_generator_multiple(test_datagen,\r\n>                                   dir1=validation_data_dir,\r\n>                                   dir2=validation_data_dir_colored,\r\n>                                   batch_size=batch_size,\r\n>                                   img_height=size,\r\n>                                   img_width=size)              \r\n> \r\n> \r\n> \r\n> tensorboardcb = keras.callbacks.TensorBoard(log_dir=\'/workspace/sqn_224/fusion\', histogram_freq=0,         batch_size=32, write_graph=True)\r\n> sn.fit_generator(\r\n>         train_generator,\r\n>     steps_per_epoch=nb_train_samples//32,\r\n>     nb_epoch=nb_epoch,\r\n>     validation_data=validation_generator,\r\n>     validation_steps = nb_validation_samples//32, \r\n>     callbacks=[tensorboardcb])\r\n> ```\r\n\r\nCould you help me with a doubt? I will train input sets on the same network, for example, model1 receives input X1 (three folders containing classes and each class has the training, validation, and test data) and model2 receives input X2 (three folders containing three classes and each class has the training, validation and test data). Then I will concatenate a convolution of model X1 with one of model X2. So I have two input data, two validation data and two test data. My question is about the following command: steps_per_epoch = nb_train_samples // batchsize. I would like to know if my nb_train_samples is the sum of only the training_class1_X1 + training_class2_X1 + training_class3_X1 or if nb_train_samples is the sum of (training_class1_X1 + training_class2_X1 + training_class3_X1) + (training_class1_X2 + training_class2_X2 + training_class3_X2).', '> > I\'m trying to train a two streams CNN architecture, but unfortunately I got an unexpected result whitch is almost the same as one of the two streams (I have already trained every one separately):\r\n> > here is my code, please tell me what I\'m doing wrong :\r\n> > @aendrs @sdw95927 @wangkechn\r\n> > `\r\n> > ## Arch\r\n> > ```\r\n> > def fusionNet(nb_classes, inputs=(3, 224, 224)):\r\n> >     input_img = Input(shape=inputs, name=\'RGB_input\')\r\n> >     conv1 = Conv2D(96, (7, 7), activation=""relu"", name=""conv1"", strides=(2, 2), padding=""same"", \r\n> >     ## ....\r\n> >     ## ....\r\n> >     ## ....\r\n> >     firstCNN_output = BatchNormalization(name=\'firstCNN_output_BN\')(last_dropout)\r\n> >     ##\r\n> >     print(firstCNN_output._keras_shape) # I got here (None, 512, 13, 13)\r\n> > \r\n> > \r\n> > \r\n> >     input_img2 = Input(shape=inputs, name=\'depth_input\')\r\n> >     conv12 = Conv2D(96, (7, 7), activation=""relu"", name=""conv12"", strides=(2, 2), padding=""same"",             kernel_initializer=""glorot_uniform"")(input_img2)\r\n> >    #....\r\n> >    #....\r\n> >    #...\r\n> > ## BN\r\n> >     SecondCNN_output = BatchNormalization(name=\'SecondCNN_output_BN2\')(last_dropout2)\r\n> >     print(SecondCNN_output._keras_shape) # I got here (None, 512, 13, 13)\r\n> >     #     \r\n> > \r\n> >     merge2CNN = merge([firstCNN_output, SecondCNN_output], mode=\'concat\', concat_axis=1)   \r\n> >     ### got here (None, 1024, 13, 13) \r\n> >     conv10 = Conv2D(\r\n> >     nb_classes, (1, 1),init=\'glorot_uniform\',\r\n> >     border_mode=\'valid\', name=\'conv10\')(merge2CNN)\r\n> >    avgpool10 = AveragePooling2D((13, 13), name=\'avgpool10\')(conv10)\r\n> > \r\n> >     flatten = Flatten(name=\'flatten\')(avgpool10)\r\n> >    softmax = Activation(""softmax"", name=\'softmax\')(flatten)\r\n> > \r\n> >     return Model(inputs=[input_img,input_img2], output=softmax)\r\n> > ```\r\n> > `\r\n> > ## Training\r\n> > ```\r\n> > sn = fusionNet(nb_classes=24)\r\n> > sgd = SGD(lr=0.001, decay=0.0002, momentum=0.9, nesterov=True)\r\n> > \r\n> > sn.compile( optimizer=sgd, loss=\'categorical_crossentropy\', metrics=[\'accuracy\')\r\n> > \r\n> > train_datagen = ImageDataGenerator(\r\n> >     rescale=1./255,\r\n> >     zoom_range=0.01,\r\n> >     rotation_range=2.,\r\n> >     featurewise_center=True,\r\n> >     featurewise_std_normalization=True,\r\n> > )\r\n> > #train_datagen = ImageDataGenerator(rescale=1./255)\r\n> > \r\n> > test_datagen = ImageDataGenerator(rescale=1./255)\r\n> > \r\n> > \r\n> > def generate_generator_multiple(generator,dir1, dir2, batch_size, img_height,img_width):\r\n> > genX1 = generator.flow_from_directory(dir1,\r\n> >                                   target_size = (img_height,img_width),\r\n> >                                   class_mode = \'categorical\',\r\n> >                                   batch_size = batch_size,\r\n> >                                   shuffle=False)\r\n> > \r\n> > genX2 = generator.flow_from_directory(dir2,\r\n> >                                   target_size = (img_height, img_width),\r\n> >                                   class_mode = \'categorical\',\r\n> >                                   batch_size = batch_size)\r\n> > while True:\r\n> >     X1i = genX1.next()\r\n> >     X2i = genX2.next()\r\n> >     yield [X1i[0], X2i[0]], X2i[1]  #Yield both images and their mutual label\r\n> >     \r\n> >     \r\n> > train_generator=generate_generator_multiple(generator=train_datagen,\r\n> >                                    dir1=train_data_dir,\r\n> >                                    dir2=train_data_dir_colored,\r\n> >                                    batch_size=32,\r\n> >                                    img_height=size,\r\n> >                                    img_width=size)       \r\n> > \r\n> > validation_generator=generate_generator_multiple(test_datagen,\r\n> >                                   dir1=validation_data_dir,\r\n> >                                   dir2=validation_data_dir_colored,\r\n> >                                   batch_size=batch_size,\r\n> >                                   img_height=size,\r\n> >                                   img_width=size)              \r\n> > \r\n> > \r\n> > \r\n> > tensorboardcb = keras.callbacks.TensorBoard(log_dir=\'/workspace/sqn_224/fusion\', histogram_freq=0,         batch_size=32, write_graph=True)\r\n> > sn.fit_generator(\r\n> >         train_generator,\r\n> >     steps_per_epoch=nb_train_samples//32,\r\n> >     nb_epoch=nb_epoch,\r\n> >     validation_data=validation_generator,\r\n> >     validation_steps = nb_validation_samples//32, \r\n> >     callbacks=[tensorboardcb])\r\n> > ```\r\n> \r\n> Could you help me with a doubt? I will train input sets on the same network, for example, model1 receives input X1 (three folders containing classes and each class has the training, validation, and test data) and model2 receives input X2 (three folders containing three classes and each class has the training, validation and test data). Then I will concatenate a convolution of model X1 with one of model X2. So I have two input data, two validation data and two test data. My question is about the following command: steps_per_epoch = nb_train_samples // batchsize. I would like to know if my nb_train_samples is the sum of only the training_class1_X1 + training_class2_X1 + training_class3_X1 or if nb_train_samples is the sum of (training_class1_X1 + training_class2_X1 + training_class3_X1) + (training_class1_X2 + training_class2_X2 + training_class3_X2).\r\n\r\nHello,\r\n\r\nDid you find an answer / solution to your question? I have the same problem / doubt.', '> > > I\'m trying to train a two streams CNN architecture, but unfortunately I got an unexpected result whitch is almost the same as one of the two streams (I have already trained every one separately):\r\n> > > here is my code, please tell me what I\'m doing wrong :\r\n> > > @aendrs @sdw95927 @wangkechn\r\n> > > `\r\n> > > ## Arch\r\n> > > ```\r\n> > > def fusionNet(nb_classes, inputs=(3, 224, 224)):\r\n> > >     input_img = Input(shape=inputs, name=\'RGB_input\')\r\n> > >     conv1 = Conv2D(96, (7, 7), activation=""relu"", name=""conv1"", strides=(2, 2), padding=""same"", \r\n> > >     ## ....\r\n> > >     ## ....\r\n> > >     ## ....\r\n> > >     firstCNN_output = BatchNormalization(name=\'firstCNN_output_BN\')(last_dropout)\r\n> > >     ##\r\n> > >     print(firstCNN_output._keras_shape) # I got here (None, 512, 13, 13)\r\n> > > \r\n> > > \r\n> > > \r\n> > >     input_img2 = Input(shape=inputs, name=\'depth_input\')\r\n> > >     conv12 = Conv2D(96, (7, 7), activation=""relu"", name=""conv12"", strides=(2, 2), padding=""same"",             kernel_initializer=""glorot_uniform"")(input_img2)\r\n> > >    #....\r\n> > >    #....\r\n> > >    #...\r\n> > > ## BN\r\n> > >     SecondCNN_output = BatchNormalization(name=\'SecondCNN_output_BN2\')(last_dropout2)\r\n> > >     print(SecondCNN_output._keras_shape) # I got here (None, 512, 13, 13)\r\n> > >     #     \r\n> > > \r\n> > >     merge2CNN = merge([firstCNN_output, SecondCNN_output], mode=\'concat\', concat_axis=1)   \r\n> > >     ### got here (None, 1024, 13, 13) \r\n> > >     conv10 = Conv2D(\r\n> > >     nb_classes, (1, 1),init=\'glorot_uniform\',\r\n> > >     border_mode=\'valid\', name=\'conv10\')(merge2CNN)\r\n> > >    avgpool10 = AveragePooling2D((13, 13), name=\'avgpool10\')(conv10)\r\n> > > \r\n> > >     flatten = Flatten(name=\'flatten\')(avgpool10)\r\n> > >    softmax = Activation(""softmax"", name=\'softmax\')(flatten)\r\n> > > \r\n> > >     return Model(inputs=[input_img,input_img2], output=softmax)\r\n> > > ```\r\n> > > `\r\n> > > ## Training\r\n> > > ```\r\n> > > sn = fusionNet(nb_classes=24)\r\n> > > sgd = SGD(lr=0.001, decay=0.0002, momentum=0.9, nesterov=True)\r\n> > > \r\n> > > sn.compile( optimizer=sgd, loss=\'categorical_crossentropy\', metrics=[\'accuracy\')\r\n> > > \r\n> > > train_datagen = ImageDataGenerator(\r\n> > >     rescale=1./255,\r\n> > >     zoom_range=0.01,\r\n> > >     rotation_range=2.,\r\n> > >     featurewise_center=True,\r\n> > >     featurewise_std_normalization=True,\r\n> > > )\r\n> > > #train_datagen = ImageDataGenerator(rescale=1./255)\r\n> > > \r\n> > > test_datagen = ImageDataGenerator(rescale=1./255)\r\n> > > \r\n> > > \r\n> > > def generate_generator_multiple(generator,dir1, dir2, batch_size, img_height,img_width):\r\n> > > genX1 = generator.flow_from_directory(dir1,\r\n> > >                                   target_size = (img_height,img_width),\r\n> > >                                   class_mode = \'categorical\',\r\n> > >                                   batch_size = batch_size,\r\n> > >                                   shuffle=False)\r\n> > > \r\n> > > genX2 = generator.flow_from_directory(dir2,\r\n> > >                                   target_size = (img_height, img_width),\r\n> > >                                   class_mode = \'categorical\',\r\n> > >                                   batch_size = batch_size)\r\n> > > while True:\r\n> > >     X1i = genX1.next()\r\n> > >     X2i = genX2.next()\r\n> > >     yield [X1i[0], X2i[0]], X2i[1]  #Yield both images and their mutual label\r\n> > >     \r\n> > >     \r\n> > > train_generator=generate_generator_multiple(generator=train_datagen,\r\n> > >                                    dir1=train_data_dir,\r\n> > >                                    dir2=train_data_dir_colored,\r\n> > >                                    batch_size=32,\r\n> > >                                    img_height=size,\r\n> > >                                    img_width=size)       \r\n> > > \r\n> > > validation_generator=generate_generator_multiple(test_datagen,\r\n> > >                                   dir1=validation_data_dir,\r\n> > >                                   dir2=validation_data_dir_colored,\r\n> > >                                   batch_size=batch_size,\r\n> > >                                   img_height=size,\r\n> > >                                   img_width=size)              \r\n> > > \r\n> > > \r\n> > > \r\n> > > tensorboardcb = keras.callbacks.TensorBoard(log_dir=\'/workspace/sqn_224/fusion\', histogram_freq=0,         batch_size=32, write_graph=True)\r\n> > > sn.fit_generator(\r\n> > >         train_generator,\r\n> > >     steps_per_epoch=nb_train_samples//32,\r\n> > >     nb_epoch=nb_epoch,\r\n> > >     validation_data=validation_generator,\r\n> > >     validation_steps = nb_validation_samples//32, \r\n> > >     callbacks=[tensorboardcb])\r\n> > > ```\r\n> > \r\n> > \r\n> > Could you help me with a doubt? I will train input sets on the same network, for example, model1 receives input X1 (three folders containing classes and each class has the training, validation, and test data) and model2 receives input X2 (three folders containing three classes and each class has the training, validation and test data). Then I will concatenate a convolution of model X1 with one of model X2. So I have two input data, two validation data and two test data. My question is about the following command: steps_per_epoch = nb_train_samples // batchsize. I would like to know if my nb_train_samples is the sum of only the training_class1_X1 + training_class2_X1 + training_class3_X1 or if nb_train_samples is the sum of (training_class1_X1 + training_class2_X1 + training_class3_X1) + (training_class1_X2 + training_class2_X2 + training_class3_X2).\r\n> \r\n> Hello,\r\n> \r\n> Did you find an answer / solution to your question? I have the same problem / doubt.\r\n\r\nI think the training size is the later one, sum of all training samples. The question is equal with:\r\n\r\nA model has three sets of parameters A, B and C. We use X1 data to train A & C with B fixed, and use X2 to train B & C with A fixed. \r\n\r\nI think now the answer is clear. ', 'I am trying similar things, but I am getting stuck with the input of the model.\r\n\r\n `  \r\n     def generate_generator_multiple(generator,dir1, dir2,dir3, batch_size, img_size):\r\n\r\n          genX1 = generator.flow_from_directory(dir1,\r\n                                          target_size = (img_size,img_size),\r\n                                          class_mode = \'categorical\',\r\n                                          batch_size = batch_size,\r\n                                          shuffle=False, \r\n                                          seed=7,subset=\'training\')\r\n\r\n          genX2 = generator.flow_from_directory(dir2,\r\n                                          target_size = (img_size,img_size),\r\n                                          class_mode = \'categorical\',\r\n                                          batch_size = batch_size,\r\n                                          shuffle=False, \r\n                                          seed=7,subset=\'training\')\r\n\r\n          genX3 = generator.flow_from_directory(dir3,\r\n                                      target_size = (img_size,img_size),\r\n                                      class_mode = \'categorical\',\r\n                                      batch_size = batch_size,\r\n                                      shuffle=False, \r\n                                      seed=7,subset=\'training\')\r\n           while True:\r\n                  X1i = genX1.next()\r\n                  X2i = genX2.next()\r\n                  X3i = genX3.next()\r\n           yield [X1i[0], X2i[0],X3i[0]], X3i[1]  #Yield both images and their mutual label\r\n\r\n    train_datagen = ImageDataGenerator(validation_split=VAL_SPLIT,rescale=1./255)\r\n    inputgenerator=generate_generator_multiple(generator=train_datagen,\r\n                                           dir1=\'mergedata\',\r\n                                           dir2=\'mergedata\',\r\n                                           dir3=\'mergedata\',\r\n                                           batch_size=BATCH_SIZE,\r\n                                           img_size=IMG_SIZE)   \r\n\r\n    modelA = load_model(\'/content/8-base_model_1.h5\')\r\n    modelA.load_weights(\'/content/8-weights.002-0.991-0.406.hdf5\') \r\n    \r\n    modelB = load_model(\'/content/30-base_model_1.h5\')\r\n    modelB.load_weights(\'/content/30-weights.010-0.499-0.374.hdf5\')\r\n    \r\n    modelC = load_model(\'/content/80-base_model_1.h5\')\r\n    modelC.load_weights(\'/content/80-weights.002-1.064-0.305.hdf5\')\r\n    for layer in modelA.layers:\r\n        layer.trainable = False\r\n        layer.name = layer.name + str(""_1"")\r\n      \r\n    for layer in modelB.layers:\r\n        layer.trainable = False\r\n        layer.name = layer.name + str(""_2"")\r\n        \r\n    for layer in modelC.layers:\r\n        layer.trainable = False\r\n        layer.name = layer.name + str(""_3"")\r\n\r\n    mod_1 = concatenate([modelA.layers[-1].output,  modelB.layers[-1].output,modelC.layers[-1].output])     \r\n    mod_1 = Dropout(0.2)(mod_1)\r\n    mod_1 = Dense(15, activation=\'relu\')(mod_1)\r\n    predictions = Dense(9, activation=\'softmax\', name=\'pred_age\')(mod_1)\r\n    \r\n    top_model = Model(inputs=[modelA.input, modelB.input, modelC.input], outputs=predictions)\r\n\r\n    opt = get_optimizer(\'adam\', lr_rate)\r\n    top_model.compile(loss=[\'categorical_crossentropy\'],\r\n              optimizer=opt,\r\n              metrics=[\'accuracy\',\'mae\'])\r\n\r\n    model.fit_generator(inputgenerator,\r\n                               epochs=EPOCHS,\r\n                               steps_per_epoch = STEPS_PER_EPOCH,\r\n                               validation_data=validgenerator,\r\n                               validation_steps = VALIDATION_STEPS,\r\n                               verbose=1,\r\n                               callbacks=callbacks)`\r\n\r\n\r\nI first trained 3 different models:\r\n\r\n- model 1 = 3 labels (8,9,10)\r\n- model 2 = 2 labels (30,31)\r\n- model 3 = 4 labels (80,81,82,83)\r\n\r\nI would like to combine these 3 models into a final model with 1 output containing 9 labels (8,9,10,30,31,80,81,82,83). The final input needs to get 1 input image instead of 3 images. But I still get stuck. I am building a kind of hierarchy here to improve the accuracy.', ""> Thanks. Here is how I solved it following some ideas from _issue 3386_, maybe someone might find it useful\r\n> \r\n> ```python\r\n> input_imgen = ImageDataGenerator(rescale = 1./255, \r\n>                                    shear_range = 0.2, \r\n>                                    zoom_range = 0.2,\r\n>                                    rotation_range=5.,\r\n>                                    horizontal_flip = True)\r\n> \r\n> test_imgen = ImageDataGenerator(rescale = 1./255)\r\n> \r\n> \r\n> \r\n> def generate_generator_multiple(generator,dir1, dir2, batch_size, img_height,img_width):\r\n>     genX1 = generator.flow_from_directory(dir1,\r\n>                                           target_size = (img_height,img_width),\r\n>                                           class_mode = 'categorical',\r\n>                                           batch_size = batch_size,\r\n>                                           shuffle=False, \r\n>                                           seed=7)\r\n>     \r\n>     genX2 = generator.flow_from_directory(dir2,\r\n>                                           target_size = (img_height,img_width),\r\n>                                           class_mode = 'categorical',\r\n>                                           batch_size = batch_size,\r\n>                                           shuffle=False, \r\n>                                           seed=7)\r\n>     while True:\r\n>             X1i = genX1.next()\r\n>             X2i = genX2.next()\r\n>             yield [X1i[0], X2i[0]], X2i[1]  #Yield both images and their mutual label\r\n>             \r\n>             \r\n> inputgenerator=generate_generator_multiple(generator=input_imgen,\r\n>                                            dir1=train_dir_1,\r\n>                                            dir2=train_dir_2,\r\n>                                            batch_size=batch_size,\r\n>                                            img_height=img_height,\r\n>                                            img_width=img_height)       \r\n>      \r\n> testgenerator=generate_generator_multiple(test_imgen,\r\n>                                           dir1=train_dir_1,\r\n>                                           dir2=train_dir_2,\r\n>                                           batch_size=batch_size,\r\n>                                           img_height=img_height,\r\n>                                           img_width=img_height)              \r\n>           \r\n>  history=model.fit_generator(inputgenerator,\r\n>                         steps_per_epoch=trainsetsize/batch_size,\r\n>                         epochs = epochs,\r\n>                         validation_data = testgenerator,\r\n>                         validation_steps = testsetsize/batch_size,\r\n>                         use_multiprocessing=True,\r\n>                         shuffle=False)\r\n> ```\r\n\r\nHow is that that the test generator and train generator(input generator) have the same directories??\r\nShouldn't it be different??\r\n(dir1 and dir2 should be different)\r\nPlease correct me if I'm wrong."", ""def generate_generator_multiple(generator,dir1, dir2, batch_size, img_height,img_width):\r\n    genX1 = generator.flow_from_directory(dir1,\r\n                                          target_size = (img_height,img_width),\r\n                                          class_mode = 'categorical',\r\n                                          batch_size = batch_size,\r\n                                          shuffle=False, \r\n                                          seed=7)\r\n    \r\n    genX2 = generator.flow_from_directory(dir2,\r\n                                          target_size = (img_height,img_width),\r\n                                          class_mode = 'categorical',\r\n                                          batch_size = batch_size,\r\n                                          shuffle=False, \r\n                                          seed=7)\r\n    while True:\r\n            X1i = genX1.next()\r\n            X2i = genX2.next()\r\n            yield [X1i[0], X2i[0]], X2i[1]  #Yield both images and their mutual label\r\n\r\nIn this method shown, \r\nI have a situation where I am returning two ground truth masks, and one image using a custom data_generator as shown above.\r\nHowever, how do I incorporate it into a custom loss function.\r\n\r\n    def custom_loss(y_true1,ytrue2, y_pred):\r\n        #compute loss here\r\n        return loss"", 'Hi, I am using the train generator for two inputs as @aendrs wrote. \r\nThe problem is that the two generators have two different orders, and this is not intended because we have mutual labels. Is there a way to use order of the first generator in the second as well? Thank you!', ""To reply to my comment, I made my own class of generator that takes as input 2 images and allows multiple labels:\r\n\r\nimport numpy as np\r\nimport keras\r\nimport imageio\r\n\r\nclass DataGenerator(keras.utils.Sequence):\r\n    'Generates data for Keras'\r\n    def __init__(self, list_IDs1, list_IDs2, labels, batch_size=32, dim=(32,32,32), n_channels=1,\r\n                 n_classes=10, shuffle=True):\r\n        'Initialization'\r\n        self.batch_size = batch_size\r\n        self.labels = labels\r\n        self.list_IDs1 = list_IDs1\r\n        self.list_IDs2 = list_IDs2\r\n        self.n_channels = n_channels\r\n        self.dim=dim\r\n        self.n_classes = n_classes\r\n        self.shuffle = shuffle\r\n        self.on_epoch_end()\r\n    def __len__(self):\r\n        'Denotes the number of batches per epoch'\r\n        return int(np.floor(len(self.list_IDs1) / self.batch_size))\r\n    def on_epoch_end(self):\r\n        'Updates indexes after each epoch'\r\n        self.indexes = np.arange(len(self.list_IDs1))\r\n        if self.shuffle == True:\r\n            np.random.shuffle(self.indexes)\r\n    def __getitem__(self, index):\r\n        'Generate one batch of data'\r\n        # Generate indexes of the batch\r\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\r\n        # Find list of IDs\r\n        list_IDs_temp1 = [self.list_IDs1[k] for k in indexes]\r\n        list_IDs_temp2 = [self.list_IDs2[k] for k in indexes]\r\n        # Generate data\r\n        [X1,X2], y = self.__data_generation(list_IDs_temp1,list_IDs_temp2,indexes)\r\n        return [X1,X2], y\r\n    def __data_generation(self, list_IDs_temp1,list_IDs_temp2,indexes):\r\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\r\n        # Initialization\r\n        X1 = np.empty((self.batch_size, *self.dim, self.n_channels))\r\n        X2 = np.empty((self.batch_size, *self.dim, self.n_channels))\r\n        y = [np.empty(self.batch_size,dtype=np.float32) for _ in range(len(self.labels))]\r\n        # Generate data\r\n        for i, ID in enumerate(list_IDs_temp1):\r\n            # Store sample\r\n            X1[i,] = np.stack((imageio.imread(ID),) * 3, axis=-1)\r\n            # Store class\r\n            for list_id in range(len(self.labels)):\r\n                y[list_id][i] = self.labels[list_id][indexes[i]]\r\n        for i, ID in enumerate(list_IDs_temp2):\r\n            X2[i,] = np.stack((imageio.imread(ID),) * 3, axis=-1)\r\n        return [X1,X2], y\r\n\r\nThis way the order of the two batches of data is the same. "", ""@enviz \r\n\r\nThanks for your guide on multiple inputs, very much appreciated.\r\n\r\nI have a similar issue which I used your shared code. However, while executing, I have the TypeError: can't pickle generator objects\r\n\r\nKindly provide guidance to resolve this issue.\r\n\r\nThank you "", ""@aendrs @enviz \r\nPlease, I have the TypeError: can't pickle generator objects\r\n\r\nKindly provide guidance to resolve this issue.\r\n\r\nThank you"", 'I experience a similar problem. \r\nI wanted to combine two input streams: 1 is an image and 2 is numerical data. \r\n\r\nI found that the best solution is to manipulate the keras.utils.Sequence.TimeseriesGenerator functionality for your own purpose here. \r\n\r\nFor my problem [x1, x2], y this is a working generator:\r\n\r\nimport numpy as np\r\nimport json\r\nimport keras\r\n\r\nclass TimeseriesGenerator(keras.utils.Sequence):\r\n\r\n    def __init__(self, img, data, targets, length,\r\n                 sampling_rate=1,\r\n                 stride=1,\r\n                 start_index=0,\r\n                 end_index=None,\r\n                 shuffle=False,\r\n                 reverse=False,\r\n                 batch_size=128):\r\n\r\n        if len(data) != len(targets):\r\n            raise ValueError(\'Data and targets have to be\' +\r\n                             \' of same length. \'\r\n                             \'Data length is {}\'.format(len(data)) +\r\n                             \' while target length is {}\'.format(len(targets)))\r\n            \r\n        self.img = img\r\n        self.data = data\r\n        self.targets = targets\r\n        self.length = length\r\n        self.sampling_rate = sampling_rate\r\n        self.stride = stride\r\n        self.start_index = start_index + length\r\n        if end_index is None:\r\n            end_index = len(data) - 1\r\n        self.end_index = end_index\r\n        self.shuffle = shuffle\r\n        self.reverse = reverse\r\n        self.batch_size = batch_size\r\n\r\n        if self.start_index > self.end_index:\r\n            raise ValueError(\'`start_index+length=%i > end_index=%i` \'\r\n                             \'is disallowed, as no part of the sequence \'\r\n                             \'would be left to be used as current step.\'\r\n                             % (self.start_index, self.end_index))\r\n\r\n    def __len__(self):\r\n        return (self.end_index - self.start_index +\r\n                self.batch_size * self.stride) // (self.batch_size * self.stride)\r\n\r\n    def __getitem__(self, index):\r\n        if self.shuffle:\r\n            rows = np.random.randint(\r\n                self.start_index, self.end_index + 1, size=self.batch_size)\r\n        else:\r\n            i = self.start_index + self.batch_size * self.stride * index\r\n            rows = np.arange(i, min(i + self.batch_size *\r\n                                    self.stride, self.end_index + 1), self.stride)\r\n\r\n        samples2 = np.array([self.img[row - self.length:row:self.sampling_rate]\r\n                            for row in rows])\r\n        samples = np.array([self.data[row - self.length:row:self.sampling_rate]\r\n                            for row in rows])\r\n        targets = np.array([self.targets[row] for row in rows])\r\n\r\n        if self.reverse:\r\n            return samples[:, ::-1, ...], targets\r\n        return [samples2, samples], targets\r\n\r\n    def get_config(self):\r\n        \'\'\'Returns the TimeseriesGenerator configuration as Python dictionary.\r\n        # Returns\r\n            A Python dictionary with the TimeseriesGenerator configuration.\r\n        \'\'\'\r\n        img = self.img\r\n        if type(self.img).__module__ == np.__name__:\r\n            img = self.img.tolist()\r\n        try:\r\n            json_img = json.dumps(img)\r\n        except TypeError:\r\n            raise TypeError(\'Data not JSON Serializable:\', img)\r\n        \r\n        data = self.data\r\n        if type(self.data).__module__ == np.__name__:\r\n            data = self.data.tolist()\r\n        try:\r\n            json_data = json.dumps(data)\r\n        except TypeError:\r\n            raise TypeError(\'Data not JSON Serializable:\', data)\r\n\r\n        targets = self.targets\r\n        if type(self.targets).__module__ == np.__name__:\r\n            targets = self.targets.tolist()\r\n        try:\r\n            json_targets = json.dumps(targets)\r\n        except TypeError:\r\n            raise TypeError(\'Targets not JSON Serializable:\', targets)\r\n\r\n        return {\r\n            \'img\': json_img,\r\n            \'data\': json_data,\r\n            \'targets\': json_targets,\r\n            \'length\': self.length,\r\n            \'sampling_rate\': self.sampling_rate,\r\n            \'stride\': self.stride,\r\n            \'start_index\': self.start_index,\r\n            \'end_index\': self.end_index,\r\n            \'shuffle\': self.shuffle,\r\n            \'reverse\': self.reverse,\r\n            \'batch_size\': self.batch_size\r\n        }\r\n\r\n    def to_json(self, **kwargs):\r\n        """"""Returns a JSON string containing the timeseries generator\r\n        configuration. To load a generator from a JSON string, use\r\n        `keras.preprocessing.sequence.timeseries_generator_from_json(json_string)`.\r\n        # Arguments\r\n            **kwargs: Additional keyword arguments\r\n                to be passed to `json.dumps()`.\r\n        # Returns\r\n            A JSON string containing the tokenizer configuration.\r\n        """"""\r\n        config = self.get_config()\r\n        timeseries_generator_config = {\r\n            \'class_name\': self.__class__.__name__,\r\n            \'config\': config\r\n        }\r\n        return json.dumps(timeseries_generator_config, **kwargs)\r\n\r\n\r\ndef timeseries_generator_from_json(json_string):\r\n    """"""Parses a JSON timeseries generator configuration file and\r\n    returns a timeseries generator instance.\r\n    # Arguments\r\n        json_string: JSON string encoding a timeseries\r\n            generator configuration.\r\n    # Returns\r\n        A Keras TimeseriesGenerator instance\r\n    """"""\r\n    full_config = json.loads(json_string)\r\n    config = full_config.get(\'config\')\r\n\r\n    img = json.loads(config.pop(\'img\'))\r\n    config[\'img\'] = img\r\n    data = json.loads(config.pop(\'data\'))\r\n    config[\'data\'] = data\r\n    targets = json.loads(config.pop(\'targets\'))\r\n    config[\'targets\'] = targets\r\n\r\n    return TimeseriesGenerator(**config)', ""> Thanks. Here is how I solved it following some ideas from _issue 3386_, maybe someone might find it useful\r\n> \r\n> ```python\r\n> input_imgen = ImageDataGenerator(rescale = 1./255, \r\n>                                    shear_range = 0.2, \r\n>                                    zoom_range = 0.2,\r\n>                                    rotation_range=5.,\r\n>                                    horizontal_flip = True)\r\n> \r\n> test_imgen = ImageDataGenerator(rescale = 1./255)\r\n> \r\n> \r\n> \r\n> def generate_generator_multiple(generator,dir1, dir2, batch_size, img_height,img_width):\r\n>     genX1 = generator.flow_from_directory(dir1,\r\n>                                           target_size = (img_height,img_width),\r\n>                                           class_mode = 'categorical',\r\n>                                           batch_size = batch_size,\r\n>                                           shuffle=False, \r\n>                                           seed=7)\r\n>     \r\n>     genX2 = generator.flow_from_directory(dir2,\r\n>                                           target_size = (img_height,img_width),\r\n>                                           class_mode = 'categorical',\r\n>                                           batch_size = batch_size,\r\n>                                           shuffle=False, \r\n>                                           seed=7)\r\n>     while True:\r\n>             X1i = genX1.next()\r\n>             X2i = genX2.next()\r\n>             yield [X1i[0], X2i[0]], X2i[1]  #Yield both images and their mutual label\r\n>             \r\n>             \r\n> inputgenerator=generate_generator_multiple(generator=input_imgen,\r\n>                                            dir1=train_dir_1,\r\n>                                            dir2=train_dir_2,\r\n>                                            batch_size=batch_size,\r\n>                                            img_height=img_height,\r\n>                                            img_width=img_height)       \r\n>      \r\n> testgenerator=generate_generator_multiple(test_imgen,\r\n>                                           dir1=train_dir_1,\r\n>                                           dir2=train_dir_2,\r\n>                                           batch_size=batch_size,\r\n>                                           img_height=img_height,\r\n>                                           img_width=img_height)              \r\n>           \r\n>  history=model.fit_generator(inputgenerator,\r\n>                         steps_per_epoch=trainsetsize/batch_size,\r\n>                         epochs = epochs,\r\n>                         validation_data = testgenerator,\r\n>                         validation_steps = testsetsize/batch_size,\r\n>                         use_multiprocessing=True,\r\n>                         shuffle=False)\r\n> ```\r\n\r\nThis code is almost perfect, but for me I have the problem that it just runs forever. If I replace the while true I could get it to work as I expected.""]","[""python\r\n#two classic CNN blocks are defined before these lines and then cocatenated\r\n#create model\r\nmodel = Model([input_img_1,input_img_2], out, name='colliculus_proto')\r\n\r\n#compile model\r\nmodel.compile(optimizer=Adam(lr=1e-3), \r\n              loss='categorical_crossentropy', \r\n              metrics=['accuracy'])\r\n\r\n# image data generators for image inputs\r\ndef input_generator(train_dir,batchsize,img_height,img_width):\r\n    train_generator = ImageDataGenerator(rescale = 1./255, \r\n                                       shear_range = 0.2, \r\n                                       zoom_range = 0.2,\r\n                                       rotation_range=5.,\r\n                                       horizontal_flip = True)\r\n    training_set = train_generator.flow_from_directory(train_dir,\r\n                                                 target_size = (img_height,img_width),\r\n                                                 class_mode = 'categorical',\r\n                                                 batch_size = batchsize,\r\n                                                 shuffle=False)\r\n    return training_set\r\n                                             \r\n\r\ndef test_generator(test_dir,batchsize,img_height,img_width):\r\n    test_gen = ImageDataGenerator(rescale = 1./255)\r\n    test_set = test_gen.flow_from_directory(test_dir,\r\n                                            target_size = (img_height,img_width),\r\n                                            class_mode = 'categorical',\r\n                                            batch_size = batchsize,\r\n                                            shuffle=False)\r\n    return test_set\r\n\r\n\r\n#fit model\r\ninput1=input_generator(train_dir_1,batchsize,img_height,img_width)\r\ninput2=input_generator(train_dir_2,batchsize,img_height,img_width)\r\ntest1=test_generator(test_dir_1,batchsize,img_height,img_width)\r\ntest2=test_generator(test_dir_2,batchsize,img_height,img_width)\r\n\r\nmodel.fit_generator([input1,input2],\r\n                        steps_per_epoch=trainsetsize/batchsize,\r\n                        epochs = epochs,\r\n                        validation_data = [test1,test2],\r\n                        validation_steps = testsetsize/batchsize,\r\n                        use_multiprocessing=True,\r\n                        shuffle=False)\r\n\r\n""]",[],0,0
356,keras,7702,closed,Keras ImageDataGenerator doesn't work properly with shuffle when transforming image and mask together,"I have created two ImageDataGenerator objects to process two images at the same time (one image and his mask) to train an autoencoder/u-net. To randomize the dataset I set . I take these images from two folders (one for the image and other for the mask), so I use two  with the same seed. However, in practice that doesn't work and the mask extracted doesn't correspond to the extracted image. Both images (image and mask) have the same file name.

The problem is that the filenames from the data directories are read in a different order. To solve that I have added the next line to the DirectoryIterator at the end of his __init__ function.


The problem seems to be solved.


- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],"['shuffle=True', 'flow_from_directory', '        self.filenames.sort()\r\n']",0,0
357,keras,9966,closed,Unknown elements appear in validation_data,"My model is compiled with , I train it with  returning tuples <X, y, sample_weight>, and evaluating it on the full devset passed as . My model has 2 inputs so  is a list of 2 numpy arrays.

Now, in [my custom callback](https://stackoverflow.com/a/49832560/932818) at , I check  and see that it's 5..

OK, the first extra element is  flattened from a sublist into 2 elements of the new validation data (although I have no idea why it happens).

But the second extra element is actually just a scalar  appended to ! 
Where does it even come from???",,"['Hi,\r\n\r\nIm writing my own evaluation callbacks and need to access valdiation_data and having the same question. \r\n\r\nDid you figure this out? Any ideas whats happening here? \r\n\r\n\r\n\r\n', ""Hi @deakkon. Sorry I don't recall having found the solution..""]",[],"['sample_weight_mode=temporal', 'fit_generator', 'validation_data=(X_dev, y_dev, weights_dev)', 'X', 'on_epoch_end', 'len(self.validation_data)', 'X_dev', '0.0', 'self.validation_data']",0,0
358,keras,7341,closed,Attention Layer,Having an Attention layer in the API like the Bidirectional Layer would be great. It is very difficult to create custom layers in Keras.,stale,"['This has been widely discussed in #4962, check it out.', 'Implementing custom layers can be tricky. You can find an example implementation of the Attention Layer here: https://github.com/datalogue/keras-attention/blob/master/models/custom_recurrents.py ', ""I've laid out a API Design suggestion for supporting recurrent attention in core keras: https://github.com/fchollet/keras/issues/7633\r\nPlease give feedback! "", 'Bumping this.  There are many threads discussing attention and many disparate code examples on how to implement it Keras.  However, I think the spirit of this issue is to discuss merging an attention layer into the Keras API as a first class citizen, rather than something you can add yourself.  ', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],0,0
359,keras,13571,closed,NaN output on Tensorflow and Theano backend while CNTK output normally,"## Environment settings

Nvidia Driver Version: 430.50

CUDA Version: 10.1

GeForce RTX 2080 Ti

Ubuntu 16.04.6 LTS

| Library                        | Version  |       |
| ------------------------------ | -------- | ----- |
|libgpuarray               |0.7.6                |h14c3975_0|
|pygpu                     |0.7.6            |py36h035aef0_0|
|tensorflow-gpu            |1.14.0                    |\<pip>|
|tensorflow-estimator      |1.14.0                    |\<pip>|
|Theano                    |1.0.4                     |\<pip>|
|cntk-gpu                  |2.7                       |\<pip>|
|numpy                     |1.14.6           |py36h3b04361_4|
|numpy-base                |1.14.6           |py36h81de0dd_4|
|keras-applications        |1.0.8                      |py_0|
|keras-preprocessing       |1.1.0                      |py_1|

---

## Description

mobilenet.1.00.224-imagenet_origin0-NAI1-LS1-GF1-GF2.h5 is a model for image classification. We find something strange when we run this model for some inputs. When we load this model in Keras, this model works well on the backend of CNTK, but when we configure Tensorflow or Theano as the backend of Keras, the output of this model is NaN, which seems quite strange.

We attach the .h5 file, input image and the code below. The phenomenon I described above can be reproduced using this command:



[20191114-nan.zip](https://github.com/keras-team/keras/files/3877869/20191114-nan.zip)
",type:bug/performance,"['In addition, I found that the model also got NAN results with backend mxnet-cu101                  ==1.5.1.post0. It appears that all three backends treat the model as the same way.']",['\r\npython get_prediction.py [backend name]\r\n'],[],0,0
360,keras,10089,closed,keras using tensorflow backend: InvalidArgumentError: <exception str() failed>,"tensorflow version==1.3 , keras ==2.0.8


code:

![image](https://user-images.githubusercontent.com/11004307/39503488-d1151ff6-4df8-11e8-97f3-ce8b9e3fb719.png)
",,"[' I have met the same problem. Have you solved your problem?', ""@JennyyiqiWang For my situation only, it's a data type issue. I've fed a float number into a function requires int32 or int64. I still don't get why tf throws a str() exception. Hope this could help you.""]",[],"[""model.train_on_batch([img,labels,input_length,label_length,rois], np.ones(1))\r\n\r\nand I print the inputs as followed: \r\n\r\n('img:', array([[[[255., 255., 255.],\r\n         [255., 255., 255.],\r\n         [255., 255., 255.],\r\n         ...,\r\n         [255., 255., 255.],\r\n         [255., 255., 255.],\r\n         [255., 255., 255.]],\r\n\r\n        [[255., 255., 255.],\r\n         [255., 255., 255.],\r\n         [255., 255., 255.],\r\n         ...,\r\n         [255., 255., 255.],\r\n         [255., 255., 255.],\r\n         [255., 255., 255.]],\r\n\r\n        [[255., 255., 255.],\r\n         [255., 255., 255.],\r\n         [255., 255., 255.],\r\n         ...,\r\n         [255., 255., 255.],\r\n         [255., 255., 255.],\r\n         [255., 255., 255.]],\r\n\r\n        ...,\r\n\r\n        [[255., 255., 255.],\r\n         [255., 255., 255.],\r\n         [255., 255., 255.],\r\n         ...,\r\n         [255., 255., 255.],\r\n         [255., 255., 255.],\r\n         [255., 255., 255.]],\r\n\r\n        [[255., 255., 255.],\r\n         [255., 255., 255.],\r\n         [255., 255., 255.],\r\n         ...,\r\n         [255., 255., 255.],\r\n         [255., 255., 255.],\r\n         [255., 255., 255.]],\r\n\r\n        [[255., 255., 255.],\r\n         [255., 255., 255.],\r\n         [255., 255., 255.],\r\n         ...,\r\n         [255., 255., 255.],\r\n         [255., 255., 255.],\r\n         [255., 255., 255.]]]], dtype=float32),\r\n 'labels:', array([[[5000., 1313.,  333., 1768.,  540.,  113.,  100.,  426.,   14.,\r\n          148.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.],\r\n        [1420., 1291.,  333., 1768.,  540.,  113.,  100.,  426.,   14.,\r\n          148.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.],\r\n        [1313.,  398.,  333., 1768.,  540.,  113.,  100.,  426.,   14.,\r\n          148.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.],\r\n        [4095.,  669.,  899., 1167.,  333., 1768.,  540.,  113.,  100.,\r\n          426.,   14.,  148.,    0.,    0.,    0.,    0.,    0.,    0.],\r\n        [1313., 2309.,  997., 1669.,  397., 2240.,  100.,  426.,   14.,\r\n          148.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.],\r\n        [ 350., 2409., 2019.,  773.,  333., 1768.,  540.,  113.,  100.,\r\n          426.,   14.,  148.,    0.,    0.,    0.,    0.,    0.,    0.],\r\n        [1313.,   41.,  333., 1768.,  540.,  113.,  100.,  426.,   14.,\r\n          148.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.],\r\n        [2198.,  182.,  333., 1768.,  540.,  113.,  100.,  426.,   14.,\r\n          148.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.],\r\n        [ 631.,   24.,  932.,  932.,   28.,   92.,   93.,   26.,   25.,\r\n         1108.,   25.,  630.,  631.,   92., 1108.,   24., 1108.,   26.],\r\n        [4095.,  669., 3226., 1167.,  333., 1768.,  540.,  113.,  100.,\r\n          426.,   14.,  148.,    0.,    0.,    0.,    0.,    0.,    0.],\r\n        [ 177., 1768.,  333., 1768.,  540.,  113.,  100.,  426.,   14.,\r\n          148.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.],\r\n        [ 151., 2409.,  333., 1768.,  540.,  113.,  100.,  426., 1174.,\r\n          641.,   14.,    0.,    0.,    0.,    0.,    0.,    0.,    0.],\r\n        [ 631.,   25.,  932.,   24., 1108.,   92.,  631.,   28.,   28.,\r\n           25.,   24.,   25.,  631.,   25.,   26.,  932., 1108.,   93.],\r\n        [  26.,  631., 1108.,   28.,  630.,   26.,   93.,   25.,   92.,\r\n           28.,   25.,   92.,   28.,   92.,   28.,  631.,   24.,  932.]]],\r\n      dtype=float32), \r\n'label_length:', array([[[18],\r\n        [18],\r\n        [18],\r\n        [18],\r\n        [18],\r\n        [18],\r\n        [18],\r\n        [18],\r\n        [18],\r\n        [18],\r\n        [18],\r\n        [18],\r\n        [18],\r\n        [18]]]),\r\n 'input_length:', array([[[36],\r\n        [36],\r\n        [36],\r\n        [36],\r\n        [36],\r\n        [36],\r\n        [36],\r\n        [36],\r\n        [36],\r\n        [36],\r\n        [36],\r\n        [36],\r\n        [36],\r\n        [36]]]))\r\n__________________\r\n""]",0,0
361,keras,12637,closed,How to represent 28x1x1 output dense layer?,"![스크린샷, 2019-04-08 23-21-12](https://user-images.githubusercontent.com/33189954/55731397-1c98b280-5a55-11e9-993d-9af62380c639.png)
I want to represent this 28x1x1 output layer 
but for keras all I see is one single number dense layer ",type:support,[],[],[],0,0
362,keras,8500,closed,Saved Model Loading Inconsistent Behavior,"I am training and persisting the model found at this [link](https://github.com/Hironsan/anago/blob/master/anago/models.py). It trains and loads on the same machine without a problem. However, if the persisted model is loaded on a different machine than the one it was trained on, then it shows the error message bellow.



The files needed to reproduce the error can be found [here](https://drive.google.com/open?id=1XxeZV3YLcP45AqfEVF8Pll3KGn9quYpK).",,[],[],"['Traceback (most recent call last):\r\n  File ""try.py"", line 12, in <module>\r\n    model.load(\'/home/cadmus/Desktop/ner.h5\')\r\n  File ""$HOMEDIR/lib/python3.5/site-packages/anago/models.py"", line 30, in load\r\n    self.model.load_weights(filepath=filepath)\r\n  File ""$HOMEDIR/lib/python3.5/site-packages/keras/engine/topology.py"", line 2622, in load_weights\r\n    load_weights_from_hdf5_group(f, self.layers)\r\n  File ""$HOMEDIR/lib/python3.5/site-packages/keras/engine/topology.py"", line 3115, in load_weights_from_hdf5_group\r\n    str(len(filtered_layers)) + \' layers.\')\r\nValueError: You are trying to load a weight file containing 8 layers into a model with 7 layers.']",0,0
363,keras,6499,closed,Evaluate_generator produces wrong accuracy scores?,"Hello, I run a slightly modified version of the [keras fine tuning examples](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) which only fine tunes the top layers (with Keras 2.0.3/Tensorflow on Ubuntu with GPU). This looks like the following:

With this, I get unreliable validation accuracy results. For example, predict_generator predicts 640 out of 800 (80%) classes correctly whereas evaluate_generator produces an accuracy score of 95%. Someone in #3477 suggests to remove the  parameter from the validation generator, then I get results of 365/800=45% and 89% from evaluate_generator.

Is there something wrong with my evaluation or is this due to a bug? There are many similar issues (e.g. #3849, #6245) where the stated accuracy (during training and afterwards) doesn't match the actual predictions. Could someone experienced maybe shine some light onto this problem? Thanks
",,"[""Having looked into the backend I believe this is due to the number of workers if you have more than one worker there is nothing to ensure consistency of file loading across them. Therefore it is possible that multiple files are shown numerous times in these methods, as the 12 generators are randomly initialised but don't actually share a file list state.\r\n\r\nMaybe @fchollet or @farizrahman4u can confirm?"", 'Thanks for your answer. I already considered that the image generators are not threadsafe. However, I was able to reproduce the problem with setting all three worker counts to 1. Does the order of the filenames maybe not correspond to the order of the scores?\r\n\r\nRecently added similar issues are #6540 and #6544.', 'Plus, @fchollet mentioned [here](https://github.com/fchollet/keras/issues/6538) that the `ImageDataGenerator` supports multiprocessing. ', '@skoch9 try to set `pickle_safe=True`. As @joeyearsley mention, for me it had to do with worker > 1. I am actually running on a custom version of Keras where I make `evaluate_generator` inside fit_generator workers=1. That way I can train with multiple workers but predict/evaluate with a single worker. \r\n\r\n@fchollet Please make evaluate_generator, and predict_generator `workers=1` always, or eliminate parameter until it is fixed.\r\n\r\nMake sure:\r\n1) shuffle = false\r\n2) pickle_safe = True\r\n3) workers = 1\r\n\r\nLet me know if that gives you consistent results.', ""Thanks @abnera for your answer. I investigated a little further and found that running `evaluate_generator` before `predict_generator` without setting `pickle_safe=True` messes up the predictions of the latter, even without multiprocessing.\r\n```python\r\nscore = model.evaluate_generator(validation_generator, nb_validation_samples/batch_size, pickle_safe=False)\r\nscores = model.predict_generator(validation_generator, nb_validation_samples/batch_size)\r\n```\r\nAs for the parameters, when setting `workers >1`, shouldn't `pickle_safe` automatically be set to `True` also for the `fit_generator`?"", '@skoch9 no. Pickle safe is false by default in all keras generator methods. The only difference between this two modes is False uses multithread and True uses multiprocess. ', ""Sure, but it doesn't make sense to allow invalid parameter configurations. And I would consider it also problematic (->a bug) that running `evaluate_generator` before `predict_generator` changes the prediction results."", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', 'Hello\r\nI made a test based on the mnist_cnn.py example. \r\nI just change model.fit by\r\n\r\n`gen=matrix_generator(x_train,y_train,batch_size=batch_size)\r\n  val_steps = len(y_test)/batch_size`\r\n\r\n ` model.fit_generator(gen,epochs=epochs,\r\n                    steps_per_epoch= steps_per_epoch,\r\n                    validation_data=(x_test, y_test))`\r\n\r\nwith the generator define to take subpart of the input matrix\r\n\r\n`def matrix_generator(x,y,batch_size=1,validation=False):`\r\n`    import numpy as np`\r\n`    item=0;`\r\n`    tot_item,dimx,dimy,dimz=x.shape`\r\n\r\n    X = np.zeros((batch_size,dimx,dimy,dimz), dtype=np.float32)\r\n    \r\n    if len(y.shape)==1:\r\n        Y = np.zeros((batch_size))\r\n    else:\r\n        Y = np.zeros((batch_size, y.shape[1]))\r\n\r\n    while True:\r\n        for bs in range(batch_size):\r\n            ximg = x[item,:,:,:]\r\n            \r\n            yout = y[item,:]                \r\n            yout = yout.reshape(1,yout.shape[0])\r\n            Y[bs,:] = yout\r\n            \r\n            X[bs,:,:,:] = ximg\r\n\r\n            item +=1\r\n            if item>tot_item-1:\r\n                item=0\r\n\r\n        if validation:\r\n            yield X\r\n        else:\r\n            yield X,Y\r\n`\r\nNow if I test the model\r\n\r\n model.evaluate(x_test, y_test, verbose=0)\r\nOut[18]: [0.16519621040821075, 0.96389999999999998]\r\n\r\nbut\r\ngen_test = matrix_generator(x_test,y_test,batch_size=batch_size)\r\nmodel.evaluate_generator(gen_test,len(y_test)/batch_size,workers=1)\r\nOut[17]: [0.31920816290837067, 0.93760016025641024]\r\n\r\nsame learn parameter, same test data, but different results\r\n\r\nWhat to do ???\r\nMany thanks for your help\r\n', 'To follow up \r\nI get the correct result only if I set \r\nmax_q_size=1\r\n\r\nif I only set worker=1 it does not woks either (and give different results each time)\r\n\r\n', 'Hi,\r\nI am having a similar problem with a binary classifier that uses 2 outputs. `model.evaluate_generator()` and `model.predict()` both suggest that I have 50% accuracy (chance) and a loss of over 1.0, while `model.fit_generator()` always gives me a loss of under 0.5 and accuracy of over 80%.\r\n\r\n    for x in range(10000):\r\n        print(""starting epoch {0}"".format(x))\r\n        mg_acc = mg_model.evaluate_generator(mg_gen, 2, \r\n            max_queue_size=1, workers=1, use_multiprocessing=False)\r\n        mg_model.fit_generator(mg_gen, 2, epochs=1, callbacks=mg_callbacks, \r\n            max_queue_size=1, workers=1, use_multiprocessing=False, verbose=1)\r\n        print(mg_acc)\r\n        print(mg_model.metrics_names)\r\n\r\nIt does not matter if use_multiprocessing is True or False, or the order of the calls (`fit_generator` before `evaluate_generator` or vice-versa), or if one call is commented out. \r\n\r\nI know that this code will not result in identical generator output (each calls the next iteration of the generator) but the output of the generator is similar between iterations, and the `evaluate_generator` is always the one that does poorly, regardless of order.\r\n\r\n\r\nEDIT: SOLVED: My accuracy difference was eliminated when I removed all batch normalization layers (also removed dropout layers, but I don\'t think that was the cause)', ""I too have a big difference between the reported fit_generator results and the later evaluate_generator results.\r\n\r\nI've looked into this a bit and found the following results:\r\n1.  When i use evaluate_generator with a generator that does not shuffle the suite, i get results that are \r\n     very different than those reported by fig_generator.\r\n     But when i use evaluate_generator with a generator that does shuffle the suite, i get results that are \r\n     similiar to those reported by fit_generator.\r\n\r\n2. When i use evaluate (without any generators) the output is exactly the same as evaluate_generator \r\n    without shuffling\r\n\r\n3. When i use model.predict and infer the measurements manually, i get the same measurements \r\n   reported by fit_generator (and the same results as evaluate_generator with shuffling)\r\n\r\nCan anyone verify that any of the above happen to them as well?\r\n\r\nA note:\r\nI use a very simple model - just one dense layer. \r\nno dropouts or batch normalizations to create any doubts as mentioned by @jeremydr2 "", '@GalAvineri : Very interesting... I just came across your post, after posting my own frustrations: https://github.com/fchollet/keras/issues/5818. I will try to add shuffling to my evaluate_generator, to see if this makes any difference, and if I get the same results as you do, although I cannot see any sense in shuffling during model evaluation...', ""I had a similar problem using fit_generator with multiprocessing under Linux: \r\nDuring training the loss was falling rapidly with implausibly high accuracies. However, these could in now way be reproduced when I tested the model on the same data. Even more strangely, when I turned off multiprocessing, accuracies were suddenly realistic again.\r\nTurns out the problem was a combination of OS behavior and my data generator, which was internally doing some shuffling using np.random. Since Linux uses 'fork(2)' to spawn child processes and the initialization of the data generator was happening outside of the MP part, all workers were using the same seed and were generating equal batches. Note that this wasn't a problem under Windows, since here each child process is spun up independently [1]. Resolution was to seed np.random in __getitem__(self, idx).\r\n\r\nMaybe this saves time for some of you.\r\n\r\n[1] http://rhodesmill.org/brandon/2010/python-multiprocessing-linux-windows/"", '`validation_generator = test_datagen.flow_from_directory(shuffle=True)` ,  the `shuffle=True` is set by default.\r\nI found that using predict_generator(validation_generator) will return a shuffled result while validation_generator.classes and validation_generator.filenames does not shuffle the results... So the accuracy calculated after using predict_generator may cause wrong answer.', ""I also ran into a similar issue where my `fit_generator` gives  an accuracy and loss of 98% and 0.08. `evaluate_generator` gives the same accuracy if I use `rescale=1. / 255` but if I don't, I get an accuracy and loss of 50% and 7.9 respectively.  predict_generator always gives only 50% accuracy even if I use `rescale=1. / 255` or not. What should I do?\r\n\r\n@jeremydr2 When you removed the batch normalization and dropout what was your accuracy and loss? Was it still 80% and 0.5? Because for me it fell to 50%\r\n\r\n@GalAvineri I have a similar issue, but I do not think that the issue is about using shuffle but it is because of the `rescale=1. / 255` in ImageDataGenerator"", 'This issue is reproduced regularly while using fit_generator / evaluate_generator, and it seems pretty critical since it makes fit_generator output during training completely useless.', ""My problem was using `loss='binary_crossentropy'` instead of `loss='categorical_crossentropy'`. This caused my accuracy to be 96% before I even started training on my 15 different classes.\r\n\r\nJust noting that for future googlers. Might not be relevant for this thread topic specifically. It might be worth to warn against this during evaluation in keras when a model has 2+ outputs and the accuracy is binary.\r\n\r\nhttps://stackoverflow.com/questions/42081257/keras-binary-crossentropy-vs-categorical-crossentropy-performance"", '> I had a similar problem using fit_generator with multiprocessing under Linux:\r\n> During training the loss was falling rapidly with implausibly high accuracies. However, these could in now way be reproduced when I tested the model on the same data. Even more strangely, when I turned off multiprocessing, accuracies were suddenly realistic again.\r\n> Turns out the problem was a combination of OS behavior and my data generator, which was internally doing some shuffling using np.random. Since Linux uses \'fork(2)\' to spawn child processes and the initialization of the data generator was happening outside of the MP part, all workers were using the same seed and were generating equal batches. Note that this wasn\'t a problem under Windows, since here each child process is spun up independently [1]. Resolution was to seed np.random in **getitem**(self, idx).\r\n> \r\n> Maybe this saves time for some of you.\r\n> \r\n> [1] http://rhodesmill.org/brandon/2010/python-multiprocessing-linux-windows/\r\n\r\nI\'m not sure I understand your solution.  When you say, ""Resolution was to seed np.random in **getitem**(self, idx)."" Could you explain how you did this a little more thoroughly?  How do you seed np.random in getitem(self,idx)?', ""Hey guys i'm having similar issues with predict_generator.\r\n\r\nWhen i'm training  i get around 92% train acc and 80% val_acc but when i make predictions and put them through a confusion matrix the acc drops to 50%, any updates on this \r\n"", 'hi, same here - manually computed accuracy on this kaggle competition https://www.kaggle.com/c/aerial-cactus-identification gives me 50-65% accuracy (binary classification problem) while predict_generator\r\ngives roughly 95%. I tried drop_duplicates=True, seed=2019, pickle_safe = True, workers=1 args and I\'m using the correct loss=\'binary_crossentropy\' for the model. I\'m happy to give full details but the overall picture seems to be the same as above.\r\n\r\ntruth_generator = datagen.flow_from_dataframe(dataframe=df_truth, directory=""test"", x_col=""id"", y_col=""has_cactus"", class_mode=""binary"", target_size=(32,32), batch_size=200, drop_duplicates=True, seed=2019, pickle_safe = True, workers=1)\r\npredictions = model.predict_generator(truth_generator, steps=20)\r\n', 'You need to add shuffle=False to flow_from_dataframe. ', ""Hi, \r\n\r\nI encountered the same issue recently, and actually the solution is quite simple. \r\nYou use validation_generator two times in a row, and I imagine your number of samples isn't exactly divisible by your batch size. Hence, your generator has a shift in its indices after you use it in model.evaluate_generator. So when you call it, the generator won't yield the sampels in the order you expect. \r\n\r\nSo you should create a second generator to use in model.predict_generator, or only evaluate your model via evaluate_generator or predict_generator : \r\n\r\n```\r\nvalidation_generator2 = test_datagen.flow_from_directory(\r\n    validation_data_dir,\r\n    target_size=(img_height, img_width),\r\n    batch_size=batch_size,\r\n    class_mode='binary', shuffle=False)\r\n\r\nscore = model.evaluate_generator(validation_generator, nb_validation_samples/batch_size, workers=12)\r\n\r\nscores = model.predict_generator(validation_generator2, nb_validation_samples/batch_size, workers=12)\r\n\r\n```\r\n"", 'I am experiencing a similar problem, my model is trained using `fit_generator`, saved using `model.save`, loaded using `load_model` and evaluated using `evaluate_generator` but its accuracy is similar to the untrained one. However, `model.predict` (without generator) works adequately well.\r\n\r\nUsing `keras.__version__ = 2.2.4`.', 'this seems to be still a problem. One simple fix was not to use multi-process ', 'I have encountered this problem today.\r\nAnd I found the solution to this:\r\n1. You must set `shuffle=False` in your generator.\r\n2. you need to reset your generator before calling `predict_generator() ` function.\r\nFor example:\r\n```\r\nvalid_generator = datagen.flow_from_dataframe(\r\n    dataframe=train_df,\r\n    directory=""../images/train/"",\r\n    x_col=""id"",\r\n    y_col=""label_2"",\r\n    subset=""validation"",\r\n    batch_size=batch_size,\r\n    seed=42,\r\n    shuffle=False,\r\n    class_mode=""categorical"",\r\n    classes=classes,\r\n    target_size=(input_shape, input_shape))\r\nstep_size_valid = np.ceil(valid_generator.n / valid_generator.batch_size)\r\nmodel.evaluate_generator(generator=valid_generator, steps=step_size_valid)\r\n...\r\nvalid_generator.reset()\r\nmodel.predict_generator(valid_generator, step_size_valid)\r\n```', ""The `predict_generator` on a custom data generator for me also produces a different result compared to `eval_generator` and the verbose output from `fit_generator`. I didn't shuffle the index in `predict_generator`"", 'Same issue. Totally confused from the answers above.  Any simple solution?', 'same issue. any simple solution from the official authors yet?', 'Similiar observations to @GalAvineri , when setting shuffle = False, my evaluate_generator accuracy goes up to 92%, which is great, but unrealistic. Setting it to true yields much more realistic results. I would appreciate some official guidelines on this - there are others who argue otherwise (@lipinski) which makes it incredibly confusing', 'Same issue. Totally confused from the answers above. Any simple solution?', 'Same issue/', 'Ok, I solved the issue.\r\nMy code previously:\r\n```\r\nscores_evaluation = model.evaluate_generator(test_generator.flow(X_test,Y_test, batch_size=32, shuffle=False),len(Y_test)/32)\r\nscores_prediction = model.predict_generator(test_generator.flow(X_test, batch_size=32, shuffle=False),len(Y_test)/32)\r\n```\r\nHowever, in **flow()** **shuffle is default to True**, so for each flow function here, I added shuffle=False. Then problem solved.', 'setting the shuffle=False just saved my life :D']","['python\r\nimg_width, img_height = 150, 150\r\ntrain_data_dir = \'data/train_s\'\r\nvalidation_data_dir = \'data/val_s\'\r\nnb_train_samples = 2000\r\nnb_validation_samples = 800\r\nepochs = 10\r\nbatch_size = 16\r\n\r\nbase_model = applications.VGG16(weights=\'imagenet\', include_top=False, input_shape=(img_width, img_height, 3))\r\n\r\ntop_model = Sequential()\r\ntop_model.add(Flatten(input_shape=base_model.output_shape[1:]))\r\ntop_model.add(Dense(256, activation=\'relu\'))\r\ntop_model.add(Dense(1, activation=\'sigmoid\'))\r\n\r\nmodel = Model(inputs=base_model.input, outputs=top_model(base_model.output))\r\nmodel.compile(loss=\'binary_crossentropy\', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\r\n              metrics=[\'accuracy\'])\r\n\r\ntrain_datagen = ImageDataGenerator(\r\n    rescale=1. / 255,\r\n    shear_range=0.2,\r\n    zoom_range=0.2,\r\n    horizontal_flip=True)\r\n\r\ntest_datagen = ImageDataGenerator(rescale=1. / 255)\r\n\r\ntrain_generator = train_datagen.flow_from_directory(\r\n    train_data_dir,\r\n    target_size=(img_height, img_width),\r\n    batch_size=batch_size,\r\n    class_mode=\'binary\')\r\n\r\nvalidation_generator = test_datagen.flow_from_directory(\r\n    validation_data_dir,\r\n    target_size=(img_height, img_width),\r\n    batch_size=batch_size,\r\n    class_mode=\'binary\', shuffle=False)\r\n\r\nmodel.fit_generator(\r\n    train_generator,\r\n    steps_per_epoch=nb_train_samples // batch_size,\r\n    epochs=epochs,\r\n    validation_data=validation_generator,\r\n    validation_steps=nb_validation_samples // batch_size,\r\n    verbose=2, workers=12)\r\n\r\nscore = model.evaluate_generator(validation_generator, nb_validation_samples/batch_size, workers=12)\r\n\r\nscores = model.predict_generator(validation_generator, nb_validation_samples/batch_size, workers=12)\r\n\r\ncorrect = 0\r\nfor i, n in enumerate(validation_generator.filenames):\r\n    if n.startswith(""cats"") and scores[i][0] <= 0.5:\r\n        correct += 1\r\n    if n.startswith(""dogs"") and scores[i][0] > 0.5:\r\n        correct += 1\r\n\r\nprint(""Correct:"", correct, "" Total: "", len(validation_generator.filenames))\r\nprint(""Loss: "", score[0], ""Accuracy: "", score[1])\r\n']",['rescale=1. / 255'],0,0
364,keras,12335,closed,K.in_top_k on CNTK backend is broken,"Reproducible script

throws


---

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:


- [x] Check that your version of CNTK is up-to-date.

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",,[],"[""python\r\nimport keras\r\nfrom keras import backend as K\r\nimport numpy as np\r\n\r\nbatch_size = 20\r\nnum_classes = 10\r\npredictions = K.variable(np.random.random((batch_size, num_classes)).astype('float32'))\r\ntargets = K.variable(np.random.randint(num_classes, size=batch_size, dtype='int32'))\r\n\r\nK.eval(K.in_top_k(predictions,targets,3))\r\n"", '\r\nAbout to throw exception \'Node \'ClassificationError10\' (ClassificationError operation): Expected MBLayout in Input 0.\'\r\nValidating --> ClassificationError10 = ClassificationError (OneHotOp6, Parameter4, Constant9) : [10 x 20], [10 x 20], [] -> [] FAILED\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/abhai/keras-env/keras/keras/backend/cntk_backend.py"", line 249, in eval\r\n    return x.eval()\r\n  File ""/home/abhai/keras-env/lib/python3.6/site-packages/cntk/ops/functions.py"", line 733, in eval\r\n    _, output_map = self.forward(arguments, outputs, device=device, as_numpy=as_numpy)\r\n  File ""/home/abhai/keras-env/lib/python3.6/site-packages/cntk/internal/swig_helper.py"", line 69, in wrapper\r\n    result = f(*args, **kwds)\r\n  File ""/home/abhai/keras-env/lib/python3.6/site-packages/cntk/ops/functions.py"", line 867, in forward\r\n    keep_for_backward)\r\n  File ""/home/abhai/keras-env/lib/python3.6/site-packages/cntk/cntk_py.py"", line 1980, in _forward\r\n    return _cntk_py.Function__forward(self, *args)\r\nRuntimeError: Node \'ClassificationError10\' (ClassificationError operation): Expected MBLayout in Input 0.\r\n']",['pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps'],0,0
365,keras,9495,closed,How to turn variational_autoencoder example into beta-VAE?,"Hi all, I'm trying to turn current VAE example into beta-VAE (https://openreview.net/pdf?id=Sy2fzU9gl). Having hard time, asking community with the suggestion or ideally a code snippet.

Here a TF implementation (https://github.com/miyosuda/disentangled_vae), but I would prefer a Keras one.

Thanks in advance!",,[],[],[],0,0
366,keras,9727,closed,Dead Lock on huge model when using `workers > 0`,"By default, model training will use  and  as the default argument. However, whatever  is True or False, unless workers = 0, the engine will always create a thread pool to handle data reading, but it seems there is a high-frequent dead lock on application exit/quit when training huge model like Inception_v3 and Resnet50, and I have to press many-time of  to kill the hanging thread created by the thread pool.

So I suggest to set default argument of  to be  which will not create any extra threads.",,[],[],"['workers = 1', 'use_multiprocess = False', 'use_multiprocess', 'Ctrl-C', 'workers', '0']",0,0
367,keras,11099,closed,TensorBoard crashes on second model run,"TensorBoard in Keras crashes deep in the callbacks for the end-of-epoch. Here's a simplified version of the code:

# The Code
    import pandas as pd
    import keras
    from keras.models import Sequential
    from keras.layers import Dense

    class ToyNet ():
        def __init__(self, run=1, layer_1_nodes=50, layer_2_nodes=100, layer_3_nodes=50):
            self.run_seq = run
            self.layer_1_nodes = layer_1_nodes
            self.layer_2_nodes = layer_2_nodes
            self.layer_3_nodes = layer_3_nodes

            # Define the model
            self.model = Sequential()
            self.model.add(Dense(self.layer_1_nodes, input_dim=9, activation='relu', name='layer_1'))
            self.model.add(Dense(self.layer_2_nodes, activation='relu', name='layer_2'))
            self.model.add(Dense(self.layer_3_nodes, activation='relu', name='layer_3'))
            self.model.add(Dense(1, activation='linear', name='output_layer'))
            self.model.compile(loss='mean_squared_error', optimizer='adam')

            # Create a TensorBoard logger
            log_dir = ""logs_{}"".format(self.run_seq)
            self.logger = keras.callbacks.TensorBoard(
                log_dir=log_dir,
                histogram_freq=5
            )

        def train(self, X, Y, epochs=50):
            # Train the model
            self.model.fit(
                X,
                Y,
                epochs=epochs,
                shuffle=True,
                verbose=2,
                validation_split=0.05,
                callbacks=[self.logger]
            )

        def test(self, Xt, Yt):
            test_error_rate = self.model.evaluate(Xt, Yt, verbose=0)
            print(""The mean squared error (MSE) for the test data set is: {}"".format(test_error_rate))

    if __name__ == '__main__':

        training_data_df = pd.read_csv(""sales_data_training_scaled.csv"")

        X = training_data_df.drop('total_earnings', axis=1).values
        Y = training_data_df[['total_earnings']].values

        # Load the test data set
        test_data_df = pd.read_csv(""sales_data_test_scaled.csv"")

        X_test = test_data_df.drop('total_earnings', axis=1).values
        Y_test = test_data_df[['total_earnings']].values

        print(""Run #1"")
        toy1 = ToyNet(1, 50, 100, 50)
        toy1.train(X, Y)
        toy1.test(X_test, Y_test)

        print(""Run #2"")
        toy2 = ToyNet(2, 5, 100, 50)
        toy2.train(X,Y)              ### <--- Crashes here at the end of first epoch while doing callbacks
        toy2.test(X_test, Y_test)

# The Output

/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from  to  is deprecated. In future, it will be treated as .
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
Run #1
Train on 950 samples, validate on 50 samples
Epoch 1/50
 - 1s - loss: 0.0314 - val_loss: 0.0043
Epoch 2/50
 - 0s - loss: 0.0048 - val_loss: 0.0011

> ... output snipped for brevity

Epoch 50/50
 - 0s - loss: 2.4237e-05 - val_loss: 5.0361e-05
The mean squared error (MSE) for the test data set is: 7.575784547952935e-05
Run #2
Train on 950 samples, validate on 50 samples
Epoch 1/50
 - 1s - loss: 0.0333 - val_loss: 0.0172
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1321     try:
-> 1322       return fn(*args)
   1323     except errors.OpError as e:

/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1306       return self._call_tf_sessionrun(
-> 1307           options, feed_dict, fetch_list, target_list, run_metadata)
   1308 

/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)
   1408           self._session, options, feed_dict, fetch_list, target_list,
-> 1409           run_metadata)
   1410     else:

InvalidArgumentError: You must feed a value for placeholder tensor 'layer_1_input' with dtype float and shape [?,9]
	 [[Node: layer_1_input = Placeholder[dtype=DT_FLOAT, shape=[?,9], _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-1-bb0bb05e17b3> in <module>()
     64 
     65     toy2 = ToyNet(2, 5, 100, 50)
---> 66     toy2.train(X,Y)              ### <--- Crashes here at the end of first epoch while doing callbacks
     67     toy2.test(X_test, Y_test)

<ipython-input-1-bb0bb05e17b3> in train(self, X, Y, epochs)
     39             verbose=2,
     40             validation_split=0.05,
---> 41             callbacks=[self.logger]
     42         )
     43 

/anaconda3/lib/python3.6/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
   1043                                         initial_epoch=initial_epoch,
   1044                                         steps_per_epoch=steps_per_epoch,
-> 1045                                         validation_steps=validation_steps)
   1046 
   1047     def evaluate(self, x=None, y=None,

/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py in fit_loop(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)
    215                         for l, o in zip(out_labels, val_outs):
    216                             epoch_logs['val_' + l] = o
--> 217         callbacks.on_epoch_end(epoch, epoch_logs)
    218         if callback_model.stop_training:
    219             break

/anaconda3/lib/python3.6/site-packages/keras/callbacks.py in on_epoch_end(self, epoch, logs)
     75         logs = logs or {}
     76         for callback in self.callbacks:
---> 77             callback.on_epoch_end(epoch, logs)
     78 
     79     def on_batch_begin(self, batch, logs=None):

/anaconda3/lib/python3.6/site-packages/keras/callbacks.py in on_epoch_end(self, epoch, logs)
    915                     assert len(batch_val) == len(tensors)
    916                     feed_dict = dict(zip(tensors, batch_val))
--> 917                     result = self.sess.run([self.merged], feed_dict=feed_dict)
    918                     summary_str = result[0]
    919                     self.writer.add_summary(summary_str, epoch)

/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    898     try:
    899       result = self._run(None, fetches, feed_dict, options_ptr,
--> 900                          run_metadata_ptr)
    901       if run_metadata:
    902         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1133     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1134       results = self._do_run(handle, final_targets, final_fetches,
-> 1135                              feed_dict_tensor, options, run_metadata)
   1136     else:
   1137       results = []

/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1314     if handle is None:
   1315       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1316                            run_metadata)
   1317     else:
   1318       return self._do_call(_prun_fn, handle, feeds, fetches)

/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1333         except KeyError:
   1334           pass
-> 1335       raise type(e)(node_def, op, message)
   1336 
   1337   def _extend_graph(self):

InvalidArgumentError: You must feed a value for placeholder tensor 'layer_1_input' with dtype float and shape [?,9]
	 [[Node: layer_1_input = Placeholder[dtype=DT_FLOAT, shape=[?,9], _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Caused by op 'layer_1_input', defined at:
  File ""/anaconda3/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/anaconda3/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py"", line 486, in start
    self.io_loop.start()
  File ""/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py"", line 127, in start
    self.asyncio_loop.run_forever()
  File ""/anaconda3/lib/python3.6/asyncio/base_events.py"", line 422, in run_forever
    self._run_once()
  File ""/anaconda3/lib/python3.6/asyncio/base_events.py"", line 1432, in _run_once
    handle._run()
  File ""/anaconda3/lib/python3.6/asyncio/events.py"", line 145, in _run
    self._callback(*self._args)
  File ""/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py"", line 117, in _handle_events
    handler_func(fileobj, events)
  File ""/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py"", line 276, in null_wrapper
    return fn(*args, **kwargs)
  File ""/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 450, in _handle_events
    self._handle_recv()
  File ""/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 480, in _handle_recv
    self._run_callback(callback, msg)
  File ""/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 432, in _run_callback
    callback(*args, **kwargs)
  File ""/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py"", line 276, in null_wrapper
    return fn(*args, **kwargs)
  File ""/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 233, in dispatch_shell
    handler(stream, idents, msg)
  File ""/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py"", line 208, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py"", line 537, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2662, in run_cell
    raw_cell, store_history, silent, shell_futures)
  File ""/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2785, in _run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2903, in run_ast_nodes
    if self.run_code(code, result):
  File ""/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2963, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-1-bb0bb05e17b3>"", line 61, in <module>
    toy1 = ToyNet(1, 50, 100, 50)
  File ""<ipython-input-1-bb0bb05e17b3>"", line 19, in __init__
    self.model.add(Dense(self.layer_1_nodes, input_dim=9, activation='relu', name='layer_1'))
  File ""/anaconda3/lib/python3.6/site-packages/keras/engine/sequential.py"", line 160, in add
    name=layer.name + '_input')
  File ""/anaconda3/lib/python3.6/site-packages/keras/engine/input_layer.py"", line 178, in Input
    input_tensor=tensor)
  File ""/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper
    return func(*args, **kwargs)
  File ""/anaconda3/lib/python3.6/site-packages/keras/engine/input_layer.py"", line 87, in __init__
    name=self.name)
  File ""/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 517, in placeholder
    x = tf.placeholder(dtype, shape=shape, name=name)
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 1734, in placeholder
    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 4924, in placeholder
    ""Placeholder"", dtype=dtype, shape=shape, name=name)
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'layer_1_input' with dtype float and shape [?,9]
	 [[Node: layer_1_input = Placeholder[dtype=DT_FLOAT, shape=[?,9], _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

# The Data

For sanity's sake I've only included a short sample of each training and testing data below.

## Training data (""sales_data_training_scaled.csv"")
    critic_rating,is_action,is_exclusive_to_us,is_portable,is_role_playing,is_sequel,is_sports,suitable_for_kids,total_earnings,unit_price
    0.4999999999999999,1.0,1.0,1.0,0.0,1.0,0.0,1.0,0.7991793127668619,1.0
    0.16666666666666663,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.15750170976506905,1.0
    0.4999999999999999,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.18970444169239015,1.0
    0.6666666666666666,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.39223304559989647,0.0
    0.0,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.21546366980277626,1.0
    0.4999999999999999,1.0,0.0,1.0,0.0,0.0,0.0,1.0,0.2675699155283636,1.0
    0.6666666666666666,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.2418106874179775,1.0
    0.4999999999999999,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.7090183175911721,1.0
    0.6666666666666666,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.4385279384854254,1.0
    0.16666666666666663,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.3957893569434946,1.0
    0.9999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.24197334614886973,0.0
    0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.33607142197001905,1.0
    0.4999999999999999,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.26054601578529046,1.0
    0.6666666666666666,0.0,0.0,1.0,1.0,1.0,0.0,0.0,0.3501229182455038,1.0
    0.8333333333333334,0.0,1.0,1.0,0.0,1.0,1.0,0.0,0.39457681004047984,0.0
    0.6666666666666666,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.2248313339864328,1.0
    0.4999999999999999,0.0,0.0,1.0,0.0,1.0,1.0,1.0,0.28864900833625995,1.0
    0.4999999999999999,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.05268664165172547,0.0
    0.9999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.24431711058945305,0.0
    0.16666666666666663,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.2740097225559601,1.0
    0.33333333333333337,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.20141217352729157,1.0
    0.4999999999999999,0.0,1.0,0.0,1.0,1.0,0.0,0.0,0.2802240254339107,0.0
    0.16666666666666663,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.4069129960629193,1.0
    0.6666666666666666,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.21487957708729966,1.0
    0.4999999999999999,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.4250642317147557,1.0
    0.4999999999999999,0.0,0.0,1.0,0.0,1.0,1.0,1.0,0.28513336167538494,1.0
    0.33333333333333337,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.40075044823570727,0.5
    0.4999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.18811482227685256,0.0
    0.33333333333333337,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1428661207741077,1.0
    0.8333333333333334,1.0,0.0,1.0,0.0,0.0,0.0,1.0,0.27292286649045305,0.5
    0.16666666666666663,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.16160144914142066,1.0
    0.4999999999999999,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.5831426406166245,1.0
    0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.18850668194672926,0.0
    0.8333333333333334,0.0,1.0,0.0,1.0,1.0,0.0,0.0,0.6118297258830706,1.0
    0.9999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.18928670449714424,0.0
    0.4999999999999999,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.13759449917746436,1.0
    0.9999999999999999,0.0,1.0,0.0,0.0,1.0,1.0,0.0,0.34539842147095245,0.0
    0.16666666666666663,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.13583113066301916,0.5
    0.9999999999999999,0.0,1.0,1.0,0.0,1.0,1.0,1.0,0.48629415352766125,0.0
    0.9999999999999999,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.7500009241973347,1.0
    0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.04253895491765401,0.0
    0.4999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.11630468937727585,0.0
    0.16666666666666663,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.2775253692168352,1.0
    0.4999999999999999,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.10216816694700652,0.5
    0.4999999999999999,0.0,1.0,0.0,0.0,1.0,1.0,0.0,0.23650949150662648,0.0
    0.33333333333333337,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.18031089998336447,0.0
    0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.05621337868061589,1.0
    0.9999999999999999,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.7949538825530027,0.5
    0.4999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.18538289495573096,0.0
    0.8333333333333334,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.2587900408495222,1.0
    0.16666666666666663,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.08373227851610877,1.0
    0.9999999999999999,1.0,1.0,1.0,0.0,0.0,0.0,1.0,0.7447329993900298,1.0
    0.16666666666666663,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.20902386277517976,1.0
    0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.18850668194672926,0.0
    0.16666666666666663,0.0,0.0,1.0,0.0,1.0,1.0,0.0,0.12509565442413267,0.5
    0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.3313875898781908,1.0
    0.33333333333333337,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.3717861037688767,1.0
    0.0,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.1434502134895843,1.0
    0.33333333333333337,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.6329051219016284,1.0
    0.8333333333333334,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.727753645958485,1.0
    0.6666666666666666,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15144267203933381,0.5
    0.8333333333333334,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.8846601726400621,1.0
    0.8333333333333334,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.30738433670357296,1.0

## Testing data (""sales_data_test_scaled.csv"")
    critic_rating,is_action,is_exclusive_to_us,is_portable,is_role_playing,is_sequel,is_sports,suitable_for_kids,total_earnings,unit_price
    0.4999999999999999,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.3747139609249367,1.0
    0.8333333333333334,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.19242527864549636,0.5
    0.33333333333333337,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.11485185116726125,0.5
    0.8333333333333334,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.14245208036820023,0.0
    0.6666666666666666,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.4806824273118796,1.0
    0.6666666666666666,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.13972015304707863,0.0
    0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.11338792258923126,0.5
    0.8333333333333334,0.0,0.0,1.0,1.0,1.0,0.0,1.0,0.44906748488937354,1.0
    0.4999999999999999,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.06127428328496702,0.0
    0.16666666666666663,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.20668009833459638,1.0
    0.8333333333333334,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.4777545701558197,1.0
    0.4999999999999999,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.13232657437015954,1.0
    0.4999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.17925361823256503,0.5
    0.6666666666666666,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.16335742407718895,1.0
    0.4999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.23946692297739414,1.0
    0.6666666666666666,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.31206816879540117,1.0
    0.4999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.2285281233248923,0.5
    0.33333333333333337,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.29274505092327313,1.0
    0.16666666666666663,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.3480564130053049,0.5
    0.9999999999999999,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.42799208887081575,1.0
    0.6666666666666666,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.2248313339864328,1.0
    0.8333333333333334,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.1280235115801926,0.5
    0.16666666666666663,0.0,1.0,0.0,1.0,1.0,0.0,1.0,0.40164507125561455,1.0
    0.6666666666666666,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.2963420269495943,0.5
    0.8333333333333334,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.2486090830114046,0.0
    0.8333333333333334,0.0,0.0,0.0,0.0,1.0,1.0,1.0,0.28923310105173655,1.0
    0.0,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.06283432838579693,0.0
    0.4999999999999999,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.29536607456424097,0.5
    0.4999999999999999,0.0,0.0,0.0,1.0,1.0,0.0,1.0,0.2693258904641319,1.0
    0.0,1.0,0.0,1.0,0.0,0.0,0.0,1.0,0.10460804791038984,0.5
    0.4999999999999999,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.4651485185116726,0.5
",Enhancement stat:contributions welcome,"[""Thanks for the report. Could you help us resolve it by simplifying the script to the maximum and see if we still get the error? I would help us narrow down the issue. I think you can try simplifying your example by:\r\n\r\n* Using np.random.uniform instead of loading a csv to make dummy data.\r\n* Remove the test method (I don't think it's causing tensorboard to crash).\r\n* Making the neural network smaller. I think the bug should still appear even if you use two layers.\r\n\r\nCan you try that and report if the bug is still there? Thanks."", 'Yes, the bug is still there with the simplified code:\r\n\r\n    import pandas as pd\r\n    import keras\r\n    from keras.models import Sequential\r\n    from keras.layers import Dense\r\n\r\n    class ToyNet ():\r\n        def __init__(self, run=1, layer_1_nodes=100, layer_2_nodes=50):\r\n            self.run_seq = run\r\n\r\n            # Define the model\r\n            self.model = Sequential()\r\n            self.model.add(Dense(layer_1_nodes, input_dim=9, activation=\'relu\', name=\'layer_1\'))\r\n            self.model.add(Dense(layer_2_nodes, activation=\'relu\', name=\'layer_3\'))\r\n            self.model.add(Dense(1, activation=\'linear\', name=\'output_layer\'))\r\n            self.model.compile(loss=\'mean_squared_error\', optimizer=\'adam\')\r\n\r\n            # Create a TensorBoard logger\r\n            log_dir = ""logs/Run_{}_Layers_{}-{}"".format(run, layer_1_nodes, layer_2_nodes)\r\n            self.logger = keras.callbacks.TensorBoard(\r\n                log_dir=log_dir,\r\n                histogram_freq=5\r\n            )\r\n\r\n        def train(self, X, Y, epochs=50):\r\n            # Train the model\r\n            self.model.fit(\r\n                X,\r\n                Y,\r\n                epochs=epochs,\r\n                shuffle=True,\r\n                verbose=2,\r\n                validation_split=0.05,\r\n                callbacks=[self.logger]\r\n            )\r\n\r\n    if __name__ == \'__main__\':\r\n\r\n        training_data = {\'c0\': np.random.uniform(0,1,100),\r\n                         \'c1\': np.random.uniform(0,1,100),\r\n                         \'c2\': np.random.uniform(0,1,100),\r\n                         \'c3\': np.random.uniform(0,1,100),\r\n                         \'c4\': np.random.uniform(0,1,100),\r\n                         \'c5\': np.random.uniform(0,1,100),\r\n                         \'c6\': np.random.uniform(0,1,100),\r\n                         \'c7\': np.random.uniform(0,1,100),\r\n                         \'c8\': np.random.uniform(0,1,100),\r\n                         \'c9\': np.random.uniform(0,1,100),\r\n        }\r\n        training_data_df = pd.DataFrame(data=training_data)\r\n\r\n        X = training_data_df.drop(\'c9\', axis=1).values\r\n        Y = training_data_df[[\'c9\']].values\r\n\r\n        # Load the test data set\r\n        test_data = {\'c0\': np.random.uniform(0,1,50),\r\n                     \'c1\': np.random.uniform(0,1,50),\r\n                     \'c2\': np.random.uniform(0,1,50),\r\n                     \'c3\': np.random.uniform(0,1,50),\r\n                     \'c4\': np.random.uniform(0,1,50),\r\n                     \'c5\': np.random.uniform(0,1,50),\r\n                     \'c6\': np.random.uniform(0,1,50),\r\n                     \'c7\': np.random.uniform(0,1,50),\r\n                     \'c8\': np.random.uniform(0,1,50),\r\n                     \'c9\': np.random.uniform(0,1,50),\r\n        }\r\n\r\n        test_data_df = pd.DataFrame(data=test_data)\r\n\r\n        X_test = test_data_df.drop(\'c9\', axis=1).values\r\n        Y_test = test_data_df[[\'c0\']].values\r\n\r\n        toy = ToyNet(1, 10, 10)\r\n        toy.train(X, Y)\r\n\r\nA workaround is to clear the Keras session just before each model creation:\r\n\r\n     keras.backend.clear_session()\r\n     self.model = Sequential()\r\n    ...', ""I see, thanks for the report and the workaround! I don't know if this can be fixed easily, there must be some assumption in the callback that there is only one model per session. Let's say PR welcome if somebody has an idea on how to solve it."", 'I think this may be something in keras/callbacks.py related to the tf session.']",[],"['float', 'np.floating', 'np.float64 == np.dtype(float).type']",0,0
368,keras,7755,closed,InvalidArgumentError ,"I use the following code to export keras model to tensorflow.



Im getting the following error while using the generated pb file.

> InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'batch_normalization_1/keras_learning_phase' with dtype bool
> 	 [[Node: batch_normalization_1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=<unknown>, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']","['\r\nfrom wide_resnet import WideResNet\r\nfrom keras.layers.core import K\r\nimport tensorflow as tf\r\nimport os.path as osp\r\nK._LEARNING_PHASE = tf.constant(0)\r\ninput_fld = \'input\'\r\nweight_file = \'weights.18-4.06.hdf5\'\r\nnum_output = 1\r\nwrite_graph_def_ascii_flag = True\r\nprefix_output_node_names_of_final_network = \'softmax\'\r\noutput_graph_name = \'age_weight_two.pb\'\r\n\r\n\r\noutput_fld = input_fld + \'tensorflow_model/\'\r\n\r\nimg_size = 64\r\nmodel = WideResNet(img_size, depth=16, k=8)()\r\nmodel.load_weights(""input/weights.18-4.06.hdf5"")\r\n\r\n\r\n\r\n\r\npred = [None]*num_output\r\npred_node_names = [None]*num_output\r\nfor i in range(num_output):\r\n    pred_node_names[i] = prefix_output_node_names_of_final_network+str(i)\r\n    print (pred_node_names[i] )\r\n    pred[i] = tf.identity(model.output[i], name=pred_node_names[i])\r\nprint(\'output nodes names are: \', pred_node_names)\r\n\r\nsess = K.get_session()\r\n\r\nif write_graph_def_ascii_flag:\r\n    f = \'only_the_graph_def.pb.ascii\'\r\n    tf.train.write_graph(sess.graph.as_graph_def(), output_fld, f, as_text=True)\r\n    print(\'saved the graph definition in ascii format at: \', osp.join(output_fld, f))\r\n\r\nfrom tensorflow.python.framework import graph_util\r\nfrom tensorflow.python.framework import graph_io\r\nconstant_graph = graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), pred_node_names)\r\ngraph_io.write_graph(constant_graph, output_fld, output_graph_name, as_text=False)\r\nprint(\'saved the constant graph (ready for inference) at: \', osp.join(output_fld, output_graph_name))\r\n']",[],0,0
369,keras,10965,closed,RNN stateful is incompatible with initial_state,"If  is being used with an RNN (e.g. LSTM), if you pass in an initial_state (either with  kwarg or as a list of tensors in the argument when calling the layer (), the initial state overrides the stored states in the stateful RNN.",,"[""I just came across this issue as well. Here is an example that fails in Tensorflow 1.14.0:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import backend as K\r\nimport numpy as np\r\n\r\nINPUT_SIZE = 4\r\nBATCH_SIZE = 1\r\nOUTPUT_SIZE = 2\r\n\r\n# Define a model without any state initialization\r\ninput_layer = keras.Input(shape=(None, INPUT_SIZE), batch_size=BATCH_SIZE)\r\nrnn_layer = keras.layers.GRU(units=OUTPUT_SIZE, return_sequences=True, stateful=True)(input_layer)\r\nmodel = keras.Model(input_layer, rnn_layer)\r\nmodel.compile(optimizer=keras.optimizers.RMSprop(), loss='mse')\r\n\r\ntest = np.full((BATCH_SIZE,1,INPUT_SIZE), 0.5)\r\n\r\n# This generates different predictions for the two time steps, as expected\r\nmodel.reset_states()\r\nprint(model.predict(test))\r\nprint(model.predict(test))\r\n# [[[0.15479106 0.3035699 ]]]\r\n# [[[0.22833839 0.4250579 ]]]\r\n\r\n# This generates the same prediction after resetting the state, as expected\r\nmodel.reset_states()\r\nprint(model.predict(test))\r\nmodel.reset_states()\r\nprint(model.predict(test))\r\n# [[[0.15479106 0.3035699 ]]]\r\n# [[[0.15479106 0.3035699 ]]]\r\n\r\n# Define a model with a constant state initialization\r\ninput_layer = keras.Input(shape=(None, INPUT_SIZE), batch_size=BATCH_SIZE)\r\ninitial_state_layer = K.constant(np.ones(OUTPUT_SIZE), shape=(1, OUTPUT_SIZE))\r\nrnn_layer = keras.layers.GRU(units=OUTPUT_SIZE, return_sequences=True, stateful=True)(input_layer, initial_state=initial_state_layer)\r\nmodel = keras.Model(input_layer, rnn_layer)\r\nmodel.compile(optimizer=keras.optimizers.RMSprop(), loss='mse')\r\n\r\n# This generates the same prediction for the two time steps, NOT EXPECTED\r\nmodel.reset_states()\r\nprint(model.predict(test))\r\nprint(model.predict(test))\r\n# [[[0.23247536 0.8585994 ]]]\r\n# [[[0.23247536 0.8585994 ]]]\r\n\r\n# This generates the same prediction after resetting the state, as expected\r\nmodel.reset_states()\r\nprint(model.predict(test))\r\nmodel.reset_states()\r\nprint(model.predict(test))\r\n# [[[0.23247536 0.8585994 ]]]\r\n# [[[0.23247536 0.8585994 ]]]\r\n```\r\n\r\nAs you can see, when the initial state is specified (second model), then the prediction is the same for both time steps (when using the same input at each time). When the initial state is not specified (first model), we see the expected behavior of different predictions at each time step."", 'This still fails with Tensorflow 2.0.0-rc0.\r\n\r\n@gowthamkpr would you mind labeling this issue so this bug might get more visibility?', 'See https://github.com/tensorflow/tensorflow/issues/32299']",[],"['stateful=True', 'initial_state=', '.call()']",0,0
370,keras,13391,closed,[tf2.0.0 keras2.3.0] keras.layers.GRU incorrect output of model.fit_generator trying to run Francois Chollet's notebook #32987,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): built from source
- TensorFlow version (use command below): 2.0.0 (i.e. the release)
- Keras version: 2.3.0
- Python version: 3.7 conda
- Bazel version (if compiling Tensorflow from source): 0.26.1
- GCC/Compiler version (if compiling Tensorflow from source): 7.4.0
- CUDA/cuDNN version: 10 / 7.6.4
- GPU model and memory: RTX 2080 Ti and Tesla V100 (tried on both. error occurs on both)

**Describe the current behavior**
I am going through Francois Chollet's book ""Deep Learning with Python"" and running the code in his Jupyter Notebooks with Tensorflow 2.0.0 as a backend to Keras 2.3.0.  Notebook 6.3, (under the heading ""1.6 Using recurrent dropout to fight overfitting"") has a model with a tensorflow.keras.layers.GRU(32, dropout=0.2, recurrent_dropout=0.2, input_shape=(None, float_data.shape[-1])).  The data is read earlier in the notebook from jena_climate_2009_2016.csv. I get a loss of 699013271268870062080.0000 after the first epoch and similar figures after subsequent epochs.  This figure is simply wrong (see below).  The original notebook (from Francois Chollet) is here: [link to github](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.3-advanced-usage-of-recurrent-neural-networks.ipynb) and includes the correct output.

**Describe the expected behavior**
The loss after 1 or 2 epochs is supposed to be around 0.3 

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Download the data as follows:
cd ~
mkdir Datasets
cd ~/Datasets
mkdir jena_climate
cd jena_climate
wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip
unzip jena_climate_2009_2016.csv.zip

Run jupyter notebook and load the notebook in a Python 3.7 environment with tensorflow 2.0.0 as the backend and keras 2.30.
Run each cell from the beginning of the notebook so you load the data and create the generators before you get to the example under heading 1.6.  Then try to run the example.  You will find that the loss is terribly wrong.

The code under heading ""1.7 Stacking recurrent layers"" also runs incorrectly.  The loss produced is ""nan"" and val_loss is ""nan"" (both should be around 0.3).  I think it is the same problem with layers.GRU

I have reproduced this problem running tensorflow.keras in tensorflow 2.0.0.
The problem also occurs running Keras 2.3.0 with a tensorflow 1.1.4 backend.
The problem does **not** occur with tensorflow.keras in tensorflow 1.1.4.",,['Have you figured out how to fix this problem? I have the same issue running the same code.'],[],[],0,0
371,keras,10784,closed,Wrong loading of weights on nested models,"Loading of nested models including BatchNormalisation and 1D convolutions fail with the following traceback:



I have been able to trim it down to a minimum example:

https://gist.github.com/Dapid/cd1be6e9d9d8d61f7f225544f0967e35

Inspecting the shapes, it seems like it is trying to load the bias vector of a conv1d on both the bias and kernel, and of course the shapes don't match. I have been unable to trace it back to the original place.
The model was trained on Tensorflow.

Possibly related:
https://github.com/keras-team/keras/issues/10777
Also, inspecting the shapes of the loading weights, it is possible that some batch normalisation is not being loaded, which would explain https://github.com/keras-team/keras/issues/10780 among others.",,"[""Dose this problem solved? I have the same error. I save a model which contains several nested model, and when load the model, comes the error: axes don't match array."", 'Sergey Ovchinnikov added on Slack:\r\n\r\n> I think its related to the issue we previously talked about in the google-mailing-list (I believe someone suggested a bug fix). it comes down to trainable weights and untrainable weights of BatchNorm and in the order in which they are saved/loaded. Bug in the get_weights() function. For each layer, the function returns first the trainable and then the untrainable weights (if you make one of the models a ""layer"", regardless of the layer order of that model, first the trainable, followed by untrainable weights are returned).', 'Curious about the status of this issue- also have been running into the same problem as above.\r\nDoes there exist any workaround outside of not using batchnorm layers within nested models at all?', 'This is still an issue and as far as I can tell it makes it impossible to save a GAN with batchnorm and continue training it later. With GANs being so popular I figured this would have been a priority to fix. ', ""Geez, if after trained the model weights can't be loaded back, then what's the point of training the model?? ""]","['\r\nTraceback (most recent call last):\r\n  File ""/home/david/research/keras_bug/save_resave.py"", line 45, in <module>\r\n    model = load_model(\'a{}.h5\'.format(i))\r\n  File ""/home/david/.virtualenvs/py36/lib/python3.6/site-packages/keras/engine/saving.py"", line 263, in load_model\r\n    load_weights_from_hdf5_group(f[\'model_weights\'], model.layers)\r\n  File ""/home/david/.virtualenvs/py36/lib/python3.6/site-packages/keras/engine/saving.py"", line 915, in load_weights_from_hdf5_group\r\n    reshape=reshape)\r\n  File ""/home/david/.virtualenvs/py36/lib/python3.6/site-packages/keras/engine/saving.py"", line 556, in preprocess_weights_for_loading\r\n    weights = convert_nested_model(weights)\r\n  File ""/home/david/.virtualenvs/py36/lib/python3.6/site-packages/keras/engine/saving.py"", line 544, in convert_nested_model\r\n    original_backend=original_backend))\r\n  File ""/home/david/.virtualenvs/py36/lib/python3.6/site-packages/keras/engine/saving.py"", line 556, in preprocess_weights_for_loading\r\n    weights = convert_nested_model(weights)\r\n  File ""/home/david/.virtualenvs/py36/lib/python3.6/site-packages/keras/engine/saving.py"", line 532, in convert_nested_model\r\n    original_backend=original_backend))\r\n  File ""/home/david/.virtualenvs/py36/lib/python3.6/site-packages/keras/engine/saving.py"", line 674, in preprocess_weights_for_loading\r\n    weights[0] = np.transpose(weights[0], (3, 2, 0, 1))\r\n  File ""/home/david/.virtualenvs/py36/lib/python3.6/site-packages/numpy/core/fromnumeric.py"", line 575, in transpose\r\n    return _wrapfunc(a, \'transpose\', axes)\r\n  File ""/home/david/.virtualenvs/py36/lib/python3.6/site-packages/numpy/core/fromnumeric.py"", line 52, in _wrapfunc\r\n    return getattr(obj, method)(*args, **kwds)\r\nValueError: axes don\'t match array\r\n\r\n']",[],0,0
372,keras,10938,closed,gumbel softmax in keras,"I'm trying to build a seq2seq model that generates sentences and classifies them. To do the latter, I want to feed the decoder's softmax output to a pre-trained classifier. This classifier however uses a Keras embedding layer so passing the raw softmax into the classifier isn't an option. I thought I could use a gumbel softmax to get a one-hot encoding then use the OneHotEmbedding layer I found here (https://github.com/keras-team/keras/issues/2505) to solve this.

Eric Jang provided this TensorFlow code for the gumbel softmax and I wanted to know how to turn this into a Keras layer. In particular, I am interested in the  property that ensures the vector on the forward pass is strictly categorical but on the backward pass, the gradient is the gumbel softmax output. Can anyone help with this please? 

Thanks.",,[],[],['hard'],0,0
373,keras,12383,closed,keras : with session.graph.as_default(): AttributeError: 'NoneType' object has no attribute 'graph',"Please make sure that the boxes below are checked before you submit your issue.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:


- [x] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",type:support,"[""I am trying  to run some code on Deep embedding clustering on mnist with the help of Keras , however, I get the following error\r\n\r\n    from keras.datasets import mnist\r\n    import numpy as np\r\n    import keras.backend as K\r\n    from keras.engine.topology import Layer, InputSpec\r\n    from keras.layers import Dense, Input\r\n    from keras.models import Model\r\n    from keras.optimizers import SGD\r\n    from keras import callbacks\r\n    from keras.initializers import VarianceScaling\r\n    from sklearn.cluster import KMeans\r\n    \r\n    \r\n    def autoencoder(dims, act='relu', init='glorot_uniform'):\r\n      \r\n        n_stacks = len(dims) - 1\r\n        # input\r\n        input_img = Input(shape=(dims[0],), name='input')\r\n        x = input_img\r\n        # internal layers in encoder\r\n        for i in range(n_stacks-1):\r\n            x = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(x)\r\n    \r\n        # hidden layer\r\n        encoded = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(x)  # hidden layer, features are extracted from here\r\n    \r\n        x = encoded\r\n        # internal layers in decoder\r\n        for i in range(n_stacks-1, 0, -1):\r\n            x = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(x)\r\n    \r\n        # output\r\n        x = Dense(dims[0], kernel_initializer=init, name='decoder_0')(x)\r\n        decoded = x\r\n        return Model(inputs=input_img, outputs=decoded, name='AE'), Model(inputs=input_img, outputs=encoded, name='encoder')\r\n    \r\n    \r\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n    \r\n    x = np.concatenate((x_train, x_test))\r\n    y = np.concatenate((y_train, y_test))\r\n    x = x.reshape((x.shape[0], -1))\r\n    x = np.divide(x, 255.)\r\n    n_clusters = len(np.unique(y))\r\n    \r\n    kmeans = KMeans(n_clusters=n_clusters, n_init=20, n_jobs=4)\r\n    y_pred_kmeans = kmeans.fit_predict(x)\r\n    \r\n    dims = [x.shape[-1], 500, 500, 2000, 10]\r\n    init = VarianceScaling(scale=1. / 3., mode='fan_in',\r\n                               distribution='uniform')\r\n    pretrain_optimizer = SGD(lr=1, momentum=0.9)\r\n    pretrain_epochs = 300\r\n    batch_size = 256\r\n    save_dir = './results'\r\n    autoencoder, encoder = autoencoder(dims, init=init)\r\n    \r\n    autoencoder.compile(optimizer=pretrain_optimizer, loss='mse')\r\n    autoencoder.fit(x, x, batch_size=batch_size, epochs=pretrain_epochs) #, callbacks=cb)\r\n    autoencoder.save_weights(save_dir + '/ae_weights.h5')\r\n    \r\n    class ClusteringLayer(Layer):\r\n      \r\n   \r\n    \r\n        def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\r\n            if 'input_shape' not in kwargs and 'input_dim' in kwargs:\r\n                kwargs['input_shape'] = (kwargs.pop('input_dim'),)\r\n            super(ClusteringLayer, self).__init__(**kwargs)\r\n            self.n_clusters = n_clusters\r\n            self.alpha = alpha\r\n            self.initial_weights = weights\r\n            self.input_spec = InputSpec(ndim=2)\r\n    \r\n        def build(self, input_shape):\r\n            assert len(input_shape) == 2\r\n            input_dim = input_shape[1]\r\n            self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\r\n            self.clusters = self.add_weight((self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\r\n            if self.initial_weights is not None:\r\n                self.set_weights(self.initial_weights)\r\n                del self.initial_weights\r\n            self.built = True\r\n    \r\n        def call(self, inputs, **kwargs):\r\n           \r\n            q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\r\n            q **= (self.alpha + 1.0) / 2.0\r\n            q = K.transpose(K.transpose(q) / K.sum(q, axis=1)) # Make sure each sample's 10 values add up to 1.\r\n            return q\r\n    \r\n        def compute_output_shape(self, input_shape):\r\n            assert input_shape and len(input_shape) == 2\r\n            return input_shape[0], self.n_clusters\r\n    \r\n        def get_config(self):\r\n            config = {'n_clusters': self.n_clusters}\r\n            base_config = super(ClusteringLayer, self).get_config()\r\n            return dict(list(base_config.items()) + list(config.items()))\r\n    \r\n    \r\n    clustering_layer = ClusteringLayer(n_clusters, name='clustering')(encoder.output)\r\n    model = Model(inputs=encoder.input, outputs=clustering_layer)\r\n    model.compile(optimizer=SGD(0.01, 0.9), loss='kld')\r\n    y_pred_last = np.copy(y_pred_kmeans)\r\n    model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\r\n    \r\n    # computing an auxiliary target distribution\r\n    def target_distribution(q):\r\n        weight = q ** 2 / q.sum(0)\r\n        return (weight.T / weight.sum(1)).T\r\n    loss = 0\r\n    index = 0\r\n    maxiter = 8000\r\n    update_interval = 140\r\n    index_array = np.arange(x.shape[0])\r\n    \r\n    tol = 0.001 # tolerance threshold to stop training\r\n    for ite in range(int(maxiter)):\r\n        if ite % update_interval == 0:\r\n            q = model.predict(x, verbose=2 )\r\n            p = target_distribution(q)  # update the auxiliary target distribution p\r\n    \r\n            # evaluate the clustering performance\r\n            y_pred = q.argmax(1)\r\n            if y is not None:\r\n                acc = np.round(metrics.acc(y, y_pred), 5)\r\n                nmi = np.round(metrics.nmi(y, y_pred), 5)\r\n                ari = np.round(metrics.ari(y, y_pred), 5)\r\n                loss = np.round(loss, 5)\r\n                print('Iter %d: acc = %.5f, nmi = %.5f, ari = %.5f' % (ite, acc, nmi, ari), ' ; loss=', loss)\r\n    \r\n            # check stop criterion - model convergence\r\n            delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\r\n            y_pred_last = np.copy(y_pred)\r\n            if ite > 0 and delta_label < tol:\r\n                print('delta_label ', delta_label, '< tol ', tol)\r\n                print('Reached tolerance threshold. Stopping training.')\r\n                break\r\n        idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\r\n        loss = model.train_on_batch(x=x[idx], y=p[idx])\r\n        index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\r\n    \r\n    model.save_weights(save_dir + '/DEC_model_final.h5')\r\n    \r\n    model.load_weights(save_dir + '/DEC_model_final.h5')\r\n\r\n  \r\n\r\nthe error: \r\n  \r\n\r\n  \r\n\r\n>    with session.graph.as_default():\r\n>    AttributeError: 'NoneType' object has no attribute 'graph'\r\n\r\n(the problem might be in saving the model but I can't figure out why I am wrong.) my code runs perfectly in jupyter notebook but I can't run it in an editor like pycharm .please help.\r\n\r\n\r\n""]",[],['pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps'],0,0
374,keras,13504,closed,Adding the Waymo Open Dataset to keras.datasets,"Hi,

I work on the [Waymo Open Dataset](https://waymo.com/open/). We'd like to make it as easy as possible for users to get up and running with it.

How might we go about adding it to keras.datasets?

Thank you!",,[],[],[],0,0
375,keras,8065,closed,"ResNet50 from tf.keras will not fine-tune, but the one from base keras will","- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [X] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [X] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I'm excluding the code where I load and preprocess (imagenetutils.preprocess_input). My data is the Kaggle cats-and-dogs data.



This never gets beyond 50-60% accuracy. VGG16 quickly gets to 98%+. If I remove ""tensorflow.contrib.keras.api."" from the start of all imports, this also gets to 98%+. So something is definitely different between tf and base Keras. I'm just not sure what.",,[],"[""\r\nfrom tensorflow.contrib.keras.api.keras import backend as K\r\n...\r\n\r\nnet = ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\r\nfor layer in net.layers:\r\n    layer.trainable = False\r\n \r\noutput = Flatten(name='my_flatten')(net.output)\r\noutput = Dense(num_categories, activation='softmax', name='my_dense')(output)\r\nsc1 = Model(inputs=net.inputs, outputs=output, name='sc1')\r\nsc1.compile(\r\n    loss=keras.losses.categorical_crossentropy,\r\n    optimizer=keras.optimizers.Adam(lr=1e-4),\r\n    metrics=[metrics.categorical_accuracy])\r\n""]",[],0,0
376,keras,10058,closed,Non training Batch Norm operator has bad performance for it running into tensorflow's non fused batch norm API,"In current Keras code with Tensorflow as it’s backend, the non-training batch norm operator/layer will finally run into Tensorflow’s old non-fused batch norm API (tf.nn.batch_normalization), which result in bad performance in both CPU and GPU.  
Tensorflow’s tf.nn.batch_normalization is non-fused batch norm, does computations using several individual Ops, While Tensorflow’s tf.nn.fused_batch_norm is fused batch norm which is just a new implementation that comprise several ops into one, has much better performance.

Following code shows the place where calling to Tensorflow's non fused batch norm happen, and suggest to call Tensorflow's fused Batch Norm API.
output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta 

Thanks.",,[],[],"['def batch_normalization(x, mean, var, beta, gamma, epsilon=1e-3):\r\n    """"""Applies batch normalization on x given mean, var, beta and gamma.\r\n\r\n    I.e. returns:\r\n    ', '\r\n\r\n    # Arguments\r\n        x: Input tensor or variable.\r\n        mean: Mean of batch.\r\n        var: Variance of batch.\r\n        beta: Tensor with which to center the input.\r\n        gamma: Tensor by which to scale the input.\r\n        epsilon: Fuzz factor.\r\n\r\n    # Returns\r\n        A tensor.\r\n    """"""\r\n    return tf.nn.batch_normalization(x, mean, var, beta, gamma, epsilon) ###Change to call tf.nn. batch_normalization() for better performance']",0,0
377,keras,7681,closed,Getting different result while training an mlp with LSTM layer on Keras,"I am using keras with tensorflow backend. I am creating a simple 1 layer LSTM model. I need my code to give same val_loss every time I train on the same data. I am running my system on CPU only. I tried:


from numpy.random import seed
seed(1337)
from tensorflow import set_random_seed
set_random_seed(1337)

on the top of my code.
I also initialized all the kernels and recurrent as ones:

model.add(LSTM(150,input_shape=(None,124), W_regularizer=l2(0.001),kernel_initializer='ones', recurrent_initializer='ones', bias_initializer='ones'))
model.add(Dense(2,kernel_initializer='ones', bias_initializer='ones'))
model.add(Activation(""softmax""))

I also set shuffle=False in model.fit(). I am using rms for optimizing.

I also set PYTHONHASHSEED to 0. But still I am getting different train loss as well as val loss on each epoch when rum multiple times.

I  am running keras on a server with 56 cpu cores and CentOS.

Pls help soon as I have tried everything everyone has suggested in other threads !!!!!",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],0,0
378,keras,12602,closed,Error when checking target: expected activation_1 to have 3 dimensions,"When I use the model with  and target values of shape  when running  the method runs fine. But when switching to  with target values of shape  the method raises this error:
    
    Error when checking target: expected activation_1 to have 3 dimensions, but got array with shape (20, 10)

Here is the model that I use:

    inp = Input(batch_shape=(batch_size, seq_size))
    mat = Embedding(num_classes, num_lstms)(inp)
    mat = LSTM(num_lstms, return_sequences=True, stateful=True)(mat)
    mat = LSTM(num_lstms, return_sequences=True, stateful=True)(mat)
    mat = Dropout(0.5)(mat)
    mat = TimeDistributed(Dense(vocab_size))(mat)
    out_1 = Activation('softmax')(mat)
    model = Model(inputs=[inp], outputs=[out_1])",type:support,"['Your input layer is recieving two parameters but it requires three check what are you passing through the layer (shapes of labels and inputs) are they of the desired format. if not consider adding a new channel to your inputs using np.newaxis(.,.,.,....) and add suitable parameter to the input in newaxis by performing numpy operations. That would do the job']",[],"['categorical_crossentropy', '(batch_size, seq_size, vocab_size)', 'fit_generator', 'sparse_categorical_crossentropy', '(batch_size, seq_size, vocab_size)']",0,0
379,keras,13343,closed,Unable to extract layer output after pushing layers back to model,"I have a classification network with last two layers as Dense and activation respectively. I popped the layers from the model as :

    activationlayer=classifier.layers.pop()
    denselayer=classifier.layers.pop()

Then I pushed the layers back into the model:

    outt1=denselayer(classifier.layers[-1].output)
    outt=activationlayer(outt1)
    classifier=Model(classifier.input,outt)

Now while indexing the output of dense layer I get error:

    classifier.layers[-2].output

    Error: Layer out2 has multiple inbound nodes, hence the notion of ""layer output"" is ill-defined.",type:support,"['If you are using keras v2.x then `classifier.layers.pop()` is not actually removing any layer from your model. It will just return you the element popped from the end of layers array. When you added 2 more layers, you just added more node paths to existing ones. Therefore the error.\r\n\r\nUse `classifier.pop()` to remove last layer instead.']",[],[],0,0
380,keras,7728,closed,"Suggestion on implementing ""group deconvolution3D"" layer","Hi all:

Below is my naive way to implement a group deconvolution3d layer in Keras 2.0+theano. Can you please give me suggestion on it? (e.g. better solution)

1. In theano,  is used to compute the default deconvolution. It actually can take the ""number of groups"" as an argument
[https://github.com/Theano/Theano/blob/master/theano/tensor/nnet/abstract_conv.py#L1163](url)

2. in keras2.0, 

- update  : add  based on  
- update : add  based on , call   and fix the output dimension calculation

Thanks
Donglai",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],"['T.nnet.abstract_conv.AbstractConv3d_gradInputs', 'backend/theano_backend.py', 'def K.conv3d_transpose_ngroup', 'K.conv3d_transpose', 'layers/convolutional.py', 'class Conv3DTranspose_ngroup', 'class Conv3DTranspose', 'K.conv3d_transpose_ngroup']",0,0
381,keras,10611,closed,Fix Adam Optimizer to Implement Paper Correctly,"Adam was recently demonstrated to be implemented incorrectly in several packages including Keras.  Propose to fix the optimizer using method described here:

https://arxiv.org/pdf/1711.05101.pdf",,"[""1. Keras doesn't call it weight decay, it is explicitly called ``l2`` regularization (see https://keras.io/regularizers/) which is indeed accurate.\r\n\r\n> While common deep learning frameworks of these algorithms implement L2 regularization (often calling it “weight decay” in what may be misleading due to the inequivalence we expose).\r\n\r\nKeras calls it ``l2`` regularization.\r\n\r\n2. https://arxiv.org/pdf/1705.08292.pdf"", ""I don't think that's right.\n\nhttps://keras.io/optimizers/#adam\n\n   - *decay*: float >= 0. Learning rate decay over each update.\n\n\n\nRegarding 2, Im not sure what this has to do with the conversation.  Did\nyou mean to say that Adam doesnt generalize as well as SGD?  That's besides\nthe point.\n\nOn Thu, Jul 5, 2018 at 5:41 PM, brge17 <notifications@github.com> wrote:\n\n>\n>    1.\n>\n>    Keras doesn't call it weight decay, it is explicitly called l2\n>    regularization (see https://keras.io/regularizers/) which is indeed\n>    accurate.\n>    2.\n>\n>    https://arxiv.org/pdf/1705.08292.pdf\n>\n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/keras-team/keras/issues/10611#issuecomment-402860787>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ALarq1UIB0nW3tWl5hBr05Z7VhP0dsHMks5uDogdgaJpZM4VETgP>\n> .\n>\n"", ""Learning rate decay != weight decay.\r\n\r\nThey just aren't the same thing."", 'Is the optimizer fixed now?']",[],[],0,0
382,keras,2370,closed,feature request: more backends,"could we support or do we plan to support more backends?
Theano can only support multiple GPU using OpenCL but not CUDA, tensorflow seems slower than other framework using GPU for now.

Could we support Caffe, Leaf, MXNET, CNTK or Torch7?
![image](https://cloud.githubusercontent.com/assets/11470826/14587597/322ccb2e-04e8-11e6-8f82-b4759b5577fe.png)
![image](https://cloud.githubusercontent.com/assets/11470826/14587599/39d0fb2a-04e8-11e6-8300-be794b350bf2.png)
",,"['+1\n', 'I think MXNet and CNTK would be the interesting ones to have. You guys want to work on that?\n', ""Consider also using neon backend: https://github.com/NervanaSystems/neon\nIt actually seems quite nice, compilation is lazy (no need to wait for graph to compile) and documentation at least so far seems to be quite decent. As a bonus it seems like this backend is [the fastest](https://github.com/soumith/convnet-benchmarks) so far at least with popular CV deep nets :D \nHere is a minimal example of neon that demonstrates some of its features:\n\n```\nfrom neon.backends import gen_backend, Autodiff\nimport numpy as np\n\n# decide where to run (gpu/cpu)\nbe = gen_backend('cpu') \n\n# create variables\nx0, x1 = be.empty((2,2)), be.empty((2,2))\n\n# construct computational graph\nf = x0 * x0 + x0 * x1\n\n# initialize variables\nx0[:] = np.random.rand(2,2);\nx1[:] = 0.0;\n\n# return result as np array (could return variable of course)\nprint f.asnumpyarray()\n\n# do autodifferentiation\nad = Autodiff(op_tree=f, be=be, next_error=None)\n\n# compute the gradient w.r.t. some variables\nprint ad.get_grad_asnumpyarray([x0, x1])\n```\n"", 'Yeah, at least with neon backend I could try contributing, I am not sure however how soon that would happen. Lets say if no one else does it I do it (if neon backend is interesting), but I cannot promise anything w.r.t. time as I am already busy with all kinds of exciting things :D\n', 'I am not familiar with MXNet and CNTK though\n', 'Just looked for examples of CNTK usage in python, google did not reveal any so far. What Microsoft say is that [python CNTK will be available in future releases](http://research.microsoft.com/pubs/226641/CNTK-Tutorial-NIPS2015.pdf). Thus it seems that it  makes sense to consider MXNet only for extra backend as of now.\n', ""CNTK isn't available in python yet\nThey probably will later this year\n\nWhat would be more interesting would be to see if the new cuDNN RNN implementation can be ported into theano/tf and keras\n"", '@fchollet : from other issues it seems that neon backend is actually interesting. Do you know if someone is already working on it and if there are already any results? \n', ""Doesn't neon support GPU only in payed licenses?\n"", 'Neon supports GPU in free license. Only supports multi-GPU in paid.\n', 'Ah, you are right - it was multi-gpu :)\n', ""I'll take a look at MXNet\n"", 'We can and want to wrap the rnn implementation from cudnn. But we don\'t\nknow when it will be done.\nLe 18 avr. 2016 05:28, ""lemuriandezapada"" notifications@github.com a\nécrit :\n\n> CNTK isn\'t available in python yet\n> They probably will later this year\n> \n> What would be more interesting would be to see if the new cuDNN RNN\n> implementation can be ported into theano/tf and keras\n> \n> —\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly or view it on GitHub\n> https://github.com/fchollet/keras/issues/2370#issuecomment-211293802\n', 'Cross-referencing... CNTK backend. https://github.com/Microsoft/CNTK/issues/797\n', '@fchollet More backend would be awesome. Do you have an estimated timeline when would the CNTK and MXNet be added into the keras backend? Thank you. \n', 'Hi! Curious as to whether anyone has followed up on integrating CNTK/other backends with Keras.', ""There's now a CNTK backend "", 'My company Vertex.AI uses Keras internally for model development and we are actively working on OpenCL support: http://vertex.ai/blog/bringing-deep-learning-to-opencl', 'Is there such a thing as a ""how to integrate a backend"" document? Or a way to translate models / data from an ""arbitrary"" deep learning library into TensorFlow or CNTK? I\'m thinking mostly about `caffe` for now, but there are others out there.', ""We've just open sourced PlaidML which includes a new Keras backend with OpenCL support: http://vertex.ai/blog/announcing-plaidml\r\n\r\n@fchollet Now that our source is open we'll see what we can do to make our integration with Keras cleaner."", ""@choongng I'm planning to test this over the weekend."", ""It would be nice to have an extension mechanism in keras that doesn't involve monkey patching (which is what @choongng and the rest of the Vertex.AI folks are doing) or submitting a pull request upstream. pytest and other projects support plugins via setuptools entry_points. Right now PlaidML monkey patching [does not work in Python 3](https://github.com/plaidml/plaidml/issues/20#issuecomment-339192660) so it would be nice to have a cleaner method. What do others think?"", 'Tracking ticket for Keras2-MXNet backend\r\nhttps://github.com/keras-team/keras/issues/8697']",[],[],0,0
383,keras,10923,closed,Cannot modified established architecture shape.,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

Hello,
I collected several models in the .json format which I import using model_from_json function.
These models have different inputs and outputs (tensors) in the respective first and final layers. (Some of these models are not Sequential). In my API the user can select the input_shape and data_format coming from the flow generator. So, I need to modify the architecture shape, in order to take data in the format the user specified. However, after spending the two days looking for alternative ways I could not find an effective way of changing the architecture input layer. Keras supposed to be intuitive. xP Hereby, I'll give an example of a small model architecture generated by me and some ideas I've tried. Honestly, I really appreciate some help with this.

Model:


Goal:
Make it receive batch data in the shape (None,32,32,3), 'channels_last'
Make it output batch data in the shape (None,6)

Attempted:
1. Using set shape
 
2. Using config

3. Changing tensor directly
def changeInputsShape(model, inputs):
   assert len(model.inputs) == len(inputs), ""Model inputs and assigned inputs mismatch.""

   for i in range(len(inputs)):
     model.inputs[i] = keras.layers.Input(shape=inputs[i])

Reason:
1. set_shape only update non defined dimensions of the previously defined tensor shape.
2. setting configuration does not update input tensors themselves, compiles not run.
3. with model.build() prior to compiling: 
3. with compiling: 

Note: This is note a weights/ knowledge tranfer problem since the model is not trained so it should be relatively simpler. I guess not.

Any help would be appreciated,
Andre
",,"['Hello,\r\nI manage to try other several approaches and the one working the best to change the input shape is very simple actually:\r\n```\r\nfrom keras.models import clone_model\r\n    newmodel = clone_model(model,Input(batch_shape=(None,32,32,3)))\r\n#(There are some dimensions which will fail due to the depth of the convolution but that is ok)\r\n```\r\n\r\nNow, I really need a way to find to change the number of classes since its only discovered in the initialization of the flow generator. I would welcome some pointers. Andre']","[""\r\nmodel = Sequential()\r\nmodel.add(Conv2D(32, (3, 3), padding='same', input_shape = (100,100,3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(Conv2D(32, (3, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Dropout(0.25))\r\n\r\nmodel.add(Conv2D(64, (3, 3), padding='same'))\r\nmodel.add(Activation('relu'))\r\nmodel.add(Conv2D(64, (3, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Dropout(0.25))\r\n\r\nmodel.add(Flatten())\r\nmodel.add(Dense(512))\r\nmodel.add(Activation('relu'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(2))\r\nmodel.add(Activation('softmax'))\r\n"", ""\r\nlayer = model.layers[0]\r\nconfig = layer.get_config()\r\nconfig['input_shape']=(32,32,3)\r\nconfig['batch_input_shape']=(None,32,32,3)\r\nmodel.layers[0] = layers.deserialize({'class_name': layer.__class__.__name__, 'config': config})\r\nmodel.inputs[0] = keras.layers.Input(shape=(32,32,3))\r\n""]","['model.layers[0].input.set_shape((None,32,32,3))', 'Graph disconnected: cannot obtain value for tensor Tensor(""conv2d_1_input:0"", shape=(?, 100, 100, 3), dtype=float32) at layer ""conv2d_1_input"". The following previous layers were accessed without issue: []', 'Could not compute output Tensor(""activation_6/Softmax:0"", shape=(?, 2), dtype=float32)']",0,0
384,keras,10388,closed,flow_from_directory error when use multi gpus,"I use Horovod Keras from my multi gpus job, and I found that when I use multi gpus and each process will count the image num by use flow_from_directory, but the image num will be different:
![](https://user-images.githubusercontent.com/3112825/41193236-f8df388a-6c3b-11e8-8ee7-2b7ac54a2c34.png)
here is the code:



Could anyone help me with it ?  It's so strange, and maybe caused by multiprocessing ?",,[],"['\r\ntrain_gen = image.ImageDataGenerator(width_shift_range=config.width_shift_range, height_shift_range=config.height_shift_range, zoom_range=config.zoom_range, horizontal_flip=config.horizontal_flip, vertical_flip=config.vertical_flip, preprocessing_function=preprocess_image)\r\n#train_gen = image.ImageDataGenerator(width_shift_range=config.width_shift_range, height_shift_range=config.height_shift_range, zoom_range=config.zoom_range, horizontal_flip=config.horizontal_flip, vertical_flip=config.vertical_flip)\r\ntrain_iter = train_gen.flow_from_directory(config.train_dir, batch_size=config.batch_size, target_size=config.target_size)\r\n']",[],0,0
385,keras,10689,closed,ModelCheckpoint doesn't work sometimes.,"Sometimes ModelCheckpoint doesn't save any model at all! Why?

",,[],"['\r\nCode:\r\n\r\ndef lstm2(do=0.2, size=64, MAX_LEN=MAX_LEN, NUM_WORDS=NUM_WORDS, EMBED_SIZE=EMBED_SIZE):\r\n    model = Sequential()\r\n    model.add(Embedding(NUM_WORDS,EMBED_SIZE,weights=[word2vec_matrix],trainable=False,input_length=MAX_LEN))\r\n    model.add(Dropout(do))\r\n    model.add(LSTM(size, return_sequences=False))\r\n    model.add(Dense(1, activation=\'sigmoid\'))\r\n\r\n    model.compile(\r\n        loss=\'binary_crossentropy\',\r\n        optimizer=\'adam\',\r\n        metrics=[\'accuracy\'])\r\n\r\n    return model\r\n\r\nfrom keras.callbacks import *\r\nmodel_name = ""RNN_BI_best.hdf5""\r\nmycallbacks = [\r\n    EarlyStopping(monitor=\'val_loss\', min_delta=0, patience=6, verbose=0, mode=\'auto\'),\r\n    ModelCheckpoint(model_name, save_best_only=True, save_weights_only=False),\r\n    ReduceLROnPlateau(patience=3, verbose=0, factor=1/2, min_lr=0.0001), \r\n    plot_losses,\r\n    TQDMNotebookCallback()\r\n]\r\n\r\n%%time\r\n\r\nmodel = lstm2()\r\n\r\nhistory = model.fit(\r\n    x_text_train,\r\n    y_train,\r\n    batch_size=64,\r\n    epochs=99,\r\n    validation_split=0.1,\r\n    verbose=0,\r\n    callbacks=mycallbacks)\r\n']",[],0,0
386,keras,11957,closed,Tensorboard histogram_freq switching from GPU to CPU to write logs,"Tensorflow GPU with Keras.

When I run Tensorboard with  where , it uses the CPU at the end of each epoch when Tensorboard writes to logs. 

Usually the GPU is used for the entirety of training and writing to logs and this is also true when . 

This [issue] (https://github.com/keras-team/keras/issues/3358) has been mentioned in previous questions but it is dissimilar in that their issue occurs when validation data is passed via a data generator and that histograms are not created - no reference to CPU or GPU. I do not use a data generator for validation and histograms are created on my end.

Notes: 

Training carried out by Keras .

Training dataset passed by .

Validation dataset is passed without data generator.

**Edit**

Switching Keras  out for  crashes when attempting to write to Tensorboard logs when  but runs fine when . The crash log is below. Notice it switches from GPU to CPU. Perhaps this could be something to work with?

     InvalidArgumentError: You must feed a value for placeholder tensor 'conv2d_1_input' with dtype float and shape [?,48,48,1]
	 [[{{node conv2d_1_input}} = Placeholder[dtype=DT_FLOAT, shape=[?,48,48,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
	 [[{{node dense_2/bias/read/_407}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_335_dense_2/bias/read"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]InvalidArgumentError: You must feed a value for placeholder tensor 'conv2d_1_input' with dtype float and shape [?,48,48,1]
	 [[{{node conv2d_1_input}} = Placeholder[dtype=DT_FLOAT, shape=[?,48,48,1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
	 [[{{node dense_2/bias/read/_407}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_335_dense_2/bias/read"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Is anyone aware of a fix to prevent CPU being used when  is enabled? I ask because the writing to logs on CPU can make the training process up to five times longer.",stat:contributions welcome type:bug/performance,"[""The current implementation of histogram_freq is really not optimal. This is one of the visible bugs. A PR has been made to fix this issue but it's been staled for a while. Let's hope a contributor is willing to work on that soon. #10513""]",[],"['histogram_freq = x', 'x != 0', 'histogram_freq = 0', 'fit_generator', 'ImageDataGenerator', 'fit_generator', 'fit', 'histogram_freq != 0', 'histogram_freq = 0', 'histogram_freq']",0,0
387,keras,8151,closed,Extract the weights of a layer,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",,"['I want to extract the weights of a layer.When i use save_weights and load_weights,I must use it in a model.This is not what i want. I need to extract the weights of the Dense layer and make it into a fully convolution  layers.Please tell me how to do it if you guys know.', 'https://github.com/fchollet/keras/issues/91', '@luciaL, hi, have you solved your problem? During training process, if I want to use the weights of one layer to another, and these two layers are in the same model but different part, do not use save_weights and load_weights, how can I do it? Thanks!\r\n']",[],[],0,0
388,keras,10239,closed,"ValueError: Error when checking target: expected dense_20 to have shape (3,) but got array with shape (22,)","I m getting this error::   
Here is my code

#Create The model
def baseline_model():
    model = Sequential()
    model.add(Dense(4, activation='relu', input_shape=(4,)))
    model.add(Dense(3, activation='sigmoid'))
    #compile the model
    model.compile(optimizer='adam', loss='categorical_crossentropy',metrics = ['accuracy'] )
    return model
#in order to fit the model
estimator = KerasClassifier(build_fn=baseline_model, nb_epoch=200, verbose=0, batch_size=5)
kfold = KFold(n=len(X), n_folds=10, shuffle=True, random_state=seed)
results = cross_val_score(estimator, X, dummy_y, cv=kfold)
print(""Accuracy: %.2f%% (%.2f%%)"" % (results.mean()*100, results.std()*100))",,"['your activation is ""sigmoid""- change that and it should work I think.']",[],[],0,0
389,keras,11465,closed,About principle of activity regularizer in Keras,"Dear keras users,

Hello everyone. I need some help about dealing with activity regularizer in keras.

As I know, the activity regularizer is the one that adding a 'norm of output' into a loss.

It is really similar to Tikhonov regularization, but is it really working well in the deep learning?

I doubt it because when differentiating the term of output norm loss with weight(w),  it becomes zero.

Actually I am not majored in Computer science, I know that I do not know that much.

So I want to ask some help know the basic principle of it.

Or, can you recommend me some related papers?

Thank you so much.  
",type:support,[],[],[],0,0
390,keras,10703,closed,Is there any way to set different learning rates for different different layers of a CNN model in Keras?,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",,[],[],[],0,0
391,keras,13150,closed,Tensorboard error with histogram_freq > 0,"<em>Please make sure that this is a Bug or a Feature Request and provide all applicable information asked by the template.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.</em>  

**System information**  
- Have I written custom code (as opposed to using example directory):  
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  1.12.0
- Keras version:  2.2.4
- Python version:  3.5.4
- CUDA/cuDNN version:  8.0, V8.0.61
- GPU model and memory:   NVIDIA Pascal P100

**Describe the current behavior**  
Getting error


tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'input_1' with dtype float and shape [?,1,512,20]
	 [[{{node input_1}} = Placeholder [dtype=DT_FLOAT, shape=[?,1,512,20], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

**Code to reproduce the issue**  
Provide a reproducible test case that is the bare minimum necessary to generate the problem.  
base_network


**Other info / logs**  

File ""testKerasClean.py"", line 329, in <module>
    history=model.fit([input_a,input_b],output_trg,batch_size=100,shuffle=True,epochs=100,class_weight=class_weight_val,validation_split=0.2,verbose=1,callbacks=[TensorBoard(log_dir='./Graph',write_graph=True,write_grads=True,histogram_freq=3)])
  File ""/project/6000341/saby2k13/DPPI2/venv/tensorflow/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py"", line 1639, in fit
    validation_steps=validation_steps)
  File ""/project/6000341/saby2k13/DPPI2/venv/tensorflow/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py"", line 233, in fit_loop
    verbose=0)
  File ""/project/6000341/saby2k13/DPPI2/venv/tensorflow/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py"", line 439, in test_loop
    batch_outs = f(ins_batch)
  File ""/project/6000341/saby2k13/DPPI2/venv/tensorflow/lib/python3.5/site-packages/tensorflow/python/keras/backend.py"", line 2986, in __call__
    run_metadata=self.run_metadata)
  File ""/project/6000341/saby2k13/DPPI2/venv/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1439, in __call__
    run_metadata_ptr)
  File ""/project/6000341/saby2k13/DPPI2/venv/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 528, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'input_1' with 
dtype float and shape [?,1,512,20]
	 [[{{node input_1}} = Placeholder \[dtype=DT_FLOAT, shape=[?,1,512,20], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]",,[],[],"['', '\r\n-A module with five CONV-BATCHNNORM-RELU layers\r\nbase_network = create_base_network((1,512,20)) \r\n\r\ninput_b_mod = Input(shape=(1,512,20))\r\ninput_a_mod = Input(shape=(1,512,20))\r\n\r\n-because we re-use the same instance ', "",\r\n- the weights of the network\r\n- will be shared across the two branches\r\nprocessed_a = base_network(input_a_mod)\r\nprocessed_b = base_network(input_b_mod)\r\n\r\nlin_net1 = Sequential(name='freezing_layer1')\r\nlin_net1.add(Dense(512,bias_initializer='zeros',kernel_initializer=x))\r\nlin_net1.add(BatchNormalization(epsilon=1e-5,momentum=0.1,gamma_initializer=gamma1T,beta_initializer='zeros'))\r\nlin_net1.add(Activation('relu'))\r\n\r\nlin_net2 = Sequential(name='freezing_layer2')\r\nlin_net2.add(Dense(512,bias_initializer='zeros',kernel_initializer=y))\r\nlin_net2.add(BatchNormalization(epsilon=1e-5,momentum=0.1,gamma_initializer=gamma2T,beta_initializer='zeros'))\r\nlin_net2.add(Activation('relu'))\r\n\r\nlin_net1_config = lin_net1.get_config()\r\nlin_net1_config['name'] = 'freezing_layer3'\r\nprint('lin_net1_config',lin_net1_config['name'])\r\nlin_net2_config = lin_net2.get_config()\r\nlin_net2_config['name'] = 'freezing_layer4'\r\nlin_net1c = Sequential.from_config(lin_net1_config)\r\nlin_net2c = Sequential.from_config(lin_net2_config)\r\n\r\nlin_net1c.layers[0].set_weights(lin_net1.layers[0].get_weights())\r\nlin_net2c.layers[0].set_weights(lin_net2.layers[0].get_weights())\r\nlin_net1c.layers[1].set_weights(lin_net1.layers[1].get_weights())\r\nlin_net2c.layers[1].set_weights(lin_net2.layers[1].get_weights())\r\nlin_net1c.layers[2].set_weights(lin_net1.layers[2].get_weights())\r\nlin_net2c.layers[2].set_weights(lin_net2.layers[2].get_weights())\r\n\r\n\r\nprocessed_a_1 = lin_net1(processed_a)\r\nprocessed_a_2 = lin_net2(processed_a)\r\nprocessed_b_1 = lin_net2c(processed_b)\r\nprocessed_b_2 = lin_net1c(processed_b)\r\n\r\nmerged_a  = Concatenate()([processed_a_1, processed_a_2])\r\nmerged_b  = Concatenate()([processed_b_1, processed_b_2])\r\n\r\nmerged_a = Flatten()(merged_a)\r\nmerged_b = Flatten()(merged_b)\r\nmulti_a_b = Multiply()([merged_a,merged_b])\r\nn=1024\r\nval = 1/math.sqrt(n)\r\nzData = np.random.uniform(low=-val,high=val,size=(1024,1))\r\nz = tf.constant_initializer(zData)\r\nmulti_a_b_dense = Dense(1,input_shape=(1024,),bias_initializer='zeros',kernel_initializer=z) (multi_a_b)\r\nout = Activation('sigmoid')(multi_a_b_dense)\r\nclass_weight_val = {-1.:1.,1.:10.}\r\nK.clear_session\r\nmodel.fit([input_a,input_b],output_trg,batch_size=100,shuffle=True,epochs=100,class_weight=class_weight_val,validation_split=0.2,verbose=1,callbacks=[TensorBoard(log_dir='./Graph',write_graph=True,write_grads=True,histogram_freq=0)])\r\n"", '']",0,0
392,keras,13597,closed,"ValueError: Arguments and signature arguments do not match. got: 13, expected: 14 ","**System information**  
- Have I written custom code (as opposed to using example directory):  

Yes

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  
Windows 10

- TensorFlow backend (yes / no):

Yes
  
- TensorFlow version:  

2.0

- Keras version:  

2.3.1

- Python version:  

3.7

- CUDA/cuDNN version:  

None - CPU only

**Describe the current behavior**  

I am attempting to generate confidence intervals using dropout as per logic in [here](https://medium.com/hal24k-techblog/how-to-generate-neural-network-confidence-intervals-with-keras-e4c0b78ebbdf) 

unfortunately I keep receiving (now) the error above?


**Describe the expected behavior**  

Expect a new model to be initialised from the saved config.

**Code to reproduce the issue**  
Provide a reproducible test case that is the bare minimum necessary to generate the problem.  



Adding a note in to mention that my train/test/forecast data is generated via tf.data.Dataset:

`


Thank you for your help!
",type:support,"['I have the same issue on macOS Catalina, using the fit_generator', ""I've fixed my issue by specifying target data (y) for both test and validation data."", 'still getting similar issue \r\n` self.agent_model.model.predict_on_batch(data)`\r\nValueError: Arguments and signature arguments do not match. got: 92, expected: 93 \r\n@claeszacho can u explain your solution a bit.\r\n', ""Using the fit_generator, I kept getting this issue when only specifying X data for both the training data and validation data. Since I'm working with unlabelled sequential data in an unsupervised training situation, I just set the same data for both X and Y in both the training and validation data.\r\n\r\nI could be a bit of a hack, and an incorrect way of doing things - I'm not sure, but my issue is gone and the results has been fine.\r\n\r\nI can't really tell for the example above, how `train_batch` is configured, but my guess will be that the fitting call should be changed to something like this.\r\n`\r\nmodel_history = model.fit(x=train_batch, y=train_batch \r\n                          validation_data = (test_batch, train_batch)\r\n                          epochs = 100,\r\n                          shuffle = False,\r\n                          callbacks = [early_stop, \r\n                                       model_checkpoint,\r\n                                       tensorboard_callback])\r\n`\r\n"", 'I\'m having a similar problem:\r\n\r\n`  File ""/project_path/venv/lib/python3.6/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper\r\n    return func(*args, **kwargs)\r\n  File ""/project_path/venv/lib/python3.6/site-packages/keras/engine/training.py"", line 1732, in fit_generator\r\n    initial_epoch=initial_epoch)\r\n  File ""/project_path/venv/lib/python3.6/site-packages/keras/engine/training_generator.py"", line 220, in fit_generator\r\n    reset_metrics=False)\r\n  File ""/project_path/venv/lib/python3.6/site-packages/keras/engine/training.py"", line 1514, in train_on_batch\r\n    outputs = self.train_function(ins)\r\n  File ""/project_path/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py"", line 3727, in __call__\r\n    outputs = self._graph_fn(*converted_inputs)\r\n  File ""/project_path/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1551, in __call__\r\n    return self._call_impl(args, kwargs)\r\n  File ""/project_path/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1591, in _call_impl\r\n    return self._call_flat(args, self.captured_inputs, cancellation_manager)\r\n  File ""/project_path/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1692, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File ""/project_path/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 527, in call\r\n    (len(args), len(list(self.signature.input_arg))))\r\nValueError: Arguments and signature arguments do not match. got: 37, expected: 39 \r\n`\r\n\r\n\r\nAnd this is the line where the error occurs:\r\n\r\n\r\n`            model.fit_generator(generator=batch_generator(X_train, y_train, batch_size, True), epochs=1,\r\n                                steps_per_epoch=np_steps),\r\n                                verbose=1,\r\n                                validation_data=batch_generator(X_valid, y_valid, batch_size, False),\r\n                                validation_steps=np_steps)`\r\n\r\nUsing:\r\n\r\n- Tensorflow-gpu: 2.1.0\r\n- Keras: 2.3.1\r\n- Ubuntu: 18.04 LTS', ""@ahmedbr The function ```batch_generator``` you're calling inside the ```fit_generator``` - is that a custom function, or is it part of the Keras? I'm not familiar.."", ""> @ahmedbr The function `batch_generator` you're calling inside the `fit_generator` - is that a custom function, or is it part of the Keras? I'm not familiar..\r\n\r\n@claeszacho Sorry for late reply. No it's custom function. The code snippet is available here: https://www.kaggle.com/c/talkingdata-mobile-user-demographics/discussion/22567"", ""@ahmedbr Can you check the shape for the X and Y values that ```batch_generator``` are yielding? My guess is that your issue is introduced due to some misalignment between the output shape of the different x/y sets. Either in the training data, validation data or between those two. But I'm not entirely sure."", '@claeszacho thanks for your suggestion but to me the problem doesn\'t seem to be related to X and y shapes. I re-run as following and still have the issue:\r\n\r\n```\r\nmodel.fit_generator(generator=batch_generator(X_train, y_train, divide_on, True), epochs=1,\r\n                                steps_per_epoch=np_steps,\r\n                                verbose=1)\r\n```\r\n\r\n```\r\ndef batch_generator(X, y, batch_size, shuffle):\r\n    number_of_batches = round(X.shape[0] / batch_size)\r\n    print(f\'y.shape: {X.shape}\')\r\n    print(f\'X.shape[0]: {X.shape[0]}\')\r\n    print(f\'batch_size: {batch_size}\')\r\n    print(f\'number_of_batches: {number_of_batches}\')\r\n    counter = 0\r\n    sample_index = np.arange(X.shape[0])\r\n    if shuffle:\r\n        np.random.shuffle(sample_index)\r\n    while True:\r\n        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\r\n        print(f\'counter: {counter}\')\r\n        print(f\'batch_index: {batch_index}\')\r\n        X_batch = X[batch_index, :].toarray()\r\n        y_batch = y[batch_index]\r\n        counter += 1\r\n        print(f\'yielded X_batch shape: {X_batch.shape} where counter: {counter}\')\r\n        print(f\'yielded y_batch shape: {y_batch.shape} where counter: {counter}\')\r\n        yield X_batch, y_batch\r\n        if counter == number_of_batches:\r\n            if shuffle:\r\n                np.random.shuffle(sample_index)\r\n            counter = 0\r\n```\r\nWhile the result is:\r\n\r\n```\r\nshape of x_train: (1407577, 207)\r\nshape of x_valid: (74084, 207)\r\nEpoch 1/1\r\ny.shape: (1407577, 207)\r\nX.shape[0]: 1407577\r\nbatch_size: 64\r\nnumber_of_batches: 21993\r\ncounter: 0\r\nbatch_index: [  55578  885043  826530 1085983  596906  990879  150577  916018 1283648\r\n  657662   22295  978107 1330422 1094582 1323311 1386819 1349709  361209\r\n 1314015  112607 1363984  179845  464715  474078  337839 1097602 1173681\r\n  910575 1113526   27510  647144 1081263  678677  567365  149537  936813\r\n  202732  727397   21943  563727  923775  661254  465769  383644   99391\r\n  918760 1152295   23548  511531  661704  590537  855781  654484 1008951\r\n  690456  414501  998788   97076  334435   57625  822775  125039  857771\r\n 1103476]\r\nyielded X_batch shape: (64, 207) where counter: 1\r\nyielded y_batch shape: (64, 1) where counter: 1\r\ncounter: 1\r\nbatch_index: [ 784817  248893  472646 1010253 1400872  522094  217448 1120737  844047\r\n  837030 1298761  768817  728090  799502  951408 1356634  365863  997856\r\n  752368  558098  732175  571739 1048726  536754 1315847 1032336  413975\r\n  366115  550960  976257 1116407  237934  386406 1160910  113013  163694\r\n  437774  372660 1164205  335589  223032 1364559  626345 1135905  396354\r\n 1120329  567954 1252556  763024  351094  445536  734038 1344405  239830\r\n  869035  728995  288278  238975  699285 1267507  131834   24448  233283\r\n  623641]\r\nTraceback (most recent call last):\r\n  File ""mlp.py"", line 443, in <module>\r\nyielded X_batch shape: (64, 207) where counter: 2\r\nyielded y_batch shape: (64, 1) where counter: 2\r\n    main()\r\n  File ""mlp.py"", line 371, in main\r\n    model1 = run_model1(X_train, y_train, X_valid, y_valid)\r\n  File ""mlp.py"", line 255, in run_model1\r\n    verbose=1)\r\n  File ""/path_to_project/venv/lib/python3.6/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper\r\n    return func(*args, **kwargs)\r\n  File ""/path_to_project/venv/lib/python3.6/site-packages/keras/engine/training.py"", line 1732, in fit_generator\r\n    initial_epoch=initial_epoch)\r\n  File ""/path_to_project/venv/lib/python3.6/site-packages/keras/engine/training_generator.py"", line 220, in fit_generator\r\n    reset_metrics=False)\r\n  File ""/path_to_project/venv/lib/python3.6/site-packages/keras/engine/training.py"", line 1514, in train_on_batch\r\n    outputs = self.train_function(ins)\r\n  File ""/path_to_project/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py"", line 3727, in __call__\r\n    outputs = self._graph_fn(*converted_inputs)\r\n  File ""/path_to_project/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1551, in __call__\r\n    return self._call_impl(args, kwargs)\r\n  File ""/path_to_project/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1591, in _call_impl\r\n    return self._call_flat(args, self.captured_inputs, cancellation_manager)\r\n  File ""/path_to_project/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1692, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File ""/path_to_project/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 527, in call\r\n    (len(args), len(list(self.signature.input_arg))))\r\nValueError: Arguments and signature arguments do not match. got: 37, expected: 39 \r\n\r\n```\r\n\r\nIt fails in the second iteration of the loop.', '> @claeszacho thanks for your suggestion but to me the problem doesn\'t seem to be related to X and y shapes. I re-run as following and still have the issue:\r\n> \r\n> ```\r\n> model.fit_generator(generator=batch_generator(X_train, y_train, divide_on, True), epochs=1,\r\n>                                 steps_per_epoch=np_steps,\r\n>                                 verbose=1)\r\n> ```\r\n> \r\n> ```\r\n> def batch_generator(X, y, batch_size, shuffle):\r\n>     number_of_batches = round(X.shape[0] / batch_size)\r\n>     print(f\'y.shape: {X.shape}\')\r\n>     print(f\'X.shape[0]: {X.shape[0]}\')\r\n>     print(f\'batch_size: {batch_size}\')\r\n>     print(f\'number_of_batches: {number_of_batches}\')\r\n>     counter = 0\r\n>     sample_index = np.arange(X.shape[0])\r\n>     if shuffle:\r\n>         np.random.shuffle(sample_index)\r\n>     while True:\r\n>         batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\r\n>         print(f\'counter: {counter}\')\r\n>         print(f\'batch_index: {batch_index}\')\r\n>         X_batch = X[batch_index, :].toarray()\r\n>         y_batch = y[batch_index]\r\n>         counter += 1\r\n>         print(f\'yielded X_batch shape: {X_batch.shape} where counter: {counter}\')\r\n>         print(f\'yielded y_batch shape: {y_batch.shape} where counter: {counter}\')\r\n>         yield X_batch, y_batch\r\n>         if counter == number_of_batches:\r\n>             if shuffle:\r\n>                 np.random.shuffle(sample_index)\r\n>             counter = 0\r\n> ```\r\n> \r\n> While the result is:\r\n> \r\n> ```\r\n> shape of x_train: (1407577, 207)\r\n> shape of x_valid: (74084, 207)\r\n> Epoch 1/1\r\n> y.shape: (1407577, 207)\r\n> X.shape[0]: 1407577\r\n> batch_size: 64\r\n> number_of_batches: 21993\r\n> counter: 0\r\n> batch_index: [  55578  885043  826530 1085983  596906  990879  150577  916018 1283648\r\n>   657662   22295  978107 1330422 1094582 1323311 1386819 1349709  361209\r\n>  1314015  112607 1363984  179845  464715  474078  337839 1097602 1173681\r\n>   910575 1113526   27510  647144 1081263  678677  567365  149537  936813\r\n>   202732  727397   21943  563727  923775  661254  465769  383644   99391\r\n>   918760 1152295   23548  511531  661704  590537  855781  654484 1008951\r\n>   690456  414501  998788   97076  334435   57625  822775  125039  857771\r\n>  1103476]\r\n> yielded X_batch shape: (64, 207) where counter: 1\r\n> yielded y_batch shape: (64, 1) where counter: 1\r\n> counter: 1\r\n> batch_index: [ 784817  248893  472646 1010253 1400872  522094  217448 1120737  844047\r\n>   837030 1298761  768817  728090  799502  951408 1356634  365863  997856\r\n>   752368  558098  732175  571739 1048726  536754 1315847 1032336  413975\r\n>   366115  550960  976257 1116407  237934  386406 1160910  113013  163694\r\n>   437774  372660 1164205  335589  223032 1364559  626345 1135905  396354\r\n>  1120329  567954 1252556  763024  351094  445536  734038 1344405  239830\r\n>   869035  728995  288278  238975  699285 1267507  131834   24448  233283\r\n>   623641]\r\n> Traceback (most recent call last):\r\n>   File ""mlp.py"", line 443, in <module>\r\n> yielded X_batch shape: (64, 207) where counter: 2\r\n> yielded y_batch shape: (64, 1) where counter: 2\r\n>     main()\r\n>   File ""mlp.py"", line 371, in main\r\n>     model1 = run_model1(X_train, y_train, X_valid, y_valid)\r\n>   File ""mlp.py"", line 255, in run_model1\r\n>     verbose=1)\r\n>   File ""/path_to_project/venv/lib/python3.6/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper\r\n>     return func(*args, **kwargs)\r\n>   File ""/path_to_project/venv/lib/python3.6/site-packages/keras/engine/training.py"", line 1732, in fit_generator\r\n>     initial_epoch=initial_epoch)\r\n>   File ""/path_to_project/venv/lib/python3.6/site-packages/keras/engine/training_generator.py"", line 220, in fit_generator\r\n>     reset_metrics=False)\r\n>   File ""/path_to_project/venv/lib/python3.6/site-packages/keras/engine/training.py"", line 1514, in train_on_batch\r\n>     outputs = self.train_function(ins)\r\n>   File ""/path_to_project/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py"", line 3727, in __call__\r\n>     outputs = self._graph_fn(*converted_inputs)\r\n>   File ""/path_to_project/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1551, in __call__\r\n>     return self._call_impl(args, kwargs)\r\n>   File ""/path_to_project/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1591, in _call_impl\r\n>     return self._call_flat(args, self.captured_inputs, cancellation_manager)\r\n>   File ""/path_to_project/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1692, in _call_flat\r\n>     ctx, args, cancellation_manager=cancellation_manager))\r\n>   File ""/path_to_project/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 527, in call\r\n>     (len(args), len(list(self.signature.input_arg))))\r\n> ValueError: Arguments and signature arguments do not match. got: 37, expected: 39 \r\n> ```\r\n> \r\n> It fails in the second iteration of the loop.\r\n\r\nThe problem solved after replacing `import keras` by `import tensorflow.keras` and making the necessary modifications']","['\r\n\r\n     53 \r\n     54 for i in range(num_iter):\r\n---> 55     result[i, :] = predict_with_dropout(input_data+[1])[0]\r\n\r\nc:\\users\\cprice2\\appdata\\local\\continuum\\envs\\tf_new\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py in __call__(self, inputs)\r\n   3738         value = math_ops.cast(value, tensor.dtype)\r\n   3739       converted_inputs.append(value)\r\n-> 3740     outputs = self._graph_fn(*converted_inputs)\r\n   3741 \r\n   3742     # EagerTensor.numpy() will often make a copy to ensure memory safety.\r\n\r\nc:\\users\\cprice2\\appdata\\local\\continuum\\envs\\tf_new\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   1079       TypeError: For invalid positional/keyword argument combinations.\r\n   1080     """"""\r\n-> 1081     return self._call_impl(args, kwargs)\r\n   1082 \r\n   1083   def _call_impl(self, args, kwargs, cancellation_manager=None):\r\n\r\nc:\\users\\cprice2\\appdata\\local\\continuum\\envs\\tf_new\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _call_impl(self, args, kwargs, cancellation_manager)\r\n   1119       raise TypeError(""Keyword arguments {} unknown. Expected {}."".format(\r\n   1120           list(kwargs.keys()), list(self._arg_keywords)))\r\n-> 1121     return self._call_flat(args, self.captured_inputs, cancellation_manager)\r\n   1122 \r\n   1123   def _filtered_call(self, args, kwargs):\r\n\r\nc:\\users\\cprice2\\appdata\\local\\continuum\\envs\\tf_new\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1222     if executing_eagerly:\r\n   1223       flat_outputs = forward_function.call(\r\n-> 1224           ctx, args, cancellation_manager=cancellation_manager)\r\n   1225     else:\r\n   1226       gradient_name = self._delayed_rewrite_functions.register()\r\n\r\nc:\\users\\cprice2\\appdata\\local\\continuum\\envs\\tf_new\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n    491           ""Arguments and signature arguments do not match. ""\r\n    492           ""got: %s, expected: %s "" %\r\n--> 493           (len(args), len(list(self.signature.input_arg))))\r\n    494 \r\n    495     function_call_options = ctx.function_call_options\r\n\r\nValueError: Arguments and signature arguments do not match. got: 13, expected: 14 \r\n', '\r\n# Model\r\ndef mlp_model():\r\n  model = tf.keras.Sequential([   \r\n    tf.keras.layers.Dense(3, input_dim = 3,\r\n                          kernel_initializer = \'glorot_uniform\',\r\n                          activation = \'elu\'),\r\n    tf.keras.layers.Dense(160, \r\n                          activation = \'elu\', \r\n                          kernel_regularizer = regularizers.l2(0.001)),                     \r\n    tf.keras.layers.GaussianNoise(0.3),  \r\n    tf.keras.layers.Dense(160, \r\n                          activation = \'elu\',\r\n                          kernel_regularizer = regularizers.l2(0.003)),\r\n    tf.keras.layers.Dropout(0.2),\r\n    tf.keras.layers.Dense(160, \r\n                          activation = \'elu\',\r\n                          kernel_regularizer = regularizers.l2(0.003)),                                               \r\n    tf.keras.layers.Dense(160, \r\n                          activation = \'relu\',\r\n                          kernel_regularizer = regularizers.l2(0.004)), #128 for unscaled\r\n    tf.keras.layers.Dense(1) \r\n  ])\r\n\r\n  model.compile(optimizer = opt, \r\n                loss = root_mean_squared_error,\r\n                metrics=[\'mean_squared_error\', \r\n                         \'mean_absolute_error\',\r\n                         root_mean_squared_error])\r\n  return model\r\n\r\n# fit model\r\nmodel = mlp_model()\r\nmodel_history = model.fit(train_batch, \r\n                          validation_data = test_batch,\r\n                          epochs = 100,\r\n                          shuffle = False,\r\n                          callbacks = [early_stop, \r\n                                       model_checkpoint,\r\n                                       tensorboard_callback])\r\n\r\n# Create dropout func \r\ndef create_dropout_predict_function(model, dropout):\r\n\r\n    # Load the config of the original model\r\n    conf = model.get_config()\r\n\r\n    # Add the specified dropout to all layers\r\n    for layer in conf[\'layers\']:\r\n        # Dropout layers\r\n        if layer[""class_name""]==""Dropout"":\r\n            layer[""config""][""rate""] = dropout\r\n\r\n    # Using Functional API\r\n    model_dropout = tf.keras.Sequential.from_config(conf)\r\n    model_dropout.set_weights(model.get_weights()) \r\n\r\n    # Predict with dropout\r\n    predict_with_dropout = K.function(model_dropout.inputs+[K.learning_phase()], model_dropout.outputs)\r\n\r\n    return predict_with_dropout\r\n\r\n# Create predictions with dropout\r\n\r\n# input data\r\ninput_data = np.array(forecast_df)\r\n\r\n# configure dropout, number of iterations\r\ndropout = 0.5\r\nnum_iter = 20\r\nnum_samples = input_data[0].shape[0]\r\n\r\npredict_with_dropout = create_dropout_predict_function(model, dropout)\r\n\r\npredictions = np.zeros((num_samples, num_iter))\r\n\r\nfor i in range(num_iter):\r\n    predictions[:, i] = predict_with_dropout(input_data+[1])[0].reshape(-1)\r\n\r\n', '\r\ntrain_data = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))\r\ntest_data = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))\r\nforecast_data = tf.data.Dataset.from_tensor_slices((forecast_ahead_df.values))\r\n']",[],0,0
393,keras,10370,closed,validation_data not passed as param to callback.set_params() in fit_generator,"In , if the validation data is a generator, it will not be passed to the callback as a parameter which makes it impossible to access the validation data for use in custom callbacks.

https://github.com/keras-team/keras/blob/632560d91286bf278228de72e7ce64f6c5aa530c/keras/engine/training_generator.py#L92-L98",,[],[],['fit_generator'],0,0
394,keras,9684,closed,how to write the objectives function roc_auc_score in tflearn by keras,"Dear everyone,
                     I found the roc_auc_score function in  https://github.com/tflearn/tflearn/blob/master/tflearn/objectives.py. Now I want to write this function by keras. But failed. The following is the code:
TensorTensorTensorTensorTensorTensorTensorTensor
**The problem is the following :**+1: 
ValueError                                Traceback (most recent call last)
<ipython-input-3-cc3e34fe5d20> in <module>()
    149         epochs=epochs,
    150         validation_data=validation_generator,
--> 151         validation_steps=validation_samples// batch_size)
    152 
    153 model.save_weights('models/basic_cnn_30_epochs.h5')

/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc in wrapper(*args, **kwargs)
     85                 warnings.warn('Update your  call to the Keras 2 API: ' + signature, stacklevel=2)
---> 87             return func(*args, **kwargs)
     88         wrapper._original_function = func
     89         return wrapper

/usr/local/lib/python2.7/dist-packages/keras/models.pyc in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)
   1119                                         workers=workers,
   1120                                         use_multiprocessing=use_multiprocessing,
-> 1121                                         initial_epoch=initial_epoch)
   1122 
   1123     @interfaces.legacy_generator_methods_support

/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc in wrapper(*args, **kwargs)
     85                 warnings.warn('Update your  call to the Keras 2 API: ' + signature, stacklevel=2)
---> 87             return func(*args, **kwargs)
     88         wrapper._original_function = func
     89         return wrapper

/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1924 
   1925         do_validation = bool(validation_data)
-> 1926         self._make_train_function()
   1927         if do_validation:
   1928             self._make_test_function()

/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc in _make_train_function(self)
    958                     training_updates = self.optimizer.get_updates(
    959                         params=self._collected_trainable_weights,
--> 960                         loss=self.total_loss)
    961                 updates = self.updates + training_updates
    962                 # Gets loss and metrics. Updates weights at each call.

/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc in wrapper(*args, **kwargs)
     85                 warnings.warn('Update your  call to the Keras 2 API: ' + signature, stacklevel=2)
---> 87             return func(*args, **kwargs)
     88         wrapper._original_function = func
     89         return wrapper

/usr/local/lib/python2.7/dist-packages/keras/optimizers.pyc in get_updates(self, loss, params)
    235         for p, g, a in zip(params, grads, accumulators):
    236             # update accumulator
--> 237             new_a = self.rho * a + (1. - self.rho) * K.square(g)
    238             self.updates.append(K.update(a, new_a))
    239             new_p = p - lr * g / (K.sqrt(new_a) + self.epsilon)

/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc in square(x)
   1356         A tensor.
   1357     """"""
-> 1358     return tf.square(x)
   1359 
   1360 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.pyc in square(x, name)
    447           indices=x.indices, values=x_square, dense_shape=x.dense_shape)
    448     else:
--> 449       return gen_math_ops.square(x, name=name)
    450 
    451 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.pyc in square(x, name)
   4565   if _ctx.in_graph_mode():
   4566     _, _, _op = _op_def_lib._apply_op_helper(
-> 4567         ""Square"", x=x, name=name)
   4568     _result = _op.outputs[:]
   4569     _inputs_flat = _op.inputs

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc in _apply_op_helper(self, op_type_name, name, **keywords)
    526               raise ValueError(
    527                   ""Tried to convert '%s' to a tensor and failed. Error: %s"" %
--> 528                   (input_name, err))
    529             prefix = (""Input '%s' of '%s' Op has type %s that does not match"" %
    530                       (input_name, op_type_name, observed))

ValueError: Tried to convert 'x' to a tensor and failed. Error: None values not supported.

Finally, Anyone can check this problem. Looking forward to reply. Thanks advanced!
",,['@jiandanjinxin Hi! Perhaps [this entry](https://github.com/keras-team/keras/issues/3230) can solve the issue? isaacgerg provides a complete implementation of the roc_auc_score function. Best!'],"['\r\n\r\n\r\n**The new code was changed to the following:**\r\n\r\n', ""\r\n\r\n**All the code are the following, it doesn't work,** \r\n **The data in** \r\nhttps://pan.baidu.com/s/12yJCWdfvVW1tEKUfEU34RQ ,  password: nu98\r\n""]","['', '\r\ndef roc_auc_score(y_pred, y_true):\r\n    """""" ROC AUC Score.\r\n    Approximates the Area Under Curve score, using approximation based on\r\n    the Wilcoxon-Mann-Whitney U statistic.\r\n    Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).\r\n    Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.\r\n    Measures overall performance for a full range of threshold levels.\r\n    Arguments:\r\n        y_pred: ', '. Predicted values.\r\n        y_true: ', ' . Targets (labels), a probability distribution.\r\n    """"""\r\n    with tf.name_scope(""RocAucScore""):\r\n        pos = tf.boolean_mask(y_pred, tf.cast(y_true, tf.bool))\r\n        neg = tf.boolean_mask(y_pred, ~tf.cast(y_true, tf.bool))\r\n        pos = tf.expand_dims(pos, 0)\r\n        neg = tf.expand_dims(neg, 1)\r\n        # original paper suggests performance is robust to exact parameter choice\r\n        gamma = 0.2\r\n        p     = 3\r\n        difference = tf.zeros_like(pos * neg) + pos - neg - gamma\r\n        masked = tf.boolean_mask(difference, difference < 0.0)\r\n        return tf.reduce_sum(tf.pow(-masked, p))\r\n\r\ndef roc_auc_score(y_pred, y_true):\r\n    """""" ROC AUC Score.\r\n    Approximates the Area Under Curve score, using approximation based on\r\n    the Wilcoxon-Mann-Whitney U statistic.\r\n    Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).\r\n    Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.\r\n    Measures overall performance for a full range of threshold levels.\r\n    Arguments:\r\n        y_pred: ', '. Predicted values.\r\n        y_true: ', ' . Targets (labels), a probability distribution.\r\n    """"""\r\n    pos = tf.boolean_mask(y_pred, K.cast(y_true, tf.bool))\r\n    neg = tf.boolean_mask(y_pred, ~K.cast(y_true, tf.bool))\r\n    pos = K.expand_dims(pos, 0)\r\n    neg = K.expand_dims(neg, 1)\r\n    # original paper suggests performance is robust to exact parameter choice\r\n    gamma = 0.2\r\n    p     = 3\r\n    difference = K.zeros_like(pos * neg) + pos - neg - gamma\r\n    masked = tf.boolean_mask(difference, difference < 0.0)\r\n    return K.sum(K.pow(-masked, p))\r\n\r\n# -*- coding: UTF-8 -*-\r\nimport os\r\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\r\nimport numpy as np\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Activation, Dropout, Flatten, Dense\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\r\nfrom keras import optimizers\r\nfrom keras import applications\r\nfrom keras.models import Model\r\nimport keras\r\nimport numpy as np\r\nfrom keras import backend as K\r\nimport tensorflow as tf\r\n\r\n\r\nprint(""---------------------------AUC----------------------------------------"")\r\n\r\n\r\n# def roc_auc_score(y_pred, y_true):\r\n#     """""" ROC AUC Score.\r\n#     Approximates the Area Under Curve score, using approximation based on\r\n#     the Wilcoxon-Mann-Whitney U statistic.\r\n#     Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).\r\n#     Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.\r\n#     Measures overall performance for a full range of threshold levels.\r\n#     Arguments:\r\n#         y_pred: ', '. Predicted values.\r\n#         y_true: ', ' . Targets (labels), a probability distribution.\r\n#     """"""\r\n#     with tf.name_scope(""RocAucScore""):\r\n\r\n#         pos = tf.boolean_mask(y_pred, tf.cast(y_true, tf.bool))\r\n#         neg = tf.boolean_mask(y_pred, ~tf.cast(y_true, tf.bool))\r\n\r\n#         pos = tf.expand_dims(pos, 0)\r\n#         neg = tf.expand_dims(neg, 1)\r\n\r\n#         # original paper suggests performance is robust to exact parameter choice\r\n#         gamma = 0.2\r\n#         p     = 3\r\n\r\n#         difference = tf.zeros_like(pos * neg) + pos - neg - gamma\r\n\r\n#         masked = tf.boolean_mask(difference, difference < 0.0)\r\n\r\n#         return tf.reduce_sum(tf.pow(-masked, p))\r\n\r\n\r\n\r\ndef roc_auc_score(y_pred, y_true):\r\n    """""" ROC AUC Score.\r\n    Approximates the Area Under Curve score, using approximation based on\r\n    the Wilcoxon-Mann-Whitney U statistic.\r\n    Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).\r\n    Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.\r\n    Measures overall performance for a full range of threshold levels.\r\n    Arguments:\r\n        y_pred: ', '. Predicted values.\r\n        y_true: ', ' . Targets (labels), a probability distribution.\r\n    """"""\r\n    pos = tf.boolean_mask(y_pred, K.cast(y_true, tf.bool))\r\n    neg = tf.boolean_mask(y_pred, ~K.cast(y_true, tf.bool))\r\n    pos = K.expand_dims(pos, 0)\r\n    neg = K.expand_dims(neg, 1)\r\n    # original paper suggests performance is robust to exact parameter choice\r\n    gamma = 0.2\r\n    p     = 3\r\n    difference = K.zeros_like(pos * neg) + pos - neg - gamma\r\n    masked = tf.boolean_mask(difference, difference < 0.0)\r\n    return K.sum(K.pow(-masked, p))\r\n\r\ndef roc_auc_score_loss(y_true, y_pred):\r\n    return roc_auc_score(y_true, y_pred)\r\nprint(""---------------------------AUC----------------------------------------"")\r\n\r\n\r\n# dimensions of our images.\r\nimg_width, img_height = 512, 512\r\n\r\ntrain_data_dir = \'data/train/\'\r\nvalidation_data_dir = \'data/validation/\'\r\n\r\n\r\n##preprocessing\r\n# used to rescale the pixel values from [0, 255] to [0, 1] interval\r\ndatagen = ImageDataGenerator(rescale=1./255)\r\nbatch_size = 32\r\n\r\n# automagically retrieve images and their classes for train and validation sets\r\ntrain_generator = datagen.flow_from_directory(\r\n        train_data_dir,\r\n        target_size=(img_width, img_height),\r\n        batch_size=batch_size,\r\n        class_mode=\'binary\')\r\n\r\nvalidation_generator = datagen.flow_from_directory(\r\n        validation_data_dir,\r\n        target_size=(img_width, img_height),\r\n        batch_size=batch_size,\r\n        class_mode=\'binary\',shuffle=False)\r\n\r\n\r\n\r\n# a simple stack of 3 convolution layers with a ReLU activation and followed by max-pooling layers.\r\nmodel = Sequential()\r\nmodel.add(Convolution2D(32, (3, 3), input_shape=(img_width, img_height,3)))\r\nmodel.add(Activation(\'relu\'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\nmodel.add(Convolution2D(32, (3, 3)))\r\nmodel.add(Activation(\'relu\'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\nmodel.add(Convolution2D(64, (3, 3)))\r\nmodel.add(Activation(\'relu\'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\nmodel.add(Flatten())\r\nmodel.add(Dense(64))\r\nmodel.add(Activation(\'relu\'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(1))\r\nmodel.add(Activation(\'sigmoid\'))\r\n\r\n# model.compile(loss=\'binary_crossentropy\',\r\n#               optimizer=\'rmsprop\',\r\n#               metrics=[\'accuracy\',\'mae\'])\r\n\r\nmodel.compile(loss=roc_auc_score_loss,\r\n              optimizer=\'rmsprop\',\r\n              metrics=[\'accuracy\',\'mae\',roc_auc_score])\r\nepochs = 5\r\n\r\ntrain_samples = 2048\r\nvalidation_samples = 832\r\n\r\n\r\nmodel.fit_generator(\r\n        train_generator,\r\n        steps_per_epoch=train_samples // batch_size,\r\n        epochs=epochs,\r\n        validation_data=validation_generator,\r\n        validation_steps=validation_samples// batch_size)\r\n\r\nmodel.save_weights(\'models/basic_cnn_30_epochs.h5\')\r\nprint(model.summary())\r\n\r\n\r\n', '', ""' + object_name +\r\n     86                               '"", ""' + object_name +\r\n     86                               '"", ""' + object_name +\r\n     86                               '""]",0,0
395,keras,12393,closed,Error selecting GPU programmatically from jupyter,"I have a multi-GPU machine and am trying to train different models on different GPUs. I'm trying to set the GPU programmatically rather than use env vars, but am running into issues. Here's my code:



However, when I try to run the code in two separate jupyter notebooks, using ids 0 and 1, I get the following error:



It works fine if I set  at the top of the script, but I'd rather not have to manage this. Any sense of what I may be doing wrong? I'm using
- keras 2.2.4
- tensorflow 1.13.1
- ubuntu 18.04
- jupyter 5.2.4
- jupyter lab 0.35.4
",stat:cross-posting to TF type:support,"['environment param need to be set before you import keras ', ""Even when I do that, one of the instances crashes at the end of the first epoch. I don't even get any error message. Just a message that the kernel was reset."", '@lminer  Is this still an issue for you? I have cross posted this issue on TF repo  as well for TF folks to take a look. Here is the [link](https://github.com/tensorflow/tensorflow/issues/27122) to it. Thanks!', '@ymodak I think I solved the problem. Kernel reset was a different issue.', 'Cool. I will go ahead and close this issue now. Thanks!', '@lminer and @ymodak what was the solution to this problem ?', '@lminer How did you manage to solve this issue? ', ""I have four GPUs on my machine (Ubuntu 1804).\r\n\r\nSetting:\r\n\r\n`export CUDA_VISIBLE_DEVICES=0,1,2,3`\r\n\r\nin the shell, or\r\n\r\n```\r\nimport os\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'\r\n```\r\n\r\nin the python script worked for me.""]","['python\r\ndef choose_gpu(gpu_id):\r\n    os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""\r\n    with K.tf.device(f\'/gpu:{gpu_id}\'):\r\n        config = tf.ConfigProto(intra_op_parallelism_threads=10,\r\n                                inter_op_parallelism_threads=10,\r\n                                allow_soft_placement=True,\r\n                                device_count={\'CPU\': 1, \'GPU\': 1})\r\n        session = tf.Session(config=config)\r\n        K.set_session(session)\r\n', '\r\n2019-03-04 10:36:27.210489: W tensorflow/compiler/xla/service/platform_util.cc:240] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 11523260416\r\n2019-03-04 10:36:27.210803: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x56311c4bccf0 executing computations on platform CUDA. Devices:\r\n2019-03-04 10:36:27.211230: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value\r\ninstead of handling error Invalid argument: device CUDA:0 not supported by XLA service\r\n']",['CUDA_VISIBLE_DEVICES'],0,0
396,keras,6810,closed,issue with conda 3.6 and pydot and plot_model,package pydot can't be installed on a conda python 3.6 platform so  doesn't work,,[],[],['plot_model()'],0,0
397,keras,11929,closed,model.summary() output problem for multi GPU model ,"Hello!

For a multi-GPU model built using , the function  outputs incorrect information. More precisely, for the following code:


The output is:


However, calling  on the original model (e.g. ) outputs the correct architecture:



Is this the expected behaviour? Is it related to the same issue as calling  on the parallel model?
",,"['Yes. The parralel model should only be used for training. All other operations should use the original model. ', 'Would it be an easy fix guarantee consistency for `summary()` (to have the same behavior for the parallel and the non-parallel model object)?', ""Technically the multi_gpu summary is correct. It's just not very informative. So it's not something that we want to fix since it's not really broken. You can look at the code in the function `multi_gpu_model` and you'll understand why the summary is like this. "", 'Ok. Thanks, Gabriel! :+1: ', 'Happy to help :) ', '@gabrieldemarmiesse \r\n\r\nReading this after creating this issue: https://github.com/keras-team/keras/issues/13833\r\n\r\nWhen you say:\r\n\r\n> The parallel model should only be used for training.\r\n> All other operations should use the original model.\r\n\r\nWhat do you mean by ""should use the original model""? If I want to access the output from an intermediate layer, if I query the original model, then the outputs would be rubbish because this ""original model"" has not been trained, right? Unless I\'m mistaken?\r\n\r\nHow can we then use the output from an intermediate layer of the **trained** model, if `multi_gpu_model` was used? In SebiSebi\'s example above, I\'d like to use the output from layer `dense_1` (after training).']","[""\r\nwith tf.device('/cpu:0'):\r\n    model = Model(inputs=[...], outputs=[...])\r\nparallel_model = multi_gpu_model(model, gpus=3)\r\nparallel_model.summary()\r\n"", '\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ntext_input (InputLayer)         (None, 1)            0                                            \r\n__________________________________________________________________________________________________\r\nlambda_1 (Lambda)               (None, 1)            0           text_input[0][0]                 \r\n__________________________________________________________________________________________________\r\nlambda_2 (Lambda)               (None, 1)            0           text_input[0][0]                 \r\n__________________________________________________________________________________________________\r\nlambda_3 (Lambda)               (None, 1)            0           text_input[0][0]                 \r\n__________________________________________________________________________________________________\r\nmodel_1 (Model)                 (None, 2)            134326      lambda_1[0][0]                   \r\n                                                                 lambda_2[0][0]                   \r\n                                                                 lambda_3[0][0]                   \r\n__________________________________________________________________________________________________\r\ndense_3 (Concatenate)           (None, 2)            0           model_1[1][0]                    \r\n                                                                 model_1[2][0]                    \r\n                                                                 model_1[3][0]                    \r\n==================================================================================================\r\nTotal params: 134,326\r\nTrainable params: 134,326\r\nNon-trainable params: 0\r\n__________________________________________________________________________________________________\r\n', '\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ntext_input (InputLayer)      (None, 1)                 0         \r\n_________________________________________________________________\r\nelmo_embedding_layer_1 (Elmo (None, 450, 1024)         4         \r\n_________________________________________________________________\r\ngru_1 (GRU)                  (None, 40)                127800    \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 70)                2870      \r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 50)                3550      \r\n_________________________________________________________________\r\ndense_3 (Dense)              (None, 2)                 102       \r\n=================================================================\r\nTotal params: 134,326\r\nTrainable params: 134,326\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n']","['multi_gpu_model', 'parallel_model.summary()', 'summary()', 'model.summary()', 'save()']",0,0
398,keras,9336,closed,RNN with dropout / recurrent dropout and 1 timestep shows warning with CNTK,"Hello,

I am creating the following network:

When using CNTK as backend, I am getting error:
'RNN dropout is not supported with the CNTK backend when using dynamic RNNs (i.e. non-unrolled).  You can either set , set  and  to 0, 'or use a different backend.'

However, I cannot unroll, because train_x_lstm.shape[1] = 1",,"['Precision, this is the error when attempting to unrolling:\r\n\r\nValueError: Cannot unroll a RNN if the time dimension is undefined or equal to 1.', ""Actually this is fixed in 2.1.3... I updated before posting but looks like I forgot to restart python session or some cache wasn't cleared. Closing issue.""]","['\r\nmodel = Sequential()\r\nmodel.add(LSTM(512,return_sequences=True,recurrent_dropout=0.2, \r\n              input_shape=(train_x_lstm.shape[1], train_x_lstm.shape[2])\r\n         ))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(LSTM(128))\r\nmodel.add(Dense(len(selected_y_cols)))\r\nmodel.add(Activation(\'sigmoid\'))\r\nmodel.compile(loss=\'binary_crossentropy\', optimizer=\'rmsprop\', metrics =[""accuracy""])\r\n\r\nhistory = model.fit(train_x_lstm, \r\n                    train_y_lstm[selected_y_cols].values,\r\n                    epochs=50,batch_size=64 ,\r\n                    validation_data=(validate_x_lstm, validate_y_lstm[selected_y_cols].values), \r\n                    verbose=1, shuffle=False)\r\n']","['unroll=True', 'dropout', 'recurrent_dropout']",0,0
399,keras,10356,closed,CuDNN RNN layers nested in TimeDistributed are not converted when loading,"Making issue from comment https://github.com/keras-team/keras/issues/10080#issuecomment-394640409 at #10080.

When converting weights between CuDNNGRU and GRU which is wrapped in TimeDistributed the conversion is skipped by mistake. Similar to Bidirectional (#8908) and Model/Sequential (#10080).

Example of failure:



Fails with:



Without TimeDistributed it works ok:


The same for the other direction (plain -> CuDNN).

Do we know of any other wrapper layers that need this conversion?",,"['@Ajk4 Fixed. Waiting for CI and review. Works for me. Thanks for the report.', 'Thanks!', 'Merged.']","[""\r\nimport tempfile\r\nimport os\r\n\r\nfrom keras.layers import Input, GRU, CuDNNGRU, TimeDistributed\r\nfrom keras.models import Model\r\nimport numpy as np\r\n\r\ninput2 = Input((4, 6, 10))\r\noutput_cudnn = TimeDistributed(CuDNNGRU(units))(input2)\r\noutput = TimeDistributed(GRU(units, activation='hard_sigmoid', reset_after=True))(input2)\r\nmodel_cudnn = Model(input2, output_cudnn)\r\nmodel_plain = Model(input2, output)\r\n\r\n_, fname = tempfile.mkstemp('.h5')\r\nmodel_cudnn.save_weights(fname)\r\nmodel_plain.load_weights(fname)\r\n"", ""\r\nlocal/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in _create_c_op(graph, node_def, inputs, control_inputs)\r\n   1568   except errors.InvalidArgumentError as e:\r\n   1569     # Convert to ValueError for backwards compatibility.\r\n-> 1570     raise ValueError(str(e))\r\n   1571 \r\n   1572   return c_op\r\n\r\nValueError: Shapes must be equal rank, but are 2 and 1 for 'Assign_11' (op: 'Assign') with input shapes: [2,6], [12].\r\n"", ""\r\ninput2 = Input((6, 10))\r\noutput_cudnn = CuDNNGRU(units)(input2)\r\noutput = GRU(units, activation='hard_sigmoid', reset_after=True)(input2)\r\nmodel_cudnn = Model(input2, output_cudnn)\r\nmodel_plain = Model(input2, output)\r\n\r\n_, fname = tempfile.mkstemp('.h5')\r\nmodel_cudnn.save_weights(fname)\r\nmodel_plain.load_weights(fname)\r\n""]",[],0,0
400,keras,3424,closed,Adjust dropout rate over epochs,"[Some research papers](http://www.clsp.jhu.edu/~samuel/pdfs/annealed_dropout.pdf) showed the benefit to adjust dropout rate over epochs.
Specifically, they used this dropout rate:



where  is the current epoch and N is a fixed parameter.

How can I achieve this using Keras? I think I can re-write the call function in the dropout layer but the problem is that how can I get the epoch variable.
",,"[""make your own layer that changes the dropout probability based on whatever condition you want.. Start from the Dropout class itself is probably a good idea.. you can see how 'self.p' can easily be altered in the class or in a separate callback.\n\nclass Dropout(Layer):\n    '''Applies Dropout to the input. Dropout consists in randomly setting\n    a fraction `p` of input units to 0 at each update during training time,\n    which helps prevent overfitting.\n\n```\n# Arguments\n    p: float between 0 and 1. Fraction of the input units to drop.\n\n# References\n    - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n'''\ndef __init__(self, p, **kwargs):\n    self.p = p\n    if 0. < self.p < 1.:\n        self.uses_learning_phase = True\n    self.supports_masking = True\n    super(Dropout, self).__init__(**kwargs)\n\ndef call(self, x, mask=None):\n    if 0. < self.p < 1.:\n        x = K.in_train_phase(K.dropout(x, level=self.p), x)\n    return x\n\ndef get_config(self):\n    config = {'p': self.p}\n    base_config = super(Dropout, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))\n```\n"", 'Yes, I implemented it using callback and a new shared variable.\nAlso a little hack is needed since function `K.dropout` check whether the level is in range 0-1 or not. \nIn this case the level is a function instead of a constant. \n', 'Hey @ncullen93 \r\nQuick question: How can I change the `parameter rate` of the `Dropout` class from a `callback` function? It cannot find any reference the callback can use to reach it.\r\n\r\nThanks!\r\n', 'In the callback function the model is given and you can iterator over the\r\nlayers.\r\n\r\n', ""I'll checkout it, thanks!\n\nOn Mon, Mar 20, 2017 at 3:37 PM, cnx <notifications@github.com> wrote:\n\n> In the callback function the model is given and you can iterator over the\n> layers.\n>\n> On Mon, 20 Mar 2017 at 07:48 oak-tree <notifications@github.com> wrote:\n>\n> > Hey @ncullen93 <https://github.com/ncullen93>\n> > Quick question: How can I change the parameter rate of the Dropout class\n> > from a callback function? It cannot find any reference the callback can\n> > use to reach it.\n> >\n> > Thanks!\n> >\n> > —\n> > You are receiving this because you modified the open/close state.\n> >\n> >\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/fchollet/keras/issues/3424#issuecomment-287737879>,\n> > or mute the thread\n> > <https://github.com/notifications/unsubscribe-auth/\n> AA5UUDdG395fg4zfWjd0XPTf6xffrD41ks5rnmekgaJpZM4Jfq9b>\n> > .\n> >\n>\n> —\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/fchollet/keras/issues/3424#issuecomment-287760767>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAouEelWXXdk6wzEKdIHnVRKHPquwWCkks5rnoEYgaJpZM4Jfq9b>\n> .\n>\n"", 'Overwriting the `call` function is not working for me because the `call` function is not called on every forward pass. Any ideas? \r\n```\r\ndef call(self, inputs, training=None):\r\n        print(""In call function."")\r\n        def noised():\r\n            print(""noised()"")\r\n            stddev = self.t * self.stddev\r\n            return inputs * K.random_normal(shape=K.shape(inputs),\r\n                                            mean=1.0,\r\n                                            stddev=stddev)\r\n\r\n        return K.in_train_phase(noised, inputs, training=training)\r\n```\r\nI increment `self.t` in a callback function.', 'Hi, but changing the dropout rate (here Dropout.p) through keras.callback.Callback did not work for me. I think the changed rate is not reflected in the compiled model. ']","['\nmax(0,cur/N)*initial_dropoutrate\n']",['cur'],0,0
401,keras,3721,closed,Autoencoder with 0 nodes learns something,"Hi all,

I am currently running some test with simple Autoencoders. I just copied and pasted the code from this keras blog entry: https://blog.keras.io/building-autoencoders-in-keras.html

However, when I was testing different architectures, I just found out that even an Autoencoder with zero nodes in the (well, technically not even existing) first layer appears to be learning something. The loss I get is comparable with the loss I get with bigger architectures.

Could this be a bug in the Keras autoencoder code or might this be a problem of my dataset (which is quite noisy). My intuition was that I shouldn't learn anything when using a layer with zero nodes.

Any suggestions would be very helpful!
Thanks a lot! 
",stale,[],[],[],0,0
402,keras,7235,closed,ImageDataGenerator object is not an iterator,"(x, y, sample_weight)(x, y)  ",,"['If you follow the documentation : https://keras.io/preprocessing/image/\r\nYou should use `flow`.', '@Dref360 Thanks, worked for me. I should have read documentation more properly.']",['\r\n\r\nFollowing is the code for the error. I am unable to debug this.\r\n\r\n'],"['', '\r\nUsing TensorFlow backend.\r\nEpoch 1/10\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/threading.py"", line 801, in __bootstrap_inner\r\n    self.run()\r\n  File ""/usr/lib/python2.7/threading.py"", line 754, in run\r\n    self.__target(*self.__args, **self.__kwargs)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 606, in data_generator_task\r\n    generator_output = next(self._generator)\r\nTypeError: ImageDataGenerator object is not an iterator\r\n\r\nTraceback (most recent call last):\r\n  File ""/home/nikhil/workspace/data learning/test.py"", line 52, in <module>\r\n    new_model.fit_generator(datagen, steps_per_epoch= len(x_train), epochs=10)    \r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.py"", line 88, in wrapper\r\n    return func(*args, **kwargs)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 1097, in fit_generator\r\n    initial_epoch=initial_epoch)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.py"", line 88, in wrapper\r\n    return func(*args, **kwargs)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 1851, in fit_generator\r\n    str(generator_output))\r\nValueError: output of generator should be a tuple ', ' or ', "". Found: None\r\n\r\nfrom keras.datasets import mnist\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.layers import Dropout\r\nfrom keras.layers import Flatten\r\nfrom keras.layers import Dense\r\nfrom keras.layers.convolutional import Conv2D\r\nfrom keras.layers.pooling import MaxPooling2D\r\nfrom keras.utils import np_utils\r\nfrom keras.models import Sequential\r\n\r\n########## DATA PREPERATION ###########\r\n\r\n# load data\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n# reshape to be [samples][width][height]\r\nx_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\r\nx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\r\n# convert from int to float\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\nx_train/=255\r\nx_test/=255\r\n# data preprocessing\r\ndatagen = ImageDataGenerator(horizontal_flip=True)\r\ntest_datagen = ImageDataGenerator()\r\n# fit parameters from data\r\ndatagen.fit(x_train)\r\ntest_datagen.fit(x_test)\r\n# one hot encoding the output\r\ny_train = np_utils.to_categorical(y_train)\r\ny_test = np_utils.to_categorical(y_test)\r\nno_of_class = y_test.shape[1]\r\n############# model ###########\r\n\r\ndef create_model():\r\n    model = Sequential()\r\n    model.add(Conv2D(32, kernel_size=(5, 5),activation='relu',input_shape=(28,28,1)))\r\n    model.add(MaxPooling2D(pool_size=(2, 2)))\r\n    model.add(Conv2D(15, (3, 3), activation='relu'))\r\n    model.add(MaxPooling2D(pool_size=(2,2)))\r\n    model.add(Dropout(0.2))\r\n    model.add(Flatten())\r\n    model.add(Dense(128, activation='relu'))\r\n    model.add(Dense(50, activation='relu'))\r\n    model.add(Dense(no_of_class, activation='softmax'))\r\n    # compiling the model\r\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\r\n    return model\r\n\r\nnew_model=create_model()\r\nnew_model.fit_generator(datagen, steps_per_epoch= len(x_train), epochs=10)\r\n"", '']",0,0
403,keras,109,closed,[help wanted] any way to update learning rate and momentum,"Hi, I am reading [a tutorial with nolearn/lasagne](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/), where the author point out it is possible to [update learning rate and momentum over time](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/#changing-learning-rate-and-momentum-over-time).

Is it possible to do it in keras? The coverage speed is low, so I am wondering whether it's possible to use the technique to improve the speed. 
",,"['Look at the `decay` parameter in SGD to get a decreasing learning rate over time.\n\nOtherwise, you can manually set `sgd.momentum`, `sgd.lr` and `sgd.decay` (where `sgd` is an instance of `SGD`). Note that `sgd.lr` is the _base_ learning rate, not the actual learning rate, ie. it is the learning rate before decay is applied.\n', 'How is the decay applied to the learning rate?  Is it multiplied by the decay factor after each epoch?  Is the decay subtracted from it?  Is it exponential?\n']",[],[],0,0
404,keras,703,closed,Models zoo,"Mentioned in the ""mega-issue"" #100, though it would be worth having a separate one.

The simplest model zoo is a github wiki page, another option it to have it as a page in the documentation and receive updates via push requests.

I think model zoo matters most for models that take ""more 6 hours to train"" (i.e. a large fraction of 24h). 
",,"['For now we have one VGG net ;-) https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3\n\nIn the near future I hope we will be able to load arbitrary Caffe models, which would give us access to the Caffe model zoo.\n', ""Hello,\n\nWorking with Keras what I have missed has been some trained model, so taking the VGG net you previously share and a model that I [manually ported](https://gist.github.com/albertomontesg/d8b21a179c1e6cca0480ebdf292c34d2) from [C3D](https://github.com/facebook/C3D) (Caffe's fork that allow 3D convolution), I created a repository and some contribution guidelines to share between the community trained models for Keras.\n\nI invite you to share your models on the repository:\nhttps://github.com/albertomontesg/keras-model-zoo\n\nThanks,\n"", 'Closing this old issue. The Keras model zoo is the applications module: https://keras.io/applications/']",[],[],0,0
405,keras,7814,closed,"y_ture.shape is expected to be (N,3) yet (?,?,?,?) is received","My model looks something like this :

now the problem is in the y_ture.shape as whenever i try to access y_trrue to do needed calculation inside the loss function i get y_true.shape as 

y_true (?, ?, ?, ?)
y_true Tensor(""decoder/FinalConv_target:0"", shape=(?, ?, ?, ?), dtype=float32)

Thank you!

- [-] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [-] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [-] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [-] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",,[],[],[],0,0
406,keras,7055,closed,CNTK cannot take variable length inputs,"When given a variable length input, CNTK fixes it to the length of the first example.

The following example runs under Theano and Tensorflow.


CNTK fails, since once it has seen the first batch, it assumes the second dimension is fixed to 100:

",stale,"['@souptc could you please advise? At the very least we would need a better error message.', 'yes it is a known limitation, we are testing the feature with variable length and hopefully we could make it work in next release. I will see could we provide better message first.', 'Bug report upstream for reference: https://github.com/Microsoft/CNTK/issues/1328', '@souptc in the upstream report it says it is now possible setting reduction_rank = 0. Is this applicable to Keras?', '@Dapid Just synced with Cha in CNTK team, the workaournd ""reduce_rank = 0"" is currently faked in cntk\'s high level python api, which is telling CNTK to do convoconvolution along depth axis instead of just dong inner product and sum. Is that what you want?\r\n\r\nWe are actively working on support cntk convolution and other ops with free dimension axis which could accept input with variable length, if everything goes well, should be ready in next CNTK release (in Sept.)', ""I see, thanks. I think I'll have to wait until September, then. My main application are 1D convolutions on variable length sequences."", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']","[""\r\nimport numpy as np\r\nfrom keras.layers import Input, Conv1D\r\nfrom keras.models import Model\r\n\r\n# Get model:\r\ninp = Input(shape=(None, 16))\r\nout = Conv1D(2, 3, padding='same')(inp)\r\n\r\nmodel = Model(inputs=inp, outputs=out)\r\nmodel.compile('adam', 'mse')\r\n\r\nfor len_ in (100, 200):\r\n    x = np.random.random((4, len_, 16))\r\n    model.predict(x)\r\n"", '\r\nTraceback (most recent call last):\r\n  File ""do.py"", line 19, in <module>\r\n    model.predict(x)\r\n  File ""/home/david/.virtualenv/py35/lib/python3.5/site-packages/keras/engine/training.py"", line 1594, in predict\r\n    batch_size=batch_size, verbose=verbose)\r\n  File ""/home/david/.virtualenv/py35/lib/python3.5/site-packages/keras/engine/training.py"", line 1218, in _predict_loop\r\n    batch_outs = f(ins_batch)\r\n  File ""/home/david/.virtualenv/py35/lib/python3.5/site-packages/keras/backend/cntk_backend.py"", line 1609, in __call__\r\n    output_values = self.metrics_func.eval(input_dict, as_numpy=False)\r\n  File ""/home/david/.virtualenv/py35/lib/python3.5/site-packages/cntk/ops/functions.py"", line 626, in eval\r\n    _, output_map = self.forward(arguments, outputs, device=device, as_numpy=as_numpy)\r\n  File ""/home/david/.virtualenv/py35/lib/python3.5/site-packages/cntk/internal/swig_helper.py"", line 69, in wrapper\r\n    result = f(*args, **kwds)\r\n  File ""/home/david/.virtualenv/py35/lib/python3.5/site-packages/cntk/ops/functions.py"", line 760, in forward\r\n    keep_for_backward)\r\n  File ""/home/david/.virtualenv/py35/lib/python3.5/site-packages/cntk/cntk_py.py"", line 1568, in _forward\r\n    return _cntk_py.Function__forward(self, *args)\r\nValueError: The trailing dimensions of the Value shape \'[4 x 200 x 16]\' do not match the Variable \'Input(\'input_1\', [#], [100 x 16])\' shape \'[100 x 16]\'.\r\n']",[],0,0
407,keras,5083,closed,Bug on the save/load mechanism of version 1.2.0,"I believe there is a bug on the save/load mechanism of version 1.2.0. The problem did not exist on version 1.1.2.

To reproduce the bug I'll be using the following code snippet taken from the Keras documentation: [Fine-tune InceptionV3 on a new set of classes](https://keras.io/applications/)


Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 143, in load_model
    model.load_weights_from_hdf5_group(f['model_weights'])
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 2753, in load_weights_from_hdf5_group
    str(len(flattened_layers)) + ' layers.')
ValueError: You are trying to load a weight file containing 190 layers into a model with 2 layers.",,"['Can you try setting trainable to True for all layers before saving model in the end? trainable might produce illusion to make model think it only has two layers. ', '@joelthchao: Thanks for the reply.\r\n\r\nIf I set all the layers to trainable before the call on the save() I get the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 179, in load_model\r\n    model.optimizer.set_weights(optimizer_weight_values)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/optimizers.py"", line 90, in set_weights\r\n    \'provided weight shape \' + str(w.shape))\r\nValueError: Optimizer weight shape (96,) not compatible with provided weight shape (320,)\r\n', 'Are you on theano or tensorflow? Can you check what your dim ordering is set to? Do you get the same error if you switch it to whatever the other one is?\r\n\r\nCheers', ""I am using tensorflow and my dim ordering is set to tf. I'll check this out but can you reproduce the problem on your side?"", ""@datumbox I'm on windows, current git master of keras and theano. I threw some random images into some directories and your code sample runs fine for me. Does it work for you if you switch to th dim ordering?"", '@bshickel: Thanks a lot for the prompt reply. Interesting, not being able to reproduce it complicates the matters...\r\n\r\nI\'ve tested the code on 2 computers (Ubuntu 14 with Tensorflow 0.12.0-rc1 & Ubuntu 16 with Tensorflow 0.12.1) and I get the same error. I am using the latest version of Keras (1.2.0) installed using pip. I also changed the dim ordering as you suggeted from tf to th and I get exactly the same exceptions.\r\n\r\nWithout setting trainable=true in all the layers:\r\nTraceback (most recent call last):\r\n  File ""test.py"", line 44, in <module>\r\n    model = load_model(\'./data/tempModel\')\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 143, in load_model\r\n    model.load_weights_from_hdf5_group(f[\'model_weights\'])\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 2753, in load_weights_from_hdf5_group\r\n    str(len(flattened_layers)) + \' layers.\')\r\nValueError: You are trying to load a weight file containing 190 layers into a model with 38 layers.\r\n\r\nWith trainable=true in all the layers:\r\nTraceback (most recent call last):\r\n  File ""test.py"", line 47, in <module>\r\n    model = load_model(\'./data/tempModel\')\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 179, in load_model\r\n    model.optimizer.set_weights(optimizer_weight_values)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/optimizers.py"", line 90, in set_weights\r\n    \'provided weight shape \' + str(w.shape))\r\nValueError: Optimizer weight shape (96,) not compatible with provided weight shape (320,)\r\n\r\n', ""I tested again with Keras v1.2.1 and the problem is resolved.\r\n\r\n@bstriner As you said earlier, you were testing using the latest snapshot at the time and you could not reproduce it. Possibly it was resolved on the repo immediately after the release of 1.2.0 and you were using a patched version. \r\n\r\nI'm closing the issue. :)""]","[""python\r\nfrom keras.preprocessing import image\r\ngenerator = image.ImageDataGenerator().flow_from_directory('./data/cifar2-train', target_size=(224, 224), batch_size=32, class_mode='categorical', shuffle=True)\r\n\r\n# ==== Start of Code Snippet from Keras Documentation: https://keras.io/applications/ ====\r\nfrom keras.applications.inception_v3 import InceptionV3\r\nfrom keras.preprocessing import image\r\nfrom keras.models import Model\r\nfrom keras.layers import Dense, GlobalAveragePooling2D\r\nfrom keras import backend as K\r\n\r\nbase_model = InceptionV3(weights='imagenet', include_top=False)\r\n\r\nx = base_model.output\r\nx = GlobalAveragePooling2D()(x)\r\nx = Dense(1024, activation='relu')(x)\r\npredictions = Dense(2, activation='softmax')(x)\r\n\r\nmodel = Model(input=base_model.input, output=predictions)\r\n\r\nfor layer in base_model.layers:\r\n    layer.trainable = False\r\n\r\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy')\r\n\r\nmodel.fit_generator(generator, samples_per_epoch=1000, nb_epoch=1)\r\n\r\nfor i, layer in enumerate(base_model.layers):\r\n   print(i, layer.name)\r\n\r\nfor layer in model.layers[:172]:\r\n   layer.trainable = False\r\nfor layer in model.layers[172:]:\r\n   layer.trainable = True\r\n\r\nfrom keras.optimizers import SGD\r\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\r\n\r\nmodel.fit_generator(generator, samples_per_epoch=1000, nb_epoch=1)\r\n# ==== End of Code Snippet ====\r\n\r\nmodel.save('./data/tempModel')\r\ndel model\r\nfrom keras.models import load_model\r\nmodel = load_model('./data/tempModel')\r\n""]",[],0,0
408,keras,5088,closed,"Saving a model that does not import the backend as ""K"" breaks model loading.","The following code produces this error: .

",stale,"['Quick solution: change backend import name, keras doesn\'t know your variable in lambda layer\r\n```python\r\nfrom keras import backend as K\r\nfrom keras.layers import Dense, Input, Lambda\r\nfrom keras.models import load_model, Model\r\n\r\nx = Input((100, ), dtype = ""float32"")\r\nx_i = Lambda(lambda x: x + K.epsilon())(x)\r\no = Dense(10, activation = ""softmax"")(x_i)\r\nmodel = Model(input = [x], output = o)\r\nmodel.compile(""sgd"", loss = ""categorical_crossentropy"")\r\nmodel.save(""toy_model.h5"")\r\nmodel = load_model(""toy_model.h5"")\r\n```', '@joelthchao - thanks for trying to help, but I had already realized that changing the import name for `backend` to `K` would make the error go away (hence my title). The problem is that Keras allows you to save a model using the actual name of the `backend` module but is unable to load such a model.', 'No quick fix but worth looking into. There are similar issues if you try to use some custom imports in your lambda. Another workaround is to do the import within the Lambda.\r\n\r\n```python\r\ndef mylambda(x):\r\n  from keras import backend\r\n  import mymodule\r\n  ... #use backend and mymodule\r\ny = Lambda(mylambda)(h)\r\n```', 'I guess this is more readable way to solve: https://github.com/keras-team/keras/issues/4609#issuecomment-329292173\r\njust wire the used lambda object so keras could know where it\'s from, like:\r\n\r\n```\r\nfrom keras import backend\r\nfrom keras.models import load_model\r\nfrom keras.activations import softmax\r\n\r\nmodel = load_model(""yourmodel.h5"", custom_objects={\r\n    ""backend"": backend,\r\n    ""softmax"": softmax,\r\n})\r\n```\r\nlooks like you have to add any extra module used in your lambda layer, even it\'s from keras itself (ex: the softmax here)', ""custom_objects helped me to provide my custom function name which i am using inside lambda function in the model. I was able to load the model now which otherwise was showing that 'customobject' is not found error.""]","['python\r\nfrom keras import backend\r\nfrom keras.layers import Dense, Input, Lambda\r\nfrom keras.models import load_model, Model\r\n\r\nx = Input((100, ), dtype = ""float32"")\r\nx_i = Lambda(lambda x: x + backend.epsilon())(x)\r\no = Dense(10, activation = ""softmax"")(x_i)\r\nmodel = Model(input = [x], output = o)\r\nmodel.compile(""sgd"", loss = ""categorical_crossentropy"")\r\nmodel.save(""toy_model.h5"")\r\nmodel = load_model(""toy_model.h5"")\r\n']","[""NameError: name 'backend' is not defined""]",0,0
409,keras,1722,closed,How to load weights learned in one network into another?,"I would like to train a graph on task, add a layer, then resume training.  What is the best way to do this?  I won't always be adding a layer to the end of the graph.

I am assuming I will perform the first step with one graph, save the weights, and attempt to initialize a new graph with the weights learned in the first step.  The current code seems to rely upon the order of the params list when naming the weights in the hdf file.  If I want to be able to make changes to the graph that would change that order, I need a method for knowing which params to set when reloading the weights.

My current idea is to generate a unique name for each param based on the layer name, and store the params in the hdf file according to their unique names.  Then when loading the weights into the modified graph I would know which params to set.

Is this a sensible approach?  Would it break things?  Would it be compatible with all existing functionality?

thanks for your excellent work
",stale,[],[],[],0,0
410,keras,3580,closed,I need to do some custom changes to a network's behavior.,"Newbie to Keras alert!
1. I need to compute a recursive network's error just on the last time step of a network and have that BPTT to correct the network, can this be done by changing the objective function?, is there some better way?
2. I need a recursive layer that doesn't have all recursive connections in the sense that each neuron only knows it's last output and not the outputs of all the neurons in its layer, can this be done by creating a model for each neuron and then merging the outputs? Could I force some of the weights to remain zero?
",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],0,0
411,keras,1023,closed,Combining different filter lengths in 1D convolutional layers,"[1] show that combining filter lengths around the 'best' filter length achieves better sentence classification results than just using the 'best' filter length. Is there a possibility to specify a combination of filter lengths for 1D convolution? If not, is this a feature that should be implemented?

Zhang, Y., & Wallace, B. (2015). A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification, (1). Retrieved from http://arxiv.org/abs/1510.03820
",,"['This is kinda of topic but thanks a lot for the paper :) \nFor the question itself i think that it should be up to the user to do that, on the other side the k-max pooling layer is missing from Keras and that is a big issue.\n', ""Np. :)\nIf it's up to the user, how would I go about implementing combined feature lengths? Can I have separate convolution layers of different configurations that convolve on the same input and merge them?\n"", 'Using the Graph model yes. Give me 1 or two hours and i will come back i was planning to add it to my codebase :)\n', 'It was faster then i thought.\n\n``` python\n#let\'s define some variables\nnb_filters = 100\nfilter_lenghts = [3,4,5]\nconvs = []\nmaxlen = 30\n\nmodel = Graph()\n#adding the input as usual\nmodel.add_input(name=\'sentence\', input_shape=(1,), dtype=int)\n#passing the sentence through the Embedding layer\nmodel.add_node(Embedding(nb_words, emb_dim), name = \'sentence_embeddings\', input = \'sentence\')\n\n#and now create the convolutions/max_pooling \nfor i in filter_lenghts:\n    model.add_node(Convolution1D(nb_filter=nb_filters,\n                                 filter_length=i,\n                                 border_mode=""valid"",\n                                 activation=""relu"",\n                                 subsample_length=1), name=str(i)+\'_convolution\', input= \'sentence_embeddings\')\n\n    #that\'s an approximation of 1-maxpooling that i never really checked.\n    model.add_node(MaxPooling1D(pool_length= maxlen - i + 1), name=str(i)+\'_maxpooling\', input=str(i)+\'_convolution\')\n    model.add_node(Flatten(), name = str(i)+\' sentence_embedding\', input=str(i)+\'_maxpooling\')\n    convs.append(str(i)+\' sentence_embedding\')\n\n#now concat  all the results and  do whatever you want with the new sentence_embedding :D\nmodel.add(NEXT_LAYER, name=\'whatever\', inputs=convs)\n\n\n```\n\nI rewritten that in that comment never really compiled but if some error is raised it should be easily debugged.\n\nIn order to implement it with the sequential model you should  replicate your input sentence many times but i\'m not a fun of this approach.\n', '@sebastianruder Just a reminder do not exaggerate on the nb_filters because depending on the task you are modelling it might not converge (just experienced that problem). `200` may be good if you have only one convolution but with more `50` may be enough.\n', '@dbonadiman: Wow, that was fast indeed! Thanks a lot for the commented implementation snippet and the tip! :D\n', '@sebastianruder It is fast to do these implementations if you have a working model to test them over ;)\n', 'It creates an identifier issue:\n  File ""build/bdist.linux-x86_64/egg/keras/layers/containers.py"", line 366, in add_node\nException: Unknown identifier: 7input\n', 'This graph model can still be created with keras 2.0 using sequential models?', '@Ambros94 They are different containers actually. You should check the [documentation](https://keras.io/layers/containers/) for further details.']",[],[],0,0
412,keras,3295,closed,How to use data (image) augmentation with fit_generator()?,"@fchollet and other kerasors,
We all know that if we are dealing with large scale of data such as ImageNet, we could write a customized generator which produces batch data (often as numpy.array) from disk. Then we could train our model with . But, if we want to use  to do the online data augmentation at the same time, what is the simplest way to implement? Note that I would like to use its  method instead of  method.
",stale,"[""It's possible to do this by doing the following:\n\n``` python\ndatagen = ImageDataGenerator(...)\ntrain_generator = datagen.flow(X_train, Y_train, batch_size=128)\nmodel.fit_generator(train_generator, samples_per_epoch=len(X_train), ...)\n```\n\nWhat I'd like to know is how it's possible to combine ImageDataGenerator with a custom thread-safe data augmentation generator and flow from these?\n"", '@tetmin You are assuming that `X_train` and `Y_train` could be loaded into memory at once. My concern is that if not, then we have to write a customized generator which loads a batch of training data from disk.  And how to combine the generator with online data augmentation. \n', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs, but feel free to re-open it if needed.\n', 'I too want to know. I have written my own batch generator that read batches from disk (all images do not fit in memory), and want to combine this with the ImageDataGenerator.\r\n\r\n```python\r\ndef batch_generator(df, batch_size, path_tiles, num_classes):\r\n    """"""This generator use a pandas DataFrame to read images (df.tile_name) from disk.\r\n    """"""\r\n    N = df.shape[0]\r\n    while True:\r\n        for start in range(0, N, batch_size):\r\n            x_batch = []\r\n            y_batch = []\r\n            end = min(start + batch_size, N)\r\n            df_tmp = df[start:end]\r\n            ids_batch = df_tmp.tile_name\r\n            for id in ids_batch:\r\n                img = cv2.imread(path_tiles+\'/{}\'.format(id))\r\n                # [0] since duplicated names\r\n                labelname=df_tmp[\'y\'][df_tmp.tile_name == id].values[0]  \r\n                labelname=np.asscalar(labelname)\r\n                x_batch.append(img)\r\n                y_batch.append(labelname)\r\n            x_batch = np.array(x_batch, np.float32) / 255\r\n            y_batch = utils.np_utils.to_categorical(y_batch, num_classes) \r\n            yield (x_batch, y_batch)\r\n\r\nmodel.fit_generator(generator=batch_generator(df_train, \r\n                                              batch_size=batch_size,\r\n                                              path_tiles=path_tiles,\r\n                                              num_classes=num_classes), \r\n                    steps_per_epoch=len(df_train) // batch_size, \r\n                    epochs=epochs)\r\n```', 'I just put this in the training loop...\r\n\r\n`\r\n#first load your batch from disk using custom functions\r\nx= load_img (data_path,batch_num*batchSize,(batch_num+1)*batchSize)\r\nx = preprocess_img(x)\r\n\r\n#then augment your batch\r\nfor x in img_datagen.flow(x, batch_size=batchSize, seed =seed):\r\n            x=x\r\n            break\r\n#this generates one batch of augmented data\r\n` ', 'Is there any suggestion for this issue. I have the same of this issue']",[],"['model.fit_generator()', 'ImageDataGenerator', 'flow()', 'flow_from_directory()']",0,0
413,keras,1989,closed,How do you predict values for categories?,"In the CNN example for the minst dataset:
https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py

they tell you how to make a good CNN network to recognise hand written digits.

The issue is that it doesn't tell you how to predict new digits.

For example give an image, if I do this:



instead of telling me what digits it think it is, it instead gives me a list of 10 numbers (presumably probabilities)

Please help!
",,"['Yes they are indeed the probability of each class.\nYou can use model.predict_classes(X, batch_size=128, verbose=1)\nFollowing the doc this  ""Generates class predictions for the input samples batch by batch.""\n', 'good question, thank you, it would be great to know answer\n', 'And why to use batches, is it possible to send one by one?\nMay you add link to this place in documentation\nOn Mar 18, 2016 12:37, ""Yahya Uddin"" notifications@github.com wrote:\n\n> Closed #1989 https://github.com/fchollet/keras/issues/1989.\n> \n> —\n> You are receiving this because you commented.\n> Reply to this email directly or view it on GitHub\n> https://github.com/fchollet/keras/issues/1989#event-595149399\n']",[],['model.predict(image)'],0,0
414,keras,1917,closed,The loss becomes negative,"Hi,
I jut ran a CNN built with Keras on a big training set, and I has weird loss values at each epoch (see below):
66496/511502 [==>...........................] - ETA: 63s - loss: 8.2800
66528/511502 [==>...........................] - ETA: 63s - loss: -204433556137039776.0000

345664/511502 [===================>..........] - ETA: 23s - loss: 8.3174
345696/511502 [===================>..........] - ETA: 23s - loss: -39342531075525840.0000

214080/511502 [===========>..................] - ETA: 41s - loss: 8.3406
214112/511502 [===========>..................] - ETA: 41s - loss: -63520753730220536.0000

How is that possible? The loss becomes suddenly to big and the value gets bigger than the double encoding?
Is there a way to avoid it?

Regards,
",stale,"[""Do you really think that's enough information for anyone to be able to answer your question?\n"", ""The loss is just a scalar that you are trying to minimize. It's not supposed to be positive! For instance a cosine proximity loss will usually be negative (trying to make proximity as high as possible by minimizing a negative scalar).\n"", ""Hi, thank you for your answers.\nThe last layer is a dense layer with a sigmoid, so the value should not be negative:\n`model.add(Dense(1))\nmodel.add(Activation('sigmoid'))`\nWhat really surprise me is that from one batch to the next, there is such a fall.\n@the-moliver: which information would you need?\n"", 'What is your training objective, `binary_crossentropy` or others?\nAnd if you are using GPU to train your network, the datatype should be in `float32` (theano restriction).\n', 'Hi, thank you for your help.\nYes, the training objective is binary_crossentropy.\nAnd yes, all my data are already float32, I made sure of that.\n', ""@FiReTiTi \nI don't think `binary_crossentropy` could return the negative values.\nIn Theano backend,\n\n```\ncrossentropy(t,o) = -(t*log(o) + (1 - t)*log(1 - o)).\n```\n\n`t` and `o` are all in range of [0,1], making the whole equation non-negative.\nyour output log more seems like an overflow.\nMaybe you can check on your data ? or you can extract a proportion of data to test if the model still gives the negative loss.\n\n> 66496/511502 [==>...........................] - ETA: 63s - loss: 8.2800\n> 66528/511502 [==>...........................] - ETA: 63s - loss: -204433556137039776.0000\n"", 'If the loss cannot be negative, then does it mean it goes over the encoding limits and then it loops back into the negative values?\n', 'I am still working on the same data, and here is an other weird thing:\n`Epoch 140/3000`\n`5s - loss: 0.5968 - val_loss: 0.4191`\n`Epoch 141/3000`\n`5s - loss: 0.5974 - val_loss: 0.4556`\n`Epoch 142/3000`\n`5s - loss: 0.5979 - val_loss: 0.4382`\n`Epoch 143/3000`\n`5s - loss: 6.0467 - val_loss: 11.1324`\n`Epoch 144/3000`\n`5s - loss: 7.7176 - val_loss: 11.1324`\n`Epoch 145/3000`\n`5s - loss: 7.7176 - val_loss: 11.1324`\n`Epoch 146/3000`\n`5s - loss: 7.7176 - val_loss: 11.1324`\nAnd nothing changes during the next 2850 epochs, perfectly identical.\n', '@FiReTiTi please give more information about your model if you want help on that.\n\nIn your last case, your optimiser is likely to be stuck in a local minima. That could explain why it remains identical during all your next iterations. \n', ""Here is the model:\n    model = Sequential()\n    model.add(Convolution2D(8, 7, 7, border_mode='valid', input_shape=(1, 31, 31), activation='tanh'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Convolution2D(16, 5, 5, border_mode='valid', activation='tanh'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Convolution2D(32, 3, 3, border_mode='valid', activation='relu'))\n    model.add(Convolution2D(65, 1, 1, border_mode='valid', activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(23))\n    model.add(Activation('tanh'))\n    model.add(Dropout(0.1))\n    model.add(Dense(11))\n    model.add(Activation('sigmoid'))\n    model.add(Dropout(0.1))\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n    optimizer = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n    model.fit(dataset, labels, batch_size=batch_size, nb_epoch=nb_epoch, shuffle=True, validation_split=0.1, verbose=2)\n\nThe dataset contains 70 000 images of size 31x31.\nWhat I don't understand is why there is this sudden jump of the loss and then the loss is stuck.\n"", ""@FiReTiTi May I ask how did you solve the negative loss problem? I ran into the same problem, my loss is a customed loss with a bunch of mse, so it shouldn't be negative either. It looks like this:\n\n```\nEpoch 1/15\n   32/33102 [..............................] - ETA: 15552s - loss: -88028794.750\n   48/33102 [..............................] - ETA: 11176s - loss: -1419246161.8\n   64/33102 [..............................] - ETA: 8987s - loss: -13590295485.3\n   80/33102 [..............................] - ETA: 7674s - loss: -107586018455.\n   96/33102 [..............................] - ETA: 6797s - loss: -661847078867.\n  112/33102 [..............................] - ETA: 6172s - loss: -3960883097561\n  128/33102 [..............................] - ETA: 5702s - loss: -3047999712303\n  144/33102 [..............................] - ETA: 5337s - loss: -2318531227797\n  320/33102 [..............................] - ETA: 3720s - loss: -1825597231654244712448.0000\n```\n"", ""I haven't, it still happen time to time :-(\r\nUse smaller NNs seems to reduce the phenomenon.\r\n"", ""@FiReTiTi Thanks for your reply. I found my problem, I use a custom loss and accidentally put y_pred and y_true in the wrong order when passing them to my loss function, so maybe it's not the same reason in your case.\n"", '@sunshineatnoon so you were using a non symmetric loss function like the cross entropy?\n', ""[Here](https://github.com/fchollet/keras/issues/2023) is my loss function, it's a bunch of squared values.\n"", 'Cool for you!\nIt happened to me with a binary_crossentropy :-(\n', ""@FiReTiTi In that case, I think it's more likely an overflow. Do you use Theano? Maybe you can try [NanGuardMode](http://deeplearning.net/software/theano/library/compile/nanguardmode.html) in theano to see if it gives you any errors or warnings. I googled a lot last night and found that Nans or Infs might cause this kind of error. Such as [this one](https://groups.google.com/forum/#!topic/theano-users/d7G-xwt6KL4)\n"", ""That's also my opinion. Thanks for the tips, I will test them when it occurs again.\n"", ""My loss is negative, what does that mean? I am using tensorflow backend.\n\nEpoch 1/10\n2536/2536 [==============================] - 584s - loss: -7.7728 - acc: 0.2492 - val_loss: -7.9712 - val_acc: 0.2500\n\nMy code is here for reference:\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.preprocessing.image import ImageDataGenerator\n\nimport numpy as np\n\nmodel = Sequential()\n\nmodel.add(Convolution2D(3, 3, 32, border_mode='valid', dim_ordering='tf', input_shape=(150, 200, 3)))\nmodel.add(Activation('relu'))\nmodel.add(Convolution2D(3, 3, 32))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25)) \n\nmodel.add(Convolution2D(64, 3, 3, border_mode='valid'))\nmodel.add(Activation('relu'))\nmodel.add(Convolution2D(64, 3, 3))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(64))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\ntrain_datagen = ImageDataGenerator(\n        rescale=1./255,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True)\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n        'Data/Train',  # this is the target directory\n        target_size=(150, 200),  # all images will be resized to 150x150\n        batch_size=32,\n        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels\n\nvalidation_generator = test_datagen.flow_from_directory(\n        'Data/Validation',\n        target_size=(150, 200),\n        batch_size=32,\n        class_mode='binary')\n\nmodel.fit_generator(train_generator, samples_per_epoch=2536, nb_epoch=10, validation_data=validation_generator, nb_val_samples=800)\n\nmodel.save_weights('thesis.h5')\n"", '@zach-nervana FYI\r\nSome possible reason is listed in [this](https://stackoverflow.com/questions/42264649/keras-binary-crossentropy-has-negative-values/47503934#47503934) stackoverflow question.', ""Hello everyone,\r\n\r\nAs we all know, the kld loss can not be negative, I am training a regression model, and get negative values.\r\nHere is my model: \r\n# model\r\nbase_model = VGG16(input_shape=(360, 480, 3), weights='imagenet', include_top=False)\r\nx = base_model.layers[-2].output\r\nx = MaxPool2D(pool_size=(2, 2), padding='same', strides=(1, 1), name='block5_pool')(x)\r\nx = Conv2D(32, (7, 7), activation='relu', padding='same', name='block5_conv5')(x)\r\nx = Conv2D(8, (7, 7), activation='relu', padding='same', name='block5_conv6')(x)\r\nx = Conv2D(1, (7, 7), activation='relu', padding='same', name='block5_conv7')(x)\r\nx = Flatten(name='flatten')(x)\r\nprediction = Activation('softmax')(x)  # problem come in!!!!!!!!!!!\r\nmodel = Model(inputs=base_model.input, outputs=prediction)\r\n\r\n# compile\r\nadam = Adam(lr=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.0)\r\nmodel.compile(optimizer=adam, loss='kld', metrics=['accuracy'])\r\n\r\nThe problem is, if I add a softmax layer at end of the model, the loss is positive, which is fine, but the loss is around 32, it is really big. But if I remove the softmax layer, the loss becomes negative. \r\n\r\nFor the input and output, input are images, I normalize the images to 0-1, and labels also 0-1. \r\nMy point is, this is a regression model, I do not want to add a softmax layer at end of the model, but the loss becomes negative, which is not right. Is there someone has a idea? How to solve the problem?\r\n"", ""@FiReTiTi Did you solve your problem? I had a similar problem. I used theano as backend, and the loss function is binary_crossentropy, during the training, the acc, val_acc, loss, and val_loss never changed in every epoch, and loss value is very high , about 8. I used 4000 training samples 1000 validation samples\r\nthis is my model:\r\n\r\n`inputs_x=Input(shape=(1,65,21))\r\nx=Conv2D(64,(3,3),padding='same',data_format='channels_first',activation='relu',use_bias=True)(x)\r\nx=Conv2D(64,(3,3),padding='same',data_format='channels_first',activation='relu',use_bias=True)(x)\r\nx=MaxPooling2D(pool_size=(2,2),strides=(2,2))(x)\r\n\r\nx=Conv2D(32,(5,5),padding='same',data_format='channels_first',activation='relu',use_bias=True)(x)\r\nx=Conv2D(16,(5,5),padding='valid',data_format='channels_first',activation='relu',use_bias=True)(x)\r\nx=MaxPooling2D(pool_size=(2,2),strides=(2,2))(x)\r\n\r\nx=Dropout(0.55)(x)\r\nx=Flatten()(x)\r\n\r\ninputs_y=Input(shape=(1,32,21))\r\ny=Conv2D(32,(2,2),padding='same',data_format='channels_first',activation='relu',use_bias=True)(y)\r\ny=Conv2D(32,(2,2),padding='same',data_format='channels_first',activation='relu',use_bias=True)(y)\r\ny=MaxPooling2D(pool_size=(2,2),strides=(2,2))(y)\r\n\r\ny=Conv2D(32,(4,4),padding='same',data_format='channels_first',activation='relu',use_bias=True)(y)\r\ny=Conv2D(8,(4,4),padding='valid',data_format='channels_first',activation='relu',use_bias=True)(y)\r\ny=MaxPooling2D(pool_size=(2,2),strides=(2,2))(y)\r\n\r\ny=Dropout(0.60)(y)\r\ny=Flatten()(y)\r\n\r\nmerged_input=keras.layers.concatenate([x,y],axis=-1)\r\n\r\nz=Dense(16,activation='softmax')(merged_input)\r\nz=Dense(8,activation='softmax')(z)\r\nz=Dense(4,activation='softmax')(z)\r\n\r\noutp=Dense(1,activation='softmax')(z)\r\n\r\nmodel=Model(inputs=[inputs_x,inputs_y],outputs=outp)\r\nmodel.compile(loss='binary_crossentropy',\r\n              optimizer='sgd',\r\n              metrics=['accuracy'])\r\n\r\nhistory=model.fit(x=[train_inputs_x,train_inputs_y],y=train_label,batch_size=32,\r\n          epochs=30,validation_split=0.2,shuffle=True)`\r\n\r\nAny ideas for this problem?"", 'No. It looks like an overflow problem that did not happen when I reduced the size of my model.\r\n\r\nHave you tried to switch for TensorFlow as backend. Things seems to be more stable for me since I use TensorFlow.', 'Ok, I will try switching the backend. Thanks', '@FiReTiTi Did you try to normalize your input? Non appropriate normalization of the input may lead to a gradient explosion problem.\r\n ', '@fregocap Yes, the input are normalized.', ""I had the same problem with negative loss binary crossentropy. \r\nMy model ended with\r\n```\r\nmodel.add(Dense(1))\r\nmodel.add(Activation('sigmoid'))\r\n```\r\nThe problem in my case was that the outputs given by generator were not 0 and 1 but several classes (0, 1, 2, ... 6) instead. The model unexpectedly **did not fail** but provided negative loss. \r\n\r\nThe solution is to use Dense(n_classes, activation='softmax')\r\nJust be careful with what you are doing"", 'When binary cross entropy predictions are negative, it is because the true values are not [0,1]. In my case I was using [-1,1]. The model does not fail, but produces negative value.', 'Thanks.', 'I got the negative loss, when i training autoencoder on image data and normalize the images to 0 mean and 1 std (half of data value is -ve) and using binary_crossentropy loss. Later i figure out, this is happening because of binary_crossentropy loss work as regression loss when the input is between 0 and 1, but in my case inputs are also -ve.\r\nhttp://neuralnetworksanddeeplearning.com/chap3.html', 'The answer is easy in my opinion. Your data are not between 0 and 1 and they are between 0 and 255. Just add a ""/ 255"" on your ground truth data and results will be positive.\r\n', 'Thanks Hamed. You are right']",[],[],0,0
415,keras,5861,closed,get_file() - reducing limitations,"get_file has some serious limitations, even with untar=true it can't unzip  it assumes it is a  file.

What about changing the parameter from untar to uncompress, and doing something similar to what's discussed in this [stackoverflow link for identifying compressed files and uncomrpressing them](http://stackoverflow.com/questions/13044562/python-mechanism-to-identify-compressed-file-type-and-uncompress)?

The parameter could be  and have options , , ,  etc...

Also, the md5 check should be a sha2 check since md5 is known to be insecure.

Relevant datasets:

    # original PASCAL VOC 2012
    # wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar # 2 GB

    # berkeley augmented Pascal VOC
    # wget http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz # 1.3 GB",,"[""`get_file()` also seems to stall out with big files. \r\n\r\nSomething like this might be necessary:\r\nhttp://stackoverflow.com/questions/1131220/get-md5-hash-of-big-files-in-python\r\n\r\nWhich I've implemented.\r\n\r\nOne additional quirk is that when files are extracted from a tar file, there isn't really a good check to see if the files are already there... However the current improvments are already quite useful. I think checking the contents of archives is a bit too tricky to include in this issue, so it can be part of a separate one.""]",[],"['file.tar', '.tar.gz', 'uncompress=', 'None', ""'auto'"", ""'tar'"", ""'zip'""]",0,0
416,keras,13459,closed,model.test_on_batch returns total loss instead of average - Keras 2.2.4 ,"model.test_on_batch returns total loss instead of average - Keras 2.2.4 

Tensorflow is the issue and it can be fixed using this",,[],[],[],0,0
417,keras,3975,closed,"After load model device (gpu0, gpu1, ...) don't loaded","I workings on multiple gpu and 'device' state does't save and nothing to load. Set default device for all after loading. I tried save mode full and save via YAML. Result the same.

Here simple test for understanding:



And output without device state.


",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs, but feel free to re-open it if needed.\n']","["" python\ndevice = '/gpu:1'\nwith tf.device(device):\n    model = Sequential()\n    model.add(Embedding(max_features, 1, input_length=xShape, name='inL'))\n    model.add(LSTM(64, return_sequences=True))\n    model.add(LSTM(64))\n    model.add(Dropout(0.5))\n    model.add(Dense(1))\n    model.add(Activation('sigmoid', name='outL'))\n\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n\nfor i in range(len(model.layers)):\n    print(model.layers[i].input)\n\nmodel.save('1.model')\n\na = load_model('1.model')\nprint('\\nloaded')\n\nfor i in range(len(a.layers)):\n    print(a.layers[i].input)\nmodel = a\n\n"", '\nTensor(""embedding_input_1:0"", shape=(?, 1), dtype=int32, device=/device:GPU:1)\nTensor(""Gather:0"", shape=(?, 1, 1), dtype=float32, device=/device:GPU:1)\nTensor(""transpose_1:0"", shape=(?, 1, 64), dtype=float32, device=/device:GPU:1)\nTensor(""Squeeze_3:0"", shape=(?, ?), dtype=float32, device=/device:GPU:1)\nTensor(""cond/Merge:0"", shape=(?, ?), dtype=float32, device=/device:GPU:1)\nTensor(""add_24:0"", shape=(?, 1), dtype=float32, device=/device:GPU:1)\n\nloaded\nTensor(""embedding_input_2:0"", shape=(?, 1), dtype=int32)\nTensor(""Gather_1:0"", shape=(?, 1, 1), dtype=float32)\nTensor(""transpose_5:0"", shape=(?, 1, 64), dtype=float32)\nTensor(""Squeeze_7:0"", shape=(?, ?), dtype=float32)\nTensor(""cond_1/Merge:0"", shape=(?, ?), dtype=float32)\nTensor(""add_49:0"", shape=(?, 1), dtype=float32)\n']",[],0,0
418,keras,2232,closed,How can I get the internal output of left or right model?,"

I want to get the the output of lstm layer in model0. I know how to get the internal output in the simple model without merging, like:



However, the inputs of my merging model have two styles of input. Please give me some suggestions!
",stale,"['I\'m not really clear on what you are asking, but is this what you are looking for?\n\n```\nmodel0_lstm_function = theano.function([model0.get_input(train=False)],model0.layers[0].get_output(train=False))\nmodel0_lstm_embedding = model0_lstm_function(np.random.random((128,maxlen, embedding_dims)).astype(""float32""))\n```\n', '@mbotumbo Thanks for your reply! I am afraid that my description is too vague. In fact, I have trained the merge model via the function `train_merge_model`, which has two branches `model0`, `model1`. And get the best model. then save the best model.\n\n```\ndef train_merge_model(train_q,train_a):\n\n    print(\'Build model...\')\n    model0 = Sequential()\n    model0.add(LSTM(lstm_output_dims,input_shape=(maxlen,embedding_dims)))\n    model0.add(Dropout(0.5))\n\n    model1 = Sequential()\n    model1.add(LSTM(lstm_output_dims,input_shape=(maxlen,embedding_dims)))\n    model1.add(Dropout(0.5))\n\n    modelmerge = Sequential()\n    modelmerge.add(Merge([model0, model1], mode=\'sum\'))\n    modelmerge.add(Dropout(0.5))\n\n    modelmerge.add(Dense(1))\n    modelmerge.add(Activation(\'sigmoid\')) \n    modelmerge.compile(loss=\'mse\', optimizer=\'rmsprop\')\n\n    hist = modelmerge.fit([train_q,train_a], label, batch_size=batch_size, nb_epoch=10, verbose=1, show_accuracy=True)\n    print (hist.history)\n    cPickle.dump(modelmerge0,open(""Model\\model.pkl"",""wb"")) \n\ntrain_merge_model(train_q,train_a)\nmodelmerge = cPickle.load(open(""Model\\model.pkl"",""rb"")) \n```\n\nNow, I want to get the output of the left branch in the loaded `modelmerge`. In this merge model, there is not a model0. How can I do?\n', '@Imorton-zd ah, ok I see. Well, the short answer, then is that if you have the final modelmerge object, then model0 is:\n`modelmerge.layers[0].layers[0]`\n\nThat is layers[0] of modelmerge is the Merge([model0, model1]....) layer, which in turn has a .layers of its own, which is the same as the first argument you passed it when creating it. So model0 is at Merge([model0, mode1]...).layers[0]. \n\nThat said, you might want to consider restructuring your code. One thing to consider is to use [model saving](http://keras.io/faq/#how-can-i-save-a-keras-model). Another is to actually return model0, mergemodel from your train_merge_model method. One more is to separate model creation and model training. Keep in mind that a .load_weights() for mergemodel should also populate the weights for model0 without you needing to do anything extra. \n', '@mbotumbo Thank you for your kind help! I tried your suggested method to get the intermediate layer output from a merged model and failed.\n\nMy model design is almost the same as @Imorton-zd \'s, which merges two LSTM together.\nthe code I use is \n\n```\nimport theano\nget_feature = theano.function([net_comb.get_input(train=False)],net_comb.layers[2].get_output(train=False),allow_input_downcast=False)\nfeature = get_feature([input_3D_tra, input_3D_wea])\n```\n\nand the running result is\n\n```\nTraceback (most recent call last):\n  File ""continue_training.py"", line 79, in <module>\n    allow_input_downcast=False)\n  File ""/usr/local/lib/python2.7/dist-packages/theano/compile/function.py"", line 307, in function\n    accept_inplace=accept_inplace, name=name)\n  File ""/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py"", line 1759, in orig_function\n    inputs = list(map(convert_function_input, inputs))\n  File ""/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py"", line 1863, in convert_function_input\n    (variable, value))\nTypeError: The value for input <TensorType(float32, 3D)> should not be a Variable or SymbolicInput instance (got: <TensorType(float32, 3D)>)\n\n```\n\nI also tried the officially suggested method which replace ""net_comb.get_input(train=False)]""  with ""[net_comb.layers[0].input]"", still got the same error.\n\nI do not know why it is illegal to use this kind of input. Could you kindly help me with this problem?\n', ""@m274d Lucky for you I ran into this one pretty recently. The issue is that .get_input(...) returns a list when  you are using a merge. The theano expect a list as the input when creating a function (which is why you have the [...] as the first argument of theano.function.). However, what it really ends up getting in this case (when you have a merge layer and use [model.get_input(...)] is something like [[input_0, input_1]], which isn't being interpreted correctly by theano.  \n\nThe fix is easy. Just remove the brackets from the code you posted. That is what worked for me under similar circumstances with this error. \n\nedit: typos\n"", '@mbotumbo Thank you for your kind answer! Actually I have tried this method before. However it seems not working for me. Here is my example:\n\nMy layer looks like this:\n\n```\n--------------------------------------------------------------------------------\nInitial input shape: [(None, None, 2), (None, None, 6)]\n--------------------------------------------------------------------------------\nLayer (name)                  Output Shape                  Param #             \n--------------------------------------------------------------------------------\nMerge (merge)                 (None, None, 52)              6832                \nGRU (gru)                     (None, 32)                    8160                \nDense (dense)                 (None, 20)                    660                 \nDense (dense)                 (None, 8)                     168                 \n--------------------------------------------------------------------------------\nTotal params: 15820\n--------------------------------------------------------------------------------\n```\n\nfirst I apply \n\n```\nimport theano\nget_feature = theano.function(net_comb.get_input(train=False)],net_comb.layers[2].get_output(train=False),allow_input_downcast=False)\nfeature = get_feature([input_3D_tra, input_3D_wea])\n```\n\ninput_3D_tra has feature dimension =2, time_stamp = 24 and length = 2\ninput_3D_wea has feature dimension =6, time_stamp = 24 and length = 2\nremoving only the brackets at get_feature, the results look like this\n\n```\nTraceback (most recent call last):\n  File ""continue_training.py"", line 82, in <module>\n    feature = get_feature([input_3D_tra, input_3D_wea])\n  File ""/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py"", line 786, in __call__\n    allow_downcast=s.allow_downcast)\n  File ""/usr/local/lib/python2.7/dist-packages/theano/tensor/type.py"", line 150, in filter\n    converted_data = theano._asarray(data, self.dtype)\n  File ""/usr/local/lib/python2.7/dist-packages/theano/misc/safe_asarray.py"", line 34, in _asarray\n    rval = numpy.asarray(a, dtype=dtype, order=order)\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/core/numeric.py"", line 482, in asarray\n    return array(a, dtype, copy=False, order=order)\nValueError: (\'Bad input argument to theano function with name ""continue_training.py:81""  at index 0(0-based)\', \'could not broadcast input array from shape (2,24,2) into shape (2,24)\')\n```\n\nI notice that it accepts (2,24), which is not [(None, None, 2), (None, None, 6)].\n\n---\n\nThen I remove all the brackets\n\n```\nimport theano\nget_feature = theano.function(net_comb.get_input(train=False)],net_comb.layers[2].get_output(train=False),allow_input_downcast=False)\nfeature = get_feature(input_3D_tra, input_3D_wea)\n```\n\nit returns a warning \n\n```\nTypeError: (\'Bad input argument to theano function with name ""continue_training.py:81""  at index 0(0-based)\', \'TensorType(float32, 3D) cannot store a value of dtype float64 without risking loss of precision. If you do not mind this loss, you can: 1) explicitly cast your data to float32, or 2) set ""allow_input_downcast=True"" when calling ""function"".\'\n```\n\nBut the results is wired. The output from layer[2] should be the first dense layer. If input length =2, it should return (2,20) array. But actually it returns a 3d array (2,24,2)\n\n```\narray([[[ 0.80015732,  0.6501751 ],\n        [ 0.51629576,  0.52233494],\n        [ 0.93145844,  0.64667356],\n        [ 0.97819852,  0.52441903],\n        [ 0.2520293 ,  0.82481259],\n        [ 0.11203669,  0.33217971],\n        [ 0.01192562,  0.03782118],\n        [ 0.66154183,  0.93990144],\n        [ 0.02929218,  0.2568529 ],\n        [ 0.76983954,  0.06340482],\n        [ 0.79384609,  0.98518246],\n        [ 0.79016042,  0.38315709],\n        [ 0.88156504,  0.86068901],\n        [ 0.38539249,  0.69239963],\n        [ 0.80161817,  0.22932781],\n        [ 0.46872046,  0.87681631],\n        [ 0.5203037 ,  0.99918174],\n        [ 0.49846036,  0.03460684],\n        [ 0.4608816 ,  0.85825613],\n        [ 0.8088823 ,  0.39046288],\n        [ 0.79354826,  0.73797197],\n        [ 0.17995967,  0.1731891 ],\n        [ 0.90153885,  0.08526699],\n        [ 0.91843736,  0.81101647]],\n\n       [[ 0.51629576,  0.52233494],\n        [ 0.93145844,  0.64667356],\n        [ 0.97819852,  0.52441903],\n        [ 0.2520293 ,  0.82481259],\n        [ 0.11203669,  0.33217971],\n        [ 0.01192562,  0.03782118],\n        [ 0.66154183,  0.93990144],\n        [ 0.02929218,  0.2568529 ],\n        [ 0.76983954,  0.06340482],\n        [ 0.79384609,  0.98518246],\n        [ 0.79016042,  0.38315709],\n        [ 0.88156504,  0.86068901],\n        [ 0.38539249,  0.69239963],\n        [ 0.80161817,  0.22932781],\n        [ 0.46872046,  0.87681631],\n        [ 0.5203037 ,  0.99918174],\n        [ 0.49846036,  0.03460684],\n        [ 0.4608816 ,  0.85825613],\n        [ 0.8088823 ,  0.39046288],\n        [ 0.79354826,  0.73797197],\n        [ 0.17995967,  0.1731891 ],\n        [ 0.90153885,  0.08526699],\n        [ 0.91843736,  0.81101647],\n        [ 0.48455888,  0.24200511]]]))\n```\n\nI am wondering whether I understand your guidance clearly or I made any mistakes?\n', '@m274d I know you used allow_intput_downcast, but that warning is just letting you know that the function is getting a float64 array, but it expects a float32 array. You could avoid that warning by explicitly casting your inputs to float32 with .astype(\'float32\'). \n\nThe only obvious (2,24,2) array I see in what you have described is your input. Are you sure this ""output"" is actually the output of the function and not part of the error message warning about the array dtype?\n', '@mbotumbo Thanks for your reply! I could get the output of first layer via your method `modelmerge.layers[0].layers[0]`.\n\n```\nget_left_first_layer_output = K.function([modelmerge.layers[0].layers[0].input], \n                                  [modelmerge.layers[0].layers[0].get_output(train=False)]) \n```\n\n but I can\'t ensure that the output is from `model0` or `model1`. Because I used `modelmerge.layers[1].layers[0]` to expect to get the output of `model1`, it returns the error `AttributeError: \'Dropout\' object has no attribute \'layers\'`. It means that `modelmerge.layers[1].layers[0]` aims to get the output of dropout layer, not `model1`\n\n```\nget_right_first_layer_output = K.function([modelmerge.layers[1].layers[0].input], \n                                  [modelmerge.layers[1].layers[0].get_output(train=False)]) \nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""C:\\Anaconda2\\lib\\site-packages\\spyderlib\\widgets\\externalshell\\sitecustomize.py"", line 699, in runfile\n    execfile(filename, namespace)\n  File ""C:\\Anaconda2\\lib\\site-packages\\spyderlib\\widgets\\externalshell\\sitecustomize.py"", line 74, in execfile\n    exec(compile(scripttext, filename, \'exec\'), glob, loc)\n  File ""D:/zhangdong/EMNLP/Denoising_LSTM.py"", line 263, in <module>\n    get_right_first_layer_output = K.function([modelmerge.layers[1].layers[0].input], \nAttributeError: \'Dropout\' object has no attribute \'layers\'\n```\n', '@m274d How do you show the parameters on console like you posted? Thanks!\n\n```\n--------------------------------------------------------------------------------\nInitial input shape: [(None, None, 2), (None, None, 6)]\n--------------------------------------------------------------------------------\nLayer (name)                  Output Shape                  Param #             \n--------------------------------------------------------------------------------\nMerge (merge)                 (None, None, 52)              6832                \nGRU (gru)                     (None, 32)                    8160                \nDense (dense)                 (None, 20)                    660                 \nDense (dense)                 (None, 8)                     168                 \n--------------------------------------------------------------------------------\nTotal params: 15820\n--------------------------------------------------------------------------------\n```\n', '@Imorton-zd You can get the result by doing \n\n```\nprint(model.summary())\n```\n\nBy the way, could you kindly tell me how do you feed the input data to you layers to get intermediate results? Did you delete the brackets and cast the input? It seems that you did not encounter the problem I and mbotumbo got when feeding in [input1, input2] to the merge layer.\n\nI am sorry for @mbotumbo for I am out so I cannot test your suggestion. I will post my testing results once I get access to my machine.\n', '@m274d Thanks for your reply! In fact, I am not sure for my code.\n\n```\nget_left_first_layer_output = K.function([modelmerge.layers[0].layers[0].input], \n                                  [modelmerge.layers[0].layers[0].get_output(train=False)])\nlayer_output = get_left_first_layer_output([dataq[0]])[0] \n```\n\nI only set the data of left model (`[dataq[0]]`) as input data to get intermediate results, because I only want to get the left model (`model0`) output.\nBTW, do you know how to show the model architecture graph by plot, which always yields errors.\n', ""@Imorton-zd you've got your list indexes backwards, you want `layers[0].layers[1]`. Please don't be offended by this, but it sounds it might be helpful for you to review some more basic programming guides before you proceed too far with neural network based work. \n\nEdit: I will say, much of what one encounters while doing this work can be confusing even if you do have experience with more basic programming. But life will be much easier if you understand certain fundamentals. Another useful skill is reading and understanding what error messages are trying to tell you. \n"", '@mbotumbo I really appreciate your help! Now it can generate the correct output as \n\n```\n[[ 0.70639598  0.83154148  1.13575983  1.1247735   1.22542107 -0.16233052\n  -2.4022553   1.8384738   1.22195613 -1.25081062 -0.89345479 -2.38926816\n  -0.58639354 -0.41196901 -0.00419718  1.15017164 -0.66081542 -1.81838787\n   3.28334069  0.82766891]\n [ 0.63114536  0.91458917  1.1166451   1.07084072  1.21291459 -0.04319236\n  -2.41196561  1.83151054  1.22122729 -1.2455616  -0.89959782 -2.28953505\n  -0.58363152 -0.40064844 -0.01030129  1.26979804 -0.51828861 -1.72511744\n   3.35471249  0.82223225]]\n\n```\n\nI think it will help a lot for others who have similar problem. By the way, I agree with you that my problem is mostly caused by my weak basis on python programming. I did not notice that the output was error information but not prediction result. I feel sorry for my carelessness.\n\n---\n\n@Imorton-zd Sorry I am not quite clear what you mean by ""show the model architecture graph by plot"". Could you kindly post your code used to generate what you need?\n', '@m274d glad I could help. Yes, having error text and solutions in the same place should help others in the future. \n\n@Imorton-zd is probably talking about http://keras.io/visualization/ in terms of plotting the graph. \n', ""@m274d @Imorton-zd  Could you please paste your final correct code? \nBecause I also tried to get outputs of the left part and right part separately. \nHowever, when I used the `modelmerge.layers[0].layers[0].get_output(train=False)`, it returned `AttributeError: 'Sequential' object has no attribute 'get_output'`.\n\nI print the type info. and know that `modelmerge.layers[0]` is `Merge`object, `modelmerge.layers[0].layers[0]` is `Sequential` object (i.e. model0).\nAnd my keras version is 1.0.1, is that the reason of the `get_output` error?\n"", ""@m274d @Imorton-zd I modified the `get_output` to `.output` to get the left output, but I don't know whether it's correct or not. Could you please help me?\n\nThis is the simple code to figure out how to get the left output. (`euc_dist`and `euc_dist_shape` is customized function to compute the euclidean distance of ma's output and mb's output.)\n\n```\nma = Sequential()\nma.add(Dense(30, input_dim=20))\nma.add(Dense(15))\n\nmb = Sequential()\nmb.add(Dense(15,input_dim=50))\n\nmodelmerge = Sequential()\nmodelmerge.add(Merge(layers=[ma,mb], mode=euc_dist, output_shape=euc_dist_shape))\n\nmodelmerge.compile(loss='mae',optimizer='sgd')\n\nnb_train = 20\nXa = np.random.random((nb_train, 20))\nXb = np.random.random((nb_train, 50))\ny_train = np.random.uniform(0,0.001, (nb_train, 1))\n\nnb_val = 10\nXa_val = np.random.random((nb_val, 20))\nXb_val = np.random.random((nb_val, 50))\ny_val = np.random.uniform(0,0.001, (nb_val, 1))\n\nhis = modelmerge.fit([Xa, Xb], y_train,batch_size=10,nb_epoch=2,\n       validation_data=([Xa_val, Xb_val], y_val))\nprint(his.history)\n\nget_left = K.function([modelmerge.layers[0].layers[0].input],\n                      [modelmerge.layers[0].layers[0].output])\nlout = get_left([Xa.astype('float32')])[0]\nprint lout.shape\nprint lout\n```\n\nAm I right to use this function?\n\n```\nget_left = K.function([modelmerge.layers[0].layers[0].input],\n                      [modelmerge.layers[0].layers[0].output])\nlout = get_left([Xa.astype('float32')])[0]\n```\n\nHere is the output:\n\n```\nlout.shape: (20, 15)\n[[  2.93951929e-01  -3.61830771e-01   1.44044161e-01   2.16279685e-01\n   -1.64534792e-01  -4.47121501e-01  -4.49277729e-01  -5.23708522e-01\n   -1.37832299e-01   3.57275121e-02   3.89458954e-01   3.33004355e-01\n   -7.57668391e-02   4.26238388e-01  -1.55398205e-01]\n...\n [ -4.75072175e-01  -2.87829861e-02  -3.34534287e-01   3.26558165e-02\n    1.46427348e-01   3.54637988e-02  -3.01553875e-01   1.20441474e-01\n    2.53385007e-01  -8.31620216e-01   9.50846136e-01   9.00234163e-01\n    3.56661558e-01   1.53614128e+00   4.42601264e-01]]\n\n```\n"", 'Yeah, it is right. [http://keras.io/getting-started/faq/#how-can-i-visualize-the-output-of-an-intermediate-layer]\n', '@Imorton-zd  Thank you for your reply!\n', '@walleva Have you checked your lout and rout? I tried your code, something was wired.\nThe lout[0] is supposed to similar to rout[0], but lout[0] is very same as the lout[1].\nDo you have any idea?\n']","[""\ndef train_merge_model(train_q,train_a):\n\n    print('Build model...')\n    model0 = Sequential()\n    model0.add(LSTM(lstm_output_dims,input_shape=(maxlen,embedding_dims)))\n    model0.add(Dropout(0.5))\n\n    model1 = Sequential()\n    model1.add(LSTM(lstm_output_dims,input_shape=(maxlen,embedding_dims)))\n    model1.add(Dropout(0.5))\n\n    modelmerge = Sequential()\n    modelmerge.add(Merge([model0, model1], mode='sum'))\n    modelmerge.add(Dropout(0.5))\n\n    modelmerge.add(Dense(1))\n    modelmerge.add(Activation('sigmoid')) \n    modelmerge.compile(loss='mse', optimizer='rmsprop')\n\n    hist = modelmerge.fit([train_q,train_a], label, batch_size=batch_size, nb_epoch=10, verbose=1, show_accuracy=True)\n    print (hist.history)\n"", '\nget_feature = theano.function([modelmerge.layers[0].input],modelmerge.layers[0].get_output(train=False),\n                              allow_input_downcast=False)\nfeature = get_feature(trains)\n']",[],0,0
419,keras,10221,closed,"Weighted metrics are not loaded using load_model, only normal metrics are set","When using sample weights its important to use  instead of  to get the correct accuracy. However, the current implementation of load/save_model does not take these metrics in to account. 

Current implementations:

-  only saves metrics, see https://github.com/keras-team/keras/blob/master/keras/engine/saving.py#L144
-  only sets metrics, see https://github.com/keras-team/keras/blob/master/keras/engine/saving.py#L286

This problem only occurs after a loading a model and then trying to continue training (or using one of the weighted metrics). Ideally, the weighted metrics should also be saved and subsequently loaded.",,"['Even I faced the same issue . Is there any way to re load the metrics back to Network.', 'I can confirm that this is still an issue. After loading an model, weighted_metrics remains None.\r\nSteps to reproduce (example): \r\n1. build some model and compile with `model.compile(optimizer=""Adam"", loss=\'binary_crossentropy\', metrics=[\'mae\', \'acc\'], weighted_metrics=[\'mae\', \'acc\'])`\r\n2. train some stuff, evaluate -> weighted_metrics are given\r\n3. save the model\r\n4. load it\r\n5. weighted_metrics are gone\r\n\r\nHowever, there\'s a workaround: you can load the model without compiling and add the weighted metrics yourself:\r\n`model = keras.models.load_model(filename, compile=False)`\r\n`model.weighted_metrics = [\'mae\', \'acc\']self.model.compile(optimizer=""Adam"", loss=\'binary_crossentropy\', metrics=[\'mae\', \'acc\'], weighted_metrics=[\'mae\', \'acc\'])`']",[],"['weighted_metrics', 'metrics', 'save_model', 'load_model']",0,0
420,keras,11204,closed,Can't use multiple GPUs with GANs,"**Test:** wrap the combined generator and discriminator in the multiple_gpu_model API
**Syntax:** 
**Reproduce issue:** https://gist.github.com/emilwallner/f2c411b83c499c1834fd1be6646f7389
**Error:** ValueError: The name ""model_1"" is used 2 times in the model. All layer names should be unique.",To investigate,"['@emilwallner You code works well on my node with 2 GPUs. Can you share the whole error stack?', '@yanboliang My bad, it works when I use the latest master. ']",[],['combined = multi_gpu_model(combined)'],0,0
421,keras,10685,closed,Input_shape Problem with VGG16,"Can VGG16 be trained fresh with Hyperspectral Dataset with input shape like (5, 5, 30)? I have not used the functions. Write the full code for VGG16. ",,[],[],[],0,0
422,keras,4238,closed,how to implemente deconv3D,"hi,all~
i wonder if there any body have ever use decovolution3D,  or unpooling3D ?
the inverse option to convolution3D or maxpooling option.

thank you in advance!

Please make sure that the boxes below are checked before you submit your issue. Thank you!

- [1 ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [1 ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ 1] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).",stale,['Might be usefull: https://gist.github.com/indraforyou/a8964a398474216917459e168e67ab65'],[],[],0,0
423,keras,2603,closed,Slow training due to 'on_batch_end()' and 'TimeDistributedDense()',"Hi,
I just finished to train my first RNN with keras, but I have two small technical problems. I have several Warning Messages because apparently something is slowing down my training phase.
The first problem is 'TimeDistributedDense()'. Apparently is deprecated and I am supposed to use 'TimeDistributed(Dense(...))' instead, as suggested by the warning msg itself. When I surf into keras documentation this method is cited in section 'Core', but the code line:

from keras.layers.core import TimeDistributed

gives me an error. How am I suposed to import TimeDistributed?

The second problem is 'on_batch_end()'. The original warning msg is the following:

Warning (from warnings module):
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/callbacks.py"", line 66
    % delta_t_median)
UserWarning: Method on_batch_end() is slow compared to the batch update (0.122654). Check your callbacks

I found another post here about this problem but people were saying that is due to heavy custom callbacks, but I didn't define any callback by my own. So what could be the problem?
",,"['First problem, `from keras.layers.wrappers import TimeDistributed`\nSecond problem, can you provide the script you run?\n', 'Thank you for your answer. Sorry for the first question but I had checked the wrappers section and there were only scikit-learn wrappers, so...\nAnyway the part of script related to RNN is the following:\n\n### CLASSIFICATION\n\n```\n#params\nin_neurons = 225\nhidden_neurons_1 = 21\nhidden_neurons_2 = 21\nout_neurons = 4\n\n# split dataset in training set and test set\nX_train, X_test, y_train, y_test = train_test_split(NPZ, y, test_size=0.3, random_state=0)\n\nX_train = sequence.pad_sequences(X_train,dtype=\'float32\')\ny_train = sequence.pad_sequences(y_train,dtype=\'int32\') \n\nmodel = Sequential()\n\nmodel.add(GRU(hidden_neurons_1, input_dim=in_neurons, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(GRU(hidden_neurons_2, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(TimeDistributed(Dense(out_neurons, activation = \'softmax\')))\n\nmodel.compile( loss = \'mse\', optimizer = \'rmsprop\',  metrics=[""accuracy""])\n\nmodel.summary()\n\nmodel.fit(X_train, y_train,batch_size =1,nb_epoch=20)\n\n# save model and weights\njson_string = model.to_json()\nopen(\'my_model_architecture.json\', \'w\').write(json_string)\nmodel.save_weights(\'my_model_weights.h5\')\n```\n', ""There is no problem, actually, except that you are running with a very strange choice of parameters.\n\nThe callback that is being slow in this case, is logging to stdout. You're setting `batch_size=1`, which you should never do since it is insanely inefficient especially for such a small network, and at the same you are setting `verbose=1` (default). This triggers one flush to stdout for every batch, i.e. in this case for sample processed. Because you have a tiny model, actually running the model on one sample is basically instant, and flushing to stdout is taking a significant time compared to that.\n"", ""Thanks! I tried to change both parameters and apparently only verbose=1 is the problem. Even by increasing the batch_size I still have the same warning message. It doesn't really makes sense because with verbose = 0 I cannot check how my training is going.\n"", 'You could try verbose = 2. This gives you one output per epoch.']",[],[],0,0
424,keras,5481,closed,Stateful RNN with different lengths for each sample,"Suppose I want to train a stateful RNN and I have two samples. For the first, I have 100 observations - 100 X values and 100 Y values to train on. For the second, I have 200 observations - 100 X values and 100 Y values to train on.

I set up a stateful RNN with a batch size of 2. I start training on batches of 2. All is well and good for the first 100 batches. But what happens on the 101st? My batch size really needs to be 1 at this point, but I don't believe I can just change the batch size mid-training.

Is masking intelligent enough to handle this situation? So, suppose I pad my first observation with 100 sets of zeros at the beginning of my X series. I also pad its Y series with 100 zeros. I use a masking layer to mask zeros from the input data. I know masking will prevent information from the X series from being used, but will it also prevent the model from doing anything with the dummy Y values?

I suppose it would also be possible to use a batch size of 1 throughout and simply reset the model state when I'm ready to switch samples. Is there any downside to doing so (other than a little more coding overhead)?

Is there another best practice for this?

I hope I have explained this well.",stale,"['Hello,\r\n\r\nI also run across this problem during training recurrent structrures in stateful-mode on timeseries of different lengths. \r\n\r\nYou worry about the dummy Y values (zeros) which influence your weight parameters I recommend you to use the sample_weight functionality of Keras framework (available in fit and in fit_generator)\r\n\r\n> sample_weight: Numpy array of weights for the training samples, used for scaling the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples\r\n> (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=""temporal"" in compile().\r\n\r\nA stretegy could be to assign a sample_weight of 0 to the ""fake"" zero padded sample to remove its influence on the loss function.\r\n\r\nYour strategy to use a batch_size of 1 is also very common. You should assess the performace and decide what works best for you.', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],0,0
425,keras,2486,closed,Unable to output custom layer,"Hi,
I've built a layer called dissimilarity which returns :

but the following error raised on buildinf the Model:


The test code is:

    crop_right_bound = Boundary(1)(A)
    crop_left = Crop_Side(3,0)(B)

    dis = Dissimilarity([crop_left, crop_right_bound])

    patch_compare = Model([part_A_input, part_B_input], dis)

Do i need to configure anything else in my implementation of the layer?
",,"['+1 would like to know the answer to this also.  I ran into the same problem implementing a simple custom layer that\'s not even at the end of the network and it fails:\n\n``` python\nclass Bilinear(Layer):\n    """""" Computes $x^T zI y rowwise for a batch""""""\n    def __init__(self, **kwargs):\n        super(Bilinear, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3, ""Input should be shape (batch_size, 3, embed_size)""\n        embed_dim = input_shape[2] # 0 is the batch dim\n        self.trainable_weights = []\n\n    def call(self, tensor, mask=None):\n        x = tensor[:,0,:]\n        y = tensor[:,1,:]\n        z = tensor[:,2,:]\n        xTz = Merge(mode=\'mul\')([x, z])\n        xTzy = K.batch_dot(xTz, y, axes=1)\n        return xTzy\n\n    def get_output_shape_for(self, input_shape):\n        return (input_shape[0], 1)\n\nvocab_size = 100\ntarget_embed_size = 100\n\nx_input = Input(shape=(1,), dtype=\'int32\', name=\'x_input\')\ny_input = Input(shape=(1,), dtype=\'int32\', name=\'y_input\')\nz_input = Input(shape=(target_embed_size,), dtype=\'float\', name=\'bilinear_input\')\n\nx_embed = Embedding(input_dim=vocab_size, output_dim=target_embed_size)(x_input)\ny_embed = Embedding(input_dim=vocab_size, output_dim=target_embed_size)(y_input)\n\nz_embed = K.expand_dims(z_input, dim=1)\n\nxyz = Merge(mode=\'concat\', concat_axis=1)([x_embed, y_embed, z_expand])\nscore = Bilinear()(xyz)\noutput = Activation(\'sigmoid\')(score)\n#output = Activation(\'linear\')(score) # trivial identity output layer doesn\'t work\n#output = Lambda(lambda x:x, lambda in_shape:in_shape)(score) # another identity output that doesn\'t work\n\nbi = Model(input=[x_input, y_input, z_input], output=[output])\nbi.compile(optimizer=RMSprop(), loss=\'accuracy\')\n```\n\nThrows error:\n\n``` python\nException: Output tensors to a Model must be Keras tensors. Found: Tensor(""Sigmoid_1:0"", shape=(?, ?), dtype=float32)\n```\n', '@teffland Three potential problem:\nFirst, I think you misuse `Merge`. ([doc](http://keras.io/layers/core/#merge)) It should be:\n\n``` python\nfrom Keras.layers import merge\n# in def call:\nxTz = merge([x, z], mode=\'mul\')\nxyz = merge([x_embed, y_embed, z_expand], mode=\'concat\', concat_axis=1) \n# z_expand not exist in your code, typo?\n```\n\nSecond, the reason why you get the error is attempting to merge ""Tensor"" and ""Keras Tensor"". Can be fixed by \n\n``` python\n z_embed = Reshape((1, target_embed_size))(z_input) # or writing Lambda layer\n```\n\nLast, `K.batch_dot` seems not work as you wish. Check this?\n\n``` python\nxTzIy = K.sum(x*y*z, axis=1, keepdims=True)\n```\n\nand `loss=\'accuracy\'`?\n', 'I also got this `Exception: Output tensors to a Model must be Keras tensors.` from a similar code.\n\nI could not find any documentation about **Keras Tensor**. Could some one point out what is the difference between a keras tensor and a tensor defined by K.variable or K.placeholder? Why does keras need them? How to convert a keras variable into a keras tensor?\n', 'After some dig I found the answers to the first two questions in my previous post. The answers are in the source code of `topology.py`. I think it would be helpful to add the Input layer to the core layers documentation.\n\n```\n    `Input()` is used to instantiate a Keras tensor.\n    A Keras tensor is a tensor object from the underlying backend\n    (Theano or TensorFlow), which we augment with certain\n    attributes that allow us to build a Keras model\n    just by knowing the inputs and outputs of the model.\n    For instance, if a, b and c and Keras tensors,\n    it becomes possible to do:\n    `model = Model(input=[a, b], output=c)`\n    The added Keras attributes are:\n        ._keras_shape: integer shape tuple propagated\n            via Keras-side shape inference.\n        ._keras_history: last layer applied to the tensor.\n            the entire layer graph is retrievable from that layer,\n            recursively.\n```\n', ""@joelthchao \nThanks for the recommendations. \n1. The way I used Merge works for the functional API\n2. This is counterintuitive.  So what you're saying is you can't mix and match backend calls with Layers w/o wrapping those calls in Layers (eg, with a LambdaLayer)?\n3. Seems like a good way to do the dot product\n4. A silly mistake (but the code didn't make it that far yet anyways)\n"", ""I've had the same error. @fchollet said it can be solved by using a Lambda layer but it does not work at all. He closed the issue..... https://github.com/fchollet/keras/issues/3130\n"", ""You guys are misusing Lambda layers.\n\nIt has no effect to have a Lambda layer of the form `lambda x: x`. It doesn't make sense.\n\nThe argument you pass to a Lambda layer should be a function that returns a TF or Theano tensor. All TF/Theano operations being applied should be inside that function. This is explained clearly in the documentation for the `Lambda` layer.\n\nOtherwise, you can use a custom layer, and put your custom TF/Theano code in the `call` method, as explained here: http://keras.io/layers/writing-your-own-keras-layers/\n\n@teffland your code is incomplete so it isn't possible to give you precise pointers. I think @joelthchao has already pointed out the most likely issues.\n"", ""@ghost : a Model has to be defined with tensor inputs and tensor outputs, as explained throughout the documentation. You are setting as output a class instance instead.\n\nYour code:\n\n``` python\ndis = Dissimilarity([crop_left, crop_right_bound])  # that's a class instance\npatch_compare = Model([part_A_input, part_B_input], dis)  # dis should be a tensor, but it's a class instance!\n```\n\nHence you get an error message that explicitly tells you what you did wrong:\n\n`Output tensors to a Model must be Keras tensors. Found: <__main__.Dissimilarity object`\n\nSo maybe you meant to write:\n\n``` python\nlayer = Dissimilarity()\ndis = layer([crop_left, crop_right_bound])  # this is now a tensor (if the Dissimilarity layer has been correctly implemented)\n```\n""]",[],"['K.sqrt(K.sum(K.square(inputs[1] - concatenated),axis=1, keepdims=True))', 'Output tensors to a Model must be Keras tensors. Found: <__main__.Dissimilarity object at 0x10f444908>', '\n        A = Input(shape=(3, None, None))\n        B = Input(shape=(3, None, None))\n\n', '', '\n', '']",0,0
426,keras,2882,closed,custom objective function,"I try to train a network like [this](http://arxiv.org/pdf/1506.02640v5.pdf). But I've no idea about the objective function. how to implement the function that return 1 in some cases depend on the output and return 0 otherwise. ?
",stale,[],[],[],0,0
427,keras,519,closed,Why the loss is weighted in compile() ?,"@wxs , @fchollet : I'm trying to transplant CTC objective into Keras. When reading the codes, I noticed in  function of  class,  there are lines as



Since the weighted_objectivemasked_y_truemasked_y_predy_truey_pred`, does this mean the objective function should expect the shapes of masked variables instead of unmasked ones?
",,"['Anybody?\n', 'To support the `sample_weight` and `class_weight` features. The exact process can be found in the code. \n']","['\n        weighted_loss = weighted_objective(objectives.get(loss))   # return a function\n        ...\n        self.weights = T.ones_like(self.y_train)\n        train_loss = weighted_loss(self.y, self.y_train, self.weights)\n        test_loss = weighted_loss(self.y, self.y_test, self.weights)\n']","['compile()', 'Sequential', ""self.weights' are already all ones, why there's need for weighting anyway?\n\nAnd, in "", ' function, shapes of ', ' and ', ' are different from ', ' and ']",0,0
428,keras,4237,closed,"in keras, how to import the dataset made by myself, and whats the requirements of the data in the dataset?","Please make sure that the boxes below are checked before you submit your issue. Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).",,[],[],[],0,0
429,keras,11094,closed,How to reduce learning rate based last epoch valid acc or valid loss?,"Time-decay or Drop-decay maybe a good strategy to train a model, but I think a better way is to reduce the learning rate when valid loss increasing or valid acc decreasing.  But I can only change learning rate by passing a  parameter into LearningRateSchedule. How can I do it?",,['I found `ReduceLROnPlateau`.'],[],['epoch'],0,0
430,keras,1597,closed, acc and val_acc don't change?,"I use LSTM to do a sequence labeling task, but I got the same acc and cal_acc for each epoch.
here is my code:

def moduleRnn(self):
        model = Sequential()
        model.add(LSTM(output_dim=64,input_length=self.seq_len,batch_input_shape=(16,1,200),input_dim=self.embed_length,return_sequences=True,stateful=False ))
        #model.add(LSTM(output_dim=16,return_sequences=True,stateful=False ))
        model.add(Dropout(0.2))
        model.add(TimeDistributedDense(output_dim=self.labs_len))
        model.add(Activation('softmax'))
        model.compile(loss=""categorical_crossentropy"" , optimizer='rmsprop' , class_mode='categorical')
        #model.fit(self.train,self.train_lab,batch_size=16,nb_epoch=3,verbose=1, validation_split=0.1,show_accuracy=True)
        model.fit(self.X_train,self.Y_train,batch_size=16,nb_epoch=15,verbose=1,show_accuracy=True,validation_split=0.2)
        score = model.evaluate(self.X_test,self.Y_test,batch_size=16)
        print score

Anyone meets the same problem?  please help me
",stale,"[""Do you mean training accuracy and validation accuracy doesn't change in training procedure?\nYou'd better post your logs\n"", '@ymcui  yes,it is.\n\nEpoch 1/15\n18272/18272 [==============================] - 118s - loss: 0.0479 - acc: 0.4296 - val_loss: 0.0285 - val_acc: 0.4286\nEpoch 2/15\n18272/18272 [==============================] - 114s - loss: 0.0322 - acc: 0.4297 - val_loss: 0.0282 - val_acc: 0.4286\nEpoch 3/15\n18272/18272 [==============================] - 113s - loss: 0.0319 - acc: 0.4297 - val_loss: 0.0281 - val_acc: 0.4286\nEpoch 4/15\n18272/18272 [==============================] - 114s - loss: 0.0317 - acc: 0.4297 - val_loss: 0.0283 - val_acc: 0.4286\nEpoch 5/15\n18272/18272 [==============================] - 120s - loss: 0.0316 - acc: 0.4297 - val_loss: 0.0281 - val_acc: 0.4286\nEpoch 6/15\n18272/18272 [==============================] - 117s - loss: 0.0314 - acc: 0.4297 - val_loss: 0.0281 - val_acc: 0.4286\nEpoch 7/15\n18272/18272 [==============================] - 115s - loss: 0.0314 - acc: 0.4297 - val_loss: 0.0280 - val_acc: 0.4286\nEpoch 8/15\n18272/18272 [==============================] - 119s - loss: 0.0314 - acc: 0.4297 - val_loss: 0.0280 - val_acc: 0.4286\nEpoch 9/15\n18272/18272 [==============================] - 116s - loss: 0.0312 - acc: 0.4297 - val_loss: 0.0280 - val_acc: 0.4286\nEpoch 10/15\n18272/18272 [==============================] - 116s - loss: 0.0314 - acc: 0.4297 - val_loss: 0.0280 - val_acc: 0.4286\nEpoch 11/15\n18272/18272 [==============================] - 115s - loss: 0.0313 - acc: 0.4297 - val_loss: 0.0280 - val_acc: 0.4286\nEpoch 12/15\n18272/18272 [==============================] - 113s - loss: 0.0312 - acc: 0.4297 - val_loss: 0.0280 - val_acc: 0.4286\nEpoch 13/15\n18272/18272 [==============================] - 113s - loss: 0.0312 - acc: 0.4297 - val_loss: 0.0279 - val_acc: 0.4286\nEpoch 14/15\n18272/18272 [==============================] - 113s - loss: 0.0312 - acc: 0.4297 - val_loss: 0.0280 - val_acc: 0.4286\nEpoch 15/15\n18272/18272 [==============================] - 114s - loss: 0.0312 - acc: 0.4297 - val_loss: 0.0280 - val_acc: 0.4286\n\nhow does this come out? \n', ""@talentlei \nIn your log, the `loss` seems to start from a very low value, and converge very soon after a few epochs.\nI've no particular idea about this, but I think you should check validity of your data. (and maybe remove `batch_input_shape` attribute in your `LSTM` layer, i guess.)\n"", ""@talentlei Have solved the problem？ I stuck in the same situation when I use RNN， but I don't know how to solve it.\n"", ""I have a similar problem. In my case when I attempt LSTM time series classification often val_acc starts with a high value and stays the same, even though loss, val_loss and acc change. I've narrowed down the issue to not enough training sequences (around 300). When I increased the number to 500+, it started to converge better, but still there are periods when loss, acc and val_loss changes, but val_acc sticks to the same value. How could that be? Is there a bug when it's not updating (even though loss, acc and val_loss update during the same epoch)?\n\nmodel = Sequential()\nmodel.add(LSTM(256, input_shape=(6, 10)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\nhist = model.fit(X_train_mat, Y_train_mat, nb_epoch=10000, batch_size=30, validation_split=0.1)\n\nEpoch 2816/10000\n50/472 [==>...........................] - ETA: 0s - loss: 0.6281 - acc: 0.6800Epoch 02815: val_acc did not improve\n472/472 [==============================] - 0s - loss: 0.5151 - acc: 0.7648 - val_loss: 1.2978 - val_acc: 0.4151\nEpoch 2817/10000\n50/472 [==>...........................] - ETA: 0s - loss: 0.4406 - acc: 0.8600Epoch 02816: val_acc did not improve\n472/472 [==============================] - 0s - loss: 0.5179 - acc: 0.7479 - val_loss: 1.2844 - val_acc: 0.4151\nEpoch 2818/10000\n50/472 [==>...........................] - ETA: 0s - loss: 0.5385 - acc: 0.7400Epoch 02817: val_acc did not improve\n472/472 [==============================] - 0s - loss: 0.5100 - acc: 0.7585 - val_loss: 1.2699 - val_acc: 0.4151\n"", ""A good method for debugging this issue is to use an ipython/jupyter notebook, compile the model, and then have it predict for one of your batches.   Then, go through the accuracy code with the ability to manually inspect the values of the matrices.   I've found stepping through code like this in mysterious situations to be enlightening. \n"", '@DSA101 Have you solved the problem? I am doing sentence classification task with variable sentence lengths using LSTMs. My problem is that training loss and training accuracy decrease over epochs but validation accuracy fluctuates in a small interval. Maybe your solution could be helpful for me too. \n', ""My solution was to increase the size of the training set, reduce the number of features, start with just one layer and not too many units (say 128). When I ensured that in such configuration the training progresses in a reasonable way, I have slowly added more features, more units, etc, and in the end got a satisfactory result. Still if I make the model overly complex (e.g. increase to 3 layers with say 512 units without providing more training data), it would behave the same as before - flat or irregular training accuracy.\n\nIn the end I don't know if there is still a bug in the framework, or it all results from an overly complicated model and the insufficient size of the training set, but all things considered, I am satisfied with the performance of the model and the results that I have achieved and believe that Keras LSTM is usable for time series classification.\n\nSo if your training acc improves but validation accuracy stays in a small interval, can it be indicative of overfitting?\n"", ""I'm having the same issue. Loss and accuracy on the training set change from epoch to epoch, but the validation accuracy / loss doesn't, which is a bit odd.\r\n\r\n```\r\nEpoch 1/20\r\n158/158 [==============================] - 24s - loss: 2.3558 - acc: 0.4051 - val_loss: 1.0986 - val_acc: 0.3684\r\nEpoch 2/20\r\n158/158 [==============================] - 24s - loss: 1.8001 - acc: 0.3924 - val_loss: 1.0986 - val_acc: 0.3684\r\nEpoch 3/20\r\n158/158 [==============================] - 24s - loss: 1.2940 - acc: 0.3608 - val_loss: 1.0986 - val_acc: 0.3684\r\nEpoch 4/20\r\n158/158 [==============================] - 24s - loss: 1.8052 - acc: 0.4114 - val_loss: 1.0986 - val_acc: 0.3684\r\nEpoch 5/20\r\n158/158 [==============================] - 24s - loss: 1.7127 - acc: 0.3734 - val_loss: 1.0986 - val_acc: 0.3684\r\nEpoch 6/20\r\n158/158 [==============================] - 24s - loss: 1.8030 - acc: 0.3734 - val_loss: 1.0986 - val_acc: 0.3684\r\nEpoch 7/20\r\n158/158 [==============================] - 24s - loss: 1.7076 - acc: 0.3861 - val_loss: 1.0986 - val_acc: 0.3684\r\nEpoch 8/20\r\n158/158 [==============================] - 24s - loss: 1.4173 - acc: 0.4241 - val_loss: 1.0986 - val_acc: 0.3684\r\nEpoch 9/20\r\n158/158 [==============================] - 24s - loss: 1.3042 - acc: 0.3797 - val_loss: 1.0986 - val_acc: 0.3684\r\n```\r\n\r\nThe model I'm using is a convnet:\r\n```\r\nmodel = Sequential()\r\nmodel.add(Convolution2D(20, 5, 5, input_shape=(3, img_width, img_height)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\nmodel.add(Flatten())\r\nmodel.add(Dense(20))\r\nmodel.add(Activation('relu'))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(Dense(3))\r\nmodel.add(Activation('sigmoid'))\r\n\r\nsgd = SGD(lr=0.0005)\r\nmodel.compile(loss='categorical_crossentropy',\r\n              optimizer=sgd,\r\n              metrics=['accuracy'])\r\n\r\n```"", ""Similar problem here. It really feels like a bug to me. The reason is that my validation set has 2500+ observations for a dataset of size like this, as long as there's change in the weights (and theres is since the training error is decreasing), there should be change in the val_loss, either positive or negative. Also it's unlikely it's overfitting as I'm really using heavy dropouts (between 0.5~0.7 for each layer).\r\n\r\nMy solution to this is changing the learning rate of the optimizer....sometimes it helps, haha. I've never experienced the same phenomenon using raw tensorflow so I think it's a keras thing."", ""I'm gunna throw my voice in here, too. I'm currently doing the Udacity Self-Driving Car Engineer Nanodegree course; my cohort is currently doing the behavioral cloning lab. We were given a dataset of approximately 20k+ features and labels; I take it and augment it with flipping - so I have about 40k of data. My convnet is the same one from the NVidia end-to-end paper (relu on all layers). I am using adam and mse for optimizer/loss. I've tried heavy dropout on the fully-connected layers, on all layers, on random layers. Ultimately, my validation accuracy stays stuck at a single value. I'd think if I were overfitting, the accuracy would peg close or at 100%? Rather, it seems like it is getting stuck in a local minima. I think I'm going to need to do some visualization of the data, to verify that it is balanced, plus I have some other ideas to try, but so far it is very frustrating. I don't know if it is a bug with the framework; my best guess is that it is not, because other students are finding success."", '@andrew-ayers Did you manage to solve this issue? I have a similar problem with NVIDIA (adam, mse, 120k samples including flipped data) model for Self_Driving Car Engineer course - validation loss changes but validation accuracy stays the same.', 'I had the same problem while training a convolutional auto encoder. I made learning rate (""lr"" parameter in optimizer) smaller and it solved the problem. ', ""Have you solved the problem? I met a similar problem with my keras CNN model, my training samples were 4000, and validation samples were 1000. During the training process, the loss and val_loss was decreasing, but the acc and val_acc never changing during this process. \r\n\r\nthis is my code:\r\n\r\n'inputs_x=Input(shape=(1,65,21))\r\nx=Conv2D(64,(3,3),padding='same',data_format='channels_first',activation='relu',use_bias=True)(x)\r\nx=Conv2D(64,(3,3),padding='same',data_format='channels_first',activation='relu',use_bias=True)(x)\r\nx=MaxPooling2D(pool_size=(2,2),strides=(2,2))(x)\r\n\r\nx=Conv2D(32,(5,5),padding='same',data_format='channels_first',activation='relu',use_bias=True)(x)\r\nx=Conv2D(16,(5,5),padding='valid',data_format='channels_first',activation='relu',use_bias=True)(x)\r\nx=MaxPooling2D(pool_size=(2,2),strides=(2,2))(x)\r\n\r\nx=Dropout(0.25)(x)\r\nx=Flatten()(x)\r\n\r\ninputs_y=Input(shape=(1,32,21))\r\ny=Conv2D(32,(2,2),padding='same',data_format='channels_first',activation='relu',use_bias=True)(y)\r\ny=Conv2D(32,(2,2),padding='same',data_format='channels_first',activation='relu',use_bias=True)(y)\r\ny=MaxPooling2D(pool_size=(2,2),strides=(2,2))(y)\r\n\r\ny=Conv2D(32,(4,4),padding='same',data_format='channels_first',activation='relu',use_bias=True)(y)\r\ny=Conv2D(8,(4,4),padding='valid',data_format='channels_first',activation='relu',use_bias=True)(y)\r\ny=MaxPooling2D(pool_size=(2,2),strides=(2,2))(y)\r\n\r\ny=Dropout(0.30)(y)\r\ny=Flatten()(y)\r\n\r\nmerged_input=keras.layers.concatenate([x,y],axis=-1)\r\n\r\nz=Dense(16,activation='softmax')(merged_input)\r\nz=Dense(8,activation='softmax')(z)\r\nz=Dense(4,activation='softmax')(z)\r\n\r\noutp=Dense(1,activation='softmax')(z)\r\n\r\nmodel=Model(inputs=[inputs_x,inputs_y],outputs=outp)\r\nmodel.compile(loss='binary_crossentropy',\r\noptimizer='sgd',\r\nmetrics=['accuracy'])\r\n\r\nhistory=model.fit(x=[train_inputs_x,train_inputs_y],y=train_label,batch_size=32,\r\nepochs=30,validation_split=0.2,shuffle=True)`\r\n\r\nany ideas for this?"", ""Does anyone know how to solve this issues?\r\nin my model, by LSTM I have got repeating training and validation accuracy for each epoch!!\r\nthe model learns slightly within the epoch and after each batch, but seems it reset before next epoch and start again from the beginning!\r\nits the training log after epochs:\r\n - 4s - loss: 0.2217 - acc: 0.6464 - val_loss: 0.1487 - val_acc: 0.8137\r\n - 3s - loss: 0.2217 - acc: 0.6469 - val_loss: 0.1487 - val_acc: 0.8137\r\n - 3s - loss: 0.2217 - acc: 0.6469 - val_loss: 0.1487 - val_acc: 0.8137\r\n - 3s - loss: 0.2216 - acc: 0.6469 - val_loss: 0.1487 - val_acc: 0.8137\r\n - 4s - loss: 0.2216 - acc: 0.6469 - val_loss: 0.1487 - val_acc: 0.8137\r\n - 4s - loss: 0.2216 - acc: 0.6469 - val_loss: 0.1487 - val_acc: 0.8137\r\n - 4s - loss: 0.2216 - acc: 0.6469 - val_loss: 0.1487 - val_acc: 0.8137\r\n - 4s - loss: 0.2216 - acc: 0.6469 - val_loss: 0.1487 - val_acc: 0.8137\r\n - 4s - loss: 0.2216 - acc: 0.6469 - val_loss: 0.1487 - val_acc: 0.8137\r\n - 4s - loss: 0.2216 - acc: 0.6469 - val_loss: 0.1487 - val_acc: 0.8137\r\n - 4s - loss: 0.2216 - acc: 0.6469 - val_loss: 0.1487 - val_acc: 0.8137\r\n - 4s - loss: 0.2216 - acc: 0.6469 - val_loss: 0.1487 - val_acc: 0.8137\r\n - 4s - loss: 0.2216 - acc: 0.6469 - val_loss: 0.1487 - val_acc: 0.8137\r\n - 4s - loss: 0.2216 - acc: 0.6469 - val_loss: 0.1487 - val_acc: 0.8137\r\n - 4s - loss: 0.2216 - acc: 0.6469 - val_loss: 0.1487 - val_acc: 0.8137\r\n - 4s - loss: 0.2216 - acc: 0.6469 - val_loss: 0.1487 - val_acc: 0.8137\r\n - 4s - loss: 0.2216 - acc: 0.6469 - val_loss: 0.1487 - val_acc: 0.8137\r\n...\r\nit's the log after batches:\r\nTrain on 21000 samples, validate on 9000 samples\r\n==================New Training Start====================\r\nEpoch 1/50\r\nbatch: 0  ended Loss: 0.21712732 Accuracy: 0.625\r\nbatch: 1  ended Loss: 0.229398 Accuracy: 0.65166664\r\nbatch: 2  ended Loss: 0.22204755 Accuracy: 0.6383333\r\nbatch: 3  ended Loss: 0.21405634 Accuracy: 0.6533333\r\nbatch: 4  ended Loss: 0.21910276 Accuracy: 0.63\r\nbatch: 5  ended Loss: 0.22354788 Accuracy: 0.70166665\r\nbatch: 6  ended Loss: 0.23390895 Accuracy: 0.62166667\r\nbatch: 7  ended Loss: 0.21102294 Accuracy: 0.62833333\r\nbatch: 8  ended Loss: 0.22611171 Accuracy: 0.66833335\r\nbatch: 9  ended Loss: 0.21904916 Accuracy: 0.62\r\nbatch: 10  ended Loss: 0.23376058 Accuracy: 0.645\r\nbatch: 11  ended Loss: 0.21929795 Accuracy: 0.6766667\r\nbatch: 12  ended Loss: 0.22111656 Accuracy: 0.6483333\r\nbatch: 13  ended Loss: 0.2131401 Accuracy: 0.65\r\nbatch: 14  ended Loss: 0.2148913 Accuracy: 0.6566667\r\nbatch: 15  ended Loss: 0.22052963 Accuracy: 0.635\r\nbatch: 16  ended Loss: 0.22950262 Accuracy: 0.6333333\r\nbatch: 17  ended Loss: 0.22890009 Accuracy: 0.64666665\r\nbatch: 18  ended Loss: 0.22269897 Accuracy: 0.65166664\r\nbatch: 19  ended Loss: 0.22959195 Accuracy: 0.645\r\nbatch: 20  ended Loss: 0.22551142 Accuracy: 0.6566667\r\nbatch: 21  ended Loss: 0.2217158 Accuracy: 0.635\r\nbatch: 22  ended Loss: 0.21928492 Accuracy: 0.64\r\nbatch: 23  ended Loss: 0.21457554 Accuracy: 0.66333336\r\nbatch: 24  ended Loss: 0.22461174 Accuracy: 0.655\r\nbatch: 25  ended Loss: 0.21772751 Accuracy: 0.665\r\nbatch: 26  ended Loss: 0.21689837 Accuracy: 0.63166666\r\nbatch: 27  ended Loss: 0.22468112 Accuracy: 0.6333333\r\nbatch: 28  ended Loss: 0.2141714 Accuracy: 0.6533333\r\nbatch: 29  ended Loss: 0.22494899 Accuracy: 0.6483333\r\nbatch: 30  ended Loss: 0.22441803 Accuracy: 0.62833333\r\nbatch: 31  ended Loss: 0.22385867 Accuracy: 0.62666667\r\nbatch: 32  ended Loss: 0.2221946 Accuracy: 0.66\r\nbatch: 33  ended Loss: 0.2230069 Accuracy: 0.64166665\r\nbatch: 34  ended Loss: 0.21400177 Accuracy: 0.66"", ""@hujiao1314 I do not know if I really understand what you are trying to do, so forgive me if it does not make sense. My observations:\r\nIn your last layer outp, you are using softmax when you only have one output neuron. You might find it useful to change to 'sigmoid'. Again, for the layers named z, they do not seem to be a final output and you are using a softmax activation function.\r\nIt is quite a bit confusing so if you could specify the characteristics of your problem I could be more helpful."", '@hadisaadat reduce ur learning rate and try for a few smaller learning rates. SHould solve ur problem\r\n', '@AkhilAshref  , even i had the similar issue as @hadisaadat  , mine worked after reducing the lr. But could you give a bit more detailed explanation as to why the gradient becomes zero. \r\nThanks', ""@vishnu-zsf I'm having the same problem it seems, what optimizer/ learning rate did you use?"", ""@amcneil1998  , i used adam optimizer and settled on a learning rate of 0.0008 , . This was when i used 100,000 data samples and had 10 epochs. But later on when i tried to run with 30 epochs , i shifted to decaying learning rate, which after tuning for a while gave me satisfactory results. I'm pretty sure that the learning rate and all the parameters in the optimizer vary with the kind of data we have and the sheer magnitude of the features.\r\n\r\n<initial code when i ran with 10 epochs >\r\n.\r\nopt = optimizers.adam(lr=0.0008)\r\nself.model.compile(loss='binary_crossentropy', optimizer=opt,metrics = ['accuracy'])  \r\n.\r\ncode to run with decaying lr in Keras\r\n.\r\n\r\nkeras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\r\n.\r\n.\r\nDo reply if you the issue still persists.\r\n\r\n\r\n"", ""@vishnu-zsf still having the issue.  I have tried reducing the learning rate, increasing the learning rate, tried both sdg and adam optimizers.  I  have event tried to over fit my data by just using a small part of my data.  I currently have 900 data points, of which I am using 100 for both test and validation, and 700 for training.  I have tried increasing my amount of data to 2800, using 400 for both test and validation, and 2000 for training.  I'm currently using a batch size of 50, and even running past 50 epochs showed no increase in accuracy or loss.  I noticed later on while trying to predict results that my predictions were heading towards 0, with them coming closer the longer I trained.  This seems to be the case really no matter what I do."", '@vishnu-zsf  @amcneil1998  in my case, the lr had no impact actually and the solution for me was shuffling data for each epoch.\r\ndepends on your data nature [time series or not] you should select a convenient cross-validation and shuffling strategy.', '@hadisaadat setting shuffle=true did not improve my results.  Accuracy still stayed around 0.5 but loss started pretty low (0.01).  So I increased the learning rate and loss started around 5.1 and then dropped of to 0.02 after the 6th Epoch.  Accuracy started at 0.5 and averaged around that on both training and validation data for the 120 epochs that I trained.  However when predicting I am only able to get 2 values from the output. ', '@amcneil1998  you may have to regularize and can even use the Earlystopping in callbacks, but before that could you share your code and ur data ( 5 sample points would do) , coz like i said the methods we use pretty much depend on the type of data we use. Mine is all resolved now btw', '@vishnu-zsf All of my input/output data is regularized from -1-1 with a mean of 0.   The input data is a 3d array with the form (Nsamples, Entries/Sample, EntryDim).  In this case it is (900, 225, 6).  The output data is a 2d array with shape (Nsamples, 2), so in this case it is (900,2). Some of the samples did not have enough entries so they are zero-padded to the correct size.  Here is the code for the model after the test data has been split off:\r\n\r\n```\r\ninitilizer = RandomNormal(mean=0.0, stddev=0.05, seed=None)\r\nmodelInput = Input(batch_shape=(batch_size, 225, 6), name=""Model_Input"")\r\nmid = LSTM(128, return_sequences=True, input_dim= (225, 6), bias_initializer=initilizer)(modelInput)\r\nmid = LSTM(128, return_sequences=False, bias_initializer=initilizer)(mid)\r\noutput = Dense(2, activation=\'linear\')(mid)\r\nmodel = Model(inputs = modelInput, outputs = output)\r\nadam = optimizers.Adam(lr = 0.000000001)\r\nmodel.compile(loss=\'mean_squared_error\', optimizer = adam, metrics=[\'accuracy\'])\r\nmodel.fit(trainInputData, trainTruthData, epochs=20, batch_size=batch_size, verbose=2, validation_split=(1/8), shuffle=True)\r\n\r\n```', 'I have faced the same issue multiple times while using Keras. I have tried data normalization, shuffling, different learning rates and different optimizers. Nothing seems to help out, except increasing the data size. Now that is a problem for me, as I am trying to compare the effect of the data sample size on my network.\r\nI see a lot of problems but rarely any solution in the discussions above. If anyone has a decent solution except sample size, kindly let me know.', ""I used to face the same result before. I found that using smaller neural network architecture. Reason behind should be due to vanishing gradient. In some situation, your input might not carry as much information as the neural network expects, and therefore, the weights are gonna vanish to zeros even after several layers. Such problem is more serious when you are doing ConvNet, and it's the reason why we got residual network. Hope this help.  "", ""I faced the same issue when trying to implement a CNN for a multi-label comment classification problem. When I trained my model on a tiny subset of my data (say, 100 of 100000), it did what I had expected with a highly imbalanced data set - loss decreasing, accuracy going up to 1 very quickly, validation accuracy a little lower, but also changing pretty fast. But when I used a bigger subset, my model seemed to get 'stuck' at a certain accuracy. It also predicted 'nan' for all test samples and labels. \r\n\r\n**What finally solved the problem for me** was applying (sigmoid) activation to my 1D convolutional layer (there was only one in my model at that point). I hadn't used any activation at first. I also set kernel_initializer to random_normal, but I think the crucial part was the activation.\r\n\r\nEverything else I had tried before that (different activation on the other layers, manipulating the learning rate, gradient clipping, balancing out my dataset) didn't make any difference.  \r\n\r\nLooking at the other examples I see nobody made the same mistake - using a convolutional layer without activation - but who knows, maybe my experience could be helpful for someone in the future. "", ""I 'm not sure but I solved this problem. I used Keras for CNN model on the Kaggle platform with GPU.     \r\nI took the same problems all epoch step had same val_loss and val_acc.  Like:  \r\nEpoch 2/50   - val_loss: 0.6931 - val_acc:0.5521\r\nEpoch 3/50   - val_loss: 0.6931 -val_acc: 0.5521\r\n...\r\nWhen I changed optimization methods from Adam to RMSprop, it was run but I refreshed all kernel and restart I took the same issue.  I changed again RMSprop to SGD. It had worked. \r\nSometimes the problem is caused by a unsuitable Dense layers. "", ""For Those who still have this problem and wondering why this occurs. The reason is pretty straightforward in your final Dense layers where you are specifying the output basically the softmax layer , here number of cells should be equal to number of classes.\r\nIf you are solving Binary Classification all you need to do this use 1 cell with sigmoid activation.\r\n# for Binary\r\nmodel.add(Dense(1,activation='sigmoid')) \r\n# for n_class\r\nmodel.add(Dense(n_class,activation='softmax')) #where n_class is number of classes \r\nThanks to :https://stackoverflow.com/questions/51581521/accuracy-stuck-at-50-keras\r\n\r\n"", '@sayedathar11 \r\nWhen I use model.add(Dense(1,activation=\'sigmoid\')), am getting the following error.\r\n**ValueError: Error when checking target: expected dense_4 to have shape (1,) but got array with shape (2,)**\r\n\r\nHere is my code:\r\n\r\nbatch_size = 32\r\nnb_classes = 2\r\ndata_augmentation = True\r\n\r\nimg_rows, img_cols = 224,224\r\nimg_channels = 3\r\n\r\n#Creating array of training samples\r\ntrain_path = ""D:/data/train\\*.*""\r\ntraining_data=[]\r\nfor file in glob.glob(train_path):\r\n    print(file)\r\n    train_array= cv2.imread(file)\r\n    train_array=cv2.resize(train_array,(img_rows,img_cols),3)\r\n    training_data.append(train_array)\r\n\r\nx_train=np.array(training_data)\r\n\r\n#Creating array of validation samples\r\nvalid_path = ""D:/data/valid\\*.*""\r\nvalid_data=[]\r\nfor file in glob.glob(valid_path):\r\n    print(file)\r\n    valid_array= cv2.imread(file)\r\n    valid_array=cv2.resize(valid_array,(img_rows,img_cols),3)\r\n    valid_data.append(train_array)\r\n\r\nx_valid=np.array(valid_data)\r\n\r\nx_train = np.array(x_train, dtype=""float"")/255.0\r\nx_valid = np.array(x_valid, dtype=""float"")/255.0\r\n\r\n#Creating array for Labels\r\ny_train=np.ones((num_trainsamples,),dtype = int)\r\ny_train[0:224]=0 #Class1=0\r\ny_train[225:363]=1 #Class2=1\r\nprint(y_train)\r\n\r\ny_valid=np.ones((num_validsamples,),dtype = int)\r\ny_valid[0:101]=0 \r\ny_valid[102:155]=1 \r\nprint(y_valid)\r\n\r\ny_train = np_utils.to_categorical(y_train,nb_classes,dtype=\'int32\')\r\ny_valid = np_utils.to_categorical(y_valid,nb_classes,dtype=\'int32\')\r\n\r\nbase_model=ResNet50(weights=\'imagenet\',include_top=False)\r\n\r\nx = base_model.output\r\nx = GlobalMaxPooling2D()(x)\r\nx=Dense(1024,activation=\'relu\')(x) \r\nx=Dense(1024,activation=\'relu\')(x) \r\nx=Dense(512,activation=\'relu\')(x) \r\nx=Dense(1, activation= \'sigmoid\')(x)\r\nmodel = Model(inputs = base_model.input, outputs = x)\r\n\r\nfor i,layer in enumerate(model.layers):\r\n  print(i,layer.name)\r\n\r\nfor layer in model.layers[:75]:\r\n    layer.trainable=False\r\nfor layer in model.layers[75:]:\r\n    layer.trainable=True\r\n\r\nadam = Adam(lr=0.0001)\r\nmodel.compile(optimizer= adam, loss=\'binary_crossentropy\', metrics=[\'accuracy\'])\r\n\r\ntrain_datagen = ImageDataGenerator(\r\n    brightness_range=(0.2,2.5),\r\n    rotation_range=180,\r\n    zoom_range=0.5,\r\n    width_shift_range=0.2,\r\n    height_shift_range=0.2,\r\n    horizontal_flip=True,\r\n    vertical_flip=True)\r\n\r\ntrain_datagen.fit(x_train)\r\n\r\nhistory= model.fit_generator(train_datagen.flow(x_train, y_train, batch_size = 10,shuffle=True),steps_per_epoch=len(x_train),epochs = 500,shuffle=True,\r\n    validation_data=(x_valid,y_valid),validation_steps=num_validsamples // batch_size,callbacks=[tensorboard])\r\n\r\neval = model.evaluate(x_valid, y_valid)\r\nprint (""Loss = "" + str(eval[0]))\r\nprint (""Test Accuracy = "" + str(eval[1]))\r\n\r\npredictions= model.predict(x_valid)\r\nprint(predictions)', 'I had same issue: epoch accuracy was growing while validation was the same value (0.41). But, I saved the weights after an epoch and then when I loaded the weights and continued training, everything worked. \r\n\r\nFirst time: create the model, compile, call fit_generator: bad validation results every epoch.\r\nThen: create the model, compile, load weights, call fit_generator: everything works beautifully.\r\n\r\nTo me it seems like I missed a step, but when calling load_weights on the model it was corrected', 'Had the same issue. Reducing Initial Learning Rate helps.', ""hey, I'm new at deep learning especially CNN. I've been trying to train 100 class with 10 images for each class. \r\nI've been using many kinds of architecture but the val_loss really high and val_acc really low. \r\nDo you guys have any suggestion for that?"", '@prabaHridayami That is very low amount of data, it can be hard to obtain good results. Are you doing any type of data augmentation? That would be my suggestion to increase the variety of data your model sees. ', ""@skhadem yeah, i'm doing several augmentations so 1 image is going to be having 88 image augmentation. i'm currently trying to train 10 class with val_acc is 0.6870 and val_loss is 1.4573. what do you think?\r\n"", '@prabaHridayami what architecture are you using?', ""> @prabaHridayami what architecture are you using?\r\n\r\nmodel = Sequential()\r\n\r\nmodel.add(Conv2D(32, (3, 3), input_shape=(100, 400, 3), activation='relu', padding='same',name='block1_conv1'))\r\nmodel.add(Conv2D(32, (3, 3), activation='relu',padding='same',name='block1_conv2'))\r\nmodel.add(Conv2D(32, (3, 3), activation='relu',padding='same',name='block1_conv3'))\r\nmodel.add(Conv2D(32, (3, 3), activation='relu',padding='same',name='block1_conv4'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2),name='block1_pool'))\r\n\r\nmodel.add(Conv2D(64, (3, 3), activation='relu',padding='same',name='block2_conv1'))\r\nmodel.add(Conv2D(64, (3, 3), activation='relu',padding='same',name='block2_conv2'))\r\nmodel.add(Conv2D(64, (3, 3), activation='relu',padding='same',name='block2_conv3'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2),name='block2_pool'))\r\n\r\nmodel.add(Conv2D(128, (3, 3), activation='relu',padding='same',name='block3_conv1'))\r\nmodel.add(Conv2D(128, (3, 3), activation='relu',padding='same',name='block3_conv2'))\r\nmodel.add(Conv2D(128, (3, 3), activation='relu',padding='same',name='block3_conv3'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2),name='block3_pool'))\r\n\r\nmodel.add(Conv2D(256, (3, 3), activation='relu',padding='same',name='block4_conv1'))\r\nmodel.add(Conv2D(256, (3, 3), activation='relu',padding='same',name='block4_conv2'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2),strides =(2,2),name='block4pool'))\r\n\r\n\r\nmodel.add(Flatten())\r\nmodel.add(Dense(256, activation='relu'))\r\nmodel.add(Dropout(0.4))\r\n\r\nmodel.add(Dense(256, activation='relu'))\r\nmodel.add(Dropout(0.4))\r\n\r\nmodel.add(Dense(20, activation='softmax'))\r\n\r\nthis is my architecture model using sequential"", ""@prabaHridayami I would recommend using a pre trained and well studied architecture for feature extraction and then fine tuning the layers on the top. My personal go-to is VGG19. In keras you can do something like this:\r\n```\r\nbase = keras.applications.VGG19(input_shape=(100,400,3), \r\n                                include_top=False, \r\n                                input_size=(100,400,3),\r\n                                weights='imagenet',\r\n                                pooling='max')\r\n# freeze base layers\r\nfor layer in base.layers:\r\n    layer.trainable=False\r\n\r\nmodel = keras.Sequential()\r\nmodel.add(base)\r\n# you should experiment with different top level designs \r\nmodel.add(Dense(1024, activation='relu'))\r\nmodel.add(Dropout(0.4))\r\nmodel.add(Dense(512, activation='relu'))\r\nmodel.add(Dropout(0.4))\r\nmodel.add(Dense(20, activation='softmax'))\r\n```\r\ncheck out https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"", ""> @prabaHridayami I would recommend using a pre trained and well studied architecture for feature extraction and then fine tuning the layers on the top. My personal go-to is VGG19. In keras you can do something like this:\r\n> \r\n> ```\r\n> base = keras.applications.VGG19(input_shape=(100,400,3), \r\n>                                 include_top=False, \r\n>                                 input_size=(100,400,3),\r\n>                                 weights='imagenet',\r\n>                                 pooling='max')\r\n> # freeze base layers\r\n> for layer in base.layers:\r\n>     layer.trainable=False\r\n> \r\n> model = keras.Sequential()\r\n> model.add(base)\r\n> # you should experiment with different top level designs \r\n> model.add(Dense(1024, activation='relu'))\r\n> model.add(Dropout(0.4))\r\n> model.add(Dense(512, activation='relu'))\r\n> model.add(Dropout(0.4))\r\n> model.add(Dense(20, activation='softmax'))\r\n> ```\r\n> check out https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\r\n\r\nthank you very much, i'll check that out...\r\ni don't really understand about dense and dropout.\r\ndo you know what is the function of these two?"", 'So Dense is just a fully connected layer, it is what does a lot of the ""decision making"" based on the resulting feature vector. It\'s a way to take large feature vectors and map to a class. The more you have the more ""flexible"" it can be, i.e. learn better, but that means more parameters. Dropout literally takes random weights and drops them by setting them to 0. The way I think about it is that if there are certain sections that are contributing a lot to a correct result, the optimizer could ignore everything else. With Dropout the optimizer is forced to focus on many different places. It helps to avoid over fitting and is almost standard at this point. ', '> So Dense is just a fully connected layer, it is what does a lot of the ""decision making"" based on the resulting feature vector. It\'s a way to take large feature vectors and map to a class. The more you have the more ""flexible"" it can be, i.e. learn better, but that means more parameters. Dropout literally takes random weights and drops them by setting them to 0. The way I think about it is that if there are certain sections that are contributing a lot to a correct result, the optimizer could ignore everything else. With Dropout the optimizer is forced to focus on many different places. It helps to avoid over fitting and is almost standard at this point.\r\n\r\nthank you very much... now i understand', '> @sayedathar11\r\n> When I use model.add(Dense(1,activation=\'sigmoid\')), am getting the following error.\r\n> **ValueError: Error when checking target: expected dense_4 to have shape (1,) but got array with shape (2,)**\r\n> \r\n> Here is my code:\r\n> \r\n> batch_size = 32\r\n> nb_classes = 2\r\n> data_augmentation = True\r\n> \r\n> img_rows, img_cols = 224,224\r\n> img_channels = 3\r\n> \r\n> #Creating array of training samples\r\n> train_path = ""D:/data/train*.*""\r\n> training_data=[]\r\n> for file in glob.glob(train_path):\r\n> print(file)\r\n> train_array= cv2.imread(file)\r\n> train_array=cv2.resize(train_array,(img_rows,img_cols),3)\r\n> training_data.append(train_array)\r\n> \r\n> x_train=np.array(training_data)\r\n> \r\n> #Creating array of validation samples\r\n> valid_path = ""D:/data/valid*.*""\r\n> valid_data=[]\r\n> for file in glob.glob(valid_path):\r\n> print(file)\r\n> valid_array= cv2.imread(file)\r\n> valid_array=cv2.resize(valid_array,(img_rows,img_cols),3)\r\n> valid_data.append(train_array)\r\n> \r\n> x_valid=np.array(valid_data)\r\n> \r\n> x_train = np.array(x_train, dtype=""float"")/255.0\r\n> x_valid = np.array(x_valid, dtype=""float"")/255.0\r\n> \r\n> #Creating array for Labels\r\n> y_train=np.ones((num_trainsamples,),dtype = int)\r\n> y_train[0:224]=0 #Class1=0\r\n> y_train[225:363]=1 #Class2=1\r\n> print(y_train)\r\n> \r\n> y_valid=np.ones((num_validsamples,),dtype = int)\r\n> y_valid[0:101]=0\r\n> y_valid[102:155]=1\r\n> print(y_valid)\r\n> \r\n> y_train = np_utils.to_categorical(y_train,nb_classes,dtype=\'int32\')\r\n> y_valid = np_utils.to_categorical(y_valid,nb_classes,dtype=\'int32\')\r\n> \r\n> base_model=ResNet50(weights=\'imagenet\',include_top=False)\r\n> \r\n> x = base_model.output\r\n> x = GlobalMaxPooling2D()(x)\r\n> x=Dense(1024,activation=\'relu\')(x)\r\n> x=Dense(1024,activation=\'relu\')(x)\r\n> x=Dense(512,activation=\'relu\')(x)\r\n> x=Dense(1, activation= \'sigmoid\')(x)\r\n> model = Model(inputs = base_model.input, outputs = x)\r\n> \r\n> for i,layer in enumerate(model.layers):\r\n> print(i,layer.name)\r\n> \r\n> for layer in model.layers[:75]:\r\n> layer.trainable=False\r\n> for layer in model.layers[75:]:\r\n> layer.trainable=True\r\n> \r\n> adam = Adam(lr=0.0001)\r\n> model.compile(optimizer= adam, loss=\'binary_crossentropy\', metrics=[\'accuracy\'])\r\n> \r\n> train_datagen = ImageDataGenerator(\r\n> brightness_range=(0.2,2.5),\r\n> rotation_range=180,\r\n> zoom_range=0.5,\r\n> width_shift_range=0.2,\r\n> height_shift_range=0.2,\r\n> horizontal_flip=True,\r\n> vertical_flip=True)\r\n> \r\n> train_datagen.fit(x_train)\r\n> \r\n> history= model.fit_generator(train_datagen.flow(x_train, y_train, batch_size = 10,shuffle=True),steps_per_epoch=len(x_train),epochs = 500,shuffle=True,\r\n> validation_data=(x_valid,y_valid),validation_steps=num_validsamples // batch_size,callbacks=[tensorboard])\r\n> \r\n> eval = model.evaluate(x_valid, y_valid)\r\n> print (""Loss = "" + str(eval[0]))\r\n> print (""Test Accuracy = "" + str(eval[1]))\r\n> \r\n> predictions= model.predict(x_valid)\r\n> print(predictions)\r\n\r\nI am also facing the exact same issue. If I keep the number of neurons in the output layer and use sigmoid, for each epochs, there is no change in the accuracy. But, if I make a change in the number of layers as mentioned above, same error as you are getting. Were you able to resolve ? In case yes, pls let us know the solution. Thank you in Advance. ', 'I have a similar issue when i tried to build an autoencoder using LSTM for sequences or CNN for images, the model  reaches around 50% accuracy, 2.5 loss then stuck, nothing improving at all.\r\nI tried to increase number of nodes, number of layers but with no progress.\r\n\r\nAfter 3 days I tuned the optimizer trying to change **learning rate** and **learning rate decay**, and finally everything improved and everything makes sense, trying to increase **learning rate decay** slightly till the model start to improve without stuck at 50%.\r\n\r\nI used Adam optimizer with following parameters\r\n`Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False)`\r\n*Tuning the parameters will change from problem to another of course.*\r\nThanks! ', 'this happened when I used     \r\nwinit = RandomNormal(mean=0.0 , stddev=0.1)\r\nfor weight initialization in the Dense layers  \r\nand it just worked when I removed it and used the default settings !!!!\r\n\r\n', '> I had the same problem while training a convolutional auto encoder. I made learning rate (""lr"" parameter in optimizer) smaller and it solved the problem.\r\n\r\ncan you send me your code of optimization of autoencoder. i want to optimize my autoencoder network but i have no idea how to do that. can you please help me .', 'had the same problem, solved by a changing `adam` optimizer to `sgd`', 'I think that the learning rate is the problem. Actually mine was equal to 7 ahah. I wrote 10-3 instead of 1e-3.', '```python\r\nmodel = keras.Sequential([\r\n    keras.layers.Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=""same"", activation=""relu""),\r\n    keras.layers.Conv2D(filters=64,kernel_size=(3,3),padding=""same"", activation=""relu""),\r\n    keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)),\r\n    keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding=""same"", activation=""relu""),\r\n    keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding=""same"", activation=""relu""),\r\n    keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)),\r\n    keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding=""same"", activation=""relu""),\r\n    keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding=""same"", activation=""relu""),\r\n    keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding=""same"", activation=""relu""),\r\n    keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)),\r\n    keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=""same"", activation=""relu""),\r\n    keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=""same"", activation=""relu""),\r\n    keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=""same"", activation=""relu""),\r\n    keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)),\r\n    keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=""same"", activation=""relu""),\r\n    keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=""same"", activation=""relu""),\r\n    keras.layers.Conv2D(filters=512, kernel_size=(3,3), padding=""same"", activation=""relu""),\r\n    keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2)),\r\n    keras.layers.Flatten(),\r\n    keras.layers.Dense(units=4096,activation=""relu""),\r\n#     keras.layers.Dropout(.5),\r\n    keras.layers.Dense(units=4096,activation=""relu""),\r\n    keras.layers.Dropout(.5),\r\n    keras.layers.Dense(units=2, activation=""sigmoid""),\r\n])\r\n\r\nmodel.compile(optimizer=""adam"",\r\n            loss=""categorical_crossentropy"",\r\n            metrics=[\'accuracy\'])\r\n```\r\n\r\nwith this architecture, I get 0.73 constantly. couldn\'t find a fix yet', 'reducing batch size solved it for me :)\r\n\r\nI guess my test set was too small to feed large batches into the CNN. \r\n\r\ni hope this may be of use!', ""Hi, I recently had the same experience of training a CNN while my validation accuracy doesn't change. I tried different setups from LR, optimizer, number of filters and even playing with the model size. **But later I discovered it was an issue with my preprocessing of data**. Basically, I was doing some preprocessing to my data before training which ends up squeezing the pixel intensity to near zero (**in short all images were just black images**). I discovered it after debugging my preprocessing step in which I tried to write some of the images in a disk. \r\n\r\nTo be honest, **I was suspecting it was a bug from Keras but boom! it was not**.  I tried to share my experience in case anyone else is facing the same issue. \r\n"", 'Were you dividing your images by 255? I am facing the same issue and am starting to suspect this is the problem. I divide my pixels by 255 (as is customary) but can still see what the image looks like when plotting it. ', ""I faced the same issue. It got resolved by **changing the optimizer** from 'rmsprop' to 'adam'.\r\n"", ""I tried changing optimizers, learning rates, momentum, network depth, and all other parameters. Turns out, I just needed to let it train for a long time before it started to find where the loss was decreasing. The AUC was stagnant for 35 epochs then it started increasing. Can't think of why, but it eventually started to learn. "", '> @sayedathar11\r\n> When I use model.add(Dense(1,activation=\'sigmoid\')), am getting the following error.\r\n> **ValueError: Error when checking target: expected dense_4 to have shape (1,) but got array with shape (2,)**\r\n> \r\n> Here is my code:\r\n> \r\n> batch_size = 32\r\n> nb_classes = 2\r\n> data_augmentation = True\r\n> \r\n> img_rows, img_cols = 224,224\r\n> img_channels = 3\r\n> \r\n> #Creating array of training samples\r\n> train_path = ""D:/data/train*.*""\r\n> training_data=[]\r\n> for file in glob.glob(train_path):\r\n> print(file)\r\n> train_array= cv2.imread(file)\r\n> train_array=cv2.resize(train_array,(img_rows,img_cols),3)\r\n> training_data.append(train_array)\r\n> \r\n> x_train=np.array(training_data)\r\n> \r\n> #Creating array of validation samples\r\n> valid_path = ""D:/data/valid*.*""\r\n> valid_data=[]\r\n> for file in glob.glob(valid_path):\r\n> print(file)\r\n> valid_array= cv2.imread(file)\r\n> valid_array=cv2.resize(valid_array,(img_rows,img_cols),3)\r\n> valid_data.append(train_array)\r\n> \r\n> x_valid=np.array(valid_data)\r\n> \r\n> x_train = np.array(x_train, dtype=""float"")/255.0\r\n> x_valid = np.array(x_valid, dtype=""float"")/255.0\r\n> \r\n> #Creating array for Labels\r\n> y_train=np.ones((num_trainsamples,),dtype = int)\r\n> y_train[0:224]=0 #Class1=0\r\n> y_train[225:363]=1 #Class2=1\r\n> print(y_train)\r\n> \r\n> y_valid=np.ones((num_validsamples,),dtype = int)\r\n> y_valid[0:101]=0\r\n> y_valid[102:155]=1\r\n> print(y_valid)\r\n> \r\n> y_train = np_utils.to_categorical(y_train,nb_classes,dtype=\'int32\')\r\n> y_valid = np_utils.to_categorical(y_valid,nb_classes,dtype=\'int32\')\r\n> \r\n> base_model=ResNet50(weights=\'imagenet\',include_top=False)\r\n> \r\n> x = base_model.output\r\n> x = GlobalMaxPooling2D()(x)\r\n> x=Dense(1024,activation=\'relu\')(x)\r\n> x=Dense(1024,activation=\'relu\')(x)\r\n> x=Dense(512,activation=\'relu\')(x)\r\n> x=Dense(1, activation= \'sigmoid\')(x)\r\n> model = Model(inputs = base_model.input, outputs = x)\r\n> \r\n> for i,layer in enumerate(model.layers):\r\n> print(i,layer.name)\r\n> \r\n> for layer in model.layers[:75]:\r\n> layer.trainable=False\r\n> for layer in model.layers[75:]:\r\n> layer.trainable=True\r\n> \r\n> adam = Adam(lr=0.0001)\r\n> model.compile(optimizer= adam, loss=\'binary_crossentropy\', metrics=[\'accuracy\'])\r\n> \r\n> train_datagen = ImageDataGenerator(\r\n> brightness_range=(0.2,2.5),\r\n> rotation_range=180,\r\n> zoom_range=0.5,\r\n> width_shift_range=0.2,\r\n> height_shift_range=0.2,\r\n> horizontal_flip=True,\r\n> vertical_flip=True)\r\n> \r\n> train_datagen.fit(x_train)\r\n> \r\n> history= model.fit_generator(train_datagen.flow(x_train, y_train, batch_size = 10,shuffle=True),steps_per_epoch=len(x_train),epochs = 500,shuffle=True,\r\n> validation_data=(x_valid,y_valid),validation_steps=num_validsamples // batch_size,callbacks=[tensorboard])\r\n> \r\n> eval = model.evaluate(x_valid, y_valid)\r\n> print (""Loss = "" + str(eval[0]))\r\n> print (""Test Accuracy = "" + str(eval[1]))\r\n> \r\n> predictions= model.predict(x_valid)\r\n> print(predictions)\r\n\r\nWhat  is the variable `num_trainsamples`?\r\n', 'Go with  the suggestion given by @kodon0 . It works !']",[],[],0,0
431,keras,6296,closed,"What's the difference between ""samples_per_epoch"" and ""steps_per_epoch""","Hello everyone, I was confused by this problem for several days...

My question is that why the training time has such massive difference between that I set the batch_size to be ""1"" and ""20"" for my generator.

If I set the **_batch_size_** to be **1**, the **training time** of _**1 epoch**_ is approximately **180 ~ 200 sec**.
If I set the **_batch_size_** to be **20**, the **training time** of **1 epoch** is approximately **3000 ~ 3200 sec**. 

However, this horrible difference between these training times seems to be abnormal..., since it should be the reversed result:
batch_size = 1, training time -> 3000 ~ 3200 sec.
batch_size = 20, training time -> 180 ~ 200 sec.

The input to my generator is not the file path, but the numpy arrays which are already loaded into the
memory via calling ""np.load()"".
So I think the I/O trade-off issue doesn't exist.

I'm using Keras-2.0.3 and my backend is tensorflow-gpu 1.0.1

I have seen the update of this merged [PR](https://github.com/fchollet/keras/pull/5879/files),
but it seems that this change won't affect anything at all. (the usage is just the same with original one)

The [link](https://gist.github.com/HappyStorm/cb6c22ffec18a8fbb4912e9c79b6d87c) here is the gist of my self-defined generator and the part of my fit_generator.

Could somebody help me explain this problem...???
Thank you so much...Orz",,"['This forum is not for discussion and is for issues.  Please remove and post in the forum.', 'Sorry for that, I will post in the forum.']",[],[],0,0
432,keras,9783,closed,"get_config on keras.models.Sequential model returns list, but get_config on keras.engine.training.Model returns dict","I noticed a weird difference in the Model API.  the  class inherits , but changes the signature of the  function.   Is there a reason for this? Seems like those two APIs should be consistent. 

https://github.com/keras-team/keras/blob/eb97bc385599dec8182963fe263bd958b9ab0057/keras/models.py#L1355-L1363
vs.
https://github.com/keras-team/keras/blob/eb97bc385599dec8182963fe263bd958b9ab0057/keras/engine/topology.py#L2326
",,"[""This issue has been fixed in #11133. You'll be able to see the changes in the next keras release. ""]",[],"['keras.models.Sequential', 'keras.engine.training.Model', 'get_config']",0,0
433,keras,13024,closed,is it possible for continues training in keras for multiclass classification for new class?,"**System information**  
- Have I written custom code (as opposed to using example directory):  
- Linux Ubuntu 16.04  
- TensorFlow backend yes
- TensorFlow version:  ''v1.13.1-0-g6612da8951' 1.13.1
- Keras version:  2.2.4
- Python version: 3.6.8
- CUDA/cuDNN version:  CUDA 10.0
- GPU model and memory:  4 GB

i was tried continues training in keras.
because i was build keras multiclass classification model, after i have new labels, and values. so i want to build new model without retraining. that is why i tried continuous train in keras. 


after completing save the model , i want to do continues training. so i tried,


i tried this. but it was thrown error.like previously 10 classes. 

but now we add new class means error occurred.

so what is my question is, **is it possible for continues training in keras for multiclass classification for new class?**


Thank you :)",type:support,"[""Hello Murugan R, in your case the model was trained to classify only 10 classes (i.e. the number of units in the output layer is 10). Now that you've added a new class, you have 11 classes so you've to change the output units of the last layer to 11 and then you can maybe load the same weights for all layers except output layer and freeze the weights but use new weights for output layer and retrain the network. Note that this may or may not work because earlier layers may not know how to interpret features for the newly added label.\r\nFurther reading on this topic: https://developer.amazon.com/blogs/alexa/post/6058f7e9-1de8-4e51-8f50-aee56e29f3ce/updating-neural-networks-to-recognize-new-categories-with-minimal-retraining"", '@MuruganR96 Adding on top of @dabasajay , if the original model was trained for classifying 10 different classes of images of dog breeds, you can add one more class in the end (11 breeds instead of 10) and train with your data to update weights for the last layer. But, if you want to use the original model  (classifying 10 dog breeds) for classifying X-ray images (entirely different from the actual purpose of the original model) of 11 classes, then you need to train few more layers in the end and then train with your data. Please check the [resource here](https://machinelearningmastery.com/how-to-use-transfer-learning-when-developing-convolutional-neural-network-models/).  There are several other great resources if you need any more help. thanks!', ""Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!""]","['\r\n    model.add(Dense(10, activation=\'sigmoid\'))\r\n\r\n    model.compile(optimizer=\'rmsprop\',\r\n                loss=\'sparse_categorical_crossentropy\',\r\n                metrics=[\'accuracy\'])\r\n    \r\n    \r\n    model.fit(training_data, labels, epochs=20, batch_size=1)\r\n\r\n    model.save(""keras_model.h5"")\r\n', '\r\n\r\n    model1 = load_model(""keras_model.h5"")\r\n    model1.fit(new_input, new_label, epochs=20, batch_size=1)\r\n    model1.save(""keras_model.h5"")\r\n', '\r\n\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Received a label value of 10 which is outside the valid range of [0, 9).  Label values: 10\r\n\t [[{{node loss/dense_7_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]\r\n']",[],0,0
434,keras,3926,closed,Output batch images,"I have a network that predicts images Y. How can I output to file all the predicted images in each batch, so that I can monitor the progress during training? 

My current understanding is to write a callback that calls  in the  routine, but I am not sure where the actual batch data X is going to be? (In my case, I have a custom generator creating X, and the model is fit using . 

Does anyone know how to do this, or are there any examples? Thanks! 
",stale,[],[],"['self.model.predict_on_batch(X)', 'on_batch_end()', 'fit_generator()']",0,0
435,keras,6171,closed,"[Dense Layer Issue] TypeError: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64","**Environment:**
TypeErrorTypeError: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64`.

The Flatten layer's output is float32. However, it seems like the Dense layer only accepts int32, int64.



**Error Output:**


All help are appreciated. Thank you!
",,"[""Hey @dat-ai, I think this is an issue with the params your passing to a layer, not the output of one layer going into the next. I had this error when I passed in a `kernel_size` argument of (1.0, 6.0, 1.0), so I think you've got something similar. I would check your `kernel_size` arguments as well as the `(sz_pool_fin, sz_pool_fin)` passed into the `AveragePooling2D` layer to make sure they are in fact integers, and not floats. "", '@sallamander : You are awesome. Thank for pinpointing the exact error. The error indeed was at ```sz_pool_fin = (input_shape[0]) / stride_L1```\r\n\r\nStep to resolve:\r\n```\r\nsz_pool_fin = int((input_shape[0]) / stride_L1)\r\n```', ""Yep good point in the right direction!!!\r\n**Broken**\r\n```\r\ndecoder_upsample = Dense(filters * img_rows / 2 * img_cols / 2, activation='relu')\r\n\r\nif K.image_data_format() == 'channels_first':\r\n    output_shape = (batch_size, filters, img_rows / 2, img_cols / 2)\r\nelse:\r\n    output_shape = (batch_size, img_rows / 2, img_cols / 2, filters)\r\n```\r\n\r\n**Working**\r\n```\r\ndecoder_upsample = Dense(int(filters * img_rows / 2 * img_cols / 2), activation='relu')\r\n\r\nif K.image_data_format() == 'channels_first':\r\n    output_shape = (batch_size, filters, int(img_rows / 2), int(img_cols / 2))\r\nelse:\r\n    output_shape = (batch_size, int(img_rows / 2), int(img_cols / 2), filters)\r\n\r\n```"", 'thank a lot ,you perfectly solve my problem same as you ']","['\r\n$ uname -a\r\nLinux tk1 4.4.0-64-generic x86_64 x86_64 x86_64 GNU/Linux\r\n$nvcc --version\r\nCuda compilation tools, release 8.0, V8.0.62\r\n', '\r\n$ pip show keras\r\nName: Keras\r\nVersion: 2.0.2\r\nSummary: Deep Learning for Python\r\n....\r\n', '\r\n$ pip show tensorflow-gpu\r\nName: tensorflow-gpu\r\nVersion: 1.0.1\r\nSummary: TensorFlow helps the tensors flow\r\nHome-page: http://tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: opensource@google.com\r\nLicense: Apache 2.0\r\n', ""shell\r\nsz_L1_filters, nb_L1_filters, stride_L1 = layer1_params\r\nsz_res_filters, nb_res_filters, nb_res_stages = res_layer_params\r\nsz_pool_fin = (input_shape[0]) / stride_L1\r\n\r\n#  INPUT LAYERS\r\n#######################\r\nframe = Input(shape=(HEIGHT, WIDTH, CHANNELS), name='video_stream')\r\n\r\n# VISION MODEL - USING CNN\r\n###########################\r\nx = Lambda(lambda image: image/255.0 - 0.5, input_shape=(HEIGHT, WIDTH, CHANNELS))(frame)\r\nx = Conv2D(filters=nb_L1_filters, kernel_size=(sz_L1_filters, sz_L1_filters),\r\n           strides=(stride_L1, stride_L1),\r\n           kernel_initializer=init,\r\n           kernel_regularizer=l2(reg),\r\n           use_bias=False,\r\n           padding='same',\r\n           name='conv0')(x)\r\nx = BatchNormalization(axis=1, name='bn0')(x)\r\nx = Activation('relu', name='relu0')(x)\r\nx = Dropout(KEEP_PROP)(x)\r\n....\r\nx = AveragePooling2D((sz_pool_fin, sz_pool_fin), name='avg_pool')(x)\r\nx = Flatten(name='flat')(x)\r\n\r\nx = Dense(1024, name='fc1', activation='relu')(x)  # <=== ERROR HERE\r\nx = Dropout(0.5)(x)\r\n"", '\r\nTraceback (most recent call last):\r\n  File ""/home/dat/bag/train/DatNet.py"", line 208, in <module>\r\n    dat = DatNet()\r\n  File ""/home/dat/bag/train/DatNet.py"", line 23, in __init__\r\n    init=init, reg=reg, use_shortcuts=use_shortcuts)\r\n  File ""/home/dat/bag/train/DatNet.py"", line 176, in build\r\n    x = Dense(1024, name=\'fc1\')(x)\r\n  File ""/home/dat/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/engine/topology.py"", line 528, in __call__\r\n    self.build(input_shapes[0])\r\n  File ""/home/dat/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/layers/core.py"", line 827, in build\r\n    constraint=self.kernel_constraint)\r\n  File ""/home/dat/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/engine/topology.py"", line 364, in add_weight\r\n    weight = K.variable(initializer(shape), dtype=K.floatx(), name=name)\r\n  File ""/home/dat/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/initializers.py"", line 205, in __call__\r\n    dtype=dtype, seed=self.seed)\r\n  File ""/home/dat/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py"", line 3146, in random_uniform\r\n    dtype=dtype, seed=seed)\r\n  File ""/home/dat/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/random_ops.py"", line 244, in random_uniform\r\n    seed2=seed2)\r\n  File ""/home/dat/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gen_random_ops.py"", line 220, in _random_uniform\r\n    seed=seed, seed2=seed2, name=name)\r\n  File ""/home/dat/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 585, in apply_op\r\n    param_name=input_name)\r\n  File ""/home/dat/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 61, in _SatisfiesTypeConstraint\r\n    "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\nTypeError: Value passed to parameter \'shape\' has DataType float32 not in list of allowed values: int32, int64\r\n']","['\r\n**Software Version:**\r\n\r\n\r\n\r\n**Issue Description:**\r\n\r\nTensorflow throws an ', ' when my program tries to create a Dense layer with an input from a Flatten layer. \r\n']",0,0
436,keras,7432,closed,forrtl: error (200): program aborting due to control-C event,"I was using the standard code, that I found in the internet.
This code is about using scikit-learn to select the parameter by using Keras.
I am using the code here
http://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/
I am not sure if this problem is because of keras or scikit-learn.

Then I got some errors.


I have no idea what happens.",stale,"['Have you solved this issue? I meet the same problems as yours.\r\nIf you have got any solution, please let me know.\r\n\r\nThanks', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', 'I am facing the same issue, Can you help?', 'me, too anybody can help? ']","['\r\nimport numpy\r\nfrom sklearn.grid_search import GridSearchCV\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense\r\nfrom keras.wrappers.scikit_learn import KerasClassifier\r\n# Function to create model, required for KerasClassifier\r\ndef create_model():\r\n    # create model\r\n    model = Sequential()\r\n    model.add(Dense(12, input_dim=8, activation=\'relu\'))\r\n    model.add(Dense(1, activation=\'sigmoid\'))\r\n    # Compile model\r\n    model.compile(loss=\'binary_crossentropy\', optimizer=\'adam\', metrics=[\'accuracy\'])\r\n    return model\r\n# fix random seed for reproducibility\r\nseed = 7\r\nnumpy.random.seed(seed)\r\n# load dataset\r\ndataset = numpy.loadtxt(""pima-indians-diabetes.csv"", delimiter="","")\r\n# split into input (X) and output (Y) variables\r\nX = dataset[:,0:8]\r\nY = dataset[:,8]\r\n# create model\r\nmodel = KerasClassifier(build_fn=create_model, verbose=0)\r\n# define the grid search parameters\r\nbatch_size = [10, 20, 40, 60, 80, 100]\r\nepochs = [10, 50, 100]\r\nparam_grid = dict(batch_size=batch_size, nb_epoch=epochs)\r\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\r\ngrid_result = grid.fit(X, Y)\r\n# summarize results\r\nprint(""Best: %f using %s"" % (grid_result.best_score_, grid_result.best_params_))\r\nfor params, mean_score, scores in grid_result.grid_scores_:\r\n    print(""%f (%f) with: %r"" % (scores.mean(), scores.std(), params))\r\n', '\r\nUsing Theano backend.\r\nUsing cuDNN version 5110 on context None\r\nMapped name None to device cuda0: GeForce GTX 750 Ti (0000:01:00.0)\r\nUsing Theano backend.\r\nUsing Theano backend.\r\nUsing Theano backend.\r\nUsing Theano backend.\r\nUsing Theano backend.\r\nUsing Theano backend.\r\nUsing Theano backend.\r\nUsing Theano backend.\r\nUsing cuDNN version 5110 on context None\r\nforrtl: error (200): program aborting due to control-C event\r\nImage              PC                Routine            Line        Source             \r\nlibifcoremd.dll    00007FF9860443E4  Unknown               Unknown  Unknown\r\nKERNELBASE.dll     00007FF99B2A5674  Unknown               Unknown  Unknown\r\nKERNEL32.DLL       00007FF99C0B2D92  Unknown               Unknown  Unknown\r\nntdll.dll          00007FF99DF89F64  Unknown               Unknown  Unknown\r\nforrtl: error (200): program aborting due to control-C event\r\nImage              PC                Routine            Line        Source             \r\nlibifcoremd.dll    00007FF9860443E4  Unknown               Unknown  Unknown\r\nKERNELBASE.dll     00007FF99B2A5674  Unknown               Unknown  Unknown\r\nKERNEL32.DLL       00007FF99C0B2D92  Unknown               Unknown  Unknown\r\nntdll.dll          00007FF99DF89F64  Unknown               Unknown  Unknown\r\nforrtl: error (200): program aborting due to control-C event\r\nImage              PC                Routine            Line        Source             \r\nlibifcoremd.dll    00007FF9860443E4  Unknown               Unknown  Unknown\r\nKERNELBASE.dll     00007FF99B2A5674  Unknown               Unknown  Unknown\r\nKERNEL32.DLL       00007FF99C0B2D92  Unknown               Unknown  Unknown\r\nntdll.dll          00007FF99DF89F64  Unknown               Unknown  Unknown\r\nforrtl: error (200): program aborting due to control-C event\r\nImage              PC                Routine            Line        Source             \r\nlibifcoremd.dll    00007FF9860443E4  Unknown               Unknown  Unknown\r\nKERNELBASE.dll     00007FF99B2A5674  Unknown               Unknown  Unknown\r\nKERNEL32.DLL       00007FF99C0B2D92  Unknown               Unknown  Unknown\r\nntdll.dll          00007FF99DF89F64  Unknown               Unknown  Unknown\r\nforrtl: error (200): program aborting due to control-C event\r\nImage              PC                Routine            Line        Source             \r\nlibifcoremd.dll    00007FF9860443E4  Unknown               Unknown  Unknown\r\nKERNELBASE.dll     00007FF99B2A5674  Unknown               Unknown  Unknown\r\nKERNEL32.DLL       00007FF99C0B2D92  Unknown               Unknown  Unknown\r\nntdll.dll          00007FF99DF89F64  Unknown               Unknown  Unknown\r\nforrtl: error (200): program aborting due to control-C event\r\nImage              PC                Routine            Line        Source             \r\nlibifcoremd.dll    00007FF9860443E4  Unknown               Unknown  Unknown\r\nKERNELBASE.dll     00007FF99B2A5674  Unknown               Unknown  Unknown\r\nKERNEL32.DLL       00007FF99C0B2D92  Unknown               Unknown  Unknown\r\nntdll.dll          00007FF99DF89F64  Unknown               Unknown  Unknown\r\nforrtl: error (200): program aborting due to control-C event\r\nImage              PC                Routine            Line        Source             \r\nlibifcoremd.dll    00007FF9860443E4  Unknown               Unknown  Unknown\r\nKERNELBASE.dll     00007FF99B2A5674  Unknown               Unknown  Unknown\r\nKERNEL32.DLL       00007FF99C0B2D92  Unknown               Unknown  Unknown\r\nntdll.dll          00007FF99DF89F64  Unknown               Unknown  Unknown\r\nforrtl: error (200): program aborting due to control-C event\r\nImage              PC                Routine            Line        Source             \r\nlibifcoremd.dll    00007FF9860443E4  Unknown               Unknown  Unknown\r\nKERNELBASE.dll     00007FF99B2A5674  Unknown               Unknown  Unknown\r\nKERNEL32.DLL       00007FF99C0B2D92  Unknown               Unknown  Unknown\r\nntdll.dll          00007FF99DF89F64  Unknown               Unknown  Unknown\r\nforrtl: error (200): program aborting due to control-C event\r\nImage              PC                Routine            Line        Source             \r\nlibifcoremd.dll    00007FF9860443E4  Unknown               Unknown  Unknown\r\nKERNELBASE.dll     00007FF99B2A5674  Unknown               Unknown  Unknown\r\nKERNEL32.DLL       00007FF99C0B2D92  Unknown               Unknown  Unknown\r\nntdll.dll          00007FF99DF89F64  Unknown               Unknown  Unknown\r\n']",[],0,0
437,keras,4613,closed,Can keras model run on specific device?,"## Problem configuration
- using tensorflow as backed. 
- computer with 1GPU card and 12 CPUs
- not distributed learning over cluster
- with only one session, use GPU or use CPUs. Not using both of them at any time.

## Way to force keras calling tensorflow in GPU or CPUs
###  run keras in CPU


### run keras in GPU
I don't write anything, let everything be as default. So since I install tensorflow in GPU version. It should assume default using GPU.


This undocumented trick works for me so far. However, since keras is a blackbox to me, while tensorflow is more structured and clear, I feel there should be an improvement for keras to better control the CPU/GPU device **with in keras**. I know we could just use keras as simplified tensorflow layer constructor. Thus it is possible to run everything under framework of tensorflow rather than living in the world of keras.

## Comment:
If you have alternative ways to force keras be used in CPU or GPU, please comment below and let everyone else know.

Best,
Shaowu
",stale,"['You can also set the environment variable: CUDA_VISIBLE_DEVICES to limit the number of gpus used. http://www.acceleware.com/blog/cudavisibledevices-masking-gpus\r\n\r\nWe use a task scheduler for our CUDA Servers and in the schell script we use this variable like this:\r\nexport CUDA_VISIBLE_DEVICES=$(getFreeGPU)', 'Put\r\n\r\n```\r\nimport os\r\nos.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""   # see issue #152\r\nos.environ[""CUDA_VISIBLE_DEVICES""] = """"\r\n```\r\n\r\nbefore tensorflow is imported.\r\n\r\nSee also: http://stackoverflow.com/a/42750563/562769', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', 'import os\r\nos.environ[""CUDA_VISIBLE_DEVICES""] = \'-1\'             works on windows 10 to force it to use CPU', 'work perfect for me (windows 10 and keras 2.2.4)\r\nThank !', '> work perfect for me (windows 10 and keras 2.2.4)\r\n> Thank !\r\n\r\n\r\nwhen I did this in keras (with the backend of tensorflow in window 10), the program will occupy all the memories of GPUs, but it only runs on one GPU. Anyone has meet this problem before?\r\n']",[],"['with tf.device(/cpu:0)', 'with tf.device(/gpu:0)']",0,0
438,keras,3420,closed,"maybe a bug in "" keras/backend/theano_backend.py: line 507 in function squeeze""??","if axis == -1,  then line 507 will be 
      broadcastable = x.broadcastable[:-1] + x.broadcastable[0:]
and will not remove   @@correctly?

504 def squeeze(x, axis):
505    '''Remove a 1-dimension from the tensor at index ""axis"".
506    '''
507    broadcastable = x.broadcastable[:axis] + x.broadcastable[axis+1:]
508    x = T.patternbroadcast(x, [i == axis for i in range(x.type.ndim)])
509    x = T.squeeze(x)
510    x = T.patternbroadcast(x, broadcastable)
511    return x
",stale,[],[],[],0,0
439,keras,7890,closed,"Getting NotImplementedError for ""on_epoch_end"" while training using fit_generator on a Sequence object","> Exception in thread Thread-8:
> Traceback (most recent call last):
>   File ""/users/gpu/rohitg1/miniconda2/envs/tensorflow/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
>     self.run()
>   File ""/users/gpu/rohitg1/miniconda2/envs/tensorflow/lib/python2.7/threading.py"", line 754, in run
>     self.__target(*self.__args, **self.__kwargs)
>   File ""/users/gpu/rohitg1/.local/lib/python2.7/site-packages/keras/utils/data_utils.py"", line 492, in _run
>     self.sequence.on_epoch_end()
>   File ""/users/gpu/rohitg1/.local/lib/python2.7/site-packages/keras/utils/data_utils.py"", line 358, in on_epoch_end
>     raise NotImplementedError
> NotImplementedError

I am trying to train a Video classification model. Gist for portion of code is available here:

https://gist.github.com/rohit-gupta/7668b79389e29598ace41813fc1a50d2


Do I need to implement a ""on_epoch_end"" function in my Sequence object to get this o work ?",,"['yes you need to, \r\nYou can do it as simple as : \r\n```\r\ndef on_epoch_end(self):\r\n        pass\r\n```\r\n@joeyearsley Should we make it optional or clarify the doc instead to avoid issues for newcomers?', ""I'ld say optional as it isn't always needed"", '@Dref360 Thanks for your reply, Its solved my issue ! Though I am curious as to what the purpose of this is ? What do people usually implement in this function ?', 'If you want to shuffle your data between epoch, on_epoch_end is called at this time.', ""Had to read keras' code to figure out why my script was raising this `NotImplementedError`. This should definitely be documented or made optional asap"", 'Seems fixed with PR #8007 ', 'Yes, it is fixed at `master`.']",[],[],0,0
440,keras,2808,closed,How does the parameter of decay in keras work concretely?,"I'm confused about the  parameter in  function of keras, how does it work concretely, for example, I use a learning rate of , and set the  parameter to be , anyone can help me? Thanks in advance!
",stale,"['The learning rate (`lr`) updates according to [code](https://github.com/fchollet/keras/blob/master/keras/optimizers.py#L126). \n', ""Thank you, but I'm confused about the `self.iterations` ,does it mean a batch updata, of an epoch, or something, for example, I want to train a RNN, and I ran the training with total size of `20000 training data, batch_size=20,np_epoch=3, initial learning rate =0.1,decay=1e-6`, then how will it do the decay?\n\n```\n def get_updates(self, params, constraints, loss):\n     grads = self.get_gradients(loss, params)\n     lr = self.lr * (1. / (1. + self.decay * self.iterations))\n     self.updates = [(self.iterations, self.iterations + 1.)]\n```\n"", 'a batch update\n', 'batch update meaning it decays the lr every batch (means decay 1000 times in the single epoch)?', 'yes, i believe so, since get_updates is called on each batch.']",[],"['decay', 'SGD()', 'lr=1.0', 'decay', 'decay=1e-6']",0,0
441,keras,4161,closed,Using allow_growth on keras with tensorflow,"I'm using Keras with TensorFlow to train a large number of tiny networks (~4 layers, less than 30 nodes in each layer). Currently TF allocates all GPU memory to a single process and therefore prevents me from opening more learning processes in parallel. I found on TF document that I can use 



to do this. However, I wasn't able to integrate that into keras. Does someone know the way to initialize a tf session on keras? Thank you very much!
",,"['Keras uses global TF session. You should be able to set it to your custom session using this function: https://github.com/fchollet/keras/blob/master/keras/backend/tensorflow_backend.py#L86\n', 'Thank you very much. This perfectly solved my problem. \n', 'I am sorry, I am new to KEras. How do I use it in my .py file. I need to have the equivalent of \r\n\r\n```\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nconfig.log_device_placement=True\r\n\r\nsess = tf.Session(config=config)  #With the two options defined above\r\n\r\nwith tf.Session() as sess:\r\n...\r\n```\r\nThanks.', '@vijaycd , if you are still looking for an actual code you can copy-paste into your Keras code to have Tensorflow dynamically allocate the GPU memory:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom keras.backend.tensorflow_backend import set_session\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\r\nconfig.log_device_placement = True  # to log device placement (on which device the operation ran)\r\n                                    # (nothing gets printed in Jupyter, only if you run it standalone)\r\nsess = tf.Session(config=config)\r\nset_session(sess)  # set this TensorFlow session as the default session for Keras\r\n```', 'Thank you, will file for later use. I was wondering whether my earlier\nrequest had entered cybervoid. Guess not, someone is active!\nBest,\nVijay\n\n\n\n<https://mailtrack.io/> Sent with Mailtrack\n<https://chrome.google.com/webstore/detail/mailtrack-for-gmail-inbox/ndnaehgpjlnokgebbaldlmgkapkpjkkb?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality>\n\nOn Thu, Feb 15, 2018 at 11:14 AM, Zoltan Fedor <notifications@github.com>\nwrote:\n\n> @vijaycd <https://github.com/vijaycd> , if you are still looking for an\n> actual code you can copy-paste into your Keras code to have Tensorflow\n> dynamically allocate the GPU memory:\n>\n> import tensorflow as tf\n> from keras.backend.tensorflow_backend import set_session\n> config = tf.ConfigProto()\n> config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n> config.log_device_placement = True  # to log device placement (on which device the operation ran)\n>                                     # (nothing gets printed in Jupyter, only if you run it standalone)\n> sess = tf.Session(config=config)\n> set_session(sess)  # set this TensorFlow session as the default session for Keras\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/keras-team/keras/issues/4161#issuecomment-366031228>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AZBBQ0Irp-8g0bil1YHlv0iMzC-1nOPBks5tVIIhgaJpZM4KeQR2>\n> .\n>\n', '@kudkudak Which function were you referring to (providing a link to a specific row in a source file is only guaranteed to work until the file is edited)?', 'I think I was referring to set_session :)', 'Thank you!', 'I am currently take advantage of dynamic GPU memory allocation, for a scikit-learn `GridSearchCV` build on the `KerasClassifier` using the functional API.\r\n\r\nI set the config via `set_session`, but somehow neither dynamic nor fixed allocation seems to make a difference, when I start the gridsearch with `n_jobs = 2`.\r\n\r\nI frequently run into:\r\n\r\n    UserWarning: A worker stopped while some jobs were given to the executor.\r\n    This can be caused by a too short worker timeout or by a memory leak.\r\n\r\nAlso in `nvidia-smi` I do not see, that the processes take up less memory, than before.\r\n\r\nAny thoughts on this?', ""Hi,\r\n\r\nI have tried implementing the code suggested by @zoltan-fedor which seems to have worked for so many others, however with no luck for me. I receive the following error message:\r\n\r\n```\r\nUnknownError: 2 root error(s) found.\r\n  (0) Unknown: Fail to find the dnn implementation.\r\n\t [[{{node bidirectional_1/CudnnRNN_1}}]]\r\n\t [[loss/mul/_197]]\r\n  (1) Unknown: Fail to find the dnn implementation.\r\n\t [[{{node bidirectional_1/CudnnRNN_1}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```\r\n\r\nFor reference, I'm writing in Keras within a Google Colab notebook, and I have GPU access.\r\n\r\nI'm a relative newbie to the field, so any ideas on what's going on here and how to fix it would be greatly appreciated. Let me know if you need any more info too. Thanks."", ""> Keras uses global TF session. You should be able to set it to your custom session using this function: https://github.com/fchollet/keras/blob/master/keras/backend/tensorflow_backend.py#L86\r\n\r\nThis link doesn't link to the right code anymore. The lines referred to was this:\r\n```\r\ndef set_session(session):\r\n    '''Sets the global TF session.\r\n    '''\r\n    global _SESSION\r\n    _SESSION = session\r\n```"", ""> Hi,\r\n> \r\n> I have tried implementing the code suggested by @zoltan-fedor which seems to have worked for so many others, however with no luck for me. I receive the following error message:\r\n> \r\n> ```\r\n> UnknownError: 2 root error(s) found.\r\n>   (0) Unknown: Fail to find the dnn implementation.\r\n> \t [[{{node bidirectional_1/CudnnRNN_1}}]]\r\n> \t [[loss/mul/_197]]\r\n>   (1) Unknown: Fail to find the dnn implementation.\r\n> \t [[{{node bidirectional_1/CudnnRNN_1}}]]\r\n> 0 successful operations.\r\n> 0 derived errors ignored.\r\n> ```\r\n> \r\n> For reference, I'm writing in Keras within a Google Colab notebook, and I have GPU access.\r\n> \r\n> I'm a relative newbie to the field, so any ideas on what's going on here and how to fix it would be greatly appreciated. Let me know if you need any more info too. Thanks.\r\n\r\nReinstall CUDA and cuDNN"", 'it works for tensorflow (1.14.0) but not keras (2.0.9)', ""> it works for tensorflow (1.14.0) but not keras (2.0.9)\r\n\r\nA slight change to @zoltan-fedor's answer solved it for me.\r\nI am running keras and tf version 2.4.x\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.compat.v1.keras.backend import set_session   \r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True \r\nconfig.log_device_placement = True  \r\nsess = tf.compat.v1.Session(config=config)\r\nset_session(sess)  \r\n\r\nand then continue with \r\n\r\nfrom keras.models import Sequential  \r\netc..\r\n\r\n\r\n""]","['\nconfig.gpu_options.allow_growth = True\nsession = tf.Session(config=config, ...)\n']",[],0,0
442,keras,10426,closed,How does model.fit () calculate loss and acc ? Documentation will  be helpful. ,"I think  it uses functions like binary_entropy , binary_accuracy etc.   Is any smoothing used ?   Some description in the documentation will be good ( pointers to the exact code will also help). ",,"[""Loss and accuracy are calculated as you train, according to the loss and metrics specified in compiling the model. Before you train, you must compile your model to configure the learning process. This allows you to specify the optimizer, loss function, and metrics, which in turn are how the model fit function knows what loss function to use, what metrics to keep track of, etc. The loss function (like binary cross entropy) documentation can be found [here](https://keras.io/losses/) and the metrics (like accuracy) documentation can be found [here](https://keras.io/metrics/)\r\n\r\nIn summary, in order for you to train (and for fit to calculate loss and acc):\r\n1.  Instantiate model\r\n```\r\nmodel = Sequential()\r\n...\r\n```\r\n2. Compile the model. This is where the functions like binary cross entropy are specified for the model to call fit. The built in loss functions are documented and implemented [here](https://github.com/keras-team/keras/blob/e6c3f77b0b10b0d76778109a40d6d3282f1cadd0/keras/losses.py#L76).\r\n```\r\n# For a binary classification problem\r\nmodel.compile(optimizer='rmsprop',\r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy'])\r\n```\r\n3. Train by calling fit \r\n```\r\nmodel.fit(data, one_hot_labels, epochs=10, batch_size=32)\r\n``` \r\n\r\n\r\n"", 'Thanks , that is a little bit more information, but not exactly the answer to what I was asking.  My question is what exactly is the formula used for calculating loss and  accuracy per  batch ( e.g. the numbers that get displayed during model.fit() ) .   If you know where that is exactly done in the code  , that answer should be right there. ', 'Yes, like I linked, the formulas for calculating loss per batch can be found [here](https://github.com/keras-team/keras/blob/e6c3f77b0b10b0d76778109a40d6d3282f1cadd0/keras/losses.py#L76). The exact implementation is backend specific. For example if you were using tensorflow look [here](https://github.com/keras-team/keras/blob/master/keras/backend/tensorflow_backend.py#L3275)', 'Thanks again -  I  have seen those code by the way. Let me try again. The following is an output from a model.fit () session . \r\n\r\n5/125 [>.............................] - ETA: 4:35 - loss: 1.1342 - acc: 0.5250\r\n\r\nWhich exact  variable holds the loss (e.g. 1.1342) and accuracy (e.g. 0.5250)  ? \r\n\r\n', ""I believe that the loss is stored [here](https://github.com/keras-team/keras/blob/58fd1f0589d33aeb33c4129cfedfb7737495efc0/keras/engine/training.py#L434); similarly the accuracy is handled by the `handle_metrics` function. However, you shouldn't be working with these variables directly, if you want to calculate the loss and accuracy for your model given some data use `model.evaluate`"", 'Thanks again.  Basically my question is what mathematically the loss and accuracy displayed during  model.fit() are.  Is it averaged  over the batch ,  or some other function is used to calculate the loss/acc that gets displayed on the screen.   The  answer is still not there, hence  need to keep looking till the answer is found  and document it.  ', 'For training loss, keras does a running average over the batches. For validation loss, a conventional average over all the batches in validation data is performed. The training accuracy is the average of the accuracy values for each batch of training data during training. ', 'Good.  Is the running average over just the batch or there is carryover from previous batch ?   Would be great to see the code for that.  ', ""The code is [here](https://github.com/keras-team/keras/blob/master/keras/callbacks.py#L224), `BaseLogger` is applied to every Keras model and will update things like `loss` and `acc`. The training loss is carried over from the previous batch: it's the average of the losses over each batch of training data."", ""You are probably close.  If  the code ( see below)  indeed is responsible for the fit() per batch   loss/acc output, then the mystery is probably is how 'v'  got into the logs list before entering the   on_batch_end() function . This function primarily seems to be for   keeping track  variables for on_epoch_end () calculations.  \r\n\r\n    def on_batch_end(self, batch, logs=None):\r\n.\r\n.\r\n.\r\n        for k, v in logs.items():\r\n            if k in self.stateful_metrics:\r\n                self.totals[k] = v   < ---- \r\n            else:\r\n                if k in self.totals:\r\n                    self.totals[k] += v * batch_size\r\n                else:\r\n                    self.totals[k] = v * batch_size\r\n\r\n"", ""'v' gets added into the batch logs [here](https://github.com/keras-team/keras/blob/master/keras/engine/training_arrays.py#L203). `outs` is the output of the train function, `f`, which is built [when you call fit on your model](https://github.com/keras-team/keras/blob/master/keras/engine/training.py#L1014), which will output your loss and metrics from [here](https://github.com/keras-team/keras/blob/master/keras/engine/training.py#L502) given some inputs. An example of the log list looks like this (if you were to add a `print(logs)` in the `on_batch_end` method) \r\n```\r\n{'loss': 1.8399812, 'batch': 2, 'size': 32, 'acc': 0.5625}\r\n{'loss': 1.6450999, 'batch': 3, 'size': 32, 'acc': 0.53125}\r\n{'loss': 1.6244895, 'batch': 4, 'size': 32, 'acc': 0.40625}\r\n{'loss': 1.4333215, 'batch': 5, 'size': 32, 'acc': 0.5625}\r\n{'loss': 1.1506481, 'batch': 6, 'size': 32, 'acc': 0.75}\r\n{'loss': 1.0616728, 'batch': 7, 'size': 32, 'acc': 0.65625}\r\n{'loss': 1.313815, 'batch': 8, 'size': 32, 'acc': 0.46875}\r\n{'loss': 1.2656202, 'batch': 9, 'size': 32, 'acc': 0.5625}\r\n{'loss': 1.2441672, 'batch': 10, 'size': 32, 'acc': 0.5625}\r\n{'loss': 0.99731344, 'batch': 11, 'size': 32, 'acc': 0.65625}\r\n{'loss': 1.0144608, 'batch': 12, 'size': 32, 'acc': 0.71875}\r\n{'loss': 1.2803181, 'batch': 13, 'size': 32, 'acc': 0.4375}\r\n{'loss': 1.2766386, 'batch': 14, 'size': 32, 'acc': 0.5}\r\n{'loss': 0.87459761, 'batch': 15, 'size': 32, 'acc': 0.6875}\r\n```"", 'That is great.  So, it is still not clear if the  per batch loss/acc is the average over just that batch or something else.  ', 'Sorry, how do Keras calculate loss (e.g mse) when doing predict_on_batch. For example you have 10 batches, the model normally will return 10 losses', '@raymond-yuan When we choose ""acc"" to calculate accuracy, how does keras choose the calculating function? I found that there are several accuracy functions.', '@DayChan, check a comment to this page https://stackoverflow.com/a/59637143/5536388.']",[],[],0,0
443,keras,2776,closed,Scalar Input(),"Is it possible to have a scalar Input() instance?

I can use  to get output from  correctly but cannot use the input defined by  as it is not a Keras Tensor.

Is there a way to generate a scalar Input(), either directly or by converting the backend Tensor object?
",stale,"['Is this what you want? [Input](https://github.com/fchollet/keras/blob/master/keras/engine/topology.py#L974)\nDocstring below might help you.\n', ""`Input()` won't work since it automatically makes a tuple, i.e `ndim>0`\nSee llne 1022, `batch_shape = (None,) + tuple(shape)`\n"", 'Any updates on this?\n', '``` python\nscalar_input = Input(batch_shape=(1,))\n```\n', 'Thanks! \n', ""How to you give the input in the training phase then?\r\nBecause once you want to train the model based on the scalar value it throws the following error.\r\n`'int' object has no attribute 'shape'`"", ""I still had trouble with @farizrahman4u 's solution.\r\n\r\nHere's the hacky solution I just came up with (assuming you're going to use the `scalar_input` in some custom or lambda layer.\r\n\r\nJust create a vector input and get the scalar via indexing in your custom layer.  Example use case: you want to supply an input with each batch that tells you the max length of the sequence.  This gets around a lot of trickiness with variable length sequences when you reshape a lot.\r\n\r\n```\r\nvector_input = Input(shape(1, ))\r\n```\r\nLater on when you need this as a scalar, just access the first position via\r\n```\r\nmax_idx = vector_input[0, 0]\r\nrelevant_sequence = x[:, :max_idx]\r\n```"", 'this seems insane', ""Any good solution to this? I'm surprised that simple thing like this is not well supported in Keras.""]",[],"['K.backend.placeholder(ndim=0)', 'theano.function()', 'K.backend.placeholder(ndim=0)']",0,0
444,keras,7945,closed,Error while running Seq2seq model in Tensorflow version 1.3 and keras version 1.2.0,"
Traceback (most recent call last):
  File ""seq2seq.py"", line 90, in <module>
    Seq2seq.train_seq2seq()
  File ""seq2seq.py"", line 74, in train_seq2seq
    model = s2s.seq2seq_plain()
  File ""/home/david/Downloads/Seq2Seq-master/seq2seq/model.py"", line 28, in seq2seq_plain
    model.add(RNN(self.hidden_dim, return_sequences=True))#, input_shape=(100, 128)))
  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 327, in add
    output_tensor = layer(self.outputs[0])
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 543, in __call__
    self.build(input_shapes[0])
  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/recurrent.py"", line 763, in build
    self.W = K.concatenate([self.W_i, self.W_f, self.W_c, self.W_o])
  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py"", line 1222, in concatenate
    return tf.concat(axis, [to_dense(x) for x in tensors])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 1061, in concat
    dtype=dtypes.int32).get_shape(
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 611, in convert_to_tensor
    as_ref=False)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 676, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py"", line 121, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py"", line 102, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py"", line 376, in make_tensor_proto
    _AssertCompatible(values, dtype)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py"", line 302, in _AssertCompatible
    (dtype.name, repr(mismatch), type(mismatch).__name__))
TypeError: Expected int32, got <tf.Variable 'lstm_1_W_i:0' shape=(100, 200) dtype=float32_ref> of type 'Variable' instead.
`",,"['have you tackled this problem? I meet this problem today', '@kelly-tlz  yeah upgraded the version.Issue solved']",[],[],0,0
445,keras,3041,closed,BatchNormalization can't be used across TimeDistributed when component of sub model,"If I have a model defined like this, no issue 



But if the  is a component of another sub model, like so:



I get the following error:

BatchNormalizationmode=2BatchNormalization

If I remove the  object from the sub model it works just fine. 

Obviously the intended case for this, the sub model is more complicated. 
- [X] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [X] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [X] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",,"['> You are attempting to share a same `BatchNormalization` layer across different data flows. This is not possible. You should use `mode=2` in `BatchNormalization`, which has a similar behavior but is shareable (see docs for a description of the behavior).\n', 'Then why does it work one way and not the other? Functionally they should be equivalent - no? \n\nSent from my iPhone\n\n> On Jun 21, 2016, at 7:19 PM, François Chollet notifications@github.com wrote:\n> \n> Closed #3041.\n> \n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n', 'I am having a similar issue even without TimeDistributed\n', ""Ok so what about now? There's no mode for BatchNormalization. "", 'I think TimeDistributed wrapper requires an input tensor of [batch_size, num_time_steps, feature_size]. The BatchNormalization layer in the first example is applied on the feature_size axis. In another word, you are applying BN 10 times across time steps. However, in the second case, you are only applying BN once before reshaping it to (10 x 10) tensor. So, these two cases are fundamentally different.']","["" python\nmain_input = Input(shape=(100,), name='main_input')\nemb = Reshape((10, 10))(main_input)\nemb = TimeDistributed(BatchNormalization())(emb)\n\nloss_out = Dense(1, activation='sigmoid', name='loss_out')(Flatten()(emb))\n\nmodel = Model(input=[main_input], output=[loss_out])\noptimizer = Adam(lr=0.001, clipnorm=grad_clip)\nmodel.compile(optimizer, loss='binary_crossentropy')\n"", "" python\nmain_in_bn = Input(shape=(100,), name='main_input')\nx = Dense(10)(main_in_bn)\nx = BatchNormalization()(x)\nsub_model = Model(input=[main_in_bn], output=[x])\n\nmain_input = Input(shape=(100,), name='main_input')\n\nemb= Reshape((10, 10))(main_input)\nemb = TimeDistributed(sub_model)(emb)\n\nloss_out = Dense(1, activation='sigmoid', name='loss_out')(Flatten()(emb))\n\nmodel = Model(input=[main_input], output=[loss_out])\noptimizer = Adam(lr=0.001, clipnorm=grad_clip)\nmodel.compile(optimizer, loss='binary_crossentropy')\n""]","['BatchNormalization', '', '\nException: You are attempting to share a same ', ' layer across different data flows. This is not possible. You should use ', ' in ', ', which has a similar behavior but is shareable (see docs for a description of the behavior).\n', '', 'BatchNormalization']",0,0
446,keras,5448,closed,border_mode = 'same' in pooling layers still changes the dimentions ,I am trying to use a pooling layer (both 1D and 2D) with border_mode = 'same' but it seems to change the dimensions anyway,stale,"['1D: `stride=(pool_length/2)`\r\n2D: `stride=(pool_size[0]/2, pool_size[1]/2)`\r\n\r\nAlso, this is entirely expected behavior - pooling is generally used to decrease output size, and [definitely will when you use the default stride](https://keras.io/layers/pooling/).', ""Yes but border_mode = 'same' should ZeroPad back to the original dimensions. \r\n"", 'Yes, if stride == poolSize / 2. By default, stride == poolSize\n\n-----Original Message-----\nFrom: ""Yam Peleg"" <notifications@github.com>\nSent: \u200e2/\u200e19/\u200e2017 11:23 PM\nTo: ""fchollet/keras"" <keras@noreply.github.com>\nCc: ""Pat York"" <pat.york@nevada.unr.edu>; ""Comment"" <comment@noreply.github.com>\nSubject: Re: [fchollet/keras] border_mode = \'same\' in pooling layers stillchanges the dimentions (#5448)\n\nYes but border_mode = \'same\' should ZeroPad back to the original dimensions.\n—\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.', 'can\'t i use a different stride size? as far as i know, strides are used to ""skip"" cells when applying the pooling filter, am i right?\r\n', 'Yes, and if you skip cells (more than poolSize/2) you reduce dimensionality.\n\nDraw a picture of a simple example (4x4 image, 2 pool and 2 stride) see what size you end up with when you do no padding, 1 (same) padding, and 2 (full padding). You\'ll see the dimensionality is reduced in the first 2 cases, but not in the 3rd. However, in the third you\'ll find that the output has a border of padding, which is essentially the output of border_mode=valid followed with a Padding2D((1,1)) layer. In other words, sure the dimensionality is technically the same, but its because it was padded with zeros..\n\n-----Original Message-----\nFrom: ""Yam Peleg"" <notifications@github.com>\nSent: \u200e2/\u200e19/\u200e2017 11:29 PM\nTo: ""fchollet/keras"" <keras@noreply.github.com>\nCc: ""Pat York"" <pat.york@nevada.unr.edu>; ""Comment"" <comment@noreply.github.com>\nSubject: Re: [fchollet/keras] border_mode = \'same\' in pooling layers stillchanges the dimentions (#5448)\n\ncan\'t i use a different stride size? as far as i know, strides are used to ""skip"" cells when applying the pooling filter, am i right?\n—\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.', ""Thank you, this is what i was asking. If this is intentional, i just wanted to pad the output with zeros even though it is not at the same dimensions.\r\nI'll just use zero padding layers for this. \r\n\r\nHowever i still think automatic zero padding should be considered to be added as a border mode.\r\n\r\n "", 'With border mode=full it is exactly what you are asking for: An artificially inflated output size. I don\'t know if keras allows that border mode in pooling layers, because it is only available in Theano and is equivalent to just padding afterwards. You\'re right that a subsequent padding layer will accomplish what you need. However, I recommend you still draw a picture so that you understand why the border mode, pool size, and stride interact in the way they do.\n\n\nPlease close this issue, and direct any future implementation/non-bug questions to the Slack channel or Stackoverflow.\n\n-----Original Message-----\nFrom: ""Yam Peleg"" <notifications@github.com>\nSent: \u200e2/\u200e19/\u200e2017 11:42 PM\nTo: ""fchollet/keras"" <keras@noreply.github.com>\nCc: ""Pat York"" <pat.york@nevada.unr.edu>; ""Comment"" <comment@noreply.github.com>\nSubject: Re: [fchollet/keras] border_mode = \'same\' in pooling layers stillchanges the dimentions (#5448)\n\nThank you, this is what i was asking. If this is intentional, i just wanted to pad the output with zeros even though it is not at the same dimensions.\nI\'ll just use zero padding layers for this.\nHowever i still think automatic zero padding should be considered to be added as a border mode.\n—\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs, but feel free to re-open it if needed.\n']",[],[],0,0
447,keras,1628,closed,Using outputs with missing values,"I'm trying to train a multi-task regression model, but my outputs are not complete (in fact I only have on average <1% of the values per training instance).  I expected mean squared error for the non-null outputs to be a reasonable objective, however, obviously using keras' mean squared error objective, the cost comes out as , as the nans will propagate.  

Is there any plans for supporting this sort of thing (or is it already supported somehow and I missed it?)

If not, anyone have an idea of a hack? I tried writing a new cost function, like:



(sorry for mixing keras and theano!)

This evaluates correctly on 1D vectors with nans in the y_true, but this doesn't work with keras, even with batch size set to 1.  My next plan was to set the NaNs in y_true to equal y_pred, but I'm not experienced with theano.
",stale,"['Why not using a if in your cost function, like if output contains a nan then loss is 0 (so no gradient, and keeps weights intact) ?\n', ""Hi all,\n\nI came back to this after spending a little time with theano and working on other things.  I think I fixed it for this use case using switch:\n\n```\n# perhaps something like this could be added to the backend API?\nK.is_nan = T.isnan                       # tf.is_nan\nK.logical_not = lambda x: 1 - x     # tf.logical_not\n\ndef squared_error_mv(y_true, y_pred):\n    return K.sum(K.switch(K.logical_not(K.is_nan(y_true)), K.square(y_pred - y_true), 0), axis=-1)\n```\n\nAllowed me to train a model with missing values.\n\nThis unfortunately didn't work with tensorflow:\n`ValueError: Shapes (?, ?) and () must have the same rank`\n\nI don't think this sort of behaviour should be added to the cost functions by default, but perhaps there is a sensible way of including it somewhere in the library?\n"", ""Ah @pommedeterresautee, just saw you commented (I've been without internet, so I couldn't submit this comment before), thanks!  This is basically what you were suggesting, I initially couldn't find the correct theano method to do this (or couldn't get them to work correctly!).  I got the slicing technique working, but was crazy slow, as one would expect. \n"", ""@richlewis42 : Could it be that you're getting the value error because your if and else options are not of the same size?\n\n`K.square(y_pred - y_true), 0`\nThe zero will only have size (), and the calculated value will be (?, ?). Perhaps something like tf.zeros() to make a list of zeros to use in the else case?\n"", 'I am interested in this too, but in recurrent network (return_sequences=True). I do not have labels for some elements in sequences.', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', 'Did any one try to use something like this:\r\n\r\n```\r\ndef custom_error_function(y_true, y_pred):\r\n    bool_finite = T.is_finite(y_true)\r\n    return K.mean(K.square(T.boolean_mask(y_pred, bool_finite) - T.boolean_mask(y_true, bool_finite)), axis=-1)\r\n```\r\n\r\nIt works for me on a small test dataset with np.nan values in y_true.\r\n\r\n', '2019: This would still be amazing to have :) Did anyone manage to implement this for TensorFlow?', 'In keras you can set different weights for each output.\r\nA solution would be to set weight to zero for the output with missing value .\r\n(And probably replace the nan by zero just in case)', ""@organic-chemistry I'm not aware this functionality exists in Keras... There's only `class_weights` and `sample_weights`. Do you have a code sample?"", ""Yes you are right, I was thinking that it was a loss_weights.\r\nYou can do it this way by creating a loss that depends on a weight loss,\r\nrather than using a switch: You define two models:\r\n```\r\nfrom keras.models import Model\r\nfrom keras.layers import Input, Dense\r\nimport keras.backend as K\r\n\r\ninp = Input(shape=(3,))\r\n\r\nw1 = Input(shape=(1,))\r\nw2 = Input(shape=(1,))\r\n\r\nout1 = Dense(1)(inp)\r\nout2 = Dense(1)(inp)\r\n\r\ndef weighted_loss(weight):\r\n\r\n    def loss(y_true,y_pred):\r\n        return K.mean(K.square(y_pred - y_true) * weight , axis=-1)\r\n    return loss\r\n\r\nmodel = Model(inputs=inp, outputs=[out1,out2])\r\n\r\nmodelw = Model(inputs=[inp,w1,w2],outputs=[out1,out2])\r\nmodelw.compile(optimizer='rmsprop',\r\n              loss=[weighted_loss(w1),weighted_loss(w2)])\r\n```\r\n\r\nThen for example to train it:\r\n\r\n```\r\nimport numpy as np\r\ninp_v = np.array([[0,0,0],[1,1,1],[0,1,1],[1,0,0],[1,1,0]])\r\n\r\nout1_v = np.array([0,0,0,0,0])\r\nout2_v = np.array([1,np.nan,np.nan,1,1])\r\n\r\nw1_v = np.array(~np.isnan(out1_v),dtype=np.int)\r\nw2_v = np.array(~np.isnan(out2_v),dtype=np.int)\r\n\r\n#Replace nan by 0 or anything just in case\r\nout1_v[np.isnan(out1_v)] = 10000\r\nout2_v[np.isnan(out2_v)] = 10000\r\n\r\nmodelw.fit([inp_v,w1_v,w2_v],[out1_v,out2_v],epochs=1000,verbose=False);\r\n\r\n```\r\n\r\nAnd then to predict: model.predict(inp_v)\r\n\r\n```\r\n[array([[0.00049998],\r\n        [0.00199989],\r\n        [0.00149992],\r\n        [0.00099995],\r\n        [0.00149993]], dtype=float32), array([[1.0005   ],\r\n        [0.8288187],\r\n        [0.8283187],\r\n        [1.0009999],\r\n        [1.0014999]], dtype=float32)]\r\n```\r\n\r\n\r\n"", ""Cool, thanks for that example! When we're already writing a custom loss function, we could also detect `nan`s directly right? Basically like your code, but create the weights dynamically based on a `nan`-mask.\r\n\r\nAFAIK, that's what previous posters here tried to build and so far nobody has succeeded for TensorFlow."", ""Hey @cpury , sorry for not replying until now!\r\n\r\nIn the end I did get this working with TF (back in 2016!). I don't have access to the code right now, but I think it worked mostly like above:\r\n\r\n```python\r\n>>> import tensorflow as tf\r\n>>> from numpy import array, nan\r\n\r\n>>> def mse_mv(y_true, y_pred):\r\n...     per_instance = tf.where(tf.is_nan(y_true),\r\n...                             tf.zeros_like(y_true),\r\n...                             tf.square(tf.subtract(y_pred, y_true)))\r\n...     return tf.reduce_mean(per_instance, axis=0)\r\n\r\n>>> y_true = array([[ 1.,  2.],\r\n...                 [ 2.,  3.],\r\n...                 [nan,  4.],\r\n...                 [ 5.,  6.]])\r\n\r\n>>> y_pred = array([[ 1.,  2.],\r\n...                 [ 2.,  4.],\r\n...                 [ 42,  4.],\r\n...                  [ 3.,  7.]])\r\n\r\n>>> loss = mse_mv(y_true, y_pred)\r\n>>> with tf.Session().as_default():\r\n...    loss.eval()\r\narray([1., 0.5.])\r\n```\r\nDunno if this will work with keras but I guess so.\r\n\r\nEdit: fixed it to me *mean* squared"", ""> Hey @cpury , sorry for not replying until now!\r\n> \r\n> In the end I did get this working with TF (back in 2016!). I don't have access to the code right now, but I think it worked mostly like above:\r\n> \r\n> ```python\r\n> >>> import tensorflow as tf\r\n> >>> from numpy import array, nan\r\n> \r\n> >>> def mse_mv(y_true, y_pred):\r\n> ...     per_instance = tf.where(tf.is_nan(y_true),\r\n> ...                             tf.zeros_like(y_true),\r\n> ...                             tf.square(tf.subtract(y_pred, y_true)))\r\n> ...     return tf.reduce_mean(per_instance, axis=0)\r\n> \r\n> >>> y_true = array([[ 1.,  2.],\r\n> ...                 [ 2.,  3.],\r\n> ...                 [nan,  4.],\r\n> ...                 [ 5.,  6.]])\r\n> \r\n> >>> y_pred = array([[ 1.,  2.],\r\n> ...                 [ 2.,  4.],\r\n> ...                 [ 42,  4.],\r\n> ...                  [ 3.,  7.]])\r\n> \r\n> >>> loss = mse_mv(y_true, y_pred)\r\n> >>> with tf.Session().as_default():\r\n> ...    loss.eval()\r\n> array([1., 0.5.])\r\n> ```\r\n> \r\n> Dunno if this will work with keras but I guess so.\r\n> \r\n> Edit: fixed it to me _mean_ squared\r\n\r\nThis solution did not work for me in the 2.2, so inspired by the previous solution and assuming targets true values have missing values, I suggest this simple code, in which I suggest replacing NaN values with the prediction values:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import losses\r\n\r\nclass MeanSquaredErrorLossThatIgnoresNaN(losses.MeanSquaredError):\r\n    def __init__(self, *args, **kwargs):\r\n        losses.MeanSquaredError.__init__(self, *args, **kwargs)\r\n\r\n    def __call__(self, y_true, y_pred, sample_weight=None):\r\n        y_true = tf.where(tf.math.is_nan(y_true), y_pred, y_true)\r\n        return losses.MeanSquaredError.__call__(self, y_true, y_pred, sample_weight=sample_weight)\r\n```\r\nUssage:\r\n\r\n`model.compile(..., loss=MeanSquaredErrorLossThatIgnoresNaN())`\r\n\r\nAm I doing something terribly wrong in your opinion?"", ""> Hey @cpury , sorry for not replying until now!\r\n> \r\n> In the end I did get this working with TF (back in 2016!). I don't have access to the code right now, but I think it worked mostly like above:\r\n> \r\n> ```python\r\n> >>> import tensorflow as tf\r\n> >>> from numpy import array, nan\r\n> \r\n> >>> def mse_mv(y_true, y_pred):\r\n> ...     per_instance = tf.where(tf.is_nan(y_true),\r\n> ...                             tf.zeros_like(y_true),\r\n> ...                             tf.square(tf.subtract(y_pred, y_true)))\r\n> ...     return tf.reduce_mean(per_instance, axis=0)\r\n> \r\n> >>> y_true = array([[ 1.,  2.],\r\n> ...                 [ 2.,  3.],\r\n> ...                 [nan,  4.],\r\n> ...                 [ 5.,  6.]])\r\n> \r\n> >>> y_pred = array([[ 1.,  2.],\r\n> ...                 [ 2.,  4.],\r\n> ...                 [ 42,  4.],\r\n> ...                  [ 3.,  7.]])\r\n> \r\n> >>> loss = mse_mv(y_true, y_pred)\r\n> >>> with tf.Session().as_default():\r\n> ...    loss.eval()\r\n> array([1., 0.5.])\r\n> ```\r\n> \r\n> Dunno if this will work with keras but I guess so.\r\n> \r\n> Edit: fixed it to me _mean_ squared\r\n\r\nHi @lewisacidic \r\nI have two questions regarding your answer: when you replace your NANs by 0, in the case that 0 has a meaning like other values (for instance in climatic time series), is it still correct to do so ? My second question is that when we average the new sum of squared, shouldn't we divide by number of non-NANs? \r\nMany thanks!""]","[' python\ndef mean_squared_error(y_true, y_pred):\n    return K.mean(K.square(y_pred - y_true)[(1 - T.isnan(y_true)).nonzero()], axis=-1)\n']",['nan'],0,0
448,keras,9634,closed,LSTM - how to reshape the input dataset if prediction is made based on both past and current features,"Let's say I have a dataset of shape **(30, 2)**, which means I have **30 samples**, and **2 features**, which are sales and promotion. I want to build a model to perform 1 day ahead forecasting of sales, based on **sales of the last 5 days** and **promotion of the last 5 days** and **the current day**. By doing some shifting and removing NA's, I could get a new dataframe with 25 rows, and 12 features, which are, **promotion(t-5), sales(t-5), promotion(t-4), sales(t-4), ... , promotion(t-1), sales(t-1), promotion(t), sales(t)**. The last column will be treated as response **y**, and the first 11 columns will be treated as **X**, the question is how to shape X, should I add a new column with constant and shape it into (25, 6, 2)?  the promotion(t) column has to be included in X",,"['Why would you do it like this? LSTM already takes care of the fact that your data is a sequence and you are breaking it by concatenating a bunch of the data into 1 timestep', ""@tRosenflanz I don't get what you were saying. How come it is 1 timestep, I am using promotion(t-5), sales(t-5), promotion(t-4), sales(t-4), ... , promotion(t-1), sales(t-1) and promotion(t) to predict sales(t), it should have at least have 5 timesteps.\r\nI asked a similar question on [Stack Overflow](https://stackoverflow.com/questions/49245331/how-to-use-lstm-to-make-prediction-with-both-feature-from-the-past-and-the-curre), it includes both the original data and the transformed data, That probably helps explain what I wanted to do. "", 'Oh sorry. I misread your setup. I will answer your question on Stack Overflow so feel free to close this issue here - it is more appropriate for Stack Overflow since this is an implementation question', '@tRosenflanz Thank you very very much!!!!']",[],[],0,0
449,keras,7262,closed,Shape of output of RNN,"Hi, I am new to Keras and RNN
I have a dataset that has 1000 videos, each video has 2000 frames and each frame has 5 features. I would like to train a RNN that can classify each video frame to 1 of the 3 categories. 

Example
Video1
  Frame1 [fc11,fc12,fc13,fc14,fc15] -> output1
  Frame2 [fc21,fc22,fc23,fc24,fc25] -> output2
  Frame3 [fc31,fc32,fc33,fc34,fc35] -> output3
  ...
Video2
...


x_shape = [1000, 2000, 5]
y_shape = [1000, 2000, 3] (Since each frame has it's own y value)
I am a little confused how to construct a Model that will fit this shape
",stale,"['@chenzhuo8804 .  Are you sure you want an RNN model in the first place?  From your description, it sounds like you want something like logistic regression type/multilayer perceptron if you are classifying individual frames, but in any case the Keras documentation might give you some guidance:  https://keras.io/layers/recurrent/ as a start.  I hope that helps.\r\n\r\n', ""@td2014 Thank you very much, I wanted to use RNN since in videos, current frames's target is somehow related to previous frame's target, that is why I want to try out RNN.\r\nWhat I am stuck now is that most of the RNN examples included will have only 1 output for a sequence instead per timestep(in my example, the y_shape will be [1000,3] instead of [1000,2000,3])"", '@chenzhuo8804 .  It sounds like what you are asking for is to set ""return_sequences=True"" in the layer definition, and having 3 output nodes???', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],0,0
450,keras,2586,closed,Mixture Density Network quickly gets to nan loss,"Hello,

I've got the latest pulls of keras and theano.

I am trying to reproduce the code from [this blog post](http://cbonnett.github.io/MDN.html).

What I have so far is in a gist [here](https://gist.github.com/sergeyf/cf20b2759a7d38035f30384769bed9df).

The issue is that after a few batch updates (on the order of 10), the loss becomes nan.

Here is a repaste of the most likely culprit: the negative log-likelihood loss function:



I've tried to alter various parts of this function, and can't spot any specific issues.

I've added the log-sum-exp trick to prevent underflow in there, and when I check out values of each intermediate variable within the loss function during any of the batches before the nan loss, they all seem very well behaved.

Any ideas?

Thanks for your help, and the fabulous package.
",,"[""Are you doing any sort of grad clipping or grad norm restraints? \n\nmy optimizers typically get instantiated something like:\n\n``` python\n        optimizer = Adam(self.igor.LR, clipnorm=self.igor.max_grad_norm, \n                                       clipvalue=self.igor.grad_clip_threshold)\n```\n\nI find that numbers in the mid single digits usually works decently.  \n\nOne question, though, is where do NaNs pop in at, though? Well, it turns out it usually only happens when infs are interacting ([stack overflow question on it](http://stackoverflow.com/questions/25506281/what-are-all-the-possible-calculations-that-could-cause-a-nan-in-python)).    And how do you get infs?  Well, a couple ways:\n\n``` python\nimport numpy as np\nnp.log([0]) # returns -inf\n1/np.array([0.]) #returns inf\nnp.exp(1e100) # returns info\n```\n\nAside from the overflow one (the exp, which you seem to be already thinking about), you should think about how you can clip your values in order to avoid zeros.  for instance, assuming `true_repeated` is on a support set of [0,1]:\n\n``` python\nnp.clip(true_repeated, 1e-8, 1.-1e-8)\n```\n\nClipping where you can reasonably clip to avoid 0s would work well, I think.\n\nFinally, if you're using theano, they have a nice page on this: http://deeplearning.net/software/theano/tutorial/nan_tutorial.html\n"", 'Thanks for the great tips @braingineer !\n\nClipping `true_repeated` solved the problem.\n', ""Actually, I took a look at this as MDNs are pretty interesting to me and definitely run into NaNs pretty easily. (Which has been my experience trying to train MDNs always as you've got I'd expect super ugly gradients)\n"", ""The gist has code that doesn't become NaNs now, but it still doesn't solve my toy problem, unfortunately.\n"", ""Yeah, I'm looking at the updated gist actually-- I think it's a an interesting toy problem. It's actually very very similar to what I'm working on. \n\nAnd re NaNs, sometimes I get them, sometimes I don't. I'll keep playing with it to see where they're coming from.\n"", 'have you run it with nanguard mode?  I find that usually helps me find where the damn nans are coming from. \n']","['\ndef neg_beta_mixture_likelihood(true, parameters):\n    m = K.shape(parameters)[-1] // 3\n\n    alphas = parameters[:, 0 * m: 1 * m] # nbatch by m\n    betas = parameters[:, 1 * m: 2 * m] # nbatch by m \n    pis = parameters[:, 2 * m: 3 * m] # nbatch by m\n\n    # true is nbatch by 1\n    # true_repeated is nbatch by m\n    true_repeated = K.repeat_elements(true, m, axis=-1)\n\n    d1 = (alphas - 1.0) * K.log(true_repeated)\n    d2 = (betas - 1.0) * K.log(1.0 - true_repeated)\n    f1 = d1 + d2\n    f2 = gammaln(alphas)      \n    f3 = gammaln(betas)      \n    f4 = gammaln(alphas + betas) \n    exponent = f1 + f4 - f2 - f3\n\n    # log sum exp trick to prevent numerical issues\n    max_exponent = K.max(exponent, axis=-1, keepdims=True)\n    max_exponent_repeated = K.repeat_elements(max_exponent, m, axis=-1)\n    likelihood = pis * K.exp( exponent - max_exponent_repeated  )  \n\n    return K.mean( -(K.log(K.sum(likelihood,axis=-1)) + max_exponent), axis=-1 )\n']",[],0,0
451,keras,2361,closed,ImportError while fitting a Keras model,"Hi, I got an error while fitting a model on Keras 1.0 on Theano on Windows 10 x64. Previously, I can fit Keras models on Keras 0.6 with Theano 0.8-dev just fine. Here a simple Perceptron code to demonstrate the error while fitting:





Is this a code regression in Keras or just another installation error? If so, any tips to debug it?

Thanks.

**Additional Info:**
- [x] Up-to-date with the master branch of Keras
- [x] Up-to-date with the master branch of Theano
- OS: Windows 10 x64
- Running on CPU
- Backend: Theano
- can import theano
",,"['This is an internal Theano error, most likely linked to your Theano\ninstall. Try reinstalling Theano.\n\nOn 16 April 2016 at 09:30, Rizky Luthfianto notifications@github.com\nwrote:\n\n> Hi, I got an error while fitting a model on Keras 1.0 on Theano on Windows\n> 10 x64. Previously, I can fit Keras models on Keras 0.6 with Theano 0.8-dev\n> just fine. Here a simple Perceptron code to demonstrate the error while\n> fitting:\n> \n> model = Sequential()\n> model.add(Dense(64, input_dim=220, activation=\'relu\'))\n> model.add(Dropout(0.5))\n> model.add(Dense(3, activation=\'softmax\'))\n> model.compile(loss=\'categorical_crossentropy\', optimizer=\'sgd\')\n> \n> model.fit(X,y,nb_epoch=1)\n> \n> ---\n> \n> ImportError                               Traceback (most recent call last)\n> <ipython-input-9-38fd95106b65> in <module>()\n> ----> 1 model.fit(X, y, nb_epoch=1)\n> \n> C:\\Anaconda2\\lib\\site-packages\\keras\\models.pyc in fit(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\n>     400                               shuffle=shuffle,\n>     401                               class_weight=class_weight,\n> --> 402                               sample_weight=sample_weight)\n> \n> ...........\n> (truncated for brevity)\n> ...........\n> \n> C:\\Anaconda2\\lib\\site-packages\\theano\\gof\\cmodule.pyc in compile_str(module_name, src_code, location, include_dirs, lib_dirs, libs, preargs, py_module, hide_symbols)\n>    2211             open(os.path.join(location, ""**init**.py""), \'w\').close()\n>    2212             assert os.path.isfile(lib_filename)\n> -> 2213             return dlimport(lib_filename)\n> \n> C:\\Anaconda2\\lib\\site-packages\\theano\\gof\\cmodule.pyc in dlimport(fullpath, suffix)\n>     297             warnings.filterwarnings(""ignore"",\n>     298                                     message=""numpy.ndarray size changed"")\n> --> 299             rval = **import**(module_name, {}, {}, [module_name])\n>     300         t1 = time.time()\n>     301         import_time += t1 - t0\n> \n> ImportError: (\'The following error happened while compiling the node\', Dot22(dense_input_1, dense_2_W), \'\\n\', \'DLL load failed: The specified module could not be found.\', \'[Dot22(dense_input_1, dense_2_W)]\')\n> \n> Is this a code regression in Keras or just another installation error? If\n> so, any tips to debug it?\n> \n> Thanks.\n> \n> _Additional Info:_\n> \n>    -\n> \n>    Up-to-date with the master branch of Keras\n>    -\n> \n>    Up-to-date with the master branch of Theano\n>    -\n> \n>    OS: Windows 10 x64\n>    -\n> \n>    Backend: Theano\n>    -\n> \n>    Running on CPU\n> \n> —\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly or view it on GitHub\n> https://github.com/fchollet/keras/issues/2361\n', ""@fchollet thank you for your very fast response. Unfortunately, reinstalling Theano didn't solve the error\n"", ""I agree that this is an error in Theano. I haven't found the solution, but because it's not an error in Keras, I'll mark this issue as closed.\nThanks.\n""]","[""\nmodel = Sequential()\nmodel.add(Dense(64, input_dim=220, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(3, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='sgd')\n# Fine\n\nmodel.fit(X,y,nb_epoch=1)\n# Error\n"", '\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\n<ipython-input-9-38fd95106b65> in <module>()\n----> 1 model.fit(X, y, nb_epoch=1)\n\nC:\\Anaconda2\\lib\\site-packages\\keras\\models.pyc in fit(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\n    400                               shuffle=shuffle,\n    401                               class_weight=class_weight,\n--> 402                               sample_weight=sample_weight)\n\n...........\n(truncated for brevity)\n...........\n\nC:\\Anaconda2\\lib\\site-packages\\theano\\gof\\cmodule.pyc in compile_str(module_name, src_code, location, include_dirs, lib_dirs, libs, preargs, py_module, hide_symbols)\n   2211             open(os.path.join(location, ""__init__.py""), \'w\').close()\n   2212             assert os.path.isfile(lib_filename)\n-> 2213             return dlimport(lib_filename)\n\nC:\\Anaconda2\\lib\\site-packages\\theano\\gof\\cmodule.pyc in dlimport(fullpath, suffix)\n    297             warnings.filterwarnings(""ignore"",\n    298                                     message=""numpy.ndarray size changed"")\n--> 299             rval = __import__(module_name, {}, {}, [module_name])\n    300         t1 = time.time()\n    301         import_time += t1 - t0\n\nImportError: (\'The following error happened while compiling the node\', Dot22(dense_input_1, dense_2_W), \'\\n\', \'DLL load failed: The specified module could not be found.\', \'[Dot22(dense_input_1, dense_2_W)]\')\n']",[],0,0
452,keras,6838,closed,Customize metrics: binary_accuracy of outputs > 0.5,"I have searched high and low but found no satisfying results...
So, here is the problem:
The neural network outputs a (4,) array, and the predicted labels should be those greater than 0.5.
The problem is, top_k_categorical_accuracy won't work.(of course)
And, when I tried to implement one by my own,
I noticed that:

the parameter **y_true and y_pred** are tensors. And what I tried to do is in-place assignment, making those values greater than 0.5 become 1 and others 0.
Any ideas?",,"['@zeka0 : Have you checked into the lambda layers?:\r\nFor example:\r\n""add a x -> x^2 layer""\r\nmodel.add(Lambda(lambda x: x ** 2))\r\n\r\nDocumentation link here for details:\r\nhttps://keras.io/layers/core/\r\n\r\nHope that helps or gives you some additional direction.', 'I solved it, here is the code to turn output into 1s(if >= 0.5) and 0s(if < 0.5):\r\n```python\r\ndef threshold_binarize(x, threshold=0.5):\r\n    ge = tf.greater_equal(x, tf.constant(threshold))\r\n    y = tf.where(ge, x=tf.ones_like(x), y=tf.zeros_like(x))\r\n    return y\r\n```\r\nI have tried using Lambda layer with the above function to transform the output, but I encountered a mysterious errors using it:\r\n```\r\n\r\nFile ""D:\\Anaconda\\lib\\site-packages\\keras\\models.py"", line 856, in fit\r\n    initial_epoch=initial_epoch)\r\n  File ""D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py"", line 1481, in fit\r\n    self._make_train_function()\r\n  File ""D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py"", line 1013, in _make_train_function\r\n    self.total_loss)\r\n  File ""D:\\Anaconda\\lib\\site-packages\\keras\\optimizers.py"", line 141, in get_updates\r\n    v = self.momentum * m - lr * g  # velocity\r\n  File ""D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py"", line 667, in _run_op\r\n    return getattr(ops.Tensor, operator)(a._AsTensor(), *args)\r\n  File ""D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py"", line 820, in binary_op_wrapper\r\n    y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=""y"")\r\n  File ""D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py"", line 639, in convert_to_tensor\r\n    as_ref=False)\r\n  File ""D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py"", line 704, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File ""D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py"", line 113, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File ""D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py"", line 102, in constant\r\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File ""D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py"", line 360, in make_tensor_proto\r\n    raise ValueError(""None values not supported."")\r\nValueError: None values not supported.\r\n```', ""I came across another problem...\r\nWhile training the model using Adam optimizer and with loss function being 'binary_crossentropy',\r\nI noticed the loss is increasing while metrics(accuracy) increasing too. Paradox isn't it?\r\nWhat the hell? I thought optimizer should minimize the entropy of the model...\r\n\r\nEdit:\r\nI seem to find the problem, tackling it right now.\r\nGood luck to myself....."", '@zeka0  how do you solve it finally?', 'If so, the threshold_binary_accuracy  doesn\'t decrease anymore? So how to modify the loss function accordingly?  Still use ""binary_crossentropy"" like this \r\nmodel.compile(loss=\'binary_crossentropy\', optimizer=adam, metrics=[ threshold_binary_accuracy, precision, recall, f1_score ] )\r\noutput are:\r\n 300s - loss: 1.3017 - threshold_binary_accuracy: 0.3165 - val_loss: 1.3797 - val_threshold_binary_accuracy: 0.3421\r\n\r\nEpoch 00001: threshold_binary_accuracy improved from -inf to 0.31647, saving model to (200,200).h5\r\nEpoch 2/20\r\n - 291s - loss: 1.2290 - threshold_binary_accuracy: 0.3033 - val_loss: 1.3601 - val_threshold_binary_accuracy: 0.3564\r\n\r\nEpoch 00002: threshold_binary_accuracy did not improve from 0.31647\r\nEpoch 3/20\r\n - 291s - loss: 1.1486 - threshold_binary_accuracy: 0.2963 - val_loss: 1.2142 - val_threshold_binary_accuracy: 0.3058\r\n\r\nEpoch 00003: threshold_binary_accuracy did not improve from 0.31647\r\nEpoch 4/20\r\n - 289s - loss: 1.0801 - threshold_binary_accuracy: 0.2767 - val_loss: 1.2386 - val_threshold_binary_accuracy: 0.3089\r\n\r\nEpoch 00004: threshold_binary_accuracy did not improve from 0.31647\r\nEpoch 5/20\r\n - 289s - loss: 1.0360 - threshold_binary_accuracy: 0.2696 - val_loss: 1.2030 - val_threshold_binary_accuracy: 0.3042\r\n\r\nEpoch 00005: threshold_binary_accuracy did not improve from 0.31647\r\nEpoch 6/20\r\n - 290s - loss: 0.9817 - threshold_binary_accuracy: 0.2606 - val_loss: 1.1841 - val_threshold_binary_accuracy: 0.3249\r\n\r\nEpoch 00006: threshold_binary_accuracy did not improve from 0.31647\r\nEpoch 7/20\r\n - 287s - loss: 0.9713 - threshold_binary_accuracy: 0.2573 - val_loss: 1.1359 - val_threshold_binary_accuracy: 0.3060\r\n\r\nEpoch 00007: threshold_binary_accuracy did not improve from 0.31647\r\nEpoch 8/20\r\n - 286s - loss: 0.9159 - threshold_binary_accuracy: 0.2410 - val_loss: 1.2064 - val_threshold_binary_accuracy: 0.2783\r\n']",[],"['def customized_metrics(y_true, y_pred):']",0,0
453,keras,7288,closed,How to visualize an attention mechanism in a classification task?,"I've managed to finish experiments using attention mechanism adopted from [@cbaziotis implementation](https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2) and now i'm confused with the visualization. I don't really understand the heatmap as well. If you guys can explain these to me, it means a lot :

1. What is heatmap? how to read them?
2. What to visualize in an attention mechanism? The weight?
3. The code to visualize an attention mechanism, in conjunction to [@cbaziotis' implementation](https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2)

Thanks in advance!",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', '@aryopg Any updates?']",[],[],0,0
454,keras,7956,closed,How can we know the final value of alpha that is learned in PReLU? ,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ Y] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [Y] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",type:support,"[""Hi\r\n\r\nThis issue isn't related to a bug/enhancement/feature-request or other accepted types of issue.\r\n\r\nTo ask questions, please see the following resources :\r\n\r\nhttps://keras.io/\r\nhttps://keras.io/#support\r\nhttps://gitter.im/Keras-io/Lobby\r\n\r\nThanks!\r\n\r\nIf you think I made a mistake, please open another issue.""]",[],[],0,0
455,keras,5912,closed,New version of imdb dataset is broken (word order?),"The IMDB dataset in imdb.npz is broken or at least incompatible with the provided word index--converting reviews to a string gives nonsense (either the wrong words or mixed up word order--not sure). Example:



Output:


- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).",stale,"[""Vaguely related: https://github.com/fchollet/keras/blob/master/keras/datasets/imdb.py#L103 looks wrong to me. \r\n\r\nIt's currently `if w >= num_words or w < skip_top:`, which will only keep overly frequent or overly infrequent words. I think you want the opposite: `if w < num_words and w > skip_top:`. (Looks like a code path that's rarely used since oov_char has a not-None default value)."", ""and related again (how I found the above):\r\nhttps://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py#L22 says that infrequent words will be dropped, but https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py#L26 calls `imdb.load_data` without setting `oov_char=None`, so it looks like it'll replace rare words with `oov_char`, not drop them.\r\n\r\nIf someone can confirm that these are indeed mistakes, I can make a PR to fix..."", ""I ran into the same issue today and it seems that the current index is shifted by 3\r\n\r\n```python\r\nfrom keras.datasets import imdb\r\n(X_train, y_train), (X_test, y_test) = imdb.load_data('/tmp/imdb.npz', num_words=None)\r\nidx = imdb.get_word_index()\r\nrev_idx = {v:k for k,v in idx.items()}\r\n\r\n# deal with non-existing indices\r\nfor i in range(-3, 1):\r\n    rev_idx[i] = '----'\r\n\r\n' '.join([rev_idx[word-3] for word in X_train[0]])\r\n```\r\n\r\nnot sure of the cause of this\r\n"", ""After comparing the index with the one [here](https://ai.stanford.edu/~amaas/data/sentiment/) they seem to roughly correspond (although the keras index is shorter), so I think it's the numbers in the data that are shifted"", 'The shift by 3 is introduced by the `index_from=3` argument in the `load_data` function: https://github.com/fchollet/keras/blob/master/keras/datasets/imdb.py#L73', 'I got this error while I\'m tryig to load the data. \r\n\r\nTraceback (most recent call last):\r\n  File ""BiLSTM.py"", line 24, in <module>\r\n    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\r\n  File ""/usr/local/lib/python3.5/site-packages/keras/datasets/imdb.py"", line 53, in load_data\r\n    x_train = f[\'x_train\']\r\nTypeError: tuple indices must be integers or slices, not str\r\n', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']","['\r\nimport keras\r\nfrom keras.datasets import imdb\r\nimport cPickle as pickle\r\nimport os.path\r\n\r\nword2index = imdb.get_word_index()\r\nindex2word = dict([(i,w) for (w,i) in word2index.items()])\r\n\r\n# use new load_data\r\n# (post https://github.com/fchollet/keras/commit/b0500764a8a451882262328c0f1676402fb0a783)\r\n(x_train, y_train), (x_test, y_test) = imdb.load_data()\r\n\r\n# Load manually downloaded old pickle file (before above commit)\r\n# from https://s3.amazonaws.com/text-datasets/imdb_full.pkl\r\npath = os.path.expanduser(\'~/.keras/datasets/imdb_full.pkl\')\r\nf = open(path, \'rb\')\r\n(x_train2, y_train2), (x_test2, y_test2) = pickle.load(f)\r\n\r\ndef totext(review):\r\n    return \' \'.join(index2word[i] for i in review)\r\n\r\nprint(keras.__version__)\r\n\r\nprint(""New commit review -- makes no sense:"")\r\nprint(totext(x_train[0][:20]))\r\n\r\nprint(""\\nOld commit review:"")\r\nprint(totext(x_train2[0][:20]))\r\n', '\r\n2.0.1\r\nNew commit review -- makes no sense:\r\nthe as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have\r\n\r\nOld commit review:\r\nbromwell high is a cartoon comedy it ran at the same time as some other programs about school life such\r\n']",[],0,0
456,keras,8121,closed,[BUG] Number of trainable weights seem to change after model compilation,"Consider the following scenario



I would expect that, since  has been compiled, the output of the two  calls would be the same. However, after running the above code the actual output is



Is this the expected behavior? 

In addition, from other experiments I get the feeling that although the number of trainable weights reported by  differs in the two cases, the actual number of trainable weights is the expected one, that is, in the example above, even after setting , training will really update only 2,112 parameters and not 5,344. So, maybe this is just a reporting issue.",,"['I get a very similar issue when running the following snippet from the Keras docs. Based on the output of summary(), it seems like even after compiling, the number of trainable parameters changes in both trainable_model and frozen_model depending on whether I set layer.trainable = True or layer.trainable = False.\r\n\r\nx = Input(shape=(32,))\r\nlayer = Dense(32)\r\nlayer.trainable = False\r\ny = layer(x)\r\n\r\nfrozen_model = Model(x, y)\r\n\r\nlayer.trainable = True\r\ntrainable_model = Model(x, y)']","['python\r\nfrom keras.layers import Input, Dense\r\nfrom keras.models import Model\r\nfrom keras.optimizers import Adam\r\n\r\nx = Input(shape=(100,))\r\ny1 = Dense(units=32)(x)\r\nmodel1 = Model(inputs=x, outputs=y1)\r\nprint(""MODEL 1"")\r\nmodel1.summary()\r\n\r\nmodel1.trainable = False\r\nx = Input(shape=(100,))\r\ny1 = model1(x)\r\ny2 = Dense(units=64)(y1)\r\nmodel2 = Model(inputs=x, outputs=y2)\r\nmodel2.compile(optimizer=Adam(), loss=\'categorical_crossentropy\')\r\n\r\nprint(""MODEL 2"")\r\nmodel2.summary()\r\n\r\nmodel1.trainable = True\r\nprint(""MODEL 2 after"")\r\nmodel2.summary()\r\n', '\r\n_________________________________________________________________\r\nMODEL 2\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_2 (InputLayer)         (None, 100)               0         \r\n_________________________________________________________________\r\nmodel_1 (Model)              (None, 32)                3232      \r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 64)                2112      \r\n=================================================================\r\nTotal params: 5,344\r\nTrainable params: 2,112\r\nNon-trainable params: 3,232\r\n_________________________________________________________________\r\nMODEL 2 after\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_2 (InputLayer)         (None, 100)               0         \r\n_________________________________________________________________\r\nmodel_1 (Model)              (None, 32)                3232      \r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 64)                2112      \r\n=================================================================\r\nTotal params: 5,344\r\nTrainable params: 5,344\r\nNon-trainable params: 0\r\n']","['model2', 'model2.summary()', 'summary()', 'model1.trainable=True']",0,0
457,keras,3241,closed,Model Summary shape not correct.,"I have a vector  of size  and  of size . I can compute the dot product to get a a vector of size . However, the model summary shape does not agree with the expected shape. Code below should reproduce the error.


- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",,"['Nevermind, upgrading keras was the issue.\n', '@shyamupa  I check the Merge source code and don\'t know how to use it in your way.\n\n> r = merge([Y, alpha], output_shape=(300, 1), name=""r_"", mode=get_R)\n\nDoes the `output_shape=(300, 1)` be set randomly?\n [ Y ,alpha]  are passing into the function get_R  \nthe shape of Y is (none ,300,20), the shape of alpha is (none,20)\n\n> def get_R(X):\n>     Y, alpha = X[0], X[1]\n>     ans = K.T.batched_dot(Y, alpha)\n>     return ans\n\nI don\'t know the shape of ans. Does the shape of ans be equal to the  `output_shape=(300, 1)`\n', '@alyato Sorry, but it is unclear to me what relationship your question has to PR #2938.  If you are looking for assistance using Keras the forum is [Keras Google Group](https://groups.google.com/forum/#!forum/keras-users). For instructions on how to submit issues please see https://github.com/fchollet/keras/blob/master/CONTRIBUTING.md. \n', ""@shaunharker I'm so sorry. I should ask shyamupa. Thanks.\n""]","[' python\nfrom keras.layers import Input, merge\nfrom keras.models import Model\nimport numpy as np\nimport keras.backend as K\n\n\ndef get_R(X):\n    Y, alpha = X[0], X[1]\n    ans = K.T.batched_dot(Y, alpha)\n    return ans\n\n\nalpha_val = np.arange(200).reshape((10, 20))\nY_val = np.arange(60000).reshape((10, 300, 20))\n\nY = Input(shape=(300, 20))\nalpha = Input(shape=(20,))\n\nr = merge([Y, alpha], output_shape=(300, 1), name=""r_"", mode=get_R)\n\nmodel_me = Model(input=[Y, alpha], output=r)\n\nmodel_me.summary()\n\nprint(model_me.predict([Y_val, alpha_val]).shape)\n\n']","['alpha', '(None,20)', 'Y', '(None,300,20)', '(None,300)']",0,0
458,keras,1533,closed,fit_generator workers fail silently,"If you use  function and pass it a parameter  which is higher than 1 and at the same time provide a non threadsafe generator as a first argument, a confusing situation occurrs.

The generator worker fails:
https://github.com/fchollet/keras/blob/master/keras/models.py#L953
because of an error - in my case it was , but it can be any other error. It fails silently because of the  statement and only sets the  event, so it's difficult to get what's wrong.

I have an older version of the repo, but in my version the line:
https://github.com/fchollet/keras/blob/master/keras/models.py#L966
doesn't exist and therefore, the program breaks in a weird way on the line:
https://github.com/fchollet/keras/blob/master/keras/models.py#L974
with a . I see that it has now been fixed, but I still had some troubles with figuring out that the problem is with the generator.

Wouldn't it be a good idea to remove this  block altogether or at least remove the _catch 'em all_ statement? 
",,"[""Sorry, the issue was fixed few days ago, I just didn't update my code. My bad.\n""]",[],"['model.fit_generator()', 'nb_worker', 'ValueError: generator already executing', 'try ... except', '_stop', 'UnboundLocalError', 'try...except']",0,0
459,keras,12188,closed,TypeError: object of type 'ImageDataGenerator' has no len(),"Hi all!

I have an error when I am trying to use **ImageDataGenerator** with **flow_from_directory** function for transfer learning of NasNet model from **keras.applications**.

OS: ArchLinux
Tensorflow version: 1.12.0
Keras version: 2.2.4 (updated from master)
GPUs: GeForce GTX 1080 Ti
CUDA version: 9.0.176-4
CUDNN version: 7.0.5-2

My code:



Output I get:



I tried to debug the code and seems like in this line https://github.com/keras-team/keras/blob/e59570ae26670f788d6c649191031e4a8824f955/keras/engine/training_generator.py#L110 if statement is false due **val_gen == False**. In the code above variable **val_gen** could be initialized as False because generator has no **\_\_next\_\_** or **next** functions.

Is it normal behavior?

",Enhancement Good first issue stat:contributions welcome,['@Dref360 already answered in #10186. Please look closely at the examples in the documentation. `flow_from_directory` returns the object you are looking for. Though I believe that the best would be to have a more meaningful error message. \r\n\r\nPR welcome for a better error message. '],"['from keras.applications.nasnet import NASNetMobile\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.models import Model\r\nfrom keras.layers import Dense, GlobalAveragePooling2D\r\nfrom keras import backend as K\r\nfrom keras.optimizers import SGD, Adam\r\nfrom keras.callbacks import ModelCheckpoint\r\n\r\nTRAIN_SAMPLES = 2081\r\nTEST_SAMPLES = 904\r\nBATCH_SIZE = 32\r\n\r\nTRAIN_DATA_DIR = \'./train\'\r\nTEST_DATA_DIR = \'./test\'\r\n\r\n# create the base pre-trained model\r\nbase_model = NASNetMobile(weights=\'imagenet\', include_top=False)\r\n\r\n# add a global spatial average pooling layer\r\nx = base_model.output\r\nx = GlobalAveragePooling2D()(x)\r\n# let\'s add a fully-connected layer\r\nx = Dense(1024, activation=\'relu\')(x)\r\n# and a logistic layer -- let\'s say we have 200 classes\r\npredictions = Dense(200, activation=\'softmax\')(x)\r\n\r\n# this is the model we will train\r\nmodel = Model(inputs=base_model.input, outputs=predictions)\r\n\r\n# first: train only the top layers (which were randomly initialized)\r\n# i.e. freeze all convolutional InceptionV3 layers\r\nfor layer in base_model.layers:\r\n    layer.trainable = False\r\n\r\n# prepare train dataset\r\ntrain_gen = ImageDataGenerator(rescale=1. / 255,\r\n                               shear_range=0.2,\r\n                               zoom_range=0.2,\r\n                               horizontal_flip=True)\r\n\r\ntrain_gen.flow_from_directory(directory=TRAIN_DATA_DIR,\r\n                              target_size=base_model.input.shape[1:3],\r\n                              class_mode=\'categorical\',\r\n                              batch_size=BATCH_SIZE,\r\n                              shuffle=True)\r\n\r\n# prepare test dataset\r\nval_gen = ImageDataGenerator(rescale=1. / 255)\r\n\r\nval_gen.flow_from_directory(directory=TEST_DATA_DIR,\r\n                            target_size=base_model.input.shape[1:3],\r\n                            class_mode=\'categorical\',\r\n                            batch_size=BATCH_SIZE)\r\n\r\n# compile the model (should be done *after* setting layers to non-trainable)\r\nmodel.compile(optimizer=Adam(lr=0.00001), loss=\'categorical_crossentropy\')\r\n\r\ncheckpoint = ModelCheckpoint(filepath=""./NasNet_mobile_model_weights.h5"",\r\n                             monitor=[""acc""],\r\n                             verbose=1,\r\n                             mode=\'max\')\r\n\r\n# train the model on the new data for a few epochs\r\nmodel.fit_generator(train_gen,\r\n                    steps_per_epoch=TRAIN_SAMPLES//BATCH_SIZE,\r\n                    epochs=50,\r\n                    validation_data=val_gen,\r\n                    validation_steps=TEST_SAMPLES//BATCH_SIZE,\r\n                    callbacks=[checkpoint],\r\n                    verbose=2, \r\n                    workers=4\r\n                    )\r\n', '\r\nUsing TensorFlow backend.\r\n2019-02-01 10:07:40.704532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.76GiB\r\n2019-02-01 10:07:40.856400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 1 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:03:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.76GiB\r\n2019-02-01 10:07:41.022663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 2 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:82:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.76GiB\r\n2019-02-01 10:07:41.204602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 3 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:83:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.76GiB\r\n2019-02-01 10:07:41.206764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1, 2, 3\r\n2019-02-01 10:07:42.265616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-02-01 10:07:42.265657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 2 3 \r\n2019-02-01 10:07:42.265663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y N N \r\n2019-02-01 10:07:42.265667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N N N \r\n2019-02-01 10:07:42.265670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2:   N N N Y \r\n2019-02-01 10:07:42.265674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 3:   N N Y N \r\n2019-02-01 10:07:42.266421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10401 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n2019-02-01 10:07:42.266867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10403 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1)\r\n2019-02-01 10:07:42.267168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10403 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:82:00.0, compute capability: 6.1)\r\n2019-02-01 10:07:42.267448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10403 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:83:00.0, compute capability: 6.1)\r\nFound 2081 images belonging to 15 classes.\r\nFound 904 images belonging to 15 classes.\r\nTraceback (most recent call last):\r\n  File ""/home/smirnvla/PycharmProjects/keras-nasnet/train.py"", line 118, in <module>\r\n    verbose=2\r\n  File ""/usr/lib/python3.6/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper\r\n    return func(*args, **kwargs)\r\n  File ""/usr/lib/python3.6/site-packages/keras/engine/training.py"", line 1418, in fit_generator\r\n    initial_epoch=initial_epoch)\r\n  File ""/usr/lib/python3.6/site-packages/keras/engine/training_generator.py"", line 133, in fit_generator\r\n    if len(validation_data) == 2:\r\nTypeError: object of type \'ImageDataGenerator\' has no len()\r\n\r\nProcess finished with exit code 1\r\n']",[],0,0
460,keras,1384,closed,Why this model cannot be compiled,"Hi!

Now I implement DAG-CNNs
（http://arxiv.org/pdf/1505.05232.pdf）

It models cannot be compiled for error. but I cannot found node don't connect.
Visualize graph is expected graph.

Why errors happen this model?

Traceback (most recent call last):
  File ""/Users/Tereka/Programing/MachineLearningCombinator/mlc/model/keras_recipe.py"", line 315, in <module>
    model.compile('sgd', {'output':'categorical_crossentropy'})
  File ""build/bdist.macosx-10.9-x86_64/egg/keras/models.py"", line 1047, in compile
  File ""build/bdist.macosx-10.9-x86_64/egg/keras/optimizers.py"", line 79, in get_updates
  File ""build/bdist.macosx-10.9-x86_64/egg/keras/optimizers.py"", line 47, in get_gradients
  File ""build/bdist.macosx-10.9-x86_64/egg/keras/backend/theano_backend.py"", line 373, in gradients
  File ""/Users/Tereka/.pyenv/versions/2.7.8/lib/python2.7/site-packages/theano/gradient.py"", line 545, in grad
    handle_disconnected(elem)
  File ""/Users/Tereka/.pyenv/versions/2.7.8/lib/python2.7/site-packages/theano/gradient.py"", line 532, in handle_disconnected
    raise DisconnectedInputError(message)
theano.gradient.DisconnectedInputError: grad method was asked to compute the gradient with respect to a variable that is not part of the computational graph of the cost, or is used only by a non-differentiable operator: <TensorType(float32, 4D)>
Backtrace when the node is created:
  File ""build/bdist.macosx-10.9-x86_64/egg/keras/backend/theano_backend.py"", line 34, in variable
    return theano.shared(value=value, name=name, strict=False)


",,"['#1275 #1321 \nStill unresolved...\n', 'A PR resolving this has been merged. Please confirm that the issue is fixed.\n', ""Thank's I will check solving this problem!\n""]","[' python\ndef built_layer_before_fc(input_shape,n_filter):\n    g = Graph()\n    input_name = ""input""\n    conv_name = ""conv""\n    activate_conv = ""activate""\n    g.add_input(name=input_name, input_shape=input_shape)\n\n    g.add_node(Convolution2D(n_filter,3,3),name=conv_name,input=input_name)\n    g.add_node(PReLU(),name=activate_conv, input=conv_name)\n\n    g.add_output(""output"",activate_conv)\n\n    return g\n\ndef built_fc_layer(input_shape,average_layer,n_class=447):\n    g = Graph()\n    pooling_size = list(input_shape)\n    pooling_size.pop(0)\n\n    input_name = ""input""\n    average_pool_name = ""avg_pool""\n    avg_normalize = ""avgn""\n    flatten_name = ""flatten""\n    dense_output = ""dense_out""\n\n    g.add_input(input_name, input_shape)\n\n    g.add_node(AveragePooling2D(pool_size=(2,2)),name=average_pool_name,input=input_name)\n    g.add_node(BatchNormalization(),name=avg_normalize,input=average_pool_name)\n    g.add_node(Flatten(), name=flatten_name, input=avg_normalize)\n    g.add_node(Dense(n_class),name=dense_output,input=flatten_name)\n\n    g.add_output(""output"",dense_output)\n\n    return g\n\ndef built_pool_layer(input_shape):\n    g = Graph()\n    input_name = ""input""\n    normalize_name = ""norm""\n    max_pool_name = ""mx_pool1""\n    g.add_input(input_name, input_shape)\n    g.add_node(BatchNormalization(),normalize_name,input_name)\n    g.add_node(MaxPooling2D(pool_size=(2,2)),max_pool_name,normalize_name)\n    g.add_output(""output"",max_pool_name)\n\n    return g\n\ndef dag_content(input_shape,n_filter=64,n_class=100,n_conv=5):\n    dag_cnn_content = Graph()\n    input_name = ""input""\n    dag_cnn_content.add_input(input_name, input_shape)\n\n    layer_names = []\n\n    conv_name = None\n    pool_layer_name = None\n    fc_layer_name = None\n\n    for i in xrange(n_conv):\n        conv_name = ""conv{}"".format(i)\n        pool_layer_name = ""pool{}"".format(i)\n        fc_layer_name = ""fc{}"".format(i)\n\n        conv_layer = built_layer_before_fc(input_shape,n_filter)\n        fc_layer = built_fc_layer(cinput_shape(conv_layer),2,n_class)\n        pool_layer = built_pool_layer(cinput_shape(conv_layer))\n\n        print i,input_shape,cinput_shape(conv_layer),cinput_shape(fc_layer),cinput_shape(pool_layer)\n        dag_cnn_content.add_node(conv_layer,conv_name,input_name)\n        print input_name,conv_name\n        dag_cnn_content.add_node(fc_layer,fc_layer_name,conv_name)\n        print conv_name,fc_layer_name\n        dag_cnn_content.add_node(pool_layer,pool_layer_name,conv_name)\n        print conv_name,pool_layer_name\n\n        input_shape = cinput_shape(pool_layer)\n\n        layer_names.append(fc_layer_name)\n        input_name = pool_layer_name\n        n_filter = n_filter * 2\n\n        #fc_layer.compile(\'sgd\', {\'output\':\'categorical_crossentropy\'})\n    conv_name = ""conv{}"".format(n_conv)\n    fc_layer_name = ""fc{}"".format(n_conv)\n\n    conv_layer = built_layer_before_fc(input_shape,n_filter)\n    fc_layer = built_fc_layer(cinput_shape(conv_layer),2,n_class)\n\n    print input_shape,cinput_shape(conv_layer),cinput_shape(fc_layer)\n\n    dag_cnn_content.add_node(conv_layer,conv_name,input_name)\n    print input_name,conv_name\n    dag_cnn_content.add_node(fc_layer,fc_layer_name,conv_name)\n    print conv_name,fc_layer_name\n    layer_names.append(fc_layer_name)\n    print layer_names\n\n    dag_cnn_content.add_node(Activation(""softmax""), name=\'softmax_layer\', inputs=layer_names,merge_mode=""sum"")\n\n    dag_cnn_content.add_output(""output"", \'softmax_layer\')\n    return dag_cnn_content\n\ndef built_dagcnns(input_shape,n_filter=64,n_class=100,n_conv=5):\n    dag_cnn_content_g = dag_content(input_shape,n_filter,n_class,n_conv)\n\n    return dag_cnn_content_g\n\nif __name__ == \'__main__\':\n    model = built_dagcnns((3,256,256),n_filter=64,n_class=447,n_conv=5)\n    model.compile(\'sgd\', {\'output\':\'categorical_crossentropy\'})\n']",[],0,0
461,keras,8924,closed,How to build correct data with a LSTM NN?,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",,[],[],[],0,0
462,keras,2155,closed,Inconsistent behavior between backends + missing docs,"The docs for  method of  a  say

> y: labels, as a numpy array.

It's unclear, however, wheter i should be a vector of ints or one-hot matrix.
Perhaps one should mention that there exists ?

Also, the following code will fail on Theano but not on Tensorflow backend.


",stale,"[""Hi @elanmart , if you look up the documentation for categorical_crossentropy, it says:\n\ncategorical_crossentropy: Also known as multiclass logloss. Note: using this objective requires that your labels are binary arrays of shape (nb_samples, nb_classes).\n\nSo it seems your y should be of shape (10000, 2) and for each row, use [0, 1] and [1, 0] to denote classes.\n\nI've been using theano backend and never tried tensorflow though.\n"", '> Also, the following code will fail on Theano but not on Tensorflow backend.\n\nEven nastier, it silently produces a wrong result on Tensorflow.\n']","["" python\nimport numpy as np\nimport numpy.random as nr\n\nfrom keras.utils import np_utils\nfrom keras.layers.core import Layer, Dense, Dropout\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Sequential\n\nX = np.random.randn(10000, 500)\ny = np.random.randint(0, 64, size=(10000, 1))\n\ntest = Sequential()\ntest.add(Layer(input_shape=(500, )))\n\ntest.add(Dense(1024, activation='relu'))\ntest.add(Dropout(0.5))\ntest.add(BatchNormalization())\n\ntest.add(Dense(64, activation='softmax'))\n\ntest.compile('adam', 'categorical_crossentropy')\ntest.fit(X, y)\n""]","['fit', 'Model', 'np_utils.to_categorical(y)']",0,0
463,keras,4822,closed,SimpleRNN : Hidden and output units of different dimensions,"Hi Keras,

Thank you for this project!

It seems that you can't make the hidden unit and the output units of a SimpleRNN layer have différent values.

The doc says in the argument part :

""output_dim: dimension of the internal projections and final output.""

It seems unclear to me what are ""internal projections"". Anyway there's only one argument of this type so it's clear you can't set the hidden state dimension as you want.

Is it wanted? What if I want a big hidden state yo remember a lot of information but a unidimensionnal output?

Merry Christmas :)",,"['A SimpleRNN layer is just plain vanilla RNN, where the layer has `output_dim` neurons. This means that the layer will output `output_dim` values to the next layer, and also to the subsequent step in time. In essence, a SimpleRNN layer will take `input_dim + output_dim` inputs (the input at time `t` and the output from the layer at time `t-1`) and output `output_dim` values.\r\n\r\n""Internal Projections"" essentially means the number of neurons. The phrasing comes from the fact that, with N neurons in a layer, the data is being ""compressed"" into N dimensions; compressing something from M dimensions into N is a ""projection"".', ""Hi patyork thanks for this answer !\r\n\r\nOk I think I understood what goes on behind the scenes... I was influenced by the litterature I read to think a RNN layer always has two distinct hidden unit and output unit, I think I'll have to add a Dense layer to the network to get what I want.\r\n\r\nIt may be possible to make this part of the doc more clear with a drawing, since RNNs are not the simplest things to understand for newcomers in my opinion.\r\n\r\nThanks again :)"", 'Hi, I have a related question. \r\n\r\nIn the document, not only SimpleRNN but also LSTM or GRU has only the argument \r\n`units: Positive integer, dimensionality of the output space.`\r\n and there is no argument about hidden state size.\r\n\r\nLSTM or GRU also has no hidden unit and its output fed into next time step? Or hidden size is influenced by output dimension = `units`?\r\n\r\nThanks in advance :)']",[],[],0,0
464,keras,1685,closed,fit_generator,"So I've been trying to work with the fit_generator method for the graph model.
All works well if I set nb_worker=1, but any value higher than that and it just crashes.

It seems thread.start automatically triggers _stop.is_set() after the first thread. This is in python3. Haven't tested python2.7

I lack the skills for any more informative debugging >.>
",stale,"[""You should read #1638 and try my solution. It worked for me in Python 2.7 for the Sequential() model. However, I must say that it doesn't work on `ImageDataGenerator` class. Write your own data generator, which I have also explained in #1638.\n""]",[],[],0,0
465,keras,4079,closed,load_model not working - NameError: global name 'batch_size' is not defined????,"I had to restart my ipython interpreter so I saved the model with model.save() but loading isnt working

batch_size is of course set. 50.
The model is a deeper deconv vae from the examples, nothing fancy. vae_loss is the same with the example.



model_fom_configSequential.from_config(config)
",stale,"['Hey,\n\nI have the same issue.\nAs workaround I created the Model and called `model.compile`. After that I loaded the weights manually with `model.load_weights(...)`\n', 'Load weight from the model.save file? (With packs the model and the weights?)\n', 'Same problem here.\n', 'Experiencing same problem. ', 'Does anyone have a simple code snippet to reproduce the issue?', 'i had the same problem by confusing model.save with model.save_weigths...', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', ""I am using a pre-trained model such as Alexnet, In this case also, ending up with the same error.\r\n\r\nI was downloaded the alexnet_weights from here-->https://github.com/heuritech/convnets-keras\r\nthen I tried like this\r\n\r\nfrom keras.models import load_model\r\nbase_model=load_model('alexnet_weights.h5')\r\n\r\nI ended up with\r\n\r\nValueError: No model found in config file.\r\n\r\nplease help me to get rid out of it."", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']","[""\nfrom keras.models import load_model\nvae = load_model('Image_VAECNN04-256_100.h5', custom_objects={'vae_loss':vae_loss})\n""]","['', ""\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n<ipython-input-38-ea67b750c140> in <module>()\n      1 from keras.models import load_model\n----> 2 vae = load_model('Image_VAECNN04-256_100.h5', custom_objects={'vae_loss':vae_loss})\n\n/opt/conda/lib/python2.7/site-packages/keras/models.pyc in load_model(filepath, custom_objects)\n    126         raise ValueError('No model found in config file.')\n    127     model_config = json.loads(model_config.decode('utf-8'))\n--> 128     model = model_from_config(model_config, custom_objects=custom_objects)\n    129 \n    130     # set weights\n\n/opt/conda/lib/python2.7/site-packages/keras/models.pyc in model_from_config(config, custom_objects)\n    175         raise Exception('"", "" expects a dictionary, not a list. '\n    176                         'Maybe you meant to use "", ""?')\n--> 177     return layer_from_config(config, custom_objects=custom_objects)\n    178 \n    179 \n\n/opt/conda/lib/python2.7/site-packages/keras/utils/layer_utils.pyc in layer_from_config(config, custom_objects)\n     34         layer_class = get_from_module(class_name, globals(), 'layer',\n     35                                       instantiate=False)\n---> 36     return layer_class.from_config(config['config'])\n     37 \n     38 \n\n/opt/conda/lib/python2.7/site-packages/keras/engine/topology.pyc in from_config(cls, config, custom_objects)\n   2373 \n   2374         for layer_data in config['layers']:\n-> 2375             process_layer(layer_data)\n   2376 \n   2377         name = config.get('name')\n\n/opt/conda/lib/python2.7/site-packages/keras/engine/topology.pyc in process_layer(layer_data)\n   2370                         layer(input_tensors[0])\n   2371                     else:\n-> 2372                         layer(input_tensors)\n   2373 \n   2374         for layer_data in config['layers']:\n\n/opt/conda/lib/python2.7/site-packages/keras/engine/topology.pyc in __call__(self, x, mask)\n    512         if inbound_layers:\n    513             # this will call layer.build() if necessary\n--> 514             self.add_inbound_node(inbound_layers, node_indices, tensor_indices)\n    515             input_added = True\n    516 \n\n/opt/conda/lib/python2.7/site-packages/keras/engine/topology.pyc in add_inbound_node(self, inbound_layers, node_indices, tensor_indices)\n    570         # creating the node automatically updates self.inbound_nodes\n    571         # as well as outbound_nodes on inbound layers.\n--> 572         Node.create_node(self, inbound_layers, node_indices, tensor_indices)\n    573 \n    574     def get_output_shape_for(self, input_shape):\n\n/opt/conda/lib/python2.7/site-packages/keras/engine/topology.pyc in create_node(cls, outbound_layer, inbound_layers, node_indices, tensor_indices)\n    152             output_shapes = to_list(outbound_layer.get_output_shape_for(input_shapes[0]))\n    153         else:\n--> 154             output_tensors = to_list(outbound_layer.call(input_tensors, mask=input_masks))\n    155             output_masks = to_list(outbound_layer.compute_mask(input_tensors, input_masks))\n    156             output_shapes = to_list(outbound_layer.get_output_shape_for(input_shapes))\n\n/opt/conda/lib/python2.7/site-packages/keras/layers/core.pyc in call(self, x, mask)\n    554         if 'mask' in arg_spec.args:\n    555             arguments['mask'] = mask\n--> 556         return self.function(x, **arguments)\n    557 \n    558     def get_config(self):\n\n/opt/conda/lib/python2.7/site-packages/keras/layers/core.pyc in sampling(args)\n\nNameError: global name 'batch_size' is not defined\n"", '']",0,0
466,keras,1761,closed,validation accuracy superior to training accuracy,"Hi,
I'm working on a deep neural network (Sequential Dense), but i don't understand why the training accuracy is lower than the validation accuracy (the two sets being separated and having roughly the same distribution).
I started working on machine learning not so long ago and i was told that a way(the only one i know) to check if my network is not overfitting is to compare validation and train accuracy. If validation accuracy start dropping while the training accuracy continue to increase that's when i should be concerned.
Problem is validation accuracy is higher than training accuracy which doesn't make any sense for me...

I'm sure i missed something somewhere (maybe the training accuracy displayed by keras is not the one i think of) or the way i put my validation data is not the good one... Or maybe am I just wrong all along.
So if any one could give me an advice, or knows if there is a way to plot or visualize the data training within Keras to prevent overfit that would help.

I'm talking about that kind of thing (i'm copy pasting a random epoch but all are roughly the same):
50000/50000 [==============================] - 32s - loss: 1.7436 - acc.: 0.5749 - val. loss: 1.5925 - val. acc.: 0.6434

My network parameters are the following : a sequential dense network of that kind 20000>1000>1000>1000>1000 
trained on ReLU for all except the last Softmax. The Loss is categorical crossentropy
",,"['Check the following things when training any type of deep neural network:\n1. the data used to calculate training accuracy is not identical to the data used to train your NN. This sounds weird, but possible in practice, especially in case of images, if you don\'t keep track of what is happening. For example, you train on random patches of images and calculate training accuracy on _random_ patches of same images. It is easy to forget that though they are same images, the patches are randomly selected.\n2. More than the values of train and val accuracy, I would be concerned about what you said, ""i\'m copy pasting a random epoch but all are roughly the same"". No, they can\'t be same. Accuracy at different epochs is mostly different, because network is learning so it is constantly changing its weights. If accuracy goes up then that means it is approaching the minima of the loss function.\n3. I think you should be more concerned about getting a low training accuracy instead of getting a lower training accuracy than the validation accuracy.\n4. Do all the sanity checks given [here](http://cs231n.github.io/neural-networks-3/#gradcheck). Read the entire article if possible, it\'s very good.\n5. Make sure you are doing pre-processing in the right manner. For example, make sure that mean over entire training data is zero. For testing data, subtract the mean vector of the training data from each instance of testing data. Don\'t subtract the mean of testing data from itself. Since, you wouldn\'t know the mean of testing data at runtime.\n6. Check if your loss at the very first epoch makes sense. For example, in a 10-class classification problem, starting loss should be -ln(0.1) = 2.302 (given [here](http://cs231n.github.io/neural-networks-3/#gradcheck)). \n7. Again, from [here](http://cs231n.github.io/neural-networks-3/#gradcheck), overfit a tiny subset of data and make sure you can achieve zero cost. Full details in the link.\n8. If nothing works, just train and test on the same data and see if you can get 90% + accuracy. Otherwise, examine your network more closely by looking at individual layer outputs (given in Keras FAQ) etc.\n\nI am sure you will find the fault somewhere if you follow all these steps for debugging a neural network.\n', ""If your validation set is small and you're still underfitting, you can sometimes get higher validation accuracy just by random chance. Usually this will go away as you train more.\n"", ""I've been tracking the ratio of the training loss to the validation loss.  For me, the ratio starts off quite above 1 and slowly converges to 1 over time.\n\nI don't know if you are using dropout, but in thinking about why my validation loss is lower than the training loss, I am considering this to be a key reason.  The training loss is based upon the specific randomly selected dropped-out network for that record, which is much less likely to be well tuned than the loss when using the full network (with adjusted weights) on the validation data.\n"", 'This is also answered in the [FAQ](http://keras.io/faq/#why-is-the-training-loss-much-higher-than-the-testing-loss)\n', ""@rpinsler Yes sorry i didn't see it,  the use of dropout indeeds make the training accuracy lower than it should be.\nAnd thanks to you @parag2489 for all thoses advices and the reading, helped me a lot.\n"", ""I am also facing the same issue. My problem is 3 class classification(with 29 columns) and classes are highly imbalance.\r\n\r\nArchitecture of network:-\r\n- 4 Hidden layers with number of nodes in each layer be [columns, 1.25 * columns, 0.75 * columns, columns] where columns = 29.\r\n- I am using relu activation for all hidden layers and for output layer i am using softmax.\r\n- Adagrad Optimizer.\r\n- I am optimizing for sparse_categorical_crossentropy loss.\r\n- Dropout 0.1\r\n\r\n I observed few things which doesn't seem intuitive:-\r\ni) To account for class imbalance, I am oversampling from minority classes. And I observe that more I oversample there is decrease in  training and validation accuracy. \r\nii) validation accuracy is always greater than training accuracy irrespective of using oversampling or not.\r\n\r\nBelow is the code for the reference:-\r\n```\r\ndef create_model(columns, num_class, optimizer='Adagrad', init_mode='normal', activation='relu',\r\n\t\t\tdropout_rate=0.1, weight_constraint=5):\r\n\t\t\r\n\t\tmodel = Sequential()\r\n\t\tnum_nodes = [columns, int(1.25 * columns), int(0.75 * columns), columns]\r\n\r\n\t\tfor i in range(len(num_nodes)):\r\n\t\t\tmodel.add(Dense(columns, input_dim = columns, init = init_mode, activation = activation, W_constraint = maxnorm(weight_constraint)))\r\n\t\t\tmodel.add(Dropout(dropout_rate))\r\n\t\t\r\n\t\tmodel.add(Dense(output_dim = num_class, activation = 'softmax')) #Output layer\r\n\t\tmodel.compile(loss='sparse_categorical_crossentropy', optimizer = optimizer, metrics=['fbeta_score'])\r\n\t\treturn model\r\n\r\nmodel = create_model(X.shape[1], len(labels))\r\nmodel.fit(X_train, y_train, nb_epoch=num_epoch, batch_size=batch_size, validation_data=(X_test, y_test),callbacks=[history], verbose = 0)\r\n```""]",[],[],0,0
467,keras,12218,closed,Reshape function,"Can anyone guide me how can I use reshape function?
encoded (?, 4)
encod (?, 2)
normalize (?, 2)
Complex_symbols (?,)
decode(?,)
concat (?, 2)
deco (?, 4)
decod (?, 4)
I have size (None,) in 2 layers decode and concat where I need the size to be (None,1)
how should I use a Reshape function in this case? Or is there any other option




Please make sure that the boxes below are checked before you submit your issue.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:


- [ ] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",type:support,[],[],['pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps'],0,0
468,keras,5362,closed,Sample loss weights with multiple losses,"I have a Keras model with 2 losses. For all even observations I want to make use of both losses, whereas for all odd observations I want to use only the 1st loss. The  parameter in  seems to only allow me to multiply the loss contribution across all losses with a single scalar for each observation. How do I make all the odd observations not make use of the 2nd loss?",stale,"['Hello, \r\n\r\nHave you tried the natural thing of passing it an array of sample_weights (sample_weights=[samp_weight_loss1,samp_weight_loss2] ) ? \r\nIf I remember correctly it should work\r\nI gave a quick look to the code and it probably should work : \r\nhttps://github.com/fchollet/keras/blob/1f5455e29efa4579eebbf894e97ee53cd1257529/keras/engine/training.py#L624\r\n\r\n\r\n\r\n']",[],"['sample_weight', 'model.fit()']",0,0
469,keras,3879,closed,TypeError: load_weights() got an unexpected keyword argument 'by_name',"I saved a model's weight in an HDF5 file and I am trying to load it to initialize a different model (with a different output layer) using this command:
model.load_weights('model_weights.h5', by_name=True)

But I got this error:
TypeError: load_weights() got an unexpected keyword argument 'by_name'

Could anyone help?
",,"['Are you sure you have the latest version of keras? This functionality was added 21 days ago in #3488. Try updating keras.\n', ""Thanks @Higgcz . I just updated Keras and it's working now.\n"", '@fish128 No problem, it happened to me many times. :) I guess the issue can be closed.\n']",[],[],0,0
470,keras,7003,closed,Multi output network generator,"model.fit support multi input/output network, but if data-set is large enough and one have to use model.fit_generator, its complicated to generate tuple for such case, is there any plan to make it more simpler like model.fit.

I have network that take one input and produce two outputs,  i created  generator for each, but i am not able to run network on this. my input should be of form x, [y1, y2].

i think i need to extend generator for such case ?

  

Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",stale,"['![model](https://user-images.githubusercontent.com/26143955/27182056-4ae8ae22-51da-11e7-8965-c4be1b50be05.png)\r\nModel is like this ', 'My experience with using `fit_generator` on a multi-input model required a generator to return something like: `([x1, x2], y)`. \r\n\r\nMy intuition leads me to believe that for a multi-output model you need to yield `(x1, [y1, y2])`\r\n\r\nI would write a quick ""generator joiner"": (untested)\r\n\r\n```\r\ndef join_generators(generators):\r\n    while True: # keras requires all generators to be infinite\r\n        data = [next(g) for g in generators]\r\n\r\n        x = [d[0] for d in data ]\r\n        y = [d[1] for d in data ]\r\n\r\n        yield x, y\r\n```\r\n\r\n*Note the assumption here is that you need to ensure that the order in which your data flows from the directory is the same for each generator*', 'Thanks \r\nSorry if question sound silly \r\nI am not able to understand input for this join_generator ? its single zipped three generators ??\r\nand with that code y ll still lead to one input, should i write separate y1 and y2 and `yield x, [y1,y2]`', 'yes i am using same seed for all generators, and i tested that as well.', ""> Sorry if question sound silly\r\n> I am not able to understand input for this join_generator ? its single zipped three generators ??\r\n\r\nDon't worry, I didn't include a doc string. \r\nIt's just a list of the generators. Each generator must return `(x, y)`.\r\nTherefore if you have:\r\n```\r\ng1 --> (x1, y1)\r\ng2 --> (x2, y2)\r\njoin_generators(g1, g2) --> ([x1, x2], [y1, y2])\r\n```\r\n\r\n> and with that code y ll still lead to one input, should i write separate y1 and y2 and yield x, [y1,y2]\r\n\r\nThis was just a skeleton of a possible implementation. For example, if your generators only return one element you could just:\r\n\r\n```\r\ndef join_generators(xgenerators, ygenerator):\r\n    while True: # keras requires all generators to be infinite\r\n        data = [next(g) for g in xgenerators]\r\n\r\n        yield x, next(ygenerator)\r\n```\r\n\r\nOr vary the code accordingly."", 'I like that trick. In my experience, writing a custom generator that yields both inputs is easier to manage than zipping two generators. It is especially evident when managing complex augmentation and sampling methods.', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', '@zafarali Thanks for that trick, this really helps me a lot a lot !\r\n\r\n@0x00b1 \r\n> Writing a custom generator that yields inputs in the right way for multiple-input or multiple-output model, is really easier to manage than zipping two generators\r\n\r\nespecially we can handle a dict or a list for return, which the fit_generator allows the x of (x, y, sample_weight) to be a list or dict (see #2568) \r\n\r\nHope this trick can show up in the keras documents\r\n@fchollet ', '@zafarali  How do we handle shuffling the dataset if we use ""flow_from_directory""?', 'I am not sure, I haven’t used the more advanced features of keras in a\nwhile and this function is unfamiliar to me.\n\nHowever, If it returns a generator, it should work.\n\nOn Wed, Jan 30, 2019 at 9:23 AM moondra2017 <notifications@github.com>\nwrote:\n\n> @zafarali <https://github.com/zafarali> How do we handle shuffling the\n> dataset if we use ""flow_from_directory""?\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/keras-team/keras/issues/7003#issuecomment-458961418>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AGAO_Mr9JQe89B7WP405nDMyh1rA73Auks5vIar4gaJpZM4N7KwF>\n> .\n>\n', '@moondra2017 in my opinion seed parameter control this. It use same randomness. try on multiple generator with Shuffule=True', ""@hassaan90 might you have a github link where I could find your full model? i think, it would be a nice reference to see how to build/train/test multi-output models in Keras. My current model is unfortunately not converging when I add the second output branch and I'm not so sure, what I'm doing wrong."", ""@tinalegre  and @hassaan90 same here I have the same problem, trying to train multi- output U-net and the code doesn't work, any progress did you resolve the problem?""]","[""\r\n  image_datagen = ImageDataGenerator(\r\n            rotation_range=15.,\r\n            width_shift_range=0.2,\r\n            height_shift_range=0.2,\r\n            rescale=1./255,\r\n            shear_range=0.2,\r\n            zoom_range=0.2,\r\n            horizontal_flip=True,\r\n            vertical_flip = True,\r\n            fill_mode='nearest')\r\n    \r\n    mask_datagen = ImageDataGenerator(\r\n            rotation_range=15.,\r\n            width_shift_range=0.2,\r\n            height_shift_range=0.2,\r\n            rescale=1./255,\r\n            shear_range=0.2,\r\n            zoom_range=0.2,\r\n            horizontal_flip=True,\r\n            vertical_flip = True,\r\n            fill_mode='nearest')\r\n    shading_datagen = ImageDataGenerator(\r\n            rotation_range=15.,\r\n            width_shift_range=0.2,\r\n            height_shift_range=0.2,\r\n            rescale=1./255,\r\n            shear_range=0.2,\r\n            zoom_range=0.2,\r\n            horizontal_flip=True,\r\n            vertical_flip = True,\r\n            fill_mode='nearest')\r\n    test_datagen = ImageDataGenerator(rescale=1./255)\r\n    seed_validation = 1\r\n    validation_image_generator = test_datagen.flow_from_directory(\r\n            os.path.join(gen_path,'clean/test'),\r\n            target_size=(128, 256),class_mode=None,classes=None,\r\n            batch_size=20,seed=seed_validation)\r\n    validation_mask_generator = test_datagen.flow_from_directory(\r\n            os.path.join(gen_path,'albedo/test'),\r\n            target_size=(128, 256),class_mode=None,classes=None,\r\n            batch_size=20,seed=seed_validation)\r\n    validation_shading_generator = test_datagen.flow_from_directory(\r\n            os.path.join(gen_path,'gray_shading/test'),\r\n            target_size=(128, 256),class_mode=None,classes=None,\r\n            batch_size=20,seed=seed_validation)\r\n    #testDataGen = ImageDataGenerator(rescale=1./255)\r\n    seed = 5\r\n    #image_datagen.fit(image_datagen, augment=True, seed=seed)\r\n    #mask_datagen.fit(mask_datagen, augment=True, seed=seed)\r\n    \r\n    image_generator = image_datagen.flow_from_directory(\r\n        os.path.join(gen_path,'clean/train'),\r\n        class_mode=None,target_size=(128,256),batch_size=20,classes=None,\r\n        seed=seed)\r\n    \r\n    mask_generator = mask_datagen.flow_from_directory(\r\n        os.path.join(gen_path,'albedo/train'),\r\n        class_mode=None,target_size=(128,256),batch_size=20,classes = None,\r\n        seed=seed)\r\n    shading_generator = shading_datagen.flow_from_directory(\r\n        os.path.join(gen_path,'gray_shading/train'),\r\n        class_mode=None,target_size=(128,256),batch_size=20,classes = None,\r\n        seed=seed)\r\n    \r\n    train_generator = itertools.izip(image_generator, mask_generator, shading_generator)\r\n""]",['separate'],0,0
471,keras,12559,closed,"model = Sequential() model.add(Embedding(10000, embedding_size, input_length=max_words)) model.add(Conv1D(64, 3, padding='same')) model.add(Conv1D(32, 3, padding='same')) model.add(Flatten()) model.add(Dropout(0.2)) model.add(Dense(512,activation='relu')) model.add(Dropout(0.2)) model.add(Dense(1,activation='softmax’))","Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",type:support,"['@pvlakshmi Is there any issue here? If there is any issue, please provide more details about the issue, fill [template](https://github.com/keras-team/keras/issues/new/choose) and provide trace of the bug. Please let us know what is the backend you are using here. Thanks!', '@pvlakshmi I noticed you had opened another issue. I am closing this as it is a duplicate issue. Thanks!']",[],[],0,0
472,keras,6061,closed,"Big Error , why i am getting this ?","Why i am getting this error ?


===============================                                                                                                                                                            
Problem occurred during compilation with the command line below:                                                                                                                           
""C:\Users\om\Anaconda3\envs\tensorflow-gpu\Library\mingw-w64\bin\g++.exe"" -shared -g -march=broadwell -mmmx -mno-3dnow -msse -msse2 -msse3 -mssse3 -mno-sse4a -mcx16 -msahf -mmovbe -maes -
mno-sha -mpclmul -mpopcnt -mabm -mno-lwp -mfma -mno-fma4 -mno-xop -mbmi -mbmi2 -mno-tbm -mavx -mavx2 -msse4.2 -msse4.1 -mlzcnt -mrtm -mhle -mrdrnd -mf16c -mfsgsbase -mrdseed -mprfchw -mad
x -mfxsr -mxsave -mxsaveopt -mno-avx512f -mno-avx512er -mno-avx512cd -mno-avx512pf -mno-prefetchwt1 -mclflushopt -mxsavec -mxsaves -mno-avx512dq -mno-avx512bw -mno-avx512vl -mno-avx512ifm
a -mno-avx512vbmi -mno-clwb -mno-pcommit -mno-mwaitx --param l1-cache-size=32 --param l1-cache-line-size=64 --param l2-cache-size=6144 -mtune=generic -DNPY_NO_DEPRECATED_API=NPY_1_7_API_V
ERSION -m64 -DMS_WIN64 -I""C:\Users\om\Anaconda3\envs\tensorflow-gpu\lib\site-packages\numpy\core\include"" -I""C:\Users\om\Anaconda3\envs\tensorflow-gpu\include"" -I""C:\Users\om\Anaconda3\en
vs\tensorflow-gpu\lib\site-packages\theano\gof"" -L""C:\Users\om\Anaconda3\envs\tensorflow-gpu\libs"" -L""C:\Users\om\Anaconda3\envs\tensorflow-gpu"" -o C:\Users\om\AppData\Local\Theano\compil
edir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64\lazylinker_ext\lazylinker_ext.pyd C:\Users\om\AppData\Local\Theano\compiledir_Windows-10-10.0.143
93-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64\lazylinker_ext\mod.cpp -lpython35                                                                                        
C:\Users\om\AppData\Local\Temp\ccUzk0H7.o: In function __imp_PyExc_ImportError'                             
C:/Users/om/Anaconda3/envs/tensorflow-gpu/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1461: undefined reference to __imp_PyCapsule_Type'                                
C:/Users/om/Anaconda3/envs/tensorflow-gpu/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1467: undefined reference to __imp_PyExc_RuntimeError'                            
C:/Users/om/Anaconda3/envs/tensorflow-gpu/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1490: undefined reference to __imp_PyExc_RuntimeError'                            
C:/Users/om/Anaconda3/envs/tensorflow-gpu/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1506: undefined reference to __imp
_PyExc_RuntimeError' follow                                                                                                                                                                
C:\Users\om\AppData\Local\Temp\ccUzk0H7.o: In function __imp_PyCapsule_Type'                                     
C:\Users\om\AppData\Local\Temp\ccUzk0H7.o: In function __imp_P
yExc_TypeError'                                                                                                                                                                            
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:58: undefined reference to CLazyLinker_init':                                                                                                                 
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:352: undefined reference to __imp_
PyExc_IndexError'                                                                                                                                                                          
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:385: undefined reference to __imp_
PyExc_IndexError'                                                                                                                                                                          
C:\Users\om\AppData\Local\Temp\ccUzk0H7.o:C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/m
od.cpp:393: more undefined references to CLazyLinker_init':                                                                                                                 
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:405: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:426: undefined reference to __imp_
PyExc_TypeError'                                                                                                                                                                           
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:444: undefined reference to c_call':                                                                                                                           
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:545: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:545: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:\Users\om\AppData\Local\Temp\ccUzk0H7.o:C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/m
od.cpp:546: more undefined references to lazy_rec_eval':                                                                                                                    
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:618: undefined reference to __imp_
PyExc_TypeError'                                                                                                                                                                           
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:649: undefined reference to __imp_
PyExc_IndexError'                                                                                                                                                                          
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:708: undefined reference to __imp_
PyExc_TypeError'                                                                                                                                                                           
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:721: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:771: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:\Users\om\AppData\Local\Temp\ccUzk0H7.o: In function __imp_
PyExc_RuntimeError'                                                                                                                                                                        
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:826: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:839: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:849: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:\Users\om\AppData\Local\Temp\ccUzk0H7.o:C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/m
od.cpp:850: more undefined references to CLazyLinker_call':                                                                                                                 
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:894: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:937: undefined reference to __imp_
_Py_NoneStruct'                                                                                                                                                                            
C:\Users\om\AppData\Local\Temp\ccUzk0H7.o: In function __imp_
PyBool_Type'                                                                                                                                                                               
C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:976: undefined reference to __i
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:48: undefined reference to __imp
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:352: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:385: undefined reference to __im
. C:\Users\om\AppData\Local\Temp\ccUzk0H7.o:C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:405: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:426: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:444: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:545: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:546: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:641: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:657: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:715: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:771: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:772: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:826: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:839: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:849: undefined reference to __im
. C:\Users\om\AppData\Local\Temp\ccUzk0H7.o:C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:894: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:937: undefined reference to __im
. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:973: undefined reference to __im
. collect2.exe: error: ld returned 1 exit status                                                                                                                                           ",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],"[""_import_array':                                                                                                                    \r\nC:/Users/om/Anaconda3/envs/tensorflow-gpu/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1455: undefined reference to "", ""__imp_PyExc_AttributeError'                          \r\nC:/Users/om/Anaconda3/envs/tensorflow-gpu/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1466: undefined reference to "", ""__imp_PyExc_RuntimeError'                            \r\nC:/Users/om/Anaconda3/envs/tensorflow-gpu/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1482: undefined reference to "", ""__imp_PyExc_RuntimeError'                            \r\nC:/Users/om/Anaconda3/envs/tensorflow-gpu/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1496: undefined reference to "", ""__imp_PyExc_RuntimeError'                            \r\nC:\\Users\\om\\AppData\\Local\\Temp\\ccUzk0H7.o:C:/Users/om/Anaconda3/envs/tensorflow-gpu/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1518: more undefined references to "", ""NpyCapsule_Check':                                                                                                                 \r\nC:/Users/om/Anaconda3/envs/tensorflow-gpu/lib/site-packages/numpy/core/include/numpy/npy_3kcompat.h:462: undefined reference to "", ""unpack_list_of_ssize_t':                                                                                                           \r\nC:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:48: undefined reference to "", ""__imp_P\r\nyExc_IndexError'                                                                                                                                                                           \r\nC:\\Users\\om\\AppData\\Local\\Temp\\ccUzk0H7.o: In function "", ""__imp_\r\nPyExc_IndexError'                                                                                                                                                                          \r\nC:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:370: undefined reference to "", ""__imp_\r\nPyExc_IndexError'                                                                                                                                                                          \r\nC:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:389: undefined reference to "", ""__imp_PyExc_IndexError' follow                                                                                                                   \r\nC:\\Users\\om\\AppData\\Local\\Temp\\ccUzk0H7.o: In function "", ""__imp_\r\nPyExc_TypeError'                                                                                                                                                                           \r\nC:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:420: undefined reference to "", ""__imp_\r\nPyExc_IndexError'                                                                                                                                                                          \r\nC:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:440: undefined reference to "", ""__imp_\r\n_Py_NoneStruct'                                                                                                                                                                            \r\nC:\\Users\\om\\AppData\\Local\\Temp\\ccUzk0H7.o: In function "", ""__imp_\r\n_Py_NoneStruct'                                                                                                                                                                            \r\nC:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:545: undefined reference to "", ""__imp_\r\n_Py_NoneStruct'                                                                                                                                                                            \r\nC:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:546: undefined reference to "", ""__imp__Py_NoneStruct' follow                                                                                                                     \r\nC:\\Users\\om\\AppData\\Local\\Temp\\ccUzk0H7.o: In function "", ""__imp_\r\nPyExc_IndexError'                                                                                                                                                                          \r\nC:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:641: undefined reference to "", ""__imp_\r\nPyExc_ValueError'                                                                                                                                                                          \r\nC:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:657: undefined reference to "", ""__imp_\r\n_Py_NoneStruct'                                                                                                                                                                            \r\nC:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:715: undefined reference to "", ""__imp_\r\nPyExc_TypeError'                                                                                                                                                                           \r\nC:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:771: undefined reference to "", ""__imp_\r\n_Py_NoneStruct'                                                                                                                                                                            \r\nC:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:772: undefined reference to "", ""CLazyLinker_call':                                                                                                                 \r\nC:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:814: undefined reference to "", ""__imp_\r\nPyExc_RuntimeError'                                                                                                                                                                        \r\nC:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:839: undefined reference to "", ""__imp_\r\n_Py_NoneStruct'                                                                                                                                                                            \r\nC:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:840: undefined reference to "", ""__imp_\r\n_Py_NoneStruct'                                                                                                                                                                            \r\nC:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:849: undefined reference to "", ""__imp__Py_NoneStruct' follow                                                                                                                     \r\nC:\\Users\\om\\AppData\\Local\\Temp\\ccUzk0H7.o: In function "", ""__imp_\r\nPyExc_AssertionError'                                                                                                                                                                      \r\nC:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:937: undefined reference to "", ""__imp_\r\n_Py_NoneStruct'                                                                                                                                                                            \r\nC:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:938: undefined reference to "", ""CLazyLinker_set_allow_gc':                                                                                                         \r\nC:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:973: undefined reference to "", '__imp_\r\n_Py_TrueStruct\'                                                                                                                                                                            \r\ncollect2.exe: error: ld returned 1 exit status                                                                                                                                             \r\n                                                                                                                                                                                           \r\nTraceback (most recent call last):                                                                                                                                                         \r\n  File ""C:\\Users\\om\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\theano\\gof\\lazylinker_c.py"", line 75, in <module>                                                                      \r\n    raise ImportError()                                                                                                                                                                    \r\nImportError                                                                                                                                                                                \r\n                                                                                                                                                                                           \r\nDuring handling of the above exception, another exception occurred:                                                                                                                        \r\n                                                                                                                                                                                           \r\nTraceback (most recent call last):                                                                                                                                                         \r\n  File ""C:\\Users\\om\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\theano\\gof\\lazylinker_c.py"", line 92, in <module>                                                                      \r\n    raise ImportError()                                                                                                                                                                    \r\nImportError                                                                                                                                                                                \r\n                                                                                                                                                                                           \r\nDuring handling of the above exception, another exception occurred:                                                                                                                        \r\n                                                                                                                                                                                           \r\nTraceback (most recent call last):                                                                                                                                                         \r\n  File ""First_Neural _Network_cha_7.py"", line 2, in <module>                                                                                                                               \r\n    from keras.models import Sequential                                                                                                                                                    \r\n  File ""C:\\Users\\om\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\__init__.py"", line 3, in <module>                                                                                \r\n    from . import activations                                                                                                                                                              \r\n  File ""C:\\Users\\om\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\activations.py"", line 3, in <module>                                                                             \r\n    from . import backend as K                                                                                                                                                             \r\n  File ""C:\\Users\\om\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\__init__.py"", line 61, in <module>                                                                       \r\n    from .theano_backend import *                                                                                                                                                          \r\n  File ""C:\\Users\\om\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\theano_backend.py"", line 3, in <module>                                                                  \r\n    import theano                                                                                                                                                                          \r\n  File ""C:\\Users\\om\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\theano\\__init__.py"", line 66, in <module>                                                                              \r\n    from theano.compile import (                                                                                                                                                           \r\n  File ""C:\\Users\\om\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\theano\\compile\\__init__.py"", line 10, in <module>                                                                      \r\n    from theano.compile.function_module import *                                                                                                                                           \r\n  File ""C:\\Users\\om\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\theano\\compile\\function_module.py"", line 21, in <module>                                                               \r\n    import theano.compile.mode                                                                                                                                                             \r\n  File ""C:\\Users\\om\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\theano\\compile\\mode.py"", line 10, in <module>                                                                          \r\n    import theano.gof.vm                                                                                                                                                                   \r\n  File ""C:\\Users\\om\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\theano\\gof\\vm.py"", line 662, in <module>                                                                               \r\n    from . import lazylinker_c                                                                                                                                                             \r\n  File ""C:\\Users\\om\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\theano\\gof\\lazylinker_c.py"", line 127, in <module>                                                                     \r\n    preargs=args)                                                                                                                                                                          \r\n  File ""C:\\Users\\om\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\theano\\gof\\cmodule.py"", line 2316, in compile_str                                                                      \r\n    (status, compile_stderr.replace(\'\\n\', \'. \')))                                                                                                                                          \r\n. C:\\Users\\om\\AppData\\Local\\Temp\\ccUzk0H7.o:C:/Users/om/Anaconda3/envs/tensorflow-gpu/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1518: more undefined references to ', '__imp\r\n. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:58: undefined reference to ', '__im\r\n. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:370: undefined reference to ', '__im\r\n. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:389: undefined reference to ', '__im\r\n. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:420: undefined reference to ', '__im\r\n. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:440: undefined reference to ', '__im\r\n. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:545: undefined reference to ', '__im\r\n. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:545: undefined reference to ', '__im\r\n. C:\\Users\\om\\AppData\\Local\\Temp\\ccUzk0H7.o:C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext\r\n. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:618: undefined reference to ', '__im\r\n. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:649: undefined reference to ', '__im\r\n. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:708: undefined reference to ', '__im\r\n. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:721: undefined reference to ', '__im\r\n. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:771: undefined reference to ', '__im\r\n. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:814: undefined reference to ', '__im\r\n. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:839: undefined reference to ', '__im\r\n. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:840: undefined reference to ', '__im\r\n. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:849: undefined reference to ', '__im\r\n. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:937: undefined reference to ', '__im\r\n. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:938: undefined reference to ', '__im\r\n. C:/Users/om/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.5.2-64/lazylinker_ext/mod.cpp:976: undefined reference to ']",0,0
473,keras,10353,closed,callbacks,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",,"['Sorry, mistakenly created an issue.  ']",[],[],0,0
474,keras,3037,closed,Implement custom layer with multiple inputs which is input layer and has trainable weights,"I'm implementing a custom layer. When I try to call it using the following code it gives me an error. Is this the right way to implement a layer with multiple inputs? Also the code in the call function of the layer is using some theano.Tensor functions instead of the functions given in keras backend. Does this mean I won't be able to use this with a functional api anymore?



Traceback (most recent call last):
  File ""/Users/Aditya/Documents/Qbit Logic/TBCNN/Tree.py"", line 338, in <module>
    fitlog = model.fit([trees, connections, n_leaves_mat], y, batch_size=1, nb_epoch=50, verbose=1)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/models.py"", line 409, in fit
    sample_weight=sample_weight)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/engine/training.py"", line 1037, in fit
    self._make_train_function()
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/engine/training.py"", line 663, in _make_train_function
    training_updates = self.optimizer.get_updates(trainable_weights, self.constraints, self.total_loss)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/optimizers.py"", line 321, in get_updates
    grads = self.get_gradients(loss, params)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/optimizers.py"", line 53, in get_gradients
    grads = K.gradients(loss, params)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/backend/theano_backend.py"", line 532, in gradients
    return T.grad(loss, variables)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/theano/gradient.py"", line 545, in grad
    handle_disconnected(elem)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/theano/gradient.py"", line 532, in handle_disconnected
    raise DisconnectedInputError(message)
theano.gradient.DisconnectedInputError: grad method was asked to compute the gradient with respect to a variable that is not part of the computational graph of the cost, or is used only by a non-differentiable operator: <TensorType(float32, matrix)>
Backtrace when the node is created:
  File ""/Users/Aditya/Documents/Qbit Logic/TBCNN/Tree.py"", line 336, in <module>
    model.compile('adam', 'mse')
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/models.py"", line 339, in compile
    **kwargs)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/engine/training.py"", line 510, in compile
    masks = self.compute_mask(self.inputs, mask=None)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/engine/topology.py"", line 1914, in compute_mask
    output_tensors, output_masks, output_shapes = self.run_internal_graph(inputs, masks)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/engine/topology.py"", line 2049, in run_internal_graph
    output_tensors = to_list(layer.call(computed_tensors, computed_masks))
  File ""/Users/Aditya/Documents/Qbit Logic/TBCNN/Tree.py"", line 178, in call
    self.build([x.shape for x in inputs])
  File ""/Users/Aditya/Documents/Qbit Logic/TBCNN/Tree.py"", line 168, in build
    self.VCi, self.Wl, self.Wr = K.variable(vci), K.variable(wl), K.variable(wr)
  File ""/Users/Aditya/anaconda/envs/acads/lib/python3.5/site-packages/keras/backend/theano_backend.py"", line 31, in variable
    return theano.shared(value=value, name=name, strict=False)



Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",stale,"['@AdityaGudimella\n I implemented something very similar which takes two inputs and has trainable weight parameters. You can find it at https://github.com/dapurv5/keras-neural-tensor-layer\n', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs, but feel free to re-open it if needed.\n']","[""\nmodel = Sequential()\nlayer1 = InputLayer((17, 33))\nlayer2 = InputLayer((17, 33, 2, 11))\nlayer3 = InputLayer((17, 33))\nmodel.add(Merge([layer1, layer2, layer3], 30, len(node_indices), 1))\nmodel.compile('adam', 'mse')\n"", ""\nclass Merge(Layer):\n    def __init__(self, layers, n_feat, vocab_len, delta, output_shape=None,\n                 node_indices=None, tensor_indices=None, name=None, **kwargs):\n        self.layers = layers\n        self._output_shape = output_shape\n        self.node_indices = node_indices\n\n        self.n_feat = n_feat\n        self.vocab_len = vocab_len\n        self.delta = delta\n\n        # layer parameters\n        self.inbound_nodes = []\n        self.outbound_nodes = []\n        self.constraints = {}\n        self.regularizers = []\n        self.non_trainable_weights = []\n        self.supports_masking = False\n        self.uses_learning_phase = False\n        self.input_spec = None  # compatible with whatever\n        if not name:\n            prefix = self.__class__.__name__.lower()\n            name = prefix + '_' + str(K.get_uid(prefix))\n        self.name = name\n\n        if layers:\n            # this exists for backwards compatibility.\n            # equivalent to:\n            # merge = Merge(layers=None)\n            # output = merge([input_tensor_1, input_tensor_2])\n            self.built = True\n            if not node_indices:\n                # by default we connect to\n                # the 1st output stream in the input layer\n                node_indices = [0 for _ in range(len(layers))]\n            self.add_inbound_node(layers, node_indices, tensor_indices)\n\n    def build(self, input_shape):\n        wr = np.random.random((self.n_feat, self.n_feat))\n        wl = np.random.random((self.n_feat, self.n_feat))\n        vci = np.random.random((1 + self.vocab_len, self.n_feat))\n        vci[0] = 0\n        b = np.random.random(vci.shape[0])\n\n        self.VCi, self.Wl, self.Wr = K.variable(vci), K.variable(wl), K.variable(wr)\n        self.trainable_weights = [self.VCi, self.Wl, self.Wr]\n        self.built = True\n\n    def call(self, inputs, mask=None):\n        # Todo: Currently this only supports batch size of 1. That is the current code will work correctly only if\n        # batch size = 1. Modify this implementation to work for batch_size = n\n        if type(inputs) is not list or len(inputs) != 3:\n            raise Exception('Munge must be called on a list of 3 tensors.'\n                            ' Got: ' + str(inputs))\n        self.build([x.shape for x in inputs])\n        import theano.tensor as T\n        tree, connection, leaves = inputs\n        Ci_init = T.zeros((tree.shape[0], tree.shape[1], tree.shape[2], self.n_feat))\n        # Nodes only correspond to nonzero values\n        node_indices = tree.nonzero()\n        # Ci is a dummy tensor created to align with shape of tree. Actual representations are in VCi. Set those\n        # representations to corresponding nodes positions in Ci. So if node p is present in position [0, i, j] in tree,\n        # it's vector representation is present at VCi[p] and Ci[0, i, j] is set to VCi[p]\n        Ci = T.set_subtensor(Ci_init[node_indices], self.VCi[tree[node_indices].astype('int32')])\n        # Get representations (Ci) corresponding to nodes in tree\n        Ci_subset = Ci[node_indices]  # shape is (num_nodes, n_feat)\n        # li_init and ri_init are tensors from which the multipliers Wl, and Wr, i.e. li and lr will be made. The\n        # formulae for li and lr as given in the paper are li = (n - i)/(n - 1) and lr = (i - 1)/(n - 1). See the paper\n        # for details on n and i.\n        li_init, ri_init = T.ones_like(tree), T.ones_like(tree)\n        # Non zero elements in tree represent nodes. Get children of those nodes\n        children_indices = connection[node_indices]  # shape is (2, max_num_children * num_nodes)\n        # Find number of non zero children and store that in n\n        n = (~T.isclose(children_indices, 0.0)).sum(axis=-1)[:, 0]\n        # if num children is 1 then change corresponding n to 2\n        ni = T.switch(T.isclose(n, 1.0), 2.0, n)\n        # Generate a matrix like [[1,2,3,...n],[1,2,3,...n],...[1,2,3,...n]] of shape (children_indices.shape[0], n)\n        # where n is the max number of children a node could have.\n        posns = T.tile(T.arange(children_indices.shape[-1]) + 1,\n                       children_indices.shape[0]).reshape((children_indices.shape[0], -1))\n        i = T.switch(T.gt(posns, ni[:, None]), 0, posns)\n        l_mult_init = (ni[:, None] - i) / (ni[:, None] - 1)\n        r_mult_init = (i - 1) / (ni[:, None] - 1)\n        # The paper says to use li, ri = 0.5 when n = 2\n        l_mult_upd = T.switch(T.eq(ni.reshape((-1, 1)), 1), 0.5, l_mult_init)\n        r_mult_upd = T.switch(T.eq(ni.reshape((-1, 1)), 1), 0.5, r_mult_init)\n        zeroed_children_ind = T.isclose(children_indices.sum(1), 0.0)\n        # Shape of l_mult, r_mult is (children_indices.shape[0], n)\n        l_mult, r_mult = T.switch(zeroed_children_ind, 0, l_mult_upd), T.switch(zeroed_children_ind, 0, r_mult_upd)\n        stacked_indices = children_indices.dimshuffle(1, 0, 2).reshape((2, -1)).astype('int32')\n        stacked_rows, stacked_columns = stacked_indices[0], stacked_indices[1]\n\n        # Shape of li, ri is tree.shape\n        li = T.set_subtensor(li_init[0, stacked_rows, stacked_columns], l_mult.ravel())\n        ri = T.set_subtensor(ri_init[0, stacked_rows, stacked_columns], r_mult.ravel())\n        Wi = li[:, :, :, None, None] * self.Wl[None, None, None, :, :] + \\\n             ri[:, :, :, None, None] * self.Wr[None, None, None, :, :]\n        Wi_subset = Wi[0, stacked_rows, stacked_columns]\n        # product_init is used to create product which represents Wi*vec(ci) in the paper for all parents and all\n        # children. product_init is basically doing the following code:\n        # result = np.empty((n, n_feat, 1))   # n is number of nodes in tree\n        # for i in range(n):\n        #   result[i] = np.dot(Wi_subset[i],Ci[i])\n        # product_init = result\n        product_init = T.tensordot(Wi_subset, Ci[0, stacked_rows, stacked_columns],\n                                   axes=[2, 1])[T.arange(Wi_subset.shape[0]), :, T.arange(Wi_subset.shape[0])]\n        product = product_init.reshape((children_indices.shape[0], -1, product_init.shape[-1]))\n        nli_subset = connection[0, stacked_rows, stacked_columns].reshape((children_indices.shape[0], -1, 1))\n        # bias = self.bias[]\n        activation = T.tanh((nli_subset * product).sum(axis=1))  # Change to product.sum(1) + b\n        di = (Ci_subset - activation).norm(2, axis=-1) ** 2\n\n        # Generate negative example and find distance for that\n        Cwi = T.zeros_like(Ci)\n        Cwi = T.set_subtensor(Cwi[node_indices], Ci_subset[::-1])\n        product_ne_init = T.tensordot(Wi_subset, Cwi[0, stacked_rows, stacked_columns],\n                                      axes=[2, 1])[T.arange(Wi_subset.shape[0]), :, T.arange(Wi_subset.shape[0])]\n        product_ne = product_ne_init.reshape((children_indices.shape[0], -1, product_ne_init.shape[-1]))\n        activation_ne = T.tanh((nli_subset * product_ne).sum(axis=1))  # Change to product.sum(1) + b\n        dci = (Cwi[node_indices] - activation_ne).norm(2, axis=-1) ** 2\n\n        J_init = self.delta + di - dci\n        J = T.nnet.relu(J_init).sum()\n        # self.non_trainable_weights.extend([Cwi, li, ri])\n        return J\n\n    def __call__(self, inputs, mask=None):\n        '''We disable successive calls to __call__ for Merge layers.\n        Although there is no technical obstacle to\n        making it possible to __call__ a Merge intance many times\n        (it is just a layer), it would make for a rather unelegant API.\n        '''\n        if type(inputs) is not list:\n            raise Exception('Merge can only be called on a list of tensors, '\n                            'not a single tensor. Received: ' + str(inputs))\n\n        # self.build([x.shape for x in inputs])\n\n        all_keras_tensors = True\n        for x in inputs:\n            if not hasattr(x, '_keras_history'):\n                all_keras_tensors = False\n                break\n\n        if all_keras_tensors:\n            layers = []\n            node_indices = []\n            tensor_indices = []\n            for x in inputs:\n                layer, node_index, tensor_index = x._keras_history\n                layers.append(layer)\n                node_indices.append(node_index)\n                tensor_indices.append(tensor_index)\n\n            self.built = True\n            self.add_inbound_node(layers, node_indices, tensor_indices)\n\n            outputs = self.inbound_nodes[-1].output_tensors\n            return outputs[0]  # merge only returns a single tensor\n        else:\n            return self.call(inputs, mask)\n\n    def get_output_shape_for(self, input_shape):\n        return tuple((1,))\n""]",[],0,0
475,keras,7424,closed,train_on_batch for multiple predictions,"Hi,

I'm not pointing a problem, I'm just wondering how I can manage to do what I want. I explain myself : 
For an image X, the Y that I want to predict is a couple of numpy array [Y1, Y2]. Y1 is (38, 60, 18) shaped and Y2 is (38, 60, 72) shaped. As I am working with a big dataset, I would like to use the train_on_batch function. However, this function should take a numpy array of X  values (which is not a problem) and a numpy array of Y values. Because Y1 and Y2 are not of the same shape, I can't stack them into a single array, and using a list raise an exception because train_on_batch calls the Y shape. Does anyone has an idea about how I can trick that ?

Thanks for reading",stale,"['If by ""I want to predict is a couple of numpy array [Y1, Y2]"", you mean that that you want to predict multiple outputs from the same model, you could opt for using functional api to build a model which allows a model to have multiple outputs and thus while using `train_on_batch`, you could supply the different outputs as a list and won\'t have to pass them in a single concatenated array.\r\nYou could have a look at [Guide to the Functional API](https://keras.io/getting-started/functional-api-guide/).', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],0,0
476,keras,4891,closed,Built-in leave-one-out validation in Keras,"Hello everyone!

Could anyone tell me, please, whether a built-in leave-one-out validation exists in Keras? I mean, let's assume I have only a training data set consisting of 100 records and want to build the model 100 times based on 99 records and validating it on the remaining one. Would like to avoid implementing this on my own with loops, if possible. :)
Of course, during this process I'd like to accumulate the error of predictions (basically, my target variable is continuous, so speeking here about a regression task) from model to model.

Thank you very much 4 your responses in advance!

Almost forgot: Happy New Year! :)",,"[""I think you could use the [sklearn wrappers](https://keras.io/scikit-learn-api/) and take advantage of sklearn's [awesome CV tools](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html)."", 'Thank you, Ohad, will take a look soon!', ""AFAIK, there aren't built-in features for cross-validation in Keras but there are other libraries that ohadle pointed out. Anyway, in order to avoid randomness I also managed to implement it with simple loop statements.""]",[],[],0,0
477,keras,9576,closed,docs correction: https://keras.io/activations/,"https://keras.io/activations/

It appears the last line is redundant here:

> 

Otherwise this contradicts to previous paragraphs in that page:

> 
> 
> This is equivalent to:
> ",,"['Thanks, this is fixed (will be updated on the website soon).']","['\r\n> from keras import backend as K\r\n> \r\n> model.add(Dense(64, activation=K.tanh))\r\n> model.add(Activation(K.tanh))\r\n> ', ""\r\n> from keras.layers import Activation, Dense\r\n> \r\n> model.add(Dense(64))\r\n> model.add(Activation('tanh'))\r\n> "", ""\r\n> model.add(Dense(64, activation='tanh'))\r\n> ""]",[],0,0
478,keras,4045,closed,Index value out of bound while training using embedding + stateful LSTM,"Hi, 
I am using my pretrained embedding layer on top of a stateful LSTM. My word2vec embedding is trained on a larger corpus(word2vec corpus) than the lstm training corpus(model corpus). i'm mapping word vectors to embedding weights using the  word2vec model. Truncated model corpus to be divided into uniform sequences
For preparing the labels of train data  i'm using the LSTM model vocab(not word2vec vocab) with one-hot encoding and no 0 masking.
While training the model, I'm getting the following error- 



Model-



Am I missing something? Thanks.
",,['Found the error. Embedding weights size should be equivalent to the largest index in the w2v_vocab+model_corpus_vocab. \n'],"["" python\nIndexError: One of the index value is out of bound. Error code: 65535.\\n\nApply node that caused the error: GpuAdvancedSubtensor1(embedding_1_W, Elemwise{Cast{int64}}.0)\nToposort index: 142\nInputs types: [CudaNdarrayType(float32, matrix), TensorType(int64, vector)]\nInputs shapes: [(12288, 280), (1024,)]\nInputs strides: [(280, 1), (8,)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[GpuReshape{2}(GpuAdvancedSubtensor1.0, TensorConstant{[ -1 280]})]]\n"", ' python\nbatch= 32\nseq_length= 32\nw2v_dim= 280\nCorpus size: 12288 and vocab size: 7625 #truncated model corpus for equal sequence length \nX_train.shape= (384, 32) #words mapped to indices using word2vec dict  \ny_train.shape= (384, 32, 7625)# words mapped to one hot using model corpus dict\nembedding.shape= (12288, 280)# embedding layer weights using word2vec model and word2vec corpus.\nmemory_units=512\n\nmodel= Sequential()\nmodel.add(Embedding(corpus_size, w2v_dim,batch_input_shape=(batch,seq_length), mask_zero=False, weights=[embedding], input_length=seq_length))\nmodel.add(LSTM(memory_units, return_sequences=True, stateful= True, init= ""orthogonal""))\nmodel.add(Dropout(0.5))\nmodel.add(LSTM(memory_units, return_sequences=True, stateful= True, init= ""orthogonal""))\nmodel.add(Dropout(0.5))\nmodel.add(TimeDistributed(Dense(vocab_size, activation=\'softmax\', init= ""orthogonal"")))\n']",[],0,0
479,keras,7539,closed,UnicodeDecode error in keras fit function with theano backend,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I have asked this question on stackoverflow, buy I haven't received any answers to it.

When I try to fit a model with the theano backend I run into the following error



Here's the full error stack


I am required to use the theano backend due to compatibility issues of the tensorflow backend with my university cluster. I have tested this code on CPU's on my personal device with the tensorflow backend and it runs error free.

Load Script

Train script

Packages
",,[],"['\r\nPython programming language version 3.3.2 loaded.\r\n\r\nUsing Theano backend.\r\n\r\nUsing gpu device 0: Tesla K20 (CNMeM is enabled with initial size: 98.0% of \r\nmemory, CuDNN not available)\r\n\r\nTraceback (most recent call last):\r\nFile ""cifar10/train.py"", line 62, in <module>\r\n\r\nmodel.fit(X_train,y_train,epochs=10,batch_size=32,validation_split =0.3,shuffle= True)\r\n\r\nFile ""/N/u/<User>/<University-cluster>/.local/lib/python3.3/site-\r\npackages/keras/engine/training.py"", line 1379, in fit \r\nself._make_test_function()\r\n\r\nFile ""/N/u/<User>/<University-cluster>/.local/lib/python3.3/site-\r\npackages/keras/engine/training.py"", line 959, in _make_test_function\r\n**self._function_kwargs)\r\n\r\nFile ""/N/u/<User>/<University-cluster>/.local/lib/python3.3/site-\r\npackages/keras/backend/theano_backend.py"", line 1206, in function\r\nreturn Function(inputs, outputs, updates=updates, **kwargs)\r\n\r\nFile ""/N/u/<User>/<University-cluster>/.local/lib/python3.3/site-\r\npackages/keras/backend/theano_backend.py"", line 1192, in __init__\r\n**kwargs)\r\n\r\nFile ""/N/u/<User>/<University-cluster>/.local/lib/python3.3/site-\r\npackages/theano/compile/function.py"", line 320, in function\r\noutput_keys=output_keys)\r\n\r\nFile ""/N/u/<User>/<University-cluster>/.local/lib/python3.3/site-\r\npackages/theano/compile/pfunc.py"", line 479, in pfunc\r\noutput_keys=output_keys)\r\n\r\nFile ""/N/u/<User>/<University-cluster>/.local/lib/python3.3/site-\r\npackages/theano/compile/function_module.py"", line 1777, in orig_function\r\ndefaults)\r\n\r\nFile ""/N/u/<User>/<University-cluster>/.local/lib/python3.3/site-\r\npackages/theano/compile/function_module.py"", line 1641, in create\r\ninput_storage=input_storage_lists, storage_map=storage_map)\r\n\r\nFile ""/N/u/<User>/<University-cluster>/.local/lib/python3.3/site-\r\npackages/theano/gof/link.py"", line 690, in make_thunk\r\nstorage_map=storage_map)[:3]\r\n\r\nFile ""/N/u/<User>/<University-cluster>/.local/lib/python3.3/site-\r\npackages/theano/gof/vm.py"", line 1003, in make_all\r\nno_recycling))\r\n\r\nFile ""/N/u/<User>/<University-cluster>/.local/lib/python3.3/site-\r\npackages/theano/sandbox/cuda/__init__.py"", line 256, in make_thunk\r\ncompute_map, no_recycling)\r\n\r\nFile ""/N/u/<User>/<University-cluster>/.local/lib/python3.3/site-\r\npackages/theano/gof/op.py"", line 970, in make_thunk\r\nno_recycling)\r\n\r\nFile ""/N/u/<User>/<University-cluster>/.local/lib/python3.3/site-\r\npackages/theano/gof/op.py"", line 879, in make_c_thunk\r\noutput_storage=node_output_storage)\r\n\r\nFile ""/N/u/<User>/<University-cluster>/.local/lib/python3.3/site-\r\npackages/theano/gof/cc.py"", line 1200, in make_thunk\r\nkeep_lock=keep_lock)\r\n\r\nFile ""/N/u/<User>/<University-cluster>/.local/lib/python3.3/site-\r\npackages/theano/gof/cc.py"", line 1143, in __compile__\r\nkeep_lock=keep_lock)\r\n\r\nFile ""/N/u/<User>/<University-cluster>/.local/lib/python3.3/site-\r\npackages/theano/gof/cc.py"", line 1595, in cthunk_factory\r\nkey=key, lnk=self, keep_lock=keep_lock)\r\n\r\nFile ""/N/u/<User>/<University-cluster>/.local/lib/python3.3/site-\r\npackages/theano/gof/cmodule.py"", line 1105, in module_from_key\r\nsrc_code = lnk.get_src_code()\r\n\r\nFile ""/N/u/<User>/<University-cluster>/.local/lib/python3.3/site-\r\npackages/theano/gof/cc.py"", line 1479, in get_src_code\r\nmod = self.get_dynamic_module()\r\n\r\nFile ""/N/u/<User>/<University-cluster>/.local/lib/python3.3/site-\r\npackages/theano/gof/cc.py"", line 1523, in get_dynamic_module\r\nself.code_gen()\r\n\r\nFile ""/N/u/<User>/<University-cluster>/.local/lib/python3.3/site-\r\npackages/theano/gof/cc.py"", line 779, in code_gen\r\nname))\r\n\r\nFile ""/N/u/<User>/<University-cluster>/.local/lib/python3.3/site-\r\npackages/theano/sandbox/cuda/blas.py"", line 937, in c_support_code_apply\r\nfor f in files]\r\n\r\nFile ""/N/u/<User>/<University-cluster>/.local/lib/python3.3/site-\r\npackages/theano/sandbox/cuda/blas.py"", line 937, in <listcomp>\r\nfor f in files]\r\n\r\nFile ""/N/soft/cle4/python/3.3.2a/lib/python3.3/encodings/ascii.py"", line 26, \r\nin decode return codecs.ascii_decode(input, self.errors)[0]\r\n\r\nUnicodeDecodeError: \'ascii\' codec can\'t decode byte 0xc3 in position 6923: \r\nordinal not in range(128)\r\n', ""\r\nimport numpy as np\r\nfrom keras.utils import to_categorical\r\n\r\nclass Load_():\r\n    \r\n    def load_data():\r\n        def unpickle(file):\r\n            import pickle\r\n            with open(file, 'rb') as fo:\r\n                dict = pickle.load(fo, encoding='latin1')\r\n            return dict\r\n\r\n        images = []\r\n        labels = []\r\n\r\n        folder = ['cifar10/data_batch_1','cifar10/data_batch_2','cifar10/data_batch_3','cifar10/data_batch_4','cifar10/data_batch_5']\r\n        for file in folder:\r\n            f = unpickle(file)\r\n            images.append(f['data'])\r\n            labels.append(f['labels'])\r\n\r\n        images = np.vstack(images)\r\n        labels = np.vstack(labels)\r\n\r\n        images = images.reshape([-1,3,32,32]).transpose([0, 2, 3, 1]).astype(np.float32)\r\n\r\n        labels = labels.reshape(50000,1)\r\n        images /= 255.0\r\n\r\n\r\n        #convert labels to one hot encoded labels\r\n        labels = to_categorical(labels)\r\n        \r\n        return(images,labels)\r\n"", '\r\nimport os\r\nos.environ[\'THEANO_FLAGS\'] = \'floatX=float32,device=gpu,lib.cnmem=1\'\r\nimport sys\r\nsys.path.insert(0,\'/N/u/<User>/<University-cluster>/.local/lib/python3.3/site-packages\')\r\nimport numpy as np\r\n\r\n#Keras Model\r\n\r\nfrom Load import Load_\r\nfrom keras.layers import Input,GlobalAveragePooling2D, Convolution2D, Dense, Dropout,BatchNormalization\r\nfrom keras import optimizers\r\nfrom keras.models import Model\r\n\r\ninp = Input(shape=[32,32,3])\r\n        \r\n#Block 1\r\nC = Convolution2D(32,(3,3),strides=(1, 1),activation = \'relu\',padding = \'same\',kernel_initializer=\'glorot_normal\')(inp)\r\nC = BatchNormalization()(C)\r\nC = Convolution2D(64,(3,3),strides=(1, 1),activation = \'relu\',padding = \'same\')(C)\r\nC = BatchNormalization()(C)\r\nC = Convolution2D(64,(3,3),strides=(1, 1),activation = \'relu\',padding = \'same\')(C)\r\nC = BatchNormalization()(C)\r\n        \r\n#Block 2\r\nC = Convolution2D(96,(3,3),strides=(2, 2),activation = \'relu\',padding = \'same\',kernel_initializer=\'glorot_normal\')(C)\r\nC = BatchNormalization()(C)\r\nC = Convolution2D(128,(3,3),strides=(1, 1),activation = \'relu\',padding = \'same\')(C)\r\nC = BatchNormalization()(C)\r\nC = Convolution2D(128,(3,3),strides=(1, 1),activation = \'relu\',padding = \'same\')(C)\r\nC = BatchNormalization()(C)\r\n\r\n#Block 3\r\nC = Convolution2D(128,(3,3),strides=(2, 2),activation = \'relu\',padding = \'same\',kernel_initializer=\'glorot_normal\')(C)\r\nC = BatchNormalization()(C)\r\nC = Convolution2D(256,(3,3),strides=(1, 1),activation = \'relu\',padding = \'same\')(C)\r\nC = BatchNormalization()(C)\r\nC = Convolution2D(256,(3,3),strides=(1, 1),activation = \'relu\',padding = \'same\')(C)\r\nC = BatchNormalization()(C)\r\n\r\nGL =  GlobalAveragePooling2D()(C)\r\nD2 =  Dense(128)(GL)\r\nDR2 = Dropout(0.3)(D2)\r\nD3 =  Dense(10,activation=""softmax"")(DR2)\r\n\r\nmodel = Model(inputs = inp, outputs = D3)\r\n\r\nX_train,y_train = Load_.load_data()\r\n\r\n\r\nsgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\r\n\r\nmodel.compile(loss=\'categorical_crossentropy\',\r\n      optimizer=sgd,\r\n      metrics=[\'accuracy\'])\r\n\r\nmodel.fit(X_train,y_train,epochs=10, \r\n  batch_size=32,\r\n  validation_split = 0.3,\r\n shuffle = True\r\n', '\r\nkeras-version: 2.0.6\r\ntheano-version: 0.8.0\r\nNumpy-version: 1.9.2\r\nCudatoolkit-version: 7.0\r\n']","[""return codecs.ascii_decode(input, self.errors)[0]\r\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 6923: ordinal not in range(128)""]",0,0
480,keras,6842,closed, 'KerasRegressor' object has no attribute 'to_json',"I'm unable to save the model trained using KerasRegressor wrapper...


Here's my import:





Any ideas?",,"['You can save the model using a ModelCheckpoint in your callbacks, and adding the callback list to the fit call.\r\n```\r\ncheckpoint = ModelCheckpoint(""./snapshots/trained_model.hdf5"", monitor=\'val_loss\', verbose=1, save_best_only=False)\r\ncallbacks_list = [checkpoint]\r\n```\r\nAnd then in your fit call\r\n`history = estimator.fit(train_images, train_labels, callbacks=callbacks_list)`', 'You need to call the *to_json* method on a *Model* instance:\r\n\r\n```\r\nmodel = KerasRegressor(build_fn=base_model, epochs=1, batch_size=10, verbose=1)\r\nmodel.fit(X,Y)\r\nmodel_json = model.model.to_json()\r\n```', 'Closing this issue since solution provided above works. Feel free to reopen if the issue still persists. Thanks!\r\n ']","['\r\n\tmodel = KerasRegressor(build_fn=base_model, epochs=1, batch_size=10, verbose=1)\r\n\tmodel.fit(X,Y)\r\n\r\n\tmodel_json = model.to_json()\r\n\twith open(""model.json"", ""w"") as json_file:\r\n\t\tjson_file.write(model_json)\r\n\t\tmodel.save_weights(""model.h5"", overwrite=True)\r\n\t\tlogger.info(""Saved model to disk"")\r\n', '\r\nfrom keras.wrappers.scikit_learn import KerasRegressor\r\nfrom keras.layers import Dense, Activation\r\nfrom keras.models import Sequential\r\nfrom keras.models import model_from_json\r\n\r\n # multi gpu\r\nimport tensorflow as tf\r\nfrom keras import backend as K\r\nfrom keras.models import Model\r\nfrom keras.layers import Input\r\nfrom keras.layers.core import Lambda\r\nfrom keras.layers.merge import Concatenate\r\n', '\r\nTraceback (most recent call last):\r\n  File ""train-model.py"", line 175, in <module>\r\n  File ""train-model.py"", line 114, in main\r\n    model.save_weights(""model.h5"", overwrite=True)\r\nAttributeError: \'KerasRegressor\' object has no attribute \'to_json\'\r\n\r\n']",[],0,0
481,keras,2607,closed,what metrics can be used in keras,"Most examples are use metrics=['accuracy'], but accuracy is not always suitable for every task. 
1. So are there any metrics such as precision, recall and so on?
2. If there are, what should I write in metrics list in order to use them?
3. If I just have one output, can I use multiple metrics to evaluate it from different aspect?
",,"['One of the doc pages says the accuracy is the only thing implemented right now. There really should be a tab for metrics that says that and can be expanded later. \n', ""I found funcitons name which like 'mae' or 'mean_absolute_error'  in keras.metrics can be used in metrics, just like the parameter loss. It seems like the metrics is just used for logging,  not joined in the training work. \nBy the way, the document really need to point that what the metrics support.\n"", ""Precision, Recall and F1-score were added by someone:\n\nhttps://github.com/fchollet/keras/blob/master/keras/metrics.py\n\nExample usage:\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer=adam,\n              metrics=['binary_accuracy', 'fmeasure', 'precision', 'recall'])\n"", 'After updating I still get this error:\nException: Invalid metric: precision\n', ""Hey Greg,\n\nAs of now, the latest Keras package doesn't contain this yet.\n\nYou can download the metrics code from GitHub, then copy it over your current one:\n\n```\nwget https://raw.githubusercontent.com/fchollet/keras/master/keras/metrics.py\nsudo cp metrics.py /usr/local/lib/python2.7/dist-packages/keras/\n```\n"", 'Thanks!  That worked great\n', 'I think the document is already updated? [https://keras.io/metrics/](https://keras.io/metrics/)', 'What is the difference between loss (objectives) and metrics?', '@wqp89324 \r\nA metric is a function that is used to judge the performance of your model. A metric function is similar to an loss function, except that the results from evaluating a metric are not used when training the model. You can find from this url: https://keras.io/metrics/', ""@wqp89324 Another way to put it, expanding on @jhli973's answer, is that the evaluation metric is what you as the _researcher_ will use to judge the model's performance (on training, test, and/or evaluation data); it's the bottom line number that you would publish. The loss function is what the _network_ will use to try to improve itself, hopefully in a way that leads to improved evaluation for the researcher's sake. For example, in a binary classification problem, the network might train using a binary crossentropy loss function with gradient descent, whereas the modeler's goal is to design a network to improve binary category accuracy on hold-out data."", ""It looks like many of the helpful metrics that used to be supported [have been removed](https://github.com/fchollet/keras/wiki/Keras-2.0-release-notes) with Keras 2.0. I'm working on a classification problem where f-score would be much more valuable to me than accuracy. Is there a way that I can use that as a metric, or am I encouraged to use `metrics.categorical_accuracy` instead? If so, why? And how does that differ from `metrics.sparse_categorical_accuracy`. Cheers!"", 'I resolved my problem by getting the old code from https://github.com/fchollet/keras/blob/53e541f7bf55de036f4f5641bd2947b96dd8c4c3/keras/metrics.py \r\n\r\nMaybe someone would put together a keras-contrib package.', 'I agree with @brannondorsey. According to @fchollet, he explained in #5794 that it was intentionally [removed](https://github.com/fchollet/keras/commit/a56b1a55182acf061b1eb2e2c86b48193a0e88f7)  in [version 2.0](https://github.com/fchollet/keras/wiki/Keras-2.0-release-notes) because it performs only approximation by batchwise evaluation. Unfortunately, there seems to be no evidence (#6002 #5705), that someone is working on a global measurement. \r\n\r\nProbably the best thing to do currently is to store the predictions and then use [Scikit](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) for calculating global measurements. For me the following worked out quite well on a classification task:\r\n\r\n1. Predict classes\r\n```\r\ntest_generator = ImageDataGenerator()\r\ntest_data_generator = test_generator.flow_from_directory(\r\n    ""test_directory"",\r\n    batch_size=32,\r\n    shuffle=False)\r\ntest_steps_per_epoch = numpy.math.ceil(test_data_generator.samples / test_data_generator.batch_size)\r\n\r\npredictions = model.predict_generator(test_data_generator, steps=test_steps_per_epoch)\r\n# Get most likely class\r\npredicted_classes = numpy.argmax(predictions, axis=1) \r\n```\r\n2. Get ground-truth classes and class-labels\r\n```\r\ntrue_classes = test_data_generator.classes\r\nclass_labels = list(test_data_generator.class_indices.keys())    \r\n```\r\n\r\n3. Use scikit-learn to get statistics\r\n```\r\nreport = metrics.classification_report(true_classes, predicted_classes, target_names=class_labels)\r\nprint(report)    \r\n```', '@apacha \r\nThanks for the detailed explanation. This is very helpful. I have a follow up question.\r\n\r\nWhile using ""predict_generator"", How to ensure that the prediction is done on all test samples once.\r\n\r\nFor example-\r\npredictions = model.predict_generator(\r\ntest_generator,\r\nsteps=int(test_generator.samples/float(batch_size)), # all samples once\r\nverbose = 1,\r\nworkers = 2,\r\nmax_q_size=10,\r\npickle_safe=True\r\n)\r\npredicted_classes = np.argmax(predictions, axis=1)\r\ntrue_classes = test_generator.classes\r\n\r\nSo the dimensions of predicted_classes and true_classes is different since total samples is not divisible by batch size.\r\n\r\nThe size of my test_set is not consistent, so the no. of steps in predict_generator would change each time depending upon the batch size. I am using flow_from_directory and cannot use predict_on_batch since my data is organized in a directory structure.\r\n\r\nOne solution is running with batch size of 1, but makes it very slow.\r\n\r\nI hope my question is clear. Thanks in advance.', ""@sxs4337 I am happy to tell you, that you don't have to worry about that, when using the ImageDataGenerator, as it automatically takes care of the last batch, if your samples are not divisible by the batch size. For example, if you have 10 samples and a minibatch-size of 4, `test_generator` will create batches of the following size: 4, 4, 2. Consecutive next()-calls will repeat the sequence from the beginning. \r\n\r\nBy using `test_steps_per_epoch = numpy.math.ceil(test_data_generator.samples / test_data_generator.batch_size)` you automatically will get 3 batches for the example from above, which will result in a total of 10 predictions."", '@apacha \r\nThank you for the reply. I did try that and it seems to miss out on the last few test samples. I may be missing something very obvious here.\r\n\r\nI have 505 test samples and tried running with a batch size of 4.\r\n\r\nBelow is my code snippet-\r\n\r\n    test_datagen = ImageDataGenerator(preprocessing_function=vgg_preprocess)\r\n    test_generator = test_datagen.flow_from_directory(\r\n        \'dataset_toy/test_toy\',\r\n        target_size=(img_rows, img_cols),\r\n        batch_size = 4,\r\n        shuffle=False,\r\n        class_mode=\'categorical\')\r\n    predictions = model.predict_generator(\r\n        test_generator,\r\n        steps = np.math.ceil(test_generator.samples / test_generator.batch_size),\r\n        verbose = 1,\r\n        workers = 2,\r\n        max_q_size=10,\r\n        pickle_safe=True\r\n        )\r\n    predicted_classes = np.argmax(predictions, axis=1)\r\n    true_classes = test_generator.classes\r\n    class_labels = list(test_generator.class_indices.keys())\r\n    report = metrics.classification_report(true_classes, predicted_classes, target_names=class_labels)\r\n    accuracy = metrics.accuracy_score(true_classes, predicted_classes)\r\n\r\nHere is the error-\r\n\r\nFound 505 images belonging to 10 classes.\r\n124/126 [============================>.] - ETA: 0sTraceback (most recent call last):\r\n  File ""keras_finetune_vgg16_landmarks10k.py"", line 201, in <module>\r\n    (report, accuracy) = test_mode(model_path)\r\n  File ""keras_finetune_vgg16_landmarks10k.py"", line 177, in test_mode\r\n    report = metrics.classification_report(true_classes, predicted_classes, target_names=class_labels)\r\n  File ""/usr/lib/python2.7/dist-packages/sklearn/metrics/classification.py"", line 1384, in classification_report\r\n    sample_weight=sample_weight)\r\n  File ""/usr/lib/python2.7/dist-packages/sklearn/metrics/classification.py"", line 956, in precision_recall_fscore_support\r\n    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\r\n  File ""/usr/lib/python2.7/dist-packages/sklearn/metrics/classification.py"", line 72, in _check_targets\r\n    check_consistent_length(y_true, y_pred)\r\n  File ""/usr/lib/python2.7/dist-packages/sklearn/utils/validation.py"", line 176, in check_consistent_length\r\n    ""%s"" % str(uniques))\r\nValueError: Found arrays with inconsistent numbers of samples: [504 505]\r\n\r\nSo the prediction has 504 values where as ground truth is 505 values.\r\n\r\nThanks again and I appreciate the help.\r\n', ""Maybe this is a bug, when having more than one worker? Try it with `workers=1` to see if the problem still remains. You can also check the `len(predicted_classes)` or run `test_generator.next()` a couple of times, to see what is reports.\r\n\r\nIf all that fails, I'm afraid that I can't help you. If you think this is a Keras bug, create a issue with detailed steps to reproduce the issue."", '@apacha \r\nIt has the same issue with workers=1. I put a debugger after model.predict_generator to check the shapes. prediction is getting just 504 samples out of 505 with batch size of 4.\r\n\r\nFound 505 images belonging to 10 classes.\r\n126/126 [==============================] - 34s\r\n> /home/shagan/maya/landmark/keras_finetune_vgg16_landmarks10k.py(170)test_mode()\r\n-> predicted_classes = np.argmax(predictions, axis=1)\r\n(Pdb) predictions.shape\r\n(504, 10)\r\n(Pdb) test_generator.classes.shape\r\n(505,)\r\n(Pdb)\r\n\r\nBTW, my keras version is 2.0.5\r\nThanks.\r\n\r\n', 'Well. Looks obvious to me now. See the number of steps? 126! 126x4=504. For\nsome reason the calculation of the number of steps seems to have an issue.\nIt should be 127, not 126\n\nOn 21 Jun 2017 6:19 pm, ""sxs4337"" <notifications@github.com> wrote:\n\n> @apacha <https://github.com/apacha>\n> It has the same issue with workers=1. I put a debugger after\n> model.predict_generator to check the shapes. prediction is getting just 504\n> samples out of 505 with batch size of 4.\n>\n> Found 505 images belonging to 10 classes.\n> 126/126 [==============================] - 34s\n>\n> /home/shagan/maya/landmark/keras_finetune_vgg16_\n> landmarks10k.py(170)test_mode()\n> -> predicted_classes = np.argmax(predictions, axis=1)\n> (Pdb) predictions.shape\n> (504, 10)\n> (Pdb) test_generator.classes.shape\n> (505,)\n> (Pdb)\n>\n> BTW, my keras version is 2.0.5\n> Thanks.\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/fchollet/keras/issues/2607#issuecomment-310130917>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAkSQQ3LUJ00Q2hruyxiXOIc3LkR2gQPks5sGUKVgaJpZM4IXJpS>\n> .\n>\n', 'Yes. That was the issue. Thanks a lot!\r\n', 'what are the available ""metrics"" if I\'m doing time series prediction(regression) in keras?', ""@NourozR am I correct in assuming that you are using a mean squared error loss function? If so popular metrics include mean absolute error (`mae`) and accuracy (`acc`). From the [metrics documentation page](https://keras.io/metrics/#custom-metrics):\r\n\r\n```python\r\nmodel.compile(loss='mean_squared_error',\r\n              optimizer='sgd',\r\n              metrics=['mae', 'acc'])\r\n```"", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', 'Why are mean absolute error (`mae`) and accuracy (`acc`) not listed in the available metrics section. Are there any other hidden metrics?', ""@damienrj, nothing is hidden is you look at the code: https://github.com/fchollet/keras/blob/master/keras/metrics.py\r\n\r\nIf you look deep enough, you'll see that many loss functions are added as metrics. Look then at the loss page: https://keras.io/losses/\r\n\r\n"", 'Is there is anyway to calculate precission@k and recall@k using the above-mentioned code ??? @mimoralea \r\n\r\n', '@apacha How can I extend your code to work for multiclass classification?\r\nMy result for the predictions i get are all 1 but I need a list like [0,0,0,1,0,0,0,0,0,0,0,0] since i have 12 classes. How can I get that?', ""As far as I know, scikit's [classification_report](http://scikit-learn.org/stable/modules/model_evaluation.html#classification-report) does support multiclass cases, but I am not sure if we are talking about the same thing. What exactly do you mean by multiclass classification: One object potentially belonging to multiple classes? Or just having 12 different classes in total? Maybe you need some one-hot encoding for the ground truth before computing the metrics. Apart from that, I'm afraid I can't help you unless you give more details, but I don't think this is the right place to answer such questions. Preferably, you should ask such questions on [Stackoverflow](http://stackoverflow.com)."", '@NourozR \r\nKeras metrics for regression are: r_square (R^2), mean absolute error (MAE), mean_squared_error (MSE) and root mean squared error (RMSE). See here: https://github.com/keras-team/keras/issues/7947\r\n', 'Closing, as the metrics docs have been updated on both keras.io and tensorflow.org. 🙂 \r\n\r\n']",[],[],0,0
482,keras,1415,closed,Keras in Malay,"Interesting.. not really an issue but  in Malay language is ''Hard""


",,"['Any 3-syllable word from one language usually means something in another language. There are, after all, many languages, and each typically has thousands of 3-syllable words. \n']","['\nadj 1. firm, unyielding (of body, muscle, etc)\n--------------------------------------------------------\nthe ground was ~ because of the drought,\na ~ mattress,\n~ muscles,\n\n2. severe, unkind (of words)\n--------------------------------------------------------\nhis father was a ~ man,\nbe ~ on so.,\n\n3. forceful\n--------------------------------------------------------\na ~ blow on the head\n\n4. performing st with great intensity, persistence, etc,\n--------------------------------------------------------\na ~ worker,\na ~ drinker, \na ~ smoker\n\n5. requiring much effort\n--------------------------------------------------------\n~ work,\n']",['Keras'],0,0
483,keras,3543,closed,How to set trainable_weights properly?,"I am trying to implement the layer normalization in a standard fully connected neural network with keras, by writing a new layer. I copy nearly all the code of Dense layer and add a function of layer normalization and corresponding parameters. My code is as below:



But during fit, it got an TypeError: unorderable types: NoneType() < NoneType(). According to the log message, seems that the reason is the trainable_weights:



Here is the code building the model and fit it:



Could you please tell me what have I done wrong and how should I fix it? Thank you in advance!
",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']","[""\nclass DenseLN(Layer):\n   def __init__(self, output_dim, init='glorot_uniform', activation='linear', weights=None,\n                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n                 W_constraint=None, b_constraint=None, bias=True, input_dim=None, gamma_init=1., **kwargs):\n        self.init = initializations.get(init)\n        self.activation = activations.get(activation)\n        self.output_dim = output_dim\n        self.input_dim = input_dim\n\n        def gamma_init_func(shape, c=gamma_init):\n            if c == 1.:\n                return initializations.get('one')(shape)\n            return K.variable(np.ones(shape) * c, **kwargs)\n        self.gamma_init = gamma_init_func\n        self.beta_init = initializations.get('zero')\n        self.epsilon = 1e-5       \n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.initial_weights = weights\n        self.input_spec = [InputSpec(ndim=2)]\n\n        if self.input_dim:\n            kwargs['input_shape'] = (self.input_dim,)\n        super(DenseLN, self).__init__(**kwargs)\n\n    def ln(self, x):\n        m = K.mean(x, axis=-1, keepdims=True)\n        std = K.sqrt(K.var(x, axis=-1, keepdims=True) + self.epsilon)\n        x_normed = (x - m) / (std + self.epsilon)\n        x_normed = self.gamma * x_normed + self.beta\n        return x_normed\n\n    def build(self, input_shape):\n        assert len(input_shape) == 2\n        input_dim = input_shape[1]\n        self.input_spec = [InputSpec(dtype=K.floatx(),\n                                     shape=(None, input_dim))]\n\n        self.gamma = self.gamma_init(input_dim)\n        self.beta = self.beta_init(input_dim)\n\n        self.W = self.init((input_dim, self.output_dim),\n                           name='{}_W'.format(self.name))\n        if self.bias:\n            self.b = K.zeros((self.output_dim,),\n                             name='{}_b'.format(self.name))\n            self.trainable_weights = [self.W, self.gamma, self.beta, self.b]\n        else:\n            self.trainable_weights = [self.W, self.gamma, self.beta]\n        self.regularizers = []\n        if self.W_regularizer:\n            self.W_regularizer.set_param(self.W)\n            self.regularizers.append(self.W_regularizer)\n\n        if self.bias and self.b_regularizer:\n            self.b_regularizer.set_param(self.b)\n            self.regularizers.append(self.b_regularizer)\n\n        if self.activity_regularizer:\n            self.activity_regularizer.set_layer(self)\n            self.regularizers.append(self.activity_regularizer)\n\n        self.constraints = {}\n        if self.W_constraint:\n            self.constraints[self.W] = self.W_constraint\n        if self.bias and self.b_constraint:\n            self.constraints[self.b] = self.b_constraint\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    def call(self, x, mask=None):\n        output = K.dot(x, self.W)\n        #output = self.ln(output)\n        if self.bias:\n            output += self.b\n        return self.activation(output)\n\n    def get_output_shape_for(self, input_shape):\n        assert input_shape and len(input_shape) == 2\n        return (input_shape[0], self.output_dim)\n\n"", ""\n    TypeError                                 Traceback (most recent call last)\n<ipython-input-429-3ab01558ab51> in <module>()\n      3           batch_size=3000,\n      4           callbacks=[history],\n----> 5           nb_epoch=300)\n\n/home/lcc/anaconda3/envs/sensequant/lib/python3.5/site-packages/keras/models.py in fit(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\n    427                               shuffle=shuffle,\n    428                               class_weight=class_weight,\n--> 429                               sample_weight=sample_weight)\n    430\n    431     def evaluate(self, x, y, batch_size=32, verbose=1,\n\n/home/lcc/anaconda3/envs/sensequant/lib/python3.5/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\n   1079         else:\n   1080             ins = x + y + sample_weights\n-> 1081         self._make_train_function()\n   1082         f = self.train_function\n   1083\n\n/home/lcc/anaconda3/envs/sensequant/lib/python3.5/site-packages/keras/engine/training.py in _make_train_function(self)\n    695\n    696             # get trainable weights\n--> 697             trainable_weights = collect_trainable_weights(self)\n    698             training_updates = self.optimizer.get_updates(trainable_weights, self.constraints, self.total_loss)\n    699             updates = self.updates + training_updates\n\n/home/lcc/anaconda3/envs/sensequant/lib/python3.5/site-packages/keras/engine/training.py in collect_trainable_weights(layer)\n    248     elif layer.__class__.__name__ == 'Model':\n    249         for sublayer in layer.layers:\n--> 250             weights += collect_trainable_weights(sublayer)\n    251     elif layer.__class__.__name__ == 'Graph':\n    252         for sublayer in layer._graph_nodes.values():\n\n/home/lcc/anaconda3/envs/sensequant/lib/python3.5/site-packages/keras/engine/training.py in collect_trainable_weights(layer)\n    256     # dedupe weights\n    257     weights = list(set(weights))\n--> 258     weights.sort(key=lambda x: x.name)\n    259     return weights\n    260\n\nTypeError: unorderable types: NoneType() < NoneType()\n"", ""\nmodel = Sequential()\nmodel.add(DenseLN(12, input_dim=12))\nmodel.add(Activation('relu'))\n#model.add(Dropout(0.5))\nmodel.add(DenseLN(108))\nmodel.add(Activation('relu'))\nmodel.add(DenseLN(108))\nmodel.add(Activation('relu'))\nmodel.add(Dense(1))\nmodel.add(Activation('relu'))\nadadelta = Adadelta(lr=0.1, rho=0.95, epsilon=1e-08)\nadagrad = Adagrad(lr=0.003, epsilon=1e-08)\n\nmodel.compile(loss='poisson',\n             optimizer=adagrad,\n             metrics=['accuracy'])\n\nmodel.fit(X_train_scale,\n          Y_train,\n          batch_size=3000,\n          nb_epoch=300)\n""]",[],0,0
484,keras,3651,closed,Mask propagation in TimeDistributed doesn't seem to work after Embedding,"

I am using  as shown above only to operate on higher order inputs, and propagate the shape information accordingly. However, when I run the code above, I see that the first print statement shows a (symbolic) mask () and the second one just gives a . I'm not sure why  does not propagate the mask.
- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",stale,"['What exactly did you expect to achieve?\n\nMasking is for timeseries. The output of `Embedding` is a timeseries. If you apply `TimeDistributed` to `Embedding`, you get a timeseries of timeseries. What does it mean to mask such an output, according to you? Do you want to mask timesteps of the output, i.e. mask entire timeseries from the second level?\n', 'I am trying to embed a set of sentences. So the input to the embedding layer is of shape `(batch_size, num_sentences, num_words)` and I expect the output to be `(batch_size, num_sentences, num_words, embedding_dim)`. So yes, the output is indeed a timeseries of timeseries (with the two time dimensions being sentences and words). Would you recommend doing something else to achieve this?\n\nI expected the output mask to be of shape `(batch_size, num_sentences, num_words)` as well, where each slice of the tensor (`mask[:, i, :]`) is the mask of the corresponding sentence. So I guess the units being masked are still the same (words), but the mask has an additional dimension.\n', 'Alternatively, changing the `get_output_shape_for` function in `Embedding` to return `input_shape + (output_dim,)` would do this. Is there a reason why `Embedding` assumes the input is 2D?\n', 'Same here.', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']","[""\nfrom keras.layers import Input, Embedding, TimeDistributed\n\ninput_layer1 = Input(shape=(2,), dtype='int32')\nembedding = Embedding(input_dim=10, output_dim=20, mask_zero=True)\noutput1 = embedding(input_layer1)\nprint embedding.get_output_mask_at(0)\n\ninput_layer2 = Input(shape=(2,5), dtype='int32')\ntd_embedding = TimeDistributed(embedding)\noutput2 = td_embedding(input_layer2)\nprint td_embedding.get_output_mask_at(0)\n""]","['TimeDistributed(Embedding)', 'Elemwise{neq,no_inplace}.0', 'None', 'TimeDistributed']",0,0
485,keras,3008,closed,JSON/YAML loading not working with Lambda merge layers,"It doesn't seem possible to save and load models to/from JSON or YAML if there's a Merge layer with a lambda.
",,"['Similar https://github.com/fchollet/keras/issues/3001\n', 'Have you updated Keras to master? Have you posted a code snippet to reproduce your issue?\n', 'Still an issue in #2582 in head version as well. \n']",[],[],0,0
486,keras,544,closed,Can't import sequential,"Hi guys,

I just updated my keras package and now I can't import Sequential properly. It will give me the following erros:

  File ""/usr/lib/python2.6/site-packages/Keras-0.1.2-py2.6.egg/keras/models.py"", line 118
    if model_name not in {'Graph', 'Sequential'}:
                                 ^
SyntaxError: invalid syntax

Could anyone help me?
",,"['> py2.6\n\nKeras is supposed to be compatible with Python 2.7-3.4. Compatibility with 2.6 is not checked. You should upgrade your version of Python.\n\nIf you cannot do that for some reason, you can fix the syntax incompatibilities for your own use...\n']",[],[],0,0
487,keras,4767,closed,"How can I create a variable with shape (None, 1) ?","My problem is, I'm writing a layer with input shape (None, 10), and I want to append the same value to the end of every row to make it (None, 11). How can I do this? 

Right now I'm thinking of creating another (None, 1), then concatenating them.

Thanks in advance.",type:support,"['@dontloo try to do this it. I think must solve\r\n img = img.reshape((1,) + img.shape) ', ""> Right now I'm thinking of creating another (None, 1), then concatenating them.\r\n\r\nIf you have 2 separate inputs (10-dimensional vectors and single scalars) then the right approach is to create an input of shape (None, 10) and another one of shape (None, 1) and concatenate them.\r\n\r\nThen again you could also create an input of shape (None, 11) and feed it the appropriate data."", ""@fchollet Thanks for the reply. But actually I want to make it as a trainable parameter (instead of an input) and add it to the end of every input vector, I didn't find a simple way to do this.\r\n\r\nSay I want to add a value `b` to the end of every row of `x`, right now what I'm doing is like \r\n\r\n```python\r\nself.b = self.init((1,))  # trainable parameter\r\n...\r\n# this is what I did to make b of shape (None, 1)\r\nb_vec = K.expand_dims(0*K.sum(x, axis=1), 1) + self.b\r\nK.concatenate([x, b_vec], axis=-1)\r\n```\r\n\r\nso I was wondering if there's a simpler way?"", ""If you need to add trainable weights, you must write a custom layer.\n\nOn 19 December 2016 at 18:04, dontloo <notifications@github.com> wrote:\n\n> @fchollet <https://github.com/fchollet> Thanks for the reply. But\n> actually I want to make it as a trainable parameter (instead of an input)\n> and add it to the end of every input vector, I didn't find a simple way to\n> do this.\n>\n> Say I want to add a value b to the end of every row of x, right now what\n> I'm doing is like\n>\n> self.b = self.init((1,))  # trainable parameter...# this is what I did to make b of shape (None, 1)\n> b_vec = K.expand_dims(0*K.sum(x, axis=1), 1) + self.bK.concatenate([x, b_vec], axis=-1)\n>\n> so I was wondering if there's a simpler way?\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/fchollet/keras/issues/4767#issuecomment-268133639>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AArWb1A_9UwUC7XUHtPFJGzsbCOW2-Vkks5rJzeQgaJpZM4LQcvb>\n> .\n>\n"", ""@fchollet yes, my problem is, in the layer I wrote, I'm not able to create a trainable weight of shape (None, 1). The way I worked around this is first squeezing the input into some shape I want, then multiplying them all by zeros and adding my parameter to it (so that it can be broadcast) like\r\n```python\r\n# this is what I did to make b of shape (None, 1)\r\nb_vec = K.expand_dims(0*K.sum(x, axis=1), 1) + self.b\r\n\r\n```""]",[],[],0,0
488,keras,4776,closed,predict_generator function does not support samples with different lengths,"Hi, I am doing a sequence labelling task(like PosTagging) using Keras + theano. The model I use is BiLSTM. For training, I did not do padding. Instead, I just used the batch size of 1, and did not specify the input_length parameter. It works well for training and evaluation(for both I use the generator version to deal with the memory problem). However, when I want to do prediction, I have the following errors:



I think the reason is the first sentence have a length of 42 words, while the second sentence have a length of 10 words. 

My question is, why it works well for training and evaluation, but does not work for prediction?

The code I think relevant is here

",stale,"['Could you kindly have a look at this problem? @fchollet ', 'This still seems to be a problem in Keras using Tensorflow 1.12.0.\r\n\r\nWhen I run model.predict_generator I get results only if each X returned by the generator has the same  length. Otherwise it throws this error:\r\n\r\n`ValueError: all the input array dimensions except for the concatenation axis must match exactly`']","['\r\nPredicting...\r\nTraceback (most recent call last):\r\n  File ""predict.py"", line 81, in <module>\r\n    main()\r\n  File ""predict.py"", line 55, in main\r\n    val_samples=samples)\r\n  File ""/home/user/.local/lib/python2.7/site-packages/keras/models.py"", line 1001, in predict_generator\r\n    pickle_safe=pickle_safe)\r\n  File ""/home/user/.local/lib/python2.7/site-packages/keras/engine/training.py"", line 1749, in predict_generator\r\n    all_outs[i][processed_samples:(processed_samples + nb_samples)] = out\r\nValueError: could not broadcast input array from shape (1,42,1310) into shape (1,10,1310)\r\n', ""\r\n    print('\\nPredicting...')\r\n\r\n    samples = 1 # only 1 work for now\r\n    prob = model.predict_generator(data_generator(X_test, X_test_feats, y_test, tag_size, shuffle=False),\r\n                                   val_samples=samples)\r\n    result = prob.argmax(axis=-1)\r\n""]",[],0,0
489,keras,8070,closed,Layer split use Lambda layer gives error,"Hi all,

I'm trying to split one keras layer using Lambda function. Follow is my code snippet:

This model compiles well.
But when I feed real training data to this model, it gives me the error messege:
  [generated_images,f_id] = generator.predict([image_batch, c_, z])
  File ""build/bdist.linux-x86_64/egg/keras/engine/training.py"", line 1653, in predict
  File ""build/bdist.linux-x86_64/egg/keras/engine/training.py"", line 1246, in _predict_loop
  File ""build/bdist.linux-x86_64/egg/keras/backend/tensorflow_backend.py"", line 2255, in __call__
  File ""/home/vivo/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/home/vivo/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""/home/vivo/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""/home/vivo/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 8768 values, but the requested shape requires a multiple of 146
	 [[Node: dense_4/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](concatenate_1/concat, dense_4/Reshape/shape)]]
	 [[Node: conv2d_transpose_3/Tanh/_53 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_1_conv2d_transpose_3/Tanh"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Caused by op u'dense_4/Reshape', defined at:
  File ""GAN_UTD_2nd_model.py"", line 428, in <module>
    generator = model_generator(latent_dim = latent_dim, input_shape = input_shape, units = units)
  File ""GAN_UTD_2nd_model.py"", line 240, in model_generator
    h_dense = Dense(15 * 20 * 64, activation = 'relu')(h)
  File ""build/bdist.linux-x86_64/egg/keras/engine/topology.py"", line 602, in __call__
    output = self.call(inputs, **kwargs)
  File ""build/bdist.linux-x86_64/egg/keras/layers/core.py"", line 841, in call
    output = K.dot(inputs, self.kernel)
  File ""build/bdist.linux-x86_64/egg/keras/backend/tensorflow_backend.py"", line 973, in dot
    xt = tf.reshape(x, [-1, x_shape[-1]])
  File ""/home/vivo/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 2630, in reshape
    name=name)
  File ""/home/vivo/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""/home/vivo/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2327, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/vivo/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1226, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Input to reshape is a tensor with 8768 values, but the requested shape requires a multiple of 146
	 [[Node: dense_4/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/gpu:0""](concatenate_1/concat, dense_4/Reshape/shape)]]
	 [[Node: conv2d_transpose_3/Tanh/_53 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_1_conv2d_transpose_3/Tanh"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]


I don't understand especially this error message""tensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 8768 values, but the requested shape requires a multiple of 146"".
The output of Dense layer is 19200 units, so it should be right for the following Reshape layer...

Any idea??
Best
",,"['I found out the cause is I missed the batch dimension in h_left and h_out...', 'hi\r\n you are splitting into h_left and h_out and then concatenating h_left with other two and proceeded. can you explain what does h_left, h_out doing? i wanna split an image into two/three, i havent found any guildlines except the abv. \r\n']",[],"['    k = 5\r\n    x = Input((120,160,1))\r\n    h = Conv2D(units / 4, (k, k), strides = (2,2), border_mode=\'same\', activation = \'elu\')(x)\r\n    h = BatchNormalization(momentum=0.8)(h)\r\n    h = Dropout(dropout)(h)\r\n    # h = MaxPooling2D(pool_size=(2, 2))(h)\r\n    # h = LeakyReLU(0.2)(h)\r\n    h = Conv2D(units / 2, (k, k),  strides = (2,2), border_mode=\'same\', activation = \'elu\')(h)\r\n    h = BatchNormalization(momentum=0.8)(h)\r\n    h = Dropout(dropout)(h)\r\n    # h = MaxPooling2D(pool_size=(2, 2))(h)\r\n    # h = LeakyReLU(0.2)(h)\r\n    h = Conv2D(units / 2, (k, k), strides = (2,2), border_mode=\'same\', activation = \'elu\')(h)\r\n    h = BatchNormalization(momentum=0.8)(h)\r\n    h = Dropout(dropout)(h)\r\n    # h = MaxPooling2D(pool_size=(2, 2))(h)\r\n    # h = LeakyReLU(0.2)(h)\r\n    h = Conv2D(units, (k, k), strides = (2,2), border_mode=\'same\', activation  = \'elu\')(h)\r\n    h = BatchNormalization(momentum=0.8)(h)\r\n    h = Dropout(dropout)(h)\r\n    # h = LeakyReLU(0.2)(h)\r\n    h = AveragePooling2D((6,8))(h)\r\n    h = Dense(latent_dim, name=""encoder_mu"")(h)\r\n    h_left = Lambda(lambda x : x[:,:,0:latent_dim/2],output_shape=(1,1,latent_dim/2))(h)\r\n    h_out  = Lambda(lambda x : x[:,:,latent_dim/2:latent_dim], output_shape=(1,1,latent_dim/2))(h)\r\n    # h_out  = K.squeeze(h_out)(h_out)\r\n    \r\n\r\n    auxiliary_c = Input(shape=(1,1, c_dim), name=\'aux_input_c\')\r\n    auxiliary_z = Input(shape=(1,1, noise_dim), name=\'aux_input_z\')\r\n    h = keras.layers.concatenate([h_left, auxiliary_c, auxiliary_z])\r\n\r\n    # h = Flatten()(h)\r\n    h_dense = Dense(15 * 20 * 64, activation = \'relu\')(h)\r\n    h = Reshape(( 15, 20, 64))(h_dense)\r\n    # h = LeakyReLU(0.2)(h)\r\n    h = Conv2DTranspose(units/2, (k,k),  strides = (2,2),  padding = \'same\', activation = \'elu\')(h) # 8*6*64\r\n    # h = Dropout(dropout)(h)\r\n    h = BatchNormalization(momentum=0.8)(h)\r\n    # h = LeakyReLU(0.2)(h)\r\n    # h = UpSampling2D(size=(2, 2))(h)\r\n    h = Conv2DTranspose(units/4, (k,k),  strides = (2,2),  padding = \'same\', activation = \'elu\')(h) # 8*6*64\r\n    # h = Dropout(dropout)(h)\r\n    h = BatchNormalization(momentum=0.8)(h)\r\n    # h = LeakyReLU(0.2)(h)\r\n    # h = UpSampling2D(size=(2, 2))(h)\r\n    h = Conv2DTranspose(1, (k,k),  strides = (2,2),  padding = \'same\', activation = \'tanh\')(h) # 8*6*64\r\n    # h = Dropout(dropout)(h)\r\n    # h = BatchNormalization(momentum=0.8)(h)\r\n    # h = LeakyReLU(0.2)(h)\r\n    # h = UpSampling2D(size=(2, 2))(h)\r\n    # output = Conv2D(1, (k, k), border_mode=\'same\', activation = \'tanh\')(h)\r\n\r\n    return Model([x, auxiliary_c,auxiliary_z], [h, h_out], name=""encoder"")\r\n']",0,0
490,keras,1198,closed,How to display RGB image (e.g. cifar) in Keras?,,,"['See #1161. For RGB, resize your vector to have shape (width, height, 3). \n', ""Well.. I got the problem of dimensions from matplotlib:\n\n```\nTypeError: Invalid dimensions for image data\n```\n\nBecause I directly use 'imshow' the image from cifar with shape(3,32,32), so the solution to solve it is to 'transpose' it.\n\n```\nplt.imshow(x.transpose(1,2,0)) #(height,width,dim)\n```\n""]",[],[],0,0
491,keras,2545,closed,Sequence to sequence training and predicting (Decoder Encoder),"I have 2 questions:
1. I discussed about sequence to sequence learning where the length of input and output are different in problem #2403 and I concluded that I have to use encoder and decoder architecture. I have a fundamental question, what is the reason that keras doesn't support this case and encoder and decoder is needed.  Is it a fundamental problem of recurrent neural networks (if it is, can anyone explain this to me?), or is it the Keras problem that doesn't support this case? 

2.When I have the following model:



consider batch_size =1, then which of the following statement is true:
A. All _n_prev_ inputs are fed to all  _hidden_neurons_ in the first LSTM layer?
B. At each time only one of _in_prev_ inputs are fed  to the first LSTM layer and it will propagate to all other _hidden_neurons_ in this layer?
",stale,"['I think B is true.\nbatch_input_shape=(batch_size, timesteps, feature_dim)\n', '@mininaNik  Question about your code: \n\nthe first LSTM has an output dimension of hidden_neurons, why do you (or why are you allowed to) define the second LSTM input dimension to be 1?\n', '@mininaNik I guess batch_input_shape for the second LSTM is ignored, as answered for Q2 in #3038 \n']","[""\n      model = Sequential()\n      hidden_neurons = 50 \n      model.add(LSTM(hidden_neurons,\n             batch_input_shape=(batch_size, n_prev, 1),\n             forget_bias_init='one',\n             return_sequences=True,\n             stateful=True))\n     model.add(LSTM(hidden_neurons,\n             batch_input_shape=(batch_size, n_prev, 1),\n             forget_bias_init='one',\n             return_sequences=False,\n             stateful=True))\n\n    model.add(Dense(n_nxt))\n    model.compile(loss='mse', optimizer='rmsprop')\n""]",[],0,0
492,keras,13268,closed,predictions different with same weight,"<em>Please make sure that this is a Bug or a Feature Request and provide all applicable information asked by the template.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.</em>  

**System information**  
- Have I written custom code (as opposed to using example directory):  
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux CentOS 7.0
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  1.14
- Keras version:  2.2.0
- Python version:  2.7
- CUDA/cuDNN version:  8.0
- GPU model and memory:  single GPU with 16GB memory

You can obtain the TensorFlow version with:  
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""  
You can obtain the Keras version with:  
python -c 'import keras as k; print(k.__version__)'  

**Describe the current behavior**  

**Describe the expected behavior**  

**Code to reproduce the issue**  
Provide a reproducible test case that is the bare minimum necessary to generate the problem.  

**Other info / logs**  
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.  

Hi ,I found I run the same code and same weight with same images but got different predictions each time ,my code and images are on github [binary_categorical](https://github.com/yasohasakii/binary_categorical) and you can run the  script on colab, and weight  file has been upload to [google dirver](https://drive.google.com/open?id=1sCIAgoQ7Og18iBmhwaOUmVSbRziAX5fo). Below codes are my result for twice times.  
  


",type:support,['Can you please take a look at this [issue](https://github.com/keras-team/keras/issues/2743) and let me know if it helps. Thanks!'],[],"['ipynb', '.h5', './PetImages/test/Cat/10000.jpg 0.070283316  \r\n./PetImages/test/Cat/10001.jpg 0.04877737  ', './PetImages/test/Cat/10000.jpg 0.39205018  \r\n./PetImages/test/Cat/10001.jpg 0.06655105  ']",0,0
493,keras,5044,closed,`ZeroDivisionError` when using `class_mode=None` with `ImageDataGenerator.flow_from_directory()`,"[Here's the script][1] to run to see the exception. One can run it like . After a quick glance at the source, it seems like  is not used in the  constructor to prevent  from being set to , which ultimately becomes  in the base .

Some reporters of a previous bug (#4699) have reported that this ""works"" when you create an additional directory under the directory passed to . However, I don't think this is ideal, for semantically, there _is_ no class/label when dealing with test images.

My proposal to fix this is to set self.nb_sample = number of valid image files in the input directory when , in . The second clause in the condition will help prevent breakage of code that has hacked its way around this issue by creating an additional directory (or we don't have to be afraid of breaking, it's up to you). Please let me know if you are okay with this, and if you have any ideas (including calling this issue a non-issue :)), and I can accordingly prepare a quick PR (or not).

[1]: https://gist.github.com/yati-sagade/ff309678a6d6ec849c488b1f9a5fa6b3",stale,"[""Just wanted to say that I also found this behavior confusing. I'm attempting to use `DirectoryIterator.flow_from_directory` on a directory containing test images (not labeled) which I want to pass to `predict_generator`. I had to put all my images in an arbitrarily named folder in order to use the function."", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],"['python imggen_reproduce.py /tmp/somedir_that_you_donot_care_about', 'class_mode', 'DirectoryIterator', 'self.nb_sample', '0', 'self.n', 'Iterator', 'flow_from_directory()', 'self.class_mode is None and not self.classes', 'DirectoryIterator::__init__']",0,0
494,keras,8073,closed,Not understanding error,"I've written my own loss function which seems to get stuck in this error which I can't figure out.
Seems it could either be sample_weight or mask, but i don't know what to do with it.

Can anyone help?

![image](https://user-images.githubusercontent.com/5853644/31272735-49a469ce-aa8c-11e7-9d48-80eb1a64b29a.png)


",,"['Hello \r\nWhat does a print(ndim) and print(weight_ndim) before line 465 displays ?', 'print(ndim): none\r\nprint(weight_ndim): 1', 'What does keras compile expect to get back from the loss function? What dimensionality?', ""The bug is in not in Keras but in range.\r\nIn python3\r\nrange( 1, None) ->typeError : 'NoneType' object cannot be interpreted as an integer\r\nYour ndim should be not be None.\r\nMost likely K.n_dim( score_array ) is problematic\r\n(This bug often also strikes in its variant np.zeros(4,5) instead of np.zeros((4,5)) )\r\n\r\nKeras expect a one dimensionnal vector as a loss : 1 float loss value by example of a batch.\r\n"", 'Thanks for clarification, do you know any solution?', ""It seems to relate quite heavily to tf.map_fn(single_loss, tf.range(tf.shape(y_pred)[0])) if I try to get the shape of this it's unknown and _dims is None.\r\nSo it is actually more of a tensorflow problem than keras.\r\nHowever if anyone knows how to get around it please help."", 'Okay so I got on by doing this:\r\n\r\n ```\r\nfull_return = tf.map_fn(single_loss, tf.range(tf.shape(y_pred)[0]))\r\n full_return.set_shape((None,))\r\n return full_return\r\n```\r\n\r\nSo now it can get the dimensions which is 1 as expected.\r\nAs always more problems follow... of course....\r\n', 'Was minor error i was able to compile it now so I will close the issue.']",[],[],0,0
495,keras,7193,closed,AttributeError: 'module' object has no attribute 'ctc_decode',"I use keras 2.02 with theano backend

When I call the  attribute 'ctc_decode' using K.ctc_decode, an error was thrown. I use dir(K) to check its avaliable attribute, but I do not find 'ctc_decode'. 
Where is the problem? Thank you in advance!",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],['from keras import backend as K'],0,0
496,keras,8827,closed,[help wanted] Input shape / target size flow from directory,"Hi,
i´m wondering about the correct input_shape and/or target_size when using fit_generator and flow_from_directory. I´m running keras 2.0.8-tf and tensorflow 1.4 as the backend. My images have the following dimensions:

width: 725 Pixel
height: 180 Pixel
channels: 3

I define the Input_shape as: (width, height, channels) --> (725,180,3), as it is suggested when using tensorflow as the backend.

I use the ImageDataGenerator with the flow_from_directory method to get the training data. Here the target_size may be specified. The target_size can be either (width, height) or (height, width). The API is defining it as (height, width). 

And now the Problem occurs. If the Input_shape is (width, height, channels) and the target_size is (heigth, width) - as it should be - a ValueError is raised, saying the shapes don´t match (Error when checking input: expected conv2d_1_input to have shape (None, 725, 180, 3) but got array with shape (50, 180, 725, 3)).

If i set the target_size to (width, height), the training is starting, but if i then plot an Image using the Generator, i see that width and height are exchanged and hence the Image is massively transformed.

How to solve this issue? In fchollets great [example](https://gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d) the target_size is also set the other way around as in the API.

Any help is appreciated! Thank you.

Here is my code example:





",,"['@vkessler https://keras.io/layers/convolutional/. by keras doc, conv2d input shape should be (height, width) == (row, colum) sequence. I think example has a slight mistake of it.\n\n<sub>Sent with <a href=""http://githawk.com"">GitHawk</a></sub>', 'Thank you very much for the clarification. I guess i didn´t pay attention enough to the convolutional API. ']","[""\r\n# Set the dimension of the images \r\nimg_width = 725 \r\nimg_height = 180\r\n\r\n# Define epochs and batch size\r\nepochs = 1\r\nbatch_size = 50\r\n\r\n# TensorFlow is the backend, so ordering of input_shape is as below\r\ninput_shape = (img_width, img_height, 3)\r\n\r\n# Helper function to compile model\r\ndef compile_model(model):\r\n    optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\r\n    model.compile(loss='categorical_crossentropy',\r\n                  optimizer=optimizer,\r\n                  #metrics=['accuracy'])\r\n                  metrics=['accuracy',metrics.categorical_accuracy])\r\n\r\n# Define the model\r\ndef generate_model(model_path):\r\n    \r\n    # check if model exists, if exists then load model from saved state\r\n    if Path(model_path).is_file():\r\n        sys.stdout.write('Loading existing model\\n\\n')\r\n        sys.stdout.flush() #Warum?\r\n        model = load_model(model_path)\r\n        compile_model(model)\r\n        return model\r\n\r\n    sys.stdout.write('Loading new model\\n\\n')\r\n    sys.stdout.flush() #Warum?\r\n\r\n    # Define new model\r\n    model = Sequential()\r\n    model.add(Conv2D(32, (4, 4), input_shape=input_shape))\r\n    model.add(Activation('relu'))\r\n    model.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\n    model.add(Conv2D(32, (4, 4)))\r\n    model.add(Activation('relu'))\r\n    model.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\n    model.add(Conv2D(32, (4, 4), input_shape=input_shape))\r\n    model.add(Activation('relu'))\r\n    model.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\n    model.add(Conv2D(64, (4, 4)))\r\n    model.add(Activation('relu'))\r\n    model.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\n    model.add(Flatten())\r\n    model.add(Dense(64))\r\n    model.add(Activation('relu'))\r\n    model.add(Dropout(0.5))\r\n    model.add(Dense(num_classes, activation='softmax'))\r\n\r\n    compile_model(model)\r\n\r\n    # Save model\r\n    with open(model_path, 'w') as outfile:\r\n        json.dump(model.to_json(), outfile)\r\n        outfile.close()\r\n\r\n    return model\r\n\r\n\r\nmodel = generate_model(model_path)\r\n\r\n# this is the augmentation configuration we will use for training\r\ntrain_datagen = ImageDataGenerator(\r\n    rescale=1. / 255,\r\n    rotation_range=180.,\r\n    #shear_range=0.2,\r\n    #zoom_range=0.2,\r\n    horizontal_flip=True)\r\n\r\n# this is the augmentation configuration we will use for testing:\r\n# only rescaling\r\ntest_datagen = ImageDataGenerator(rescale=1. / 255)\r\n\r\ntrain_generator = train_datagen.flow_from_directory(\r\n    train_data_dir,\r\n    target_size=(img_height, img_width), # not running this way\r\n    batch_size=batch_size,\r\n    class_mode='categorical')\r\n\r\nvalidation_generator = test_datagen.flow_from_directory(\r\n    validation_data_dir,\r\n    target_size=(img_height, img_width), # not running this way\r\n    batch_size=batch_size,\r\n    class_mode='categorical')\r\n\r\nhist = model.fit_generator(\r\n    train_generator,\r\n    steps_per_epoch=nb_train_samples // batch_size,\r\n    epochs=epochs,\r\n    validation_data=validation_generator,\r\n    validation_steps=nb_validation_samples // batch_size)\r\n\r\nmodel.save(model_path)\r\nprint('Saved trained model at %s ' % model_path)\r\n\r\n\r\nx, y = train_generator.next()\r\nclasses_val = validation_generator.class_indices\r\nprint(classes_val)\r\nfor i in range(0,12):\r\n    img = x[i]\r\n    print(y[i])\r\n    plt.imshow(img)\r\n    plt.show()\r\n""]",[],0,0
497,keras,4094,closed,"Same model, different outputs for Keras versions 1.0.5 and 1.1.0","While trying to load an earlier trained model and reproduce the outputs, I observed very different outputs for the same input and same model loaded in different Keras versions (namely 1.0.5 and 1.1.0). The model was trained with Keras 1.0.5. I tried to also verify this with a dummy input data fed to the same model loaded in different versions. I checked that the loaded weights match for two versions. Is there any difference between how the layers are defined for v_1.0.5 and v_1.1.0? I noticed at least the default parameters for batch normalization layer is slightly different. For what it's worth, I get similar accuracy when I train two models with same parameters with these two versions (about 1% change), but the difference when loading the model in the other version is huge. I attached the model parameters and summary output.

[model_summary.txt](https://github.com/fchollet/keras/files/533414/model_summary.txt)
",stale,"['BN was in Theano backend around there (97d2a73, 0588393). By the way, how did you get the model summary text file?\n']",[],[],0,0
498,keras,4412,closed,Use images as both x and y,"Hi,
I am trying to make a model of [this](http://tinyclouds.org/colorize/residual_encoder.png). Here is the relevant code:

Here, I want to train the model on a bunch of images in a directory where the input to the model is rgb2gray(image) and op should be close to rgb2uv(image). How do I do that?
Also, have I defined the models correctly?",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']","[""\r\nbase_model = VGG16(weights='imagenet')\r\nb_model = Model(input=base_model.input, output=[ base_model.get_layer('block4_conv3').output,\r\n    base_model.get_layer('block3_conv3').output,\r\n    base_model.get_layer('block2_conv2').output,\r\n    base_model.get_layer('block1_conv2').output ])\r\n\r\nconv4_3, conv3_3, conv2_2, conv1_2 = b_model.predict(x) # Here x should be a modified image\r\n\r\n# Use the output of the layers of VGG16 on x in the model\r\nconv1 = Convolution2D(256, 1, 1, border_mode='same')(BatchNormalization()(conv4_3))\r\nconv1_scaled = resize(conv1, 56)\r\n.\r\n.\r\n.\r\nconv5 = Convolution2D(3, 3, 3, border_mode='same')(merge([ip_img, conv4], mode='sum'))\r\nop = Convolution2D(2, 3, 3, border_mode='same')(conv5)\r\n\r\nmodel = Model(input=base_model.input, output=op)\r\nmodel.compile(optimizer='sgd', loss=custom_loss_fn)\r\n""]",[],0,0
499,keras,4186,closed,Loss averaging over batches does not take into account sequence lengths and masking when return_sequences=True,"An rnn can work on batches having sequences of different length. in the case of return_sequences=True, we should average the losses over batches taking into account not only the batch size but also the different sequence length for the averaging weights. a batch with longer sequences should have a stronger effect on the loss than a batch of same batch size but with shorter sequences.

A related issue is to also take into account the effective length changes due to masking.
",stale,['please reopen. this is still an issue. i missed the stale tag.'],[],[],0,0
500,keras,2257,closed,Can anyone explain how to find the derivative of internal states and include them in the keras code ?,"Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",stale,[],[],[],0,0
501,keras,9042,closed,How to multiply 2 matrices of the same size element-wise using Keras backend?,"I am writing a custom loss function but couldn't find the function do perform element-wise multiplication. Any suggestion?
Thanks
",,['Answer (myself): *'],[],[],0,0
502,keras,11023,closed,Cannot load_model,"Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I am using Google Colab to train a CNN and then save the entire model to a  file. The code is available here: [CNN-Colab](https://gist.github.com/abhisheksoni27/184c49ca703eb124e1b17eb8dd8af518)

The model gets saved but when I later try to load it back, I get the following error:



The entire Output log is here: [CNN - Colab - Error](https://gist.github.com/abhisheksoni27/732bec240629d2dd721e80130cb2956b)
",,"['@abhisheksoni27 I think you hit [this](https://github.com/keras-team/keras/issues/10718) bug, it has been fixed at ```keras-team/keras```, but you are using ```tf.keras```. Actually I have sent a [fix](https://github.com/tensorflow/tensorflow/pull/21005) to ```tf.keras``` as well, but not merged until now.', 'Closing this issue since the fixed has been merged. Feel free to reopen the issue if the problem still persists. Thanks!', ""Is there a way to apply the fix as a patch to Tensorflow 1.12 installed in Anaconda?\r\n\r\nEdit:\r\nFor those looking to fix this issue, follow these steps.\r\n\r\n1. Open a python interpreter\r\n2. `import tensorflow`\r\n3. Type `tensorflow.__file__` to find the location of your tensorflow install (call this $TENSORFLOW_DIR)\r\n4. Navigate to your tensorflow install directory\r\n5. Edit `$TENSORFLOW_DIR/python/keras/layers/advanced_activations.py`\r\n6. In the stack trace where you saw the `TypeError: float() argument must be a string or a number` message, a few lines up there should be a trace from advanced_activations.py.\r\n7. Edit the line in advanced_activations.py to use a try/except block\r\n\r\n`try:\r\n  self.SOME_VALUE = K.cast_to_floatx(SOME_VALUE)\r\nexcept TypeError:\r\n  self.SOME_VALUE = K.cast_to_floatx(SOME_VALUE['value'])\r\n`""]","[""\r\nTypeError: float() argument must be a string or a number, not 'dict'\r\n""]",['.h5'],0,0
503,keras,10699,closed,cannot install keras on raspberry pi 3 python=3.5.3,"Using  I run into a 404 error on PyPi because there is no arm7 wheel for pyYAML. 



I've tried to just install Keras from this git repo using


but when I try to import the library I have other dependency issues 


Can somebody suggest a fix to install keras with tensorflow backend?",,"['install keras 1.2.2 ', 'I still get the same 404 not found error on PyPI for PyYAML\r\n`(keras) pi@raspberrypi:~ $ pip install keras==1.2.2\r\nLooking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple\r\nCollecting keras==1.2.2\r\n  Downloading https://www.piwheels.org/simple/keras/Keras-1.2.2-py3-none-any.whl (210kB)\r\n    100% |████████████████████████████████| 215kB 2.2MB/s \r\nRequirement already satisfied: six in ./.virtualenvs/keras/lib/python3.5/site-packages (from keras==1.2.2) (1.11.0)\r\n\r\nCollecting pyyaml (from keras==1.2.2)\r\n  HTTP error 404 while getting https://www.piwheels.org/simple/pyyaml/PyYAML-4.1-cp35-cp35m-linux_armv7l.whl#sha256=a5b125a24244830d574ccfd8fab861198d2ffe491b73179ffd75abf78caee6ff (from https://www.piwheels.org/simple/pyyaml/)\r\n  Could not install requirement pyyaml from https://www.piwheels.org/simple/pyyaml/PyYAML-4.1-cp35-cp35m-linux_armv7l.whl#sha256=a5b125a24244830d574ccfd8fab861198d2ffe491b73179ffd75abf78caee6ff (from keras==1.2.2) because of error 404 Client Error: Not Found for url: https://www.piwheels.org/simple/pyyaml/PyYAML-4.1-cp35-cp35m-linux_armv7l.whl\r\nCould not install requirement pyyaml from https://www.piwheels.org/simple/pyyaml/PyYAML-4.1-cp35-cp35m-linux_armv7l.whl#sha256=a5b125a24244830d574ccfd8fab861198d2ffe491b73179ffd75abf78caee6ff (from keras==1.2.2) because of HTTP error 404 Client Error: Not Found for url: https://www.piwheels.org/simple/pyyaml/PyYAML-4.1-cp35-cp35m-linux_armv7l.whl for URL https://www.piwheels.org/simple/pyyaml/PyYAML-4.1-cp35-cp35m-linux_armv7l.whl#sha256=a5b125a24244830d574ccfd8fab861198d2ffe491b73179ffd75abf78caee6ff (from https://www.piwheels.org/simple/pyyaml/)`\r\n', 'Seeing as this issue might become stale, the fix that worked for me was: \r\nthe `keras-preprocessing` and `keras.applications` have to be installed seperately\r\nNot sure what the issue with `pip install keras` but I am using a fresh install of Raspbian.\r\n\r\n```\r\npip install git+git://github.com/keras-team/keras.git --upgrade --no-deps\r\npip install keras-preprocessing\r\npip install keras.applications \r\n```\r\n\r\n']",[],"['pip install keras', 'Looking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple\r\nCollecting keras\r\n  Using cached https://files.pythonhosted.org/packages/68/12/4cabc5c01451eb3b413d19ea151f36e33026fc0efb932bf51bcaf54acbf5/Keras-2.2.0-py2.py3-none-any.whl\r\nCollecting pyyaml (from keras)\r\n  HTTP error 404 while getting https://www.piwheels.org/simple/pyyaml/PyYAML-4.1-cp35-cp35m-linux_armv7l.whl#sha256=a5b125a24244830d574ccfd8fab861198d2ffe491b73179ffd75abf78caee6ff (from https://www.piwheels.org/simple/pyyaml/)\r\n  Could not install requirement pyyaml from https://www.piwheels.org/simple/pyyaml/PyYAML-4.1-cp35-cp35m-linux_armv7l.whl#sha256=a5b125a24244830d574ccfd8fab861198d2ffe491b73179ffd75abf78caee6ff (from keras) because of error 404 Client Error: Not Found for url: https://www.piwheels.org/simple/pyyaml/PyYAML-4.1-cp35-cp35m-linux_armv7l.whl\r\nCould not install requirement pyyaml from https://www.piwheels.org/simple/pyyaml/PyYAML-4.1-cp35-cp35m-linux_armv7l.whl#sha256=a5b125a24244830d574ccfd8fab861198d2ffe491b73179ffd75abf78caee6ff (from keras) because of HTTP error 404 Client Error: Not Found for url: https://www.piwheels.org/simple/pyyaml/PyYAML-4.1-cp35-cp35m-linux_armv7l.whl for URL https://www.piwheels.org/simple/pyyaml/PyYAML-4.1-cp35-cp35m-linux_armv7l.whl#sha256=a5b125a24244830d574ccfd8fab861198d2ffe491b73179ffd75abf78caee6ff (from https://www.piwheels.org/simple/pyyaml/)', 'pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps', '>>> import keras\r\nUsing TensorFlow backend.\r\n/home/pi/.virtualenvs/keras_tf/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.4 of module \'tensorflow.python.framework.fast_tensor_util\' does not match runtime version 3.5\r\n  return f(*args, **kwds)\r\n/home/pi/.virtualenvs/keras_tf/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 432, got 412\r\n  return f(*args, **kwds)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/pi/.virtualenvs/keras_tf/lib/python3.5/site-packages/keras/__init__.py"", line 5, in <module>\r\n    from . import applications\r\n  File ""/home/pi/.virtualenvs/keras_tf/lib/python3.5/site-packages/keras/applications/__init__.py"", line 11, in <module>\r\n    import keras_applications\r\nImportError: No module named \'keras_applications\'\r\n>>> import keras_applications.io\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\nImportError: No module named \'keras_applications\'']",0,0
504,keras,3048,closed,Classification using 1D and 2D Conv net for time series data with multiple features,"Hi, 

Am trying to solve a classification problem using time series/ Sequential data using CNN.
I have created some features using simple mathematical transformation from same data. 
Following are the things I tried.
1. 1D conv net with features as multiple row
2. 2D conv net with single channel and features as rows( Filter nb_rows =1)
3. 2D conv net with channel = number of features(Filter nb_rows =1)

I want to know what would be the correct approach. 

My understanding is as follows:
1. 1D conv net with multiple rows net treats each row as different information. 
2. 2D with 1 channel and multiple rows with filter nb_rows=1 should behave same as 1D conv net with multiple rows
3. 2D conv net with features as different channels help to learn new abstractions.

Appreciate your comments on which approach fits best theoritically.

Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],0,0
505,keras,7206,closed,UnicodeDecodeError for GloVe,"Hi,

when im using




print('Loaded %s word vectors.' % len(embeddings_index))`

I get the following error in  for line in f:

",,"['oh it works now after i use\r\n\r\n`f = open(glove_data, encoding=""utf8"")`', 'Hi,\r\nI am experiencing the same problem, when using \'utf8\' decoding I get this error ;\r\n\'charmap\' codec can\'t decode byte 0x9d in position 3692: character maps to <undefined>\r\nit seems ""utf8"" decoding dose not resolve the pb. I am using windows 10, do you know if there is a specific encoder/decoder for windows 10.Thanks\r\n', ""'utf8' as suggested by leonardltk worked for me. thanks!"", '> Hi,\r\n> I am experiencing the same problem, when using \'utf8\' decoding I get this error ;\r\n> \'charmap\' codec can\'t decode byte 0x9d in position 3692: character maps to \r\n> it seems ""utf8"" decoding dose not resolve the pb. I am using windows 10, do you know if there is a specific encoder/decoder for windows 10.Thanks\r\n\r\nHi,\r\nhave you resolved the issue? I\'m facing same problem.', ""**try it**\r\n\r\nimport os, re\r\nword_embeddings = {}\r\nwith open(os.path.join('../input/glove6b50dtxt/glove.6B.50d.txt')) as f:\r\n\r\n    for line in f:\r\n\r\n        values = line.split()\r\n\r\n        word = values[0]\r\n\r\n        coefs = np.asarray(values[1:],  dtype='float32')\r\n\r\n        word_embeddings[word] = coefs\r\n\r\n    f.close()"", '![image](https://user-images.githubusercontent.com/49189425/66677305-2aa2d800-ec1e-11e9-9a1c-06adcbe68385.png)\r\nhelp plz', 'It worked with encoding=""utf8""!!']","[""\r\nembeddings_index = {}\r\nglove_data = 'glove.6B.50d.txt'\r\nf = open(glove_data)\r\nfor line in f:\r\n    values = line.split()\r\n    word = values[0]\r\n    coefs = np.asarray(values[1:], dtype='float32')\r\n    embeddings_index[word] = coefs\r\nf.close()\r\n"", ""\r\nUnicodeDecodeError                        Traceback (most recent call last)\r\n<ipython-input-72-ad0473c921c9> in <module>()\r\n      2 glove_data = 'glove.6B.50d.txt'\r\n      3 f = open(glove_data)\r\n----> 4 for line in f:\r\n      5     values = line.split()\r\n      6     word = values[0]\r\n\r\nC:\\Users\\Leonard\\Anaconda3\\lib\\encodings\\cp1252.py in decode(self, input, final)\r\n     21 class IncrementalDecoder(codecs.IncrementalDecoder):\r\n     22     def decode(self, input, final=False):\r\n---> 23         return codecs.charmap_decode(input,self.errors,decoding_table)[0]\r\n     24 \r\n     25 class StreamWriter(Codec,codecs.StreamWriter):\r\n\r\nUnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 2273: character maps to <undefined>\r\n""]",[],0,0
506,keras,12041,closed,Cannot use categorical_crossentropy,"Hello guys, i would like to know why i cannot use 'categorical_crossentropy'. 
I have 12 class both in training and validation folder.

the structure of the class like this

> database/validation/P1
> database/validation/...
> database/validation/P12

How to make this model should be able to compile with  'categorical_crossentropy'? Thanks

This is my actual code:


The terminal output:
categorical_crossentropycategorical_crossentropycategorical_crossentropysparse_categorical_crossentropy",,"[""I solve the issue by  changing train generator and validation generator into categorical in order to use categorial crossentropy\r\n\r\n```\r\ntrain_generator = train_datagen.flow_from_directory(\r\n    train_data_dir,\r\n    target_size=(img_width, img_height),\r\n    batch_size=batch_size,\r\n    class_mode='categorical')\r\n\r\nvalidation_generator = test_datagen.flow_from_directory(\r\n    validation_data_dir,\r\n    target_size=(img_width, img_height),\r\n    batch_size=batch_size,\r\n    class_mode='categorical')\r\n```""]","[""\r\nfrom keras import applications, optimizers\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.models import Sequential, Model\r\nfrom keras.layers import Conv2D, MaxPooling2D\r\nfrom keras.layers import Activation, Dropout, Flatten, Dense, ZeroPadding2D\r\nfrom keras import backend as K\r\nimport matplotlib.pyplot as plt\r\n\r\n# dimensions of our images.\r\nimg_width, img_height = 224, 224\r\n\r\ntrain_data_dir = 'database/train'\r\nvalidation_data_dir = 'database/validation'\r\nnb_train_samples = 46\r\nnb_validation_samples = 26\r\nepochs = 50\r\nbatch_size = 2\r\n\r\n\r\nif K.image_data_format() == 'channels_first':\r\n    input_shape = (3, img_width, img_height)\r\nelse:\r\n    input_shape = (img_width, img_height, 3)\r\n\r\n# build the VGG16 network\r\nvgg_conv = applications.VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\r\nprint('VGG Pretrained Model loaded.')\r\n\r\nvgg_conv.summary()\r\n\r\n# Freeze the layers except the last 4 layers\r\n# for layer in vgg_conv.layers[:-4]:\r\n#     layer.trainable = False\r\n\r\n# Check the trainable status of the individual layers\r\nfor layer in vgg_conv.layers:\r\n    print(layer, layer.trainable)\r\n\r\n# Create the model\r\nmodel = Sequential()\r\n\r\n# Add the vgg convolutional base model\r\nmodel.add(vgg_conv)\r\n\r\n# First\r\nmodel.add(Flatten())\r\nmodel.add(Dense(12))\r\nmodel.add(Activation('relu'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(1))\r\nmodel.add(Activation('softmax'))\r\n\r\nmodel.summary()\r\n\r\n\r\n# this is the augmentation configuration we will use for training\r\ntrain_datagen = ImageDataGenerator(\r\n    rescale=1. / 224,\r\n    shear_range=0.2,\r\n    zoom_range=0.2,\r\n    horizontal_flip=True)\r\n\r\n# this is the augmentation configuration we will use for testing:\r\n# only rescaling\r\ntest_datagen = ImageDataGenerator(rescale=1. / 224)\r\n\r\ntrain_generator = train_datagen.flow_from_directory(\r\n    train_data_dir,\r\n    target_size=(img_width, img_height),\r\n    batch_size=batch_size,\r\n    class_mode='binary')\r\n\r\nvalidation_generator = test_datagen.flow_from_directory(\r\n    validation_data_dir,\r\n    target_size=(img_width, img_height),\r\n    batch_size=batch_size,\r\n    class_mode='binary')\r\n\r\n# compile model\r\n# model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizers.RMSprop(lr=2e-4), metrics=['accuracy'])\r\nmodel.compile(loss='categorical_crossentropy',\r\n              optimizer=optimizers.RMSprop(lr=2e-4),\r\n              metrics=['accuracy'])\r\n\r\n# Train the model\r\nhistory = model.fit_generator(\r\n    train_generator,\r\n    steps_per_epoch=nb_train_samples / batch_size,\r\n    epochs=epochs,\r\n    validation_data=validation_generator,\r\n    validation_steps=nb_validation_samples / batch_size)\r\n\r\n# Save the model\r\nmodel.save('vgg16_pretrained_2.h5')\r\n\r\n\r\n\r\n# Check Performance\r\nacc = history.history['acc']\r\nval_acc = history.history['val_acc']\r\nloss = history.history['loss']\r\nval_loss = history.history['val_loss']\r\n\r\nepochs = range(len(acc))\r\n\r\nplt.plot(epochs, acc, 'b', label='Training acc')\r\nplt.plot(epochs, val_acc, 'r', label='Validation acc')\r\nplt.title('Training and validation accuracy')\r\nplt.legend()\r\n\r\nplt.figure()\r\n\r\nplt.plot(epochs, loss, 'b', label='Training loss')\r\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\r\nplt.title('Training and validation loss')\r\nplt.legend()\r\n\r\nplt.show()\r\n""]","['', '\r\nVGG Pretrained Model loaded.\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\ninput_1 (InputLayer)         (None, 224, 224, 3)       0\r\n_________________________________________________________________\r\nblock1_conv1 (Conv2D)        (None, 224, 224, 64)      1792\r\n_________________________________________________________________\r\nblock1_conv2 (Conv2D)        (None, 224, 224, 64)      36928\r\n_________________________________________________________________\r\nblock1_pool (MaxPooling2D)   (None, 112, 112, 64)      0\r\n_________________________________________________________________\r\nblock2_conv1 (Conv2D)        (None, 112, 112, 128)     73856\r\n_________________________________________________________________\r\nblock2_conv2 (Conv2D)        (None, 112, 112, 128)     147584\r\n_________________________________________________________________\r\nblock2_pool (MaxPooling2D)   (None, 56, 56, 128)       0\r\n_________________________________________________________________\r\nblock3_conv1 (Conv2D)        (None, 56, 56, 256)       295168\r\n_________________________________________________________________\r\nblock3_conv2 (Conv2D)        (None, 56, 56, 256)       590080\r\n_________________________________________________________________\r\nblock3_conv3 (Conv2D)        (None, 56, 56, 256)       590080\r\n_________________________________________________________________\r\nblock3_pool (MaxPooling2D)   (None, 28, 28, 256)       0\r\n_________________________________________________________________\r\nblock4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160\r\n_________________________________________________________________\r\nblock4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808\r\n_________________________________________________________________\r\nblock4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808\r\n_________________________________________________________________\r\nblock4_pool (MaxPooling2D)   (None, 14, 14, 512)       0\r\n_________________________________________________________________\r\nblock5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808\r\n_________________________________________________________________\r\nblock5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808\r\n_________________________________________________________________\r\nblock5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808\r\n_________________________________________________________________\r\nblock5_pool (MaxPooling2D)   (None, 7, 7, 512)         0\r\n=================================================================\r\nTotal params: 14,714,688\r\nTrainable params: 14,714,688\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n<keras.engine.topology.InputLayer object at 0x000001C834ABD0F0> False\r\n<keras.layers.convolutional.Conv2D object at 0x000001C848B69828> True\r\n<keras.layers.convolutional.Conv2D object at 0x000001C848319A20> True\r\n<keras.layers.pooling.MaxPooling2D object at 0x000001C8482E72E8> True\r\n<keras.layers.convolutional.Conv2D object at 0x000001C848C00CC0> True\r\n<keras.layers.convolutional.Conv2D object at 0x000001C848C0E0F0> True\r\n<keras.layers.pooling.MaxPooling2D object at 0x000001C848C18DD8> True\r\n<keras.layers.convolutional.Conv2D object at 0x000001C848C305F8> True\r\n<keras.layers.convolutional.Conv2D object at 0x000001C848C38EB8> True\r\n<keras.layers.convolutional.Conv2D object at 0x000001C848C40EF0> True\r\n<keras.layers.pooling.MaxPooling2D object at 0x000001C848C4CC50> True\r\n<keras.layers.convolutional.Conv2D object at 0x000001C848C62710> True\r\n<keras.layers.convolutional.Conv2D object at 0x000001C848C629B0> True\r\n<keras.layers.convolutional.Conv2D object at 0x000001C848C7B5C0> True\r\n<keras.layers.pooling.MaxPooling2D object at 0x000001C848C86DA0> True\r\n<keras.layers.convolutional.Conv2D object at 0x000001C848C99860> True\r\n<keras.layers.convolutional.Conv2D object at 0x000001C848CA4D68> True\r\n<keras.layers.convolutional.Conv2D object at 0x000001C848CAD710> True\r\n<keras.layers.pooling.MaxPooling2D object at 0x000001C848CB8EB8> True\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\nvgg16 (Model)                (None, 7, 7, 512)         14714688\r\n_________________________________________________________________\r\nflatten_1 (Flatten)          (None, 25088)             0\r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 12)                301068\r\n_________________________________________________________________\r\nactivation_1 (Activation)    (None, 12)                0\r\n_________________________________________________________________\r\ndropout_1 (Dropout)          (None, 12)                0\r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 1)                 13\r\n_________________________________________________________________\r\nactivation_2 (Activation)    (None, 1)                 0\r\n=================================================================\r\nTotal params: 15,015,769\r\nTrainable params: 15,015,769\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nFound 46 images belonging to 12 classes.\r\nFound 26 images belonging to 12 classes.\r\nEpoch 1/50\r\nTraceback (most recent call last):\r\n  File ""C:/Users/w024029h/PycharmProjects/keras_pretrained/vgg16_2.py"", line 92, in <module>\r\n    validation_steps=nb_validation_samples / batch_size)\r\n  File ""C:\\Users\\w024029h\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\legacy\\interfaces.py"", line 91, in wrapper\r\n    return func(*args, **kwargs)\r\n  File ""C:\\Users\\w024029h\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\models.py"", line 1315, in fit_generator\r\n    initial_epoch=initial_epoch)\r\n  File ""C:\\Users\\w024029h\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\legacy\\interfaces.py"", line 91, in wrapper\r\n    return func(*args, **kwargs)\r\n  File ""C:\\Users\\w024029h\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\training.py"", line 2230, in fit_generator\r\n    class_weight=class_weight)\r\n  File ""C:\\Users\\w024029h\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\training.py"", line 1877, in train_on_batch\r\n    class_weight=class_weight)\r\n  File ""C:\\Users\\w024029h\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\training.py"", line 1493, in _standardize_user_data\r\n    self._feed_output_shapes)\r\n  File ""C:\\Users\\w024029h\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\training.py"", line 256, in _check_loss_and_target_compatibility\r\n    \' while using as loss ', "". '\r\nValueError: You are passing a target array of shape (2, 1) while using as loss "", '. ', ' expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can con\r\nvert them to the expected format via:\r\n\r\nfrom keras.utils import to_categorical\r\ny_binary = to_categorical(y_int)\r\n\r\n\r\nAlternatively, you can use the loss function ', ' instead, which does expect integer targets.\r\n\r\n', '']",0,0
507,keras,7352,closed,One problem for examples/imdb_fasttext.py,"I found one small bug for function 'add_ngram' in the file examples/imdb_fasttext.py. When adding n grams, the grams with size of 1, 2,..., n-1 should be all added according to the original paper. However, this function cannot fully achieve this goal due to these two lines: ""for i in range(len(new_list) - ngram_range + 1):""; ""for ngram_value in range(2, ngram_range + 1):"". I think the positions of the two lines should be swapped so that it looks like this: ""for ngram_value in range(2, ngram_range + 1): for i in range(len(new_list) - ngram_value + 1):"". Please check on this issue to make updates. Thank you!",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],0,0
508,keras,61,closed,Adding Batch Size as explicit parameter for Batch Normalization layer,"I think it would be much more clear and easy to have a ""batch size""  parameter separately for Batch Normalization layer.  We can just directly pass the outputs of our convolution or pooling layers to it. The layers as a whole will be more coherent. 
(As an aside,  did anyone have any luck with batch normalization?  I tried many times,  but actually got worse results most of the time.) 
",,"["" As an example, batch normalization seems to have a significant positive impact on the Reuters example. \n\nBatch size during training is assumed to not be fixed, and may vary when iterating over the data (in particular, if you use the fit() method of models.Sequential, the last batch will generally be smaller). How would a batch_size layer parameter work?\n\nI also don't see the argument for introducing one. Can you expand on that?\n"", 'Good to hear about the Reuters example. \n\nRegarding my argument :\nWhat you said is exactly what I\'m trying to point out. There are 2 separate things here:\n1. Batch Normalization layer in practice would look something like ""Model.add(BatchNormalization(batch_size, stack_size,  nb_rows,  nb_columns)).  The last three parameters are something we calculate,  and they are similar to the parameters of other layers.  So,  I just suggested if it would be better to have it has BatchNormalization(stack_size,  nb_rows,  nb_columns, batch_size=32). Not much difference between them,  but. I thought, since it is ""batch""  normalization,  it would more explicit and clear. \n2. Like you said,  the last batch size < batch_size usually.  But a batch normalization layer,  with batch size passed in,  to the constructor will give you an error if a batch of lesser items is fed into the network .\n', 'The current implement is not dependent on any batch size. The input_shape parameter is assumed to be the shape of a **single** sample, not of a batch. It seems that this where the confusion originates.\n\nThis parameter input_shape is used to determine the shape of the internal gamma and beta parameters, which are used as such:\n\n```\nout = self.gamma * X_normed + self.beta\n```\n\nHere X_normed has shape `(batch_size,) + input_shape` (which is the shape of the whole batch fed as input), and gamma and beta have shape `input_shape`. Theano handles these operations like numpy would.\n\nI will update the documentation to reflect this.\n', ""You're right \n""]",[],[],0,0
509,keras,2378,closed,Not able to resume training after loading model + weights,"I work at an institute where it is not allowed to run a workstation overnight, hence I had to split the training process into multiple days. I trained a model for 10 epochs which took approximately 1 day, and saved the model + weights using the methods described in keras documentation like this:



and load the model the next day like this:



but when I restarted the training process it initialized to the same training and validation loss that I had got the earlier day at the 1st epoch! It should have started with an accuracy of 60% which was the last best accuracy I got the earlier day, but it doesn't. 

I have also tried to call model.compile() before and after load_weights, as well as leaving it out altogether, but that doesn't work either. 

Please help me in this regard. Thanks in advance. 
",,"['Does it work when you construct the model with the original code instead of loading it from json?\n', ""Nope. It doesn't. Still starts with 20% accuracy as it did on the 1st epoch.\n"", 'Did the weights file already exist before you tried to save them?\n', 'It did, but now I tried using the ModelCheckpoint callback which saves weight files for each epoch. In my case last weights file for epoch 70 was created (it was not present), I tried loading that into the model loaded using i) JSON ii) using original code, but still no luck. \n', ""> It did\n\nThat's it, `save_weights()` doesn't overwrite existing files unless you also pass `overwrite=True`. It should have asked for user input, though.\n"", ""Actually sorry for my last comment, all the architectures I save and all weights I save have unique names, and yes I know `save_weights()` asks for user input when overwriting the file, but in my case it doesn't since the files do not exist. Se we can safely rule out the possibility that the file was not overwritten. \n"", '![screen](https://cloud.githubusercontent.com/assets/10273405/14606107/50d35708-0599-11e6-9ea9-d0121a3ff9a1.png)\nYou can see the weights saved after every epoch. When I try to load these weights the training still restarts from where it started initially.\n\nHere\'s my full `loadModel()` function:\n\n```\n# optimzers\noptim_sgd = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.002, nesterov=True)\noptim_adadelta = keras.optimizers.Adadelta()\noptim_adagrad = keras.optimizers.Adagrad()\noptim_adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n\nimageSize = (19, 19)\nimg_rows, img_cols = imageSize[0], imageSize[1]\nbatch_size = 200\n# number of convolutional filters to use\nnb_filters = 32\n# size of pooling area for max pooling\nnb_pool = 2\n# convolution kernel size\nnb_conv = 3\n\nnb_epoch = 1000\n\n# callbacks\ndef scheduler(epoch):\n    if epoch % 10 == 0 and epoch is not 0:\n        x = float(input(""Enter a learning rate (Current: {}): "".format(model.optimizer.lr.get_value())))\n        model.optimizer.lr.set_value(x)\n        print(""Changed learning rate to: {}"".format(model.optimizer.lr.get_value()))\n    return model.optimizer.lr.get_value()\n\nchange_lr = oc.LearningRateScheduler(scheduler)\nearly_stop = oc.StopEarly(10)\nplot_history = oc.PlotHistory()\n\n# # Load the model\nmodelPath = \'./SegmentationModels/\'\nmodelName = \'Arch_1_40\'\nmodel = model_from_json(open(str(modelPath + modelName + \'.json\')).read())\nmodel.compile(loss=\'categorical_crossentropy\', optimizer=optim_sgd)\nmodel.load_weights(str(modelPath + \'weights.70-0.74.hdf5\'))\n#     import cPickle as pickle\n#     with open(str(modelPath + modelName + \'_hist.pckl\'), \'r\') as f:\n#         history = pickle.load(f)\nmodel.summary()\n```\n', ""That's strange... \n\n~~[replace this line](https://github.com/fchollet/keras/blob/55159cf451cfe621745e5a0a7f96391f8994cdc5/keras/engine/topology.py#L2272) with `if 1:` and try to load the weights again.~~ nope, dont\n"", ""I found out that I was using an older version of Keras. I upgraded the version and found `model_summary()` is no longer there. Delved deeper and found that it has now been changed to `print_summary()`.\n\nAnyways, I tried changing the line of code you asked, but that didn't work as well. \n"", ""UPDATE: Came to the institute this morning, built the model using original code and loaded the model weights saved using ModelCheckpoint callback. Started training and it still restarts from the beginning; no memory of past metrics. The performance is actually even worse than it was earlier when it started training the first epoch. In my case, normally the network starts at 20% accuracy and goes to around 70% in 60 epochs. But when I restart the training process using loaded weights, the network starts at 20% on epoch 1 and keeps going lower and lower until 16% at epoch 5. I have no idea what's happening here. \n\nUPDATE 2: When I try to evaluate the loaded model + weights on the same validation data, I get 60% accuracy, as intended. But if I do `model.fit()` then training starts from 20% and oscillates on it. So I can confirm that the weights are being loaded correctly since the model can make predictions, but the model is not able to retrain. \n\nPlease help! @NasenSpray \n"", ""So what model do you have precisely? Perhaps some weights aren't actually saved or loaded at all (like the states in a LSTM or something)? Or perhaps they are accidentally shuffled (flipped dimensions or whatever) somehow.\n\nEDIT: Check https://github.com/fchollet/keras/issues/2378#issuecomment-211910392\n"", ""Grasping at straws here, but some optimizers are stateful right? Are you just using SGD? I'm not familiar with this part of Keras but perhaps the optimizers should be saved as well because otherwise when you reinitiate learning and start a new epoch but with pretrained weights instead of your original weight initialization, perhaps training diverges due to high learning rates.\n"", 'Run this plz\n\n```\nmodel  = make_model()\nw1 = model.get_weights()\nmodel.load_weights(\'your_saved_weights.h5\')\nw2 = model.get_weights()\n\nfor a,b in zip(w1, w2):\n  if np.all(a == b):\n    print ""wtf is happening""\n```\n\nDoes it print?\n', ""Doesn't print. The weights are loaded successfully I suppose. Its the training procedure that's problematic. After running this script (it didn't print anything), I ran `model.fit()` and it started with a loss 10x higher than originally it was at epoch 1, and with accuracy 20% again _sigh_\n"", ""Obviously something must be different as you're seeing different results. Perhaps get_weights() doesn't actually return everything it could.\n\nI'm curious if you have the same problem just by restarting training in the same session, nevermind loading a model and its weights with Keras' builtins. If not, consider saving states with [something like this](https://docs.python.org/2/library/shelve.html) instead.\n"", 'Thanks. When I restart an interrupted training process, the training continues from where it left of successfully. The problem is when I load the model and weights. \n\nMy main aim to save ""snapshots"" or ""states"" of the model that can be loaded back and used as a starting point when training on the other day. I\'ll have a look at the `shelf` module too, thanks!. But I think the problem with Keras must be debugged as well. \n\nPlease guide me on how can I help you reproduce this issue for you guys to fix it sometime in the future. I would love to help. \n', 'In your `loadModel()`, hardcode the learning rate to 0. Does it make the loaded model better?\n\n\\- and -\n\nInstead of training, just evaluate the loaded model on the training set. Still worse than before?\n', ""I'll try your suggestion as soon as I get to the institute tomorrow. \n"", 'I could not try validating on the training set for some reason, but I solved my problem by pickling the model after training it for the day. I restarted my iPython Notebook kernel, loaded the pickled model, and restarted the training process. Fortunately it started from where it left of. \n\nI will also try your suggestion and report back what I got. \n', ""Dang! That clearly means some states are not saved properly, whether they are weights or something else.\n\nI assume the intended use case for having load and save functions in Keras has more to do with being able to share pretrained models like people do with Caffe, rather than it being for pausing your own training, in which pickling is probably safer.\n\nI do wonder though if it wouldn't be easier to just scratch the manual parsing of states which is bug-prone and simply have everything rely on Python's builtin object serialization with pickle, shelve or similar. Keras' builtins are pretty meaty though so I'm probably missing something important in why they're needed.\n\nI could do a pr with shelve for save_model(...), load_model(...), save_weights(...), load_weights(...) if it is of interest @fchollet.\n"", 'From what @carlthome said [here](https://github.com/fchollet/keras/issues/2378#issuecomment-211901705), you could try to take a snapshot of the optimizer too.\nI have 2 functions working to be able to serialize the model and the optimizer as in the pre 1.0 release. Note that I return a dictionnary instead of a json dump. It\'s basically something really similar to the old functionnalities.\nYou could try them and let me know if it\'s working (I didn\'t have the time to really test them extensively):\n\n``` python\n\ndef get_function_name(o):\n    """"""Utility function to return the model\'s name\n    """"""\n    if isinstance(o, six.string_types):\n        return o\n    else:\n        return o.__name__\n\ndef to_dict_w_opt(model):\n    """"""Serialize a model and add the config of the optimizer and the loss.\n    """"""\n    config = dict()\n    config_m = model.get_config()\n    config[\'config\'] = {\n        \'class_name\': model.__class__.__name__,\n        \'config\': config_m,\n    }\n    if hasattr(model, \'optimizer\'):\n        config[\'optimizer\'] = model.optimizer.get_config()\n    if hasattr(model, \'loss\'):\n        if isinstance(model.loss, dict):\n            config[\'loss\'] = dict([(k, get_function_name(v))\n                                   for k, v in model.loss.items()])\n        else:\n            config[\'loss\'] = get_function_name(model.loss)\n\n    return config\n\n\ndef model_from_dict_w_opt(model_dict, custom_objects=None):\n    """"""Builds a model from a serialized model using `to_dict_w_opt`\n    """"""\n    if custom_objects is None:\n        custom_objects = {}\n\n    model = layer_from_config(model_dict[\'config\'],\n                              custom_objects=custom_objects)\n\n    if \'optimizer\' in model_dict:\n        model_name = model_dict[\'config\'].get(\'class_name\')\n        # if it has an optimizer, the model is assumed to be compiled\n        loss = model_dict.get(\'loss\')\n\n        # if a custom loss function is passed replace it in loss\n        if model_name == ""Graph"":\n            for l in loss:\n                for c in custom_objects:\n                    if loss[l] == c:\n                        loss[l] = custom_objects[c]\n        elif model_name == ""Sequential"" and loss in custom_objects:\n            loss = custom_objects[loss]\n\n        optimizer_params = dict([(\n            k, v) for k, v in model_dict.get(\'optimizer\').items()])\n        optimizer_name = optimizer_params.pop(\'name\')\n        optimizer = optimizers.get(optimizer_name, optimizer_params)\n\n        if model_name == ""Sequential"":\n            sample_weight_mode = model_dict.get(\'sample_weight_mode\')\n            model.compile(loss=loss,\n                          optimizer=optimizer,\n                          sample_weight_mode=sample_weight_mode)\n        elif model_name == ""Graph"":\n            sample_weight_modes = model_dict.get(\'sample_weight_modes\', None)\n            loss_weights = model_dict.get(\'loss_weights\', None)\n            model.compile(loss=loss,\n                          optimizer=optimizer,\n                          sample_weight_modes=sample_weight_modes,\n                          loss_weights=loss_weights)\n    return model\n\n```\n\n@carlthome, if this solution is ok, we could work on a PR that includes these functionnalities and the other relevant elements (weights, states, ...)?\nIt should be possible to include all of this in a HDF5 file.\n', ""@tboquet, cool! Sounds good to me! I'm no authority on Keras but I would probably have based loading/saving around object serialization of Model() and Sequential() just to be safe. In the future, new things will probably be stateful which will screw up things again. The slight additional overhead of saving too much is worth the extra stability and reduced code complexity, in my mind.\n"", ""This is what I am using (took from [keras docs](http://keras.io/getting-started/faq/#how-can-i-save-a-keras-model)) and it works without a problem on Keras 1.0:\n\n```\ndef load_model():\n    model = model_from_json(open('model.json').read())\n    model.load_weights('weights.h5')\n    model.compile(optimizer=rmsprop, loss='mse')\n    return model\n\n\ndef save_model(model):    \n    json_string = model.to_json()\n    open('model.json', 'w').write(json_string)\n    model.save_weights('weights.h5', overwrite=True)\n```\n\nI had one example with say 10 epochs and another example with save and load in a loop of 10 iterations each with 1 epoch, and the loss for both were similarly decreasing. Additionally both resulting models were predicting fine.\n\nHave you tried to call `model.load_weights` before `model.compile`?\n"", 'Thank you for your suggestions everyone. I will try your suggestions again and revert back what I got. If the method described on official Keras documentation works for everyone, it should for me too. I will dig a little deeper and find out if its something I am doing wrong. \n', ""I ran into a similar problem today. It really seems like it could be the optimizer that needs to be saved/loaded too, aside from the weights.\n\nBasically anything like this seems to go bonkers (in my case `loss='mse'` and `optimizer='rmsprop'`):\n\n``` Python\n# Starting fresh, training for a while and saving the weights to file.\nmodel = create_model()\nmodel.compile(...)\nmodel.fit(...)\nmodel.save_weights(...)\n\n# Creating the model again, but loading the previous weights and resuming training.\nmodel = create_model()\nmodel.compile(...)\nmodel.load_weights(...)\nmodel.fit(...) # Diverges!\n```\n\nThe data is the same in both fit calls.\n"", ""@carlthome Had the same problem. Didn't check recently for the current status but now I use vanilla cPickle to pickle my trained model. Loading the pickled model and resuming training seems to be working just as expected. However I'm not sure about the JSON + h5 weight saving/loading functionality. If you are having the same problem then there must be something wrong. \n"", '@carlthome: RMSprop makes _really_ shitty updates during the first couple of steps which easily wreck pre-trained models. Could you retry with plain SGD?\n', ""I also encountered this problem training a 2-layer LSTM with one dense layer at the end. Testing showed the following:\n\n-Compiling two identical models in the same script, training the first model and then loading the weights in the second model via save_weights and load_weights worked as it should even if the two models had separate optimizer instances. If I did this and then started training with the second model its training loss was the same as the training loss of the first model when the weights were saved, as expected.\n\n-However, once Python was closed and reopened loading weights saved in the previous instance resulted in, if anything, a -worse- loss at the start of training than the untrained model, though it quickly learned again.\n\n-I'm not sure if the optimizer is at fault, because I've tried saving the weights from a model, reloading them and then testing predictions without any further training. If the two models were compiled in the same session it works fine, but if I close the session, start a new session, compile a new model and load the previous session's weights then its predictions are garbage.\n"", ""Also, I'm using the Theano backend and training on Windows with CUDA, which is probably a weird use-case. Not sure what backend/OS the other people with this problem are using.\n"", ""Wait, I've been -extremely- stupid, please ignore.\n\nFor the interested, I was making a character-prediction RNN with a one-hot character encoding, but instead of pickling the map of characters to one-hot indices I was generating it in the code each time from a set of allowed characters using enumerate(). This of course meant that the mapping generated by enumerate() was different every time, because sets have no guaranteed order, which explains why everything worked fine until I restarted the script (and so regenerated the mapping).\n\nThis is embarrassingly obvious in retrospect.\n"", ""I'm having this same issue using adagrad. After hours of training, when I load weights and resume, my MSE goes back up to where it started on the first epoch (the first epoch ever, where it was using the initial random weights).\n\nWhat's the disadvantage of using vanilla cPickle instead of the save_weights and to_json (which don't seem to work unless you're using SGD)?\n"", ""Heyho. I'm new to Deep Learning and Keras and ran into the same / a similar issue. I trained my model with SGD for some time and saved the weights after each epoch using the `save_weights()` function. When I load weights from a particular epoch and I use SGD again, everything is fine (evaluation metrics are still good).\n\nAdditionally, I tried to use my already learned weights but use a different optimizer for further training. When choosing Adam, Adagrad or RMSprop the evaluation metrics dropped and it looked like as if the learning started from scratch.\n\nHow can this happen? Why is everything fine, when I use SGD again - _even with changed learning rate_ - but not when using a different Optimizer?\n\nThanks for your help!\n\n**EDIT**:\n\n> @carlthome: RMSprop makes really shitty updates during the first couple of steps which easily wreck pre-trained models. Could you retry with plain SGD?\n\n@NasenSpray Hmm. Could this be my problem? As far as I know all my chosen optimizers are related to RMSprop. Could they all 'destroy' my already learned weights and affect the performance in such negative way?\n"", ""It generally _not_ advisable to retrain a pretrained model on an altogether\ndifferent optimizer compared to what it was trained on. This just doesn't\nmake any sense. My question is - do you have a valid reason behind this\nsetting, where you want to train a pre-trained network using a different\noptimizer like RMSProp or Adam?\n\nOn Tue, Aug 9, 2016 at 6:17 PM, Thomas notifications@github.com wrote:\n\n> Heyho. I'm new to Deep Learning and Keras and ran into the same / a\n> similar issue. I trained my model with SGD for some time and saved the\n> weights after each epoch using the save_weights() function. When I load\n> weights from a particular epoch and I use SGD again, everything is fine\n> (evaluation metrics are still good).\n> \n> Additionally, I tried to use my already learned weights but use a\n> different optimizer for further training. When choosing Adam, Adagrad or\n> RMSprop the evaluation metrics dropped and it looked like as if the\n> learning started from scratch.\n> \n> How can this happen? Why is everything fine, when I use SGD again - _even\n> with changed learning rate_ - but not when using a different Optimizer?\n> \n> Thanks for your help!\n> \n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/fchollet/keras/issues/2378#issuecomment-238541753,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AJzCfX6axe8wAHs_UFXuAcreh9gLx69eks5qeHbcgaJpZM4IJZgO\n> .\n"", ""Does this also apply to partially pretrained models? For example if you have a network with 5 convolutional layers and you take the weights for the first 3 layers from a pretrained network (_transfer learning_) and set `trainable=False` for those layers? \n\nConcerning your question: As I wrote, I'm new to Keras and Deep Learning. I'm trying to get a feel for different techniques, so I'm playing around a bit and observing the resulting effects and trying to understand the behaviour.\n"", ""Sorry if I'm bumping an old thread - is this resolved for you folk?"", 'Just hit this myself. I think the confusion is model.load_weights() only loads the model weights, it does not load any of the intermediate state of the optimizer. If you want to completely reload a model the only option appears to be load_model().\r\n\r\nI would propose closing this issue as works-as-designed and updating the FAQ to make this a little bit more clear: if you want to resume training, your best option is load_model().', ""`load_model` isn't documented: https://keras.io/models/about-keras-models/"", 'But it is documented here: https://keras.io/getting-started/faq/', 'I have the same problem on keras 1.2.0. \r\nIt was fixed on 1.2.1.', 'is this fixed?', ""I've been using the latest version of keras. I can confirm this problem is not fixed even with `model.save`."", 'Any update?', 'model.save_weights() saves only the weights of the model. Instead try using model. save() and load_model() to save and reload the model respectively which saves the entire model state.\r\n\r\nmodel.save(""model.h5"", overwrite=True)\r\n.\r\n.\r\nmodel = load_model(""model.h5"")          """"""when reloading the model""""""\r\n ', '@shalabhsingh  does not help\r\n', 'Can someone with this issue *please* provide a complete and minimal example that reproduces the issue? There are tests in place to check that this does not happen, so we need to understand what is different from those tests to nail it down. Try to use a dataset from Keras, so we can all easily reproduce it. \r\n\r\nThanks!', ""@Rocketknight1 \r\nThanks, your posts made me aware I was doing the same thing. A lot of people might have this issue because the code referenced in\r\n\r\nhttps://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/\r\n\r\ngets exactly this wrong. This code section in RNN_utils.py \r\n\r\n```\r\ndata = open(DATA_DIR, 'r').read()\r\nchars = list(set(data))\r\nVOCAB_SIZE = len(chars)\r\n```\r\n\r\nshould be something like \r\n\r\n```\r\ndata = open(data_dir, 'r').read()\r\nchars = list(set(data))\r\nchars.sort() # SORT THE CHARS so mapping is the same even when restarting the script!\r\nVOCAB_SIZE = len(chars)\r\n```\r\n\r\ninstead so that the char mapping is always the same when reading the same file in a new session."", 'Ran into the same issue. Is it sorted folks?', ""I saved a model with `model.save(mdl)`, and load it with the following codes which works great.\r\n```python\r\n\tif mdl == None:\r\n\t\tmodel = Sequential()\r\n\t\tmodel.add(Dense(256, input_dim=n_input, activation='relu'))\r\n\t\tmodel.add(Dropout(0.5))\r\n\t\tmodel.add(Dense(256, activation='relu'))\r\n\t\tmodel.add(Dropout(0.3))\r\n\t\tmodel.add(Dense(128, activation='relu'))\r\n\t\tmodel.add(Dropout(0.1))\r\n\t\tmodel.add(Dense(n_classes, activation='softmax'))\r\n\t\r\n\t\tmodel.compile(loss='categorical_crossentropy',\r\n\t              optimizer='adam',\r\n\t              metrics=['accuracy'])\r\n\telse:\r\n\t\tmodel = load_model(mdl)\r\n        # Then train it with a usual way\r\n        model.fit_generator(...)\r\n\r\n```"", '@lionlai1989 Can you verify saving it from one session, and load it into different session? Or even load it and make predictions in different machine entirely? `model.save` and `load_model` does not work for me. If I load the model, the accuracy goes back to like it has never been trained.', ""Got the same issue. Fuck it, It's not solved. Spent 18 hours training a DenseNet on AWS to get to 89% accuracy on Cifar10, the connection interrupted but I thought I was safe because I had my model saved every 30 epochs. The truth is that it works for model.test(), but when I try model.fit(), it breaks and reverts to 10% accuracy when it was 89%. I've lost 1 day of work due to this shitty issue."", ""@EricAlcaide Would you mind providing your code? Because at least you can use it to test while my model can't do anything at all. Did you try using it to make predictions?"", ""So as far as I got it, it's still not possible to load the state of the optimizer other than using `load_model()`?\r\n\r\nInstead of using 'load_model', it would sometimes be nicer to define the nn-architecture, load the associated weights, load the saved optimizer and then compile again.\r\nThis would make it a lot easier to change nn-architectures in between epochs (e.g. add new layer, change dropout rate, etc.)."", 'hi,\r\n\r\nI think I found an answer in a different post. It\'s about the implementation of Adam and RMSProp etc in Tensorflow. When the model finds good weights on the first day it creates small losses, and Adam and other optimizers with a given learning rate ignores the previously adapted (and probably smaller) learning rate, and restarts the learning with the issue described below (basically: low errors are handled with a small epsilon). So saving the adapted learning rate could help too.\r\n\r\n[https://github.com/ibab/tensorflow-wavenet/issues/143](https://github.com/ibab/tensorflow-wavenet/issues/143)\r\n\r\nlet me quote here:\r\n\r\nExplanation\r\n\r\nI\'ve seen the behavior Zeta36 is describing in our test/test_model.py. When that test uses adam or rmsprop, I would see the loss drop and drop till a small number, then jump up to a large loss at some random time.\r\n\r\nYou can reproduce that problematic behavior if you change (what at the time of this writing in master) MomentumOptimizer to AdamOptimizer, make the learning rate 0.002 and delete the momentum parameter. Uncomment the statements that print loss.\r\n\r\nIf you run the test with\r\n\r\npython test/test_model.py\r\n\r\nevery second or third time or so that you run the test you will see the loss will drop and then at some point jump up to a larger value, and sometimes cause the test to fail. I ""worked around"" that problem by futzing with the learn rate and number of training iterations we run in the test until it would reliably pass.\r\n\r\nAnyway, I think I\'ve found the cause. If you look at the tensorflow implementations of rmsprop and adam you will see they compute the change to a weight by dividing by a sinister lag-filtered rate-of-change or error magnitude. When the error or rate of change of error gets small, or even zero, near the bottom of the error basin, then the denominator gets close to zero. The only thing saving us from a NaN or Inf is they add an epsilon in the denominator. That epsilon defaults to 1e-10 for rmsprop and 1e-8 for adam. That\'s enough to make the change to our parameter a big number, presumably big enough give us a large loss.\r\n\r\nSo in PR 128 I specified a larger epsilon for rmsprop, and in PR 147 for adam. I found that these changes fix the problem of randomly increasing loss during the tests in test_model.py.\r\n', ""I just read through this and I have the same issue (Adam optimizer).  If you are having troubles reproducing it, you can pull this down: https://github.com/brainstain/CapsNet\r\n\r\nJust change the epochs to 2 and run it twice.  I'll try @guyko81 's advice and save off the learning rate in a bit.  I'll post an update on whether the symptoms are improved."", 'Hello everybody, does this issue still hold with latest keras? i have an MLP network which i want to re-train once i have more data, ie incremental fitting. The model will be saved and loaded for preciction and also loaded to be re-trained with more data without losing previous performance if possible. How can i do that? Please advise.\r\n\r\nThanks,\r\nNikos', 'Lowering the learning rate on the second instance helped me.\r\n\r\nBut you can save the whole model with model.save\r\nhttps://keras.io/getting-started/faq/\r\n""\r\nHow can I save a Keras model?\r\nSaving/loading whole models (architecture + weights + optimizer state)\r\nIt is not recommended to use pickle or cPickle to save a Keras model.\r\n\r\nYou can use model.save(filepath) to save a Keras model into a single HDF5 file which will contain:\r\n\r\nthe architecture of the model, allowing to re-create the model\r\nthe weights of the model\r\nthe training configuration (loss, optimizer)\r\nthe state of the optimizer, allowing to resume training exactly where you left off.\r\n""\r\n\r\nI mean at Adam we don\'t really change the learning rate, we rather calculate the moving average of the gradient and the square of it at weight level, so you need to save everything!', ""I think this issue has been fixed. Use model.save() to save the model, and import the saved model using:\r\n```\r\nfrom keras.models import load_model\r\nmodel = load_model('my_model.h5')\r\n```\r\nIf you want to resume training it should work as this function saves all the optimiser information as well. "", ""Nope, I don't think the issue has been solved. For me its still the case that after training for a long time I reach a particular accuracy, save the model with `ModelCheckpoinnt` which essentially is using `model.save()` and then when I resume training I get a 20% difference from the actual accuracy when I first saved the model. Which means I still have to spend a whole lotta of time and energy to resume training. Not good for the environment either. The optimizer here is `Adam`. "", ""I encountered the same problem using load_weights.\r\nBut my situation is a little bit complex, I have 2 models sharing some layers.\r\nSo I can't use 'load_model' here, what can I do to resume training?\r\n\r\nAnd I know it's rare, but what if someone wants to inherit the Model?\r\nWon't it be quite problematic to load a subclass of Model using 'load_model'?"", ""indeed not solved, same issue for me\r\ntried 2 things, both did not work\r\n```\r\n1)\r\nfrom keras.models import load_model\r\nmodel = load_model('my_model.h5')\r\n\r\n2)\r\ndef load_model():\r\n    model = model_from_json(open('trainingmodel.json').read())\r\n    model.load_weights('weights.h5')\r\n    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\r\n    return model\r\n\r\n\r\ndef save_model(model):    \r\n    json_string = model.to_json()\r\n    open('trainingmodel.json', 'w').write(json_string)\r\n    model.save_weights('weights.h5', overwrite=True)`\r\n```\r\n\r\ntested with latest versions\r\n-Keras 2.1.5\r\n-Python 3.6.3\r\n-Tensorflow 1.5.0\r\n\r\nBut Keras does give this warning  which points to the problem.  In my network I use the output of 1 LSTM (the encoder) to initialize the input of the second (decoder).   Apparentely this input can not be serialized...  So, I'm stuck...\r\n\r\n`C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py:2368: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\r\n  str(node.arguments) + '. They will not be included '`"", 'Same issue here, and also with LSTM. Keras 2.1.2, TF 1.5, Python 3.6. When I am loading using load_model(), training continues well. But, when I am loading only weights, all the following iterations never improve loss/val_loss.\r\n\r\n```\r\nmodel.fit(...)\r\nmodel.save(""lstm_model.h5"")\r\n\r\nif os.path.isfile(""lstm_model.h5""):\r\n   #1: model = load_model(""lstm_model.h5"")    # training continues well\r\n   #2: model.load_weights(""lstm_model.h5"")    # losses never fall anymore\r\n\r\nmodel.fit(...)\r\n```', 'I\'m having the same issue. Whenever I run the ""fit"" function of a saved model, all weights go bad and my predictions are wrong.\r\n\r\n\r\n```\r\nopt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, decay=0.01)\r\nrnn_model.compile(loss=\'binary_crossentropy\', optimizer=opt, metrics=[""accuracy""])\r\nrnn_model.save(\'./models/my_model.h5\')\r\n\r\n#This predicts correctly\r\nmodel = load_model(\'my_model.h5\')\r\nmodel.predict(x)\r\n\r\n#This does NOT predict correctly\r\nmodel=load_model(\'my_model.h5\')\r\nmodel.fit(X, Y, batch_size = 5, epochs=1)\r\nmodel.predict(x)\r\n\r\n```\r\n\r\nIt looks like it\'s not able to resume training. Anyone have any suggestions, please?\r\n\r\n**Update:**\r\nI haven\'t figured out the root of the problem. But it seems that the model that I was loading was saved on Keras 2.0.6 and I am loading it on to Keras 2.1.5. Something with the ""save_weights"" and ""load_weights"" functions was not working, so I had to load the weights layer by layer on an architecture I built from scratch manually (loading the architecture from the saved model using json worked as well):\r\n\r\n```\r\nfor layer_loaded, layer_built in zip(loaded_model,built_model):\r\n   layer_built.set_weights(layer_loaded.get_weights())\r\n```', 'I am having the same problem, using model.save and load_model() but somehow lose the earlier training when I use ModelCheckpoint and a custom callback to compute AUROC on my loaded model. If I `del model`, load the model and train the model in the same instance **without** re running my callbacks code, it continues to train from the previous run. Any ideas? Using keras 2.1.4 and python 3.6.4\r\n```\r\nearly = EarlyStopping(monitor=""val_loss"", mode=""min"", patience=3)\r\n\r\nauroc = MultipleClassAUROC(sequence=validation_sequence,\r\n                           class_names=class_names,\r\n                           weights_path=output_weights_path,\r\n                           stats=training_stats,\r\n                           workers=8)\r\ncheckpoint = ModelCheckpoint(output_weights_path, verbose=1, \r\n                             save_best_only=True, save_weights_only = True)\r\ncallbacks = [early, checkpoint, auroc,\r\n            ReduceLROnPlateau(monitor=\'val_loss\', factor=0.1, patience=patience_reduce_lr,\r\n                              verbose=1, mode=""min"", min_lr=min_lr)]\r\n```\r\n\r\nand my model.fit_generator\r\n\r\n```\r\nhistory=model.fit_generator(generator=train_sequence,\r\n                            steps_per_epoch=10,\r\n                            epochs=1,\r\n                            validation_data=validation_sequence,\r\n                            validation_steps=validation_steps,\r\n                            callbacks=callbacks,\r\n                            class_weight=class_weights,\r\n                            workers=8,\r\n                            shuffle=False, verbose=1)\r\n```', 'I got the same problem. Anyone has an idea?\r\n\r\n', 'I got the same problem!', ""I got the same problem too, any ideas?\r\n\r\nMy architecture is:\r\n# Encoder part\r\n```python\r\nencoder_input_layer = Input(shape=(None,), name='encoder_Input')\r\nencder_embedding_layer = Embedding(src_token_num, embedding_dim, name='encoder_Embedding')(encoder_input_layer)\r\nencoder_lstm_1_layer = LSTM(embedding_dim, return_sequences=True, return_state=True, name='encoder_LSTM_1')(encder_embedding_layer)\r\nencoder_lstm_2_layer = LSTM(embedding_dim, return_sequences=True, return_state=True, name='encoder_LSTM_2')(encoder_lstm_1_layer)\r\nencoder_output, state_h, state_c = LSTM(embedding_dim, return_state=True, name='encoder_LSTM_Final')(encoder_lstm_2_layer)\r\nencoder_states = [state_h, state_c]\r\n```\r\n# Decoder part\r\n```python\r\ndecoder_input_layer = Input(shape=(None,), name='decoder_Input')\r\ndecoder_embedding_output = Embedding(trgt_token_num, embedding_dim, name='decoder_Embedding')(decoder_input_layer)\r\n# Use encoder states to initialize decoder LSTM\r\ndecoder_lstm_1_layer = LSTM(embedding_dim, return_sequences=True, return_state=True, name='decoder_LSTM_1')\r\ndecoder_lstm_1_layer_output = decoder_lstm_1_layer(decoder_embedding_output, encoder_states)\r\ndecoder_lstm_2_layer = LSTM(embedding_dim, return_sequences=True, return_state=True, name='decoder_LSTM_2')\r\ndecoder_lstm_2_layer_output = decoder_lstm_2_layer(decoder_lstm_1_layer_output)\r\n# State_h and state_c discarded.\r\ndecoder_lstm_fianl_layer = LSTM(embedding_dim, return_sequences=True, return_state=True, name='decoder_LSTM_Final')\r\ndecoder_lstm_output, _, _ = decoder_lstm_fianl_layer(decoder_lstm_2_layer_output)\r\n# Classify words\r\ndecoder_dense_1_layer = Dense(embedding_dim, activation='relu', name='decoder_Dense_1_relu')\r\ndecoder_dense_1_output = decoder_dense_1_layer(decoder_lstm_output)\r\ndecoder_dense_final_layer = Dense(trgt_token_num, activation='softmax', name='decoder_Dense_Final')\r\ndecoder_dense_output = decoder_dense_final_layer(decoder_dense_1_output)\r\n```\r\n# Final Model\r\n```python\r\nencoder_decoder_model = Model(inputs=[encoder_input_layer, decoder_input_layer], outputs=decoder_dense_output)\r\n```\r\n![image](https://user-images.githubusercontent.com/16774189/38795660-8c6e765a-418b-11e8-98c3-dbd66ff0394e.png)\r\n\r\nWhen I tried to save model, I got warning as follows:\r\n![image](https://user-images.githubusercontent.com/16774189/38795716-b2b19496-418b-11e8-9bc8-ed0713ce2eb2.png)\r\n\r\nWhen I load saved model, and ran \r\n```python\r\nmodel.fit()\r\n```\r\nthe training accuracy is close to 0!!!"", 'Hello everyone, I faced the same problem. But I think it\'s solved in my case.\r\nFirst save the model and weights as in the code below....\r\n\r\n#Save the final model\r\nmodel_json = model.to_json()\r\nmdl_save_path = \'model.json\'\r\nwith open(mdl_save_path, ""w"") as json_file:\r\n    json_file.write(model_json)\r\n###serialize weights to HDF5\r\nmdl_wght_save_path = \'model.h5\'\r\nmodel.save_weights(mdl_wght_save_path)\r\n\r\nThen I started another session completely closing all python opened files for retraining.I also tried this by checkpointing the model while training. At the time of resuming training, I first loaded the model architecture from .json file and then loaded the weights using load_weights() from .h5 file. Then compiled the model using model.compile() and fit it with model.fit().\r\n\r\nN.B: I used SGD at both times while training and resuming training.....It worked.........\r\n\r\nThough I did not check this with other optimizers. I saw that at the time of retraining if I use other optimizers other than SGD(I used SGD in normal training), the issue persists. So, I am pretty confident using different optimizers during normal training and resumed training will cause you a problem.\r\n', 'Guys,\r\n\r\nI fixed the problem by reducing the learning rate to 1e-5 (small lr for Adam) when I fine tune my pretrained model, which has been trained using Adadelta with a much higher starting lr. I think, the issue is the starting lr for Adam that mess things up. For fine tuing, just use a small lr for a new optimiser of your choice.. Hope this helps.', 'Thanks @peymanrah this is exaclty the solution I needed!', '@peymanrah when you first trained the model with Adam, what learning rate did your training stop at. Trying to see by what factor I should decrease the lr for fine_tuning', '@guyko81 @peymanrah I agree with you! The main reason is that when we save the model after several epochs, its learning rate is pretty smaller. Therefore, once we load the same model and continue training it, we have to set the learning rate to its last value instead of the default value( default value is for the first epoch, it is very large).  \r\n\r\nThank you very much. This problem is fixed finally.', 'It is sad that such a basic issue has still not been solved. \r\n\r\nIt is happening because model.save(filename.h5) does not save the state of the optimizer. So the optimizers like Adam, RMSProp does not work but SGD works as mentioned in one of the previous comments (I verified this) since it is stateless optimizer (learning rate is fixed).\r\n\r\nThis is just sad that such a popular library has such basic/glaring/trivial bugs/problems :(', '> \r\n> \r\n> Guys,\r\n> \r\n> I fixed the problem by reducing the learning rate to 1e-5 (small lr for Adam) when I fine tune my pretrained model, which has been trained using Adadelta with a much higher starting lr. I think, the issue is the starting lr for Adam that mess things up. For fine tuing, just use a small lr for a new optimiser of your choice.. Hope this helps.\r\n\r\nReducing learning rate solved my problem, too. At first lr was 0.01 and then I reduced it to 0.001 at second try. After one epoch it returned to last state (in terms of acc and loss).\r\nNote than I just saved and loaded the weights.', ""> It is sad that such a basic issue has still not been solved.\r\n> \r\n> It is happening because model.save(filename.h5) does not save the state of the optimizer. So the optimizers like Adam, RMSProp does not work but SGD works as mentioned in one of the previous comments (I verified this) since it is stateless optimizer (learning rate is fixed).\r\n> \r\n> This is just sad that such a popular library has such basic/glaring/trivial bugs/problems :(\r\n@champnaman \r\n\r\nWhat states of Adam, RMSProp did you refer to? The weights(states) of the optimizers such as `RMPSProp` and `Adam` are saved except tensorflow optimizers `TFOptimizer` if you look at save_model() in [saving.py](https://github.com/keras-team/keras/blob/master/keras/engine/saving.py) (mine is keras 2.1 which differs from the current code but even the old version saves the optimizer states), which is confirmed by @fchollet in this [issue](https://github.com/keras-team/keras/issues/3704#issuecomment-245052225). And the tensorflow keras' [save_model](https://www.tensorflow.org/api_docs/python/tf/keras/models/save_model) document also confirmed this. \r\n\r\nAnd the learning rate (`lr`) of that epoch along with `epsilon`, `rho`, the whole optimizer instance are saved as well. `load_model() in saving.py` loads those hyperparemeters successfully in my case. However, my loaded loss is very different than the saved model, which is a problem that I'm still investigating. It could be related to the problem with multiple GPUs which is beyond the scope of this issue."", ""@trane293 You mentioned that you used pickle to save and load the model which worked very well [here](https://github.com/keras-team/keras/issues/2378#issuecomment-212935382). Saving the model implies saving weights of model, states of optimizers, custom objects like layers, custom loss function, custom accuracy metrics and more..\r\n\r\nCan you please share your code to save and load model. I too am facing this issue and in an urgent need to finding an alternative for now. My models takes approx 10 days to get trained perfectly but because of electricity cutoff, it's training got interrupted. Please help..\r\n\r\nAlso, I have opened a new issue in keras. Can you help be debug the problem in s step-by-step manner? It would be a great help to a newbie like me. Thank you..\r\n\r\nIssue link - https://github.com/keras-team/keras/issues/12263"", '@trane293 You mentioned that the issue is fixed [here](https://github.com/keras-team/keras/issues/2378#issuecomment-354903113). But I am still facing the issue. I am saving my model using ModelCheckpoint callback in keras. Is it like the issue is fixed in model.save() but not in ModelCheckpoint?\r\n\r\nI am also using the function multi_gpu_model() for training. Is it interfering? Can you please help with my issue mentioned in [above comment](https://github.com/keras-team/keras/issues/2378#issuecomment-463252518)?', 'I experienced this issue with Keras both with Mxnet and Tensorflow backends. My solution was to switch from using keras to tensorflow.keras. This obviously only works with tensorflow backend. However, if you are already using tensorflow backend, it is just a matter of changing your import statements as the functionality of tensorflow.keras is almost identical to keras\r\nSince switching I have not experienced this annoying bug', ""Thanks, I'll try the solution this week."", 'This issue is not yet fixed. I’m experiencing this issue with Tensorflow as backend.any idea?', ""I have the same issue.Though I can't resume training after load_weight and loss value is just like the epoch1, I can load the weight and predict well.\r\nAnd i found that the loss will down quickly after load_weight.\r\nIn the normal case, loss from 3 down to 1.5 maybe 10 epochs.\r\nBut when I call function load_weight, loss from 3 down to 1.5 maybe 3 or 4 epochs.\r\nI have search a solution for a long time and not fix it yet.\r\nFortunately, It predicts well."", ""After loading your weights, when you train your model set parameter initial_epoch to the last epoch you trained your model before. E.g. you trained your model 100 epochs and saved via ModelCheckpoint weights after each epoch and want to resume training from 101st epoch you should do it in the next way\r\n`model.load_weights('path_to_the_last_weights_file')`\r\n`model.fit(initial_epoch=100)`\r\nOther parameters keep the same."", 'Also experiencing this, especially with multi_gpu_model of a multi-model (model of models), saving the original multi-model. When I load weights it\'s as if they\'ve never been saved (although load isn\'t erroring).\r\n\r\nI\'m using an ""altmodelcheckpoint"" to save the weights of the original model. Not sure if it is working.\r\nWhen checking val loss I get the exact same patterns, as if the weights have been reinitialized... \r\n\r\nI think there might actually be a bug in here somewhere.', 'There is no more an issue of saving/loading model weights in Keras. I know as I am a Keras user. It some error in your program which leads to such issues.', ""I'll try to find reproduction steps.... But if this bug shows up only in rater circumstances then it's still a bug. But, with 1.14 we'll no longer use multi_gpu_model so it doesn't really matter"", '@veqtor You can check my mini project. I faced same issues when built it but now everything works fine. I added support for multi gpu and that too is working. Check scripts here - https://github.com/ParikhKadam/bidaf-keras', 'model.fit(x_train, y_train,batch_size=batch_size,initial_epoch=30,\r\n                        epochs=epochs,validation_data=(x_test, y_test),\r\n                        callbacks=[modelcheckpoint, earlystopping])\r\nmodel.fit(x_train, y_train,batch_size=batch_size,\r\n                        epochs=epochs,\r\n                        validation_data=(x_test, y_test),\r\n                        callbacks=[modelcheckpoint, earlystopping])\r\n添加一个initial_epoach,表示之前训练的次数，我之前只写了一次fit，然后程序只是输出，没有继续训练，后面又添加了一个fit，才开始继续训练的。', ""> My solution was to switch from using keras to tensorflow.keras. This obviously only works with tensorflow backend. \r\n> Since switching I have not experienced this annoying bug\r\n\r\nHere's my scripts\r\n```python\r\n from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau, EarlyStopping\r\n\r\nsaver = ModelCheckpoint(os.path.join(save_path, model_name + '-{epoch:02d}-{val_loss:.2f}.hdf5'),\r\n                                                monitor='val_loss',\r\n                                                verbose=0,\r\n                                                save_best_only=False,\r\n                                                save_weights_only=False,\r\n                                                mode='auto',\r\n                                                save_freq=save_interval)\r\n```\r\n\r\nstill not working... And I have trained one epoch but it seems not speed up the speed of convergence"", ""> > My solution was to switch from using keras to tensorflow.keras. This obviously only works with tensorflow backend.\r\n> > Since switching I have not experienced this annoying bug\r\n> \r\n> Here's my scripts\r\n> \r\n> ```python\r\n>  from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau, EarlyStopping\r\n> \r\n> saver = ModelCheckpoint(os.path.join(save_path, model_name + '-{epoch:02d}-{val_loss:.2f}.hdf5'),\r\n>                                                 monitor='val_loss',\r\n>                                                 verbose=0,\r\n>                                                 save_best_only=False,\r\n>                                                 save_weights_only=False,\r\n>                                                 mode='auto',\r\n>                                                 save_freq=save_interval)\r\n> ```\r\n> \r\n> still not working... And I have trained one epoch but it seems not speed up the speed of convergence\r\n\r\nmy sulotion seems working well.[Here's my repository](https://github.com/jiayiwang5/Chinese-ChatBot). you can try to save weights only then rebuild the network and load the weights ."", ""As @peymanrah and @turb0bur said, I setting the `initial_epoch=39` where my `train_eopch` paused in tensorboard. And `lr=0.008` which didn't change, I tried reducing `lr`,  but I don't give it enough time to train a few epochs. here's my tensorboard visualization, We can see after one epoch, the network seems  back on track. \r\nBut unfortunately, after `load_weights` the model still cannot predict.\r\n\r\n![epoch_loss:blue line is validation trend, orange line is train trend](https://user-images.githubusercontent.com/28484395/77371768-09ced900-6d9f-11ea-85b1-7cf682ab2549.PNG)\r\n**epoch_loss:blue line is validation trend, orange line is training trend**\r\n\r\n\r\n"", 'great！']","[""\n modelPath = './SegmentationModels/\n modelName = 'Arch_1_10'\n sys.setrecursionlimit(10000)\n json_string = model.to_json()\n open(str(modelPath + modelName + '.json'), 'w').write(json_string)\n model.save_weights(str(modelPath + modelName + '.h5'))\n import cPickle as pickle\n with open(str(modelPath + modelName + '_hist.pckl'), 'wb') as f:\n     pickle.dump(history.history, f, -1)\n"", ""\n modelPath = './SegmentationModels/'\n modelName = 'Arch_1_10'\n model = model_from_json(open(str(modelPath + modelName + '.json')).read())\n model.compile(loss='categorical_crossentropy', optimizer=optim_sgd)\n model.load_weights(str(modelPath + modelName + '.h5'))\n #     import cPickle as pickle\n #     with open(str(modelPath + modelName + '_hist.pckl'), 'r') as f:\n #         history = pickle.load(f)\n model.summary()\n""]",[],0,0
510,keras,2356,closed,Error after final epoch when using fit_generator,"Using a batch generator and fit_generator raises an error after the final epoch when running with tensorflow backend + gpu on linux and reading images from disk during next(generator) rather than simply taking entries from a pre-populated numpy array. 

Here is a gist of the issue and the log: https://gist.github.com/aachkar-miovision/89daac9ce598dbcb5a698612a3a42684

In my use case, it is not possible to load all the images into a single numpy array as in the cifar10_cnn fit_generator example, so I have to resort to loading images in batches from within my own ImageDataGenerator. I use cv2 because I perform different transforms involving color conversion, etc., so I load the images for a batch using cv2.imread(). 

With small images, I did not see this issue, but my images are on the order of 480x720 pixels and I do see it with images on that size (related to time it takes to read the image?). 

A possible fix would be to also check for AttributeError at https://github.com/fchollet/keras/blob/master/keras/engine/training.py#L399

Is there a reason that this line only checks for ValueError?

Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [yes] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [using tf] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [done] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",stale,"[""I have updated my prior comment with a gist of the issue and a log of the program's output. \n"", 'I have almost the exact same issue, in a completely different problem, but I\'m also using fit_generator and get the same cryptic errors about NoneType. What seems to happen is that the thread which runs the generator starts to unload libraries (cv2 and numpy in your case, only numpy in mine) while the generator still runs. Then you get errors like those, where it tries to run someting on cv2 or np, but those are ""None"" so it just crashes. Very, very strange error.', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs, but feel free to re-open it if needed.\n', 'I am also having this problem, did you fix it?', '+1', '+1']",[],[],0,0
511,keras,6279,closed,[Request] Add warnings when LeakyReLU is passed into Activation,"Activations from  (e.g. ) are , so typically we should do something like this:

    model.add(LeakyReLU())

However, there're numbers of people who struggled  which doesn't work and will not raise a warning.  (e.g. https://github.com/fchollet/keras/issues/3816) 

I think maybe we can add some warnings?",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],"['keras.layers.advanced_activations', 'LeakyReLU', 'Layers', 'model.add(Activation(LeakyReLU()))']",0,0
512,keras,5343,closed,Meta-RL,"I would like to experiment with meta-RL, but the central idea involves adding the previous prediction and its quality to the input vector at each timestep.

This means I'll be writing my own train loop and in the middle I will have something like this.



But there are two problems with that.
1. It runs the prediction step twice, once internally for the training step, and once to give me the prediction
2. It gives me a scalar loss, not a vector representing the individual loss for each prediction.

So basically, I would like to have a single function that does everything in one go.


",stale,"['Hello,\r\n\r\nYou will probably have to write your custom update function, to achieve what you want to do.\r\nYou can have a look at what someone did, in a somewhat similar fashion to improve GAN training.\r\n\r\nhttps://github.com/fchollet/keras/issues/5312\r\n\r\n', ""If there were the following functions, then I'd be able to experiment in myriad ways.\r\n\r\n```\r\nprediction = model.run_predict()\r\nlosses = model.get_loss_vector()\r\ngradients = model.get_gradients()\r\netc.\r\n```\r\nI suppose that using functions like these would probably invalidate most of the optimisations needed to run stuff efficiently on a gpu. But it would be top for running stuff on a cpu.\r\n\r\nMaybe I should just learn Tensorflow."", 'Hi @jpeg729 , did you check  https://github.com/matthiasplappert/keras-rl ?', ""Yes, but only briefly.\r\n\r\nThe readme lists A3C as still experimental, but meta-RL requires a modified version of A3C that adds the previous action and its value to the input at each timestep.\r\n\r\nI'll go with [https://github.com/awjuliani/Meta-RL](https://github.com/awjuliani/Meta-RL) an existing tensorflow implementation of meta-rl.""]","['python\r\nloss = model.train_on_batch(X[t],Y[t])\r\npredicted = model.predict_on_batch(X[t])\r\nX[t+1][""previous""] = predicted\r\nX[t+1][""loss""] = loss\r\n']","['vector_loss, prediction = model.train_and_predict_on_batch(X,Y)']",0,0
513,keras,11415,closed,"Name Error-"" name 'Flatten' is not defined "", when i try to fine tune the pretrained model on InceptionV3","NameError                                 Traceback (most recent call last)
<ipython-input-46-095f9679650a> in <module>()
      1 # set up transfer learning on pre-trained ImageNet Inception_V3 model - remove fully connected layer and replace
      2 # with softmax for classifying 10 classes
----> 3 incepV3_model = InceptionV3(weights = 'imagenet', include_top = False, input_shape=(299,299,3))
      4 x = incepV3_model.output
      5 x = Flatten(name='flatten')(x)

~/anaconda3/envs/tensorflow-venv/lib/python3.6/site-packages/keras_applications/inception_v3.py in InceptionV3(include_top, weights, input_tensor, input_shape, pooling, classes)
    362         elif pooling == 'max':
    363             x = layers.GlobalMaxPooling2D()(x)
--> 364         x = Flatten(name='flatten')(x)
    365  # Ensure that the model takes into account
    366  # any potential predecessors of .

NameError: name 'Flatten' is not defined
",User error,"[""name xxx is not defined in python means that this variable does not exist. It's very likely unrelated to keras. Maybe you forgot to import Flatten?"", 'I have imported Flatten from keras.layers , even though the error occurs ', 'import keras.utils']",[],['input_tensor'],0,0
514,keras,6619,closed,Specifying initial state for Stateful Recurrent layers,"Hi, is it possible to specify the initial state for a stateful recurrent layer? For example, I have the following code, will this work? 


The idea is to have the need_initial_flag turned on in the beginning of a sequence and then turned off.
I am not sure if the logic is correct for Keras implementation. Thanks! @farizrahman4u",stale,"['To be more clear, I want to define my model to have two inputs. The first input is the image data. The second input is some data else which is used to generate the initial states (Note they are not initial states). I want to define a model that has some recurrent layer (e.g. ConvLSTM2D). The layer is stateful because the sequence is long. And I want the model to set the initial states when the second input is some actual data while **NOT** to set the initial states when the second input is None or something similar. \r\n\r\nHow can I achieve the above goal? Is there any examples? Thanks!', '@fchollet  any idea? Thanks!', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']","['\r\nconv_lstm = ConvLSTM2D(128, (3,3), stateful=True)\r\nx = Input(batch_shape=(1,1,32, 32, 3))\r\nif need_initial_flag == True:\r\n    h_tm1 = Input(batch_shape=(1,32, 32, 3))\r\n    y = conv_lstm(x, initial_state=[h_tm1, h_tm1])\r\nelse:\r\n    y = conv_lstm(x)\r\n']",[],0,0
515,keras,11000,closed,Keras Error when input Dense layer with a reshaped layer,"I am trying to implement a lstm model which process 100 texts and then concatenate them together to feed into a dense layer. However there comes an error, which is:



The code shows below:


Can anyone gives some help, thanks!
",,"[""@dzhao123 after you squeeze `utterance_representation` has shape `(?, 100)`, which makes `K.shape(utterance_representation)[0]*K.shape(utterance_representation)[1]` evaluate to None, which in turn makes `fan_in` of your Dense layer `None`, which breaks weight init. The error that ultimately causes this is\r\n\r\n```python\r\nscale /= max(1., float(fan_in + fan_out) / 2)\r\n```\r\n\r\nsince you can't add ints and None.\r\n\r\nThe flexibility `Reshape` gives you comes at a cost: you need to carefully evaluate if the shapes you provide make sense. Replace your line above with \r\n\r\n```python\r\n    utterance_outputs = K.reshape(utterance_representation, [-1, 100])\r\n```\r\n\r\nand watch it succeed. Note that I have no clue what `outputs_one` is though.""]","[""\r\nfrom keras.layers import Embedding, Dense, LSTM, Masking, concatenate, Flatten, Activation, RepeatVector, Permute, Multiply, Lambda, TimeDistributed, Reshape\r\nfrom keras import Input, Model\r\nfrom keras import backend as K\r\n\r\n\r\nlstm = LSTM(100)\r\nembd = Embedding(1000, 128)\r\ndense = Dense(1, activation='tanh')#(lstm_outputs) \r\n\r\n\r\ninputs = []\r\noutputs = []\r\n\r\n\r\nfor _ in range(2):\r\n    encoder_inputs = Input(shape=(160,))\r\n    print(encoder_inputs.shape)\r\n    masked_outputs = Masking(0)(encoder_inputs)\r\n    encoder_embd = embd(masked_outputs)\r\n    print(encoder_embd.shape)\r\n    lstm_outputs = lstm(encoder_embd)\r\n    attention = dense(lstm_outputs)\r\n    attention = Activation('softmax')(attention)\r\n    attention = RepeatVector(100)(attention)\r\n    attention = Permute([2, 1])(attention)\r\n\r\n    utterance_representation = Multiply()([lstm_outputs, attention])\r\n    print(utterance_representation.shape)\r\n\r\n    utterance_representation = K.squeeze(utterance_representation,axis=1)\r\n\r\n    utterance_outputs = K.reshape(utterance_representation, [-1, K.shape(utterance_representation)[0]*K.shape(utterance_representation)[1]])\r\n\r\n    encoder_outputs = Dense(200, activation='relu')(utterance_outputs)\r\n    model_outputs = Dense(1, activation='softmax')(encoder_outputs)\r\n\r\n    inputs.append(encoder_inputs)\r\n    outputs.append(model_outputs)\r\n\r\n\r\nencoder_outputs = Dense(200, activation='relu')(outputs_one)\r\nmodel_outputs = Dense(1, activation='softmax')(encoder_outputs)\r\n\r\nmodel = Model(input=inputs, output=outputs)\r\nmodel.compile(optimizer='rmsprop', loss='mse', metrics=['accuracy'])\r\n\r\n\r\nmodel.fit_generator(feed_generator('data.h5'), steps_per_epoch=250, epochs=1, verbose=1, callbacks=[EarlyStopping(monitor='val_loss', patience=4)])\r\n""]","[""TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'""]",0,0
516,keras,6011,closed,Lambda function and transpose operation,"I have the following function:

    def transpose_dot(vects):
        x, y = vects
        # <x,x> + <y,y> - 2<x,y>
    
        return K.dot(x, K.transpose(y))

When try to evaluate it with  it works 

    x = K.variable(np.array(np_x))
    y = K.variable(np.array(np_x))
    obj = transpose_dot
    objective_output = obj((x, y))
    print('-----------------')
    print (K.eval(objective_output))

result with:

    [[ 1.  1.  1.  2.]
     [ 1.  2.  2.  4.]
     [ 1.  2.  2.  4.]
     [ 2.  4.  4.  8.]

**But**, when try to use it as function for  layer it does not work. 
    
    np_x = [[1, 0], [1, 1], [1, 1], [2, 2]]
    features = np.array([np_x])
    test_input = Input(shape=np.array(np_x).shape)
    dot_layer= Lambda(transpose_dot, output_shape=(4,4))([test_input, test_input])
    x = Model(inputs=test_input, outputs=dot_layer)
    x.predict(features, batch_size=1)


Result with 

    self.fn() if output_subset is None else\
    ValueError: Shape mismatch: x has 2 cols (and 4 rows) but y has 4 rows (and 2 cols)
    Apply node that caused the error: Dot22(Reshape{2}.0, Reshape{2}.0)
    Toposort index: 11
    Inputs types: [TensorType(float32, matrix), TensorType(float32, matrix)]
    Inputs shapes: [(4, 2), (4, 2)]
    Inputs strides: [(8, 4), (8, 4)]
    Inputs values: ['not shown', 'not shown']
    Outputs clients: [[Reshape{4}(Dot22.0, MakeVector{dtype='int64'}.0)]]
 

Any idea what I'm missing here?

P.s 
[Stackoverflow link provide as well ](http://stackoverflow.com/questions/43049984/keras-lambda-function-dot-product-mistmatch)",,"['Hello, \r\nYou are probably missing the fact that using Input( shape=np.array(np_x).shape) silently adds a batching dimension on axis 0.', '@unrealwill \r\nThanks, I confused between the batch size and the actual input. I have change it to be \r\n\r\n\r\n\r\n```\r\ndef distance_matrix_part1(vects):\r\n    x, y = vects\r\n    # <x,x> + <y,y> - 2<x,y>\r\n    #return K.transpose(x)\r\n    #\r\n    return K.expand_dims(K.dot(K.transpose(y),x ),axis=0)\r\n```\r\n\r\n   ```\r\n np_x = [[1, 0], [1, 1], [1, 1], [2, 2]]\r\n    features = np.array([np_x])\r\n    test_input = Input(shape=(1,2))\r\n    distance_layer = Lambda(distance_matrix_part1,output_shape=(4,4))([test_input, test_input])\r\n    x = Model(inputs=test_input, outputs=distance_layer)\r\n    x.compile(optimizer=""rmsprop"",loss=\'mean_squared_error\')\r\n    x.evaluate(features.transpose((1, 0, 2)),y_pred, batch_size=4)\r\n \r\n```\r\n\r\nThis actually does run the method. But it complains about the output shape  of Lambda function. The dot function produce a matrix 4x4 based on the input. So inorder to fit the right dim I added `expand_dims` function. But I get the following error \r\n\r\n> ValueError: Error when checking model target: expected lambda_1 to have 3 dimensions, but got array with shape (4, 4)\r\n\r\nWhich seems like the `expand_dims` function does not add another 1 dim\r\n\r\nAny ideas?\r\n', 'Ok, I got this part working, Transpose actully the same mistake. Because this is a `batch` I need to `reshape` it \r\n\r\n`K.expand_dims(K.dot(K.reshape(x,(x_shape[0],x_shape[1])), K.reshape(y,(y_shape[1],y_shape[0]))),0) `\r\n\r\nBut because I\'m working on the entire batch I\'m changing the `batch` dim. But `keras` does not let me to do that.\r\n\r\n`""ValueError: Input arrays should have the same number of samples as target arrays. Found 4 input samples and 1 target samples.""`\r\n\r\nIs it possible to change output of layer including the number of samples ?', 'The output shape of the lambda should be (batch_size,4,4) instead of (4,4) for batching reasons. \r\nI think that should solve your error.', '@unrealwill thanks I actually wanted to change output shape i.e to be (batch_size,batch_size) instead of (batch_size,x_dim,y_dim).  With your help  I understood my mistake. I got the dim right now and works! thanks!']",[],"['keras', 'Lambda']",0,0
517,keras,2091,closed,"SimpleRNN - Wrong number of dimensions, expected 3, got 2 with shape (119,80)","I'm new to Keras and having some trouble with shapes, specially when it comes to RNNs and LSTMs.

**I'm running this code:**



The variable predictor_train is a numpy array with 119 inner arrays, each one having 80 different items.

**I'm having this error:**



So far what I found out is that an RNN receives 3D tensor with shape of (batch_size, timesteps, dimension) and when you set input_shape the batch_size is usually omitted, and you should just provide a tuple of (timesteps, dimension). But how exactly do I write that in my code??
",stale,"['`timesteps`  usually indicate the length of your sequence (each sample should have same length, or you should do some paddings of your input to have the same length)\n`dimension` usually indicate the number of units in a specific layer. for e.g. if your word embedding is 100 dimension, then you just set this param as 100.\n', 'Your code is good. The problem is with your data.\n', ""Thanks @farizrahman4u ! I was focusing too much in the code itself.\nThanks for the new clarifications @ymcui !\n\n@ymcui @farizrahman4u, my input (predictor train) has the following characteristics:\ntype: 'numpy.ndarray'\nshape: (119,80)\ndtype: float64\nI made this array out of a pandas dataframe where 80 was the number of rows (each row meaning one date) and 119 was the number of columns (each column meaning one feature). As far as I understood I should adapt that so that the shape has 3 dimensions (nb_samples, timesteps, input_dim), is that right?\n\nIf yes, input_dim should be the number of features and timesteps (in this case 119) the number of samples in each features (in other words the lenght of the index of the original predictor dataframe, in this case 80)? If that's right, what should nb_samples stand for?\n"", 'Do you want to classify each (119,1) vector, or?  I think the nb_samples is  the number of your total samples, the timestep is the length of your each sample.You can have a look at this issue #2045 \n', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']","['\nmodel.add(SimpleRNN(init=\'uniform\',output_dim=1,input_dim=len(pred_frame.columns)))\nmodel.compile(loss=""mse"", optimizer=""sgd"")\nmodel.fit(X=predictor_train, y=target_train, batch_size=len(pred_frame.index),show_accuracy=True)\n', '\nTypeError: (\'Bad input argument to theano function with name ""/Library/Python/2.7/site-packages/keras/backend/theano_backend.py:362""  at index 0(0-based)\', \'Wrong number of dimensions: expected 3, got 2 with shape (119, 80).\')\n']",[],0,0
518,keras,6761,closed,Standard bibtex entry for reference in academic documents,"Would be nice to have a standard way to reference this library on papers.

TensorFlow have it: https://www.tensorflow.org/versions/r0.11/resources/bib

Best regards",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', 'https://keras.io/getting-started/faq/#how-should-i-cite-keras']",[],[],0,0
519,keras,6938,closed,The issue to implement multiple CNN models on the individual thread with keras,"I want to run the different model in the same time. 
However, I cannot get the correct answer for each running time.
The following is my code.
I generate the MNIST-like data, and build the 2 CNN network to do the training and testing.

I create individual session and graph object. 
Sometime I got the correct answer as the following:

However, I got the error sometimes...

Is there any wrong about my implementation?
or some thing I should notice while doing multi-thread work?",,"['I got some hint from the stackoverflow.\r\nThe following split the code as the two part.\r\nThe first part is the code of kera models, and the other one is main program.\r\nI named the model code as `model.py`.\r\nThe contain of `model.py` is shown below:\r\n```python\r\nfrom keras.layers import Conv2D, Dense, Flatten, Input, MaxPool2D\r\nfrom keras.models import Sequential\r\nfrom keras import backend as K\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport threading\r\nimport os\r\n\r\nclass Has1ConvLayerCNN(object):\r\n    graph = None\r\n    model = None\r\n    model_name = None\r\n    def __init__(self):\r\n        pass\r\n\r\n    def createModelAndOp(self, model_name=\'small_model.h5\'):\r\n        self.graph = tf.Graph()\r\n        self.model_name = model_name\r\n        \r\n        with self.graph.as_default():\r\n            self.model = Sequential()\r\n            self.model.add(Conv2D(5, (2, 2), activation=\'relu\', input_shape=(27, 27, 1)))\r\n            self.model.add(MaxPool2D())\r\n            self.model.add(Conv2D(10, (2, 2), activation=\'relu\'))\r\n            self.model.add(MaxPool2D())\r\n            self.model.add(Conv2D(15, (2, 2), activation=\'relu\'))\r\n            self.model.add(Conv2D(20, (2, 2), activation=\'relu\'))\r\n            self.model.add(Flatten())\r\n            self.model.add(Dense(100, activation=\'relu\'))\r\n            self.model.add(Dense(10, activation=\'sigmoid\'))\r\n            self.model.compile(optimizer=\'sgd\', loss=\'mse\')\r\n\r\n        def train(x, y):\r\n            with tf.Session(graph=self.graph) as sess:\r\n                sess.run(tf.global_variables_initializer())\r\n                self.model.fit(x, y, verbose=0)\r\n                self.model.save_weights(self.model_name)\r\n\r\n        def test(x):\r\n            with tf.Session(graph=self.graph) as sess:\r\n                sess.run(tf.global_variables_initializer())\r\n                if os.path.exists(model_name):\r\n                    self.model.load_weights(model_name)\r\n                return self.model.predict(x)\r\n\r\n        # Use closure to return the function\r\n        return train, test\r\n\r\nclass Has2ConvLayerCNN(object):\r\n    graph = None\r\n    model = None\r\n    model_name = None\r\n    def __init__(self):\r\n        pass\r\n\r\n    def createModelAndOp(self, model_name=\'big_model.h5\'):\r\n        self.graph = tf.Graph()\r\n        self.model_name = model_name\r\n        with self.graph.as_default():\r\n            self.model = Sequential()\r\n            self.model.add(Conv2D(8, (2, 2), activation=\'relu\', input_shape=(27, 27, 1)))\r\n            self.model.add(MaxPool2D())\r\n            self.model.add(Conv2D(16, (2, 2), activation=\'relu\'))\r\n            self.model.add(MaxPool2D())\r\n            self.model.add(Conv2D(32, (2, 2), activation=\'relu\'))\r\n            self.model.add(Conv2D(64, (2, 2), activation=\'relu\'))\r\n            self.model.add(Flatten())\r\n            self.model.add(Dense(128, activation=\'relu\'))\r\n            self.model.add(Dense(10, activation=\'sigmoid\'))\r\n            self.model.compile(optimizer=\'sgd\', loss=\'mse\')\r\n\r\n        def train(x, y):\r\n            with tf.Session(graph=self.graph) as sess:\r\n                sess.run(tf.global_variables_initializer())\r\n                self.model.fit(x, y, verbose=0)\r\n                self.model.save_weights(self.model_name)\r\n\r\n        def test(x):\r\n            with tf.Session(graph=self.graph) as sess:\r\n                sess.run(tf.global_variables_initializer())\r\n                if os.path.exists(model_name):\r\n                    self.model.load_weights(model_name)\r\n                return self.model.predict(x)\r\n\r\n        # Use closure to return the function\r\n        return train, test\r\n```\r\nAs you can see, the function should use closure style to program.\r\nOr the error will occure.\r\nI don\'t really know the reason, but I haven\'t encounter the problem while I use closure.\r\n\r\nNext, the main program is this:\r\n```python\r\nfrom keras.layers import Conv2D, Dense, Flatten, Input, MaxPool2D\r\nfrom model import Has1ConvLayerCNN, Has2ConvLayerCNN\r\nfrom keras.models import Sequential\r\nfrom keras import backend as K\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport threading\r\nimport time\r\nimport os\r\n\r\nif __name__ == \'__main__\':\r\n    # Create data and model instance\r\n    x, y = np.random.random([4, 27, 27, 1]), np.random.random([4, 10])\r\n    small_cnn = Has1ConvLayerCNN()\r\n    big_cnn = Has2ConvLayerCNN()\r\n\r\n    # Form operations list \r\n    small_cnn_train, small_cnn_test = small_cnn.createModelAndOp()\r\n    big_cnn_train, big_cnn_test = big_cnn.createModelAndOp()\r\n    train_ops = [small_cnn_train, big_cnn_train]    \r\n    test_ops = [small_cnn_test, big_cnn_test]\r\n    \r\n    # Create the list of threads (train operations)\r\n    threads = []\r\n    for i in range(len(train_ops)):\r\n        threads.append(threading.Thread(target=train_ops[i], args=(x, y)))\r\n\r\n    print ""------------ Train --------------------""\r\n    # Show the time of multithreading\r\n    _time = time.time()\r\n    for i in range(len(threads)):\r\n        threads[i].start()\r\n    print ""<<      Start whole thread     >> time spend: "", time.time() - _time\r\n    for i in range(len(threads)):\r\n        threads[i].join(5)\r\n    print ""<<   Wait whole threads join   >> time spend: "", time.time() - _time\r\n       \r\n    # Show the time of working with order\r\n    _time = time.time()\r\n    train_ops[0](x, y)\r\n    train_ops[1](x, y)\r\n    print ""<<  Main thread work in order  >> time spend: "", time.time() - _time\r\n    \r\n    # Create the list of threads (test operations)\r\n    threads = []\r\n    for i in range(len(train_ops)):\r\n        threads.append(threading.Thread(target=test_ops[i], args=(x,)))\r\n\r\n    print ""------------ Test --------------------""\r\n    # Show the time of multithreading\r\n    _time = time.time()\r\n    for i in range(len(threads)):\r\n        threads[i].start()\r\n    print ""<<      Start whole thread     >> time spend: "", time.time() - _time\r\n    for i in range(len(threads)):\r\n        threads[i].join(5)\r\n    print ""<<   Wait whole threads join   >> time spend: "", time.time() - _time\r\n\r\n    # Show the time of working with order\r\n    _time = time.time()\r\n    for i in range(len(test_ops)):\r\n        _x = np.expand_dims(x[0], 0)\r\n        test_ops[i](_x)\r\n    print ""<<  Main thread work in order  >> time spend: "", time.time() - _time\r\n```\r\n\r\nAn the following is the result:\r\n```\r\nUsing TensorFlow backend.\r\n------------ Train --------------------\r\n2017-06-18 17:23:28.364516: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn\'t compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-18 17:23:28.364537: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn\'t compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-18 17:23:28.364542: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn\'t compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-18 17:23:28.364546: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn\'t compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-18 17:23:28.364550: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn\'t compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n<<      Start whole thread     >> time spend:  0.000718832015991\r\n<<   Wait whole threads join   >> time spend:  0.377538919449\r\n<<  Main thread work in order  >> time spend:  0.152100801468\r\n------------ Test --------------------\r\n<<      Start whole thread     >> time spend:  0.00042986869812\r\n<<   Wait whole threads join   >> time spend:  0.079735994339\r\n<<  Main thread work in order  >> time spend:  0.0590391159058\r\n```\r\n\r\nAfter this testing, I didn\'t suggest to use multithreading technique to implement the different model in individual thread.\r\nAs you can see, the program will spend more time in training or testing.\r\n\r\nIf there\'s some error or misunderstanding, welcome to raise your opinion.']","['python\r\nfrom keras.layers import Conv2D, Dense, Flatten, Input, MaxPool2D\r\nfrom keras.models import Model, Sequential\r\nfrom keras import backend as K\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport threading\r\nimport time\r\nimport os\r\n\r\nK.set_image_dim_ordering(\'tf\')\r\nx, y = np.random.random([4, 27, 27, 1]), np.random.random([4, 10])\r\n\r\n# Model objects\r\nsmall_model = None\r\nbig_model = None\r\n\r\nclass Has1ConvLayerCNN(object):\r\n    small_graph = tf.get_default_graph()\r\n    sess = None\r\n    def __init__(self, model_name=\'small_model.h5\'):\r\n        with self.small_graph.as_default():\r\n            self.sess = tf.Session(graph=self.small_graph)\r\n            self.model = Sequential()\r\n            self.model.add(Conv2D(8, (4, 4), activation=\'relu\', input_shape=(27, 27, 1)))\r\n            self.model.add(MaxPool2D())\r\n            self.model.add(Flatten())\r\n            self.model.add(Dense(10, activation=\'sigmoid\'))\r\n            self.sess.run(tf.global_variables_initializer())\r\n            if os.path.exists(model_name):\r\n                self.model.load_weights(model_name)          \r\n    \r\n    def compile(self):\r\n        self.model.compile(loss=\'mse\', optimizer=\'adam\')\r\n\r\n    def train(self, x, y, model_name=\'small_model.h5\'):\r\n        with self.small_graph.as_default():\r\n            self.model.fit(x, y, epochs=1)\r\n            self.model.save_weights(model_name)\r\n\r\n    def test(self, x):\r\n        with self.small_graph.as_default():\r\n            return self.model.predict(x, batch_size=1)\r\n\r\nclass Has2ConvLayerCNN(object):\r\n    big_graph = tf.get_default_graph()\r\n    sess = None\r\n    def __init__(self, model_name=\'big_model.h5\'):\r\n        with self.big_graph.as_default():\r\n            self.sess = tf.Session(graph=self.big_graph)\r\n            self.model = Sequential()\r\n            self.model.add(Conv2D(8, (4, 4), activation=\'relu\', input_shape=(27, 27, 1)))\r\n            self.model.add(Conv2D(16, (2, 2), activation=\'relu\'))\r\n            self.model.add(MaxPool2D())\r\n            self.model.add(Flatten())\r\n            self.model.add(Dense(10, activation=\'sigmoid\'))\r\n            self.sess.run(tf.global_variables_initializer())\r\n            if os.path.exists(model_name):\r\n                self.model.load_weights(model_name)          \r\n    \r\n    def compile(self):\r\n        with self.big_graph.as_default():\r\n            self.model.compile(loss=\'mse\', optimizer=\'adam\')\r\n\r\n    def train(self, x, y, model_name=\'big_model.h5\'):\r\n        with self.big_graph.as_default():\r\n            self.model.fit(x, y, epochs=1)\r\n            self.model.save_weights(model_name)\r\n\r\n    def test(self, x, model_name=\'big_model.h5\'):\r\n        with self.big_graph.as_default():\r\n            return self.model.predict(x, batch_size=1)\r\n\r\ndef smallThread():\r\n    global small_model\r\n    global x\r\n    res = small_model.test(x)\r\n    print ""small result: "", res\r\n\r\ndef bigThread():\r\n    global big_model\r\n    global x\r\n    res = big_model.test(x)\r\n    print \'big result: \', res\r\n\r\nif __name__ == \'__main__\':\r\n    # Train\r\n    small_model = Has1ConvLayerCNN()\r\n    small_model.compile()\r\n    big_model = Has2ConvLayerCNN()\r\n    big_model.compile()\r\n    print ""----- Train 1 conv layer model -----""\r\n    small_model.train(x, y)\r\n    print ""----- Train 2 conv layer model -----""\r\n    big_model.train(x, y)\r\n\r\n    # Clear the model objects\r\n    small_model = None\r\n    big_model = None\r\n\r\n    # Load model and Test\r\n    small_model = Has1ConvLayerCNN()\r\n    small_model.compile()\r\n    big_model = Has2ConvLayerCNN()\r\n    big_model.compile()\r\n    threading.Thread(target=smallThread).start()\r\n    threading.Thread(target=bigThread).start()\r\n', ""\r\nUsing TensorFlow backend.\r\n2017-06-11 17:45:01.548647: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-11 17:45:01.548673: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-11 17:45:01.548678: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-11 17:45:01.548683: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-11 17:45:01.548699: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n----- Train 1 conv layer model -----\r\nEpoch 1/1\r\n4/4 [==============================] - 0s - loss: 0.1300\r\n----- Train 2 conv layer model -----\r\nEpoch 1/1\r\n4/4 [==============================] - 0s - loss: 0.0883\r\nbig result:  [[ 0.51761824  0.49175     0.50416106  0.43220499  0.37879816  0.3602626\r\n   0.33406499  0.41810563  0.39064723  0.46406019]\r\n [ 0.51863891  0.51891971  0.49938035  0.46237913  0.38975871  0.3768242\r\n   0.33751875  0.46174175  0.38691592  0.44897127]\r\n [ 0.51071256  0.50159967  0.50252235  0.44284269  0.39376053  0.36824402\r\n   0.33978003  0.44148278  0.41221312  0.45410269]\r\n [ 0.50967991  0.49973318  0.50594532  0.45762312  0.4220691   0.37480459\r\n   0.34047222  0.44234774  0.38909501  0.44603276]]\r\nsmall result:  [[ 0.54407543  0.54171276  0.46328813  0.2413315   0.27085894  0.33460364\r\n   0.22706869  0.30919045  0.22811837  0.49186015]\r\n [ 0.57744735  0.53639919  0.44922575  0.25701386  0.27335384  0.35873184\r\n   0.21858442  0.30952871  0.2886436   0.47201097]\r\n [ 0.55285573  0.50686091  0.43080822  0.23862234  0.28417778  0.38735572\r\n   0.2486203   0.3243944   0.2822212   0.42979807]\r\n [ 0.50508142  0.51574749  0.502177    0.25125584  0.26323137  0.35078073\r\n   0.23977536  0.35091829  0.24196538  0.46264389]]\r\n"", '\r\nUsing TensorFlow backend.\r\n2017-06-11 17:52:38.162244: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn\'t compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-11 17:52:38.162269: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn\'t compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-11 17:52:38.162274: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn\'t compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-11 17:52:38.162278: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn\'t compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-11 17:52:38.162284: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn\'t compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n----- Train 1 conv layer model -----\r\nEpoch 1/1\r\n4/4 [==============================] - 0s - loss: 0.1469\r\n----- Train 2 conv layer model -----\r\nEpoch 1/1\r\n4/4 [==============================] - 0s - loss: 0.1124\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/threading.py"", line 801, in __bootstrap_inner\r\n    self.run()\r\n  File ""/usr/lib/python2.7/threading.py"", line 754, in run\r\n    self.__target(*self.__args, **self.__kwargs)\r\n  File ""test1.py"", line 76, in smallThread\r\n    res = small_model.test(x)\r\n  File ""test1.py"", line 42, in test\r\n    return self.model.predict(x, batch_size=1)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 902, in predict\r\n    return self.model.predict(x, batch_size=batch_size, verbose=verbose)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 1582, in predict\r\n    self._make_predict_function()\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 1049, in _make_predict_function\r\n    **kwargs)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py"", line 2251, in function\r\n    return Function(inputs, outputs, updates=updates)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py"", line 2214, in __init__\r\n    self.updates_op = tf.group(*updates_ops)\r\n  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3168, in __exit__\r\n    self._graph._pop_control_dependencies_controller(self)\r\n  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3188, in _pop_control_dependencies_controller\r\n    assert self._control_dependencies_stack[-1] is controller\r\nAssertionError\r\n\r\nException in thread Thread-2:\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python2.7/threading.py"", line 801, in __bootstrap_inner\r\n    self.run()\r\n  File ""/usr/lib/python2.7/threading.py"", line 754, in run\r\n    self.__target(*self.__args, **self.__kwargs)\r\n  File ""test1.py"", line 82, in bigThread\r\n    res = big_model.test(x)\r\n  File ""test1.py"", line 71, in test\r\n    return self.model.predict(x, batch_size=1)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 902, in predict\r\n    return self.model.predict(x, batch_size=batch_size, verbose=verbose)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 1585, in predict\r\n    batch_size=batch_size, verbose=verbose)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 1212, in _predict_loop\r\n    batch_outs = f(ins_batch)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py"", line 2227, in __call__\r\n    session = get_session()\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py"", line 164, in get_session\r\n    _initialize_variables()\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py"", line 337, in _initialize_variables\r\n    sess.run(tf.variables_initializer(uninitialized_variables))\r\n  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 778, in run\r\n    run_metadata_ptr)\r\n  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 982, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1032, in _do_run\r\n    target_list, options, run_metadata)\r\n  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1052, in _do_call\r\n    raise type(e)(node_def, op, message)\r\nInvalidArgumentError: You must feed a value for placeholder tensor \'conv2d_4_input\' with dtype float\r\n\t [[Node: conv2d_4_input = Placeholder[dtype=DT_FLOAT, shape=[], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]\r\n\r\nCaused by op u\'conv2d_4_input\', defined at:\r\n  File ""test1.py"", line 101, in <module>\r\n    small_model = Has1ConvLayerCNN()\r\n  File ""test1.py"", line 24, in __init__\r\n    self.model.add(Conv2D(8, (4, 4), activation=\'relu\', input_shape=(27, 27, 1)))\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 429, in add\r\n    dtype=layer.dtype, name=layer.name + \'_input\')\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 1414, in Input\r\n    input_tensor=tensor)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.py"", line 88, in wrapper\r\n    return func(*args, **kwargs)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 1325, in __init__\r\n    name=self.name)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py"", line 391, in placeholder\r\n    x = tf.placeholder(dtype, shape=shape, name=name)\r\n  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 1507, in placeholder\r\n    name=name)\r\n  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 1997, in _placeholder\r\n    name=name)\r\n  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 768, in apply_op\r\n    op_def=op_def)\r\n  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2336, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1228, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor \'conv2d_4_input\' with dtype float\r\n\t [[Node: conv2d_4_input = Placeholder[dtype=DT_FLOAT, shape=[], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]\r\n\r\n']",[],0,0
520,keras,9232,closed,Use LSTM for recurrent convolutional network without time distributed wrapper,"I am facing an issue with a project I'm currently working on. I am attempting to build a many-to-many model that takes a series of images and classifies them. That part is relatively straight forward. I have a model built using Keras that uses convolutional layers inside the time distributed wrapper that feed into an LSTM and it works fine. The complexity in my current project comes from the fact that this model needs to be converted to CoreML for deployment. I feel I'm up against a wall with this so any help provided would be a life saver.

Like I said previously, my current model is trainable using the time distributed wrapper, but this doesn't seem to be supported by CoreML. I have seen some examples using an LSTM with CoreML where the LSTM states are passed in and out of the model with each item in the sequence. This essentially creates a recurrent network that takes only a single item from the sequence (as well as the previous predictions LSTM states) as an input, rather than the whole sequence at once. That LSTM state loop (for lack of a better term) seems to be the best option as CoreML doesn't support sequential image inputs. My issue then comes from training. How can I train my network properly on sequential data, then convert it to CoreML?

If I remove the time distribution from the non-LSTM layers, the model won't compile because it's missing the extra time dimension. Essentially, the catch here is I can't remove the time distribution wrappers as the model isn't functional without the inclusion of time steps, and I can't convert to CoreML while they are present.

Does anyone have any ideas on how to do this? I hope this question is understandable. It's quite late and I've been working on this for 20+ hours straight so I'm a bit fried at the moment. Thanks in advance for any input, thoughts, or ideas provided. Cheers!

My model:

    image_input = Input(shape=(max_sequence_length, 224, 224, 3))
    
    convolutional_1 = TimeDistributed(Conv2D(64, (3, 3), activation='relu', data_format = 'channels_last'))(image_input)
    pooling_1 = TimeDistributed(MaxPooling2D((2, 2), strides=(1, 1)(convolutional_1)

    convolutional_2 = TimeDistributed(Conv2D(128, (4,4), activation='relu'))(pooling_1)
    pooling_2 = TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2)))(convolutional_2)

    convolutional_3 = TimeDistributed(Conv2D(256, (4,4), activation='relu'))(pooling_2)
    pooling_3 = TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2)))(convolutional_3)

    flatten_1 = TimeDistributed(Flatten())(pooling_3)
    dropout_1 = TimeDistributed(Dropout(0.5))(flatten_1)

    lstm_1 = LSTM(256, return_sequences=True, return_state=False, stateful=False, dropout=0.5)(dropout_1)

    dense_1 = TimeDistributed(Dense(num_classes, activation='sigmoid'))(lstm_1)

    model = Model(inputs = image_input, outputs = dense_1)


I wanted to add that I have seen some posts where it seems people were using time distribution wrappers with CoreML, however when I try to convert my model it raises this error as soon as it hits the first wrapper:

""AttributeError: The layer has never been called and thus has no defined output shape.""

I have modified the conversion script for Keras -> CoreML to handle a 4D input (although I haven't been able to test it to see if it works as expected as I can't convert my model) for the image sequence, so if I can get it to convert with the time distribution layers in place, it would be functional.

[Link to an Apple article discussing RNN's in CoreML](https://developer.apple.com/documentation/coreml/core_ml_api/making_predictions_with_a_sequence_of_inputs)

[Link to a GitHub repo with an implementation of an LSTM RNN](https://github.com/akimach/GestureAI-CoreML-iOS)",,"['If your sequences have a fixed length, it is possible to implement `TimeDistributed` with a Python loop, without using custom layers, nor `TimeDistributed`, nor `Lambda` (thus it will be compatible with CoreML). \r\n\r\n```python\r\nfrom keras import layers\r\n\r\nconvnet = ...  # your base conv model\r\ntimestep_inputs = [layers.Input(...) for _ in range(num_timesteps)]\r\nconv_outputs = []\r\nfor x in timestep_inputs:\r\n   y = convnet(x)\r\n   conv_outputs.append(y)\r\nx = layers.concatenate(conv_outputs, axis=1)\r\ny = layers.LSTM(...)(x)\r\n\r\nmodel = Model(timestep_inputs, y)\r\n```\r\n\r\nYou could greatly simply this by having a `Lambda` layer that takes a single input tensor and decomposes it into n timesteps.', ""Thank you very much for the information! Very helpful.\r\n\r\nany idea why this is giving me the wrong dimensions for the LSTM layer? When I build it I get:\r\n\r\n> ValueError: Input 0 is incompatible with layer lstm_1: expected ndim=3, found ndim=2\r\n\r\n```\r\nconv_input = Input(shape=(224, 224, 3))\r\n\r\nconvolutional_1 = Conv2D(64, (3, 3), activation='relu', data_format = 'channels_last')(conv_input)\r\npooling_1 = MaxPooling2D((2, 2), strides=(1, 1))(convolutional_1)\r\n\r\nconvolutional_2 = Conv2D(128, (4,4), activation='relu')(pooling_1)\r\npooling_2 = MaxPooling2D((2, 2), strides=(2, 2))(convolutional_2)\r\n\r\nconvolutional_3 = Conv2D(256, (4,4), activation='relu')(pooling_2)\r\npooling_3 = MaxPooling2D((2, 2), strides=(2, 2))(convolutional_3)\r\n\r\nflatten_1 = Flatten()(pooling_3)\r\ndropout_1 = Dropout(0.5)(flatten_1)\r\n\r\nconvnet = Model(inputs = conv_input, outputs = dropout_1)\r\n\r\nimage_input = Input(shape=(224, 224, 3))\r\ntimestep_inputs = [image_input for _ in range(num_timesteps)]\r\nconv_outputs = []\r\nfor x in timestep_inputs:\r\n   y = convnet(x)\r\n   conv_outputs.append(y)\r\n   x = concatenate(conv_outputs, axis = 1)\r\n   y = LSTM(64, return_sequences=True, return_state=False, stateful=False, dropout=0.5)(x)\r\n\r\nmodel_block_1 = Model(inputs = timestep_inputs, outputs = y)\r\n```\r\n\r\nIt looks like it should work! maybe an issue with the concat?"", 'Hi, @fchollet is there a way to join the output of TimeDistributed flatten  layer?\r\neg from [None, None, 100] to [None,1000]  if number of time-stamp =10', ""I'm getting the same issue as Michael, @fchollet your solution doesn't work. Are there any other options?"", ""> Thank you very much for the information! Very helpful.\r\n> \r\n> any idea why this is giving me the wrong dimensions for the LSTM layer? When I build it I get:\r\n> \r\n> > ValueError: Input 0 is incompatible with layer lstm_1: expected ndim=3, found ndim=2\r\n> \r\n> ```\r\n> conv_input = Input(shape=(224, 224, 3))\r\n> \r\n> convolutional_1 = Conv2D(64, (3, 3), activation='relu', data_format = 'channels_last')(conv_input)\r\n> pooling_1 = MaxPooling2D((2, 2), strides=(1, 1))(convolutional_1)\r\n> \r\n> convolutional_2 = Conv2D(128, (4,4), activation='relu')(pooling_1)\r\n> pooling_2 = MaxPooling2D((2, 2), strides=(2, 2))(convolutional_2)\r\n> \r\n> convolutional_3 = Conv2D(256, (4,4), activation='relu')(pooling_2)\r\n> pooling_3 = MaxPooling2D((2, 2), strides=(2, 2))(convolutional_3)\r\n> \r\n> flatten_1 = Flatten()(pooling_3)\r\n> dropout_1 = Dropout(0.5)(flatten_1)\r\n> \r\n> convnet = Model(inputs = conv_input, outputs = dropout_1)\r\n> \r\n> image_input = Input(shape=(224, 224, 3))\r\n> timestep_inputs = [image_input for _ in range(num_timesteps)]\r\n> conv_outputs = []\r\n> for x in timestep_inputs:\r\n>    y = convnet(x)\r\n>    conv_outputs.append(y)\r\n>    x = concatenate(conv_outputs, axis = 1)\r\n>    y = LSTM(64, return_sequences=True, return_state=False, stateful=False, dropout=0.5)(x)\r\n> \r\n> model_block_1 = Model(inputs = timestep_inputs, outputs = y)\r\n> ```\r\n> \r\n> It looks like it should work! maybe an issue with the concat?\r\n\r\nIts because LSTM takes 3 dimensions and you need to reshape the final layer in the base model instead of flattening layer \r\n""]",[],[],0,0
521,keras,9394,closed,AttributeError: 'Model' object has no attribute 'stateful_metric_names',"Hi,

I just upgrade my Keras to latest version and getting a error related to keras/engine/training on the code that used to work fine beforehand (my previous version was 2.1.0). I am not sure if it's some kind of deprectation or etc.


It's happening on a brach of [Dropout + BB-alpha for detecting adversarial examples](https://github.com/YingzhenLi/Dropout_BBalpha/). Unfortunately, I coudn't share my own version due to disclusure of the project. Thanks for being understanding. 

I found [Add Stateful (Global) Metrics #9200](https://github.com/keras-team/keras/pull/9200/files/1ba271554f3cebf6268d382090e7097f075e5794), but I couldn't track down changes. Any idea how to solve the issue? 

Thanks,
Shek 

- [checked] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [checked] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

",,"['Have you compiled your model?\r\nThe field is initialized there : https://github.com/keras-team/keras/blob/master/keras/engine/training.py#L855', '@Dref360 Thanks for your answer. Yes.\r\nI used the exact same code before updating my Keras version, and It was working fine. Nothing has changed in the code.  ', 'I\'m also seeing this error on Keras v2.1.4 only. \r\n\r\nFor more information, I\'m trying to load the model from files:\r\n\r\n```{python}\r\nfrom keras.models import model_from_json\r\n\r\n# load json and create model\r\njson_file = open(LSTM_MODEL, \'r\')\r\nloaded_model_json = json_file.read()\r\nloaded_model = model_from_json(loaded_model_json)\r\n# load weights into new model\r\nloaded_model.load_weights(MODEL_WEIGHTS)\r\n```\r\nThe error occurs in the predict_proba call, which should not need compiling.\r\n\r\n```\r\nscore = loaded_model.predict_proba(seq_array,verbose=1)\r\nprint(score.shape)\r\nprint(score)\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-23-e9fff1ef7d52> in <module>()\r\n----> 1 score = loaded_model.predict_proba(seq_array,verbose=1)\r\n      2 print(score.shape)\r\n      3 print(score)\r\n\r\n~/lib/conda/envs/azureml_7753abb5a4b4d3655662bae7f7e3b6fe/lib/python3.5/site-packages/keras/models.py in predict_proba(self, x, batch_size, verbose, steps)\r\n   1110             A Numpy array of probability predictions.\r\n   1111         """"""\r\n-> 1112         preds = self.predict(x, batch_size, verbose, steps=steps)\r\n   1113         if preds.min() < 0. or preds.max() > 1.:\r\n   1114             warnings.warn(\'Network returning invalid probability values. \'\r\n\r\n~/lib/conda/envs/azureml_7753abb5a4b4d3655662bae7f7e3b6fe/lib/python3.5/site-packages/keras/models.py in predict(self, x, batch_size, verbose, steps)\r\n   1023             self.build()\r\n   1024         return self.model.predict(x, batch_size=batch_size, verbose=verbose,\r\n-> 1025                                   steps=steps)\r\n   1026 \r\n   1027     def predict_on_batch(self, x):\r\n\r\n~/lib/conda/envs/azureml_7753abb5a4b4d3655662bae7f7e3b6fe/lib/python3.5/site-packages/keras/engine/training.py in predict(self, x, batch_size, verbose, steps)\r\n   1840         f = self.predict_function\r\n   1841         return self._predict_loop(f, ins, batch_size=batch_size,\r\n-> 1842                                   verbose=verbose, steps=steps)\r\n   1843 \r\n   1844     def train_on_batch(self, x, y,\r\n\r\n~/lib/conda/envs/azureml_7753abb5a4b4d3655662bae7f7e3b6fe/lib/python3.5/site-packages/keras/engine/training.py in _predict_loop(self, f, ins, batch_size, verbose, steps)\r\n   1290             else:\r\n   1291                 progbar = Progbar(target=num_samples,\r\n-> 1292                                   stateful_metrics=self.stateful_metric_names)\r\n   1293 \r\n   1294         indices_for_conversion_to_dense = []\r\n\r\nAttributeError: \'Model\' object has no attribute \'stateful_metric_names\'\r\n```\r\n\r\nOn Keras 2.1.3, this returned the expected output.\r\n\r\n```\r\nscore = loaded_model.predict_proba(seq_array,verbose=1)\r\nprint(score.shape)\r\nprint(score)\r\n15631/15631 [==============================] - 11s 698us/step\r\n(15631, 1)\r\n[[3.7404552e-05]\r\n [3.7972197e-05]\r\n [3.8670321e-05]\r\n ...\r\n [9.9917173e-01]\r\n [9.9919158e-01]\r\n [9.9921858e-01]]\r\n\r\n\r\n```\r\n', ""Quick fix for both of you and I'll do something more permanent \r\njust do a dummy model.compile\r\n```python\r\nloaded_model = model_from_json(loaded_model_json)\r\n# load weights into new model\r\nloaded_model.load_weights(MODEL_WEIGHTS)\r\nloaded_model.compile('sgd','mse')\r\n```\r\nThis won't affect your results for prediction in any way. "", 'works for me. Thanks.', '@Dref360 thank you very much. It seems, it solved my issue as well. ', '@Dref360 thanks. so nice.', 'this is my code \r\n\r\n- `\r\n\r\n1. from keras.preprocessing.image import load_img\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nimport csv, re, os\r\nimport numpy as np\r\nfrom keras.preprocessing.image import img_to_array\r\nfrom keras.applications.imagenet_utils import decode_predictions\r\nimport matplotlib.pyplot as plt\r\nimport cv2\r\nimport keras\r\nfrom keras import Model\r\nfrom keras.layers import Dense, Dropout\r\nimport numpy as np\r\nfrom keras.utils import plot_model\r\nfrom keras.applications import inception_v3\r\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = \'0\'\r\nfrom utils import layer_utils\r\n\r\n\r\n\r\ndensenet_model = keras.applications.densenet.DenseNet201(weights=None, include_top=False, input_shape=(None,None,3))\r\n#all_layers = layer_utils.get_layer_names_bw_2_layers(densenet_model, \'input_1\', \'pool 1\')\r\n\r\ninput_img = densenet_model.get_layer(\'input_1\').output\r\nencoded = densenet_model.get_layer(\'pool1\')(input_img)\r\ndecoded = densenet_model.get_layer(\'input_1\')(encoded)\r\n\r\n\r\nmodel = Model(input_img, decoded)\r\n\r\n\r\nmodel.compile(loss=""binary_crossentropy"", optimizer=\'adadelta\')\r\n#mypath = \'images/\'\r\nencoder = Model(input_img, encoded)\r\nencoded_input = keras.layers.Input(shape=(1024,1024,3))\r\ndecoder_layer = model.layers[-1]\r\ndecoder = Model(encoded_input, decoder_layer(encoded_input))\r\ntrain_datagen = ImageDataGenerator(rescale=1. / 255,\r\n        fill_mode=\'constant\', cval=0.,\r\n        zoom_range=0,\r\n        rotation_range=0,\r\n        width_shift_range=0,\r\n        height_shift_range=0,\r\n        shear_range=0,\r\n        validation_split=0.1\r\n    )\r\ntrain_generator = train_datagen.flow_from_directory( \'organized_data22/\',batch_size=128, class_mode=\'input\', subset=\'training\')\r\ntest_generator = train_datagen.flow_from_directory( \'organized_data22/\', batch_size=128, class_mode=\'input\', subset=\'validation\')\r\n\r\n\r\nfor s in range(1000):\r\n        w = s+1\r\n        print(\'Iteration - \'+str(w))\r\n        model.fit_generator(train_generator,\r\n                epochs=1,\r\n                validation_data=test_generator,\r\n                )\r\n\r\n        model.save(\'models/inception_animal_classify_new_test_train_\'+str(i)+\'.hf5\')\r\n        encoded_imgs = encoder.predict(x_test)\r\n        decoded_imgs = decoder.predict(encoded_imgs)\r\n        for i in range(n):\r\n                # display original\r\n                ax = plt.subplot(2, n, i + 1)\r\n plt.imshow(x_test[i].reshape(28, 28))\r\n                plt.gray()\r\n                ax.get_xaxis().set_visible(False)\r\n                ax.get_yaxis().set_visible(False)\r\n\r\n                # display reconstruction\r\n                ax = plt.subplot(2, n, i + 1 + n)\r\n                plt.imshow(decoded_imgs[i].reshape(28, 28))\r\n                plt.gray()\r\n                ax.get_xaxis().set_visible(False)\r\n                ax.get_yaxis().set_visible(False)\r\n        plt.show()\r\nshowing error\r\nTraceback (most recent call last):\r\n  File ""densenet_autoencoder.py"", line 28, in <module>\r\n    model = Model(input_img, decoded)\r\n  File ""/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py"", line 91, in wrapper\r\n    return func(*args, **kwargs)\r\n  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/network.py"", line 91, in __init__\r\n    self._init_graph_network(*args, **kwargs)\r\n  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/network.py"", line 183, in _init_graph_network\r\n    \'The tensor that caused the issue was: \' +\r\nAttributeError: \'Model\' object has no attribute \'name\'\r\n']","['\r\nTraceback (most recent call last):\r\n  File ""template_model_1.py"", line 118, in <module>\r\n    acc, ll = test_MC_dropout(test_model, X_test, Y_test)\r\n  File ""/data/home/shekoofeh/Project/Dropout-BB/BBalpha-TeUS/BBalpha_dropout.py"", line 72, in test_MC_dropout\r\n    pred = model.predict(X, verbose=1)  # N x K x D\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 1842, in predict\r\n    verbose=verbose, steps=steps)\r\n  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 1292, in _predict_loop\r\n    stateful_metrics=self.stateful_metric_names)\r\nAttributeError: \'Model\' object has no attribute \'stateful_metric_names\'\r\n']",[],0,0
522,keras,3538,closed,How to reset keras random state many times inside python?,"For generating different initial random state for neural network I have been doing this based on 
suggestions in the forum:

import numpy as np ;
import datetime ; 
np.random.seed(datetime.datetime.now().microsecond)
from keras import ....

(setting numpy random seed before importing anything from keras). 

Now, assume that I want to train/evaluate a neural network with different initial random states, 
for 10 times, and take average of f-scores. I had to run the file externally for 10 times (e.i., python myprog.py) to make it work. 

Now assume I want to this inside python:

for i in range(10):
         import numpy as np ;
         import datetime ; 
         np.random.seed(datetime.datetime.now().microsecond)
         from keras import ....
         train()
         evaluate()

but I guess this will not do it, because keras is already imported. 
does **del np, del keras**, inside the function works? 
does **reload (np), reload (keras)** works ? 

**What is the correct way of doing this?** 

Thanks in advance. 
",,['I tested. Above code works perfectly for me. No need to do anything else :)\n'],[],[],0,0
523,keras,6852,closed,Invalid `output_shape` of a model,"Consider the following example:



This seems to cause confusing bugs downstream:
ConcatenateConcatenate

However if we just examine the size of  all seems fine:

",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']","['python\r\nseq_len = 5\r\nseq_of_int = Input((seq_len,))\r\nseq_of_3vec = Embedding(10, 3)(seq_of_int)\r\nseq_of_1vec = Flatten()(Embedding(10, 1)(seq_of_int))\r\nout = Dot((0,0))([seq_of_1vec, seq_of_3vec])\r\nm = Model(seq_of_int, out)\r\n\r\nprint(m.output_shape)\r\n# Outputs (5,3)\r\n\r\nprint(m.predict(np.arange(5).reshape(1,-1)).shape)\r\n# Outputs (1,3)\r\n', '\r\n\r\nThe concatenation problem occurs without using an intermediate model:\r\n', 'python\r\nout.get_shape()\r\n# TensorShape([Dimension(None), Dimension(3)])\r\n']","['', 'python\r\nother = concatenate([m(seq_of_int), Input((3,))])\r\n# ValueError: ', ' layer requires inputs with matching\r\n#     shapes except for the concat axis. Got inputs shapes: [(5, 3), (None, 3)]\r\npython\r\nother = concatenate([out, Input((3,))])\r\n# ', ' layer requires inputs with matching shapes \r\n#   except for the concat axis. Got inputs shapes: [(5, 3), (None, 3)]\r\n', '', 'out']",0,0
524,keras,13194,closed,"Hi, ","Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",,[],[],[],0,0
525,keras,4738,closed,Vgg16 example: Layer weight shape not compatible with provided weight shape,"This is the error and it seems that the layer shape shown in the example is not the same as the 'vgg16_weights.h5' downloaded. 

**Here is the error message:**
Traceback (most recent call last):
<module>
    model.layers[k].set_weights(weights)
  File ""/usr/local/lib/python3.4/dist-packages/keras/engine/topology.py"", line 889, in set_weights
    'provided weight shape ' + str(w.shape))
Exception: Layer weight shape (3, 3, 128, 64) not compatible with provided weight shape (64, 3, 3, 3)

**System configuration:**
Running vgg16 example on ubuntu 14.04, python 3.4
Using TensorFlow backend",,"[""Are you using `keras.applications.VGG16`? That's what you should be using."", ""thanks, i was following:\nhttps://blog.keras.io/how-convolutional-neural-networks-see-the-world.html\nthus using:\nhttps://github.com/fchollet/keras/blob/master/examples/\nconv_filter_visualization.py\n\n\n\n\nOn Sat, Dec 17, 2016 at 6:05 AM, François Chollet <notifications@github.com>\nwrote:\n\n> Are you using keras.applications.VGG16? That's what you should be using.\n>\n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/fchollet/keras/issues/4738#issuecomment-267707144>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AJyMP-1SJ-b2d7_gV8HF2vFZ3OH5AYhiks5rIwsOgaJpZM4LO3_s>\n> .\n>\n"", 'changed to vgg16 under applications. no problem now.', '@jeffreynghm \r\nHow did u corrected this error? The explanation given here not clear to me.', '@sajjo79 , look for file under a different directory. e.g. for my case it is under /usr/local/lib/python 3.5/dist-packages/keras/applications/']",[],[],0,0
526,keras,1153,closed,print_layer_shapes is gone,"In a744b60 this highly useful function disappeared. What is the reason, and is there a chance of bringing it back?
",,"['The reason is that Keras now does automatic shape inference, and you can get the output shape of a layer by calling `layer.output_shape`. \n\nWhat we could potentially bring back would be a convenience function that simply calls `output_shape` on all layers of a model.\n', 'My use case is figuring out the output dimension of stacked 1D convolutional layers. As long as no input is provided, the shape along the convolutional dimension will remain unknown, or am I missing something?\n']",[],[],0,0
527,keras,3309,closed,"Hi,","Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",,[],[],[],0,0
528,keras,5989,closed,Keras 2. - resnet training speed,"Description
-----------------------
I used to transfer learning from Resnet50, basicly retrain it with some different top network above it. It worked just fine. But now with Keras 2 it seems that it takes forever. I'm using  and the code is runs on GPU ( I ran  to verify that GPU is actually being used and it does)

I checked both  and  backend. I'm getting huge number like 1151049s per epoch. Which means around 13 days! Where before the update I got around 4000s per epoch

Any ideas what can be the cause for that? 


Thanks!



Some details
-----------------------

1) Ubuntu 16 LTS
2) I'm using fit_generator
3) Tesla K80
4) tested with keras 2.0.1 and 2.0.2
5) Theano - update from branch master
6) Tensorflow - 1.0.1
7) Running python mnist_cnn.py from  takes aroudn 8s per epoch on  backend



- [Done ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [Done] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [Done] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",,"['Hi,\r\nJust a wild guess:\r\n\r\nIn release notes it says:\r\n""The methods fit_generator, evaluate_generator and predict_generator now work by drawing a number of batches from a generator (number of training steps), rather than a number of samples.""\r\n\r\nSo can it be that you didn\'t change the value of argument when moving from ""number of samples"" to ""number of batches""?  That would explain it because your epochs would be batchsize times longer.  \r\n\r\nPlease check.', 'Wow! I have missed that! Thanks']",[],"['Tesla K80', 'sudo watch nvidia-smi', 'Tensorflow', 'Theano', 'example', 'Theano']",0,0
529,keras,2845,closed,Error: AbstractConv2d Theano optimization failed,"Hi people.
I'm new to both Neural networks and Keras.
I'm working on ipython notebook & Windows 8.1 on CPU.

**I'm getting this error while trying to train the network:
AssertionError: AbstractConv2d Theano optimization failed: there is no implementation available supporting the requested options. Did you exclude both ""conv_dnn"" and ""conv_gemm"" from the optimizer? If on GPU, is cuDNN available and does the GPU support it? If on CPU, do you have a BLAS library installed Theano can link against?**

I've looked at similar issues on Theano/Keras github issues page. 
Almost all the solutions pointed towards updating Theano & Keras. 
I've tried updating both of them, and that didn't resolve the issue. 

Is the issue in how we give the images to the training function(model.fit)?
In what format should the 2D grey scale images be given as the training set?
Could someone please help me out in fixing this.

Please find the link to the code & error [here](https://github.com/Vivek-B/Kaggle/blob/master/Digit%20Recognition/Conv-2D%20--%20Keras.ipynb)
",,"['It means you need to install a BLAS (in order to run on CPU). Look at http://www.openblas.net/\n', ""I've tried hard to link BLAS to theano, but had no luck. Eventually I gave up on installing Theano on windows. \nWhy don't you try installing the same libraries on some distribution of Linux. Its very simple compared to windows.\n"", 'Installing the Theano with blas on Windows should be simple with anaconda.\nIt is the GPU that is hard. At least from memory. I didn\'t do it recently.\n\nUsing Linux will make sure you have the best Theano experience as this is\nwhat we use to develop it.\n\nFred\n\nLe 14 sept. 2016 05:20, ""Bakaraju Vivek"" notifications@github.com a\nécrit :\n\n> I\'ve tried hard to link BLAS to theano, but had no luck. Eventually I gave\n> up on installing Theano on windows.\n> Why don\'t you try installing the same libraries on some distribution of\n> Linux. Its very simple compared to windows.\n> \n> —\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/fchollet/keras/issues/2845#issuecomment-246954677,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AALC-8oKr4KczoAfQoKkjvr5U9SmwjTsks5qp7xNgaJpZM4IpVjN\n> .\n', 'Still having this problem. in 2016 Dec 18\r\n\r\nMy system is Windows 7 pro sp1. Blas and Theano are installed but still having exactly same error. \r\n\r\nPlease someone look into it.\r\n\r\nThank you.', 'This should be working. Be sure to install Python with Anaconda and install\nTheano dev version. It was tested recently and it was working.\n\n\nOn Sun, Dec 18, 2016 at 10:22 AM, Lothian <notifications@github.com> wrote:\n\n> Still having this problem. in 2016 Dec 18\n>\n> My system is Windows 7 pro sp1. Blas and Theano are installed but still\n> having exactly same error.\n>\n> Please someone look into it.\n>\n> Thank you.\n>\n> —\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/fchollet/keras/issues/2845#issuecomment-267827161>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AALC-_787xI6nSi_-XdML8iPIO4MdjKfks5rJU-xgaJpZM4IpVjN>\n> .\n>\n', '@nouiz  \r\nI am using a ubuntu 32 bit system and installed keras and theano and openblas by building but still getting the error \r\nmy config file is : \r\n[global]\r\nfloatX = float32\r\ndevice = cpu\r\n\r\n[blas]\r\nldflags = -L/usr/local/lib -lopenblas\r\n\r\nand ERROR is \r\nERROR (theano.gof.opt): Optimization failure due to: local_abstractconv_check\r\nERROR (theano.gof.opt): node: AbstractConv2d{border_mode=\'half\', subsample=(1, 1), filter_flip=True, imshp=(None, None, None, None), kshp=(32, 3, 3, 3)}(convolution2d_input_3, convolution2d_9_W)\r\nERROR (theano.gof.opt): TRACEBACK:\r\nERROR (theano.gof.opt): Traceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/theano/gof/opt.py"", line 1772, in process_node\r\n    replacements = lopt.transform(node)\r\n  File ""/usr/local/lib/python2.7/dist-packages/theano/tensor/nnet/opt.py"", line 402, in local_abstractconv_check\r\n    node.op.__class__.__name__)\r\nAssertionError: AbstractConv2d Theano optimization failed: there is no implementation available supporting the requested options. Did you exclude both ""conv_dnn"" and ""conv_gemm"" from the optimizer? If on GPU, is cuDNN available and does the GPU support it? If on CPU, do you have a BLAS library installed Theano can link against?', 'met the same problem and fixed by adding ""optimizer = None"" in "".theanorc""', ""I can confirm that on Windows 7 64 bit and trying lapack and openblas, there appears to be no compatibility with blas and theano and the abstractconv2d error always turns up. Certainly setting optimizer=None goes around the problem but at the expense of no optimisation and therefore only useful for code debugging (slowly) and not running training.\r\n\r\nI run two Pascal gpu's but I would really like to also run on CPU for development and debug so as not to tie one up as they run 24/7.\r\n\r\nOne possible problem is that the error message is not related to the problem, for instance the actual blas lib may be of an incompatible build to theano. I tried 32 and 64 bit binaries. Mingw64 is installed and there are no problems with compiling for GPU (it is really easy once you have done it and documented the procedure for yourself).\r\n\r\nWould it be possible for the theano team to try out a vanilla win64 box and bring it up with Anaconda2 and a fresh theano install, find and install to taste a blas version, document the procedure precisely with all bells and whistle, and link it out from the main theano page on deeplearning.net for all to access directly ? Thanks and much appreciated Frederic.\r\n\r\nBest regards, Brendan\r\n"", 'Continuum is doing conda packages for Theano that include mkl. While doing\nso, they pushed upstream fixes to have Theano link with there mkl version.\nSo now again, Theano link with anaconda mkl on windows.\n\nThe current packages aren\'t on the master of anaconda. They will be there\nwhen we release Theano 0.9. We have the packages for the beta and new\npackages (still at non default place) should be done this week after the rc.\n\nCan you confirm that this will fix your issue? Updating Theano do the dev\nversion and using anaconda while installaing the packages ""mkl-service""\nshould have you working well right now.\n\nOn Mon, Jan 23, 2017 at 12:39 PM brendanruff <notifications@github.com>\nwrote:\n\n> I can confirm that on Windows 7 64 bit and trying lapack and openblas,\n> there appears to be no compatibility with blas and theano and the\n> abstractconv2d error always turns up. Certainly setting optimizer=None goes\n> around the problem but at the expense of no optimisation and therefore only\n> useful for code debugging (slowly) and not running training.\n>\n> I run two Pascal gpu\'s but I would really like to also run on CPU for\n> development and debug so as not to tie one up as they run 24/7.\n>\n> One possible problem is that the error message is not related to the\n> problem, for instance the actual blas lib may be of an incompatible build\n> to theano. I tried 32 and 64 bit binaries. Mingw64 is installed and there\n> are no problems with compiling for GPU (it is really easy once you have\n> done it and documented the procedure for yourself).\n>\n> Would it be possible for the theano team to try out a vanilla win64 box\n> and bring it up with Anaconda2 and a fresh theano install, find and install\n> to taste a blas version, document the procedure precisely with all bells\n> and whistle, and link it out from the main theano page on deeplearning.net\n> for all to access directly ? Thanks and much appreciated Frederic.\n>\n> Best regards, Brendan\n>\n> —\n> You are receiving this because you were mentioned.\n>\n>\n> Reply to this email directly, view it on GitHub\n> <https://github.com/fchollet/keras/issues/2845#issuecomment-274560510>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AALC-_wioHSardYgYBuAquYy-ZE-87kMks5rVOXEgaJpZM4IpVjN>\n> .\n>\n', ""I'm also getting the same error but still was unable to find a fix. "", '\r\nI am also facing the same error \r\nAssertionError: AbstractConv2d Theano optimization failed: there is no implementation available supporting the requested options. Did you exclude both ""conv_dnn"" and ""conv_gemm"" from the optimizer? If on GPU, is cuDNN available and does the GPU support it? If on CPU, do you have a BLAS library installed Theano can link against?\r\n\r\nI aslo installed blas library but all is fruitless \r\n![blas](https://user-images.githubusercontent.com/26816358/37593276-33cd1820-2b93-11e8-8dcf-d57ccffa9d98.PNG)\r\n\r\n', '\r\n[.theanorc.txt](https://github.com/keras-team/keras/files/2204972/default.theanorc.txt)\r\n[global]\r\nfloatX = float32\r\ndevice = cpu\r\noptimizer = None\r\n\r\nadd this as .theanorc.txt to C:/ users/whatever\r\nHope this helps.', '@suvodip1212 \r\n[global]\r\nfloatX = float32\r\ndevice = cpu\r\noptimizer = None\r\nthanks,this method help me resolve this issue', '@duqq where i can add this method ? ', ""Do  \r\n```\r\nimport os\r\nos.environ['THEANO_FLAGS'] = 'optimizer=None'\r\n```\r\ninstead"", ""> Do\r\n> \r\n> ```\r\n> import os\r\n> os.environ['THEANO_FLAGS'] = 'optimizer=None'\r\n> ```\r\n> \r\n> instead\r\n\r\nthanks\r\n""]",[],[],0,0
530,keras,1741,closed,does my similairty matrix layer implementation right?,"hi, guys:
   why Merge layer doesn't exist 'similarity matrx' merge-mode? i implement a simiarity matrix layer, but i suspect its effectiveness? is my implementation right?


def __init__(self, dim1, dim2, layers, init='uniform',
             W_regularizer=None, activity_regularizer=None,
             W_constraint=None, **kwargs):

    assert len(layers) == 2
    self.layers = layers
    self.dim1 = dim1
    self.dim2 = dim2

    self.init = initializations.get(init)
    self.W_constraint = constraints.get(W_constraint)
    self.W_regularizer = regularizers.get(W_regularizer)
    self.activity_regularizer = regularizers.get(activity_regularizer)

    self.params = []
    self.regularizers = []
    self.constraints = []
    self.updates = []
    for l in self.layers:
        params, regs, consts, updates = l.get_params()
        self.regularizers += regs
        self.updates += updates
        # params and constraints have the same size
        for p, c in zip(params, consts):
            if p not in self.params:
                self.params.append(p)
                self.constraints.append(c)

    self.W = self.init((self.dim1, self.dim2))
    self.params.append(self.W)

    if self.W_regularizer:
        self.W_regularizer.set_param(self.W)
        self.regularizers.append(self.W_regularizer)

    if self.activity_regularizer:
        self.activity_regularizer.set_layer(self)
        self.regularizers.append(self.activity_regularizer)

    super(SimilarityMatrix, self).__init__(**kwargs)

@property
def output_shape(self):
    return [self.layers[0].output_shape[0], 1]

def get_params(self):
    return self.params, self.regularizers, self.constraints, self.updates

def get_input(self, train=False):
    res = []
    for i in range(len(self.layers)):
        o = self.layers[i].get_input(train)
        if not type(o) == list:
            o = [o]
        for output in o:
            if output not in res:
                res.append(output)
    return res

def get_output(self, train):

    s1 = self.layers[0].get_output(train)
    s2 = self.layers[1].get_output(train)

    sim = T.sum(T.dot(s1, self.W)*s2, axis=1)

    return sim.dimshuffle(0,'x')

@property
def input(self):
    return self.get_input()

def supports_masked_input(self):
    return False

def get_output_mask(self, train=None):
    return None

def get_weights(self):
    weights = []
    for l in self.layers:
        weights += l.get_weights()
    return weights

def set_weights(self, weights):
    for i in range(len(self.layers)):
        nb_param = len(self.layers[i].params)
        self.layers[i].set_weights(weights[:nb_param])
        weights = weights[nb_param:]
",stale,[],[],"['class SimilarityMatrix(Layer):\n\n', '', '\n', '']",0,0
531,keras,12652,closed,K.clear_session() does not reset layer naming conventions,"python 3.7 tensorflow 2.0

Create a model. K.clear_session(). Create a model.

The layer names are not being reset. Can create example if this does not make sense immediately.",,['The backend function: ``keras.backend.reset_uids()`` does what you want.'],[],[],0,0
532,keras,895,closed,weird memory error,"So I'm not sure why this is coming up. I included the snippet of code and the error message. This seems to be stemming from NumPy, what I'm really confused on is why. When I run it, I often dip way into my memory before the error comes up. I feel like I'm missing something obvious. 

Thanks in Advance!

Error Message:



Snippet of Code Below:


",,['Your GPU is out of memory. You can either reduce your batch size or reduce the size of your network.\n'],"['\nUsing gpu device 0: GeForce GTX 980 (CNMeM is disabled)\nTraceback (most recent call last):\n  File ""prototype_two.py"", line 65, in <module>\n    model.add(Dense(2048));\n  File ""build/bdist.linux-x86_64/egg/keras/layers/containers.py"", line 37, in add\n  File ""build/bdist.linux-x86_64/egg/keras/layers/core.py"", line 37, in set_previous\n  File ""build/bdist.linux-x86_64/egg/keras/layers/core.py"", line 557, in build\n  File ""build/bdist.linux-x86_64/egg/keras/initializations.py"", line 43, in glorot_uniform\n  File ""build/bdist.linux-x86_64/egg/keras/initializations.py"", line 16, in uniform\n  File ""build/bdist.linux-x86_64/egg/keras/utils/theano_utils.py"", line 12, in sharedX\n  File ""/usr/lib/python2.7/dist-packages/numpy/core/numeric.py"", line 460, in asarray\n    return array(a, dtype, copy=False, order=order)\nMemoryError\n', '\nmodel = Sequential()\n# C1\nmodel.add(Convolution2D(nb_filter=96,    # number of kernels\n                        nb_row=11,       # number of rows in the kernel\n                        nb_col=11,       # number of columns in the row\n                        border_mode=""full"",\n                        input_shape=(3,X1,X2)\n                    ));     \nmodel.add(LeakyReLU(alpha=LEAKY_RELU));\n\n# Flatten\nmodel.add(Flatten());\n\n# D1\nmodel.add(Dense(2048));\nmodel.add(LeakyReLU(alpha=LEAKY_RELU));\n# D2\nmodel.add(Dense(1024));\nmodel.add(LeakyReLU(alpha=LEAKY_RELU));\n\n# O\nmodel.add(Dense(NUM_CLASSES,activation=\'softmax\'));\n']",[],0,0
533,keras,11885,closed,How can I remove / delete specific weights from a neural network layer (Dense layer or convolution layer) in Keras (or Tensorflow)?,"Please make sure that the boxes below are checked before you submit your issue.
If your issue is an **implementation question**, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [on the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) instead of opening a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:


- [x] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",type:support,"['I am in need of a help. @fchollet \r\nI have a neural network model in Keras which is trained. So, I have weight values.\r\nNow, I have to remove / delete certain weight connections that meet a particular criteria.\r\n(So that the network size is reduced)\r\n\r\nIs there a way in Keras or Tensorflow to do this ?\r\n\r\nI have tried to make a custom layer, but am unable to set weights. \r\nChallenges I am facing are:\r\nDo I need to replace Dense / Convolutional layer with my custom layer? Or can I do it in existing Dense layer by some means? \r\n\r\nSecondly, if I have to use a custom layer, then how to proceed with weight updates. \r\n\r\nPlease advise and help.\r\nYour kind help will be much appreciated.', ""google: 'how to do transfer learning with keras'\r\nalso, you can use the functional API to create a new model from specific weights for example:\r\n\r\n```\r\nmodel = get_model()\r\nsmaller_model = Model(model.input, model.layers[-5].output) \r\n```\r\nIn this way you create a model that contains all the layers (with their weights) of the original model except the last 5"", ""@Golbstein Sorry Golbstein, but you did not understand the problem. Let me give an example of the problem. Suppose there is a neural network which has two dense layers A and B. So, all neurons in layer A would be connected to all neurons in layer B. Now, after the neural network is trained, the connections between neurons of layer A and layer B will have some weights. Now, I want to remove / delete some weights based on a specific criteria (such was delete weight connections where weight < 0.5). So is there a way to do this in Keras or Tensorflow ? I tried developing a custom layer in Keras but couldn't get how to remove specific weights. Hi @fchollet can you please help me in this regard. Tagging @harshini-gadige "", '@fchollet @harshini-gadige Any update? ', '@AksBrijSandy , please study this article and the paper to prune weights:\r\nhttps://nervanasystems.github.io/distiller/pruning/index.html\r\nhttps://openreview.net/pdf?id=HJIY0E9ge\r\nLet us know if you have implemented this. Closing issue for now. Thanks.\r\n', '@AksBrijSandy , Here is a Stack Overflow post that also addresses your query:\r\nhttps://stackoverflow.com/questions/53815904/how-can-i-remove-delete-specific-weights-below-a-threshold-value-from-a-neural/53816353', 'You should know that making a weight zero does not mean to delete it. Because backpropagation is still taking it into the calculation. So a subfunction required to delete a specific connection.']",[],['pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps'],0,0
534,keras,699,closed,Has an NTM actually been built in keras?,"On the home page of the documentation, there is a claim that ""...Building ... a Neural Turing Machine ... is just as fast"". This doesn't sound implausible, but I didn't think anyone had successfully reproduced the results from the NTM paper; does anyone know if this is referring to a specific NTM implementation, and if so, how well that implementation worked?

from http://keras.io/
",,"['Maybe not, but how hard would it be? Just use a `Graph` container.\n\nThis specific sentence in the doc is meant to draw attention to the fact that arbitrary directed acyclic graphs are supported (such as NTMs), not just Sequential models.\n', 'fair enough. In that case, it seems worth testing.\n']",[],[],0,0
535,keras,5990,closed,Keras API ,"I am new to Keras so maybe I just don't know it better yet but there are a few things I noticed.

For example  changes its return type from a scalar to a list depending on whether the network has just one or many outputs. If I wanted to create something more generic that handles both cases, I am forced to perform type-checking operation on my end. This is why I wanted to ask whether it was possible to change thing like this and simply _always_ return a list.

Something similar, but way worse, happens when one changes the model inputs from single, to multi channel. I noticed that I have to change because in a single-input network I might hand in a  matrix whereas this changes to a  for each input where each list has to contain the data for each channel etc. Imho it would be easier to simply _always_ demand a list because that way you do not run into the issue where you have to re-write your batch generation logic. 

This is not a complaint. I love Keras and I think it's awesome and please forgive me if I am wrong with what I suggest here but if not it would be great to keep such things in mind.

And since I am on it .. I noticed that _a lot_ of questions are getting asked here on github. A lot of questions that would also fit to stackoverflow. I am not sure why but wouldn't it be better to enforce this as some point by close all new issues and ask the people to move their general questions to stackoverflow? I am not affiliated with so but it's simply much easier to get an overview of existing questions over there. 

What do you guys think?",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],"['train_on_batch()', '[batch_size, nb_samples, nb_dimensions]', 'list']",0,0
536,keras,3752,closed,is there a bug with 'relu'?,"Here is my network :

model = Sequential()
model.add(Convolution2D(128, 3, 3, border_mode='valid',input_shape=data.shape[-3:]))
model.add(Activation(act))
model.add(Convolution2D(128, 3, 3, border_mode='valid'))
model.add(Activation(act))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Convolution2D(256, 3, 3, border_mode='valid'))
model.add(Activation(act))
model.add(Convolution2D(256, 3, 3, border_mode='valid'))
model.add(Activation(act))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Convolution2D(512, 3, 3, border_mode='valid'))
model.add(Activation(act))
model.add(MaxPooling2D(pool_size=(2, 2)))
# 

model.add(Flatten())
model.add(Dense(4096, init='normal'))
model.add(Activation(act))
model.add(Dropout(0.5))
model.add(Dense(4096, init='normal'))
model.add(Activation(act))
model.add(Dropout(0.5))

model.add(Dense(classNumber, init='normal'))
model.add(Activation('softmax'))

First I set The act = 'relu' ,  the loss and accuracy does't change.
Then I changed the act = 'tanh' and traing it again,  the loss and acc were normal。
I that a known issue？  
",,"[""Do you use negative inputs? ReLu will convert negative inputs to 0., whereas tanh can handle inputs between -1 and 1 just fine. ReLu is a very simple function, there won't be a bug in there.\n"", '@semitom  Thank you for reply me!\n\nI think I found the reason :  Setting the momentum=0.9 for  SGD optimizer will cause this problem. I changed momentum to 0.5 the training is normal.Here is my optimizer and model setting:\n\nsgd = SGD(lr=0.0005, decay=0.00001, momentum=0.9, nesterov=True)\nmodel.compile(loss=\'categorical_crossentropy\', optimizer=sgd,metrics=[""accuracy""])\nmodel.fit(data, label, batch_size=32, nb_epoch=3000,shuffle=True,verbose=1,validation_split=0.1)\n\nBut why did this happen?\nI tried another Deep learning framework , let the momentum=0.9 ,using \'relu\' activation and the same traing data .There was no problem.\n\nThank you again!\n']",[],[],0,0
537,keras,4605,closed,Training keras LSTM model with variable-length-sequence: mask or pading or batch_size = 1 or group ?,"Hey! Guys, recently I'm working on a project for training LSTM with variable-length sequences, the length of the sequences is 4-12, I'm using one-hot representation, following the example here: [https://github.com/erikbern/rnn-lang-model/blob/master/train_lstm.py](url) , but the problem is that I have to train the network with variable-length sequences,
**First**, I tried to use making layer, but I don't know if my code is correct:(I pasted major part of the code)

training data and validation data: where and mylist_test is list of sequences, is total vocabulary of the letters of sequences, this is to get one-hot representation of the training data and validation data


network: 

then is training:


**second**, I tried to train the network with one pair of data per-time, which is  method and remove the masking layer, to test if my use of masking layer is correct
**third**, I tried to use the 'pad with neutral data ' method following [https://github.com/fchollet/keras/issues/40](url), I paded the sequences with end symbol 

I have not tried group method yet, the performance of my above try are(evaluated with validation loss): 
batch_size =1 method >masking >padding with neutral data

my question is : am I right on using the masking method?  since its performance is much lower than the batch_size =1 method, I think there is some problem with my using of masking layer, can anyone help me ? thanks in advance!",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs, but feel free to re-open it if needed.\n']","['\r\nX = np.zeros((len(mylist), MAXLEN, len(chars)), dtype=np.bool)\r\ny = np.zeros((len(mylist), MAXLEN, len(chars)), dtype=np.bool)\r\n\r\nfor i, sentence in enumerate(mylist):\r\n    for t in range(len(sentence)-Data_end):\r\n        X[i, t, char_indices[sentence[t]]] = 1\r\n        y[i, t, char_indices[sentence[t+1]]] = 1\r\n', '\r\nX_val = np.zeros((len(mylist_test), MAXLEN, len(chars)), dtype=np.bool)\r\ny_val = np.zeros((len(mylist_test), MAXLEN, len(chars)), dtype=np.bool)\r\n\r\nfor i, sentence in enumerate(mylist_test):\r\n    for t in range(len(sentence)-Data_end):\r\n        X_val[i, t, char_indices[sentence[t]]] = 1\r\n        y_val[i, t, char_indices[sentence[t+1]]] = 1\r\n', ""\r\nmodel = Sequential()\r\nmodel.add(Masking(mask_value=0., input_shape=(None, len(chars))))\r\nmodel.add(LSTM(2000, return_sequences=True))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(LSTM(2000, return_sequences=True))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(TimeDistributed(Dense(len(chars))))\r\nmodel.add(Activation('softmax'))\r\n""]","['mylist ', 'chars ', 'model.fit(X, y, callbacks=[early_stopping],validation_data=(X_val, y_val),batch_size=32, nb_epoch=1)', 'batch_size = 1', '/n']",0,0
538,keras,12083,closed,Multiprocessing with model.fit_generator and data augmentation,"I need to use the model.fit_generator method with use_multiprocessin=True and workers>1 because I want to parallelize augmentation.

The Keras ImageDataGenerator would be a perfect match for the model.fit_generator. But several people found out, that this causes problems because of lacking thread safety. 

Is there a simple way to make those two work together? Not everyone working with Keras wants (or is able) to write a data generator from scratch.

It would be greatly appreciated if you would give an example in the documentation.
In my case, the images that need to be augmented are in directories. One for each class,

Thanks! ",,['Please submit your issue in keras-team/keras_preprocessing. Thanks! '],[],[],0,0
539,keras,4292,closed,SSIM as objective,"Please make sure that the boxes below are checked before you submit your issue. Thank you!

- [X] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [X] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [X] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).


Hi guys, 
I am wondering if any of you have implemented the SSIM (**structural similarity index**) to be used as objective. It is often used for measuring the similarity between two images **x** and **y**. Its formulation is as follow:
![image](https://cloud.githubusercontent.com/assets/810340/20015848/3a1ed87c-a2a4-11e6-825b-fac27edcb146.png)

I think this must be easy to implement using generic functions of the backends (Theano os TF), but I am not familiarized with them enough.
",stale,"['hi!\nI made my custom loss function but it requires tf 0.11rc. This is the DSSIM so (1- SSIM) / 2.\nhttps://gist.github.com/Dref360/a48feaecfdb9e0609c6a02590fd1f91b\n\nI got great result with it on image analysis task. (background extraction for example)\n', ""Thank you very much, I am currently using Theano, but I can use your code as basis for mine.\nI didn't understand why the need of the `tf.extract_image_patches`, couldn't I just take the means and variances of the true and predicted imgs on the batch?\n"", 'Yeah but it would be less precise. The mean of a 80x80 image is a lot less meaningfull than the mean of small 5x5 images. Theano has something like findNeighbors that replace this operation.\n', ""Thanks @Dref360 for the code sample and the point towards the comparable Theano function.\r\n\r\nBelow is the implementation for Theano that follows your code, or so I believe. However, I've seen little movement with the Keras optimizers and DSSIM as implemented below. It seems the loss fluctuates so little that as it backpropagates it goes almost directly to zero. Hundreds of thousands of examples barely get it to budge.\r\n\r\nIf tested this with multiple optimizers (AdaDelta and SGD, mostly) with the same result. I ended up having to explode the loss from the DSSIM function via either a power function or a multiplier of 10^6 or greater to get any movement in minimizing the loss.\r\n\r\nHave you seen this as well, or do you have any thoughts on this?\r\n\r\n```python\r\nfrom theano import tensor as T # for NNET module\r\nimport keras.backend as K\r\n\r\ndef loss_DSSIM_theano(y_true, y_pred):\r\n    \r\n    # expected net output is of shape [batch_size, row, col, image_channels]\r\n    # e.g. [10, 480, 640, 3] for a batch of 10 640x480 RGB images\r\n    # We need to shuffle this to [Batch_size, image_channels, row, col]\r\n    y_true = y_true.dimshuffle([0, 3, 1, 2])\r\n    y_pred = y_pred.dimshuffle([0, 3, 1, 2])\r\n    \r\n    \r\n    # There are additional parameters for this function\r\n    # Note: some of the 'modes' for edge behavior do not yet have a gradient definition in the Theano tree\r\n    #   and cannot be used for learning\r\n    patches_true = T.nnet.neighbours.images2neibs(y_true, [4, 4])\r\n    patches_pred = T.nnet.neighbours.images2neibs(y_pred, [4, 4])\r\n\r\n    u_true = K.mean(patches_true, axis=-1)\r\n    u_pred = K.mean(patches_pred, axis=-1)\r\n    var_true = K.var(patches_true, axis=-1)\r\n    var_pred = K.var(patches_pred, axis=-1)\r\n    std_true = K.sqrt(var_true)\r\n    std_pred = K.sqrt(var_pred)\r\n    c1 = 0.01 ** 2\r\n    c2 = 0.03 ** 2\r\n    ssim = (2 * u_true * u_pred + c1) * (2 * std_pred * std_true + c2)\r\n    denom = (u_true ** 2 + u_pred ** 2 + c1) * (var_pred + var_true + c2)\r\n    \r\n    ssim /= K.clip(denom, K.epsilon(), np.inf)\r\n    #ssim = tf.select(tf.is_nan(ssim), K.zeros_like(ssim), ssim)\r\n    \r\n    return K.mean((1.0 - ssim) / 2.0)\r\n```"", ""Your imp is so much nicer than mine :+1: \r\nI had the issue with the loss not moving at all and getting stuck in a local minima. I fixed it by adding a simple weighted (y-yi)^2. It solved my problem. Now with your problem for the exploding value, it's weird because DSSIM should be between 0 and 0.5. Maybe the output from images2neibs is not the same as the tf function..."", ""I meant that I had to explode the loss (in the same way you did, with an additional factor) to get movement - not that the loss exploded on it's own, as it is indeed locked between [0, .5].\r\n\r\nI've started thinking that the DSSIM isn't great for the issue I'm trying to solve which would explain the lack of optimization, but I think this implementation should be correct."", '@fchollet Do you have any requirements for adding loss functions? In other words, are you wanting to stick with the loss functions you have so far in Keras, with no additions?; or is there a chance to add something like this, where SSIM (DSSIM loss) is pretty heavily used in image comparison, moreso than MSE pixel differences for many applications?', 'Relevant article\r\nhttps://arxiv.org/pdf/1511.08861v2.pdf', '@Dref360 Thanks for the link, very interesting article.\r\nCurrently I am  working on other project, but I still wanna try some SSIM-based metrics on training (somewhat similar to what the authors reported in the article).\r\n\r\nBtw, I was wondering, how difficult would it be to use a combination of different loss functions (similar to what is done in the article), for example:\r\n\r\n```python\r\ndef loss_mix(y_true, y_pred):\r\n    return alpha*mean_absolute_error(y_true, y_pred) + beta* mean_squared_error(y_true, y_pred) + gamma * loss_DSSIM(y_true,y_pred)\r\n```\r\n\r\njust combining three losses and using different weights for each one?!\r\n', ""That's what I did, (DSSIM + L1) / 2 if you want your weight to be user define, you could create a functor.\r\n\r\nclass superLoss():\r\n   def __init__(self,a,b,c):\r\n      ....\r\n   def __call__(self,y_true,y_pred):\r\n     ....."", ""@patyork I've tried the exact code you've shared, but I am always getting 'nan' when computing the loss.\r\nMaybe some of the functions have changed in recent versions of Theano?"", ""The `patches_*` variables probably need to be reshaped to (.., 4, 4) in this case; I think it's currently (..., 16) which will probably mess up the variance calculations. Hopefully the supporting functions for this will be merged into the Keras backend soon, and this loss will be available for both backends via the contrib library."", ""@patyork Thanks for the code! Would like to add one little thing - you can reference `nnet` directly from `K` like this:\r\n```\r\nK.T.nnet.neighbours.images2neibs\r\n```\r\nTherefore you don't need this extra import:\r\n```\r\nfrom theano import tensor as T\r\n```"", '@ogurets That is true, however I prefer not to use K functions that are not available in both Theano and TensorFlow. The import makes it explicit that this is Theano only, whereas `K.T` appears to be backend independent if you don\'t know that `T.nnet...` is a part of Theano.\r\n\r\nAs I said, hopefully the ""neighbors"" code in #5248 is merged and this can become backend independent.', ""@patyork There was not an error on the shapes, I don't know why, but the K.sqrt was returning 'nan's. I've tried to take the abs of the var, and it didn't work. The solution I've found was adding an eps before taking the sqrt. The code  I am currently using is:\r\n\r\n```python\r\ndef loss_DSSIM_theano(y_true, y_pred):\r\n    # There are additional parameters for this function\r\n    # Note: some of the 'modes' for edge behavior do not yet have a gradient definition in the Theano tree\r\n    #   and cannot be used for learning\r\n    \r\n    patches_true = T.nnet.neighbours.images2neibs(y_true, [4,4])\r\n    patches_pred = T.nnet.neighbours.images2neibs(y_pred, [4,4])\r\n    u_true = K.mean(patches_true, axis=-1)\r\n    u_pred = K.mean(patches_pred, axis=-1)\r\n    var_true = K.var(patches_true, axis=-1)\r\n    var_pred = K.var(patches_pred, axis=-1)\r\n    eps = 1e-9\r\n    std_true = K.sqrt(var_true+eps)\r\n    std_pred = K.sqrt(var_pred+eps)\r\n    c1 = 0.01 ** 2\r\n    c2 = 0.03 ** 2\r\n    ssim = (2 * u_true * u_pred + c1) * (2 * std_pred * std_true + c2)\r\n    denom = (u_true ** 2 + u_pred ** 2 + c1) * (var_pred + var_true + c2)\r\n    ssim /= denom #no need for clipping, c1 and c2 make the denom non-zero\r\n    return K.mean((1.0 - ssim) / 2.0)\r\n\r\n```"", ""That's pretty weird. Don't really see how the root could cause nan/inf's in this case.\r\n\r\nNote: you can use `K.epsilon()` for consistency (there's already an epsilon defined in keras)."", ""I've had this issue with sqrt, I believe the issue is the gradient at zero. Adding K.epsilon() is the best solution I think."", 'That\'s an issue I would say. I think the issue here though is that the variance might become a very small negative number in float32 representation where it should be zero. `sqrt(-n)` pops out a NaN. Adding an epsilon pushes it back into the positive domain (given that the epsilon is large enough 1e-10 or so).\r\n\r\nGood catch, anyways. This is planned to go into the contrib repo once the PR adding a backend-agnostic ""extract_patches"" method is merged.\r\n\r\nEdit: actually, if the nans happen when/after training, it probably is the gradient. I had assumed the nans were occurring in general, before any training. Regardless, adding epsilon is the correct solution.', 'SSIM has been merged to keras-contrib. You may close your issue', 'Hello, I want achieve complex wavelet SSIM(CW-SSIM) using keras, anyone has idea to help me?', 'can any one has the MS-SSIM implementation in keras?', ""@patyork, sigma_xy is the covariance of x and y, and it is not equal to sigma_x multiplied by sigma_y. \r\nThe equation for covariance is Cov(x, y)=E(xy)-E(x)E(y). So the implementation should be:\r\n\r\n`\r\nfrom theano import tensor as T # for NNET module\r\nimport keras.backend as K\r\ndef loss_DSSIM_theano(y_true, y_pred):\r\n\r\n    # expected net output is of shape [batch_size, row, col, image_channels]\r\n    # e.g. [10, 480, 640, 3] for a batch of 10 640x480 RGB images\r\n    # We need to shuffle this to [Batch_size, image_channels, row, col]\r\n    y_true = y_true.dimshuffle([0, 3, 1, 2])\r\n    y_pred = y_pred.dimshuffle([0, 3, 1, 2])\r\n    \r\n    \r\n    # There are additional parameters for this function\r\n    # Note: some of the 'modes' for edge behavior do not yet have a gradient definition in the Theano tree\r\n    #   and cannot be used for learning\r\n    patches_true = T.nnet.neighbours.images2neibs(y_true, [4, 4])\r\n    patches_pred = T.nnet.neighbours.images2neibs(y_pred, [4, 4])\r\n\r\n    u_true = K.mean(patches_true, axis=-1)\r\n    u_pred = K.mean(patches_pred, axis=-1)\r\n    var_true = K.var(patches_true, axis=-1)\r\n    var_pred = K.var(patches_pred, axis=-1)\r\n    covar_true_pred = K.mean(y_true*y_pred, axis=-1) - u_true*u_pred\r\n\r\n    c1 = 0.01 ** 2\r\n    c2 = 0.03 ** 2\r\n    ssim = (2 * u_true * u_pred + c1) * (2 * covar_true_pred + c2)\r\n    denom = (u_true ** 2 + u_pred ** 2 + c1) * (var_pred + var_true + c2)\r\n    \r\n    ssim /= K.clip(denom, K.epsilon(), np.inf)\r\n    #ssim = tf.select(tf.is_nan(ssim), K.zeros_like(ssim), ssim)\r\n    \r\n    return K.mean((1.0 - ssim) / 2.0)\r\n`"", '@temperatemarine good catch, but I believe the covariance should be on patches, like the means and variances:\r\n```\r\ncovar_true_pred = K.mean(patches_true*patches_pred, axis=-1) - u_true*u_pred\r\n```', 'Yes, covariance should be consistent with means and variances.', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', ""@bernardohenz, I am just wondering for using SSIM as a loss layer, we should not define it's gradient manually? "", '@fqassemi as long as you use basic operators (+ - * / )  and/or backend operators, the backend (Theano of TF) computes the gradient by itself.\r\nAlso, as indicated by @Dref360 , SSIM is implemented on keras-contrib (https://github.com/farizrahman4u/keras-contrib/blob/master/keras_contrib/losses/dssim.py)', 'I am trying to use dssim, however, what is the right shape of y_pred or y_true? For the start, I just want to try this dssim between two input images.', '@fqassemi There is a sample code comparing DSSIM.\r\n\r\n```python\r\nimport numpy as np\r\nfrom keras_contrib.losses import DSSIMObjective\r\nfrom keras import backend as K\r\n\r\n#Shape should be (batch,x,y,channels)\r\nimga = np.random.normal(size=(1,256,256,3))\r\nimgb = np.random.normal(size=(1,256,256,3))\r\n\r\nloss_func = DSSIMObjective()\r\n\r\nresulting_loss1 = K.eval(loss_func(K.variable(imga),K.variable(imgb)))\r\nresulting_loss2 = K.eval(loss_func(K.variable(imga),K.variable(imga)))\r\n\r\nprint (""Loss for different images: %.2f"" % resulting_loss1)\r\nprint (""Loss for same image: %.2f"" % resulting_loss2)\r\n```\r\nIt is quite tricky because the function is implemented by the backend, so you must evaluate it.\r\n', '@bernardohenz Thanks. I forgot about eval part! BTW, I think it should be also mentioned shape should be (b, w, h, cc). Additionally, @ line 39, y_pred should be replaced with y_true in ""self.__int_shape(y_pred)[1:])"". (it probably does not change anything, however, it is more consistent)', 'I am going to combine dssim and some other function in loss function. To see the values and effect of each components, I define each one as metric as well. But it leads to compute each one twice, once in loss function and another in metric function. Is there a way to compute each component once and use the values of them as metric and in loss function?\r\nThanks', ""@sjsy I've never written a metric function, but I think they are handle like two different things. In other words, you will compute the loss function in order to backpropagate the gradients, and you'll compute the metric function in order to show to the user the desired metric.\r\n\r\nI don't think you can compute them only once (unless you change a bit of the workflow of keras).\r\n\r\nObs: If your 'other function' is quite heavy, I suggest you to remove it from metrics, you do not need to compute it on each interation, you can compute it on the test phase."", '@bernardohenz Thanks. Computing metrics in test phase is a good idea sometimes.\r\nAnother solution is using multi-output model, as suggested in this [link](https://stackoverflow.com/questions/50488882/is-there-a-way-to-reuse-an-evaluated-portion-of-the-computation-graph-in-keras).', 'Hi, everyone, I\'m a beginner at Deep learning and I have to use the ""SSIM Loss Function"", Could you please help me? I use Keras ', 'Is it possible to use this implementation with tensorflow code?\r\nThanks.', 'I am also trying to customise loss function, for that I need SSIM and MS-SSIM as objective loss function.\r\nKindly send me the code', '@DIPTIMISHRA I suggest you to take a look in the DSSIM implementation ([DSSIM])(https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/losses/dssim.py). This implementation is up to date and works on current versions of keras.', '@bernardohenz but this implementation uses `K.reshape(y_true, [-1] + list(self.__int_shape(y_pred)[1:]))` which prevent any use with a ""None"" size (for example, in order to use your convnet on any image size). So, can\'t use it sadly. If someone could fix it, that would be really nice :smile: (I\'m too lazy to do it myself).', ""@aviallon actually, I don't know why this line is needed. You could try to comment them and reinstall the package."", ""Yeah. I actually ended up taking the answers from this issue and commented\r\nout the first four lines, since I use Tensorflow with ROCm (and it sure\r\nrocks 👌)\r\nThanks. I think I'll try to modify it, and then make a nice patch for\r\nothers.\r\n "", ""Why not use tensorflow's ssim function?"", '@isaacgerg because I want my code to be portable to other backends as well, so I do not use any Tensorflow function directly.']",[],[],0,0
540,keras,1686,closed,How to get output given intermedia layer input ?,"From Document(FAQ), I can get output given specific input(layer 0).
Then I want to know whether to get outpu given intermedia layer input. For example, we have 7 layers totally, if I give the 3th layer's input, can we get the output of the 7th layers use keras?
Thanks.
",,"[""``` python\n#Create model with 7 layers\n\nmodel = Sequential()\nmodel.add(Dense(input_dim=10, output_dim=10))\n#add 6 more layers\nfor i in range(6):\n    model.add(Dense(10))\n\nmodel.compile(loss='mse', optimizer='sgd')\nmodel.fit(x, y)\n\n#Let X be your input for 3rd layer\n\ntmp = model.layers[:]\nmodel.layers = model.layers[2:]\nY = K.eval(model(K.variable(X)))\nmodel.layers = tmp[:]\n\nprint(Y)\n\n```\n"", '@farizrahman4u Thanks very much.\n']",[],[],0,0
541,keras,4097,closed,Use inheritance rather than class name equality,"Type checking is looking at class names rather than inheritance (for example  - it is also somewhat old Python style).

This is preventing the use of inheritance-based customization patterns. For example:

model

With the pull request https://github.com/fchollet/keras/pull/4069 , the code above would be working.
",stale,[],[],"['obj.__class__.__name__ = ""Model""', '', "" python\n\nimport keras.models\n\n# Customize the class Sequential with, say, a custom representation and a custom add method\nclass MySequentialModel(keras.models.Sequential):\n    def __repr__(self):\n        print('Fancy representation')\n    def add(self, layer):\n        super(MySequentialModel, self).add(layer)\n        # do extra things like sanity checks\n        pass\n\nmodel = MySequentialModel()\n\n# now use it\n\nimport keras.engine.training\n\n# the following will not work since "", ' should have a class name equal to Sequential\n# for the right thing to happen\nkeras.engine.trainig.collect_trainable_weights(model)\n\n', '']",0,0
542,keras,2790,closed,An error when saving model,"When I am using the functional API to create the model and try to save it using the following line, It gives me the following error saying:

  File ""trainer.py"", line 48, in create_training_features
  json_string = model.to_json()
  File ""/usr/local/lib/python2.7/site-packages/keras/engine/topology.py"", line 2368, in to_json
  config = self.get_config()
  File ""/usr/local/lib/python2.7/site-packages/keras/engine/topology.py"", line 2163, in get_config
  new_node_index = node_conversion_map[node_key]
  KeyError: 'input_1_ib-0'

Here is the gist: https://gist.github.com/akmahaja/ef406b2087b5c50befc1a479989b1921
",,"['The error message should be more helpful in that case, but the issue is that your model is not well-formed. You are setting as input `[input1, input2]`, but your output only relies on `input2`. Which is why the model (which is being built top-bottom) is not registering `input1` and complains about it later.\n', 'This works fine, if I do not save/load the model and directly use the trained model to predict. Any suggested workaround for this?\n', ""> Any suggested workaround for this?\n\nProperly defining the underlying model. In your case half of your model is simply never called. This doesn't prevent the other half from training, but since the model topology is broken, you will have errors when saving / loading.\n"", ""Coming back to this old thread as I came across this in a scenario I am currently working on - \r\nSay I am adding an input to be used only by the loss function (for allowing weighting the loss pixel-wise, for which as much as I understand the 1-d input weights can't do) - currently this model will break during the save of the model.\r\nIs there a nice way to do this?\r\nCurrently I stand one of two 'clean' choices - change the default save_model callback to save only the internal function (not so convenient for continuing training), or add a parallel small network to compute the loss (and then discard it at prediction time).\r\nEither way, unless I miss something about how to work right, it seems lacking a bit from the perspective of saving and then loading 'trainable' model."", '@yHalumi did u find a way to do it i exactly has the same issue passing a variable to loss function and saving the model', ""@harikrishnavydana  - well, I didn't find a clean way to do it, but what I eventually did was to encode the information I did to the y_true tensor, and then have the loss function being a special one which know to extract the real labels from the rest of the information, and then perform in the loss function all of the rest of it.\r\nOther than that, it made go native TF (:""]",[],[],0,0
543,keras,12741,closed,fit_generator not working in Keras 2.2.4 with python 3.6.8,"**fit_generator**. 

It does work in:  
Python 3.6.5
Keras 2.1.6

But doesn't in:
Python 3.6.8
Keras 2.2.4

Unless related to version of other modules. ",type:support,"['In case this is just a product of my ignorance and not a malfunctioning, this is the output and error:\r\n\r\n```\r\n2019-04-27 16:24:12.660651: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2019-04-27 16:24:12.661810: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 12. Tune using inter_op_parallelism_threads for best performance.\r\n\r\n```\r\nEpoch 1/100\r\n```\r\nOMP: Error #15: Initializing libiomp5.dylib, but found libiomp5.dylib already initialized.\r\nOMP: Hint: This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/.\r\nAbort trap: 6\r\n```', 'Please fill the issue [template](https://github.com/keras-team/keras/issues/new?template=a--tensorflow-backend-users.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. It would be great if you can provide a small code to reproduce the error. Thanks!', 'Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!']",[],[],0,0
544,keras,337,closed,ModelCheckpoint - a bug in line 177,"You forgot to initialize self.monitor with monitor.
The line appears like this:
self.monitor

and the error
AttributeError: 'ModelCheckpoint' object has no attribute 'monitor'

Would appreciate if you could fix that, thanks
",,['Fixed.\n'],[],[],0,0
545,keras,4178,closed,Problem with TimeDistributed() and Learning Phase,"(**EDIT:** The following issue is only a minimal example of how to produce the error. My actual goal is to use a more complicated model instead of  here.)

When executing the following script a  occurs:



This is the simplest model that produces the error (In my original architecture, I tried to distribute a more complex model). The same issue occurs when replacing the  layer with e.g. , , but not for e.g. . I think the error boils down to the combination of  and any layer (or model) that uses the learning phase.

Maybe there is a conceptual problem with  and the learning phase input?

These issues seem to be somewhat related: #3834, #2609, #3686, #2391

The full stack trace is this:



---

Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [ x] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [x ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [x ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",stale,"['I think you have incorrectly applied TimeDistributed to Dropout. \n\n`TimeDistributed(Dropout(0.5))(in1)` should be `TimeDistributed(Dropout(0.5)(in1))(in1)`\n', 'Thanks for the reply.\nI am quiet sure that the `TimeDistributed()` layer expects a `Layer` object and not a tensor (which `Dropout(0.5)(in1)` would return).\nAlso, when changing \n`out1 = TimeDistributed(Dropout(0.5))(in1)` to\n`out1 = TimeDistributed(Dense(10))(in1)`\neverything works fine. \n', ""This is a bug in theano when `RandomStreams` are present inside a `scan` op. See: https://groups.google.com/forum/#!topic/theano-users/8diyZjq6ngc\n\nSolutions : \n- Don't provide `batch_size` or,\n- Don't use `TimeDistributed` over `Dropout`. `TimeDistributed(Dropout(0.5))(x)` and `Dropout(0.5)(x)` are equivalent.\n\nIf you are trying to drop the same set of nodes for all timesteps in a sequence, simply wrapping in `TimeDistributed` will not do the job. See my solution at #3995 \n"", 'Thanks for the pointer, @farizrahman4u .\n\nThe solutions that you suggested sadly do not apply to my real use case (which I simplified for this Issue). My actual goal is to have an inner model:\n\n``` python\ninner_in1 = Input(batch_shape=(batch_size, n_elements, element_size), name=""inner_in1"")\ninner_output = GRU(2, dropout_U=0.5, dropout_W=0.5, return_sequences=False, name=""gru"")(inner_in1)\ninner_model = Model(input=inner_in1, output=inner_output, name=""inner_model"")\n```\n\nthat I use in a `TimeDistributed()` layer inside an outer model:\n\n``` python\nouter_in1 = Input(batch_shape=(batch_size, n_sequences, n_elements, element_size), name=""outer_in1"")\nouter_output = TimeDistributedModel(inner_model, name=""distr"")(outer_in1)\nouter_output = SomeOtherComputations()(outer_output)\nouter_model = Model(input=outer_in1, output=outer_output, name=""outer_model"")\nouter_model.compile(""adam"", ""mse"")\nouter_model._make_predict_function()\n```\n\nYou could see this as a sentence model (`inner_model`) that I apply to each sentence in a document (`outer_model`). In this setup, the error appears when using `dropout_W` or `dropout_U` in the GRU.\n\nNot specifying `batch_size` is not possible here, since [these lines in the TimeDistributed() layer](https://github.com/fchollet/keras/blob/master/keras/layers/wrappers.py#L133-L145) wouldn\'t make much sense with an RNN.\n', '@farizrahman4u as i reported in https://github.com/fchollet/keras/issues/4182\n1. batch_size is required when using stateful rnn\n2. what i want to get in timedistributed(dropout) is _not_ the same dropout nodes for every timestep, but having every timestep drop exactly x% of nodes. without timedistributed you would get different fractions for different timesteps\n', '@sjebbara There is no reason for you to provide the `batch_size` unless you are having a stateful RNN. Both rnn based and reshape based TimeDistributed implementations are strictly mathematically equivalent. (reshape based implementation being faster). If you still want to specify `batch_size`, here you go:\n\n``` python\nouter_in1 = Input(batch_shape=(batch_size, n_sequences, n_elements, element_size), name=""outer_in1"")\nTimeDistributedModel = TimeDistributed(inner_model, name=""distr"")\nTimeDistributedModel.build((None,) + outer_in1._keras_shape[1:])\nTimeDistributedModel.build = lambda *_: None\nouter_output = TimeDistributedModel(outer_in1)\nouter_output = SomeOtherComputations()(outer_output)\nouter_model = Model(input=outer_in1, output=outer_output, name=""outer_model"")\nouter_model.compile(""adam"", ""mse"")\nouter_model._make_predict_function()\n```\n', 'Similarly @eyaler, \nTo drop the exact number of nodes at every time step (when batch_size has to be provided becauses of stateful RNN):\n\n``` python\nmodel = Sequential()\nmodel.add(LSTM(10, batch_input_shape=(100, 20, 10), stateful=True, return_sequences=True))\n\ndropout = TimeDistributed(Dropout(0.5))\ndropout.build((None,) + model.output_shape[1:])\ndropout.build = lambda *_: None\n\nmodel.add(dropout)\n\nmodel.add(...)\nmodel.add(...)\n```\n', ""thanks @farizrahman4u !\n1. if reshape is faster why isn't it used also when batch_size is given?\n2. how would your solution look using the functional api? my attempt failed on assert_input_compatibility(x)\n"", '1. If batch size is given, then it is possible that the layer being wrapped is a stateful RNN (or any layer which requires a static batch size). Since the reshape method messes with the batch dimension, we go for the rnn method instead\n2. Maybe you forgot return_sequences=True\n', ""got it!\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import LSTM, Dropout, TimeDistributed, Input\nimport numpy as np\n\nx=np.zeros((100,20,10))\ny=np.zeros((100,20,10))\n\nmodel = Sequential()\nmodel.add(LSTM(10, batch_input_shape=(100, 20, 10), stateful=True, return_sequences=True))\ndropout = TimeDistributed(Dropout(0.5))\ndropout.build((None,) + model.output_shape[1:])\ndropout.build = lambda *_: None\nmodel.add(dropout)\nmodel.compile(optimizer='sgd', loss='mse')\nmodel.fit(x,y,nb_epoch=1,batch_size=100)\n\ninput = Input(batch_shape=(100, 20, 10))\na = LSTM(10, stateful=True, return_sequences=True)(input)\ndropout = TimeDistributed(Dropout(0.5))\ndropout.build((None,) + a._keras_shape[1:])\ndropout.build = lambda *_: None\noutput = dropout(a)\nfmodel = Model(input, output)\nfmodel.compile(optimizer='sgd', loss='mse')\nfmodel.fit(x,y,nb_epoch=1,batch_size=100)\n"", 'I think I misunderstood the reshape-based implementation. I was just about to point out why reshaping makes no sense with a distributed RNN layer, but then the pieces fell together 😆.\n\nSo the solution is simply to leave `batch_size` undefined?!\nI will try that tomorrow.\nThanks all!\n', 'Hello!\r\n\r\nIs there any update on this?\r\nBy the way for me it works with a tensorflow backend, but not with the theano one... ', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', ""I was having a similar issue with Tensorflow.  Whenever I used the TimeDistributed wrapper on a model containing layers that used the learning phase, the resulting tensor would have the property _uses_learning_phase = False.  This meant that when I created a final model containing that tensor, the model's _uses_learning_phase would incorrectly be set to False.\r\n\r\nIn the case below, my intermediate_model had a Dropout layer; before passing it through the wrapper, intermediate_model.uses_learning_phase=True.\r\n\r\n    input_scan = Input(shape=(ANGLES,FINAL_WIDTH,FINAL_HEIGHT//2,CHANNELS))\r\n    #Time distributed model\r\n    sequenced_model = TimeDistributed(intermediate_model)(input_scan)\r\n  \r\n    sequenced_model._uses_learning_phase = True #Manually setting the tensor's property fixed the issue.\r\n    \r\n    out = GlobalAveragePooling1D()(sequenced_model)\r\n    #Complete model\r\n    model = Model(input_scan,out)"", '@eyaler\r\nI can\'t get your functional example to work.\r\n\r\nI tried with a Dense Layer instead of an LSTM. I get an error that says.\r\nTensors don\'t have keras_shape.\r\n\r\n```python\r\n\r\ndropout.build((None,) + a.keras_shape[1:])\r\n```\r\n\r\nThe other thing I tried was to have a Dense Layer as input to a dropout layer\r\nwrapped by a timeDistributed layer.\r\n\r\n```python\r\ninput_1 = Input(batch_shape=(batch_size, seq_len, num_inputs))\r\n\r\nx1 = Dense(32, activation=\'tanh\')(input_1)\r\nx1 = TimeDistributed(Dropout(0.5))(x1)\r\n```\r\nwhich ends with:\r\n\r\n```python\r\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor \'time_distributed_1/keras_learning_phase\' with dtype bool\r\n\t [[Node: time_distributed_1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=<unknown>, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]\r\n```\r\n\r\nEither way will cause an exception.\r\n\r\nWhat I want to do is sequence to sequence learning and I\'d like to do it with the functional API.\r\n\r\nThat would be a timeDistributed dense layer on top of a LSTM if I understood correctly and\r\nthat works.\r\n\r\nHaving dropout would be the icing on the cake though.\r\n\r\nLike @farizrahman4u said I\'d like to drop the exact same number of nodes at every time step with\r\na stateful RNN.\r\n\r\nCan anybody provide a pointer on how to do this with the functional API. I can\'t figure out this\r\nbuild magic.\r\n\r\n______________________________________________________________________________________\r\n**EDIT!:**\r\n\r\nI tried using \r\n```python\r\ntuple(a.get_shape().as_list())[1:]\r\n```\r\n\r\nto make the snippet work.\r\n\r\n```python\r\nfrom keras.models import Sequential, Model\r\nfrom keras.layers import LSTM, Dropout, TimeDistributed, Input\r\nimport numpy as np\r\n\r\nx=np.zeros((100,20,10))\r\ny=np.zeros((100,20,10))\r\n\r\ninput = Input(batch_shape=(100, 20, 10))\r\na = LSTM(10, stateful=True, return_sequences=True)(input)\r\ndropout = TimeDistributed(Dropout(0.5))\r\ndropout.build((None,) + tuple(a.get_shape().as_list())[1:])\r\ndropout.build = lambda *_: None\r\noutput = dropout(a)\r\nfmodel = Model(input, output)\r\nfmodel.compile(optimizer=\'sgd\', loss=\'mse\')\r\nfmodel.fit(x,y,nb_epoch=1,batch_size=100)\r\n```\r\n\r\nAgain it terminates with an exception. This time in the training phase:\r\n\r\n```python\r\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor \'time_distributed_1/keras_learning_phase\' with dtype bool\r\n\t [[Node: time_distributed_1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=<unknown>, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]\r\n```\r\n\r\n**EDIT!:**\r\n\r\nThanks @brayan07 \r\n\r\nyour workaround fixed the issue and it compiles. I don\'t know if the dropout is applied correctly\r\nthough.\r\n', ""> sequenced_model._uses_learning_phase = True #Manually setting the tensor's property fixed the issue.\r\n> \r\n> ```\r\n\r\nThis was the key to solve this for me, too. \r\nThe model contained in the timedistributed was indeed not training without this.""]","[' python\nfrom keras.models import Model\nfrom keras.layers import Input, TimeDistributed, Dropout\n\nin1 = Input(batch_shape=(10, 8, 6), name=""in1"")\nout1 = TimeDistributed(Dropout(0.5))(in1)\n\nmodel = Model(input=in1, output=out1)\nmodel.compile(""adam"", ""mse"")\nmodel._make_predict_function()\n', '\n... \n  File ""/homes/sjebbara/git/keras-original/keras/engine/training.py"", line 752, in _make_predict_function\n    **kwargs)\n  File ""/homes/sjebbara/git/keras-original/keras/backend/theano_backend.py"", line 787, in function\n    return Function(inputs, outputs, updates=updates, **kwargs)\n  File ""/homes/sjebbara/git/keras-original/keras/backend/theano_backend.py"", line 773, in __init__\n    **kwargs)\n  File ""/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/compile/function.py"", line 326, in function\n    output_keys=output_keys)\n  File ""/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/compile/pfunc.py"", line 486, in pfunc\n    output_keys=output_keys)\n  File ""/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/compile/function_module.py"", line 1776, in orig_function\n    output_keys=output_keys).create(\n  File ""/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/compile/function_module.py"", line 1430, in __init__\n    accept_inplace)\n  File ""/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/compile/function_module.py"", line 176, in std_fgraph\n    update_mapping=update_mapping)\n  File ""/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/gof/fg.py"", line 180, in __init__\n    self.__import_r__(output, reason=""init"")\n  File ""/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/gof/fg.py"", line 351, in __import_r__\n    self.__import__(variable.owner, reason=reason)\n  File ""/homes/sjebbara/.local/lib/python2.7/site-packages/Theano-0.9.0.dev3-py2.7.egg/theano/gof/fg.py"", line 396, in __import__\n    variable=r)\ntheano.gof.fg.MissingInputError: An input of the graph, used to compute Shape(<TensorType(float32, matrix)>), was not provided and not given a value.Use the Theano flag exception_verbosity=\'high\',for more information on this error.\n\nBacktrace when the variable is created:\n  File ""/homes/sjebbara/PyCharmProjects/NeuralSentiment/src/Test2.py"", line 5, in <module>\n    out1 = TimeDistributed(Dropout(0.5))(in1)\n  File ""/homes/sjebbara/git/keras-original/keras/engine/topology.py"", line 514, in __call__\n    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)\n  File ""/homes/sjebbara/git/keras-original/keras/engine/topology.py"", line 572, in add_inbound_node\n    Node.create_node(self, inbound_layers, node_indices, tensor_indices)\n  File ""/homes/sjebbara/git/keras-original/keras/engine/topology.py"", line 149, in create_node\n    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))\n  File ""/homes/sjebbara/git/keras-original/keras/layers/wrappers.py"", line 131, in call\n    initial_states=[], input_length=input_length, unroll=unroll)\n  File ""/homes/sjebbara/git/keras-original/keras/backend/theano_backend.py"", line 947, in rnn\n    go_backwards=go_backwards)\n']","['Dropout()', 'MissingInputError', 'Dropout()', 'GaussianNoise()', 'GRU(dropout_W=0.5)', 'Dense()', 'TimeDistributed()', 'TimeDistributed()']",0,0
546,keras,312,closed,Stacking Merge layers?,"Is it possible to stack Merge layers? The model compiles but the train methods breaks with 



",,"['maybe try\n\n`model.train([[left_left_x, left_right_x], [right_left_x, right_right_x]], y)`\n', 'Can you post a code snippet to repro this, which includes data (use `np.random.random(shape)` to generate your data)? Your network appears to be correct, but there might be issues with the shape of your inputs / outputs.\n', 'Can someone please clarify the correct syntax for passing training data to stacked merge layers? Should I use a nested list as @simonhughes22  suggested?\n']","[""\nleft_left = Sequential()\nleft_left.add(TimeDistributedDense(10, 20))\n\nleft_right = Sequential()\nleft_right.add(TimeDistributedDense(10, 20))\n\nright_left = Sequential()\nright_left.add(TimeDistributedDense(10, 20))\n\nright_right = Sequential()\nright_right.add(TimeDistributedDense(10, 20))\n\nleft = Sequential()\nleft.add(Merge([left_left, left_right], mode = 'sum'))\n\nright = Sequential()\nright.add(Merge([right_left, right_right], mode = 'sum'))\n\nmodel = Sequential()\nmodel.add(Merge([left, right], mode='concat'))\nmodel.add(TImeDistributedDense(40, 100))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='sgd')\n\nmodel.train([left_left_x, left_right_x, right_left_x, right_right_x], y)\n""]",['dimension mismatch in args to gemm'],0,0
547,keras,10518,closed,"i am working on a regressor problem with 50,000 rows and 20 coloumns ,i want to implement CNN and find MSE on different architecture,  model = Sequential() model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3))) model.add(Conv2D(32, (3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25))  model.add(Conv2D(64, (3, 3), activation='relu')) model.add(Conv2D(64, (3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25))  model.add(Flatten()) model.add(Dense(256, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(10, activation='tanh'))  sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='mean_squared_error', optimizer=sgd)  model.fit(Bx_train, Fx_train, batch_size=32, epochs=10) score = model.evaluate(Bx_test, Fx_test, batch_size=32), what dimension sholud i used as input ","Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",,"['According to your architecture input shape should be (100,100,3). Also please read this : \r\nPlease make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on StackOverflow or join the Keras Slack channel and ask there instead of filing a GitHub issue.', ""Hi!\r\nThis issue isn't related to a bug/enhancement/feature request or other accepted types of issue.\r\n\r\nTo ask questions, please see the following resources : \r\n* https://keras.io/\r\n* https://keras.io/#support\r\n* https://gitter.im/Keras-io/Lobby\r\n\r\nThanks!\r\n\r\nIf you think I made a mistake, please re-open this issue.""]",[],[],0,0
548,keras,676,closed,Python 3 compatibility problem with Image loading,"Loading an Image using the  results in an error.


",,[],"['\nTraceback (most recent call last):\n  File ""keras/autoencoder.py"", line 45, in <module>\n    X_train, Y_train, X_test, Y_test, nb_classes = io.load_images(join(DATA_DIR, \'dataset0\'))\n  File ""/home/jnphilipp/Documents/cnn/hieroglyphs/keras/utils/io.py"", line 27, in load_images\n    X_train.append(img_to_array(load_img(picture, True)))\n  File ""/home/jnphilipp/.local/lib/python3.4/site-packages/Keras-0.1.2-py3.4.egg/keras/preprocessing/image.py"", line 107, in load_img\n  File ""/home/jnphilipp/.local/lib/python3.4/site-packages/PIL/Image.py"", line 2330, in open\n    % (filename if filename else fp))\nOSError: cannot identify image file <_io.TextIOWrapper name=\'/home/jnphilipp/Documents/cnn/hieroglyphs/data/dataset0/train/P1_train0.png\' mode=\'r\' encoding=\'ISO-8859-1\'>\n']",['load_img'],0,0
549,keras,1584,closed,theano.scan() unused in keras source?,"Hi,

Shouldn't theano.scan() be used for things like the get_output() in Graph containers and such? Would theano be able to (GPU) optimize such functions if scan() was used instead of normal python dictionary iterations?
",,"['this was discussed before: https://github.com/fchollet/keras/issues/954\n\nthe unrolled scan actually runs faster, although it may take longer to compile\n', ""I still see it in keras/backend/theano_backend.py in the method rnn. I\ndon't see the code to unroll it in keras, did I miss it?\n\nIf you want a number of step that change at each call, in Theano, you must\nuse scan.\n\nOn Thu, Jan 28, 2016 at 9:04 PM, François Chollet notifications@github.com\nwrote:\n\n> Closed #1584 https://github.com/fchollet/keras/issues/1584.\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/fchollet/keras/issues/1584#event-530689864.\n"", ""Right. Only the TensorFlow version is unrolled, and that is why you can't use variable-length sequences with the TensorFlow backend. In the future a new solution for handling variable-length sequences will be released in TensorFlow, and we will bring it into Keras.\n""]",[],[],0,0
550,keras,2711,closed,Loss not changing when training,"I have a model that I am trying to train where the loss does not go down. I have a custom image set that I am using. These images are 106 x 106 px (black and white) and I have two (2) classes,  Bargraph or Gels. These two classes are very different. I have run the Cifar10 dataset and it did reduce the loss, but I am very confused as to why my model will always predict only one class for everything. 

Xtrain is a numpy array of images (which are numpy arrays), Ytrain is a numpy array of arrays ([0,1] or [1,0]) the shapes look like this: 



Here is my model:



Right now I am just doing very small training sets (I tried doing 1000 examples as well, with similar results).

I have also tried RMS and SDG with large and small learning rates.

What else can I try ?
- [X] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [X] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
",,"['Try increasing the learning rate to a higher value, possibly to 0.1. That way you can ensure that noticeable changes to weights are made for each successive update. \n', 'Tried it, it stopped after 2 epochs. Here are the results\n\n```\nsgd = SGD(lr=0.1, decay=1e-6, momentum=1.9)\nmodel.compile(loss=\'categorical_crossentropy\', optimizer=sgd,metrics=[""accuracy""])\nmodel.fit(Xtrain[950:1050], Ytrain[950:1050], batch_size=32, nb_epoch=20,\n          verbose=1, validation_split=0.2,\n          callbacks=[EarlyStopping(monitor=\'val_loss\', patience=0)])\n\nTrain on 80 samples, validate on 20 samples\nEpoch 1/20\n80/80 [==============================] - 84s - loss: 5.9848 - acc: 0.5250 - val_loss: 1.1921e-07 - val_acc: 1.0000\nEpoch 2/20\n80/80 [==============================] - 85s - loss: 10.0738 - acc: 0.3750 - val_loss: 1.1921e-07 - val_acc: 1.0000\nOut[60]: <keras.callbacks.History at 0x7f6b30891c90>\n\nmodel.predict(Xtrain[995:1005])\nOut[62]: \narray([[ 1.,  0.],\n       [ 1.,  0.],\n       [ 1.,  0.],\n       [ 1.,  0.],\n       [ 1.,  0.],\n       [ 1.,  0.],\n       [ 1.,  0.],\n       [ 1.,  0.],\n       [ 1.,  0.],\n       [ 1.,  0.]])\n```\n\nThe first class is from 0 to 999 and the second class is from 1000 to 1999\n\nI tried to predict right at the border and got all [1,0]. Shuffling the training set should not matter should it?\n', 'After reading some blogs, looks like the batch size is important, because if our data is not shuffled it will learn one class for a few batches and then another class for a few batches. Similarly My loss seems to stay the same, here is an interesting read on the loss function. I really am still unsure as to what I may be doing wrong.\n\n### Here are a few things I tried:\n- number of layers (reduction)\n- size of the filters (reduction)\n- SGD learning rate from 0.000000001 to 0.1\n- SGD decay to 1e-2\n- Batch size\n- Different images\n- Shuffling the images around\n\nI am really unsure as to what I can do to get my loss to go down. Any other ideas?\n\n### Code:\n\n```\nmodel = Sequential()\nmodel.add(Convolution2D(32, 10, 10, border_mode=\'same\',name=\'conv1\', input_shape = (1, 106, 106)))\n#flatten\nmodel.add(Flatten())\nmodel.add(Dense(2))\nmodel.add(Activation(\'softmax\'))\nrms = RMSprop()\nsgd = SGD(lr=0.1, decay=1e-2, momentum=0.9)\nmodel.compile(loss=\'categorical_crossentropy\', optimizer=sgd,metrics=[""accuracy""])\nmodel.fit(Xtrain, Ytrain, batch_size=64, nb_epoch=10,\n          verbose=1)\n```\n\n### Here is the code for me to read in the images if it helps:\n\n```\ndef loadCustomData():\n    #read directories\n    #files = []\n    fileRoots = []\n    for root, directories, filenames in os.walk(\'modified/convNetPanelsPaneled_sorted/\'):\n        for filename in filenames: \n            #files.append(os.path.join(root,filename))\n            fileRoots.append(root)\n    fileRoots = list(sorted(set(fileRoots)))\n    bar = imgs.getFiles(fileRoots[19])\n    bar = [fileRoots[19] + ""/"" + s for s in list(bar)]\n    gel = imgs.getFiles(fileRoots[0])\n    gel = [fileRoots[0] + ""/"" + s for s in list(gel)]\n    Xtrain = []\n    Ytrain = []\n    Xtest = []\n    Ytest = []\n    #for i in range(int(len(bar)*.95)):\n    for i in range(1000):\n        imgArray = imgs.load_image(bar[i])\n        if(len(np.shape(imgArray)) == 3):\n            imgArray = imgArray[:,:,1]#set to 1 so we keep the greyscale\n        Xtrain.append(imgArray)\n        Ytrain.append([1])\n\n    #for i in range(len(bar) - int(len(bar)*.95)):\n    for i in range(1000,1100):\n        imgArray = imgs.load_image(bar[i])\n        if(len(np.shape(imgArray)) == 3):\n            imgArray = imgArray[:,:,1]\n        Xtest.append(imgArray)\n        Ytest.append([1])\n\n    for i in range(1000):\n        imgArray = imgs.load_image(gel[i])\n        if(len(np.shape(imgArray)) == 3):\n            imgArray = imgArray[:,:,1]\n        Xtrain.append(imgArray)\n        Ytrain.append([0])\n\n    #for i in range(len(gel) - int(len(gel)*.95)):\n    for i in range(1000,1100):\n        imgArray = imgs.load_image(gel[i])\n        if(len(np.shape(imgArray)) == 3):\n            imgArray = imgArray[:,:,1]\n        Xtest.append(imgArray)\n        Ytest.append([0])   \n\n    Ytrain = np_utils.to_categorical(Ytrain, 2)\n    Ytrain = np.array(Ytrain)\n    Ytest = np_utils.to_categorical(Ytest, 2)\n    Ytest = np.array(Ytest)\n    c = np.c_[np.array(Xtrain).reshape(len(Xtrain), -1), np.array(Ytrain).reshape(len(Ytrain), -1)]\n    np.random.shuffle(c)\n    Xtrain = c[:, :np.array(Xtrain).size//len(Xtrain)].reshape(np.array(Xtrain).shape)\n    Ytrain = c[:, np.array(Xtrain).size//len(Xtrain):].reshape(np.array(Ytrain).shape)\n    #reshape to fit the network\n    Xtrain = np.array(Xtrain).reshape(2000, 1, 106, 106)#np.array(Xtrain).reshape(4415, 1, 106, 106)\n    Xtrain = Xtrain.astype(\'float32\')\n    Xtest = np.array(Xtest).reshape(200, 1, 106, 106)#np.array(Xtest).reshape(233, 1, 106, 106)\n    Xtest = Xtest.astype(\'float32\')\n\n    Xtrain /= 255\n    Xtest /= 255\n```\n\n### Here is my output:\n\n```\nEpoch 1/10\n2000/2000 [==============================] - 73s - loss: 8.0590 - acc: 0.5000     \nEpoch 2/10\n2000/2000 [==============================] - 75s - loss: 8.0590 - acc: 0.5000     \n...SNIP...    \nEpoch 7/10\n2000/2000 [==============================] - 81s - loss: 8.0590 - acc: 0.5000     \n\n```\n\nLinks:\n[What is batch size in neural network?](http://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network)\n\n[http://cs231n.github.io/neural-networks-3/#loss](http://cs231n.github.io/neural-networks-3/#loss)\n', ""1. Miss activation (e.g. relu) after `Convolution2D`. I use your network on cifar10 data, loss does not decrease but increase. With activation, it can learn something basic.\n2. Network is too shallow. It's hard to learn with only a convolutional layer and a fully connected layer. Try Alexnet or VGG style to build your network or read examples (cifar10, mnist) in Keras. \n3. I recommend you to take some online courses about deep learning, it would be helpful.\n"", 'Hi @joelthchao,\n\nI am unsure as to what you mean in 1 for miss activation. Are you saying if you remove the activation the loss increases and when you use activation it learns?\n\nFor 2: I actually tried with a deeper network, but I figured since it was giving me no improvement, it may be best to simplify the model and troubleshoot with that. \n\nI am wondering if this could be an issue with my data. \n\nI can try to increasing the depth. Anything else I should look at?\n\nI will post my results from the cifar10.\n\nEDIT: I found this example:\n\n[https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py](https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py)\n\nI will try it and adapt it to my needs. Can anyone explain why we do double convolution like this?\n', ""@kevkid Try this, does loss still not decrease?\n\n``` python\nmodel.add(Convolution2D(32, 10, 10, border_mode='same',name='conv1', input_shape = (1, 106, 106)))\nmodel.add(Activation('relu'))\n# ...\n```\n"", '@joelthchao \n\nJust tried it:\n\n```\nTrain on 1600 samples, validate on 400 samples\nEpoch 1/20\n1600/1600 [==============================] - 60s - loss: 8.8503 - acc: 0.3969 - val_loss: 1.1921e-07 - val_acc: 1.0000\nEpoch 2/20\n1600/1600 [==============================] - 60s - loss: nan - acc: 0.3750 - val_loss: nan - val_acc: 1.0000\nOut[17]: <keras.callbacks.History at 0x7f0cc152f7d0>\n```\n\nloss goes to nan. Could the weights be blowing up? I will try the example from keras for cifar 10.\n', 'Could this be my architecture? Are there any resources for designing the neural network? Here is how the model currently looks:\n\n```\nmodel = Sequential()\n\nmodel.add(Convolution2D(32, 5, 5, border_mode=\'same\',name=\'conv1_1\', input_shape = (1, 106, 106)))\n#first_layer = model.layers[0]\n# this is a placeholder tensor that will contain our generated images\n#input_img = first_layer.input\n#dream = input_img\nmodel.add(Activation(""relu""))\nmodel.add(Convolution2D(32, 5, 5, border_mode=\'same\',name=\'conv1_2\'))\nmodel.add(Activation(""relu""))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n#2\nmodel.add(Convolution2D(64, 5, 5, border_mode=\'same\',name=\'conv2_1\'))\nmodel.add(Activation(""relu""))\nmodel.add(Convolution2D(64, 5, 5, border_mode=\'same\',name=\'conv2_2\'))\nmodel.add(Activation(""relu""))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n#flatten\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation(""relu""))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(2))\nmodel.add(Activation(\'softmax\'))\n\nrms = RMSprop()\nsgd = SGD(lr=0.01, decay=1e-6, momentum=1, nesterov=True)\nmodel.compile(loss=\'categorical_crossentropy\', optimizer=sgd,metrics=[""accuracy""])\nmodel.fit(Xtrain, Ytrain, batch_size=32, nb_epoch=20,\n          verbose=1, validation_split=0.2,\n          callbacks=[EarlyStopping(monitor=\'val_loss\', patience=2)])\n```\n\nBased on cifar10 example. I am currently running the model.\n', 'I was able to make a decent model that gave me excellent results. I am unsure why rmsprop seems to make the loss go up, but here is my model:\n\n```\nmodel = Sequential()\n\nmodel.add(Convolution2D(32, 5, 5, border_mode=\'same\',name=\'conv1_1\', input_shape = (1, 106, 106)))\n#first_layer = model.layers[0]\n# this is a placeholder tensor that will contain our generated images\n#input_img = first_layer.input\n#dream = input_img\nmodel.add(Activation(""relu""))\nmodel.add(Convolution2D(32, 5, 5, border_mode=\'same\',name=\'conv1_2\'))\nmodel.add(Activation(""relu""))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n#2\nmodel.add(Convolution2D(64, 5, 5, border_mode=\'same\',name=\'conv2_1\'))\nmodel.add(Activation(""relu""))\nmodel.add(Convolution2D(64, 5, 5, border_mode=\'same\',name=\'conv2_2\'))\nmodel.add(Activation(""relu""))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n#flatten\nmodel.add(Flatten())\n#model.add(Dense(512))\n#model.add(Activation(""relu""))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(2))\nmodel.add(Activation(\'softmax\'))\n\nrms = RMSprop()\nsgd = SGD(lr=0.001, decay=1e-6, momentum=0.5, nesterov=True)\nmodel.compile(loss=\'categorical_crossentropy\', optimizer=sgd,metrics=[""accuracy""])\nmodel.fit(Xtrain, Ytrain, batch_size=32, nb_epoch=100,\n          verbose=1)\n\n--SNIP--\nEpoch 99/100\n2000/2000 [==============================] - 11s - loss: 0.0570 - acc: 0.9795     \nEpoch 100/100\n2000/2000 [==============================] - 11s - loss: 0.0576 - acc: 0.9815     \nOut[14]: <keras.callbacks.History at 0x7f204ba9f0d0>\n--SNIP--\nprint(\'Classifcation rate %02.3f\' % model.evaluate(Xtest, Ytest)[1])\n200/200 [==============================] - 0s     \nClassifcation rate 0.930\n\n```\n', 'I have got the same problem as you , but I guess there must be something worng with my function of load_data, and I am not sure if I get the right Xtrain and Ytrain, so if you please share your code of load_data , thanks a lot.\n', '@111hypo I have posted my loadCustomData function a few posts above this one. The portion :\n\n```\n    c = np.c_[np.array(Xtrain).reshape(len(Xtrain), -1), np.array(Ytrain).reshape(len(Ytrain), -1)]\n    np.random.shuffle(c)\n    Xtrain = c[:, :np.array(Xtrain).size//len(Xtrain)].reshape(np.array(Xtrain).shape)\n    Ytrain = c[:, np.array(Xtrain).size//len(Xtrain):].reshape(np.array(Ytrain).shape)\n```\n\nis unnecessary because we do not need to shuffle the input (This was just a test to try and figure out why My network would not converge).\n\nI still have problems with RMSprop. It quickly gains loss, and the accuracy goes to 0 (which to me is funky). I tried a few different SGDs and the one in my latest post seemed to work the best for me.\n', ""@kevkid I also meet your problem. I collect 1505 numbers pics as my dataset and use a simple model.\nThe valid_acc doesn't change. Do you give me some advices. Thanks.\n"", '@111hypo ,how do you solve your problem. \n', ""@kevkid \nHave you found the solution now?I have met the same problem to yours.\n\nHave you tried to change the ' momentum=1.9 '.I found that this problem may connected to the argument named 'momentum' in SGD optimizter.\n\nI did't find the solution yet but when I changed the  momentum to 0.5, the loss  changed.But after several epoch , the loss did not change again......\n\nhope this can help you !\n"", 'You may want to reduce your drop out rate and shuffle your data\n', ""Hello, I used vgg19 architecture to classify my data set into 2 classes but the problem is the value of accuracy doesn't change after 30 iteration and I don't what is the problem and this my code:\r\ndata,Label = shuffle(immatrix,label, random_state=2)\r\ntrain_data = [data,Label]\r\nprint (train_data[0].shape) # the train data\r\nprint (train_data[1].shape)  # the labels of these data\r\n#batch_size to train\r\nbatch_size = 16\r\n# number of output classes\r\nnb_classes = 3\r\n# number of epochs to train\r\nnb_epoch = 150 #each epochs contains around 70000/128=468 batches with 128 images\r\nimg_rows, img_cols = 200, 200\r\n(X, y) = (train_data[0],train_data[1])\r\n# STEP 1: split X and y into training and testing sets\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\r\nX_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\r\nX_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\r\nX_train = X_train.astype('float32')\r\nX_test = X_test.astype('float32')\r\n#Normalization\r\nX_train /= 255\r\nX_test /= 255\r\n# compute the total time of prearing and importing the dataset\r\nt_generateArray = time.time()\r\nprint('Generating Array Time:{}'.format(t_generateArray - t_start))\r\n\r\nprint('X_train shape:', X_train.shape)\r\nprint(X_train.shape[0], 'train samples')\r\nprint(X_test.shape[0], 'test samples')\r\n\r\n# convert class vectors to binary class matrices\r\nY_train = np_utils.to_categorical(y_train, nb_classes)\r\nY_test = np_utils.to_categorical(y_test, nb_classes)\r\n\r\n# Step 1:Network structure\r\nmodel = Sequential()\r\nmodel.add(ZeroPadding2D((1, 1), input_shape=(1, img_rows, img_cols)))\r\nmodel.add(Convolution2D(64, 3, 3, activation='relu',init='glorot_uniform'))\r\nmodel.add(ZeroPadding2D((1, 1)))\r\nmodel.add(Convolution2D(64, 3, 3, activation='relu',init='glorot_uniform'))\r\nmodel.add(MaxPooling2D((2, 2), strides=(2, 2)))\r\n\r\nmodel.add(ZeroPadding2D((1, 1)))\r\nmodel.add(Convolution2D(128, 3, 3, activation='relu',init='glorot_uniform'))\r\nmodel.add(ZeroPadding2D((1, 1)))\r\nmodel.add(Convolution2D(128, 3, 3, activation='relu',init='glorot_uniform'))\r\nmodel.add(MaxPooling2D((2, 2), strides=(2, 2)))\r\n\r\nmodel.add(ZeroPadding2D((1, 1)))\r\nmodel.add(Convolution2D(256, 3, 3, activation='relu',init='glorot_uniform'))\r\nmodel.add(ZeroPadding2D((1, 1)))\r\nmodel.add(Convolution2D(256, 3, 3, activation='relu',init='glorot_uniform'))\r\nmodel.add(ZeroPadding2D((1, 1)))\r\nmodel.add(Convolution2D(256, 3, 3, activation='relu',init='glorot_uniform'))\r\nmodel.add(ZeroPadding2D((1, 1)))\r\nmodel.add(Convolution2D(256, 3, 3, activation='relu',init='glorot_uniform'))\r\nmodel.add(MaxPooling2D((2, 2), strides=(2, 2)))\r\n\r\nmodel.add(ZeroPadding2D((1, 1)))\r\nmodel.add(Convolution2D(512, 3, 3, activation='relu',init='glorot_uniform'))\r\nmodel.add(ZeroPadding2D((1, 1)))\r\nmodel.add(Convolution2D(512, 3, 3, activation='relu',init='glorot_uniform'))\r\nmodel.add(ZeroPadding2D((1, 1)))\r\nmodel.add(Convolution2D(512, 3, 3, activation='relu',init='glorot_uniform'))\r\nmodel.add(ZeroPadding2D((1, 1)))\r\nmodel.add(Convolution2D(512, 3, 3, activation='relu',init='glorot_uniform'))\r\nmodel.add(MaxPooling2D((2, 2), strides=(2, 2)))\r\n\r\nmodel.add(ZeroPadding2D((1, 1)))\r\nmodel.add(Convolution2D(512, 3, 3, activation='relu',init='glorot_uniform'))\r\nmodel.add(ZeroPadding2D((1, 1)))\r\nmodel.add(Convolution2D(512, 3, 3, activation='relu',init='glorot_uniform'))\r\nmodel.add(ZeroPadding2D((1, 1)))\r\nmodel.add(Convolution2D(512, 3, 3, activation='relu',init='glorot_uniform'))\r\nmodel.add(ZeroPadding2D((1, 1)))\r\nmodel.add(Convolution2D(512, 3, 3, activation='relu',init='glorot_uniform'))\r\nmodel.add(MaxPooling2D((2, 2), strides=(2, 2)))\r\n\r\nmodel.add(Flatten())\r\nmodel.add(Dense(1024, activation='relu',init='glorot_uniform'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(1024, activation='relu',init='glorot_uniform'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(nb_classes, activation='softmax'))  # neuron number\r\n\r\n#step 2: Learning target(computiong the loss using the entropy function\r\nsgd = SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\r\n# adagrad = Adagrad(lr=0.01, epsilon=1e-08)\r\nmodel.compile(loss='categorical_crossentropy',optimizer= sgd,metrics=['accuracy'])\r\n\r\n#checkpointer=ModelCheckpoint(filepath='exp_161123_best_lr0.0001_weights.h5',monitor='acc', verbose=1, save_best_only=True, mode='max')\r\n\r\n#training the model\r\nprint ('Training Start....')\r\nhist = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\r\n               verbose=1, validation_data=(X_test, Y_test))\r\n#,callbacks=[checkpointer]\r\n\r\n# model.save_weights('final_last5_weights.h5')\r\n#model.save_weights('exp_161123_final_lr0.0001_weights.h5')\r\n\r\n#evaluate the model\r\nscore = model.evaluate(X_test, Y_test,verbose=0)\r\nprint('Test score:', score[0])\r\nprint('Test accuracy:', score[1]) "", ""If you have unbalanced classes, maybe you should consider weighting classes, check class_weight and sample_weight in Keras docs\r\n\r\nHere's a similar question asked on stackoverflow\r\nhttp://stackoverflow.com/questions/41881220/keras-predict-always-output-same-value-in-multi-classification"", '@redouanelg Do you mean by adding `sample_weight` in `fit()`?\r\nOr do you give an example how to do ? thanks.', '@alyato Sorry for the late reply.\r\n\r\nMy modest experience tells me that if you have only two classes use a dict in class_weight.\r\n\r\nIf you have more you\'ll get the error class_weight not supported for +3 dim.\r\n\r\nA way to overcome this consists in adding sample_weight in fit() using a 2D weight array  (one weight per timestep per sample), and adding sample_weight_mode=""temporal"" in compile()\r\n\r\nIt\'s not an elegant solution but it works. I\'ll be glad if someone has another answer.', 'Hey, i am having a similar problem i am trying to train a network to learn word embeddings using skip grams. i have a vocabulary of 256 and a sequence of about 166000 words. But when i train, the accuracy stays the same at around 0.1327 no matter what i do, i tried changing learning rates and batch_size. But no luck. This has happened every time i used keras. But it usually starts learning after tweaking the batch size a bit. But this one just doesn\'t work. \r\n\r\nHere is the model:\r\n`\r\n\r\n      def make_model(self,vocab_size=256,vec_dim=100):\r\n\t\tmodel=Sequential()\r\n\t\tmodel.add(Dense(vec_dim,activation=""sigmoid"",input_dim=vocab_size))\r\n\t\tmodel.add(Dense(vocab_size,activation=""sigmoid""))\r\n\t\tsgd=SGD(lr=1.0)\r\n\t\tmodel.compile(loss=""categorical_crossentropy"",optimizer=sgd,metrics=[""accuracy""])\r\n\t\treturn model\r\n\r\n\tdef _get_callbacks(self):\r\n\t\tearlystop=EarlyStopping(monitor=""val_loss"",min_delta=0.0001,patience=10,verbose=2)\r\n\t\tcheckpoint=ModelCheckpoint(""checkpt.hdf5"",period=10,verbose=2)\r\n\t\treducelr=ReduceLROnPlateau(monitor=""val_loss"",factor=0.1,patience=5,verbose=2)\r\n\t\treturn [earlystop,checkpoint,reducelr]\r\n\r\n\tdef train(self,model,X,y):\r\n\t\tmodel.fit(X,y,nb_epoch=1000,callbacks=self._get_callbacks(),validation_split=0.1,verbose=2,batch_size=300)\r\n\t\tmodel.save(""model.hdf5"")\r\n`\r\nX is a one hot vector of len 256 for every word\r\ny in a one hot vector of len 256 representing the skip word in the context of X.\r\nso for instance is the sequence is [2,6,5,7,9]\r\nX will be \r\n[5,5,5,5,7,7,7...]\r\n\r\ny will be\r\n[2,6,7,9,6,5,9...]\r\nand so on for every word in the sequence.\r\n\r\nThis is what happens when i try to train:\r\n\r\n9s - loss: 4.5012 - acc: 0.1794 - val_loss: 4.5873 - val_acc: 0.1327\r\nEpoch 2/1000\r\n9s - loss: 4.2679 - acc: 0.1801 - val_loss: 4.8339 - val_acc: 0.1327\r\nEpoch 3/1000\r\n9s - loss: 4.2363 - acc: 0.1801 - val_loss: 4.7040 - val_acc: 0.1327\r\nEpoch 4/1000\r\n9s - loss: 4.2102 - acc: 0.1801 - val_loss: 4.6947 - val_acc: 0.1327\r\nEpoch 5/1000\r\n9s - loss: 4.1882 - acc: 0.1801 - val_loss: 4.6625 - val_acc: 0.1327\r\nEpoch 6/1000\r\n9s - loss: 4.1777 - acc: 0.1801 - val_loss: 4.6303 - val_acc: 0.1327\r\n\r\nI ve waited for a about 50 epochs and the acc still does not change. \r\n\r\nAny idea what i am doing wrong? Ive faced this problem everytime ive used keras even when training other models like language modelling using RNNs text generation using LSTMs.', 'Hi @adityashinde1506, You may have already found a solution but if not, try to decrease your learning rate.', 'Hi guys, I am having a similar problem. I am training an LSTM model for text classification and my loss does not improve on subsequent epochs. I tried many optimizers with different learning rates. But same problem.\r\n```\r\nmax_length = 275\r\nX_train = sequence.pad_sequences(train_data_new, maxlen=max_length, padding=\'post\')\r\nX_test = sequence.pad_sequences(test_data_new, maxlen=max_length, padding=\'post\')\r\n\r\ny_train = []\r\ny_test = []\r\n\r\n# preparing y_test and y_train\r\nfor label in train_label:\r\n\tif label == \'first\':\r\n\t\ty_train.append([1,0])\r\n\telse:\r\n\t\ty_train.append([0,1])\r\n\r\ny_train = np.array(y_train)\r\n\r\nfor label in test_label:\r\n\tif label == \'second\':\r\n\t\ty_test.append([1,0])\r\n\telse:\r\n\t\ty_test.append([0,1])\r\n\t\r\ny_test = np.array(y_test)\r\n\r\n# Create the model\r\nrmsprop = RMSprop(lr=0.1)\r\nsgd = SGD(lr=0.1)\r\nmodel = Sequential()\r\nmodel.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, \r\n\tweights=[embedding_matrix], \r\n\tinput_length=max_length,\r\n\ttrainable=True))\r\n# model.add(Dropout(0.2))\r\nmodel.add(LSTM(128, return_sequences=True))\r\n# model.add(Dropout(0.2))\r\nmodel.add(LSTM(64))\r\nmodel.add(Dense(2, activation=\'sigmoid\'))\r\nmodel.compile(loss=\'categorical_crossentropy\', optimizer=rmsprop, metrics=[\'accuracy\'])\r\nprint(model.summary())\r\n\r\nprint(\'Training model...\')\r\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=64)\r\n\r\n# Final evaluation of the model\r\nscores = model.evaluate(X_test, y_test, verbose=0)\r\nprint(""Accuracy: %.2f%%"" % (scores[1]*100))\r\n```\r\nThe OUTPUT I get is this:\r\n```\r\nTotal train samples:  931\r\nTotal test samples:  390\r\nProcessing text dataset\r\nFound 6132 unique tokens.\r\nIndexing word vectors.\r\nFound 400000 word vectors.\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nembedding_1 (Embedding)      (None, 275, 200)          1226600   \r\n_________________________________________________________________\r\nlstm_1 (LSTM)                (None, 275, 128)          168448    \r\n_________________________________________________________________\r\nlstm_2 (LSTM)                (None, 64)                49408     \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 2)                 130       \r\n=================================================================\r\nTotal params: 1,444,586\r\nTrainable params: 1,444,586\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nNone\r\nTraining model...\r\nTrain on 931 samples, validate on 390 samples\r\nEpoch 1/10\r\n931/931 [==============================] - 16s - loss: 0.8362 - acc: 0.5081 - val_loss: 0.6939 - val_acc: 0.4949\r\nEpoch 2/10\r\n931/931 [==============================] - 14s - loss: 0.6935 - acc: 0.5016 - val_loss: 0.6936 - val_acc: 0.4949\r\nEpoch 3/10\r\n931/931 [==============================] - 14s - loss: 0.6933 - acc: 0.5016 - val_loss: 0.6933 - val_acc: 0.4949\r\nEpoch 4/10\r\n931/931 [==============================] - 14s - loss: 0.6933 - acc: 0.5016 - val_loss: 0.6933 - val_acc: 0.4949\r\nEpoch 5/10\r\n931/931 [==============================] - 14s - loss: 0.6933 - acc: 0.5016 - val_loss: 0.6932 - val_acc: 0.4949\r\nEpoch 6/10\r\n931/931 [==============================] - 14s - loss: 0.6932 - acc: 0.5016 - val_loss: 0.6932 - val_acc: 0.4949\r\nEpoch 7/10\r\n931/931 [==============================] - 14s - loss: 0.6932 - acc: 0.5016 - val_loss: 0.6932 - val_acc: 0.4949\r\nEpoch 8/10\r\n931/931 [==============================] - 13s - loss: 0.6932 - acc: 0.5016 - val_loss: 0.6931 - val_acc: 0.4949\r\nEpoch 9/10\r\n931/931 [==============================] - 13s - loss: 0.6931 - acc: 0.5016 - val_loss: 0.6931 - val_acc: 0.4949\r\nEpoch 10/10\r\n931/931 [==============================] - 14s - loss: 0.6931 - acc: 0.5016 - val_loss: 0.6931 - val_acc: 0.4949\r\nAccuracy: 49.49%\r\n```\r\n**I am unable to figure out what the problem is.**\r\n', 'Out of curiosity, why are you passing in a ""weights"" matrix to the Embedding layer?  Thanks.', 'Hi @td2014 , that `weights` in `Embedding layer` is just because i want to give my own embeddings (GloVe in this case) for the word inputs. Even when i removed the weights and ran the file, my loss didnt change. ', 'Okay.  And you set the learning rate to 0.1 for your optimizer(s).  Just curious, but was the default not working?  Thanks.', 'Initially, it was default. Then I read on a similar issue page on stackoverflow where it told to alter learning rates. So I was trying to see the change on different values. Using lr=0.1 the loss starts from 0.83 and becomes constant at 0.69. When I was using default value, loss was stuck same  at 0.69', ""Okay.  I created a simplified version of what you have implemented, and it does seem to work (loss decreases).  Here is the code you can cut and paste.  Note that the first section is setting up the environment for reproducible results (which I provide at the end in my case).  In your case, you may want to check a few things:\r\n1) Is your input data making sense?  It could be that the preprocessing steps (the padding) are creating input sequences that cannot be separated (perhaps you are getting a lot of zeros or something of that sort).\r\n2) You might want to simplify your architecture to include just a single LSTM layer (like I did) just until you convince yourself that the model is actually learning something.\r\n\r\nI hope this helps.  Thanks.\r\n\r\n---------\r\n# Start: Set up environment for reproduction of results\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport random as rn\r\nimport os\r\nos.environ['PYTHONHASHSEED'] = '0'\r\nnp.random.seed(42)\r\nrn.seed(12345)\r\n#single thread\r\nsession_conf = tf.ConfigProto(\r\n      intra_op_parallelism_threads=1,\r\n      inter_op_parallelism_threads=1)\r\n\r\nfrom keras import backend as K\r\ntf.set_random_seed(1234)\r\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\r\nK.set_session(sess)\r\n# End:  Set up environment for reproduction of results\r\n\r\n#\r\nfrom keras.layers import LSTM, Dense, Embedding\r\nfrom keras.models import Sequential\r\nfrom keras.preprocessing import sequence\r\n\r\n#\r\n# Create input sequences\r\n#\r\nword_index=21\r\ntrain_data_new = []\r\ntrain_data_new.append([1, 1, 1, 1, 1, 1, 1 ])\r\ntrain_data_new.append([2, 2, 2, 2, 2, 2, 2 ])\r\ntrain_data_new.append([2, 2, 2, 2, 2, 2, 2 ])\r\ntrain_data_new.append([1, 1, 1, 1, 1, 1, 1 ])\r\ntrain_data_new.append([2, 2, 2, 2, 2, 2, 2 ])\r\ntrain_data_new.append([2, 2, 2, 2, 2, 2, 2 ])\r\ntrain_data_new.append([1, 1, 1, 1, 1, 1, 1 ])\r\ntrain_data_new.append([2, 2, 2, 2, 2, 2, 2 ])\r\ntrain_data_new.append([2, 2, 2, 2, 2, 2, 2 ])\r\n\r\n#\r\n# Preprocess\r\n#\r\n\r\nmax_length = 5\r\nX_train = sequence.pad_sequences(train_data_new, maxlen=max_length, padding='post')\r\n\r\n# preparing y_train\r\ny_train = []\r\ny_train.append([1,0])\r\ny_train.append([0,1])\r\ny_train.append([0,1])\r\ny_train.append([1,0])\r\ny_train.append([0,1])\r\ny_train.append([0,1])\r\ny_train.append([1,0])\r\ny_train.append([0,1])\r\ny_train.append([0,1])\r\n\r\ny_train = np.array(y_train)\r\n\r\n#\r\n# Create model\r\n#\r\n\r\nEMBEDDING_DIM=16\r\n\r\nmodel = Sequential()\r\nmodel.add(Embedding(word_index + 1, EMBEDDING_DIM, input_length=max_length))\r\nmodel.add(LSTM(5))\r\nmodel.add(Dense(2, activation='sigmoid'))\r\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\r\nprint(model.summary())\r\n\r\n#\r\n# Train\r\n# \r\nprint('Training model...')\r\nmodel.fit(X_train, y_train, epochs=10, shuffle=False)\r\n\r\n#\r\n# output predictions\r\n#\r\n\r\npredictions = model.predict(X_train)\r\n\r\n====OUTPUT BELOW====\r\n\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nembedding_29 (Embedding)     (None, 5, 16)             352       \r\n_________________________________________________________________\r\nlstm_29 (LSTM)               (None, 5)                 440       \r\n_________________________________________________________________\r\ndense_29 (Dense)             (None, 2)                 12        \r\n=================================================================\r\nTotal params: 804.0\r\nTrainable params: 804\r\nNon-trainable params: 0.0\r\n_________________________________________________________________\r\nNone\r\nTraining model...\r\nEpoch 1/10\r\n9/9 [==============================] - 2s - loss: 0.7013 - acc: 0.3333\r\nEpoch 2/10\r\n9/9 [==============================] - 0s - loss: 0.6953 - acc: 0.3333\r\nEpoch 3/10\r\n9/9 [==============================] - 0s - loss: 0.6911 - acc: 0.3333\r\nEpoch 4/10\r\n9/9 [==============================] - 0s - loss: 0.6875 - acc: 1.0000\r\nEpoch 5/10\r\n9/9 [==============================] - 0s - loss: 0.6842 - acc: 1.0000\r\nEpoch 6/10\r\n9/9 [==============================] - 0s - loss: 0.6812 - acc: 1.0000\r\nEpoch 7/10\r\n9/9 [==============================] - 0s - loss: 0.6783 - acc: 1.0000\r\nEpoch 8/10\r\n9/9 [==============================] - 0s - loss: 0.6754 - acc: 1.0000\r\nEpoch 9/10\r\n9/9 [==============================] - 0s - loss: 0.6726 - acc: 1.0000\r\nEpoch 10/10\r\n9/9 [==============================] - 0s - loss: 0.6698 - acc: 1.0000"", '@td2014 \r\n\r\nDude, you architecture just **does** not work. Try something _different_.', 'It works, I had to clean the data. Then the loss started to converge', ""try 'sigmoid' activation for the last layer since it's a binary classification problem"", 'Here is a good list of issues to check for that I have found useful: https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607', 'Another reason could be class imbalance. So try upsampling or downsampling using SMOTE/OneSidedSelection from imblearn package, then reshape your data back to 4 dimensions for your model.', ""I encountered the problem while I was trying to finetune a pretrained VGGFace model, using keras_vggface.utils.preprocess_input as my custom preprocessing function.\r\n\r\n\r\n```\r\ndef preprocess_input(x, data_format=None, version=1):\r\n    if data_format is None:\r\n        data_format = K.image_data_format()\r\n    assert data_format in {'channels_last', 'channels_first'}\r\n\r\n    if version == 1:\r\n        if data_format == 'channels_first':\r\n            x = x[:, ::-1, ...]\r\n            x[:, 0, :, :] -= 93.5940\r\n            x[:, 1, :, :] -= 104.7624\r\n            x[:, 2, :, :] -= 129.1863\r\n        else:\r\n            x = x[..., ::-1]\r\n            x[..., 0] -= 93.5940\r\n            x[..., 1] -= 104.7624\r\n            x[..., 2] -= 129.1863\r\n\r\n    elif version == 2:\r\n        if data_format == 'channels_first':\r\n            x = x[:, ::-1, ...]\r\n            x[:, 0, :, :] -= 91.4953\r\n            x[:, 1, :, :] -= 103.8827\r\n            x[:, 2, :, :] -= 131.0912\r\n        else:\r\n            x = x[..., ::-1]\r\n            x[..., 0] -= 91.4953\r\n            x[..., 1] -= 103.8827\r\n            x[..., 2] -= 131.0912\r\n    else:\r\n        raise NotImplementedError\r\n\r\n    return x\r\n\r\n```\r\n\r\nThe problem seems to come from the scaling. I used preprocessing_function=keras_vggface.utils.preprocess_input and got into that problem. However, when I rescale it with 1/255. the problem is fixed. I think it may be that the pretrained model was trained additionally with a scaling factor to normalize it to [0,1], but the preprocessing function only gives us the mean so we know which means to subtract to center the data. I'd recommend you check if your scaling makes sense; a bad scaling of inputs into a Neural Network may cause your updates to either move very slowly (i.e. the derivative of the sigmoid function beyond -3 and +3 are near 0 and so your gradients are almost 0), or if you're using something like the ReLU function, the updates may be big (the derivative is 1) and a wrong update makes you jump pass the local minima very easily.\r\n\r\nALSO, if you're rescaling in python 2, make sure you have that dot in 1/255., or else all your inputs will be multiplied by 0 and you aren't making any updates!!!"", 'sigmoid_cross_entropy_with_logits may encounters the gradients explosion problem, try using clip_gradients.', ""In my case, It is the normalization problem:\r\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\n\r\nx_train /= 255 \r\nx_test /= 255  # normalization [0, 255]--->[0, 1]"", 'I am trying to Create DNN but it is not converging, any idea\r\nmodel = Sequential()\r\nmodel.add(Dense(5000, input_dim=5, activation=\'relu\', kernel_regularizer=regularizers.l2(0.1)))\r\nmodel.add(Dropout(0.1))\r\nmodel.add(Dense(2000, kernel_regularizer=regularizers.l2(0.1),activation=\'relu\'))\r\nmodel.add(Dropout(0.1))\r\nmodel.add(Dense(400, kernel_regularizer=regularizers.l2(0.1),activation=\'relu\'))\r\nmodel.add(Dropout(0.1))\r\nmodel.add(Dense(1,activation=\'relu\'))\r\n\r\n\r\nrmsprop = optimizers.RMSprop(lr=0.01, rho=0.7, epsilon=1e-8, decay=0.0)\r\nsgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.4, nesterov=True)\r\nmodel.compile(loss=\'mse\', optimizer=\'adam\', metrics=[""mae""])\r\ncallbacks = [EarlyStopping(monitor=\'val_loss\', patience=5),\r\n             ModelCheckpoint(filepath=\'DNN_Adam.h5\', monitor=\'val_loss\', save_best_only=True)]\r\nnp.random.seed(3)\r\nhistory=model.fit(X_train, y_train, epochs=500 , batch_size=5000, validation_split=0.1, verbose=2,callbacks=callbacks)\r\nTrain on 16109 samples, validate on 1790 samples\r\nEpoch 1/500\r\n - 20s - loss: 322.9844 - mean_squared_error: 7.8310e-04 - val_loss: 243.3298 - val_mean_squared_error: 2.3419e-08\r\nEpoch 2/500\r\n - 15s - loss: 216.4914 - mean_squared_error: 6.4440e-04 - val_loss: 156.6757 - val_mean_squared_error: 2.3419e-08\r\nEpoch 3/500\r\n - 15s - loss: 137.8335 - mean_squared_error: 6.8178e-04 - val_loss: 96.6401 - val_mean_squared_error: 2.3419e-08\r\nEpoch 4/500\r\n - 15s - loss: 84.1424 - mean_squared_error: 7.0834e-04 - val_loss: 57.3809 - val_mean_squared_error: 2.3419e-08\r\nEpoch 5/500\r\n - 15s - loss: 49.5767 - mean_squared_error: 7.1517e-04 - val_loss: 33.2029 - val_mean_squared_error: 2.3419e-08\r\nEpoch 6/500\r\n - 15s - loss: 28.6330 - mean_squared_error: 7.1524e-04 - val_loss: 19.2849 - val_mean_squared_error: 2.3419e-08\r\nEpoch 7/500\r\n - 15s - loss: 16.7852 - mean_squared_error: 7.1524e-04 - val_loss: 11.7314 - val_mean_squared_error: 2.3419e-08\r\nEpoch 8/500\r\n - 15s - loss: 10.4144 - mean_squared_error: 7.1524e-04 - val_loss: 7.7523 - val_mean_squared_error: 2.3419e-08\r\nEpoch 9/500\r\n - 15s - loss: 7.0391 - mean_squared_error: 7.1524e-04 - val_loss: 5.5379 - val_mean_squared_error: 2.3419e-08\r\nEpoch 10/500\r\n - 15s - loss: 5.0998 - mean_squared_error: 7.1524e-04 - val_loss: 4.1133 - val_mean_squared_error: 2.3419e-08\r\nEpoch 11/500\r\n - 15s - loss: 3.7908 - mean_squared_error: 7.1524e-04 - val_loss: 3.0279 - val_mean_squared_error: 2.3419e-08\r\nEpoch 12/500\r\n - 16s - loss: 2.7628 - mean_squared_error: 7.1524e-04 - val_loss: 2.1295 - val_mean_squared_error: 2.3419e-08\r\nEpoch 13/500\r\n - 16s - loss: 1.9126 - mean_squared_error: 7.1524e-04 - val_loss: 1.4014 - val_mean_squared_error: 2.3419e-08\r\nEpoch 14/500\r\n - 18s - loss: 1.2362 - mean_squared_error: 7.1524e-04 - val_loss: 0.8581 - val_mean_squared_error: 2.3419e-08\r\nEpoch 15/500\r\n - 18s - loss: 0.7441 - mean_squared_error: 7.1524e-04 - val_loss: 0.4902 - val_mean_squared_error: 2.3419e-08\r\nEpoch 16/500\r\n - 16s - loss: 0.4204 - mean_squared_error: 7.1524e-04 - val_loss: 0.2675 - val_mean_squared_error: 2.3419e-08\r\nEpoch 17/500\r\n - 16s - loss: 0.2305 - mean_squared_error: 7.1524e-04 - val_loss: 0.1482 - val_mean_squared_error: 2.3419e-08\r\nEpoch 18/500\r\n - 16s - loss: 0.1316 - mean_squared_error: 7.1524e-04 - val_loss: 0.0910 - val_mean_squared_error: 2.3419e-08\r\nEpoch 19/500\r\n - 16s - loss: 0.0850 - mean_squared_error: 7.1524e-04 - val_loss: 0.0645 - val_mean_squared_error: 2.3419e-08\r\nEpoch 20/500\r\n - 15s - loss: 0.0629 - mean_squared_error: 7.1524e-04 - val_loss: 0.0500 - val_mean_squared_error: 2.3419e-08\r\nEpoch 21/500\r\n - 16s - loss: 0.0496 - mean_squared_error: 7.1524e-04 - val_loss: 0.0388 - val_mean_squared_error: 2.3419e-08\r\nEpoch 22/500\r\n - 17s - loss: 0.0388 - mean_squared_error: 7.1524e-04 - val_loss: 0.0285 - val_mean_squared_error: 2.3419e-08\r\nEpoch 23/500\r\n - 16s - loss: 0.0289 - mean_squared_error: 7.1524e-04 - val_loss: 0.0196 - val_mean_squared_error: 2.3419e-08\r\nEpoch 24/500\r\n - 15s - loss: 0.0204 - mean_squared_error: 7.1524e-04 - val_loss: 0.0127 - val_mean_squared_error: 2.3419e-08\r\nEpoch 25/500', 'I had a similar issue today when training on Google cloud GPU. I tried changing network architecture, weights, etc. The solution was to reset the TF graph: tf_reset_default_graph()\r\nSomehow the GPU seemed to have a ""memory"" across different runs and was stuck at a local minima.', '> I had a similar issue today when training on Google cloud GPU. I tried changing network architecture, weights, etc. The solution was to reset the TF graph: tf_reset_default_graph()\r\n> Somehow the GPU seemed to have a ""memory"" across different runs and was stuck at a local minima.\r\n\r\nThx，I will have a try，hope it would work！', 'For me, theese 3 things did the trick:\r\n1. **Lower the learning rate** (0.1 converges too fast and already after the first epoch, there is no change anymore). Just for test purposes try a very low value like `lr=0.00001`.\r\n2. Check the **input** for proper **value range** and **normalize** it\r\n3. Add **BatchNormalization** (`model.add(BatchNormalization())`) after each layer', 'For me, this works:\r\nAdd BatchNormalization (model.add(BatchNormalization())) after each layer\r\nThanks @Fellfalla \r\n', 'I had the same problem. But for me it is just the learning_rate is too small. Try with a bigger learning_rate. It might help', ""Based on my own experience as a starter, one possible reason or bug in your model is that you probably used a wrong activation function, i.e. the way you activated your result at the last output layer, for example, if you are trying to solve a multi class proplem, usually we use softmat rather sigmoid, while sigmoid is meant to activate the output for binary task. And in this case, it's a binary application, therefore just change your activation function as sigmoid, you should not find such exception."", ""Try removing the Activation('softmax') layer."", 'I had a model that did not train at all. It just stucks at random chance of particular result with no loss improvement during training. Loss was constant 4.000 and accuracy 0.142 on 7 target values dataset.\r\n\r\nIt become true that I was doing regression with ReLU last activation layer, which is obviously wrong.\r\n\r\nBefore I was knowing that this is wrong, I did add Batch Normalisation layer after every learnable layer, and that helps. However, training become somehow erratic so accuracy during training could easily drop from 40% down to 9% on validation set. Accuracy on training dataset was always okay.\r\n\r\nThen I realized that it is enough to put Batch Normalisation before that last ReLU activation layer only, to keep improving loss/accuracy during training. That probably did fix wrong activation method.\r\n\r\nHowever, when I did replace ReLU with Linear activation (for regression), no Batch Normalisation was needed any more and model started to train significantly better.', ""I'd highly recommend messing around with learning rates... Testing with extreme variability, like `lr = 0.1` & `lr = 1e-4`, should do the trick in most instances."", 'I think that the problem comes from the learning rate, Mine was actually equal to 7 ahah. I wrote 10-3 instead of 1e-3.', ""My data had 3 classes but last layer was `Dense(1, activation='sigmoid')` changing it to `Dense(3, activation='sigmoid')` made the loss change. With the 1 output neuron it didn't return any errors, just had a constant loss.\r\n\r\nAlso if you are training binary classifier, you can just use `Dense(1, activation='sigmoid')` as output with binary_crossentropy, instead of `Dense(2, activation='sigmoid')` with categorical_crossentropy"", 'Happened many times, my solution is change or remove the activation function in the last layer.']","['\nnp.shape(Xtrain)\nOut[58]: (2000, 1, 106, 106)\n\nnp.shape(Ytrain)\nOut[59]: (2000, 2)\n\n', '\nmodel = Sequential()\nmodel.add(Convolution2D(32, 16, 16, border_mode=\'same\',name=\'conv1\', input_shape = (1, 106, 106)))\nfirst_layer = model.layers[0]\nmodel.add(Activation(""relu""))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n#2\nmodel.add(Convolution2D(64, 15, 15, border_mode=\'same\',name=\'conv2\'))\nmodel.add(Activation(""relu""))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n#3\nmodel.add(Convolution2D(128, 14, 14, border_mode=\'same\',name=\'conv3\'))\nmodel.add(Activation(""relu""))\n\n#flatten\nmodel.add(Flatten())\nmodel.add(Dense(2))\nmodel.add(Activation(\'softmax\'))\n\nrms = RMSprop()\nsgd = SGD(lr=0.000001, decay=1e-6, momentum=1.9)\nmodel.compile(loss=\'categorical_crossentropy\', optimizer=sgd,metrics=[""accuracy""])\nmodel.fit(Xtrain[950:1050], Ytrain[950:1050], batch_size=32, nb_epoch=20,\n          verbose=1, validation_split=0.2,\n          callbacks=[EarlyStopping(monitor=\'val_loss\', patience=0)])\n']",[],0,0
551,keras,9824,closed,"ValueError: Error when checking input: expected input_1 to have shape (None, 4096) but got array with shape (0, 1)","from numpy import array
from pickle import load
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
#from keras.utils import plot_model
from keras.models import Model
from keras.layers import Input
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Embedding
from keras.layers import Dropout
from keras.layers.merge import add
from keras.callbacks import ModelCheckpoint
import itertools

# load doc into memory
def load_doc(filename):
	# open the file as read only
	file = open(filename, 'r')
	# read all text
	text = file.read()
	# close the file
	file.close()
	return text

# load a pre-defined list of photo identifiers
def load_set(filename):
	doc = load_doc(filename)
	dataset = list()
	# process line by line
	for line in doc.split('\n'):
		# skip empty lines
		if len(line) < 1:
			continue
		# get the image identifier
		identifier = line.split('.')[0]
		dataset.append(identifier)
	return set(dataset)

# load clean descriptions into memory
def load_clean_descriptions(filename, dataset):
	# load document
	doc = load_doc(filename)
	descriptions = dict()
	for line in doc.split('\n'):
		# split line by white space
		tokens = line.split()
		# split id from description
		image_id, image_desc = tokens[0], tokens[1:]
		# skip images not in the set
		if image_id in dataset:
			# create list
			if image_id not in descriptions:
				descriptions[image_id] = list()
			# wrap description in tokens
			desc = 'startseq ' + ' '.join(image_desc) + ' endseq'
			# store
			descriptions[image_id].append(desc)
	return descriptions

# load photo features
def load_photo_features(filename, dataset):
	# load all features
	all_features = load(open(filename, 'rb'))
	# filter features
	features = {k: all_features[k] for k in dataset}
	return features

# covert a dictionary of clean descriptions to a list of descriptions
def to_lines(descriptions):
	all_desc = list()
	for key in descriptions.keys():
		[all_desc.append(d) for d in descriptions[key]]
	return all_desc

# fit a tokenizer given caption descriptions
def create_tokenizer(descriptions):
	lines = to_lines(descriptions)
	tokenizer = Tokenizer()
	tokenizer.fit_on_texts(lines)
	return tokenizer

# calculate the length of the description with the most words
def max_length(descriptions):
	lines = to_lines(descriptions)
	return max(len(d.split()) for d in lines)

# create sequences of images, input sequences and output words for an image
def create_sequences(tokenizer, max_length, descriptions, photos):
	X1, X2, y = list(), list(), list()
	# walk through each image identifier
	for key, desc_list in descriptions.items():
		# walk through each description for the image
		for desc in desc_list:
			# encode the sequence
			seq = tokenizer.texts_to_sequences([desc])[0]
			# split one sequence into multiple X,y pairs
			for i in range(1, len(seq)):
				# split into input and output pair
				in_seq, out_seq = seq[:i], seq[i]
				# pad input sequence
				in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
				# encode output sequence
				out_seq = to_categorical([out_seq], num_classes=vocab_size)
				# store

				if key in photos.items():
					X1.append(photos[key][0])

				X2.append(in_seq)
				y.append(out_seq)
	return array(X1), array(X2), array(y)

# define the captioning model
def define_model(vocab_size, max_length):
	# feature extractor model
	 inputs1 = Input(shape=(4096,))
	 fe1 = Dropout(0.5)(inputs1)
	 fe2 = Dense(256, activation='relu')(fe1)
	# sequence model
	 inputs2 = Input(shape=(max_length,))
	 se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
	 se2 = Dropout(0.5)(se1)
	 se3 = LSTM(256)(se2)
	# decoder model
	 decoder1 = add([fe2, se3])
	 decoder2 = Dense(256, activation='relu')(decoder1)
	 outputs = Dense(vocab_size, activation='softmax')(decoder2)
	# tie it together [image, seq] [word]
	 model = Model(inputs=[inputs1, inputs2], outputs=outputs)
	 model.compile(loss='categorical_crossentropy', optimizer='adam')
	# summarize model
	 print(model.summary())
	# plot_model(model, to_file='model.png', show_shapes=True)
	 return model

# train dataset

# load training dataset (6K)
filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'
train = load_set(filename)
print('Dataset: %d' % len(train))
# descriptions
train_descriptions = load_clean_descriptions('descriptions.txt', train)
print('Descriptions: train=%d' % len(train_descriptions))
# photo features
train_features = load_photo_features('features.pkl', train)
print('Photos: train=%d' % len(train_features))
# prepare tokenizer
tokenizer = create_tokenizer(train_descriptions)
vocab_size = len(tokenizer.word_index) + 1
print('Vocabulary Size: %d' % vocab_size)
# determine the maximum sequence length
max_length = max_length(train_descriptions)
print('Description Length: %d' % max_length)
#print(train_descriptions.type())
#print(train_features.type())
interdesc = dict(itertools.islice(train_descriptions.items(),1000))
interfeatures = dict(itertools.islice(train_features.items(),1000))
#print(train_descriptions)
#print(interfeatures)
# prepare sequences
X1train, X2train, ytrain = create_sequences(tokenizer, max_length, interdesc, interfeatures)

# dev dataset

# load test set
filename = 'Flickr8k_text/Flickr_8k.devImages.txt'
test = load_set(filename)
print('Dataset: %d' % len(test))
# descriptions
test_descriptions = load_clean_descriptions('descriptions.txt', test)
print('Descriptions: test=%d' % len(test_descriptions))
# photo features
test_features = load_photo_features('features.pkl', test)
print('Photos: test=%d' % len(test_features))
# prepare sequences
X1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions, test_features)

# fit model

# define the model
model = define_model(vocab_size, max_length)
# define checkpoint callback
filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'
checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')
# fit model
model.fit([X1train, X2train], ytrain, epochs=20, verbose=2, callbacks=[checkpoint], validation_data=([X1test, X2test], ytest))",,[],[],[],0,0
552,keras,3880,closed,binary classification get all 0s with GRU,"I am studying Chinese stock market with  binary classification with GRU, the traing samples is balance and the output is fine with class 1s and 0s, however, when i want to predict the real, almost all  return is all 0s.
 It is very strange the i have one mode can predict 2 classes with oneday, the predict output is similar to training data, which is fine.
when this mode is trained with more epoches, I try to use it to predict the same day data, it's predict data is all like 0.2-0.3, which means I get all 0s?
I prepare all real data with same code so it is hard to explain one day is fine and the others are wrong
Any idea?thanks
",,"[""Are you using softmax as the output layer? \nIf it's binary and just 1 output node, it'll normalize all the outputs to the same value. USe 'Sigmoid'. \n\nAlternatively - what's the class distribution like?\n""]",[],[],0,0
553,keras,13190,closed,.,,,[],[],[],0,0
554,keras,5963,closed,Bug in Siamese Example Accuracy Calculation,"The siamese network example for mnist (https://github.com/fchollet/keras/blob/master/examples/mnist_siamese_graph.py) uses the following function to compute accuracy



This seems flawed to me. For example:


Surely the order of the predictions is important? In the example above, all of the predictions are different from the labels, and yet the compute_accuracy returns 100%. 

Am I missing something obvious, or is this a mistake.




- [X] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [X] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [X] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",stale,"['Prediction of the siamese model in this example is distance between pairs, therefore the calculation of accuracy is taking ""distance (predictions) < 0.5"" as True.', 'Ok I picked a bad example, but I still believe that the function has problems.\r\n\r\n```\r\nimport numpy as np\r\nlabs = np.array([0,1,0,1,0,1,0,1,0,1])\r\npred = np.array([1,1,1,1,1,1,1,1,1,0])\r\nprint(compute_accuracy(pred, labs))\r\n# prints 1.0\r\n```\r\n```\r\nimport numpy as np\r\nlabs = np.array([0,1,0,1,0,1,0,1,0,1])\r\npred = np.array([0,0,0,0,0,0,0,0,0,1])\r\nprint(compute_accuracy(pred, labs))\r\n# prints 0.4444\r\n```\r\n\r\n```\r\nimport numpy as np\r\nlabs = np.array([0,1,0,1,0,1,0,1,0,1])\r\npred = np.array([1,1,1,1,1,1,1,1,1,1])\r\nprint(compute_accuracy(pred, labs))\r\nacc.py:4: RuntimeWarning: Mean of empty slice.\r\n  return labels[predictions.ravel() < 0.5].mean()\r\n/usr/local/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\r\n  ret = ret.dtype.type(ret / rcount)\r\nnan\r\n```\r\n\r\nSurely this is not correct?\r\n\r\n\r\n', 'Agree, the old one actually is precision, and accuracy should be\r\n```python\r\ndef compute_accuracy(predictions, labels):\r\n    return np.mean(np.equal(predictions.ravel() < 0.5, labels))\r\n\r\n# * Accuracy on training set: 99.53%\r\n# * Accuracy on test set: 97.03%\r\n```', 'What about the loss function, and similarity labels? Is it correct or not? @joelthchao @sixhobbits', '@aryopg Should be correct. Any concern?', 'i got a suggestion from stackoverflow to ""modify"" the equation of contrastive loss and the sequence of the labels just like in this [paper](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf). But when i tried to aplly it, it gives me a bad result. But when i use the original keras example, it gives me a good result. Can you explain how can the code given by keras can accomodate the equation from the paper while the other code cannot?', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']","[""\r\ndef compute_accuracy(predictions, labels):\r\n    '''Compute classification accuracy with a fixed threshold on distances.\r\n    '''\r\n    return labels[predictions.ravel() < 0.5].mean()\r\n"", '\r\nimport numpy as np\r\nlabs = np.array([0,0,0,0,0,1,1,1,1,1])\r\npred = np.array([1,1,1,1,1,0,0,0,0,0])\r\nprint(compute_accuracy(pred, labs))\r\n# >>> 1.0\r\n']",[],0,0
555,keras,6912,closed,different learning rates to different layers of a CNN.,"Hi,
I wonder if there is a way to specify different learning rates to different layers of a CNN.
thanks
",stale,"['I also have this question - can there be different learning rates for different network layers? Also, can weights and biases have different learning rates?', 'Hi Sourya,\r\ncould you please install and check this:\r\n\r\n[https://github.com/nagash91/keras/commit/08d73ef7957fbe5f59099b9e2986d8165ee131e8](url)\r\nI receive this error when I want to call `Convolution1D`:\r\n\r\n`TypeError: __init__() takes at least 3 arguments (1 given)`\r\n\r\nand here is the code:\r\n\r\n```\r\ninp = Input(shape=(94319,1))\r\nconv1= Convolution1D(filters=129, kernel_size=401)(inp)\r\n```', 'You could create your own optimizer where lr is a matrix and not a scalar.\r\nhttps://github.com/fchollet/keras/blob/master/keras/optimizers.py#L133.', ""Thanks @Dref360 I'll give it a try.\r\n@sajabdoli Your link is not working for me."", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],0,0
556,keras,1093,closed,LSTM `go_backwards`,"Hello,

When I try to run the  example, the model fails to build, and it throws the following error:



I'm using the latest Keras from , as well as the latest Theano from github, as was suggested many times. The error obviously occurs even when I try to create a backwards LSTM layer in my own models.

Any clue why this might be?
",,"['> I\'m using the latest Keras from pip\n\nThat is not the latest Keras. The latest Keras is the master branch on\nGithub.\n\nOn 26 November 2015 at 11:40, Jakub Fiala notifications@github.com wrote:\n\n> Hello,\n> \n> When I try to run the imdb_bidirectional_lstm.py example, the model fails\n> to build, and it throws the following error:\n> \n>   File ""/usr/local/lib/python2.7/site-packages/keras/layers/recurrent.py"", line 385, in **init**\n>     super(LSTM, self).**init**(**kwargs)\n>   File ""/usr/local/lib/python2.7/site-packages/keras/layers/core.py"", line 23, in **init**\n>     assert kwarg in {\'input_shape\'}, ""Keyword argument not understood: "" + kwarg\n> AssertionError: Keyword argument not understood: go_backward\n> \n> I\'m using the latest Keras from pip, as well as the latest Theano from\n> github, as was suggested many times. The error obviously occurs even when I\n> try to create a backwards LSTM layer in my own models.\n> \n> Any clue why this might be?\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/fchollet/keras/issues/1093.\n', 'oh okay, should `go_backwards` work in the github version?\n', 'Yes.\n', 'Thanks!\n']","['\n  File ""/usr/local/lib/python2.7/site-packages/keras/layers/recurrent.py"", line 385, in __init__\n    super(LSTM, self).__init__(**kwargs)\n  File ""/usr/local/lib/python2.7/site-packages/keras/layers/core.py"", line 23, in __init__\n    assert kwarg in {\'input_shape\'}, ""Keyword argument not understood: "" + kwarg\nAssertionError: Keyword argument not understood: go_backward\n']","['imdb_bidirectional_lstm.py', 'pip']",0,0
557,keras,9054,closed,Siamese network forward pass / test accuracy ,"Question regarding this example file: https://github.com/keras-team/keras/blob/master/examples/mnist_siamese.py

In the contrastive loss layer, the example file is using a margin for setting up the training process.
I would assume that the margin would be the same parameter as the threshold for the accuracy of the model. (here 0.5 in the compute_acc function). What is the reason for using a different threshold?

",,"['margin has absolutely nothing to do with the threshold, except that `0 < threshold < margin`']","[""\r\ndef contrastive_loss(y_true, y_pred):\r\n    '''Contrastive loss from Hadsell-et-al.'06\r\n    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\r\n    '''\r\n    margin = 1\r\n    return K.mean(y_true * K.square(y_pred) +\r\n                  (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\r\n\r\n\r\ndef compute_accuracy(y_true, y_pred):\r\n    '''Compute classification accuracy with a fixed threshold on distances.\r\n    '''\r\n    pred = y_pred.ravel() < 0.5\r\n    return np.mean(pred == y_true)\r\n\r\n""]",[],0,0
558,keras,2946,closed,Error saving the model keras.__version__=1.0.3,"

I upgraded the keras to 1.0.3 and since then it has started giving me this error. Earlier was 0.3.3, however it is running fine on another system with keras 1.0.3 version. Unable to figure out the issue. 
",stale,"['I am getting a similar error, was there ever a resolution to this? Are you by chance running on windows and creating at least one layer using a lambda function?', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']","['\nTraceback (most recent call last):\n File ""trainer.py"", line 61, in <module>\n create_training_features(input_file, en_parse_file)\n File ""trainer.py"", line 49, in create_training_features\n json_string =model.to_json()\n File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 2373, in to_json\n return json.dumps(model_config, default=get_json_type, **kwargs)\n File ""/usr/lib/python2.7/json/__init__.py"", line 250, in dumps\n sort_keys=sort_keys, **kw).encode(obj)\n File ""/usr/lib/python2.7/json/encoder.py"", line 207, in encode\n chunks = self.iterencode(o, _one_shot=True)\n File ""/usr/lib/python2.7/json/encoder.py"", line 270, in iterencode\n return _iterencode(o, 0)\n UnicodeDecodeError: \'utf8\' codec can\'t decode byte 0x8f in position 169: invalid start byte\n']",[],0,0
559,keras,2160,closed,Bugs in save and load weights,"I want to train a model with parts of parameters fixed. When I tried to load saved weights from the HDF5 file, it fails. It seems the saveweight function does not save none-trainable weights, while loadweight function requires them. See example code below:

<pre><code>
from keras.models import Sequential
from keras.layers.core import Merge
from keras.layers.convolutional import Convolution2D


model_range_5 = Sequential()
model_range_5.add(Convolution2D(128,5,5,dim_ordering='tf',activation='relu',border_mode='same',input_shape=(256,200,3),trainable=False))

model_range_4 = Sequential()
model_range_4.add(Convolution2D(128,5,5,dim_ordering='tf',activation='relu',border_mode='same',input_shape=(256,200,3)))


test_model = Sequential([Merge([model_range_5,model_range_4])])

test_model.compile(optimizer='SGD',loss='mse')
test_model.save_weights('weight.model',overwrite=True)
test_model.load_weights('weight.model')
</code></pre>
",stale,"['same issue, when I load the model it outputs the same value. \r\n']",[],[],0,0
560,keras,5176,closed,Scikit wrapper on Functional API as well?,From the documentation in Keras it seems that the scikit-learn API is (only) supported on Sequential models. Does it then make any sense to use it as well when using the Keras Functional API?,stale type:feature,[],[],[],0,0
561,keras,2905,closed,Rerun graph.fit many times,"Hi, Guys, 

I have come across a problem that the training dataset is very large so I split it into many subset and then load each subset for training. In each subset, I split it into batches for training. Here, after compiling, can I use the function **graph.fit()** many times to update the weights for different subsets? 
",stale,"[""Yes. If you've already split your data into minibatches, you may also consider using the `model.train_on_batch()` function.\n""]",[],[],0,0
562,keras,2521,closed,"Setting callbacks order, including default callbacks","A minor issue, admittedly, but is there a builtin way to set the call order of a model's callbacks (,  and so on)? I assume the callbacks follow the input list's ordering, but what about the defaults. Specifically ProgbarLogger, since it seems to be called last, the output gets wonky in case any other callback also prints to stdout during .
",stale,"[""I think that this issue could/should trigger a modification to how callbacks are passed.  I've been overriding callback instantiations with: \n\n``` python\n        self.history = cbks.History()\n        callbacks = [cbks.BaseLogger()] + callbacks + [self.history]\n        if verbose and all([not isinstance(cbk, cbks.ProgbarLogger) for cbk in callbacks]):\n            callbacks += [cbks.ProgbarLogger()]\n        callbacks = cbks.CallbackList(callbacks)\n```\n\nbecause I want to input my own progbar.  Maybe the `CallbackList` could be exposed as an optional input parameter to the `Model` constructor, so you can get more fine-grained control.  then you could do something like \n\n``` python\nif callback_list_input is None: \n    ### default behavior\n```\n"", 'A much better way would be to use factories but that doesn\'t seem common in Python. \n\nA fast and silly solution would be to allow the `callbacks` parameter to `model.fit(...)` to be either a list or a dictionary with optional priority, and maybe just have the defaults accessible through documented keys and letting -1 represent ""never call"". It would be completely backwards compatible but ugly.\n\nUltimately maybe just follow the decorator-like functional API for the callbacks?\n', ""> a much better way\n\noh ya. my way is totally ugly and hackish. I just needed it to work quickly. \n\nso, idea list:\n1. allow `callbacks` to be input into `model.fit(...)` \n2. decorator? not sure I fully understand what you mean.  Do you mean the `__call__` ability of containers?\n\npossible 3, just to elucidate a full range of options:\n\nWhat if it became fully functional and Callbacks were functional as well? \n\n``` python\nsome_callbacks = [History(), Whatever()]\ncallbacks = Callbacks(some_callbacks, option='something')\nmodel = Model(...)\nmodel = callbacks(model) ## callbacks __call__ just modifies model and returns the model\n```\n\non calling `model.fit` or some other method, it would internally check to see if it had any callbacks added, and if not, then put in the defaults.  \n\nfor short code, I have a compose function \n\n``` python\ndef compose(*layers):\n    def func(x):\n        out = x\n        for layer in layers[::-1]:\n            out = layer(out)\n        return out\n    return func\n```\n\nand then do things like\n\n``` python\n        F_alignlex = compose(RepeatVector(repeat_N),\n                             Dropout(p_mlp), \n                             Dense(mlp_size, activation='relu'),\n                             concat)\n```\n\nso then, you could do:\n\n``` python\ncallbacks = Callbacks([some_callback, etc])\noptimizer = Adam(...)\ncompose(callbacks, optimizer)(model)\n```\n\nIn this way, you could allow for the full range of customization without having to muck with the internals of Model.  It resembles the [visitor design pattern](https://en.wikipedia.org/wiki/Visitor_pattern)\n"", 'On a different note, that compose function looks neat. I think it should be used to deprecate Sequential() entirely and have everyone use the functional API.\n', ""It makes sense to have it, given function composition is a major point of this API.   And the repeatable composed functions are easier to read (fuller example):\n\n``` python\n        concat = lambda layers: merge(layers, mode='concat')\n        F_multiword = compose(F_attend,\n                              F_embedword)\n        F_singleword = compose(Fix(),\n                               F_embedword)\n        F_alignlex = compose(RepeatVector(repeat_N),\n                             Dropout(p_mlp), \n                             Dense(mlp_size, activation='relu'),\n                             concat)\n        children = F_multiword(child_in)\n        parent = F_singleword(parent_in)\n        grandparent = F_singleword(grandparent_in)\n\n        lexical_context = F_alignlex([children, parent, grandparent])\n```\n"", 'Any news on this regard? Can the order be set in newer versions?']",[],"['on_epoch_end', 'on_batch_begin', 'on_epoch_end']",0,0
563,keras,2560,closed,Race condition in fit_generator() when generator exits,"Users of  like  have a race condition when the generator exits after generating a number of samples equal to or slightly greater than . The function will occasionally fetch  from the queue instead of the final elements.
",stale,"['The generator is expected to loop indefinitely according to the documentation so if you by _""when the generator exits""_ mean that it\'s finished and that another next() call will result in a `StopIteration` being raised, then this is not a necessary fix.\n', 'You might want to iterate over a finite data set once; right now you have to add an unspecified number of padding elements to do this. Also, `evaluate_generator()` and `predict_generator()` have the same problem, but I should update the doc to say ""generator must return at least (samples_per_epoch \\* nb_epoch) elements"".\n', ""If you want to iterate over a finite data set just once, then you're expected to set `nb_epochs=1` when calling `fit_generator()`. Doesn't that work?\n\nThe thing is that the generator runs on its own thread to maximize GPU throughput (and there even used to be multi-threading support!), which is why it should never stop. The thread has a queue of samples that handles everything, but for that to work the generator must never finish.\n"", ""> If you want to iterate over a finite data set just once, then you're expected to set nb_epochs=1 when calling fit_generator. Doesn't that work?\n\nYes, that's what I'm doing, but because of the race condition the generator has to stay alive even after it has yielded all of its samples. The fix is simple, it's just a matter of making sure the queue is empty before you check the `_stop` event.\n"", 'Again, the generator should yield its samples indefinitely and could therefore never exit. You\'re not making much sense. \n\n> ""generator must return at least (samples_per_epoch \\* nb_epoch) elements"".\n\nNo, the generator should yield exactly `samples_per_epoch` distinct samples repeatedly, forever.\n', ""My dataset doesn't fit into memory, so I load new files and create a new generator after every epoch, passing it to fit_generator() with nb_epochs = 1.\n\nI guess I could use train_on_batch() or make the generator loop over its data multiple times, but I don't see why finite generators are a problem when you know exactly how many samples will be consumed.\n"", ""If you do that you loose the immense performance benefit of being able to buffer up samples in the queue at the end of an epoch for the start of the next epoch. \n\nUnlike the first batches of the first epoch in which your model has to wait a while for the data to be processed initially, the start of every successive epoch can start immediately because the generator queue keeps at it. \n\nYou really shouldn't kill the data generator thread like you're doing, for this reason alone.\n\nWhat do you even gain from initializing a data generator per epoch?\n"", ""As part of the preprocessing I have to evaluate the input on a separate large model which is too big to coexist with the training model in RAM. So I can't easily do this inside of the generator since it would swap out the model being trained and they would both grind to a halt. Probably suboptimal, but my epochs are so long that it isn't a huge performance hit.\n"", ""That seems like a legitimate use case (albeit exotic). It seems like an uphill battle to reload models continuously during training, and nothing was built with this in mind as far as I know.\n\nWhen you say RAM do you mean for the CPU or GPU VRAM? I'd just focus on having enough memory such that the models fit, but it could be difficult with VRAM [until summer](http://www.nvidia.com/object/gpu-architecture.html). RAM is cheap though! What is your memory usage? I'm curious.\n- Depending on your backend there are optimizations that can be turned on to reduce memory usage at the cost of increased computation time. [Hints for Theano, for example.](http://deeplearning.net/software/theano/faq.html#theano-memory-speed-trade-off)\n- You should also consider [reusing parts of the computational graph](http://keras.io/getting-started/functional-api-guide/) between the models. You shouldn't need to declare both, unless they're totally different of course.\n- If all else fails, run the models on separate machines, and use `on_epoch_end` to communicate over a network. This is probably your safest bet.\n"", ""There's an option to pass in a generator to validation_data in fit_generator. There's a finite amount of data to loop over and you'll have a StopIteration to indicate the end.\n\nMore broadly, I'd like Keras to have better support for generators -- they're a handy of working with large amounts of data.\n"", 'Similar problem here. I understand that the generator should yield indefinitely, and that does ""fix"" the problem. But to me, requiring the generator to yield indefinitely IS the problem. Here is why:\r\n\r\nI\'m using predict_generator() to classify images. My generator code is similar to this (simplified):\r\n\r\n```\r\ndef gen():\r\n    for i in range(100):\r\n        yield load_images([i*batch_size : (i+1)*batch_size])\r\n```\r\n\r\nKeras returns this error:\r\n\r\n```\r\nValueError: output of generator should be a tuple (x, y, sample_weight) or (x, y). Found: None\r\n```\r\n\r\nAfter spending about an hour debugging my code to find out why my generator was returning None, I finally stumbled upon this thread and realized that Keras expects generators to yield indefinitely, even when they don\'t have anything else to yield. \r\n\r\n- Unlike in training, when you call evaluate_generator() and predict_generator(), there is a limited number of samples and each sample is expected to be processed once. It seems odd to ask me to re-loop through my data again, and it goes against the simple straightforward style that Keras is known for. \r\n- Even in training, when an infinite supply of samples is expected, the error message should make the requirement to yield indefinitely very clear. As it is right now, it makes it sound like the generator did not return any results at all. Which is what sent me debugging the wrong problem.\r\n', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs, but feel free to re-open it if needed.\n']",[],"['generator_queue()', 'fit_generator()', 'samples_per_epoch', 'None']",0,0
564,keras,3425,closed,Sudden accuracy drop when training LSTM or GRU,"Hi! My **recurrent neural network (LSTM, resp. GRU)** behaves in a way I cannot explain. The training starts and it trains well (the results look quite good) when **suddenly accuracy drops (and loss rapidly increases)** - both training and testing metrics. Sometimes the net just _goes crazy_ and returns random outputs and sometimes (as in the last of three given examples) it starts to return _same output to all the inputs_. 

![image](https://cloud.githubusercontent.com/assets/8523511/17508785/f94aaf50-5e16-11e6-92cd-4d842b4c0e9c.png)

Do you have **any explanation for this behavior**? Any opinion is welcome. Please, see the task description and the figures below.

**The task:** From a word predict its word2vec vector
**The input:** We have an own word2vec model (normalized) and we feed the network with a word (letter by letter). We pad the words (see the example below).
**Example:** We have a word _football_ and we want to predict its word2vec vector which is 100 dimensions wide. Then the input is . 

Three examples of the behavior:

**Single layer LSTM**



![image](https://cloud.githubusercontent.com/assets/8523511/17507845/43eecbfe-5e12-11e6-8ae3-179ac10d82b7.png)

**Single layer GRU**



![image](https://cloud.githubusercontent.com/assets/8523511/17507923/8ec75934-5e12-11e6-891b-a2d2f2c10e39.png)

**Double layer LSTM**



![image](https://cloud.githubusercontent.com/assets/8523511/17507947/a29ad68e-5e12-11e6-9355-bde7711df71f.png)

We have also _experienced this kind of behavior in another project before_ which used similar architecture but its objective and data were different. Thus the reason should not be hidden in the data or in the particular objective but rather in the architecture.
",stale,"['My only 2 cents is that in the original W2V paper they used a shallow FFNN so there must be something about the RNN structures. I would think exploding/vanishing gradient but you are seeing it with the LSTM so....\n', 'I have already experienced the same behavior with non-word2vec data and the behavior was very similar. Any idea on how to extract gradients from Keras? ... in order to examine closer whether the gradient is exploding/vanishing/... ? \n', ""I'm seeing something similar with a single layered LSTM classifier on acoustic data. After around 50 epochs, the loss suddenly started increasing and training and validation accuracy dropped badly.\n\n```\nmodelV1 = Sequential()\nmodelV1.add(LSTM(60, return_sequences=False,\n                     input_shape=(20, 3), activation='relu'))\nmodelV1.add(Dropout(0.2))\nmodelV1.add(Dense(7, activation='softmax'))\nmodelV1.compile(loss='categorical_crossentropy',\n               optimizer='Nadam',\n               metrics=['accuracy'])\nmodelV1.fit(F_train, V1_train, nb_epoch=100, validation_split=0.15)\n```\n\nEdit: I have two separate test sets and the really weird thing is the model evaluates similar to training/validation (i.e. badly) on one but seems to perform decently on the other.\n"", 'Oh I had the same problem. Very annoying. I trained a network with LSTM and also used (word2vec), and got a sudden drop as in below:\nEpoch 37/500\n6400/6400 [==============================] - 44s - loss: 1.6209 - acc: 0.4892 - val_loss: 1.7309 - val_acc: 0.4562\nEpoch 38/500\n6400/6400 [==============================] - 44s - loss: 1.6402 - acc: 0.4805 - val_loss: 1.6928 - val_acc: 0.4463\nEpoch 39/500\n6400/6400 [==============================] - 44s - loss: 1.6328 - acc: 0.4917 - val_loss: 1.6404 - val_acc: 0.5044\nEpoch 40/500\n6400/6400 [==============================] - 44s - loss: 2.2682 - acc: 0.3142 - val_loss: 2.7337 - val_acc: 0.1806\nEpoch 41/500\n6400/6400 [==============================] - 44s - loss: 2.7492 - acc: 0.1811 - val_loss: 2.6720 - val_acc: 0.1806\nEpoch 42/500\n6400/6400 [==============================] - 44s - loss: 2.6774 - acc: 0.1814 - val_loss: 2.6636 - val_acc: 0.1806\nEpoch 43/500\n6400/6400 [==============================] - 44s - loss: 2.6739 - acc: 0.1814 - val_loss: 2.6631 - val_acc: 0.1806\n', 'I have the same problem in a normal CNN using TensorFlow backend in keras. The loss frops to about 10^-7. Anybody an idea why this happens?', ""[accuracy_training_plot.pdf](https://github.com/fchollet/keras/files/857537/accuracy_training_plot.pdf)\r\n\r\nHas this problem any solution yet!? It quite frustrating to have the training go bananas all of a sudden. \r\nI have run other CNN architectures on the same data with no problems but maybe it is some problem with sending in multiple references to the same data as I do? (I haven't tried inputting copies instead, training takes a day.)\r\nHere is what I do: (I have only 2 classes but use categorical_crossentropy because I will have more later)\r\n\r\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n....\r\nhistory = model.fit([X_data_x,X_data_x,X_data_x, X_data_y,X_data_y,X_data_y], Y_data, validation_split=0.25, nb_epoch=epochs, batch_size=batch_size,callbacks=callbacks)"", 'It happens when somehow log returns NaN ', ""when I train model, using CNN with relu , acc and val_acc suddenly get zero.I'm clueless on what to do with this. It's bug? \r\n![screenshot from 2017-05-15 17-28-53](https://cloud.githubusercontent.com/assets/12960656/26051141/db8ed690-3993-11e7-8507-bd5786a53e68.png)\r\n"", ""Switched from theano to Tensorflow today and saw this kind of thing happen for the first time ever.\r\nTraining went down to the expected best loss (~0.3), accuracy of ~91%, for the task at hand and then proceeded to suddenly go up to 15+ with an accuracy of less than 2%. It then stayed there, did not bounce back. Something must be numerically instable? This is on a wide residual network, no RNNs involved.\r\nMy learning rate at this point was 0.00005 using Adam.\r\n\r\nMaybe this is what happened at least to me, and maybe some others in this thread: https://stackoverflow.com/a/42420014/5709630\r\n\r\nHowever it does seem... odd that I've trained this setup dozens of times using theano and never saw this happen and now, during the very first run with tensorflow it happens. Maybe something in the theano backend handles this better? \r\nOr it uses float64 in the place that matters, the reason I am trying tensorflow is because I could not get theano to stop doing that."", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', 'maybe because the relu,\r\n\r\nThe ReLU units can irreversibly die during training since they can get knocked off the data manifold. For example, you may find that as much as 40% of your network can be “dead” (i.e., neurons that never activate across the entire training dataset) if the learning rate is set too high. With a proper setting of the learning rate this is less frequently an issue. \r\nread more about it in here : http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html', ""@wildanputra I don't believe that the dead relus would cause the network to drop the accuracy so suddenly. Moreover, in our initial case, there were no relus. "", 'I am using Keras framework and this happened to me only when I use SGD or nadam optimizer. It only works if I use adam optimizer. My model is 3 layers of LSTMs each with 100 cells.', 'In my case, reducing the initial learning rate and adding decay helped.', 'This happened to me as well in two different RNN-based NLP tasks.\r\nOne is using LSTM to predict topic of document, another one is a bidirectional GRU-language model to predict next word.\r\n\r\nGRU activation `tanh`\r\nLSTM activation `tanh`\r\n\r\nOptimizer `adam` with configurations `lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0`\r\nLoss `categorical_cross_entropy` and `KL_divergence`\r\n\r\n\r\n![loss](https://user-images.githubusercontent.com/6015707/37241002-7ac83932-2407-11e8-85b4-9357e2971149.png)\r\n\r\nany clue on this?', ""Same here. NLP task with bidirectional LSTMs, using `rmsprop` and `categorical_crossentropy`.  ReLU units throughout.  Will try with `tanh` activations, but I've seen those are slow."", ""I also faced this issue multiple times and it seems to me this is linked to vanishing gradient/dead neuron issue.\r\nWhat helped (although it didn't solve the issue in all cases) was this:\r\n- replace ReLU with SeLU\r\n- check the input distribution and rescale it if it is too centered around 0 (you may want to add several standard deviations and/or multiply by a factor 2 or 3)"", 'Using `tanh` fixed the issue for me, without significant loss of performance.', 'For me, reducing the learning rate of ADAM from `lr=0.001` to `lr=0.0005` fixed it, using a single layer LSTM (ELU) for a classification problem.', '![Screenshot from 2021-01-09 19-28-01](https://user-images.githubusercontent.com/33377212/104090301-d3703500-52b0-11eb-80de-fcf3ed5bae31.png)\r\n I have a similar bug, any idea?']","['\nmodel = Sequential([\n    LSTM(1024, input_shape=encoder.shape, return_sequences=False),\n    Dense(w2v_size, activation=""linear"")\n])\n\nmodel.compile(optimizer=\'adam\', loss=""mse"", metrics=[""accuracy""])\n', '\nmodel = Sequential([\n    GRU(1024, input_shape=encoder.shape, return_sequences=False),\n    Dense(w2v_size, activation=""linear"")\n])\n\nmodel.compile(optimizer=\'adam\', loss=""mse"", metrics=[""accuracy""])\n', '\nmodel = Sequential([\n    LSTM(512, input_shape=encoder.shape, return_sequences=True),\n    TimeDistributed(Dense(512, activation=""sigmoid"")),\n    LSTM(512, return_sequences=False),\n    Dense(256, activation=""tanh""),\n    Dense(w2v_size, activation=""linear"")\n])\n\nmodel.compile(optimizer=\'adam\', loss=""mse"", metrics=[""accuracy""])\n']",['$football$$$$$$$$$$'],0,0
565,keras,2758,closed,What accuracy is reported on sequence output?,"Should be a fairly simple question:

I am training a sequence to sequence model, and I would like to have accuracy reported during training. Note that this score should measure exact accuracy, i.e. **all** output symbols need to be correct for one input sequence in order for the prediction to be correct. In other words, the metric should measure zero-one loss over all outputs together, rather than per individual output.

For instance:



Most letters are correct, but the prediction only counts if all of them are.

Currently, my last layers are:



and I compile using 



Is this correct, or will this report per-symbol accuracy? If so, how can I best implement the custom metric?
",,"[""Verified myself that it's the per-symbol accuracy. Getting full-sequence accuracy in this case can be done through something like \n\n```\npred = self.model.predict([X_s, X_q])\npred = np.eye(self.num_inputs)[np.argmax(pred, axis=2)]\nscore = np.apply_along_axis(\n            all, 1,  # all timesteps correct?\n            np.apply_along_axis(all, 2, pred == Y)  # prediction of timestep correct?\n        ).mean()\n```\n""]","['\ntarget: bcdefgh, output: bcdefha -> score 0\ntarget: abcdefg, output: abcdefg -> score 1\n\noverall accuracy: 50%\n', '\nmodel.add(LSTM(self.decoder_hidden_units, return_sequences=True))\nmodel.add(Dropout(0.3))\nmodel.add(TimeDistributed(Dense(self.num_inputs, activation=""softmax"")))\n', '\nself.model.compile(loss=\'categorical_crossentropy\', optimizer=RMSprop(lr=self.lr), metrics=[""accuracy""])\n']",[],0,0
566,keras,7146,closed,Do Merge layers need more documentations and example? ,"Related to #3921 and maybe more, I think we need a template for merge layer (this page)[https://keras.io/layers/merge/} (probably @fchollet should put some input on it?) and few examples that I can work in after understanding things clear.

1. Why documentation?
- Because we need to inform how to import merge layers. Otherwise,


- Also I think we need to make it clear that it's about dealing with Tensors and not layers which is already done in the docstrings but not really in the documentation -- it's outdated though. 

python
    tensor_a = Input(shape=(32,))
    tensor_b = Input(shape=(32,))
    merged_tensor = merge([tensor_a, tensor_b], mode='concat', concat_axis=1)
    concatdotcoscompute_output_shape

2. Examples
Just to show clearly that how to do it with 
- tensors within a simple functional model
- maybe how to use them with  models? 
",,[],"['python\r\n>>> import keras.layers.merge\r\n>>> merge.Add\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\nNameError: name \'merge\' is not defined\r\n\r\n>>> import keras.layers.merge.Add\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\nImportError: No module named Add\r\n\r\n# It should be like this\r\n>>> from keras.layers.merge import Add\r\n# and are we supposed to know this? maybe I\'m lack of something on Python conventions in general? Anyway it would help. \r\n', 'python\r\n>>> print keras.layers.merge.__doc__\r\nFunctional merge, to apply to Keras tensors (NOT layers).\r\n    Returns a Keras tensor.\r\n    # Example\r\n    ']","['', ""\r\n    # Arguments\r\n        mode: String or lambda/function. If string, must be one\r\n            of: 'sum', 'mul', 'concat', 'ave', 'cos', 'dot', 'max'.\r\n            If lambda/function, it should take as input a list of tensors\r\n            and return a single tensor.\r\n        concat_axis: Integer, axis to use in mode "", '.\r\n        dot_axes: Integer or tuple of integers,\r\n            axes to use in mode ', ' or ', '.\r\n        output_shape: Shape tuple (tuple of integers), or lambda/function\r\n            to compute output_shape (only if merge mode is a lambda/function).\r\n            If the latter case, it should take as input a list of shape tuples\r\n            (1:1 mapping to input tensors) and return a single shape tuple,\r\n            including the batch size\r\n            (same convention as the ', ' method of layers).\r\n        node_indices: Optional list of integers containing\r\n            the output node index for each input layer\r\n            (in case some input layers have multiple output nodes).\r\n            will default to an array of 0s if not provided.\r\n        tensor_indices: Optional list of indices of output tensors\r\n            to consider for merging\r\n            (in case some input layer node returns multiple tensors).\r\n', '', 'Sequential']",0,0
567,keras,536,closed,Learning rate and momentum as shared scalars,"I'd like to ask if we could make the optimizer's learning rate () and momentum () into shared scalars. This way we could change their values during training with  using custom rules.

I could work on the PR as long as you guys don't have a reason for not to.
",,"[""I would have issues with changing the definition of the learning rate. But if the PR is just about switching `lr` and `momentum` to shared scalars (hence modifiable after compilation) instead of scalars, without touching the logic, that's fine by me.\n"", 'This could be a very useful feature. Actually, I can wait to have it:)\n', 'So right now, the only way to change learning rate during training is 1) Save weights of the model. 2) Compile a new model with same architecture and different learning rate. 3) Load weights and continue training??\n', ""I think you can also create a shared scalar and pass it to the optimizer's initializer.\nI tried something like this for changing momentum from a callback.\nhttps://github.com/wuaalb/keras_extensions/blob/master/keras_extensions/callbacks.py\n"", ""The problem is, since you don't recompile the graph it won't use your new learning rate or momentum, even if you change it with a callback.\n"", ""Something like this doesn't work, or isn't what you're looking for?\n\n``` python\nmodel = ...\noptimizer = SGD(lr=shared_scalar(0.01))\nmodel.compile(optimizer, loss)\n...\noptimizer.lr.set_value(0.02) # e.g. from a callback during fit()\n```\n\nAnyways, making them shared scalars by default sounds good..\n"", ""That is exactly what I'm doing! What we were discussing was, should we make `lr` a `shared_scalar` by default or not? It seems that we don't have anything to lose with that. I'm also writing a callback that gets a dict of `epoch x lr` pairs and switch values on the go.\n"", ""> What we were discussing was, should we make lr a shared_scalar by default or not? It seems that we don't have anything to lose with that.\n\nIndeed. You would pass a float to the constructor and internally this float would be used as the initial value of a shared scalar. The logic of the optimizer itself would be unchanged.\n""]",[],"['lr', 'momentum', '.set_value']",0,0
568,keras,1524,closed,One class classification,"Hey there,

I have a binary cross-entropy problem that I would like to solve using one-class classification.

So to do this, I have the following model...



However, this is returning the following error.



Can anyone help?
",,['Problem fixed. Forgot to switch off to_categorical on my data loader(!)\n'],"['\n    model = Sequential()\n    model.add(Dense(256, input_shape=(1022, )))\n    model.add(Activation(""relu""))\n    model.add(Dropout(0.5))\n    model.add(Dense(128))\n    model.add(Activation(""relu""))\n    model.add(Dropout(0.5))\n    model.add(Dense(1))\n    model.add(Activation(""sigmoid""))\n\n    adam = Adam()\n    model.compile(loss=\'binary_crossentropy\', optimizer=adam, class_mode=""binary"")\n', ""\nValueError: Input dimension mis-match. (input[0].shape[1] = 2, input[1].shape[1] = 1)\nApply node that caused the error: Elemwise{Composite{EQ(i0, RoundHalfAwayFromZero(i1))}}(<TensorType(float32, matrix)>, Elemwise{Composite{scalar_sigmoid((i0 + i1))}}[(0, 0)].0)\nToposort index: 54\nInputs types: [TensorType(float32, matrix), TensorType(float32, matrix)]\nInputs shapes: [(8, 2), (8, 1)]\nInputs strides: [(8, 4), (4, 4)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[Sum{acc_dtype=int64}(Elemwise{Composite{EQ(i0, RoundHalfAwayFromZero(i1))}}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\n""]",[],0,0
569,keras,868,closed,Using custom loss function with model_from_json,"It doesn't work.



It is obviously trying to load the loss function from the relevant keras module, and not finding it.

How about adding some parameters to model.model_from_json? Something like  would be great.

More generally though, maybe we should add the possibility of giving a dictionary of named functions for all the custom functions the model might use: activation, loss, optimizer, ...
",,"[""> More generally though, maybe we should add the possibility of giving a dictionary of named functions for all the custom functions the model might use: activation, loss, optimizer, ...\n\nYes, that's a pertinent idea. We already have such support for custom layers. Let's see if we can extend it the custom loss functions and optimizers in the simplest possible way. Is this something you would like to take on?\n"", ""Looking at the code that manages the custom layers...\nhttps://github.com/fchollet/keras/blob/master/keras/utils/layer_utils.py#L130\n\n``` python\ndef get_layer(identifier, kwargs=None, custom_layers={}):\n    # Insert custom layers into globals so they can be accessed by `get_from_module`.\n    for cls_key in custom_layers:\n        globals()[cls_key] = custom_layers[cls_key]\n    return get_from_module(identifier, globals(), 'layer', instantiate=True, kwargs=kwargs)\n```\n\nIt just adds the whole dictionary into the global scope.\nThat's kind of cheating, if you don't mind me saying so, but it is neat and easy.\n\nI _think_ that adding whatever I want to `custom_layers` when I call `model_from_json` would cover most cases, exceptions being custom containers and custom functions given to containers.\n\nMy suggestion would be to rename `custom_layers` to `custom_elements`, and move the code that adds stuff to the global scope to the beginning of [`container_from_config`](https://github.com/fchollet/keras/blob/master/keras/utils/layer_utils.py#L20)\n\nHow does that sound?\n"", '> How does that sound?\n\nSounds great, it\'s the simplest thing we can do. Not a big fan of ""custom_elements"" as an API, maybe ""custom_objects"" would be better? \n', 'I second that request. Having support for custom loss functions would be gr8.\n', 'It is a pleasure when contributing is so easy.\n']","['\nTraceback (most recent call last):\n  File ""myFile.py"", line 100, in <module>\n    model = model_from_json(open(\'savefile.json\').read())\n  File ""build/bdist.linux-x86_64/egg/keras/models.py"", line 116, in model_from_json\n  File ""build/bdist.linux-x86_64/egg/keras/models.py"", line 142, in model_from_config\n  File ""build/bdist.linux-x86_64/egg/keras/models.py"", line 346, in compile\n  File ""build/bdist.linux-x86_64/egg/keras/objectives.py"", line 64, in get\n  File ""build/bdist.linux-x86_64/egg/keras/utils/generic_utils.py"", line 12, in get_from_module\nException: Invalid objective: custom_loss_function\n']","['model.model_from_json(""savefile.json"", loss=my_custom_loss_function)']",0,0
570,keras,11757,closed,keras/examples/tensorboard_embeddings_mnist.py gives an ,"- [ yes] Check that you are up-to-date with the master branch of Keras. You can update with:


- [ yes] Check that your version of TensorFlow is up-to-date. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [https://github.com/keras-team/keras/blob/master/examples/tensorboard_embeddings_mnist.py] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I am using Anaconda python(3.6) for running the example code for tensorboard embedding in mnist. After training for an epoch, when the call back is called, it gives the following error. 


Epoch 1/12
60000/60000 [==============================] - 8s 139us/step - loss: 0.2665 - acc: 0.9170 - val_loss: 0.0716 - val_acc: 0.9774
2018-11-29 13:04:24.254467: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:137 : Unknown: Failed to rename: ./logs\keras_embedding.ckpt-0.data-00000-of-00001.tempstate7943206387758954579 to: ./logs\keras_embedding.ckpt-0.data-00000-of-00001 : Access is denied.
; Input/output error
Traceback (most recent call last):
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\client\session.py"", line 1334, in _do_call
    return fn(*args)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\client\session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\client\session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.UnknownError: Failed to rename: ./logs\keras_embedding.ckpt-0.data-00000-of-00001.tempstate7943206387758954579 to: ./logs\keras_embedding.ckpt-0.data-00000-of-00001 : Access is denied.
; Input/output error
         [[{{node save/SaveV2}} = SaveV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, features_embedding/_137)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""embedding.py"", line 88, in <module>
    validation_data=(x_test, y_test))
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\engine\training.py"", line 1039, in fit
    validation_steps=validation_steps)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\engine\training_arrays.py"", line 217, in fit_loop
    callbacks.on_epoch_end(epoch, epoch_logs)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\callbacks.py"", line 79, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\callbacks.py"", line 981, in on_epoch_end
    epoch)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\training\saver.py"", line 1441, in save
    {self.saver_def.filename_tensor_name: checkpoint_file})
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\client\session.py"", line 929, in run
    run_metadata_ptr)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\client\session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\client\session.py"", line 1328, in _do_run
    run_metadata)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\client\session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnknownError: Failed to rename: ./logs\keras_embedding.ckpt-0.data-00000-of-00001.tempstate7943206387758954579 to: ./logs\keras_embedding.ckpt-0.data-00000-of-00001 : Access is denied.
; Input/output error
         [[node save/SaveV2 (defined at C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\callbacks.py:887)  = SaveV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, features_embedding/_137)]]


Caused by op 'save/SaveV2', defined at:
  File ""embedding.py"", line 88, in <module>
    validation_data=(x_test, y_test))
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\engine\training.py"", line 1039, in fit
    validation_steps=validation_steps)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\engine\training_arrays.py"", line 117, in fit_loop
    callbacks.set_model(callback_model)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\callbacks.py"", line 54, in set_model
    callback.set_model(model)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\callbacks.py"", line 887, in set_model
    self.saver = tf.train.Saver(list(embeddings_vars.values()))
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\training\saver.py"", line 1102, in __init__
    self.build()
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\training\saver.py"", line 1114, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\training\saver.py"", line 1151, in _build
    build_save=build_save, build_restore=build_restore)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\training\saver.py"", line 792, in _build_internal
    save_tensor = self._AddSaveOps(filename_tensor, saveables)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\training\saver.py"", line 284, in _AddSaveOps
    save = self.save_op(filename_tensor, saveables)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\training\saver.py"", line 202, in save_op
    tensors)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\ops\gen_io_ops.py"", line 1690, in save_v2
    shape_and_slices=shape_and_slices, tensors=tensors, name=name)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\util\deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\framework\ops.py"", line 3274, in create_op
    op_def=op_def)
  File ""C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\tensorflow\python\framework\ops.py"", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

UnknownError (see above for traceback): Failed to rename: ./logs\keras_embedding.ckpt-0.data-00000-of-00001.tempstate7943206387758954579 to: ./logs\keras_embedding.ckpt-0.data-00000-of-00001 : Access is denied.
; Input/output error
         [[node save/SaveV2 (defined at C:\Users\abalu\AppData\Local\Continuum\anaconda3\envs\keras\lib\site-packages\keras\callbacks.py:887)  = SaveV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, features_embedding/_137)]]


I searched for the possible reasons for this to happen, I found the following few issues/discussion:
https://github.com/balancap/SSD-Tensorflow/issues/72
https://stackoverflow.com/questions/43644893/windows-tensorflow-could-not-restore-checkpoint-access-is-denied

my understanding is that, this could be something to do with the behavior in windows. I dont know what the exact issue is, I would like to contribute if someone could let me know what might be the issue. Thanks

",,"[""This is an access denied error. This just means that you don't have the windows permissions to do something, like create or rename a file. This is unrelated to keras or tensorflow."", 'But I do have write access to that folder... Something else seems to be the problem']",[],['pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps'],0,0
571,keras,11600,closed,Add Trainable Swish Layer to Keras,"I use a custom layer for a trainable Swish Activation function : https://arxiv.org/abs/1710.05941. It can be set to non-trainable with any beta value, so it's quite general. I'd like to add it to Keras so that it can be part of the next release. It's quite popular. Or I can add it to keras contrib. 

Where should I add the my code for Swish before making a pull request?",,"[""Please see the guide: https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md#pull-requests. Thanks!\r\n\r\nAlso you can check if somebody didn't already try to make a PR for this layer and was closed. https://github.com/keras-team/keras/pulls?q=is%3Apr+swish+is%3Aclosed . Maybe you can try again. @fchollet  will decide if it goes in keras or not.\r\n\r\nJust know that in general, you can make a library out of your layer on your repo, so that users can do `pip install swish` (for example). And it should be as easy to use as if it was in core keras. Just the import will differ. `from swish import Swish` instead of `from keras.layers import Swish`.""]",[],[],0,0
572,keras,6657,closed,datagen.flow question,"I using Keras 2.x ‘tf’ seeting.
Why I can’t using
X_batch, y_batch = datagen.flow(train, train, batch_size=32)
For example :

# Code
from keras.datasets import mnist
from keras.preprocessing.image import ImageDataGenerator

(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train = X_train.reshape(X_train.shape[0], 28, 28,1)
X_test = X_test.reshape(X_test.shape[0], 28, 28,1)
X_train = X_train.astype(‘float32’)
X_test = X_test.astype(‘float32’)
datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)
datagen.fit(X_train)
X_batch, y_batch = datagen.flow(X_train, y_train, batch_size=9)

#Question 
Can anyone tell me why?
Thanks!",stale,"['datagen.flow() returns a generator. To obtain the batch, use:\r\nX_batch, y_batch = datagen.flow(X_train, y_train, batch_size=9).next()', '@sabersword \r\nThanks a lot! ', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],0,0
573,keras,7510,closed,'Keyword argument not understood' error when importing a Keras model from json,"When trying to import a model exported with  using , I get the following error:
 
     File ""Q:\Anaconda2\lib\site-packages\keras\models.py"", line 325, in model_from_json
      return layer_module.deserialize(config, custom_objects=custom_objects)
    File ""Q:\Anaconda2\lib\site-packages\keras\layers\__init__.py"", line 46, in deserialize
      printable_module_name='layer')
    File ""Q:\Anaconda2\lib\site-packages\keras\utils\generic_utils.py"", line 140, in deserialize_keras_object
      list(custom_objects.items())))
    File ""Q:\Anaconda2\lib\site-packages\keras\engine\topology.py"", line 2374, in from_config
      process_layer(layer_data)
    File ""Q:\Anaconda2\lib\site-packages\keras\engine\topology.py"", line 2343, in process_layer
      custom_objects=custom_objects)
    File ""Q:\Anaconda2\lib\site-packages\keras\layers\__init__.py"", line 46, in deserialize
      printable_module_name='layer')
    File ""Q:\Anaconda2\lib\site-packages\keras\utils\generic_utils.py"", line 141, in deserialize_keras_object
      return cls.from_config(config['config'])
    File ""Q:\Anaconda2\lib\site-packages\keras\engine\topology.py"", line 1206, in from_config
      return cls(**config)
    File ""Q:\Anaconda2\lib\site-packages\keras\legacy\interfaces.py"", line 88, in wrapper
      return func(*args, **kwargs)
    File ""Q:\Anaconda2\lib\site-packages\keras\layers\recurrent.py"", line 931, in __init__
      super(LSTM, self).__init__(**kwargs)
    File ""Q:\Anaconda2\lib\site-packages\keras\layers\recurrent.py"", line 181, in __init__
      super(Recurrent, self).__init__(**kwargs)
    File ""Q:\Anaconda2\lib\site-packages\keras\engine\topology.py"", line 277, in __init__
      raise TypeError('Keyword argument not understood:', kwarg)
    TypeError: ('Keyword argument not understood:', u'return_state')

The problem occurs when trying to import an LSTM layer: in ,  is called with the keyword args; however,  throws an exception if the keyword is not in the  list. However,  is an allowed kwarg of  (and ) as per the [documentation](https://keras.io/layers/recurrent/). The model was json-serialized using the same version of Keras as when I try to load it. Is this desired behavior? One might expect that when a model can be serialized, it should also be deserializable using the same version of Keras.

The JSON description of the model can be found in [this gist](https://gist.github.com/benjaminalt/04f00629cf52b735414d700e2352930c).

Here is my setup:
OS: 64-bit Windows 10
Python: Python 2.7.13 (Anaconda 4.4.0 64-bit)
Keras: 2.0.6
Theano: 0.9.0
",,"['Found the error: Serialization happened in a different Anaconda distribution. Using the same Anaconda distribution fixes the error.', '@benjaminalt, by anaconda distribution, do you mean environment?', '@Flock1 I meant Python 2.7 Anaconda v. Python 3.6 Anaconda.']",[],"['to_json()', 'keras.models.model_from_json(...)', 'keras.layers.recurrent.py', 'super.__init__', 'keras.engine.topology.Layer.__init__', 'allowed_kwargs', 'return_state', 'LSTM', 'Recurrent']",0,0
574,keras,1490,closed,AttributeError: 'module' object has no attribute 'relu',"Keras and Theano are all newest. I have used sudo pip install git+git://github.com/Theano/Theano.git. How can I solve this problem? Many thanks!
",,"['@fchollet  If you have time, could you help me solve this problem? Thanks\n', 'You might have an old version of Theano floating around that is getting priority during import time. Not really a Keras issue...\n', '@lukedeo The editions of these tools are Theano-0.8.0 and Keras-0.3.1. If using theano only, there are not any problems. However, running imdb_cnn.py yielded this problem. Are there some suggestions? Thanks a lot\n![1](https://cloud.githubusercontent.com/assets/13865086/12412839/20d4b936-bec5-11e5-928c-c25a52845550.png)\n', 'You need to get Theano from the git repo, not pip for the theano.nnet functions to be available.\n', '@around1991  I indeed get Theano from the git repo by sudo pip install git+git://github.com/Theano/Theano.git\n', ""Maybe it's because you are installing theano with pip pointing to another version of python. If sudo pip and pip are linked with two different version it might be the problem. Why are you using sudo?\n"", ""Uninstall Theano multiple time. You probably have an old version somewhere\nthat is causing conflict. Then make sure you can't import theano. Then\nreinstall Theano.\n\nUsing pip with the git path install the development version.\n\nOn Tue, Jan 19, 2016 at 7:48 AM, Thomas Boquet notifications@github.com\nwrote:\n\n> Maybe it's because you are installing theano with pip pointing to another\n> version of python. If sudo pip and pip are linked with two different\n> version it might be the problem. Why are you using sudo?\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/fchollet/keras/issues/1490#issuecomment-172843525.\n""]",[],[],0,0
575,keras,8298,closed,index_generator:,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",,[],[],[],0,0
576,keras,2003,closed,Loss increasing after each epoch.,"I'm using Keras to build and train a recurrent neural network.



 is an an array of sequences of [latitude, longitude, temperature], all padded to the same length with values of [0,0,0]. When I train this network, the first epoch gives me a loss of about 63, and _increases_ after more epochs.
This is causing a  call later in the code to give values that are completely off of the training values. For example, most of the training values in each sequence is around , but the RNN outputs values consistently around , which causes me to think something is wrong with the masking layer. 

The training X () data looks something like this (only much larger): 



and the training Y () data looks like this:


",stale,"['may you share data and source coded to try\n', '> and the training Y (`training_final_steps`) data looks like this:\n> \n> ```\n> [\n> [44.652, 39.649], [37.362, 54.106], [37.115, 57.66501]\n> ]\n> ```\n\n`softmax` outputs values in the range `[0, 1]` that always sum to 1 (i.e. a probability distribution). Your targets have to be just like that.\n', '@NasenSpray @Sandy4321 @fchollet https://github.com/jeshaitan/migration-lstm\n']","['\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation, Masking\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers.normalization import BatchNormalization\n\n#build and train model\nin_dimension = 3\nhidden_neurons = 300\nout_dimension = 2\n\nmodel = Sequential()\nmodel.add(BatchNormalization(input_shape=((max_sequence_length, in_dimension))))\nmodel.add(Masking([0,0,0], input_shape=(max_sequence_length, in_dimension)))\nmodel.add(LSTM(hidden_neurons, activation=\'softmax\', return_sequences=False))\nmodel.add(Dense(out_dimension, activation=\'linear\'))\n\nmodel.compile(loss=""mse"", optimizer=""sgd"")\nmodel.fit(padded_training_seqs, training_final_steps, nb_epoch=5, batch_size=1)\n', '\n[\n[[43.103, 27.092, 19.078], [43.496, 26.746, 19.198], [43.487, 27.363, 19.092], [44.107, 27.779, 18.487], [44.529, 27.888, 17.768]], \n[[44.538, 27.901, 17.756], [44.663, 28.073, 17.524], [44.623, 27.83, 17.401], [44.68, 28.034, 17.601], [0,0,0]],\n[[47.236, 31.43, 13.905], [47.378, 31.148, 13.562], [0,0,0], [0,0,0], [0,0,0]]\n]\n', '\n[\n[44.652, 39.649], [37.362, 54.106], [37.115, 57.66501]\n]\n']","['padded_training_seqs', 'model.predict', '[40, 40, 20]', '[0.4, 0.5]', 'padded_training_seqs', 'training_final_steps']",0,0
577,keras,303,closed,Omit input dimensions (for hidden layers) and auto-calculate if not specified,"Most of the time when I have a bug it's because I haven't correctly figured out the input dimensionality for some hidden layer that's layered on top of a convolutional or max pooling layer. Can we have the code auto-compute the input dimensionality of non-input layers for ease of use? This would have saved me hours initially, as is how Passage works (you just specify the output dimensionality).
",stale,"[""We're thinking about it. It would require significant architecture changes, but it is doable.\n"", ""Awesome. On the flip side, it's forced me to better understand these operations :)\n"", 'To be honest I like having to specify both dimensions, because it helps you think about what the layer is doing (projections from space A to space B). It\'s the ""functional"" way to think about NNs, in terms of inputs and outputs, rather than in terms of hidden units.\n\nHowever it is true that when it comes to convolutions and max pooling, computing the right values by hand can be a pain.\n', ""I think it would help newer users. Deep learning is complicated, I frequently recommend keras as it's easy to use yet fully featured (and seems to work well!). I think that would help further in terms of approachability.\n"", ""Getting latest theano fixed the concat issue. Many thanks! I think you should maybe add a comment in the code or docs, as most people will pip install Theano, and who knows when it'll be fixed.\n"", ""Same - I was about to post an issue asking for help figuring out how to calculate the sizes when stacking convolutional layers. (I'm failing at calcing them)\n"", '@ddofer Please refer this excellent source: http://cs231n.github.io/convolutional-networks/\n', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],0,0
578,keras,579,closed,"Can CNN  and LSTM classify muti-categories texts, how to modify the code? Thanks!",,,"['For eample, imdb_lstm_text_2_classification.\n\n```\nprint(\'Build model...\')\nmodel = Sequential()\nmodel.add(Embedding(max_features, 128))\nmodel.add(LSTM(128, 128))  # try using a GRU instead, for fun\nmodel.add(Dropout(0.5))\nmodel.add(Dense(128, 1))\nmodel.add(Activation(\'sigmoid\'))\n\n# try using different optimizers and different optimizer configs\nmodel.compile(loss=\'binary_crossentropy\', optimizer=\'adam\', class_mode=""binary"")\n\nprint(""Train..."")\nmodel.fit(X_train, y_train, batch_size=batch_size, nb_epoch=4, validation_data=(X_test, y_test), show_accuracy=True)\nscore, acc = model.evaluate(X_test, y_test, batch_size=batch_size, show_accuracy=True)\nprint(\'Test score:\', score)\nprint(\'Test accuracy:\', acc)\n```\n', 'Note that the last layer of this model is `Dense(128, 1)`. Thus it has only one output. If you need multiple outputs use `Dense(128, number_of_classes)` and use a different desired `y_test`. Also the new cost function will be `categorical_crossentropy` if only one class can be active at a time check this example out https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py#L46\n', ""One more thing: you will need to set `class_mode='categorical'` in `compile`. It's the default, so you could also just remove `class_mode='binary'`.\n"", '@fchollet  After removing class_mode=\'binary\', the accuracy becomes 1.0000. Why? I diin\'t modify other codes. \nMy whole codes as following: modification from imdb_cnn\nI want to classify 6 categories. X has 6 categories texts, and y is the label\n\n``` python\nX_train = X[:int(len(X)*(1-test_split))]\ny_train = labels[:int(len(X)*(1-test_split))]\n\nX_test = X[int(len(X)*(1-test_split)):]\ny_test = labels[int(len(X)*(1-test_split)):]\n\nprint(len(X_train), \'train sequences\')\nprint(len(X_test), \'test sequences\')\n\n# maxlen = 100\nprint(""Pad sequences (samples x time)"")\nX_train = sequence.pad_sequences(X_train, maxlen=maxlen)\nX_test = sequence.pad_sequences(X_test, maxlen=maxlen)\nprint(\'X_train shape:\', X_train.shape)\nprint(\'X_test shape:\', X_test.shape)\n\nprint(\'Build model...\')\nmodel = Sequential()\n\n# we start off with an efficient embedding layer which maps\n# our vocab indices into embedding_dims dimensions\nmodel.add(Embedding(max_features, embedding_dims))\nmodel.add(Dropout(0.25))\n\n# we add a Convolution1D, which will learn nb_filters\n# word group filters of size filter_length:\nmodel.add(Convolution1D(input_dim=embedding_dims,\n                        nb_filter=nb_filters,\n                        filter_length=filter_length,\n                        border_mode=""valid"",\n                        activation=""relu"",\n                        subsample_length=1))\n\n# we use standard max pooling (halving the output of the previous layer):\nmodel.add(MaxPooling1D(pool_length=2))\n\n# We flatten the output of the conv layer, so that we can add a vanilla dense layer:\nmodel.add(Flatten())\n\n# Computing the output shape of a conv layer can be tricky;\n# for a good tutorial, see: http://cs231n.github.io/convolutional-networks/\noutput_size = nb_filters * (((maxlen - filter_length) / 1) + 1) / 2\n\n# We add a vanilla hidden layer:\nmodel.add(Dense(output_size, hidden_dims))\nmodel.add(Dropout(0.25))\nmodel.add(Activation(\'relu\'))\n\n# We project onto a single unit output layer, and squash it with a sigmoid:\nmodel.add(Dense(hidden_dims, 1))\nmodel.add(Activation(\'sigmoid\'))\n\nmodel.compile(loss=\'categorical_crossentropy\', optimizer=\'rmsprop\')\nmodel.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch,show_accuracy=True, validation_data=(X_test, y_test))\nscore, acc = model.evaluate(X_test, y_test, batch_size=batch_size, show_accuracy=True)\nprint(\'Test score:\', score)\nprint(\'Test accuracy:\', acc)\n```\n\noutput:\nLoading data...\nmax_features23823\nlen(words)23820\n9600 train sequences\n2400 test sequences\nPad sequences (samples x time)\nX_train shape: (9600L, 200L)\nX_test shape: (2400L, 200L)\nBuild model...\nTrain on 9600 samples, validate on 2400 samples\nEpoch 0\n\n  32/9600 [..............................] - ETA: 1900s - loss: 0.6991 - acc: 1.0000\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n  64/9600 [..............................] - ETA: 1900s - loss: -0.4477 - acc: 1.0000\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\x08\n  96/9600 [..............................] - ETA: 1898s - loss: -4.1642 - acc: 1.0000\n', '@EderSantana  I modified the code as you say.\n\nmodel.add(Dense(hidden_dims, 6))\nmodel.add(Activation(\'softmax\'))\n\nmodel.compile(loss=\'categorical_crossentropy\', optimizer=\'rmsprop\')\n\nbut:\nTraceback (most recent call last):\n  File ""D:\\workspace\\search\\src\\CNN\\text_classification_test.py"", line 293, in <module>\n    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch,show_accuracy=True, validation_data=(X_test, y_test))\n  File ""D:\\Python27\\lib\\site-packages\\keras\\models.py"", line 413, in fit\n    validation_split=validation_split, val_f=val_f, val_ins=val_ins, shuffle=shuffle, metrics=metrics)\n  File ""D:\\Python27\\lib\\site-packages\\keras\\models.py"", line 168, in _fit\n    outs = f(*ins_batch)\n  File ""D:\\Python27\\lib\\site-packages\\theano\\compile\\function_module.py"", line 606, in __call__\n    storage_map=self.fn.storage_map)\n  File ""D:\\Python27\\lib\\site-packages\\theano\\compile\\function_module.py"", line 595, in **call**\n    outputs = self.fn()\nValueError: Input dimension mis-match. (input[0].shape[1] = 1, input[1].shape[1] = 6)\nApply node that caused the error: Elemwise{Composite{(i0 \\* log((i1 / i2)))}}(AdvancedSubtensor1.0, Elemwise{clip,no_inplace}.0, InplaceDimShuffle{0,x}.0)\nInputs types: [TensorType(float64, matrix), TensorType(float64, matrix), TensorType(float64, col)]\nInputs shapes: [(32L, 1L), (32L, 6L), (32L, 1L)]\nInputs strides: [(8L, 8L), (48L, 8L), (8L, 8L)]\nInputs values: [\'not shown\', \'not shown\', \'not shown\']\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag \'optimizer=fast_compile\'. If that does not work, Theano optimizations can be disabled with \'optimizer=None\'.\nHINT: Use the Theano flag \'exception_verbosity=high\' for a debugprint and storage map footprint of this apply node.\n', ""@Imorton-zd, see the value error:\n`ValueError: Input dimension mis-match. (input[0].shape[1] = 1, input[1].shape[1] = 6)`\nYou are probably still using a desired signal `y` with only one class (last dimension equal to 1).\n\nBtw, just a little note, if posting python code, please consider using  surrounding it with: \n```python  \n`...code goes inside...`\n```\nIt will give it a nice syntax highlighting and won't mess up with the # signs.\n"", 'Thanks for your help! @EderSantana I can classify multi-category texts. However, a new problem came. The accuracy of train samples is good as I expect, but The accuracy of test samples is very low, as following：\n9600/9600 [==============================] - 2098s - loss: 0.5812 - acc: 0.7870 - val_loss: 1.4665 - val_acc: 0.5363\n\nmy complete code（omiting data reading）:\n\n``` python\nbatch_size = 32\nembedding_dims = 100\nnb_filters = 250\nfilter_length = 3\nhidden_dims = 250\nnb_epoch = 10\ntest_split=0.2\nseed=113\nnb_words=None\nskip_top=0\nmaxlen=200\nstart_char=1 \noov_char=2\nindex_from=3\nnb_classes = 6\n# convert class vectors to binary class matrices\nY_train = np_utils.to_categorical(y_train, nb_classes)\nY_test = np_utils.to_categorical(y_test, nb_classes)\n\nprint(len(X_train), \'train sequences\')\nprint(len(X_test), \'test sequences\')\n\n# maxlen = 100\nprint(""Pad sequences (samples x time)"")\nX_train = sequence.pad_sequences(X_train, maxlen=maxlen)\nX_test = sequence.pad_sequences(X_test, maxlen=maxlen)\nprint(\'X_train shape:\', X_train.shape)\nprint(\'X_test shape:\', X_test.shape)\n\nprint(\'Build model...\')\nmodel = Sequential()\n\n# we start off with an efficient embedding layer which maps\n# our vocab indices into embedding_dims dimensions\nmodel.add(Embedding(max_features, embedding_dims))\nmodel.add(Dropout(0.25))\n\n# we add a Convolution1D, which will learn nb_filters\n# word group filters of size filter_length:\nmodel.add(Convolution1D(input_dim=embedding_dims,\n                        nb_filter=nb_filters,\n                        filter_length=filter_length,\n                        border_mode=""valid"",\n                        activation=""relu"",\n                        subsample_length=1))\n\n# we use standard max pooling (halving the output of the previous layer):\nmodel.add(MaxPooling1D(pool_length=2))\n\n# We flatten the output of the conv layer, so that we can add a vanilla dense layer:\nmodel.add(Flatten())\n\n# Computing the output shape of a conv layer can be tricky;\n# for a good tutorial, see: http://cs231n.github.io/convolutional-networks/\noutput_size = nb_filters * (((maxlen - filter_length) / 1) + 1) / 2\n\n# We add a vanilla hidden layer:\nmodel.add(Dense(output_size, hidden_dims))\nmodel.add(Dropout(0.25))\nmodel.add(Activation(\'relu\'))\n\n# We project onto a single unit output layer, and squash it with a sigmoid:\nmodel.add(Dense(hidden_dims, nb_classes))\nmodel.add(Activation(\'softmax\'))\n\n# model.add(Dense(128, 1, init=\'normal\'))\n# model.add(Activation(\'relu\'))\n\n# sgd = SGD(l2=0.0,lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n# model.compile(loss=\'categorical_crossentropy\', optimizer=sgd, class_mode=""categorical"")\n# model.compile(loss=\'categorical_crossentropy\', optimizer=\'sgd\', class_mode=""categorical"")\nmodel.compile(loss=\'categorical_crossentropy\', optimizer=\'rmsprop\')\nmodel.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,show_accuracy=True, validation_data=(X_test, Y_test))\nscore, acc = model.evaluate(X_test, Y_test, batch_size=batch_size, show_accuracy=True)\nprint(\'Test score:\', score)\nprint(\'Test accuracy:\', acc)\n```\n\nHow to set the parameters? My computer memory is 8G, if using too many samples or other ways occupying most memory, I\'m afraid my computer will carsh. How can I do? Thanks\n', ""How big is your dataset? 250 filters seems to be a lot, you may be overfitting the training data, in that case you will need a bigger dataset or use a smaller net. If you don't have enough labeled data for you problem, maybe some unsupervised training in a larger dataset should help.\n\nThe following video class should give you some insights on that https://www.youtube.com/watch?v=jCGplSKrl2Y\n"", ""@EderSantana My dataset is 12000, 9600 for train, 2400 for test, classifying to 6 classes. If using a smaller net, is it ok to reduce some filters  only? At first, I want to add a layer, but I don't kown if it is effective and what layer I can add?\n"", ""well, welcome to neural networks\nthe only way to answer these questions is with experience\nyou will have to run as much experiments as you can and build your intuition with the results. It also helps to read papers and watch video lectures (did you see Hinton's Coursera and De Freitas youtube videos?). Try starting with an architecture used in a similar problem, with a similar dataset size.\n\nBut yeah, your dataset is small, to overfit it is the first step, now you have to regularize your model so it can generalize well. You should try less filters and maybe a dropout of .5. Instead of a single 250 filter layer, try two layers with 20 for example.\n\nGood luck!\n"", ""@EderSantana the Hinton's Coursera is actualy indigestible. For cnn doing texts classification, how many samples of one category are suitable? I have read the keras documentation, in which the input of convolutional layer has the 'nb_samples', not 'embedding_dims' in example of cnn. Why? And, I want add another convolutional layer using Convolution1D  after the first convolutional layer and maxpooling layer expecting higher accuracy. According to the cnn example above, what code do I need to add? Many thanks.\nBy the way, why does the 'output_size' calculate like this in the code above? If adding another convolutional layer and maxpooling layer, what the 'output_size' should be?\n"", 'Hi @Imorton-zd\n\nWhat is the shape of X_train and X_test in your code?\n\nMany thanks,\n']",[],[],0,0
579,keras,2943,closed,Typo,,,[],[],[],0,0
580,keras,197,closed,Add a character-based RNN example.,"@karpathy's [character-based RNN in Torch](https://github.com/karpathy/char-rnn) has gotten a great deal of attention recently after [his blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) entitled ""The Unreasonable Effectiveness of Recurrent Neural Networks"".

It would be nice if there was an example of doing the same thing in Keras.
",,"[""Completely agreed. It's possible in Keras but relatively inefficient due to the way our LSTM/GRU layers work right now. \n\nIt's on the backlog.\n"", '@fchollet Can you explain a bit more as to why it would be inefficient ? I am currently attempting exactly this thing ?( in keras! ) - so your pointers would be helpful. \n', ""The recurrent layers in Keras have to process every sample from its first time step to the last. These layers are stateless (memory is cleared after every sample).\n\nSo if you've used Keras to generate samples from t=0 to t=n, in order to generate the sample at t=n+1 you will have to re-input samples 0..n. You will do n steps, whereas if you conserved the state of the memory you would only have to do one step.\n\nIt doesn't change the overall performance, it's just slower. \n\nWe will add stateful RNNs soon to solve this.\n"", ""@ganarajpr I would like to experiment with this in Keras to, but I can't get my model to work. Would you be so kind to share an example based on your work?\n"", 'Here is a code sample.  This code divide a long character string to chunks of 200 characters, and it learns a model for the next character given the previous ones. At the end it inefficiently generates 128 sentences, each of 200 chars.\n\n```\nimport numpy\nimport sys\nsys.path.append(\'/home/USER/python/keras/\')\n\n# Obtain the corpus of character sequence to train from.\n# Here it is just the sequence 123456789 repeated 100000 times.\nx = ""123456789""*100000\n\n# Construct a dictionary, and the reverse dictionary for the participating chars.\n# \'*"" is a \'start-sequence\' character.\ndct = [\'*\'] + list(set(x))\nmax_features = len(dct)\nrev_dct = [(j, i) for i, j in enumerate(dct)]\nrev_dct = dict(rev_dct)\n\n# Convert the characters to their dct indexes. \nx = [rev_dct[ch] for ch in x]\n\n# Divide the corpuse to substrings of length 200.\nn_timestamps = 200\nx = x[:len(x)- len(x) % n_timestamps]\nx = numpy.array(x, dtype=\'int32\').reshape((-1, n_timestamps))\n\n# Generate input and ouput per substring, as an indicator matrix.\ny = numpy.zeros((x.shape[0], x.shape[1], max_features), dtype=\'int32\')\nfor i in numpy.arange(x.shape[0]):\n    for j in numpy.arange(x.shape[1]):\n        y[i, j, x[i, j]] = 1        \n\n# Shift-1 the input sequences to the right, and make them start with \'*\'.\nx = numpy.roll(y, 1, axis=1)\nx[:, 0, :] = 0\nx[:, 0, 0] = 1\n\n# Build the model.\nfrom keras.models import Sequential\nfrom keras.layers.core import TimeDistributedDense, Dropout, Activation\nfrom keras.layers.recurrent import LSTM\n\nmodel = Sequential()\nmodel.add(LSTM(max_features, 256, return_sequences=True))\nmodel.add(LSTM(256, 256, return_sequences=True))\nmodel.add(LSTM(256, 256, return_sequences=True))\nmodel.add(TimeDistributedDense(256, max_features))\nmodel.add(Activation(\'time_distributed_softmax\'))\n\nmodel.compile(loss=\'categorical_crossentropy\', optimizer=\'rmsprop\')\n\nmodel.fit(x, y, batch_size=64, nb_epoch=50)\n\n# Sample 128 sentences (200 characters each) from model.\n\ndef mnrnd(probs):\n    rnd = numpy.random.random()\n    for i in xrange(len(probs)):\n        rnd -= probs[i]\n        if rnd <= 0:\n            return i\n    return i\n\nsentences = numpy.zeros((128, n_timestamps+1, max_features))\nsentences[:, 0, 0] = 1\n\n# Start sampling char-sequences. At each iteration i the probability over\n# the i-th character of each sequences is computed. \nfor i in numpy.arange(n_timestamps):\n    probs = model.predict_proba(sentences)[:,i,:]\n    # Go over each sequence and sample the i-th character.\n    for j in numpy.arange(len(sentences)):\n        sentences[j, i+1, mnrnd(probs[j, :])] = 1\nsentences = [sentence[1:].nonzero()[1] for sentence in sentences]\n\n# Convert to readable text.\ntext = []\nfor sentence in sentences:\n    text.append(\'\'.join([dct[word] for word in sentence]))\n```\n', '@jonilaserson thanks for the example! \n', ""Thank you for sharing @jonilaserson ! I'll try to contribute an example too if I manage to get something interesting working.\n"", 'Due to the simple nature of the data, the example could be greatly simplified (a single LSTM instead of 3 stacked LSTM, 128 hidden dimensions instead of 256, 5 epochs instead of 50). \n\nHere\'s a generated sequence:\n\n```\n95346779123456789123456789123456789123456789123456789123456789123456789123456789123456789123456889123456789123456789123456789123456789123456789123456789123456789123456789123456789123456789123456789123455678123456789123456789123456\n```\n\nRemarkably the network starts being ""lost"" in the first few characters, then gets back on its feet as soon as it finds a 1. \n', 'I fed the Hebrew Bible to the 3-layer LSTM network and generated some text,\nit was a lot of fun.\n\nOn Sun, Jun 14, 2015, 23:57 François Chollet notifications@github.com\nwrote:\n\n> Due to the simple nature of the data, the example could be greatly\n> simplified (a single LSTM instead of 3 stacked LSTM, 128 hidden dimensions\n> instead of 256, 5 epochs instead of 50).\n> \n> Here\'s a generated sequence:\n> \n> 95346779123456789123456789123456789123456789123456789123456789123456789123456789123456789123456889123456789123456789123456789123456789123456789123456789123456789123456789123456789123456789123456789123455678123456789123456789123456\n> \n> Remarkably the network starts being ""lost"" in the first few characters,\n> then gets back on its feet as soon as it finds a 1.\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/fchollet/keras/issues/197#issuecomment-111875646.\n', '@jonilaserson that sounds really cool! Do you want to add the code / data to our example folder? I think a lot of people would potentially be interested in it.\n', ""One thing probably worth changing in the code is the use of 'xrange' which doesn't exist in Python 3 as Keras is targeting this version too. \n"", 'We now have a character-level text generation LSTM example: https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n', ""Aren't you missing a lot of signal by using only the last character of each\nsubsequence as the label? I think you can do a lot more with a lot less by\nusing the TimeDistributedDense.\n\nOn Tue, Jun 16, 2015 at 4:06 AM, François Chollet notifications@github.com\nwrote:\n\n> We now have a character-level text generation LSTM example:\n> https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/fchollet/keras/issues/197#issuecomment-112248303.\n"", ""@fchollet, I have the bible in a 2.5M text file. Where would be suitable\nplace for it?\n\nOn Tue, Jun 16, 2015 at 11:15 AM, Jonathan Laserson jonilaserson@gmail.com\nwrote:\n\n> Aren't you missing a lot of signal by using only the last character of\n> each subsequence as the label? I think you can do a lot more with a lot\n> less by using the TimeDistributedDense.\n> \n> On Tue, Jun 16, 2015 at 4:06 AM, François Chollet <\n> notifications@github.com> wrote:\n> \n> > We now have a character-level text generation LSTM example:\n> > https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n> > \n> > —\n> > Reply to this email directly or view it on GitHub\n> > https://github.com/fchollet/keras/issues/197#issuecomment-112248303.\n"", ""> Aren't you missing a lot of signal by using only the last character of each\n> subsequence as the label? I think you can do a lot more with a lot less by\n> using the TimeDistributedDense.\n\nMaybe, what sampling strategy would you use when outputting sequences? If it turns out to converge faster, I'll edit the example to switch to sequence generation instead of character-by-character generation.\n\n> I have the bible in a 2.5M text file. Where would be suitable\n> place for it?\n\nAnywhere publicly accessible where the data can stay in the long term. I recommend Amazon S3. \n"", 'You can check that strategy in the code I published earlier in this thread.\nIf the text is\n\n""There are four lights.""\n\nThen that sequence should be the label, and the input sequence should be:\n\n""*There are four lights""\n\nWhere \'*\' marks the beginning of a sentence.\n\nOn Tue, Jun 16, 2015 at 8:40 PM, François Chollet notifications@github.com\nwrote:\n\n> Aren\'t you missing a lot of signal by using only the last character of each\n> subsequence as the label? I think you can do a lot more with a lot less by\n> using the TimeDistributedDense.\n> \n> Maybe, what sampling strategy would you use when outputting sequences? If\n> it turns out to converge faster, I\'ll edit the example to switch to\n> sequence generation instead of character-by-character generation.\n> \n> I have the bible in a 2.5M text file. Where would be suitable\n> place for it?\n> \n> Anywhere publicly accessible where the data can stay in the long term. I\n> recommend Amazon S3.\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/fchollet/keras/issues/197#issuecomment-112509469.\n', ""@jonilaserson I've been running your script with my Nietzsche corpus, but it keeps outputting gibberish. It doesn't seem to make any progress from epoch to epoch. What results have you had so far? Are there any fundamental differences between your corpus and the Nietzsche corpus?\n"", ""I don't know the Nietzsche corpus. Can you provide a link?\n\nOn Wed, Jun 17, 2015 at 4:29 AM, François Chollet notifications@github.com\nwrote:\n\n> @jonilaserson https://github.com/jonilaserson I've been running your\n> script with my Nietzsche corpus, but it keeps outputting gibberish. It\n> doesn't seem to make any progress from epoch to epoch. What results have\n> you had so far? Are there any fundamental differences between your corpus\n> and the Nietzsche corpus?\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/fchollet/keras/issues/197#issuecomment-112619507.\n"", '@jonilaserson \n\n``` python\nfrom keras.datasets.data_utils import get_file\npath = get_file(\'nietzsche.txt\', origin=""https://s3.amazonaws.com/text-datasets/nietzsche.txt"")\ntext = open(path).read().lower()\n```\n', 'Does anyone have some samples of the output from training on the Nietzche corpus?\n', 'There were some examples in the mailing list:\nhttps://groups.google.com/d/msg/keras-users/Y_FG_YEkjXs/PaKAefgbIrQJ\n\nOn 11 November 2015 at 08:12, Michael R. Bernstein <notifications@github.com\n\n> wrote:\n> \n> Does anyone have some samples of the output from training on the Nietzche\n> corpus?\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/fchollet/keras/issues/197#issuecomment-155830143.\n', 'Ah, thanks, @fchollet!\n\nQuoting here for reference (perhaps this should be added as a comment to the example?):\n\n> - ""he has given it the sense of unity and self-control as look to the individuals and platoness of men in the soul and the common power, the madied of morals and presurable and belief in the same time and the conscience of their influence, which is the present the conscience of the common end""\n> - ""the law is a goversion of the common."" (I take it to mean, ""law is the government of the plebe"")\n> - ""will the same time and beings and art of the strong and self-distrust of the same and all not only a soul and still store of the same time and artist in sacrifice their own soul, and always the most distrous of a man.""\n> - ""we can nation of everywhere, the strength of the foundation, and also us the most dinge in the master and art""\n\nIs there an way to take the output of a trained LSTM, edit it, and feed the changes via some form of backpropagation to further improve the results?\n', 'ping @fchollet \n', '@bskaggs @fchollet @ganarajpr @Tener @jonilaserson : In his blog karpathy talks of ""5 example character models"". I was wondering if anyone has implemented all 5 architectures in keras ? \n', 'I have made little modifications to the text generation example, to learn in a many-to-many fashion which Karpathy actually does in his implementation\n\nThe code is available here\nhttps://github.com/mineshmathew/char_rnn_karpathy_keras\n', 'Thanks for the additional example, @mineshmathew, much appreciated. :+1:\n', ""@mineshmathew @webmaven Can any of you confirm whether @mineshmathew's implementation has the same inefficiency that fchollet pointed out in his post on June 9, 2015?\n"", ""@rohan589  yes my implementation has the same 'inefficiency' which  fchollet was talking about.\n"", 'Are there stateful RNN examples yet?\n', 'Any updates on this guys?', '@jithurjacob there are examples using stateful RNNs . But not sure if there are any for language modelling task', '@jithurjacob @webmaven \r\nhere is my take on it: https://github.com/thomasZen/stateful_lstm_keras_text_generation/blob/master/stateful_lstm_text_generation.py\r\nIt is basically a modification of the text generation example. Using the stateful lstm is about 10 times faster on my CPU and I seem to get similar results. It is not a very sophisticated implementation, any suggestions for improvements are welcome.', 'Something odd is going on with the example at https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py If I run it as is, after 30 epochs I get to loss: 1.2435 - acc: 0.6137 and the generated text with diversity: 0.5 is:\r\n\r\n[and frame to ""shreaking and a sympathy when the here as the little the impulsed the case of refective of the stricte the exotlecss in the little man of seems to the spirituality and the sanctity in the individual is also the streak--every and all the satisfaction of the old duting one\'s own heart--what just to a law to]\r\n\r\nIf I add a layer to the network by inserting:\r\nmodel.add(LSTM(128, input_shape=(maxlen, len(chars)), return_sequences=True))\r\nbefore the current LSTM, it does a little better and gets to loss: 1.1389 - acc: 0.6449 after 30 epochs. The generated text at diversity 0.5 is:\r\n\r\n[of our own profoundest midnight and middle-in science, and in the case of the existence of the rarely in an age are not the pression of free things of man and forgotten of the pression and far from the spectable, and what i have no means the deprese to an actions are the]\r\n\r\nIf I add another layer though, the network fails to learn at all and just produces gibberish - similarly to what  @fchollet  said about the initial implementation by @jonilaserson. \r\n\r\nSlightly modernizing @mineshmathew solution though works very well with 3 layers:\r\n\r\nmodel = Sequential()\r\nmodel.add(LSTM(128, input_shape=(None, len(chars)), return_sequences=True))\r\nmodel.add(LSTM(128, return_sequences=True))\r\nmodel.add(LSTM(128, return_sequences=True))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(TimeDistributed(Dense(len(chars))))\r\nmodel.add(Activation(\'softmax\'))\r\nmodel.compile(loss=\'categorical_crossentropy\', optimizer=\'rmsprop\', metrics=[\'accuracy\'])\r\n\r\nThis gives me loss: 1.0730 - acc: 0.6615 with text generated at 0.5:\r\n""aspect of the old mistaken and she is or stonest morality of the historical sense of new christian spirit who has the same delights of the nearness of mankind, and one is possible is a matter of language of acquired to the inmention as a pers""\r\n\r\nI would suggest to replace the current example in Keras with @mineshmathew implementation. Happy to send a pull request.', 'I have implemented many-to-many character-based RNN using stateful LSTM as is done by Karpathy. You may consider using it as a reference.\r\nhttps://github.com/yxtay/char-rnn-text-generation/blob/master/keras_model.py', ""@yxtay I implemented a version of your network on generating new song lyrics and it's working pretty well. ""]",[],[],0,0
581,keras,1641,closed,"Help: 'Wrong number of dimensions: expected 3, got 2 with shape (32L, 60L).' in LSTM model","Hey everyone,

I'm trying to use custom data on the LSTM model, but it keeps giving shape errors. After reading some other issues along the same lines, I even tried reshaping the input data to size (nb_inputs, timestamps, 1) which looks approximately like (4200, 60, 1), but that returns an error that says a shape of (None, 4200, 60, 1) is no good. Any thoughts?



Output:

> Using Theano backend.
> Loading data...
> 4130 train sequences
> 1016 test sequences
> X_train shape: (4130L, 60L)
> X_test shape: (1016L, 60L)
> Build model...
> Train...
> Train on 4130 samples, validate on 1016 samples
> Epoch 1/3
> Traceback (most recent call last):
>   File ""main.py"", line 52, in <module>
>     validation_data=(X_test, y_test), show_accuracy=True)
>   File ""C:\Miniconda2\lib\site-packages\keras\models.py"", line 507, in fit
>     shuffle=shuffle, metrics=metrics)
>   File ""C:\Miniconda2\lib\site-packages\keras\models.py"", line 226, in _fit
>     outs = f(ins_batch)
>   File ""C:\Miniconda2\lib\site-packages\keras\backend\theano_backend.py"", line 357, in __call__
>     return self.function(*inputs)
>   File ""C:\Miniconda2\lib\site-packages\theano\compile\function_module.py"", line 513, in **call**
>     allow_downcast=s.allow_downcast)
>   File ""C:\Miniconda2\lib\site-packages\theano\tensor\type.py"", line 169, in filter
>     data.shape))
> TypeError: ('Bad input argument to theano function with name ""C:\Miniconda2\lib\site-packages\keras\backend\theano_backend.py:354""  at index 0(0-based)', 'Wrong number of dimensions: expected 3, got 2 with shape (32L, 60L).')
",stale,"["">  I even tried reshaping the input data to size (nb_inputs, timestamps, 1) \n\nYou will need to do that, but then you shouldn't put X_train.shape as the `input_shape` parameter of your LSTM. It doesn't care about the total number of training points for the model architecture. You should be able to instead pass in `input_shape=X_train.shape[1:]`.\n"", 'Thank you! Reshaping the arrays and adding .shape[1:] lets it run. May I ask why the input shape needs to be .shape[1:]?\n\nAlso, (off topic) the output looks like:\n\n> Using Theano backend.\n> Using gpu device 0: GeForce GTX 970\n> Loading data...\n> 4262 train sequences\n> 1083 test sequences\n> X_train shape: (4262L, 60L, 1L)\n> X_test shape: (1083L, 60L, 1L)\n> Build model...\n> Train...\n> Train on 4262 samples, validate on 1083 samples\n> Epoch 1/3\n> 4262/4262 [==============================] - 48s - loss: -28.0929 - acc: 1.0000 - val_loss: -64.3520 - val_acc: 1.0000\n> Epoch 2/3\n> 4262/4262 [==============================] - 48s - loss: -64.6251 - acc: 1.0000 - val_loss: -64.3520 - val_acc: 1.0000\n> Epoch 3/3\n> 4262/4262 [==============================] - 48s - loss: -64.6251 - acc: 1.0000 - val_loss: -64.3520 - val_acc: 1.0000\n> 1083/1083 [==============================] - 1s\n> Test score: -64.3520374245\n> Test accuracy: 1.0\n\nIs there a reason the loss is negative?\n', 'So X.shape is (samples, timesteps, dimension), but the model architecture doesn\'t care about many training examples (samples) you have. Once you\'ve built the model you can feed it a hundred million examples, doesn\'t matter. So you don\'t pass that as a parameter when you build your model. So X.shape[1:] is just (timesteps, samples) the two dimensions that matter\n\nIncidentally if you\'re on a Theano backend you _also_ don\'t need to specify the number of timesteps, but you need to pass ""None"" for that dimension, then.So instead you would pass in `(None, X.shape[2])`\n\nAs to why your score is negative: there\'s still something a bit fishy with your model. Your LSTM has 128 output dimensions and then you\'re evaluating binary cross-entropy on that? Is your `y` target also 128 dimensional? If it\'s not, you probably meant to put a `Dense(1)` layer, bringing your output down to  a single output that is compatible with y. Also if you\'re using a cross-entropy objective you want your output to be a probability distribution so you probably meant to put some sort of activation on top of it to normalize its output.\n\nOr else you probably meant to use a different objective function.\n\nWithout knowing more about your data (for instance the size of your y matrix) it\'s hard for me to help further.\n', '> So X.shape[1:] is just (timesteps, samples) the two dimensions that matter\n\nI\'m guessing you meant (timesteps, dimension)? \n\nThat makes sense, though. Thank you for the information. As for the output data, yes, a _binary__crossentropy loss function doesn\'t make much sense, considering the data look like:\n\n> [\n>    [5.45, 5.42, ..., 5.26],\n>    [5.25, 5.28, ..., 5.30],\n>    ...\n>    [5.12, 5.15, ..., 5.65]\n> ],\n> [5.13, 5.17, ..., 5.05]\n\nWhere the first list contains sequences of input (which are themselves lists), and the output is a single float value.\n\nI\'ve changed the model:\n\n``` python\nbatch_size = 32\n\nprint(\'Loading data...\')\n(X_train, y_train), (X_test, y_test) = t.LoadData()\nprint(len(X_train), \'train sequences\')\nprint(len(X_test), \'test sequences\')\n\nX_train = np.reshape(X_train, X_train.shape + (1,))\nX_test = np.reshape(X_test, X_test.shape + (1,))\n\nprint(\'X_train shape:\', X_train.shape)\nprint(\'X_test shape:\', X_test.shape)\n\nprint(\'Build model...\')\nmodel = Sequential()\nmodel.add(LSTM(1, input_shape=X_train.shape[1:]))\n\nmodel.compile(loss=\'mse\',\n              optimizer=\'sgd\',\n              class_mode=""categorical"")\n\nprint(""Train..."")\nmodel.fit(X_train, y_train, batch_size=batch_size, nb_epoch=3,\n          validation_data=(X_test, y_test), show_accuracy=True)\nscore, acc = model.evaluate(X_test, y_test,\n                            batch_size=batch_size,\n                            show_accuracy=True)\nprint(\'Test score:\', score)\nprint(\'Test accuracy:\', acc)\n```\n\nAnd it now produces output closer to the desired result:\n\n> Using Theano backend.\n> Loading data...\n> 4109 train sequences\n> 998 test sequences\n> X_train shape: (4109L, 60L, 1L)\n> X_test shape: (998L, 60L, 1L)\n> Build model...\n> Train...\n> Train on 4109 samples, validate on 998 samples\n> Epoch 1/3\n> 4109/4109 [==============================] - 3s - loss: 26.1860 - acc: 1.0000 - val_loss: 26.4226 - val_acc: 1.0000\n> Epoch 2/3\n> 4109/4109 [==============================] - 3s - loss: 26.1860 - acc: 1.0000 - val_loss: 26.4226 - val_acc: 1.0000\n> Epoch 3/3\n> 4109/4109 [==============================] - 3s - loss: 26.1860 - acc: 1.0000 - val_loss: 26.4226 - val_acc: 1.0000\n> 998/998 [==============================] - 0s\n> Test score: 26.4226496511\n> Test accuracy: 1.0\n\nI\'ll keep plugging away. :)\n', ""For most applications you would probably want more than 1 hidden state on your LSTM! You can put a Dense layer (or TimeDistributedDense) with an output dimension of 1 to project the hidden state down to 1 dimension on output, while still retaining more than 1 dimension of state. So something like:\n\nmodel.add(LSTM(128, input_shape=X_train.shape[1:]))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n"", '```\r\nfrom pandas import DataFrame\r\nfrom pandas import Series\r\nfrom pandas import concat\r\nfrom pandas import read_csv\r\nfrom sklearn.metrics import mean_squared_error\r\nfrom sklearn.preprocessing import MinMaxScaler\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense\r\nfrom keras.layers import LSTM\r\nfrom math import sqrt\r\nfrom matplotlib import pyplot\r\nimport numpy as np\r\n\r\n\r\n \r\n# frame a sequence as a supervised learning problem\r\ndef timeseries_to_supervised(data, lag=1):\r\n\tdf = DataFrame(data)\r\n\tcolumns = [df.shift(i) for i in range(1, lag+1)]\r\n\tcolumns.append(df)\r\n\tdf = concat(columns, axis=1)\r\n\tdf.fillna(0, inplace=True)\r\n\treturn df\r\n \r\n# create a differenced series\r\ndef difference(dataset, interval=1):\r\n\tdiff = list()\r\n\tfor i in range(interval, len(dataset)):\r\n\t\tvalue = dataset[i] - dataset[i - interval]\r\n\t\tdiff.append(value)\r\n\treturn Series(diff)\r\n \r\n# invert differenced value\r\ndef inverse_difference(history, yhat, interval=1):\r\n\treturn yhat + history[-interval]\r\n \r\n# scale train and test data to [-1, 1]\r\ndef scale(train):\r\n\t# fit scaler\r\n\tscaler = MinMaxScaler(feature_range=(-1, 1))\r\n\tscaler = scaler.fit(train)\r\n\t# transform train\r\n\ttrain = train.reshape(train.shape[0], train.shape[1])\r\n\ttrain_scaled = scaler.transform(train)\r\n \r\n\treturn scaler, train_scaled\r\n \r\n# inverse scaling for a forecasted value\r\ndef invert_scale(scaler, X, value):\r\n\tnew_row = [x for x in X] + [value]\r\n\tarray = np.array(new_row)\r\n\tarray = array.reshape(1, len(array))\r\n\tinverted = scaler.inverse_transform(array)\r\n\treturn inverted[0, -1]\r\n\r\ndef generate_features(x, forecast, window):\r\n    """""" Concatenates a time series vector x with forecasts from\r\n        the iterated forecasting strategy.\r\n\r\n    Arguments:\r\n    ----------\r\n        x:        Numpy array of length T containing the time series.\r\n        forecast: Scalar containing forecast for time T + 1.\r\n        window:   Autoregressive order of the time series model.\r\n    """"""\r\n    augmented_time_series = np.hstack((x, forecast))\r\n\r\n    return augmented_time_series[-window:].reshape(1, -1)\r\n\r\n    # fit an LSTM network to training data\r\ndef fit_lstm(train, batch_size, nb_epoch, neurons):\r\n\tX, y = train[:, 0:-1], train[:, -1]\r\n\tX = X.reshape(X.shape[0], 1, X.shape[1])\r\n\tmodel = Sequential()\r\n\tmodel.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\r\n\tmodel.add(Dense(1))\r\n\tmodel.compile(loss=\'mean_squared_error\', optimizer=\'adam\')\r\n\tfor i in range(nb_epoch):\r\n\t\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\r\n\t\tmodel.reset_states()\r\n\treturn model\r\n \r\ndef iterative_forecast(model, x, window, H):\r\n    """""" Implements iterative forecasting strategy\r\n\r\n    Arguments:\r\n    ----------\r\n        model: scikit-learn model that implements a predict() method\r\n               and is trained on some data x.\r\n        x:     Numpy array containing the time series.\r\n        h:     number of time periods needed for the h-step ahead\r\n               forecast\r\n    """"""\r\n    forecast = np.zeros(H)    \r\n    forecast[0] = model.predict(x.reshape(1, -1))\r\n\r\n    for h in range(1, H):\r\n        features = generate_features(x, forecast[:h], window)\r\n\r\n        forecast[h] = model.predict(features)\r\n\r\n    return forecast\r\n    \r\n    \r\n# load dataset\r\nseries = read_csv(\'shampoosales.csv\', header=0, index_col=0, squeeze=True)\r\n\r\n\r\n\r\n# transform data to be stationary\r\nraw_values = series.values\r\ndiff_values = difference(raw_values, 1)\r\n \r\n# transform data to be supervised learning\r\nsupervised = timeseries_to_supervised(diff_values, 1)\r\nsupervised_values = supervised.values\r\n\r\n\r\ntrain = supervised_values[0:-12]\r\ntest = supervised_values[-12:]\r\n\r\n# transform the scale of the data\r\nscaler, train_scaled = scale(train)\r\n# fit the model\r\nlstm_model = fit_lstm(train_scaled, 1, 3000, 4)\r\n\r\nyhat = iterative_forecast(lstm_model, train, 1, 10)\r\npredictions = list()\r\npredictions.append(yhat)\r\n```\r\ni am trying to discover an algorithm for iterative forecast using LSTM. seems to be something wrong with the code. would you be kind enough to help? \r\n\r\nerror that i am getting \r\n\r\n\'Error when checking : expected lstm_2_input to have 3 dimensions, but got array with shape (1, 46)\'\r\n', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', ""Hi,\r\n\r\nI have an input data with three variables / dimensions. with 4080 total samples. I am trying the below RNN script but getting the error.\r\nAny help ?\r\n\r\nmodel=Sequential()\r\nmodel.add(GRU(3,return_sequences=True,input_shape=(4080,3)))\r\nmodel.add(Dense(1))\r\nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\r\nmodel.fit(x_train,dummy_y_train,nb_epoch=20,batch_size=20,verbose=1)\r\n\r\nERROR: Error when checking input: expected gru_1_input to have 3 dimensions, but got array with shape (4080, 3)"", ""@wxs Don't you think there is something fishy in @DanHenry4  work as he is getting the same loss after each epoch and accuracy is always 1 (100%), which is near to impossible in most of machine learning predictions and specially in stock price prediction. I am also getting the loss 0.0 , therefore i am confused, may be i did something wrong.\r\n\r\nPlease reply me on this, I am using LSTM for the first time and I am confused by seeing the accuracy that may be I am doing something wrong.\r\n\r\nYour guidance will be appreciated. Thanks, "", ""if the matrix size is different in the test and the data on which model was trained than what can I do? Keras in r.   my results are really poor just because I've to add dummy columns to match matrix size.""]","[' python\nmaxlen = 60\nbatch_size = 32\n\nprint(\'Loading data...\')\n(X_train, y_train), (X_test, y_test) = t.LoadData()\nprint(len(X_train), \'train sequences\')\nprint(len(X_test), \'test sequences\')\n\nprint(\'X_train shape:\', X_train.shape)\nprint(\'X_test shape:\', X_test.shape)\n\nprint(\'Build model...\')\nmodel = Sequential()\nmodel.add(LSTM(128, input_shape=X_train.shape))\n\nmodel.compile(loss=\'binary_crossentropy\',\n              optimizer=\'sgd\',\n              class_mode=""categorical"")\n\nprint(""Train..."")\nmodel.fit(X_train, y_train, batch_size=batch_size, nb_epoch=3,\n          validation_data=(X_test, y_test), show_accuracy=True)\nscore, acc = model.evaluate(X_test, y_test,\n                            batch_size=batch_size,\n                            show_accuracy=True)\nprint(\'Test score:\', score)\nprint(\'Test accuracy:\', acc)\n']",[],0,0
582,keras,6241,closed,AttributeError: 'NoneType' object has no attribute 'get_weights',"hi, I just got a problem when I run a sequential model. 
I fit the model firstly, and then I want to get the first FullConnect Layer's parameters ,that is  ""dense1 = model.get_layer(index=1).get_weights()"".  But I got the wrong ""AttributeError: 'NoneType' object has no attribute 'get_weights' ""。I have no idea about this. Can anyone help me?

Sincerely",stale,"[""@Alisaincd Do you have a script that could recreate the error? It looks like you're trying to index for a layer that the model doesn't actually have. Using `model.layers`, you can get a list of the layers that the model has, and index from there. From the looks of the above note, you're model doesn't have a layer at index 1. "", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],0,0
583,keras,2529,closed,Missing file docs/sources/layers/writing-your-own-keras-layers.md when building documentation,"When running  on a up to date Keras repo (c9f7d970e97d5a) following the instructions in  the following error happens:



In addition, there could be a small error in the example in :
 is supposed to be  on line 7 in function .
- [X] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [X] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [X] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",,"[""I can confirm I'm missing this file too (i.e. not autogenerated or in templates).\n"", 'Thanks @fchollet - 36a829c20d5dcd4d8ba120781235c278588b6bb7\n']",[],"['mkdocs build', 'docs/README.md', ""FileNotFoundError: [Errno 2] No such file or directory: '[..]/keras/docs/sources/layers/writing-your-own-keras-layers.md'"", 'http://keras.io/layers/writing-your-own-keras-layers/', 'super(Layer, self).__init__(**kwargs)', 'super(mylayer, self).__init__(**kwargs)', '__init__']",0,0
584,keras,3873,closed,Feature: ASCII prints for sequential models,"I wanted to be able to show sequential networks in a clean and minimalistic way for didactic purpose. Both  and graph export were not enough - I wanted dimensions, numbers of parameters and activation functions in one place, at the same time without unnecessary overhead.

Bear in mind that I purposefully make no distinction between adding activation function as a keyword argument or as a separate layer (vide [Activations - Keras documentation](https://keras.io/activations/)), unlike in  or .

Code: https://gist.github.com/stared/8411d4e7e457b0f14f39d700afc8511c

Should I clean and generalise it, so that it can be a part of ?

Any comments, remarks and (sub)feature requests ale welcomed! :)
## Examples
### Proof of principle


### VGG16


",stale,"['Just in case, I started repo with this project: https://github.com/stared/keras-sequential-ascii']","['\n      OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n\n          Input   #####   (1, 28, 28)\n  Convolution2D    \\|/  -------------------       100     0.6%\n           relu   #####   (10, 26, 26)\n   MaxPooling2D   YYYYY -------------------         0     0.0%\n                  #####   (10, 13, 13)\n        Flatten   ||||| -------------------         0     0.0%\n                  #####   (1690,)\n          Dense   XXXXX -------------------     16910    98.8%\n                  #####   (10,)\n        Dropout    | || -------------------         0     0.0%\n           relu   #####   (10,)\n          Dense   XXXXX -------------------       110     0.6%\n        softmax   #####   (10,)\n', '\n      OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n\n          Input   #####   (3, 224, 224)\n  Convolution2D    \\|/  -------------------      1792     0.0%\n           relu   #####   (64, 224, 224)\n  Convolution2D    \\|/  -------------------     36928     0.0%\n           relu   #####   (64, 224, 224)\n   MaxPooling2D   YYYYY -------------------         0     0.0%\n                  #####   (64, 112, 112)\n  Convolution2D    \\|/  -------------------     73856     0.1%\n           relu   #####   (128, 112, 112)\n  Convolution2D    \\|/  -------------------    147584     0.1%\n           relu   #####   (128, 112, 112)\n   MaxPooling2D   YYYYY -------------------         0     0.0%\n                  #####   (128, 56, 56)\n  Convolution2D    \\|/  -------------------    295168     0.2%\n           relu   #####   (256, 56, 56)\n  Convolution2D    \\|/  -------------------    590080     0.4%\n           relu   #####   (256, 56, 56)\n  Convolution2D    \\|/  -------------------    590080     0.4%\n           relu   #####   (256, 56, 56)\n   MaxPooling2D   YYYYY -------------------         0     0.0%\n                  #####   (256, 28, 28)\n  Convolution2D    \\|/  -------------------   1180160     0.9%\n           relu   #####   (512, 28, 28)\n  Convolution2D    \\|/  -------------------   2359808     1.7%\n           relu   #####   (512, 28, 28)\n  Convolution2D    \\|/  -------------------   2359808     1.7%\n           relu   #####   (512, 28, 28)\n   MaxPooling2D   YYYYY -------------------         0     0.0%\n                  #####   (512, 14, 14)\n  Convolution2D    \\|/  -------------------   2359808     1.7%\n           relu   #####   (512, 14, 14)\n  Convolution2D    \\|/  -------------------   2359808     1.7%\n           relu   #####   (512, 14, 14)\n  Convolution2D    \\|/  -------------------   2359808     1.7%\n           relu   #####   (512, 14, 14)\n   MaxPooling2D   YYYYY -------------------         0     0.0%\n                  #####   (512, 7, 7)\n        Flatten   ||||| -------------------         0     0.0%\n                  #####   (25088,)\n          Dense   XXXXX ------------------- 102764544    74.3%\n           relu   #####   (4096,)\n        Dropout    | || -------------------         0     0.0%\n                  #####   (4096,)\n          Dense   XXXXX -------------------  16781312    12.1%\n           relu   #####   (4096,)\n        Dropout    | || -------------------         0     0.0%\n                  #####   (4096,)\n          Dense   XXXXX -------------------   4097000     3.0%\n                  #####   (1000,)\n                  ||||| -------------------         0     0.0%\n        softmax   #####   (1000,)\n']","['model.summary()', 'model.summary()', ""SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"", 'keras/utils']",0,0
585,keras,4007,closed,How to visualize the output of one of two merged layers ? ,"So for example in this code : 



If I want to see the output of the right branch model given a test input **(before the merge layer)** how to do that ?
Thanks
",stale,"['Would it be possible to use my code from #4006 ?\n', 'I do not know how exactly, \nTo be more specific : what is the correct layer index  I should use ? \n', 'i dont know =) try it out. use weights=model.get_weights() to determine where you ""are"" \n', 'update: I used the get_weights() command. Then built a new model with the same architecture (as the branch I wanted to test) and used the correct weights. \n']",[],"[""from keras.layers import Merge\n\nleft_branch = Sequential()\nleft_branch.add(Dense(32, input_dim=784))\n\nright_branch = Sequential()\nright_branch.add(Dense(32, input_dim=784))\n\nmerged = Merge([left_branch, right_branch], mode='concat')\n\nfinal_model = Sequential()\nfinal_model.add(merged)\nfinal_model.add(Dense(10, activation='softmax'))""]",0,0
586,keras,6973,closed,Problem with batch normalization layer,"I am trying to use batch normalization, but for some reason, even for the simplest network, when I run model.fit  even for one epoch,the loss is nan and naturally no learning is performed.
For example - I use a simple model like this:
 model = Sequential()
 model.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=(16,16,3)))
 model.add(MaxPool2D(pool_size=(2, 2)))
 model.add(BatchNormalization())
model.add(Flatten())
model.add(Dense(2,activation='softmax'))
 model.compile (loss='binary_crossentropy',  optimizer='adam',  metrics=['accuracy'])

If I remove the batch normalization, everything works great.
I am using keras 2.0.4 and theano 0.9.0, cuda 7. I tried removing cudnn, got the same results.
I tried a diffrent axis (axis=1) when calling BN,  (although this should not be right) and got the same result.
What am I doing wrong ?
Thank YOU!
",stale,"['Try BN with a range of different parameters, in particular for `epsilon`. Also try to see what happens for your model on CPU.', ""When Running in CPU Mode, everything is OK (But not practical...)\r\nChanging the parameters didn't help (what parameters are there other then momentum, and epsilon ?)\r\nNote that when using model.predict (before the first fit), i receive a valid output (not nan).\r\nI used this code:\r\nmodel = Sequential()\r\nmodel.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=(8,8,3)))\r\nmodel.add(BatchNormalization(axis =-1,epsilon=0.02,momentum=0.97))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(2,activation='softmax'))\r\nmodel.compile (loss='binary_crossentropy',  optimizer='adam',  metrics=['accuracy'])\r\n\r\nxTrain = numpy.random.randn(100,8,8,3)\r\nyTrain = numpy.random.randint (0,2,size=(100,2))\r\n\r\nmodel.fit (xTrain,yTrain)"", 'Sounds like a problem with your CUDA/cuDNN install.', 'Thank You !\r\nOther than that, everything was swell..\r\nI use Cuda7\r\nUsing gpu device 0: GeForce GTX TITAN (CNMeM is disabled, cuDNN 4007)\r\nThis happens if I remove the cuDNN as well.\r\nI am using an old Ubuntu, 12.04.\r\n', ""Also do you only have 2 categories that are mutually exclusive? You should encode them as 0s and 1s and I think your last layer should be:\r\n```\r\nmodel.add(Dense(1, activation='sigmoid'))\r\n```"", ""Good catch, don't use `binary_crossentropy` with a categorical class encoding unless you actually have multiple labels per sample."", ""I wasn't aware that this was not allowed... I have been using it quite a lot and had no problems so far..\r\nI will check and update"", ""It's not a keras issue, it's an understanding what you're doing issue"", ""I changed the code to this, but got the same results.\r\nresult:\r\nEpoch 1/1\r\n100/100 [==============================] - 0s - loss: nan - acc: 0.0000e+00     \r\n\r\ncode:\r\nmodel = Sequential()\r\nmodel.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=(8,8,3)))\r\nmodel.add(BatchNormalization(axis =-1,epsilon=0.02,momentum=0.97))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(1,activation='sigmoid'))\r\nmodel.compile (loss='binary_crossentropy',  optimizer='adam',  metrics=['accuracy'])\r\n\r\nxTrain = numpy.random.randn(100,8,8,3)\r\nyTrain = numpy.random.randint (0,2,size=(100,1))\r\n\r\nmodel.fit (xTrain,yTrain)"", 'I have the same NaN problem with batch normalization. Did you solve it?', 'No. I can only say that on a different computer the same code trained with no problem. It is probably something to do with an old cuda / umbuntu version.\r\n', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', 'Did anyone find a solution for this problem?']",[],[],0,0
587,keras,3988,closed,Slicing layers from a pre-trained Model,"Say I have a pertained model like the VGG16. If I want to select only the first few convolutional blocks I can so something like:



Is there a way to use the same API to slice a middle part of a model (including weights) ? I've tried something like:



but got an error:


",stale,"[""I don't think you can do that because it would involve changing the graph which, from what I understand, is not possible (at least in tensorflow). What you could to is, instead of getting the tensors, create an input tensor and them apply the layers to it in order. If you use the same layers, the weights will be shared and it should work like you expect it to.\n"", 'I couldn\'t find a way either but you\'re right @robertomest, adding them one layer at a time is going to do it.\n\nSomething like this:\n\n``` python\n#adding the last conv block from vgg and one hidden layer on top of that:\ninput2 = Input(shape=(512, 9, 9))  #pick whatever the output of the previous model is\nx = vgg16.get_layer(""block5_conv1"")(input2)\nx = vgg16.get_layer(""block5_conv2"")(x)\nx = vgg16.get_layer(""block5_conv3"")(x)\nx = vgg16.get_layer(""block5_pool"")(x)\nx = Flatten()(x)\nx = Dense(512, activation=""relu"")(x)\nx = Dropout(0.5) (x)\npreds = Dense(1, activation=""sigmoid"")(x)\n\nmodel_top = Model(input2, preds)\n```\n', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs, but feel free to re-open it if needed.\n']","['\nfrom keras.applications import VGG16\nvgg16 = VGG16(include_top=False, input_tensor=input)\n\nconv_output = vgg16.get_layer(""block4_pool"").output \nfeature_extractor = Model(vgg16.input, conv_output)\n', '\nmodel_top = Model(input = vgg16.get_layer(""block5_conv1"").input, \n                  output=vgg16.get_layer(""block5_conv3"").output)\n', '\nException: Graph disconnected: cannot obtain value for tensor input_6 at layer ""input_6"". The      following previous layers were accessed without issue: []\n']",[],0,0
588,keras,4156,closed,callbacks e.g. CSV Callback receive constant lr when doing a LR decay,"Please make sure that the boxes below are checked before you submit your issue. Thank you!
- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
  pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

Having a callback in the model:



and model compilation:



In the final csv in column 'lr' I get always 0.03 instead of lr calculated by decay.

After looking at callbacks.py I think the problem is in the 'logs' object passed to callback not in the callback itself. 

Any hints? I could try to make a PR - given some guidance
",stale,"[""The learning rate decay in the Keras' SGD implementation does not change the value of its `lr` variable, it is computed every batch through the iteration number and the global learning rate, which remains unchanged.\n\nhttps://github.com/fchollet/keras/blob/master/keras/optimizers.py#L146\n""]","[""\ncsvc = keras.callbacks.CSVLogger('models/train/results.csv', separator=',', append=True)\n"", ""\nfrom keras.optimizers import SGD\nsgd = SGD(lr=0.03, decay=0.01)\nmodel.compile(loss='binary_crossentropy',\n              optimizer=sgd,\n              metrics=['accuracy'])\n""]",[],0,0
589,keras,2619,closed,Why nested shared layers as such network doesn't work?,"Hi,
I design a network which composes of shared layers, I write code using functional API. However, there always be errors.  
I paste my network structure graph , code , and error information below, hope to provide a clue.
Thanks!

Network Structure:

![network structure jpeg](https://raw.githubusercontent.com/ylqfp/attachments/bb396650622b37b2a311d29864795aec67301042/cnn2_2_lstm.jpg)
Share layers:
1. Embedding is shared among 3 input.(blue color). However, input_length is different for input1/input2 and input3. The input_length for input1/input2 is LSTM_MAXLEN, 1000, for input3 is QUERY_MAXLEN, 5, eg. max 5 words for a query.
2. CNN feature learning layer is shared between input1 and input2. (green color)
",stale,"['``` python\nRNN=recurrent.LSTM\nEMBED_HIDDEN_SIZE=50\nSENT_HIDDEN_SIZE=100\nCOMM_HIDDEN_SIZE=100\nQUERY_HIDDEN_SIZE=100\nBATCH_SIZE=32\nEPOCHS=6\nQUERY_MAXLEN=5\n\nSTEP=6\n\nRUN=1\n\nvocab = 149999\n\nvocab_size=vocab+1\n\nprint ""build model...""\n\nnb_filter=128\nfilter_length=3\npool_length=2\n\ndef max_1d(X):\n    return K.max(X, axis=1)\n\n#embedding_layer = Embedding(vocab_size, EMBED_HIDDEN_SIZE, mask_zero=False)()\nsentence_input = Input(shape=(LSTM_maxlen,))\nquery_input = Input(shape=(QUERY_MAXLEN,))\n\nx = Embedding(vocab_size, EMBED_HIDDEN_SIZE, mask_zero=False)(sentence_input)\nx = Dropout(0.2)(x)\nx = Convolution1D(nb_filter=nb_filter,\n                                filter_length = filter_length,\n                                border_mode=\'valid\',\n                                activation=\'relu\',\n                                subsample_length=1)(x)\nx = MaxPooling1D(pool_length=pool_length)(x)\nx = Convolution1D(nb_filter=nb_filter,\n                                filter_length = filter_length,\n                                border_mode=\'valid\',\n                                activation=\'relu\',\n                                subsample_length=1)(x)\nx = Lambda(max_1d, output_shape=(nb_filter, ))(x)\nx = Dense(SENT_HIDDEN_SIZE, activation=\'relu\')(x)\noutx = Dropout(0.2)(x)\ncnn_share = Model(sentence_input, outx)\n\na = Input(shape=(LSTM_maxlen, ))\nb = Input(shape=(LSTM_maxlen, ))\nc = Input(shape=(QUERY_MAXLEN, ))\n\nshared_cnn1 = cnn_share(a)\nshared_cnn2 = cnn_share(b)\n\ny = Input(shape=(QUERY_MAXLEN,))\ny = Embedding(vocab_size, EMBED_HIDDEN_SIZE, mask_zero=True)(y)\ny = RNN(QUERY_HIDDEN_SIZE, return_sequences=False, activation=\'relu\')(y)\ny = Dropout(y)\n\nmodel_query = Model(query_input, y)\n\nqrnn = model_query(c)\n\nmerge_vector = Merge([shared_cnn1, shared_cnn2, qrnn], mode=\'concat\', concat_axis=-1)\ndense1 = Dense(100, activation=\'relu\')(merge_vector)\ndp1 = Dropout(0.2)(dense1)\noutput1 = Dense(1, activation=\'sigmoid\')(dp1)\n\nmodel = Model(input=[shared_cnn1, shared_cnn2, qrnn], output=output1)\n\nmodel.compile(loss=\'binary_crossentropy\', optimizer=\'rmsprop\', metrics=[\'accuracy\'])\n\nif RUN:\n    model3.fit([X, Xc, Xq],Y, batch_size=BATCH_SIZE, nb_epoch = EPOCHS,validation_split=0.05, show_accuracy=True )\n```\n', 'Share layers:\n1. Embedding is shared among 3 input.(blue color). However, input_length is different for input1/input2 and input3. The input_length for input1/input2 is LSTM_MAXLEN, 1000, for input3 is QUERY_MAXLEN, 5, eg. max 5 words for a query.\n2. CNN feature learning layer is shared between input1 and input2. (green color)\n', 'run ERROR:\nTraceback (most recent call last):\n  File ""shared_cnn_1.py"", line 105, in <module>\n    x = Embedding(vocab_size, EMBED_HIDDEN_SIZE, mask_zero=False)(sentence_input)\n  File ""/home/yulei/.local/lib/python2.7/site-packages/Keras-1.0.1-py2.7.egg/keras/engine/topology.py"", line 485, in **call**\n    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)\n  File ""/home/yulei/.local/lib/python2.7/site-packages/Keras-1.0.1-py2.7.egg/keras/engine/topology.py"", line 543, in add_inbound_node\n    Node.create_node(self, inbound_layers, node_indices, tensor_indices)\n  File ""/home/yulei/.local/lib/python2.7/site-packages/Keras-1.0.1-py2.7.egg/keras/engine/topology.py"", line 148, in create_node\n    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))\n  File ""/home/yulei/.local/lib/python2.7/site-packages/Keras-1.0.1-py2.7.egg/keras/layers/embeddings.py"", line 133, in call\n    out = K.gather(W, x)\n  File ""/home/yulei/.local/lib/python2.7/site-packages/Keras-1.0.1-py2.7.egg/keras/backend/theano_backend.py"", line 156, in gather\n    return reference[indices]\n  File ""/home/yulei/.local/lib/python2.7/site-packages/Theano-0.8.1-py2.7.egg/theano/tensor/var.py"", line 503, in **getitem**\n    return self.take(args[axis], axis)\n  File ""/home/yulei/.local/lib/python2.7/site-packages/Theano-0.8.1-py2.7.egg/theano/tensor/var.py"", line 535, in take\n    return theano.tensor.subtensor.take(self, indices, axis, mode)\n  File ""/home/yulei/.local/lib/python2.7/site-packages/Theano-0.8.1-py2.7.egg/theano/tensor/subtensor.py"", line 2387, in take\n    return take(a, indices.flatten(), axis, mode).reshape(shape, ndim)\n  File ""/home/yulei/.local/lib/python2.7/site-packages/Theano-0.8.1-py2.7.egg/theano/tensor/subtensor.py"", line 2365, in take\n    return advanced_subtensor1(a, indices)\n  File ""/home/yulei/.local/lib/python2.7/site-packages/Theano-0.8.1-py2.7.egg/theano/gof/op.py"", line 611, in **call**\n    node = self.make_node(_inputs, *_kwargs)\n  File ""/home/yulei/.local/lib/python2.7/site-packages/Theano-0.8.1-py2.7.egg/theano/tensor/subtensor.py"", line 1687, in make_node\n    raise TypeError(\'index must be integers\')\nTypeError: index must be integers\n', ""Input should be an int type.  you can specify in the constructor.  \n\n``` python\nX = Input(shape=shape, dtype='int32')\n```\n"", ""It seems that the it's not the type problem.\nI'm writing a non-shared version first, hoping to get everything correct.\nAnd then shared version.\n"", 'Are you getting the same error or a different one? because it looks like the ""index must be integers"" error that I usually get when I forgot to cast my inputs as ints. \n', ""Same error.\nI found some other mistakes in my code, after I modified them, and make sure non-shared version to be OK, and currently the shared version is OK.\nHowever, the model is not completely shared, case the word embedding is not shared between input3 and input1/2.\nI'll report after complete share.\n"", 'Hi, all,\nThere comes the error information, I\'m wondering if the shared layers can be nested.\n\nTraceback (most recent call last):\n  File ""shared_cnn_4.py"", line 130, in <module>\n    model.fit([X, Xc, Xq],Y, batch_size=BATCH_SIZE, nb_epoch = EPOCHS,validation_split=0.05)\n  File ""/home/yulei/.local/lib/python2.7/site-packages/Keras-1.0.1-py2.7.egg/keras/engine/training.py"", line 1015, in fit\n    self._make_train_function()\n  File ""/home/yulei/.local/lib/python2.7/site-packages/Keras-1.0.1-py2.7.egg/keras/engine/training.py"", line 653, in _make_train_function\n    *_self._function_kwargs)\n  File ""/home/yulei/.local/lib/python2.7/site-packages/Keras-1.0.1-py2.7.egg/keras/backend/theano_backend.py"", line 503, in function\n    return Function(inputs, outputs, updates=updates, *_kwargs)\n  File ""/home/yulei/.local/lib/python2.7/site-packages/Keras-1.0.1-py2.7.egg/keras/backend/theano_backend.py"", line 489, in __init__\n    **kwargs)\n  File ""/home/yulei/.local/lib/python2.7/site-packages/Theano-0.8.1-py2.7.egg/theano/compile/function.py"", line 320, in function\n    output_keys=output_keys)\n  File ""/home/yulei/.local/lib/python2.7/site-packages/Theano-0.8.1-py2.7.egg/theano/compile/pfunc.py"", line 442, in pfunc\n    no_default_updates=no_default_updates)\n  File ""/home/yulei/.local/lib/python2.7/site-packages/Theano-0.8.1-py2.7.egg/theano/compile/pfunc.py"", line 187, in rebuild_collect_shared\n    (store_into, update_d[store_into]))\nValueError: (\'this shared variable already has an update expression\', (embedding_1_W, Elemwise{sub,no_inplace}.0))\n', ""``` python\nshared_embed = Embedding(vocab_size, EMBED_HIDDEN_SIZE, mask_zero=False)\nx = shared_embed(sentence_input)\nx = Dropout(0.2)(x)\nx = Convolution1D(nb_filter=nb_filter,\n                                filter_length = filter_length,\n                                border_mode='valid',\n                                activation='relu',\n                                subsample_length=1)(x)\nx = MaxPooling1D(pool_length=pool_length)(x)\nx = Convolution1D(nb_filter=nb_filter,\n                                filter_length = filter_length,\n                                border_mode='valid',\n                                activation='relu',\n                                subsample_length=1)(x)\nx = Lambda(max_1d, output_shape=(nb_filter, ))(x)\nx = Dense(SENT_HIDDEN_SIZE, activation='relu')(x)\noutx = Dropout(0.2)(x)\n\ncnn_share = Model(sentence_input, outx)\n\nshared_cnn1 = cnn_share(a)\nshared_cnn2 = cnn_share(b)\n\n#y = Input(shape=(QUERY_MAXLEN,), dtype='int32')\n#y = Embedding(vocab_size, EMBED_HIDDEN_SIZE, mask_zero=True)(c)\ny = shared_embed(c)\ny = RNN(QUERY_HIDDEN_SIZE, return_sequences=False, activation='relu')(y)\ny = Dropout(0.2)(y)\n\n#model_query = Model(query_input, y)\n#qrnn = model_query(c)\n\n#merge_vector = Merge([shared_cnn1, shared_cnn2, qrnn], mode='concat', concat_axis=-1)\nmerge_vector = merge([shared_cnn1, shared_cnn2, y], mode='concat', concat_axis=-1)\n#merge_vector = merge([outx, outxx, y], mode='concat', concat_axis=-1)\ndense1 = Dense(100, activation='relu')(merge_vector)\ndp1 = Dropout(0.2)(dense1)\noutput1 = Dense(1, activation='sigmoid')(dp1)\n\nmodel = Model(input=[a,b,c], output=output1)\n\n```\n\nValueError: ('this shared variable already has an update expression', (embedding_1_W, Elemwise{sub,no_inplace}.0))\n\nIs it because Keras doesn't support nested shared layers?\nIf I want to achieve my goal, how should I do?\n@braingineer @fchollet \n"", 'I get a same error as you did.@ylqfp Is there anyone can solve this problem? Thanks a lot.\n', ""I get the same error in a similar context, using `TimeDistributed`  layer. After some digging, it looks like that the issue there is `Wrapper` layer class takes `trainable_weights` from the wrapped layer, and somehow that causes `Model.layers` in `engine/training.py` to contain duplicated copies of parameters from wrapped layers, resulting in duplicates `trainable_weights` inside function `_make_train_function` of `Model` class.  After removing the part in `Wrapper` layer class that tries to duplicate, everything works. However, I don't know if that has any un-intended effects in other use cases. \n"", 'Hi @ylqfp , I tried some other structures without work well. I think maybe Keras doesnot support the nested shared layer right now. For your problem specially, you can pre-train the word embeddings by something like word2vec, then make the embeddings unchangeable during the training.\n', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs, but feel free to re-open it if needed.\n', ""Same issue here. I'm trying to load the Keras model which is trained in Python into MATLAB. However, Matlab complains that weight sharing is not yet supported. Is there other method I can load the model into Matlab? Thanks!"", '@zuoyigehaobing Same problem here, did you find any solution?']",[],[],0,0
590,keras,5284,closed,Problems using fit_generator with a custom generator,"

`

I am precomputing the features of a convolutional layer and training fully connected layers using them. I have to use hdf5 file and a custom generator because the size of dataset is quite large.

Following is the output:
<img width=""544"" alt=""screen shot 2017-02-06 at 11 36 13 am"" src=""https://cloud.githubusercontent.com/assets/6660192/22636432/0c2d05a4-ec61-11e6-9193-1a8bd0ac64df.png"">

**The loss when using fit decreases much more quickly then using fit_generator with a custom generator.**
Am I doing something wrong? To me it looks like some bug in fit_generator. 
",,['I had to shuffle the batches. That did the trick.'],[],"['', ""\r\nf = h5py.File(path+'results/precomp.h5', 'r')\r\nconv_trn_feat = f['train_features'][:]\r\ntrn_labels = np.tile(f['train_labels'][:], reps=(2,1))\r\n\r\ndef myGenerator():\r\n    while True:\r\n        for i in range(0, len(conv_trn_feat), batch_size):\r\n            yield conv_trn_feat[i:i+batch_size], trn_labels[i:i+batch_size]\r\n\r\ntrain_datagen = myGenerator()\r\n\r\nbn_model = Sequential(get_bn_layers(0, num=num_hidden))\r\nbn_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\r\nbn_model.optimizer.lr = 1e-3\r\nbn_model.fit_generator(train_datagen, samples_per_epoch=len(conv_trn_feat), nb_epoch=4)\r\nbn_model = Sequential(get_bn_layers(0, num=num_hidden))\r\nbn_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\r\nbn_model.optimizer.lr = 1e-3\r\nbn_model.fit(conv_trn_feat, trn_labels, nb_epoch=4)"", '']",0,0
591,keras,1592,closed,Convolutional Neural Net bug,"I am trying to run the specific code on a pc with a Titan X gpu:



and it yields the following error



also the versions are 



When i run the same code on another pc it passes!



It is a machine without gpu and versions 



What could i do in order to use such a good gpu ? 
Should i fall back to keras 3.0 or is it a theano problem ? 

Thank you in advance
",,"['You seem confused. The error message is clear enough:\n\n> Exception: The shape of the input to ""Flatten"" is not fully defined (got (5, 1, 0). Make sure to pass a complete ""input_shape"" or ""batch_input_shape"" argument to the first layer in your model.\n\nSo your Flatten layer is getting an input of shape (5, 1, 0), i.e. with 0 filters, which is not a valid input shape. Hence an error. You are pooling 0 columns of your previous input.\n', 'Hello Mr. Chollet \n\nI understand the error printed. \n\nThe problem is that it should not be there.\n\nIf you observe the code the Flatten layer should not get this input. \n\nAlso i have posted the output of two different machines. \nThe one with the gpu yields an error.\nThe second without the gpu runs smoothly the same code and presents an output.\n', ""If you haven't solved it by now, its because you're pooling layer has completely shrunk the input. remove that and you should be fine\n"", 'Have you find the solution? \nI have the same problem. My code was running on my computer without GPU, but after I added a GPU, I got the same error.\n', 'This problem occurred when i used flatten after a pooling layer.\nI had used pool_size in a wrong way.\nI found out the solution carefully reading the documentation,\nas well as using\n\n`model.layers[-1].get_weights()[0].shape`\n\nwhich was extremely helpful in order to place the appropriate values in pool_size.\n\nAlso try using dummy data created with numpy (or a really small subcorpus) just to make it compile.\nThen use the full data set in hand.\n\nFinally the problem was  not that it yielded the error in the first computer.\nThe problem was that it did not yield the error on the second one when in fact it should do just that.\n\nI hope that helps.\n', 'Thank you for your reply.\n\nMy problem was that I had forgotten to change image_dim_ordering to TH when I changed the backend to theano.\n', '@hadikazemi AHA! there is my answer. Thanks so much!']","[""\n\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\nfrom keras.layers.convolutional import Convolution2D, MaxPooling2D, MaxPooling1D, Convolution1D\nfrom keras.optimizers import SGD\nimport numpy as np\n\n\nX = [\n[\n[\n[1,2,3],\n[1,2,3],\n[1,2,3],\n[1,2,3]\n]\n]\n]\n\nY = [\n[1,2,3,4,5]\n]\n\nX = np.array(X)\n\nY = np.array(Y) \n\nConv_size = 10\nDense_size = 10\n\nfilters = 5\nrows= 2\ncols= X.shape[-1]\n\n\nmodel = Sequential()\nmodel.add(Convolution2D(filters, rows, cols, activation='sigmoid' , input_shape = X.shape[1:]))\nmodel.add(MaxPooling2D(pool_size=(rows, cols)))\nmodel.add(Flatten())\nmodel.add(Dense(Dense_size, activation='sigmoid'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(5, activation='linear'))\nmodel.compile(loss='rmse', optimizer='sgd')\nmodel.fit(X, Y, nb_epoch=50)\n\n\n"", '\n\n>>> model.add(Dense(Dense_size, activation=\'sigmoid\'))\nTraceback (most recent call last):\n  File ""<stdin>"", line 1, in <module>\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/containers.py"", line 68, in add\n    self.layers[-1].set_previous(self.layers[-2])\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 82, in set_previous\n    assert self.input_ndim == len(layer.output_shape), (\'Incompatible shapes: layer expected input with ndim=\' +\n  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/core.py"", line 814, in output_shape\n    \'(got \' + str(input_shape[1:]) + \'. \'\nException: The shape of the input to ""Flatten"" is not fully defined (got (5, 1, 0). Make sure to pass a complete ""input_shape"" or ""batch_input_shape"" argument to the first layer in your model.\n\n', '\npip show theano | grep Version\nMetadata-Version: 1.1\nVersion: 0.8.0.dev0\n\npip show keras | grep Version\nMetadata-Version: 1.1\nVersion: 0.3.1\n', ""\n>>> model = Sequential()\n>>> model.add(Convolution2D(filters, rows, cols, activation='sigmoid' , input_shape = X.shape[1:]))\n>>> model.add(MaxPooling2D(pool_size=(rows, cols)))\n>>> model.add(Flatten())\n>>> model.add(Dense(Dense_size, activation='sigmoid'))\n>>> model.add(Dropout(0.5))\n>>> model.add(Dense(5, activation='linear'))\n>>> model.compile(loss='rmse', optimizer='sgd')\n\n>>> model.fit(X, Y, nb_epoch=5)\nEpoch 1/5\n1/1 [==============================] - 0s - loss: 3.4482\nEpoch 2/5\n1/1 [==============================] - 0s - loss: 3.7623\nEpoch 3/5\n1/1 [==============================] - 0s - loss: 3.7835\nEpoch 4/5\n1/1 [==============================] - 0s - loss: 3.8206\nEpoch 5/5\n1/1 [==============================] - 0s - loss: 4.4849\n<keras.callbacks.History object at 0x5751610>\n\n"", '\npip show theano| grep Version\nMetadata-Version: 1.1\nVersion: 0.7.0\n\npip show keras| grep Version\nMetadata-Version: 2.0\nVersion: 0.3.0\n\n']",[],0,0
592,keras,5167,closed,Merge layer with lambda - potential issue with reloading model after save,"Hi, I have a model which uses a Merge() layer. The model runs and saves. I am having trouble loading the model again.



The error I get is:

> arg 5 (closure) must be None or tuple

After researching some previous issues, I saw that other people had potentially similar issues using the _Lambda()_. But I didn't get whether there was a solution to this. 

I am running into this issue with both Python 2.7/3.5


Update:

I changed the merge layer. Lambda now takes only one argument, and the output shape is literally defined:

 where  is a list of output layers from models being merged.

Now, when I try to load the model, the error is:

> The layer has never been called and thus it has no defined output shape

Any help would be appreciated.


Gerti


Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [ ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [ x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",stale,"['This seems to be the same problem as #5396.\r\n\r\nTo reproduce this bug, just try to save the model in the variational autoencoder example [here](https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder.py).\r\n\r\nThis definitely has to do with Lambdas, because if you change line 40 to say ""z_mean"" instead of ""z"", this problem goes away.\r\n', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],"['merged_layer = Merge(group_out, mode=lambda x: weighted_merge_fun(x, (num_experts, num_classes)),output_shape=(num_classes,))', 'merged_layer = Merge(group_out, mode=lambda x: weighted_merge_fun(x), output_shape=(7,))', 'group_out']",0,0
593,keras,8926,closed,load_modelCrashing,,,[],[],[],0,0
594,keras,4966,closed,"How to crop ""dinamically""","I'm trying to implement the following architecture with Keras (Theano backend). 

I have a first Sequential network (say S1) which takes an image as input and has 4 linear output, which do correspond to the upper-left and the bottom-right coordinates of a rectangular ""window"" in the input image which is supposed to contain the object I want to identify. 

Once I have those four outputs... I'd like to actually crop the image!

So I thought to take again my image as input in a new Sequential network (say S2) and merge these two networks using a Merge Layer with function mode.

I turned out to be a bit more complicated than I expected and I'm stuck with some Theano errors I can't get rid of.

Here's the relevant part of the code:


    model1 = Sequential()
    model1.add(Convolution2D(64, 3, 3, border_mode='same', input_shape=(1280, 720, 3), activation='relu'))
    # now model.output_shape == (None, 1280, 720, 64)

    # add a 3x3 convolution on top, with 32 output filters:
    model1.add(Convolution2D(32, 3, 3, border_mode='same',activation='relu'))
    # now model.output_shape == (None, 1280, 720, 32)
    model1.add(Flatten())
    model1.add(Dense(4, activation='linear'))

    model2 = Sequential()
    # How can I create a simple ""input"" net?
    model2.add(Reshape((1280, 720, 3), input_shape=(1280, 720, 3)))

    def merger(l):
        image = l[1]
        indexes = l[0]
        index_0 = indexes[:][0]
        index_1 = indexes[:,1]
        index_2 = indexes[:,2]
        index_3 = indexes[:,3]
        cropped_image = image[:,index_0:index_1+1, index_2:index_3+1,:]
        return cropped_image

    merged_model = Sequential()
    merged_model.add(Merge([model1, model2], mode=merger))

And here you have the error:

     <ipython-input-25-3b844142556c> in merger(l)
          11     index_2 = indexes[:,2]
          12     index_3 = indexes[:,3]
     ---> 13     cropped_image = image[:,index_0:index_1+1, index_2:index_3+1,:]

     [...]

    ValueError: ('TensorType could not be cast to have 0 dimensions', TensorType(float32, vector))


What's going on?",stale,"[""Just a thought, but without any kind of checks (e.g. negative check, left/right must be less than right/bottom) you can wind up with a slice like `image[:, 5:2, 6:-1, :]` which would wind up with a generic shape of `(None, 0, 714, 3)`.\r\n\r\nI don't think that has to do with your error, as the above shape with a 0 dimension is valid. But you should check out some examples of the Merge layer, especially with a custom mode, because at the very least you will also need to supply a function for `output_shape` since you are using a custom mode.\r\n\r\nI'd recommend getting it working without doing the cropping first (e.g. just return the image in your merger) as you'll need to do the `output_shape` function as well, and then add in the actual cropping."", ""Actually, now that I'm thinking about it, you're going to run into issues whenever you have `batch_size` > 1. Tensors are essentially multidimensional matrices, so you can't have subtensors of mismatched dimensions. In other words, you can't really combine tensors of shapes (851, 254, 3) and (498, 387, 3) automatically, you would have to pad up to (851, 387, 3) and probably keep a reference list of the valid areas; so depending on what you are trying to do, it will probably just be better to keep the full images plus the diagonal corner info, and not do any cropping. But again, that's dependent on what you are trying to accomplish."", ""Of course I'll have to implement checks to get a valid window (left corner to the left of the right corner, etc.) but that's not the problem here because the error I posted was raised before any attempt of training, that's why I have not supplied an output_shape function in my issue.\r\n\r\nBut actually I had not foreseen the batch_size problem... Getting around it could be complicated."", 'Fair enough, but I still recommend starting smaller.\r\n\r\nThat said, your error is `ValueError: (\'TensorType could not be cast to have 0 dimensions\', TensorType(float32, vector))`. This says that a float32 vector could not be cast to a 0 dimensional item. Looking at the code, and simplifying to a minimal slice attempt:\r\n```python\r\ndef merger(l):\r\n    image = l[1]\r\n    indexes = l[0]\r\n    index_0 = indexes[:,0]\r\n    return image[:, index_0:, :, :]\r\n```\r\n..still throws this error. We can now say that slicing by index_0 is the issue; index_0 is a slice of shape (None, 1), which is a vector. Trying to slice by a vector is nonsensical in Theano, as it expects the slicer index (index_0) to be an integer, which is a 0 dimensional item. So, the error in English is saying: ""Expected an integer, got a vector of floats; cannot cast a vector of floats to an integer.\r\n\r\nIn other words, your issues are:\r\n\r\n- The output of model 1 is float32s, which should be typecasted to integers if you want to use them to slice anything (small issue, but if you got the below issue fixed, you\'d still wind up with an ""Indexes must be integers"" error).\r\n- Tensors are matrices, and you can\'t slice the subtensors into different sizes using a vector\r\n\r\nSo, your options are:\r\n\r\n- code everything as if you\'ll have only a `batch_size` of 1, and can therefore index using 4 integers. There would be no issues with matrices, dimensions, etc.\r\n- don\'t do cropping, just pass the corner data around with the images.', ""I understood the issue and I'm going to follow you advice.\r\nThanks for the patience, I learned something in the process! ;D"", ""Anyway, just for the sake of completeness, I conceived the following workaround:\r\n\r\n    def merger(l):\r\n         image = l[1]\r\n         indexes = T.tensor.iround(l[0])\r\n         index_0 = indexes[:,0]\r\n         index_1 = indexes[:,1]\r\n         index_2 = indexes[:,2]\r\n         index_3 = indexes[:,3]\r\n         nb_of_samples = T.tensor.shape(index_0)[0]\r\n         cropped_image = T.tensor.zeros_like(image)\r\n         for i in range(nb_of_samples):\r\n             cropped_image = T.tensor.setsubtensor(cropped_image[i,min(index_0[i],index_1[i]):max(index_0[i],index_1[i])+1,min(index_2[i],index_3[i]):max(index_2[i],index_3[i])+1,:], image[i,min(index_0[i],index_1[i]):max(index_0[i],index_1[i])+1,min(index_2[i],index_3[i]):max(index_2[i],index_3[i])+1,:])\r\n         return cropped_image\r\n\r\nwhich **almost** does what I expect it to do. In fact\r\n\r\n    'TensorVariable' object cannot be interpreted as an integer\r\n\r\neven though the index_j[i] is TensorVariable with dtype=int64 and scalar dimension.\r\n"", ""You've switched from Symbolic computing (Theano) to python computing via the use of a for loop; python works with Integers/Floats/etc. and Theano works with Tensors. You'll need to either convert everything to integers/numpy arrays, do your loop on the CPU and convert back to Theano tensors (which will in all likelihood break the graph and autodifferentiation) or use the Theano looping functionality via `scan`."", 'If the number of iterations of the loop is small and fixed, then you can\nuse the for loop to build a bigger Theano graph and don\'t use scan.\n\nLe 9 janv. 2017 11:48, ""Pat York"" <notifications@github.com> a écrit :\n\n> You\'ve switched from Symbolic computing (Theano) to python computing via\n> the use of a for loop; python works with Integers/Floats/etc. and Theano\n> works with Tensors. You\'ll need to either convert everything to\n> integers/numpy arrays, do your loop on the CPU and convert back to Theano\n> tensors (which will in all likelihood break the graph and\n> autodifferentiation) or use the Theano looping functionality via scan.\n>\n> —\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/fchollet/keras/issues/4966#issuecomment-271337370>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AALC-6kRoeZG3FbgKwCSB7sikwQeBcR2ks5rQmTEgaJpZM4Ldyfg>\n> .\n>\n', ""@LukeMathWalker the trickier issue you are going to have is when you train. If your network outputs indexes, those are discrete steps, so backprop won't work.\r\n\r\nIf you want to be able to backprop so your network actually learns how to crop, you need to make everything differentiable. That means the crops are real numbers and you interpolate to get fractional cropping.\r\n\r\nDepending on what you're trying to do, you could rescale all of the crops to the same size, and then you could do multiple crops in a batch.""]",[],[],0,0
595,keras,3776,closed,"fit_generator, unexpected output dimension","I'm using latest version of Keras with theano backend.
Here is a short description of my problem (a code that reproduces the problem is also shown):
I have a convolution network with a 1-channel 2D input.
This input layer is shared by two nodes, each performing some convolution operations.
Then outputs of the two nodes are merged by another node, which produces the final output.
I am expecting to get an output dimension of (batch_size, 2x4x9), which is (15, 72).
However, the error message says I am getting (30, 72), which is (2xbatch_size, 2x4x9).
It seems something is wrong with the merge node by looking at ""2xbatch_size"".

The structure of my model is:
![capture](https://cloud.githubusercontent.com/assets/22214494/18573430/e74a2964-7bf5-11e6-9d28-82239e29d293.PNG)

Here is my model:



The error info is pasted here:



I have tried to make this minimum example as simple as possible.
Any idea of where I was doing wrong?
Thanks a lot for your help!
",,[],"["" python\nbatch_size = 15\nx = 2\n\ndef create_batch_generator():\n    while True:\n        yield ({'input': np.random.random((batch_size,1,16,36)).astype(np.float32),\n                'output': np.random.random((batch_size,x*4*9)).astype(np.float32)})\n\n\ndef get_model():\n    model = Graph()\n    model.add_input(name='input',input_shape=(1,16,36))\n    # path 1\n    model.add_node(MaxPooling2D(pool_size=(2,2)),name='node11',input='input')\n    model.add_node(Convolution2D(8,3,3,border_mode='same',subsample=(2,2),activation='relu'),\n                   name='node12',input='node11')\n    # path 2\n    model.add_node(Convolution2D(8,3,3,border_mode='same',subsample=(2,2),activation='relu'),\n                   name='node21',input='input')\n    model.add_node(Convolution2D(8,3,3,border_mode='same',subsample=(2,2),activation='relu'),\n                   name='node22',input='node21')\n\n    # merge\n    model.add_node(Convolution2D(x,1,1,border_mode='same',activation='linear'),\n                   name='node_merge1',merge_mode='concat',concat_axis=0,inputs=['node12','node22'])\n    model.add_node(Flatten(),name='node_merge2',input='node_merge1')\n    model.add_output(name='output',input='node_merge2')\n\n    optimizer = SGD()\n    model.compile(loss='mean_squared_error', optimizer=optimizer)\n    return model\n\n\nmodel = get_model()\nmodel.fit_generator(create_batch_generator(),samples_per_epoch=10000, nb_epoch=100)\n"", ' python\nTraceback (most recent call last):\n  File ""test_merge.py"", line 43, in <module>\n    model.fit_generator(create_batch_generator(),samples_per_epoch=10000, nb_epoch=100)\n  File ""/home/dikai/bin/anaconda2/lib/python2.7/site-packages/keras/legacy/models.py"", line 646, in fit_generator\n    max_q_size=max_q_size)\n  File ""/home/dikai/bin/anaconda2/lib/python2.7/site-packages/keras/engine/training.py"", line 1364, in fit_generator\n    raise e\nValueError: GpuElemwise. Input dimension mis-match. Input 1 (indices start at 0) has shape[0] == 15, but the output\'s size on that axis is 30.\nApply node that caused the error: GpuElemwise{Sub}[(0, 0)](GpuReshape{2}.0, GpuFromHost.0)\nToposort index: 160\nInputs types: [CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix)]\nInputs shapes: [(30, 72), (15, 72)]\nInputs strides: [(72, 1), (72, 1)]\nInputs values: [\'not shown\', \'not shown\']\nOutputs clients: [[GpuCAReduce{pre=sqr,red=add}{0,1}(GpuElemwise{Sub}[(0, 0)].0), GpuElemwise{Composite{((i0 * i1 * i2 * i3) / i4)}}[(0, 3)](CudaNdarrayConstant{[[ 2.]]}, GpuDimShuffle{x,x}.0, GpuDimShuffle{0,x}.0, GpuElemwise{Sub}[(0, 0)].0, GpuElemwise{Mul}[(0, 0)].0)]]\n']",[],0,0
596,keras,11943,closed,The link to deconv paper is broken in Conv2DTranspose comments,"This is very minor, but the second link used as reference in Conv2DTranspose seems to be invalid. 
It can be changed from https://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf to https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf.",Good first issue stat:contributions welcome type:docs,"['Pull request welcome. ', 'I will work on this.', 'https://github.com/keras-team/keras/pull/11947']",[],[],0,0
597,keras,5617,closed,Obtain output at each timestep,"Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).

I need the LSTM to generate words at each timestep which is given as input to the next timestep. Can this be done in keras?
",stale,"['As far as I know, it can not easily be done.\r\n\r\nYou will have to use either theano or tensorflow ""scan"" to allow the output to be fed to the input in a recursive fashion.\r\n\r\nAlternatively you can have a look at https://github.com/datalogai/recurrentshop with parameter readout=True.\r\nBut in its current state, I can not recommend usage of this library.\r\n\r\n', 'you can make the model stateful and predict one time step at a time. The performance is not very bad. As you only require it in the test time which the performance impact is not that great\r\n', ""@unrealwill I didn't quite understand what you meant by 'scan'. \r\n@ParthaEth How would the training be done in this case? What would be the samples and the targets?"", '@AahanSingh By scan I mean http://deeplearning.net/software/theano/library/scan.html\r\nYou train your model with stateful = False the usual way (This mean that during training the generated word is not used as input of the next time step) . Then for generation you build a similar model with stateful = True, and you feed it one time step at a time, and it will give an output which you will feedback in.', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],0,0
598,keras,5164,closed,Automatically closing abandoned issues,"@fchollet I came across something to make life better. There are a handful of tools out there to automatically close issues if they have been abandoned. Here is one example but there are plenty of options.

https://github.com/twbs/no-carrier

A configuration like this might be reasonable:
* Send a warning at 2, 4 and 6 weeks.
* Close after no activity for 8 weeks (other than the warnings), making sure to mark the resolution as something like ""automatic closure"" so it doesn't get confused with things that are manually closed. It would be easy to browse the automatic closures if you wanted to.
* If something gets closed by accident, someone could always reopen it
* Make some exceptions for any issues you want to pin and keep active.

There are currently 1,945 issues (not including this one) and rising. Does this sound like something reasonable to implement?

Cheers",,"['Yes, sounds good to me.', ""I'll try it out and setup a demo in a test repository. If it looks good, we would just need to add a cron job to travis."", '@fchollet The available tools weren\'t exactly what we needed, so I just put together a new module. I haven\'t done the PyPI release yet but I\'ve tried it out and it looks good. Please take a look at the docs or code and let me know if there are any additional features that would be useful.\r\n\r\nhttps://github.com/bstriner/github-bot-close-inactive-issues\r\n\r\nThe bot posts warnings every `warning_frequency` days starting at `warning_start` days after no activity. The bot closes issues after `closing` days. It can also mark that issue with a label like ""closed_automatically"" so it will be easy to search. If you run the bot with `--test` it will log the actions it would take but won\'t actually do anything.\r\n\r\nI also put in an option `first_closing_date` so this can be phased in. It will only post warnings and not close any issues (no matter how old) until that date, so people can have a heads-up.\r\n\r\nI did some testing in a separate repo. I turned the delay to 0 so it would immediately close the issues so I could test. This is what those automatically closed issues look like:\r\n\r\nhttps://github.com/bstriner/github-bot-test/issues?q=is%3Aissue+is%3Aclosed\r\n\r\nIn terms of setup:\r\n* Create a new account for the bot, something like `keras-bot`\r\n* Generate a token for the account so it can connect with a high rate limit\r\n* Confirm an email address for the bot so it doesn\'t get flagged\r\n* Add the bot as a contributor to Keras so it will have permission to close issues\r\n* Edit the configuration file and run `github-close-inactive-issues.py`\r\n\r\n**My only concern is how to secure the bot account password.** If the bot password is stored somewhere on the main travis server, someone could possibly get to it. Maybe we could setup travis to be secure but I would feel better if the bot was running on a different machine. Only need to run it every week or so for maintenance. Would you want to setup a local cron job so you know that password can\'t get out?', 'Maybe I should add some special handling to ignore issues created by @fchollet and issues with certain labels like help wanted.\r\n\r\nAlso, regarding setup, you would have to add a new label to the project that we could use for tracking the automatically closed issues.\r\n\r\nCheers', 'I just ran `--test` against keras. Here is what the results look like. Tons of issues to potentially close.\r\n\r\nThere are so many issues, I hit the rate limit when I was nearing the end.\r\n\r\nThe rate resets every hour, so I will add some logic to wait for the rate limit to reset. That means it might take 2-3 hours to run, at least until the number of issues gets down to a manageable number.\r\n\r\n> 2017-01-25 02:27:27,540 - root - INFO - Checking rate limit for user github-bot-bot\r\n> 2017-01-25 02:27:27,861 - root - INFO - Limit: 5000, Remaining: 4915, Reset: 2017-01-25 02:56:54\r\n> 2017-01-25 02:27:27,862 - root - INFO - Starting close_inactive_issues\r\n> 2017-01-25 02:27:27,864 - root - INFO - Repo: fchollet/keras, User: github-bot-bot\r\n> 2017-01-25 02:27:27,864 - root - INFO - warning_start: 14, warning_frequency: 7, closing: 56\r\n> 2017-01-25 02:28:11,351 - root - INFO - Issue 4987: warning posted, inactive for 14 days, will be closed after 2017-03-08 03:59:58\r\n> 2017-01-25 02:28:12,773 - root - INFO - Issue 4984: warning posted, inactive for 14 days, will be closed after 2017-03-08 01:16:20\r\n> 2017-01-25 02:28:13,115 - root - INFO - Issue 4982: warning posted, inactive for 14 days, will be closed after 2017-03-07 21:48:54\r\n> 2017-01-25 02:28:13,513 - root - INFO - Issue 4981: warning posted, inactive for 14 days, will be closed after 2017-03-08 00:44:50\r\n> 2017-01-25 02:28:14,171 - root - INFO - Issue 4978: warning posted, inactive for 14 days, will be closed after 2017-03-07 13:50:14\r\n> 2017-01-25 02:28:14,513 - root - INFO - Issue 4977: warning posted, inactive for 14 days, will be closed after 2017-03-07 11:04:31\r\n> 2017-01-25 02:28:14,845 - root - INFO - Issue 4976: warning posted, inactive for 14 days, will be closed after 2017-03-07 10:42:54\r\n> 2017-01-25 02:28:15,733 - root - INFO - Issue 4974: warning posted, inactive for 15 days, will be closed after 2017-03-07 03:59:06\r\n> 2017-01-25 02:28:16,479 - root - INFO - Issue 4969: warning posted, inactive for 15 days, will be closed after 2017-03-06 08:08:11\r\n> \r\n', 'Added some logic to periodically check the remaining rate limit and sleep until it is reset. Should be close to ready now.', 'With current settings, the script took about 2 hours to run and wants to close 1,809  issues that are inactive. This list will give you an idea of about how long each issue has been inactive.\r\n\r\nhttps://gist.github.com/bstriner/967b45c6d854d257055bcf9d86f44d99\r\n\r\nCheers!', 'Just released v0.0.2 to PyPI of `github-bot-close-inactive-issues`. I added the ability to ignore issues based on the creator and based on the label. I also fixed the entry points so I can use it on linux and windows.\r\n\r\nhttps://pypi.python.org/pypi?:action=display&name=github-bot-close-inactive-issues\r\nhttps://github.com/bstriner/github-bot-close-inactive-issues\r\n\r\nWith this update, you can just `pip install github-bot-close-inactive-issues` and that will put `github-rate-limit` (which just checks the rate limit) and `github-close-inactive-issues` (which closes the issues) onto your path.\r\n\r\nSo, I would envision the bot skipping anything created by @fchollet and anything with help wanted or bug as the label.\r\n\r\nEven if we only automatically close issues after six months, that will still knock a ton of issues out of Keras.\r\n\r\nAre there any other features or contingencies to consider?\r\n\r\nCheers', ""Kind of off topic, but I've had luck parring down the people that post multiple implementation questions as issues using the below code if/when I comment:\r\n\r\n```\r\n<br />\r\n---\r\n\r\nIf your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.\r\n```\r\n\r\nPeople generally aren't reading the stuff that is in the issue template, but (unsurprisingly) they *do* read answer people give them ;)\r\n\r\nIt looks like the below: \r\n\r\n<br />\r\n\r\n---\r\n\r\nIf your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue."", ""@patyork not off topic at all. That text or something similar should be in the warning for the bot.\r\n\r\nhttps://github.com/bstriner/github-bot-close-inactive-issues/blob/master/github_bot.yml\r\n\r\nI've started playing with the Github API and you could automate whatever you want. Crazy idea: use Github labels to mark issues as bugs vs. implementation questions and train a classifier to do some automation.\r\n\r\nGoogle group also seems like a good place to talk. I'm not a huge fan of slack yet. Maybe include SO, slack and google group in the message?"", '@fchollet I did some poking around on the best way to configure something like this. Create a separate github repo with just a travis.yml and a github bot configuration file. Enable travis, disable builds on pull requests and enable cron builds. Configure the bot through travis environment variables.\r\n\r\nThe travis environment variables will be hidden to other users and they will not be able to write malicious code to access the variables if you disable builds on pull requests. That means the bot needs to run on a separate travis instance than the rest of keras.\r\n\r\nSeems like it should be reasonable to get the ball rolling on the backlog. Still interested?', 'Yes, I still think this would be a good feature to have. Albeit it should\nbe paired with a reasonable level of formal handling of Github issues\n(currently relatively few people look at them, and not in a structured\nfashion).\n\nOn 8 April 2017 at 14:58, Ben <notifications@github.com> wrote:\n\n> @fchollet <https://github.com/fchollet> I did some poking around on the\n> best way to configure something like this. Create a separate github repo\n> with just a travis.yml and a github bot configuration file. Enable travis,\n> disable builds on pull requests and enable cron builds. Configure the bot\n> through travis environment variables.\n>\n> The travis environment variables will be hidden to other users and they\n> will not be able to write malicious code to access the variables if you\n> disable builds on pull requests. That means the bot needs to run on a\n> separate travis instance than the rest of keras.\n>\n> Seems like it should be reasonable to get the ball rolling on the backlog.\n> Still interested?\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/fchollet/keras/issues/5164#issuecomment-292748471>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AArWbyVPLG4u3QH7FvZNUdK4w_V4Avkhks5ruALwgaJpZM4LsxTZ>\n> .\n>\n', '@fchollet You could configure the bot to just post warnings on stale issues and not actually close anything. That way you have some automated way to keep the ball rolling on issues and see if they are still active without accidentally closing something legit.\r\n\r\nThe bot could be a good start but there has to be a better way to handle issues. There are some actual legit issues buried in stacks of questions and there is no way to get a good handle on what is what.\r\n\r\nIt would be nice to use tags to differentiate, but you either do that all yourself or give other people the ability to nuke keras.\r\n\r\nGithub permissions are junk. It is basically all or none. I\'m thinking about making some other apps to help with this kind of issue (not keras-specific).\r\n\r\nAutomatic closing and better permissions should be a part of Github. It would be like one line of SQL. However, while those things aren\'t in Github, it is something that can be built using the API.\r\n\r\nWhat would you think of a Github permissions app? You would be able to add users and give them fine-grained permissions like adding tags, removing tags, closing issues, etc. The app would store the permissions table, and proxy commands to an owner account if they pass the permissions check.\r\n\r\nThe users view issues and PRs through the proxy app. Commands to comment go through their account. Commands to close, tag, merge, or whatever are proxied to an owner account that posts a comment like ""closed on behalf of [username]"" if permissions pass.\r\n\r\nThe bot for warnings and closings on issues could be rolled into the app as well.\r\n\r\nDoes that seem like something that would actually be useful on a broader scale? Wouldn\'t be hard to put something together if I can find some cheap hosting.\r\n\r\nCheers', ""@fchollet Glad you got something up and running. probot-stale looks like a javascript clone of my python repo but at least I don't have to support anything this way. It looks like they have some nice features. Please let me know any thoughts on how it works out.\r\n\r\nCheers""]",[],[],0,0
599,keras,4982,closed,Multichannel Images,"Would it be possible to support images with more channels than just 3 or 1. I'm currently working on a project where I have images with 8 channels, and currently I have to truncate all but 3 of the channels. Any way around this?",stale,"[""There shouldn't be an issue. The 2D convolutional layers already support this since a stack of feature maps has identical structure to a multichannel image. Without seeing your code I can't help any more."", ""Oh sorry I wasn't specific enough, I mean with saving images using the image processing library, \r\nso for example i have an image c of shape `(64, 64, 8)` and I want to call \r\n` image.array_to_img(c)` and I get \r\n`ValueError: ('Unsupported channel number: ', 8)`"", 'You can\'t save an 8 channel image as an RGB image (or really as any image format that I know of). You could save the Numpy arrays yourself (via pickle or h5).\r\n\r\nSimply put, standard image formats (JPG, BMP, PNG) generally only support 1, 3, or 4 channel images, where each channel has a specific meaning; 8 channels don\'t fit as an ""image"" per say.', ""It seems like you want to use the real-time data augmentation and that doesn't support multichannel images. Right? From looking at the code, it should be really easy to modify it to do what you want. I doubt any changes you make will make it into mainline Keras though, since it's a pretty niche issue."", ""I don't think that it is a niche issue. There are lots of people looking for whys to do data augmentation on 3D images (MRI, CT scans, etc.). That is how I found my way to this discussion. As I have been reading many people just treat the three dimension as just many (10s) of channels. If tools such as ImageDataGenerator could handle more channels it would be used."", ""Definitely agree with this. I'm working with CT data and I've stacking images in the third channel so that I have 512x512x10 inputs into my network. "", 'Any update on this?', '@SimonWalsh1000 Do you mind explaining how you fed the images into keras? Currently I am working with a hyperspectral image of 220 channels. My image is blocked into 5x5x220 blocks. That is 5 pixels by 5 pixels by 220 channels. Can you provide insight into how you formatted your code? Thank you.', 'Also trying to do this for satellite imagery with many bands, plus supporting GIS channels. At the moment I have a terrible hack of creating a new generator for every channel, with a random seed.']",[],[],0,0
600,keras,1625,closed,TimeStopping callback,"I've been using this callback for stopping training after a fixed period of time, useful when testing different architectures consecutively. Is it worth contributing?

One thing I noticed was  only stops the model after an epoch, but with some training where the epochs are very long you might want to stop after a batch.


",,"['@kylemcdonald 👍  yes, this is super helpful. Especially if you run your jobs on a HPC cluster where a 24h job limit applies ;-)\r\n\r\nI would even add `safety_factor` so that the training stops earlier by a factor of the average duration per epoch:\r\n\r\n```python\r\nclass TimedStopping(Callback):\r\n    \'\'\'Stop training when enough time has passed.\r\n    # Arguments\r\n        seconds: maximum time before stopping.\r\n        safety_factor: stop safety_factor * average_time_per_epoch earlier\r\n        verbose: verbosity mode.\r\n    \'\'\'\r\n    def __init__(self, seconds=None, safety_factor=1, verbose=0):\r\n        super(Callback, self).__init__()\r\n\r\n        self.start_time = 0\r\n        self.safety_factor = safety_factor\r\n        self.seconds = seconds\r\n        self.verbose = verbose\r\n        self.time_logs = []\r\n\r\n    def on_train_begin(self, logs={}):\r\n        self.start_time = time.time()\r\n\r\n    def on_epoch_end(self, epoch, logs={}):\r\n        elapsed_time = time.time() - self.start_time\r\n        self.time_logs.append(elapsed_time)\r\n\r\n        avg_elapsed_time = float(sum(self.time_logs)) / \\\r\n            max(len(self.time_logs), 1)\r\n\r\n        print("" "", self.seconds - self.safety_factor * avg_elapsed_time)\r\n        if elapsed_time > self.seconds - self.safety_factor * avg_elapsed_time:\r\n            self.model.stop_training = True\r\n            if self.verbose:\r\n                print(\'Stopping after %s seconds.\' % self.seconds)\r\n```\r\n\r\nare submitting a PR?\r\n\r\n', 'Please consider adding it to the contrib repo: https://github.com/farizrahman4u/keras-contrib', ""@fchollet didn't know about contrib. Maybe it `contrib` would get more attention if you would move both `keras` and `contrib` into a new keras github organisation."", 'Thanks @fchollet closing this and moving to https://github.com/farizrahman4u/keras-contrib/issues/87']","["" python\nimport time\nfrom keras.callbacks import Callback\nclass TimedStopping(Callback):\n    '''Stop training when enough time has passed.\n    # Arguments\n        seconds: maximum time before stopping.\n        verbose: verbosity mode.\n    '''\n    def __init__(self, seconds=None, verbose=0):\n        super(Callback, self).__init__()\n\n        self.start_time = 0\n        self.seconds = seconds\n        self.verbose = verbose\n\n    def on_train_begin(self, logs={}):\n        self.start_time = time.time()\n\n    def on_epoch_end(self, epoch, logs={}):\n        if time.time() - self.start_time > self.seconds:\n            self.model.stop_training = True\n            if self.verbose:\n                print('Stopping after %s seconds.' % self.seconds)\n""]",['self.model.stop_training'],0,0
601,keras,5896,closed,Cannot use Keras in threads,"I'm using Ubuntu 16.04, Python 3.5.2, Keras 2.0.1 and Tensorflow 1.01. 

Keras/tensorflow crash when using threads. The following code is a simplified version of what I am trying to do but recreates the crash.

https://gist.github.com/eyesonlyhack/c43dea734f872a9c45da8587eefec581

I found a way to get around this issue but think this is probably a bug in Keras. My workaround is:
https://gist.github.com/eyesonlyhack/2f0b20f1e73aaf5e9b83f49415f3601a

",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', 'Calling `K.clear_session()` at the beginning or at the end of the function also works.', '> Calling `K.clear_session()` at the beginning or at the end of the function also works.\r\n\r\ncan you get me more details? what do you mean the beginning or at the end of the function?\r\nwhat do you mean by ""function""?']",[],[],0,0
602,keras,4740,closed,How to control the number of threads when running sequential with Keras,"I try to run sequential with Keras; I found that the thread running sequential model generated about 13 sub-threads. How to control the number of sub-threads? 

part of my code: 
model = Sequential()
model.add(...)
.....
model.compile(loss='binary_crossentropy', optimizer=params['optimizer'])
model.fit(.....)

Please help me with this problem. Thanks in advance!",stale,"['More detail to be added, the backbench for my Keras is Tensor Flow.', ""This is what I use. \r\n\r\n```\r\nfrom keras import backend as K\r\nimport tensorflow as tf\r\n\r\nconfig = tf.ConfigProto(intra_op_parallelism_threads=args.jobs, \\ \r\n                        inter_op_parallelism_threads=args.jobs, \\\r\n                        allow_soft_placement=True, \\\r\n                        device_count = {'CPU': args.jobs})\r\nsession = tf.Session(config=config)\r\nK.set_session(session)\r\n```\r\n""]",[],[],0,0
603,keras,3486,closed,max mode can not be used in merge function ? Why ?,"merge, Not Merge

> > > merge([forwards, backwards], mode='max')
> > > Traceback (most recent call last):
> > >   File ""<stdin>"", line 1, in <module>
> > >   File ""/home/job/analyse/env/lib/python2.7/site-packages/keras/engine/topology.py"", line 1490, in merge
> > >     name=name)
> > >   File ""/home/job/analyse/env/lib/python2.7/site-packages/keras/engine/topology.py"", line 1148, in **init**
> > >     self.add_inbound_node(layers, node_indices, tensor_indices)
> > >   File ""/home/job/analyse/env/lib/python2.7/site-packages/keras/engine/topology.py"", line 543, in add_inbound_node
> > >     Node.create_node(self, inbound_layers, node_indices, tensor_indices)
> > >   File ""/home/job/analyse/env/lib/python2.7/site-packages/keras/engine/topology.py"", line 154, in create_node
> > >     output_masks = to_list(outbound_layer.compute_mask(input_tensors, input_masks))
> > >   File ""/home/job/analyse/env/lib/python2.7/site-packages/keras/engine/topology.py"", line 1372, in compute_mask
> > >     raise Exception('Invalid merge mode: {}'.format(self.mode))
> > > Exception: Invalid merge mode: max
",,"['It seems that the ""max"" mode for merge is not supported if you mask the inputs. The support for ""max"" mode in Merge (#3128) was added after the support for masking (#2413) and it was apparently forgotten to add ""max"" to the ""compute_mask"" method of Merge.\n']",[],[],0,0
604,keras,4862,closed,`K.int_shape` bug,"It looks like commit https://github.com/fchollet/keras/commit/2a3d4722c21d99d882b2cbc2da451108147fe1c4 introduced a bug in (at least) .  

Rolling back the commit fixes this problem, but I'm not sure whether this breaks something somewhere else.",stale,[],[],['examples/mnist_hierarchical_rnn.py'],0,0
605,keras,6412,closed,New Loss function and Metric to monitor best weights,"Hello I am still exploring keras in the past few months so I am new here. I like it so much thanks for @fchollet  . I just have 3 problem now.
1) How we can create new loss function?[better with example]
In here for example I have 4 pairs of coordinate  which allocated in 1x8 array as the output of network. I want to create euclidean loos function which compare  and  so I will have 4 distance as the result comparing every point in the array. The problem is I just know that I must use Keras Backend operation. Now the proble is how can I iterate through the tensor? for example I am taking the mean of all 4 distance:









It still have error, I know maybe because of tensor we must use backend operation, but I dont know what to do.
2) How can we create new metrics, for example we define our custom metric of accuracy? like in my case 
3) How we monitor using new metrics and save the best weight using our custom metrics?
Any suggestion is welcomed. Thank you.",stale,"['Here you find an example of how a custom dice function is as loss function:\r\nhttps://github.com/EdwardTyantov/ultrasound-nerve-segmentation/search?utf8=%E2%9C%93&q=dice_coef&type=', '@Danielhiversen Yes I have check it before post this question. From that post I know that it must using Keras Backend. But I just ask how about we need iterate through index?, because in my case I need taking it by index every 2 elements like `d1 = sqrt((predx1-truex1)^2+(predy1-truey1)^2)` where `truex1 = true[0], and truey1 = true[1] ` `predx1 = pred[0], and predy1 = pred[1] ` basically it becomes `d1 = sqrt((pred[0]-true[0])^2+(pred[1]-true[1])^2)` , `d2 = sqrt((pred[2]-true[2])^2+(pred[3]-true[3])^2)`, then compute `d3 and d4`. For example if I just take one by one not per 2 elements in array, I think I just simply can do like in that example.', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],"['p = x1,y1,x2,y2,x3,y3,x4,y4', 'y_pred', 'y_true', 'def euclidean(y_true, y_pred):', '    ktotal = K.zeros((1,4))', '    i = 0', '    j = 0', '    while(i<7):', '        ktotal[j] = K.sqrt(K.square(y_true[i]-y_pred[i]) + K.square(y_true[i+1]-y_pred[i+1]))', '        j = j + 1', '        i= i + 2', '    return K.means(ktotal)', 'acc = 1-(max distance/mean(euclidean)']",0,0
606,keras,1872,closed,resume training from previous epoch,"I saved the model and weights after each epoch using callbacks.ModelCheckpoint. I want to train it again from the last epoch.
How to set the model.fit() command to start from the previous epoch?
",,"['Do you want to do something special with the history? If not, you can just call `.fit` one or several times and you will be able to continue to train the model. If you want to continue the training in another process, you just have to load the weights and call `model.fit()`.\n', 'when I call model.fit()  after loading models and weights , it showing epoch  = 1. If I stop the training at 100 epoch. I want to resume the training with epoch=101.\n', ""I think it is no matter whether it SHOWs the training is at epoch = 1 or epoch = 101.\nAs far as I know, the model itself doesn't save the EPOCH information into model file.\nIf you have loaded the correct previous model (the model should have been saved with epoch number), it should be no problem on continuing your training process.\n"", 'thank u\n', '@ymcui is right, the label of `epoch` is only a name for the iterations in the current `fit`. Sorry when I said history I meant the history dictionary the `fit` method returns. I think #1868 is basically the same question. If you think it resolves your problem please close the issue!\n', ""But there is a problem with this approach. What about hyper parameters that change according to epoch, say learning rate with a decay. Just restarting it with fit method doesn't take that into account.\n"", 'yeah, this happens to me when I resume the training process by loading weights.\r\n\r\nI was training resnet18 with imagenet dataset, the model saved the weights at 1st epoch with lr=0.1 at beginning. I stopped it, then tried the resume functionality, and it turns out that the model starts with the same lr=0.1, and the loss increase for each iteration. To set the lr to the state of the 1st epoch, I changed the lr according to SGD lr update func: lr = lr * (1. / (1+decay*iterations)), however, it didnt work, the loss sill increases, but slower than with lr=0.1. Probably I should still lower the lr, but I dont understand why the loss still increase even the lr is set accordantly.', 'Try the **initial_epoch** argument in **.fit** method.', ""using initial_epoch didn't work in this case"", ""> But there is a problem with this approach. What about hyper parameters that change according to epoch, say learning rate with a decay. Just restarting it with fit method doesn't take that into account.\r\n\r\nSetting the `initial_epoch` in `fit_generator` is not enough to solve this problem when using the `ReduceLROnPlateau` callback because there's no way for the callback to know what the learning rate should be without having the history of the previous (ie. before resuming training) epochs. Perhaps the callback constructor should have an optional `history` parameter that can be used to correctly initialize the learning rate and the `wait` variable (see https://github.com/fchollet/keras/blob/ab3b93e8dd103f1d9729305825791a084c7c8493/keras/callbacks.py#L744)\r\n"", 'Besides using the `initial_epoch` argument of `fit`, I re-wrote the history callback:\r\n\r\n```\r\nclass History(Callback):\r\n    """"""\r\n    Callback that records events into a `History` object.\r\n\r\n    This callback is automatically applied to\r\n    every Keras model. The `History` object\r\n    gets returned by the `fit` method of models.\r\n    """"""\r\n\r\n    def on_train_begin(self, logs=None):\r\n        if not hasattr(self, \'epoch\'):\r\n            self.epoch = []\r\n            self.history = {}\r\n\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        logs = logs or {}\r\n        self.epoch.append(epoch)\r\n        for k, v in logs.items():\r\n            self.history.setdefault(k, []).append(v)\r\n```\r\n\r\nThis allows using the same callback and it just appends to the end. @fchollet should I post a pull request for this? It seems to me that this is more useful than the current behaviour of overwriting the\r\nlogs in `on_train_begin`.', ""@MartinThoma ,\r\nOne would probably need to replace [this line](https://github.com/fchollet/keras/blob/2.0.4/keras/engine/training.py#L1099) with\r\n```\r\n    if initial_epoch==0:\r\n        self.history = cbks.History()\r\n```\r\nto make your suggestion work, right? I've tried to make this stuff work, and eventually ran into a feeling that too many different things should be changed, see #6697. What do you think?"", 'if you want to resume from epoch 101 ,simply use ""initial_epoch = 101"" in model.fit().\r\n\r\ninitial_epoch: Epoch at which to start training (useful for resuming a previous training run)', 'Seems that tensorflow estimators also support resuming training. [""Since the state of the model is persisted (in model_dir=PATH above), the model will improve the more iterations you train it, until it settles""](https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html)', 'Related question: What happens to all the gradient computations that rely on a history of the gradients (when momentum is present, such as in ADAM and most gradient descent algorithms)? Does the checkpoint store these as well? Thanks!', '@bupedroni: As far as I know, every time I loaded the existing model, all the hyperparameters were set to default values. \r\n\r\nBest way to resume is to write a custom callback and store all the hyperparameters and then start the training as mentioned by @MartinThoma  ', ""@MartinThoma I'd like a pull request implementing that, basically I'm training a model, but if I notice that the metrics haven't diverged I'd like to train for another x epochs. And also be able to plot the history overall in an additive way.\r\n\r\nFor now I'm just accumulating histories like this https://www.kaggle.com/morenoh149/keras-continue-training"", 'Still have this issues... any update on it? ', 'anything new here? ', 'just port your code to pytorch :laughing:', 'Ya. That actually worked for me. 2 years and counting. ', ""> I think it is no matter whether it SHOWs the training is at epoch = 1 or epoch = 101.\r\n> As far as I know, the model itself doesn't save the EPOCH information into model file.\r\n> If you have loaded the correct previous model (the model should have been saved with epoch number), it should be no problem on continuing your training process.\r\n\r\nSo does that mean if i call\r\nmodel.fit(epochs = 20)\r\n\r\nand \r\n\r\nmodel.fit(epochs=5)\r\nmodel.fit(epochs=5)\r\nmodel.fit(epochs=5)\r\nmodel.fit(epochs=5)\r\n\r\nboth are same ??"", ""> > I think it is no matter whether it SHOWs the training is at epoch = 1 or epoch = 101.\r\n> > As far as I know, the model itself doesn't save the EPOCH information into model file.\r\n> > If you have loaded the correct previous model (the model should have been saved with epoch number), it should be no problem on continuing your training process.\r\n> \r\n> So does that mean if i call\r\n> model.fit(epochs = 20)\r\n> \r\n> and\r\n> \r\n> model.fit(epochs=5)\r\n> model.fit(epochs=5)\r\n> model.fit(epochs=5)\r\n> model.fit(epochs=5)\r\n> \r\n> both are same ??\r\n\r\nYes, they are equivalent. At least that is what I found using the TensorFlow Keras API in TensorFlow 2.0"", 'How can I get the epoch at which model was saved in ModelCheckpoint ? ', 'save epoch number in the name of the model. Fetch that number with regex when resuming training.', 'I managed to do this with an optimizer whose learning rate depends on the number of iterations eg Adam.\r\n\r\nHere is the pseudo-code:\r\n```python\r\n...\r\nif os.path.isfile(checkpoint_path+"".index""):\r\n    # This loads `(root).optimizer.iter`from the checkpoint\r\n    model.load_weights(checkpoint_path)\r\n\r\n# Recover the iterations from the model and convert to epochs\r\ninitial_epoch = model.optimizer.iterations.numpy() // STEPS_PER_EPOCH\r\ncallback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True)\r\nmodel.fit(train_data, epochs=NUM_EPOCHS, initial_epoch=initial_epoch,\r\n                 callbacks=[callback])\r\n```\r\n\r\nHope this helps :-)', 'I got tired of this so I ended up writing a Keras wrapper that autosaves and restores the epoch number, training history, and model weights:\r\n\r\n`pip install keras-buoy`\r\n[Link to Github project](https://github.com/dorukkarinca/keras-buoy)\r\n\r\nLet me know what you think. PRs more than welcome.', ""@dorukkarinca is this handled in tensor flow v2? that's supposed to supersede keras "", '@morenoh149 not to the best of my knowledge. This wrapper wraps tensorflow.keras anyway.', '@dorukkarinca  UwU and Orz . Your wrapper help me so much. I didnt know why I wasted 1 week to retrain at start.']",[],[],0,0
607,keras,4869,closed,Object Detection using Keras,"Can you please advice how to to do Object Detection using Keras?
",stale,"['What problem exactly are you looking at? There seems to be a useful post about object recognition here:\r\n\r\nhttp://machinelearningmastery.com/object-recognition-convolutional-neural-networks-keras-deep-learning-library/\r\n\r\nAre you trying to get bounding boxes, multiple labels, or just single labels? What is your training set?\r\n\r\nCheers,\r\nBen', '@bstriner \r\nI need to get bounding boxes for different objects from different classes in a single image.\r\n\r\n', '@Walid-Ahmed you got any resource?', '@santoshgsk \r\nI got a dataset of 20,000 images.', ""@Walid-Ahmed you'll want to start with looking around sites like arxiv to see what some of the latest models are. If you find something specific you want to implement someone can help you.\r\n\r\nSome interesting ideas:\r\nhttps://arxiv.org/pdf/1312.2249v1.pdf\r\nhttps://pdfs.semanticscholar.org/713f/73ce5c3013d9fb796c21b981dc6629af0bd5.pdf\r\n\r\nPredicting multiple boxes normally involves some post-processing to count the boxes from the neural network activations. A relatively simple approach is to just train a CNN to predict pixel or superpixel labels, and draw a box around the pixels. Another simple approach is to train a sliding window at multiple scales. You then select local maxima, or some other subset of windows.\r\n\r\nMulticlass multiple boxes can get trickier. In one example, you train a network to not only produce masks, but label left, right, up and down. This helps disambiguate touching objects.\r\n\r\nCheers"", 'The [Overfeat paper](https://arxiv.org/abs/1312.6229) is certainly interesting in this context (I hope somebody wants to [write a summary](http://www.shortscience.org/paper?bibtexKey=journals/corr/1312.6229)?)', '@Walid-Ahmed Any luck with the bounding box approach?', 'Take a look at the below. Probably what you are looking for.\r\nhttps://github.com/yhenon/keras-frcnn ', '@iamsiva11 \r\nI am  now using the code from\r\nhttps://github.com/rykov8/ssd_keras', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', ""How does one go about handling variable number of objects per image for this problem of object detection?\r\n\r\nWe have to have a fixed model to train, so I'm stumped"", '@gauravk97  The region proposal network used in object detection has a fixed number of candidate bounding boxes in which objects can be found so the network structure can still be fixed.', ""Tensorflow's object detection API is the best resource available online to do object detection. I have a small [blog post](https://abhijit-2592.github.io/Keras-with-TFODAPI/ ) that explains how to integrate Keras with the object detection API, with this small trick you will be able to convert any classification model trained in Keras to an object detection model using the API."", 'Can you please advice how to to do single label Object Detection and localization  using Keras?']",[],[],0,0
608,keras,4479,closed,Model gives same answers,"
before predicting give 


to different vectors",,"['@nazandr Can you post what your loss for each epoch looks like?  Are you missing a section at the end of your post?', '```\r\nEpoch 1/1\r\n490/490 [==============================] - 5s - loss: 0.6829     \r\n1\r\nEpoch 1/1\r\n490/490 [==============================] - 5s - loss: 0.6702     \r\n2\r\nEpoch 1/1\r\n490/490 [==============================] - 5s - loss: 0.6692     \r\n3\r\nEpoch 1/1\r\n490/490 [==============================] - 5s - loss: 0.6689     \r\n4\r\nEpoch 1/1\r\n490/490 [==============================] - 5s - loss: 0.6686     \r\n5\r\nEpoch 1/1\r\n490/490 [==============================] - 5s - loss: 0.6682     \r\n6\r\nEpoch 1/1\r\n490/490 [==============================] - 5s - loss: 0.6683     \r\n7\r\nEpoch 1/1\r\n490/490 [==============================] - 5s - loss: 0.6685     \r\n8\r\nEpoch 1/1\r\n490/490 [==============================] - 5s - loss: 0.6685     \r\n9\r\nEpoch 1/1\r\n490/490 [==============================] - 5s - loss: 0.6691     \r\n10\r\nEpoch 1/1\r\n490/490 [==============================] - 5s - loss: 0.6713     \r\n11\r\nEpoch 1/1\r\n490/490 [==============================] - 5s - loss: 0.6680     \r\n12\r\nEpoch 1/1\r\n490/490 [==============================] - 5s - loss: 0.6684     \r\n13\r\nEpoch 1/1\r\n490/490 [==============================] - 5s - loss: 0.6685     \r\n14\r\nEpoch 1/1\r\n490/490 [==============================] - 5s - loss: 0.6683 \r\n```', ""```\r\nhabrFeed = feedparser.parse('https://habrahabr.ru/rss/feed/posts/5d0c9b4397559e2e7cb380b29ec8151b/')\r\n\r\nprint ('Start predicting...')\r\nfor i in range(len(habrFeed.entries)):\r\n\ttitleVec = []\r\n\ttitleVec.append(text2vec(habrFeed.entries[i].title))\r\n\ttitleVec = np.reshape(titleVec, (len(titleVec),30))\r\n\ttitleVec = titleVec / float(lenVocab)\r\n\tlinkVec = []\r\n\tlinkVec.append(text2vec(habrFeed.entries[i].link))\r\n\tlinkVec = np.reshape(linkVec, (len(linkVec),30))\r\n\tlinkVec = linkVec / float(30)\r\n\tpred = model.predict_on_batch([linkVec,titleVec])\r\n\tprint(pred)\r\n\tif pred[0,0] > 0.5:\r\n\t\tprint ('\\n' , habrFeed.entries[i].title)\r\n\t\tprint ('понравилась')\r\n\telse:\r\n\t\tprint ('\\n' , habrFeed.entries[i].title)\r\n\t\tprint ('нет')\r\n\r\n```\r\nIt is predicting part of the code"", ""Your model isn't learning.  Maybe you have bad data or the learning rate\nset to high.  Please close this ticket as this is not a keras issue.\n\nOn Wed, Nov 23, 2016 at 10:05 AM, nazandr notifications@github.com wrote:\n\n> Epoch 1/1\n> 490/490 [==============================] - 5s - loss: 0.6829\n> 1\n> Epoch 1/1\n> 490/490 [==============================] - 5s - loss: 0.6702\n> 2\n> Epoch 1/1\n> 490/490 [==============================] - 5s - loss: 0.6692\n> 3\n> Epoch 1/1\n> 490/490 [==============================] - 5s - loss: 0.6689\n> 4\n> Epoch 1/1\n> 490/490 [==============================] - 5s - loss: 0.6686\n> 5\n> Epoch 1/1\n> 490/490 [==============================] - 5s - loss: 0.6682\n> 6\n> Epoch 1/1\n> 490/490 [==============================] - 5s - loss: 0.6683\n> 7\n> Epoch 1/1\n> 490/490 [==============================] - 5s - loss: 0.6685\n> 8\n> Epoch 1/1\n> 490/490 [==============================] - 5s - loss: 0.6685\n> 9\n> Epoch 1/1\n> 490/490 [==============================] - 5s - loss: 0.6691\n> 10\n> Epoch 1/1\n> 490/490 [==============================] - 5s - loss: 0.6713\n> 11\n> Epoch 1/1\n> 490/490 [==============================] - 5s - loss: 0.6680\n> 12\n> Epoch 1/1\n> 490/490 [==============================] - 5s - loss: 0.6684\n> 13\n> Epoch 1/1\n> 490/490 [==============================] - 5s - loss: 0.6685\n> 14\n> Epoch 1/1\n> 490/490 [==============================] - 5s - loss: 0.6683\n> \n> —\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/fchollet/keras/issues/4479#issuecomment-262538991,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ALarq0e8tI-agM3WNcaQT8oCzg--0Xjnks5rBFZXgaJpZM4K6Dod\n> .\n""]","[""\r\nimport csv\r\nfrom keras.preprocessing import text as prep\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Activation, Dropout, Embedding, LSTM\r\nfrom keras.layers import Convolution1D, GlobalMaxPooling1D, Merge\r\nfrom keras.optimizers import RMSprop\r\nfrom keras.utils import np_utils\r\nfrom keras.callbacks import ModelCheckpoint\r\nimport numpy as np\r\n\r\nf = open('/Users/andrey/Projects/News-parser/vocab.txt').read().lower()\r\ncsvfile = open('/Users/andrey/Projects/News-parser/habr.csv')\r\ndataSet = csv.DictReader(csvfile)\r\n\r\nxTrain = []\r\nyTrain = []\r\nxLink = []\r\n\r\nvocab = prep.text_to_word_sequence(f)\r\n\r\ndef text2vec(text):\r\n    bow = prep.text_to_word_sequence(text)\r\n    vec = []\r\n    for i in bow:\r\n        if i in vocab:\r\n            n = vocab.index(i)\r\n            vec.append(n)\r\n    while len(vec) < 30:\r\n        vec.append(0)   \r\n    return vec\r\n\r\nlenVocab = len(vocab)\r\nprint (lenVocab)\r\n\r\n\r\nprint('Model compiling...')\r\nmodelLink = Sequential()\r\nmodelLink.add(Embedding(lenVocab, 128, input_length=30))\r\nmodelLink.add(Convolution1D(30, 3, border_mode='valid'))\r\n\r\nmodelTitle = Sequential()\r\nmodelTitle.add(Embedding(lenVocab, 128, input_length=30))\r\nmodelTitle.add(Convolution1D(30, 3, border_mode='valid'))\r\n\r\nmodel = Sequential()\r\n\r\nmodel.add(Merge([modelLink, modelTitle], mode='concat', concat_axis=1))\r\nmodel.add(GlobalMaxPooling1D())\r\nmodel.add(Dense(150, activation='relu'))\r\nmodel.add(Dense(1, activation='sigmoid'))\r\nmodel.compile(loss='binary_crossentropy',\r\n              optimizer='adam')\r\n\r\nprint('Vectorization...')\r\nfor row in dataSet:\r\n    a = row['title'].lower()\r\n    b = row['link'].lower()\r\n    a = text2vec(a)\r\n    b = text2vec(b)\r\n    xLink.append(b)\r\n    xTrain.append(a)\r\n    yTrain.append(int(row['rate']))\r\n\r\nprint(len(xTrain))\r\nxLink = np.reshape(xLink, (len(xLink), 30))\r\nprint (xLink[1])\r\nprint (xLink[2])\r\nxLink = xLink / float(lenVocab)\r\nxTrain = np.reshape(xTrain, (len(xTrain), 30))\r\nxTrain = xTrain / float(lenVocab)\r\nyTrain = np.reshape(yTrain, (len(yTrain),1))\r\n\r\nprint('Start model training...')\r\n\r\nfor i in range(15):\r\n    print(i)\r\n    model.fit([xTrain, xLink], yTrain, nb_epoch=1, batch_size=32)\r\n\r\n    model.save_weights('/Users/andrey/Projects/News-parser/cnn-weights.h5')\r\n"", '\r\n[[ 0.40467674]]\r\n[[ 0.40467674]]\r\n[[ 0.40467674]]\r\n[[ 0.40467674]]\r\n']",[],0,0
609,keras,1643,closed,"Help: Is there any way to set up taps for scan op in Keras, for LSTM application","Hello, 

I am actually new to python, 

I'm wondering is there some way to use taps in Keras?

E.g. say, I want to compute,  i_t = T.dot(W_xi, x_t) + T.dot (W_hi1, h_tm1) + T.dot(W_hi2,h_tm2)

h_tm1 :h(t-1)
h_tm2 : h(t-2)

It's quite straightforward in theano, just use output_info, and specify the taps.
",,"['Yes: simply write your own RNN layer for Keras, using Theano.\n', ""I wrote my own class, but is there any small examples that show how to use my own class in the Keras? Sorry for the stupid question, I'm really new to python. \n""]",[],[],0,0
610,keras,8642,closed,can we use other than 2x2 strides size in maxpooling in model?,can we use different sizes of strides in max pooling other than 2x2 . if we use other than that is there any problem or changes happen to the model like accuracy prediction changes?,,"[""Yes, you can use any stride value (2, 4...). But in general, I would recommend using 2, which downsamples feature maps by 2x -- that's already a lot.""]",[],[],0,0
611,keras,1103,closed,Fixing the Travis config for TensorFlow,"I can't figure out how to fix the Travis config to install TF and run the tests with the TF backend (Python 2.7 only). Any Travis experts up for the challenge? 

See the  branch: https://github.com/fchollet/keras/blob/backend/.travis.yml
",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],['backend'],0,0
612,keras,4965,closed,pygpu.gpuarray.GpuArrayException on Theano OpenCL backend,"I'm using latest versions of Keras & Theano on a MacBook Pro with discrete AMD graphics. It works fine on CPU, but I'm getting this error when I try to use OpenCL backend:

(the description of the exception is >30000 characters long so I removed it; let me know if it's useful)

The code:
",stale,"[""You might read through [here](https://github.com/Theano/libgpuarray/issues/203).\r\n\r\nNot many ops in Theano are implemented for the OpenCL backend at this point; I can't find a full list of what's available and what's not, but so far the consensus seems to be that OpenCL backend is not ready for use with Theano.""]","['\r\nTraceback (most recent call last):\r\n  File ""/Users/egorzh/train.py"", line 107, in <module>\r\n    model.fit(x, y1, nb_epoch=50, validation_split=0.2)\r\n  File ""/usr/local/lib/python3.6/site-packages/keras/models.py"", line 671, in fit\r\n    initial_epoch=initial_epoch)\r\n  File ""/usr/local/lib/python3.6/site-packages/keras/engine/training.py"", line 1100, in fit\r\n    self._make_test_function()\r\n  File ""/usr/local/lib/python3.6/site-packages/keras/engine/training.py"", line 735, in _make_test_function\r\n    **self._function_kwargs)\r\n  File ""/usr/local/lib/python3.6/site-packages/keras/backend/theano_backend.py"", line 967, in function\r\n    return Function(inputs, outputs, updates=updates, **kwargs)\r\n  File ""/usr/local/lib/python3.6/site-packages/keras/backend/theano_backend.py"", line 953, in __init__\r\n    **kwargs)\r\n  File ""/usr/local/lib/python3.6/site-packages/theano/compile/function.py"", line 326, in function\r\n    output_keys=output_keys)\r\n  File ""/usr/local/lib/python3.6/site-packages/theano/compile/pfunc.py"", line 486, in pfunc\r\n    output_keys=output_keys)\r\n  File ""/usr/local/lib/python3.6/site-packages/theano/compile/function_module.py"", line 1784, in orig_function\r\n    defaults)\r\n  File ""/usr/local/lib/python3.6/site-packages/theano/compile/function_module.py"", line 1651, in create\r\n    input_storage=input_storage_lists, storage_map=storage_map)\r\n  File ""/usr/local/lib/python3.6/site-packages/theano/gof/link.py"", line 699, in make_thunk\r\n    storage_map=storage_map)[:3]\r\n  File ""/usr/local/lib/python3.6/site-packages/theano/gof/vm.py"", line 1059, in make_all\r\n    impl=impl))\r\n  File ""/usr/local/lib/python3.6/site-packages/theano/gof/op.py"", line 924, in make_thunk\r\n    no_recycling)\r\n  File ""/usr/local/lib/python3.6/site-packages/theano/gof/op.py"", line 828, in make_c_thunk\r\n    output_storage=node_output_storage)\r\n  File ""/usr/local/lib/python3.6/site-packages/theano/gof/cc.py"", line 1190, in make_thunk\r\n    keep_lock=keep_lock)\r\n  File ""/usr/local/lib/python3.6/site-packages/theano/gof/cc.py"", line 1131, in __compile__\r\n    keep_lock=keep_lock)\r\n  File ""/usr/local/lib/python3.6/site-packages/theano/gof/cc.py"", line 1589, in cthunk_factory\r\n    key=key, lnk=self, keep_lock=keep_lock)\r\n  File ""/usr/local/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 1118, in module_from_key\r\n    src_code = lnk.get_src_code()\r\n  File ""/usr/local/lib/python3.6/site-packages/theano/gof/cc.py"", line 1465, in get_src_code\r\n    mod = self.get_dynamic_module()\r\n  File ""/usr/local/lib/python3.6/site-packages/theano/gof/cc.py"", line 1509, in get_dynamic_module\r\n    self.code_gen()\r\n  File ""/usr/local/lib/python3.6/site-packages/theano/gof/cc.py"", line 782, in code_gen\r\n    name))\r\n  File ""/usr/local/lib/python3.6/site-packages/theano/gpuarray/basic_ops.py"", line 376, in c_support_code_apply\r\n    bins = \'\\n\'.join(self._generate_kernel_bin(k, ctx) for k in kernels)\r\n  File ""/usr/local/lib/python3.6/site-packages/theano/gpuarray/basic_ops.py"", line 376, in <genexpr>\r\n    bins = \'\\n\'.join(self._generate_kernel_bin(k, ctx) for k in kernels)\r\n  File ""/usr/local/lib/python3.6/site-packages/theano/gpuarray/basic_ops.py"", line 318, in _generate_kernel_bin\r\n    **k._get_py_flags())\r\n  File ""pygpu/gpuarray.pyx"", line 2185, in pygpu.gpuarray.GpuKernel.__cinit__ (pygpu/gpuarray.c:27154)\r\n  File ""pygpu/gpuarray.pyx"", line 431, in pygpu.gpuarray.kernel_init (pygpu/gpuarray.c:7261)\r\npygpu.gpuarray.GpuArrayException\r\n', ""\r\nmodel = Sequential()\r\nmodel.add(Dense(256, input_shape=(128, 2400)))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(256))\r\nmodel.add(Dense(nb_classes, activation='softmax'))\r\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n""]",[],0,0
613,keras,3866,closed,Keras on AWS EC2,"I have developed an ML app on my local machine and now wanted to do the deployment on AWS EC2 (CPU instance only at this time as I talk about a predict flow)

After installation of all packages and specifically Keras and Theano backend, I am able to call under Python (Ubuntu 14:04  - Python version 2.7.6) all packages nicely.

So, as example, when I interactively do call up Python and then import keras, I get the standard print 'Using Theano backend.' as desired and set in the keras.json file

Now, I installed flask server and and apache2 server working with wsgi package.
I test this server out with several types of routes and returns and all is working nicley.

my flask file is looking essentially like



my predict.py file is in the python directory and looks like



so, I start my apache server and look at the logs and the logs tell me 
'Using TensorFlow backend.' which is not what I want.

Can anybody give me a hint why this is happening
many thanks
Peter
",stale,"['You can check `~/.keras/keras.json` on your EC2 instance for the value of `""backend""`.\n', 'Hey, I have done this but it works only when I am in interactive mode as stated above\n', 'you can print out the ""user folder"" by `os.path.expanduser(\'~\')` to see where the configuration is located, I guess Apache server is running under a different user than yours?\n', 'Thanks so far but not sure what to do with it. Your hint is interesting to say that the apache server is maybe running under a different user. I am on Amazon Web Services EC2, so going to launch your command, it comes back as /home/ubuntu while apache is installed in the root of the instance, I guess in etc/apache2 (no sure about this).\n???\n', ""There are many ways of configuring Keras. I guess a quick fix could be using `os.environ['KERAS_BACKEND']='theano'` before `import keras` in your script importing to make it explicit.\n"", 'That is an excellent hint.\nI will try this\nWish me luck!\nThanks\n']","['\nfrom flask import Flask, jsonify, request\nimport json \nimport os\nfrom python import predict \n\n@app.route(""/api/ML/predict"", methods=[\'POST\'])\ndef dicom_tag():\n\n    #print (request.data)\n\n    dataIn = request.get_json()\n    data = dataIn[u\'id\']\n\n    predicted = predict.doPrediction(data)\n\n    return jsonify(predicted)\n', '\nimport os\nimport cv2\nimport json\nimport time\nimport dicom\nimport scipy\nimport keras\n\n# more imports and ...\n# and the all my code to do the predict flow\n\n']",[],0,0
614,keras,4168,closed,Get current epoch predictions from inside callback (on_epoch_end) ,"Is it possible to get the training set predictions of the model at the end of each epoch from inside the function on_epoch_end at the end of a callback without having to call self.model.predict() every time, since the model already computed them?
",stale,"['Similar: https://github.com/fchollet/keras/issues/2859\n', 'this might help. https://stackoverflow.com/questions/47079111/create-keras-callback-to-save-model-predictions-and-targets-for-each-batch-durin']",[],[],0,0
615,keras,1556,closed,Travis CI tests are failing due to connection errors to data (possible aws problems?),"e.g. https://travis-ci.org/fchollet/keras/jobs/104864770


",,"['Possibly a network error --restarting it should work.\n', 'OK, there is no real possibility to restart it for non-core devs, but closing and reopening PR works the same. \nCheers\n']","['\n...\nERROR collecting tests/test_loss_weighting.py \n\ntests/test_loss_weighting.py:22: in <module>\n    (X_train, y_train), (X_test, y_test) = mnist.load_data()\nkeras/datasets/mnist.py:17: in load_data\n    data = cPickle.load(f)\n../../../miniconda/envs/test-environment/lib/python2.7/gzip.py:464: in readline\n    c = self.read(readsize)\n../../../miniconda/envs/test-environment/lib/python2.7/gzip.py:268: in read\n    self._read(readsize)\n../../../miniconda/envs/test-environment/lib/python2.7/gzip.py:319: in _read\nuncompress = self.decompress.decompress(buf)\nE   error: Error -3 while decompressing: invalid block type\n----------- Captured stdout ------------\nDownloading data from https://s3.amazonaws.com/img-datasets/mnist.pkl.gz\n15302656/15296311 [==============================] - 1s   \n...  \n']",[],0,0
616,keras,649,closed,No test coverage for Keras,"Just noticed that there is no tests for Convolution1D, this is a side effect of not having code coverage metrics.
",,"['I just filed a PR with test coverage for all convolutional layers #647 . I agree we should start tracking coverage! Travis and Coveralls, for example.\n\nHaving tests passing and coverage badge on the front page will also send a good message to people considering using keras.\n\n@tleeuwenburg has travis set up on his fork, but for the main repo only @fchollet can do it.\n', 'Which PR ?\n', 'See edit above, #647 \n', 'I see that helps for the Conv1D aspect; but does not fixes the underlying issue: no test coverage metrics.  It is difficult to improve what is not measured.\n', 'Indeed. I run coverage locally and use that to select targets for tests I write. we are currently at ca 70% coverage.\n\nYou can generate a nice coverage report by running \n\n> py.test --cov-report html --cov keras tests\n\nin the project root.\n\nBut obviously the best thing would be to have automated coverage reporting. Would be nice to hear from @fchollet what his thinking is on this matter.\n', 'I have CI and coverage monitoring running on my fork now:\n\nhttps://travis-ci.org/phreeza/keras\nhttps://coveralls.io/github/phreeza/keras\n\nWill send a PR for this if there is interest.\n', ""Thanks @phreeza! So we are now at 70% test coverage (the remainder tends to be rarely used utils, but we'll cover them too in time), and we have coverage monitoring as part of the Travis integration.\n""]",[],[],0,0
617,keras,5792,closed,small bugs in local.py on LocallyConnected2D?,"Hello, 

I'm wondering if the line 411-413 is correct ?
https://github.com/fchollet/keras/blob/master/keras/layers/local.py#L412-L413

output = K.reshape(output, (self.output_row, self.output_col, -1, filters))

output is of dimension :  output_row x output_col x batch_size x filters

output = K.permute_dimensions(output, (2, 0, 1, 3))

should this be (1,2,0,3) ?

Thanks !
",,"['`channels_last` output should be in shape `(batch_size, output_row, output_col, filters)`, which is corresponding to `(2, 0, 1, 3)`.']",[],[],0,0
618,keras,131,closed,model.fit(shuffle=False) gives TypeError,"

Fix is probably just change  to  in  line 188.
",,['Good catch : ) This is fixed now. \n'],"['\nTraceback (most recent call last):\n  File ""toy_mlp.py"", line 24, in <module>\n    model.fit(X, y, validation_split=0.5, shuffle=False, nb_epoch=50, verbose=2)\n  File ""build/bdist.macosx-10.5-x86_64/egg/keras/models.py"", line 189, in fit\nTypeError: object of type \'slice\' has no len()\n']","['slice()', 'range()', 'models.py']",0,0
619,keras,7453,closed,"Node index for deep model, recursion depth exceeded","A deep very deep model (200 layers) is giving me this error:

",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']","['\r\nFile ""/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py"", line 1678, in build_map_of_graph\r\n    node_key = layer.name + \'_ib-\' + str(node_index)\r\n...\r\n...\r\nFile ""/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py"", line 1678, in build_map_of_graph\r\n    node_key = layer.name + \'_ib-\' + str(node_index)\r\nRecursionError: maximum recursion depth exceeded while getting the str of an object\r\n']",[],0,0
620,keras,4010,closed,Stateful LSTM: dimensions using input_batch_shape?,"Hi,
I'm trying to model a stateful LSTM for TTS system. I've added-


However, I'm getting  with the following setup-



Any ideas? Thanks!
",,[],"[' python\nX_train.shape= (55, 2000, 425), y_train.shape= (55, 2000, 199)\n\n####model####\nn_input=425\nn_output=199\nsize=2000\nbatch=5\nno_epochs=2\n\nmodel = Sequential()\nmodel.add(LSTM(64, return_sequences=True, batch_input_shape=(size, size, n_input), activation=""linear"", stateful=True))\nmodel.add(SimpleRNN(n_output,return_sequences=True, activation=\'tanh\', stateful=True))\n\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(optimizer=sgd, loss=\'mean_squared_error\' ,metrics=[\'accuracy\'])\n\nmodel.fit(X_train, y_train, batch_size=batch, nb_epoch=no_epochs)\n']","['batch_input_shape=(batch_size, timesteps, features), stateful=True', 'ValueError: dimension mismatch in args to gemm (2000,64)x(64,64)->(5,64)']",0,0
621,keras,12724,closed,Keras appears to generate non-deterministic tensorflow graph,"**System information**  
- Have I written custom code (as opposed to using example directory):  Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04
- TensorFlow backend (yes / no):   yes
- TensorFlow version:  1.13.1
- Keras version:  2.4.4 (bug exists on latest master as well)
- Python version:  3.6.8
- CUDA/cuDNN version:  not using cuda for this
- GPU model and memory:  not using cuda for this

**Describe the current behavior**  
If I set all the random seeds, use only the CPU, disable CPU multiprocessing, and I run the same experiment 10 times, the loss of the second  call comes out as one of two different values each time.

**Describe the expected behavior**  
The loss should be bit-perfect reproducible in this situation.

**Code to reproduce the issue**  

Check the reproducibility with the following command:

    for i in  ; do python3 code_example.py 2>/dev/null | tail -n 1 ; done

Result:


**Other info / logs**


There are a variety of things which affect whether or not the training is bit-perfect reproducible:
 - The loss is always consistent after the first .  Training for three steps exhibits identical behavior to training for two steps (all losses come out to one of two values)
 - Commenting out various layers restores reproducibility (see comments in code example)
 - Eliminating the decay from  optimizer (or using  optimizer with decay) restores reproducibility
 - Metric selection affects reproducibility.  Some metrics break reproducibility when included twice, one metric,  breaks reproducibility when included at all.
 - I noticed no change in behavior when I used the git master branch.
 - Inserting a  into the graph, while using the RMSprop with decay, will restore reproducibility.  Patch:

",stat:awaiting tensorflower type:support,"['@rb-determined-ai I checked with Keras==2.2.4 and tensorflow==1.14.0, the results are consistent. Please check the gist [here](https://colab.sandbox.google.com/gist/jvishnuvardhan/db6974f2f50ca954241bd1f17c5fb19b/keras_12724_repeatability.ipynb). Thanks!', ""Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!""]","['\r\nimport random\r\nrandom.seed(999)\r\nimport numpy as np\r\nnp.random.seed(999)\r\nimport tensorflow as tf\r\ntf.set_random_seed(999)\r\n\r\nimport keras\r\nfrom keras.layers import (Activation, Conv2D, Dense, Dropout, Flatten,\r\n                          MaxPooling2D, Layer)\r\nfrom keras.losses import categorical_crossentropy\r\nfrom keras.models import Sequential\r\nfrom keras.optimizers import RMSprop, Adam\r\nfrom keras.utils.data_utils import get_file\r\n\r\nsession = tf.Session(\r\n        graph=tf.get_default_graph(),\r\n        config=tf.ConfigProto(intra_op_parallelism_threads=1,\r\n                              inter_op_parallelism_threads=1)\r\n        )\r\nkeras.backend.set_session(session)\r\n\r\n# Model taken from the keras cifar10 example\r\nmodel = Sequential()\r\n# specifying input shape upfront does not matter\r\n#model.add(Conv2D(32, (3, 3), padding=""same"", input_shape=[ 32, 32, 3 ]))\r\nmodel.add(Conv2D(32, (3, 3), padding=""same""))\r\nmodel.add(Activation(""relu""))\r\nmodel.add(Conv2D(32, (3, 3)))\r\nmodel.add(Activation(""relu""))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Dropout(0.25))\r\n\r\nmodel.add(Conv2D(64, (3, 3), padding=""same""))\r\nmodel.add(Activation(""relu""))\r\nmodel.add(Conv2D(64, (3, 3)))\r\nmodel.add(Activation(""relu""))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Dropout(0.25))\r\n\r\nmodel.add(Flatten())\r\nmodel.add(Dense(512))\r\nmodel.add(Activation(""relu""))\r\n# eliminating this dropout layer restores reproducibility:\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(10))\r\nmodel.add(Activation(""softmax""))\r\n\r\n# setting decay to 0.0 or using Adam restores reproducibility\r\noptimizer = RMSprop(lr=1e-4, decay=1e-6)\r\n#optimizer = RMSprop(lr=1e-4, decay=0.0)\r\n#optimizer = Adam(lr=1e-4, decay=1e-6)\r\nmodel.compile(loss=categorical_crossentropy,\r\n              optimizer=optimizer,\r\n              metrics=[keras.metrics.categorical_accuracy,\r\n                       keras.metrics.categorical_accuracy])\r\n\r\n# metrics which break reproducibility:\r\n#   losses.categorical_hinge x1\r\n#   metrics.categorical_accuracy x2\r\n#   losses.hinge x2\r\n#   losses.mean_absolute_percentage_error x2\r\n\r\n# metrics which seem fine:\r\n#   losses.mean_squared_error\r\n#   losses.mean_absolute_error\r\n\r\n# build some phony data\r\nxtrain = np.ones([64,32,32,3])\r\nxtest = np.ones([64,32,32,3])\r\nytrain = np.array([[1,0,0,0,0,0,0,0,0,0] for _ in range(64)])\r\nytest = np.array([[1,0,0,0,0,0,0,0,0,0] for _ in range(64)])\r\n\r\nxtrain = xtrain.astype(\'float32\')\r\nxtest = xtest.astype(\'float32\')\r\nxtrain /= 255\r\nxtest /= 255\r\n\r\n# train two batches\r\n\r\nstart = 0\r\nxbatch = xtrain[start:start+32]\r\nybatch = ytrain[start:start+32]\r\nprint( model.train_on_batch(xbatch, ybatch) )\r\n\r\nstart = 32\r\nxbatch = xtrain[start:start+32]\r\nybatch = ytrain[start:start+32]\r\nprint( model.train_on_batch(xbatch, ybatch) )\r\n\r\n', '\r\n[2.2911897, 1.0, 1.0]\r\n[2.2910979, 1.0, 1.0]\r\n[2.2910979, 1.0, 1.0]\r\n[2.2910979, 1.0, 1.0]\r\n[2.2911897, 1.0, 1.0]\r\n[2.2911897, 1.0, 1.0]\r\n[2.2910979, 1.0, 1.0]\r\n[2.2910979, 1.0, 1.0]\r\n[2.2911897, 1.0, 1.0]\r\n[2.2910979, 1.0, 1.0]\r\n', ""\r\n*** optimizers.py\t2019-04-23 11:10:15.269380564 -0700\r\n--- new\t2019-04-23 09:57:50.773179254 -0700\r\n***************\r\n*** 255,274 ****\r\n--- 255,275 ----\r\n      def get_updates(self, loss, params):\r\n          grads = self.get_gradients(loss, params)\r\n          accumulators = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\r\n          self.weights = accumulators\r\n          self.updates = [K.update_add(self.iterations, 1)]\r\n  \r\n          lr = self.lr\r\n          if self.initial_decay > 0:\r\n              lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\r\n                                                        K.dtype(self.decay))))\r\n+         self.updates.append(tf.print(lr))\r\n  \r\n          for p, g, a in zip(params, grads, accumulators):\r\n              # update accumulator\r\n              new_a = self.rho * a + (1. - self.rho) * K.square(g)\r\n              self.updates.append(K.update(a, new_a))\r\n              new_p = p - lr * g / (K.sqrt(new_a) + self.epsilon)\r\n  \r\n              # Apply constraints.\r\n              if getattr(p, 'constraint', None) is not None:\r\n                  new_p = p.constraint(new_p)\r\n\r\n""]","['train_for_step()', 'seq 10', 'train_for_batch()', 'RMSprop', 'Adam', 'losses.categorical_hinge', 'tf.print()']",0,0
622,keras,5880,closed,"Hello, Can I  ask you how to build a 3D Convolution Autoencoder?","
",stale,"[""Same as a [2D one](https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder_deconv.py), except Keras doesn't have transposed 3D convolution. The following _might_ be equivalent (edit: it definitely isn't) to a transposed 3d conv with 1x1x1 kernel and 0.5 strides:\r\n\r\n```\r\ndef transposed_conv_3d(chans, input, **kwargs):\r\n    tmp = Upsampling3D()(input)\r\n    tmp = Convolution3D(chans, 1, 1, 1, **kwargs)(tmp)\r\n    return tmp\r\n```\r\n\r\nFor a complete 3D architecture, take a look at [V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation](https://arxiv.org/abs/1606.04797)."", 'Thank you so much for your reply, I want to know that if I also use convolution3D while decoding, can this autoencoder works well?', ""You'll have to experiment and see what works best. Another piece of advice though: if you do use a V-Net type architecture, don't use residual (skip) connections, as that will make the autoencoder learn a trivial transform."", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],0,0
623,keras,11881,closed,Code runs fine with CPU but AttributeError with GPU,"Hi,
I wanted to train a U-net, and everything works fine on my laptop with the tensorflow CPU version. However, on the cluster with a GPU, I can create and compile it, but when I call the  method I get the following error:

    Traceback (most recent call last):
      File ""Cluster.py"", line 46, in <module>
    unet.fit_generator(gen, steps_per_epoch=gen.getStepsPerEpoch(), epochs=101)
      File ""/home/exacloud/tempwork/ChangLab/Guillaume/Python/lib/python3.5/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper
    return func(*args, **kwargs)
      File ""/home/exacloud/tempwork/ChangLab/Guillaume/Python/lib/python3.5/site-packages/keras/engine/training.py"", line 1418, in fit_generator
    initial_epoch=initial_epoch)
      File ""/home/exacloud/tempwork/ChangLab/Guillaume/Python/lib/python3.5/site-packages/keras/engine/training_generator.py"", line 40, in fit_generator
    model._make_train_function()
      File ""/home/exacloud/tempwork/ChangLab/Guillaume/Python/lib/python3.5/site-packages/keras/engine/training.py"", line 509, in _make_train_function
    loss=self.total_loss)
      File ""/home/exacloud/tempwork/ChangLab/Guillaume/Python/lib/python3.5/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper
    return func(*args, **kwargs)
      File ""/home/exacloud/tempwork/ChangLab/Guillaume/Python/lib/python3.5/site-packages/keras/optimizers.py"", line 410, in get_updates
    self.updates.append(K.update(a, new_a))
      File ""/home/exacloud/tempwork/ChangLab/Guillaume/Python/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py"", line 973, in update
    return tf.assign(x, new_x)
      File ""/home/exacloud/tempwork/ChangLab/Guillaume/Conda/DL35/lib/python3.5/site-packages/tensorflow/python/ops/state_ops.py"", line 277, in assign
    return ref.assign(value)
    AttributeError: 'Tensor' object has no attribute 'assign'

So the exact same code runs perfectly on my laptop on the cpu only version of Tensorflow, but not on the GPU one.

Any idea what the issue may be?

Thanks in advance!",stat:awaiting response,"[""When you are using tf.assign(a,b), 'a' should be a variable. In your example tf.assign(x, new_x), the error could be due to x being declared as a constant. Please change it to a mutable tensor(variable) and try it. \r\nThanks !"", ""I don't assign anything because it happens in the keras fit_generator method, and it works fine on my laptop."", ""Hi,\r\nWe could work on this better if you could provide a standalone reproducible example.\r\nRequirements:\r\n* Don't use custom data or custom paths.\r\n* Use random arrays or even np.ones, np.zeros.\r\n* The example should run with Keras (and deps) alone.\r\n* Should be Python3 compatible.\r\n* Should not be OS specific.\r\n* The file should reproduce the bug with **high* fidelity.\r\n* Link to a gist would be appreciated.\r\n\r\nThanks!\r\nDref360"", 'Mmm... I cannot reproduce the issue everytime I start the soft. I am going to investigate.']",[],['fit_generator'],0,0
624,keras,3518,closed,Survey: do we need a threshold in Early stopping?,"Sometimes I need to set a threshold for early stopping. The threshold is the minimum amount of improvement as considered to be new maximum performance. 

Please add a comment if you think you would need it, too!
",stale,"['That would be definitely a welcome addition especially for hyperparameter searching.\n@fchollet is this something that could be fit in or is there already a way to do this?\n', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],0,0
625,keras,2801,closed,Split tensor,"I'm doing a lambda layer in which I'd like to split a tensor into two (so the opposite of K.concatenate, essentially) to perform some different operations on the two parts, before concatenating them again. Any thoughts on how to split with the Keras backend?


",,"['Slicing should work\n\n``` python\nimport keras.backend as K\nt = K.ones((12, 3))\nt1 = t[:, :1] + 1\nt2 = t[:, 1:] - 1\nt3 = K.concatenate([t1, t2])\nprint(K.eval(t3))\n```\n', ""Yes, @joelthchao, that's what I'm currently doing. Thanks. :smile: \n\nHowever, that requires knowing where to slice. Any way to get around that? \n"", 'Do you mean split tensor into half without knowing shape?\n', 'Yes.\n', ""doesn't something like t1 = t[:, :t.shape[-1]//2] + 1 work?\n"", ""@lemuriandezapada, it seems like it should, but \n\n`t[:, :, int(t.shape[-1]/2):])`\n\nresults in \n\n> TypeError: int() argument must be a string, a bytes-like object or a number, not 'TensorVariable'\n"", ""It's not possible to inference tensor shape without eval in this situation. You need to write a customize layer.\n"", 'It would be nice to have the method slice from Tensorflow in the backend of Keras.', '@joelthchao But when I train such a model, there always throws the error ""Exception: Output tensors to a Model must be Keras tensors. Found: Tensor(""add_292:0"", shape=(?, 10), dtype=float32)""', '@lixiaosi33 Since we are using `keras.backend` to do tensor operation, it produces TF or TH tensor but keras tensor. To build a model, you can use lambda layer to build keras layer:\r\n```python\r\nfrom keras.layers import Input, Lambda\r\nfrom keras.models import Model\r\na = Input(shape=(3,))\r\ndef slice(x):\r\n    return x[:, x:1]\r\nb = Lambda(slice)(a)\r\nmodel = Model(a, b)\r\nmodel.summary()\r\n```\r\n\r\n', ""Just to add to the post, the return of the function should be return x[:, 0:1] rather than return x[:, x:1], if I am not wrong. \r\n\r\nI have another relevant issue of splitting tensor along the very first channel, the batch channel.  Just following the code above, one can result in the following code snippet:\r\n```\r\nfrom keras.layers import Input, Lambda\r\nfrom keras.models import Model\r\na = Input(batch_shape=(10,3))\r\ndef slice(x):\r\n    return x[0:4]\r\nb = Lambda(slice)(a)\r\nmodel = Model(a, b)\r\nmodel.summary()\r\n```\r\nEverything goes as expected, we ended up with a model with input of shape (10,3) and output of shape (4,3). However, the model can't even make a prediction:\r\n```\r\nmodel.predict(np.zeros((10,3)))\r\n```\r\ngives\r\n```\r\n/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc in predict(self, x, batch_size, verbose)\r\n   1583         f = self.predict_function\r\n   1584         return self._predict_loop(f, ins,\r\n-> 1585                                   batch_size=batch_size, verbose=verbose)\r\n   1586\r\n   1587     def train_on_batch(self, x, y,\r\n\r\n/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc in _predict_loop(self, f, ins, batch_size, verbose)\r\n   1219\r\n   1220             for i, batch_out in enumerate(batch_outs):\r\n-> 1221                 outs[i][batch_start:batch_end] = batch_out\r\n   1222             if verbose == 1:\r\n   1223                 progbar.update(batch_end)\r\n\r\nValueError: could not broadcast input array from shape (4,3) into shape (10,3)\r\n\r\n```\r\nAny comments is appreciated. "", '1. `return x[:, x:1]` change to ` return x[:, :1]`, just a typo\r\n2. batch size should not change during training/testing, if you insist, I recommend you to use another dimension (e.g. make it (1, 10, 3) to (1, 4, 3)) rather than batch_size.', 'I have a mistake like this:\r\nain = Input(shape=(1,N))\r\nx=Dense(12)(ain[:,0:1])\r\ny=Dense(12)(ain[:,1:2])\r\nThen it is impossible to concatenate x,y\r\nError:\r\nValueError: A Concatenate layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 0, 32), (None, 1, 32)]\r\n\r\ntake ain[:,0:1] always gives me shape: (None, 1, 32) but others always give (None, 0, 32), i.e. for all i not equal to 0, ain[:,i:i+1] gives (None, 0, 32).\r\n\r\nDoes anyone know the reason?\r\nMany thanks!!!', ""@tat-dat when you set to `shape=(1,N)` in `Input` you actually set input shape to `(batch_size, 1, N)`.\r\nThen you take slices `[:, 0:1, :]` which is `(batch_size, 1, N)`, and `[:, 1:2, :]` which is `(batch_size, 0, N)` so it's empty."", '@arquolo Thanks!', '@arquolo how can I slice a tensor horizontally in a specific size of chunk? how should be the lambda layer']","[' Python\nimport keras.backend as K\nt = K.ones((12,3))\nt1, t2 = ...  # Split somehow?\nt = K.concatenate([t1,t2])\n']",[],0,0
626,keras,9941,closed,New layer instantiation from config failing. ,"Hello, 
I've created a layer as follows : 
from keras import backend as K
from keras.engine.topology import Layer
import numpy as np
import cv2

class MedianLayer(Layer):

    def __init__(self, output_dim, input_shape, **kwargs):
        self.output_dim = output_dim
        super(MedianLayer, self).__init__(**kwargs)
    def build(self, input_shape):
        # Create a trainable weight variable for this layer.
        self.kernel = self.add_weight(name='kernel', 
                                      shape=(input_shape[1], self.output_dim),
                                      initializer='uniform',
                                      trainable=True)
        super(MedianLayer, self).build(input_shape)  # Be sure to call this somewhere!
    def call(self, x):
        return cv2.medianBlur(x,5) - x

    def compute_output_shape(self, input_shape):
        return (input_shape[0], self.output_dim)
===================================================================
After this, I do the following: 
k = Sequential()
layer = MedianLayer(output_dim=(100,100), input_shape=(100,100))
lc = layer.get_config()
x = np.random.randn(100, 100)
lc[""input_shape""] = x.shape
lc[""output_dim""] = x.shape
print(lc)
This prints : 
{'name': 'median_layer_3', 'trainable': True, 'input_shape': (100, 100), 'output_dim': (100, 100)}
 AS expected. 
==================================================================
Now, when I do : 
# Instantiate a new layer object from old config
layer1 = layer.from_config(lc)
print(layer1.get_config())
print(lc)
I get the following : 
{'name': 'median_layer_3', 'trainable': True}  # New layer config
{'name': 'median_layer_3', 'trainable': True, 'input_shape': (100, 100), 'output_dim': (100, 100)} # old layer config, used as input for re-instantiation.

The re-instantiation of another layer object using an existing layer config is failing - any clues highly appreciated. 
Thank you. 
Kumar",,[],[],[],0,0
627,keras,12986,closed,How to Obtain Layer Output of Nested Model in Keras,"Does Keras provide a way to get the output of a nested model? Or is there a way to flatten the nested model? 


For the model architecture below, how can I 'get layer1' or 'layer2'? When I compile m2, I only see 'layer3' and the TimeDistributed layer.

**System information**  
- Have I written custom code (as opposed to using example directory):  No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Mac OS
- TensorFlow backend (yes / no):  yes
- TensorFlow version:  b'v1.13.0-rc2-5-g6612da8951' 1.13.1
- Keras version:  2.2.4
- Python version:  3.6.5
- CUDA/cuDNN version: no  
- GPU model and memory: no ",type:support,"['@kaushik88 Please check a [resource](https://datascience.stackexchange.com/questions/19362/how-to-obtain-output-of-intermediate-model-in-keras) that provide info of output layer. Thanks!', ""Hi!\r\nThis issue isn't related to a bug/enhancement/feature request or other accepted types of issue.\r\nTo ask questions, please see the following resources :\r\n[StackOverflow](http://stackoverflow.com/questions/tagged/keras)\r\n[Keras Slack channel](https://keras-slack-autojoin.herokuapp.com)\r\n[Keras.io support](https://keras.io/#support)\r\n[Gitter](https://gitter.im/Keras-io/Lobby)\r\nThanks!\r\nIf you think we made a mistake, please open another issue and fill all the information asked by the template. Thanks!"", 'Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!', 'I am facing a similar problem. Please share the working solution snippet. ']","[""\r\nx2 = layer1(x1, name='layer1')\r\nx3 = layer2(x2, name='layer2')\r\nm1 = Model(x1, x3)\r\nx5 = layer3(x4, name='layer3')\r\nx6 = TimeDistributed(m1)(x5)\r\nm2 = Model(x5, x4)\r\n""]",[],0,0
628,keras,1309,closed,Slow BatchNormalization layers (CPU tested),"Hi,

I added BatchNormalization layers to my model and it suddenly took much more time to train. 

It takes 561 seconds for one epoch with them:
Epoch 1/1
4096/4096 [==============================] - 561s - loss: 0.0946 - acc: 0.9006

This is if I comment out the BatchNorm layers (186 seconds):
Epoch 1/1
4096/4096 [==============================] - 186s - loss: 4.5043 - acc: 0.5933

I wrote on the user group and someone informed me that adding Dropout slows it down further, but even without Dropout this is much slower (above 500 seconds too).

Is it a CPU-related issue? Or is such a slowdown expected? Below is my model.


",stale,"[""Not sure if it's relevant but should BN be applied before activations? \n"", 'Hi,\n\nAny update on this ?\n\nThe `BatchNorm` is Keras is still tremendously slow. And not just on CPU, but on GPU as well.\nIs there any way to alleviate the problem ?\n\nAnd it increases linearly with batch size. So even if my GPU is under utilized, I cannot increase my batch size, since that would make it even more slower.\n', '+1 I observe same thing\n', 'In Theano, we juste merged a wrapper of cuDNN batch norm that must be used\nmanually:\n\nhttp://deeplearning.net/software/theano_versions/dev/library/sandbox/cuda/dnn.html#batch-normalization\n\nIt speed up the compilation and execution time.\n\nSo if you or someone else modify keras to use it, it would be great.\n\nOn Thu, Jul 14, 2016 at 12:57 PM, iaroslav-ai notifications@github.com\nwrote:\n\n> +1 I observe same thing\n> \n> —\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/fchollet/keras/issues/1309#issuecomment-232726054,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AALC-3IwuQ6xSPdXSJRiNkyk6zBBgWoCks5qVmqBgaJpZM4G4bYz\n> .\n', 'Nice @nouiz !!!\n\n@fchollet @farizrahman4u tensorflow also has batch norm by default, should we benchmark this and create a `batch_norm` operator in backend? Are both implementations compatible?\n', 'If its faster, then we defenitely should.\n', 'I just observed almost a 10x slowdown when using batch normalization using GPUs. I am using the Theano backend, CUDA 8 RC and CuDNN. I am training a VGG-style CNN, on CIFAR-10, that is around 20 layers deep.\n', 'Has anyone attempted this? I would like to use it in my implementation (though it looks like the theano cudnn BN doesnt support 5 dimensional data yet)\n', 'Have encountered the same problem with Theano backend + GPU (980 Ti). At least two times slower than models without BatchNormalization.\n', ""Batch norm have a computation cost. Be sure to use cudnn 5.1. In particular\nfor Pascal GPU.\n\nI don't know if Keras finished to wrap Theano cudnn batch norm or not. Make\nsure that it use it.\n\nOn Sat, Sep 24, 2016 at 8:18 PM, dk1013 notifications@github.com wrote:\n\n> Have encountered the same problem with Theano backend + GPU (980 Ti). At\n> least two times slower than models without BatchNormalization.\n> \n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/fchollet/keras/issues/1309#issuecomment-249394900,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AALC-wsycDfppe9hsNyOF4w7L_C3C7k6ks5qtb3sgaJpZM4G4bYz\n> .\n"", 'Question : Is batchnorm actually worth all this trouble?\n', '@farizrahman4u I really wish it was not... but at least for GANs, it is mandatory.\n', 'Yes Keras wraps cuDNN batch norm when available on the Theano side.\n\nYou could replace BN with gradient normalization or activation\nnormalization.\n\nOn Sep 27, 2016 12:44 PM, ""Eder Santana"" notifications@github.com wrote:\n\n> @farizrahman4u https://github.com/farizrahman4u I really wish it was\n> not... but at least for GANs, it is mandatory.\n> \n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/fchollet/keras/issues/1309#issuecomment-249976357,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AArWb3IDz_SiTGYyIgmCvL6F5wo4i6lvks5quXI5gaJpZM4G4bYz\n> .\n', 'Update:\nBatchNormalization takes an ""axis"" parameter (default to 0). This may cause the different speed between Theano and Tensorflow.\n############ old #############\n@nouiz Have updated my cudnn from v5005 to v5103, but observed no performance gain.\nIs cudnn important for BatchNormalization?\nI also tested the same model with tensorflow backend (after adjusting image_dim_ordering conventions etc) and found tensorflow runs way faster than theano.\nEdit1: Using 980 Ti, haven\'t tried pascal yet.\nEdit2: To make it quatitative, one epoch takes about 500s with Tensorflow, 2000s with Theano.\n', 'I wonder if there is any progress on this issue?', 'Yes. Update Theano to the dev version and a ""recent"" enought keras will use\nthe new Theano interface that should have this fast from Theano side. So it\nwork on the CPU and the GPU with and without CuDNN. If CuDNN is there, it\nwill be faster.\n\nI\'m not sure which recent enough mean for keras. For Theano the master of\nTheano 0.9beta1 is recent enough.\n\nOn Tue, Feb 14, 2017 at 8:58 AM jiqiujia <notifications@github.com> wrote:\n\n> I wonder if there is any progress on this issue?\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/fchollet/keras/issues/1309#issuecomment-279713703>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AALC-_ydEJPYtJuylD-xk3R1SD52b2yoks5rcbMXgaJpZM4G4bYz>\n> .\n>\n']","[""\n    model = Sequential()\n    model.add(Convolution2D(32, 11, 11, subsample=(4,4),input_shape=(3,227,227)))\n    model.add(PReLU())\n    model.add(BatchNormalization())\n\n    model.add(Convolution2D(64, 5, 5, subsample=(2,2)))\n    model.add(PReLU())\n    model.add(BatchNormalization())\n\n    model.add(Convolution2D(64, 3, 3))\n    model.add(PReLU())\n    #model.add(BatchNormalization())\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n\n    model.add(Flatten())\n    model.add(BatchNormalization())\n    model.add(Dense(400))\n    model.add(PReLU())\n    model.add(BatchNormalization())\n    model.add(Dropout(0.25))\n\n    model.add(Dense(400))\n    model.add(PReLU())\n    model.add(BatchNormalization())\n    model.add(Dropout(0.25))\n\n    model.add(Dense(3, activation='softmax'))\n    optimizer = keras.optimizers.RMSprop(lr=0.0005, rho=0.9, epsilon=1e-6)\n\n    model.compile(loss='categorical_crossentropy',optimizer=optimizer)\n\n""]",[],0,0
629,keras,6243,closed,"Inefficient, newly introduced pause in train_on_batch()","Since upgrading to Keras 2 and latest Theano (last updated ~Jan '17), the train_on_batch() call now takes a ~10 second pause after approximately the first 10 batches for Convolutional layers.  Is there a reason for this pause, and is there a way to get around it?",stale,"['Try using a profiler to determine which specific lines of code are are involved in a potential ""pause"". Please report back with a line-by-line breakdown of time taken and a code example for reproduction.', 'The offending call is _make_train_function in training.py.\r\n\r\nAs seen from profile output below, this is only affected by Theano on Cuda. I am using Theano master branch, commit 388805f946685e86225cdf602eb8a4f0059f9667 from April 17th.  I am using Keras master branch commit 73bf06fb023a8b37ddf2e2a168bbf920c7a6c766 from April 17th.\r\n\r\nBelow is profile output of a single-layer 64 neuron dense layer for 1 mini-batch on Cuda device. Blacked out areas for proprietary info.  First screenshot is running with Theano backend. Second screenshot is running with Tensorflow backend. Both have the same highlighted call. I also tested CPU on Theano and Tensorflow and there is no problem.  I am running on a GeForce GT 750M with CuDNN 5105.\r\n\r\nI will work on reproducible code. Currently this is embedded in a large proprietary project.\r\n\r\n![theano](https://cloud.githubusercontent.com/assets/596484/25103606/1487255c-2372-11e7-9902-92843d08c5ff.png)\r\n![tensorflow](https://cloud.githubusercontent.com/assets/596484/25103607/1490b14e-2372-11e7-81bd-b9c3231a3e58.png)\r\n', 'Normally, `_make_train_function` is only called once, then the resulting\nfunction is cached and reused. Why would it be called several times in your\ncase?\n\nOn 17 April 2017 at 13:37, wulabs <notifications@github.com> wrote:\n\n> The offending call is _make_train_function in training.py.\n>\n> As seen from profile output below, this is only affected by Theano. I am\n> using Theano master branch, commit 6c3d8b4302eab84c7ad8ad375c0d4061f675a11e\n> from April 13th.\n>\n> Below is profile output of a single-layer 64 neuron dense layer for 1\n> mini-batch on Cuda device. Blacked out areas for proprietary info. First\n> screenshot is running with Theano backend. Second screenshot is running\n> with Tensorflow backend. Both have the same highlighted call.\n>\n> I will work on reproducible code. Currently this is embedded in a large\n> proprietary project.\n>\n> [image: theano]\n> <https://cloud.githubusercontent.com/assets/596484/25103606/1487255c-2372-11e7-9902-92843d08c5ff.png>\n> [image: tensorflow]\n> <https://cloud.githubusercontent.com/assets/596484/25103607/1490b14e-2372-11e7-81bd-b9c3231a3e58.png>\n>\n> —\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/fchollet/keras/issues/6243#issuecomment-294585392>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AArWb-RSaeLJPpCH37T3M4gWXz9ZGcx7ks5rw81-gaJpZM4M8xpj>\n> .\n>\n', 'It seems `_make_train_function` is called for every call of `train_on_batch`. We are using this in a RL DQN so there are usually multiple calls to `train_on_batch`, one for each step in an epoch. The model is compiled only once, but inputs and outputs change for each epoch. For this use case, should I still only see one call to `_make_train_function`?\r\n\r\nIn the previous profiler generation there was an off-by-1 bug causing 2 epoch steps.  Attached is updated profile output for only 1 iteration, and hence one call to `train_on_batch`.  The `_make_train_function` delay still exists, but only for the first call (as can be seen between the difference in these and last screenshots). First screenshot is Theano-Cuda. Second is Tensorflow-Cuda.\r\n\r\n![theano_cuda](https://cloud.githubusercontent.com/assets/596484/25111633/01ecab36-23a1-11e7-8fb2-1f6be00d0b13.png)\r\n![tensorflow_cuda](https://cloud.githubusercontent.com/assets/596484/25111637/03d9da40-23a1-11e7-8c8d-ac6baf7cf0c5.png)\r\n', ""What's your code like? Do you have a snippet to reproduce?\n\nOn 17 April 2017 at 19:18, wulabs <notifications@github.com> wrote:\n\n> It seems _make_train_function is called for every call of train_on_batch.\n> We are using this in a RL DQN so there are usually multiple calls to\n> train_on_batch, one for each step in an epoch. The model is compiled only\n> once, but inputs and outputs change for each epoch. For this use case,\n> should I still only see one call to _make_train_function?\n>\n> In the previous profiler generation there was an off-by-1 bug causing 2\n> epoch steps. Attached is updated profile output for only 1 iteration, and\n> hence one call to train_on_batch. The _make_train_function delay still\n> exists, but only for the first call (as can be seen between the difference\n> in these and last screenshots). First screenshot is Theano-Cuda. Second is\n> Tensorflow-Cuda.\n>\n> [image: theano_cuda]\n> <https://cloud.githubusercontent.com/assets/596484/25111633/01ecab36-23a1-11e7-8fb2-1f6be00d0b13.png>\n> [image: tensorflow_cuda]\n> <https://cloud.githubusercontent.com/assets/596484/25111637/03d9da40-23a1-11e7-8c8d-ac6baf7cf0c5.png>\n>\n> —\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/fchollet/keras/issues/6243#issuecomment-294653236>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AArWb7lgfKhchobSKdLHXKoraXVBA9sTks5rxB2QgaJpZM4M8xpj>\n> .\n>\n"", 'Reproducible code below. Attached profile output screenshots for this snippet.\r\n\r\n```""""""\r\nTesting for Theano.Cuda _make_train_function delay bug.\r\n""""""\r\nimport numpy as np\r\nimport random\r\nnp.random.seed(7)\r\nrandom.seed(7)\r\n\r\nimport keras\r\nimport keras.layers as k_layers\r\nfrom keras.layers import  Dense, Input\r\nfrom keras.models import Model as Keras_Model\r\n\r\nx_train = np.array([[ 0.4995946,  0.4995946,  0.4995946,  0.4451019,  0.4451019, 0.4451019]])\r\ntargets = np.array([[ 0.00432717,  0. ]])\r\nx_input = Input(shape=(x_train.shape[1],))\r\n\r\nx = Dense(64, kernel_initializer=""uniform"")(x_input)\r\n\r\noutput = Dense(2, activation=\'linear\', name=\'output_layer\')(x)\r\n\r\nmodel = Keras_Model(inputs=x_input, outputs=output)\r\n\r\nmodel.compile(\r\n    optimizer=""adam"",\r\n    loss=""mse"",\r\n    metrics=[""accuracy""])\r\n\r\nmetrics = model.train_on_batch(x_train, targets)\r\nprint metrics\r\n```\r\n\r\nUsing Keras April 17 v2.0 commit 73bf06f\r\nUsing Theano April 17 TOT commit 388805f946685e86225cdf602eb8a4f0059f9667\r\nRunning on Theano.Cuda with GeForce GT 750M, CuDNN v5105\r\n\r\n![theano_cuda_snippet](https://cloud.githubusercontent.com/assets/596484/25112799/6e598d0a-23a8-11e7-9488-acb7a72cceb4.png)\r\n\r\n![tensorflow_cuda_snippet](https://cloud.githubusercontent.com/assets/596484/25112798/6e5755f8-23a8-11e7-966e-5bc2305b1c4c.png)', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],0,0
630,keras,6849,closed,Error when checking VGG16 input shape with include_top=False,"I'm trying to use Keras' implementation of VGG16 with grayscale images, channels first.
My training data is shaped as follows:



When I build my model:



it throws this error:

input_shape=' + str(input_shape) + 'input_shape=(1, 128, 128)

This seems odd since I am using  and  but it still seems to expect a 3 channel tensor and is passing my shape through , which seems inflexible in allowing for anything < 3 channels (see ).

Is this desirable?

If I keep  then I get a shape mismatch error at the beginning of training epoch 1.

Keras 2.0.4
Python 3.5",stale,"[""Although I do think that the flexibility is desirable, I just want to quickly point out for anyone else who reaches this thread what the argument, 'input_shape' specifies for vgg16:\r\n\r\n`        input_shape: optional shape tuple, only to be specified\r\n            if `include_top` is False (otherwise the input shape\r\n            has to be `(224, 224, 3)` (with `channels_last` data format)\r\n            or `(3, 224, 224)` (with `channels_first` data format).\r\n            **It should have exactly 3 input channels,\r\n            and width and height should be no smaller than 48.**\r\n            E.g. `(200, 200, 3)` would be one valid value.`\r\n\r\nso for now it looks like you'll have to copy out the architecture in order to apply it to grayscale or other formats"", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n', ""I'm having the same problem, but on TF 1.10.0\r\n\r\nThis is my line of code:\r\n`model = tf.keras.applications.VGG16(weights='imagenet',include_top=False,input_shape=(48,48,3))`\r\n\r\nAnd I get this:\r\n\r\n`ValueError: The input must have 3 channels; got `input_shape=(48, 48, 3)`\r\n\r\nMaybe I missed something? I admit I kind of new to using TF. I also hope this is the right place to post about the issue (Because I am not using Keras library directly, but rather through TF integration)"", '@amwfarid As you have set `include_top=False`, you must have `input_shape=(3,48,48)`']","['\r\nx_train shape: (7961, 1, 128, 128)\r\ny_train shape: (7961, 1)\r\nX_test shape: (1991, 1, 128, 128)\r\ny_test shape: (1991, 1)\r\n', '\r\nmodel_vgg16 = VGG16(include_top=False, \r\n                        weights=None, \r\n                        input_tensor=None, \r\n                        input_shape=(1, 128, 128), \r\n                        pooling=None,\r\n                        classes=CLASSES)\r\n']","['', '\r\nFile ""/Library/.../keras/applications/vgg16.py"", line 101, in VGG16\r\n    include_top=include_top)\r\n  File ""/Library/.../keras/applications/imagenet_utils.py"", line 116, in _obtain_input_shape\r\n    \'', ""')\r\nValueError: The input must have 3 channels; got "", '\r\n', '', 'include_top=False', 'weights=None', 'imagenet_utils.py', '_obtain_input_shape', 'input_shape=None']",0,0
631,keras,6893,closed,Error exporting model from keras/examples/babi_rnn.py,"If I run the keras/examples/babi_rnn.py script
using: 


it runs fine, from there are wanted to export the model according to your example code found on https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html#exporting-a-model-with-tensorflow-serving : 



If I do that I get the following error. Do you know what's going wrong?
...
",,[],"[""\r\n# Default QA1 with 1000 samples\r\nchallenge = 'tasks_1-20_v1-2/en/qa1_single-supporting-fact_{}.txt'\r\n"", '\r\n# serialize the model and get its weights, for quick re-building\r\nconfig = model.get_config()\r\nweights = model.get_weights()\r\n\r\n# re-build a model where the learning phase is now hard-coded to 0\r\nfrom keras.models import model_from_config\r\nnew_model = model_from_config(config)\r\nnew_model.set_weights(weights)\r\n\r\nfrom tensorflow_serving.session_bundle import exporter\r\n\r\nexport_path = ""./saved/"" # where to save the exported graph\r\nexport_version = ""1"" # version number (integer)\r\n\r\nsaver = tf.train.Saver(sharded=True)\r\nmodel_exporter = exporter.Exporter(saver)\r\nsignature = exporter.classification_signature(input_tensor=model.input,\r\n                                              scores_tensor=model.output)\r\nmodel_exporter.init(sess.graph.as_graph_def(),\r\n                    default_graph_signature=signature)\r\nmodel_exporter.export(export_path, tf.constant(export_version), sess)\r\n', '\r\n950/950 [==============================] - 7s - loss: 1.0030 - acc: 0.6074 - val_loss: 1.2415 - val_acc: 0.5200\r\n1000/1000 [==============================] - 1s     \r\nTest loss / test accuracy = 1.2763 / 0.4910\r\nTraceback (most recent call last):\r\n  File ""qa_test.py"", line 223, in <module>\r\n    new_model = model_from_config(config)\r\n  File ""/Users/travis/repos/tfenv/lib/python3.6/site-packages/keras/models.py"", line 304, in model_from_config\r\n    return layer_module.deserialize(config, custom_objects=custom_objects)\r\n  File ""/Users/travis/repos/tfenv/lib/python3.6/site-packages/keras/layers/__init__.py"", line 54, in deserialize\r\n    printable_module_name=\'layer\')\r\n  File ""/Users/travis/repos/tfenv/lib/python3.6/site-packages/keras/utils/generic_utils.py"", line 122, in deserialize_keras_object\r\n    raise ValueError(\'Improper config format: \' + str(config))\r\nValueError: Improper config format: {\'name\': \'model_1\', \'layers\': [{\'name\': \'input_2\', \'class_name\': \'InputLayer\', \'config\': {\'batch_input_shape\': (None, 4), \'dtype\': \'int32\', \'sparse\': False, \'name\': \'input_2\'}, \'inbound_nodes\': []}, {\'name\': \'embedding_2\', \'class_name\': \'Embedding\', \'config\': {\'name\': \'embedding_2\', \'trainable\': True, \'batch_input_shape\': (None, None), \'dtype\': \'int32\', \'input_dim\': 22, \'output_dim\': 50, \'embeddings_initializer\': {\'class_name\': \'RandomUniform\', \'config\': {\'minval\': -0.05, \'maxval\': 0.05, \'seed\': None}}, \'embeddings_regularizer\': None, \'activity_regularizer\': None, \'embeddings_constraint\': None, \'mask_zero\': False, \'input_length\': None}, \'inbound_nodes\': [[[\'input_2\', 0, 0, {}]]]}, {\'name\': \'input_1\', \'class_name\': \'InputLayer\', \'config\': {\'batch_input_shape\': (None, 66), \'dtype\': \'int32\', \'sparse\': False, \'name\': \'input_1\'}, \'inbound_nodes\': []}, {\'name\': \'dropout_2\', \'class_name\': \'Dropout\', \'config\': {\'name\': \'dropout_2\', \'trainable\': True, \'rate\': 0.3}, \'inbound_nodes\': [[[\'embedding_2\', 0, 0, {}]]]}, {\'name\': \'embedding_1\', \'class_name\': \'Embedding\', \'config\': {\'name\': \'embedding_1\', \'trainable\': True, \'batch_input_shape\': (None, None), \'dtype\': \'int32\', \'input_dim\': 22, \'output_dim\': 50, \'embeddings_initializer\': {\'class_name\': \'RandomUniform\', \'config\': {\'minval\': -0.05, \'maxval\': 0.05, \'seed\': None}}, \'embeddings_regularizer\': None, \'activity_regularizer\': None, \'embeddings_constraint\': None, \'mask_zero\': False, \'input_length\': None}, \'inbound_nodes\': [[[\'input_1\', 0, 0, {}]]]}, {\'name\': \'lstm_1\', \'class_name\': \'LSTM\', \'config\': {\'name\': \'lstm_1\', \'trainable\': True, \'return_sequences\': False, \'go_backwards\': False, \'stateful\': False, \'unroll\': False, \'implementation\': 0, \'units\': 50, \'activation\': \'tanh\', \'recurrent_activation\': \'hard_sigmoid\', \'use_bias\': True, \'kernel_initializer\': {\'class_name\': \'VarianceScaling\', \'config\': {\'scale\': 1.0, \'mode\': \'fan_avg\', \'distribution\': \'uniform\', \'seed\': None}}, \'recurrent_initializer\': {\'class_name\': \'Orthogonal\', \'config\': {\'gain\': 1.0, \'seed\': None}}, \'bias_initializer\': {\'class_name\': \'Zeros\', \'config\': {}}, \'unit_forget_bias\': True, \'kernel_regularizer\': None, \'recurrent_regularizer\': None, \'bias_regularizer\': None, \'activity_regularizer\': None, \'kernel_constraint\': None, \'recurrent_constraint\': None, \'bias_constraint\': None, \'dropout\': 0.0, \'recurrent_dropout\': 0.0}, \'inbound_nodes\': [[[\'dropout_2\', 0, 0, {}]]]}, {\'name\': \'dropout_1\', \'class_name\': \'Dropout\', \'config\': {\'name\': \'dropout_1\', \'trainable\': True, \'rate\': 0.3}, \'inbound_nodes\': [[[\'embedding_1\', 0, 0, {}]]]}, {\'name\': \'repeat_vector_1\', \'class_name\': \'RepeatVector\', \'config\': {\'name\': \'repeat_vector_1\', \'trainable\': True, \'n\': 66}, \'inbound_nodes\': [[[\'lstm_1\', 0, 0, {}]]]}, {\'name\': \'add_1\', \'class_name\': \'Add\', \'config\': {\'name\': \'add_1\', \'trainable\': True}, \'inbound_nodes\': [[[\'dropout_1\', 0, 0, {}], [\'repeat_vector_1\', 0, 0, {}]]]}, {\'name\': \'lstm_2\', \'class_name\': \'LSTM\', \'config\': {\'name\': \'lstm_2\', \'trainable\': True, \'return_sequences\': False, \'go_backwards\': False, \'stateful\': False, \'unroll\': False, \'implementation\': 0, \'units\': 50, \'activation\': \'tanh\', \'recurrent_activation\': \'hard_sigmoid\', \'use_bias\': True, \'kernel_initializer\': {\'class_name\': \'VarianceScaling\', \'config\': {\'scale\': 1.0, \'mode\': \'fan_avg\', \'distribution\': \'uniform\', \'seed\': None}}, \'recurrent_initializer\': {\'class_name\': \'Orthogonal\', \'config\': {\'gain\': 1.0, \'seed\': None}}, \'bias_initializer\': {\'class_name\': \'Zeros\', \'config\': {}}, \'unit_forget_bias\': True, \'kernel_regularizer\': None, \'recurrent_regularizer\': None, \'bias_regularizer\': None, \'activity_regularizer\': None, \'kernel_constraint\': None, \'recurrent_constraint\': None, \'bias_constraint\': None, \'dropout\': 0.0, \'recurrent_dropout\': 0.0}, \'inbound_nodes\': [[[\'add_1\', 0, 0, {}]]]}, {\'name\': \'dropout_3\', \'class_name\': \'Dropout\', \'config\': {\'name\': \'dropout_3\', \'trainable\': True, \'rate\': 0.3}, \'inbound_nodes\': [[[\'lstm_2\', 0, 0, {}]]]}, {\'name\': \'dense_1\', \'class_name\': \'Dense\', \'config\': {\'name\': \'dense_1\', \'trainable\': True, \'units\': 22, \'activation\': \'softmax\', \'use_bias\': True, \'kernel_initializer\': {\'class_name\': \'VarianceScaling\', \'config\': {\'scale\': 1.0, \'mode\': \'fan_avg\', \'distribution\': \'uniform\', \'seed\': None}}, \'bias_initializer\': {\'class_name\': \'Zeros\', \'config\': {}}, \'kernel_regularizer\': None, \'bias_regularizer\': None, \'activity_regularizer\': None, \'kernel_constraint\': None, \'bias_constraint\': None}, \'inbound_nodes\': [[[\'dropout_3\', 0, 0, {}]]]}], \'input_layers\': [[\'input_1\', 0, 0], [\'input_2\', 0, 0]], \'output_layers\': [[\'dense_1\', 0, 0]]}\r\n']",[],0,0
632,keras,1854,closed,LSTM argument,"I am getting an error when i am trying to change those argument inside a LSTM function 
Did they always exist or did the typo changed ? Because they are on the keras documentation. Thanks!
",,"['These are new parameters (introduced ~last week), are you using the latest Keras?\n', 'Well i am using 0.3.2, and a pip install --upgrade keras gives me a Requirement already up-to-date..\n', 'This feature is not included in 0.3.2.  When creating an issue, you are presented with (...you seemed to delete):\n- [ ] Check that you are up-to-date with the master branch of Keras. You can update with:\n  pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps\n', 'You are the man! Thanks for the great answer!\n', ""You're welcome. :)\n""]",[],"['LSTM(W_regularizer=None, U_regularizer=None, b_regularizer=None, dropout_W=0.0, dropout_U=0.0)']",0,0
633,keras,11410,closed,ConnectionResetError: [Errno 104] Connection reset by peer,"Getting this error when I try to load resnet50:



And when I try to load xception i get this:

input_shape=(48, 48, 3)

Please check the code sample [here](https://gist.github.com/hasibzunair/606c9037abed51836e5dc6059141bfff)",User error,"['Connection reset by peer is your internet network not working, the xception error message is already very clear and explicit. You cannot set this input shape with the network.', '@gabrieldemarmiesse thank you.']",[],"['ConnectionResetError: [Errno 104] Connection reset by peer', 'ValueError: Input size must be at least 71x71; got ', '']",0,0
634,keras,2293,closed,master branch seems not compatible ,"master branch seems not compatible with previous version's model dump file, when using json read the old model file, there's a bug
",,"[""Yes, JSON files saved with previous can't be loaded with the new version. Everything else (with a few exceptions) is backwards compatible.\n""]",[],[],0,0
635,keras,11673,closed,Callbacks documentation not showing bullet points correctly,"The current [documentation on callbacks](https://keras.io/callbacks/) isn't showing bullet points correctly under the ""Arguments"" section of a few models. Here's the example for :

> filepath: string, path to save the model file. monitor: quantity to monitor. verbose: verbosity mode, 0 or 1. save_best_only: if save_best_only=True, the latest best model according to the quantity monitored will not be overwritten. mode: one of {auto, min, max}. If save_best_only=True, the decision to overwrite the current save file is made based on either the maximization or the minimization of the monitored quantity. For val_acc, this should be max, for val_loss this should be min, etc. In auto mode, the direction is automatically inferred from the name of the monitored quantity. save_weights_only: if True, then only the model's weights will be saved (model.save_weights(filepath)), else the full model is saved (model.save(filepath)). period: Interval (number of epochs) between checkpoints.

Looking at the source code, the docstring seems to be organized correctly:
https://github.com/keras-team/keras/blob/dc9e510192d0a8a6f6943cd46e9554364d4dcdd2/keras/callbacks.py#L371-L390

It is however showing up correctly for other models, e.g. :

> # Arguments
> * __count_mode:__ One of ""steps"" or ""samples"". Whether the progress bar should count samples seen or steps (batches) seen.
> * __stateful_metrics:__ Iterable of string names of metrics that should not be averaged over an epoch. Metrics in this list will be logged as-is. All others will be averaged over time (e.g. loss, etc).",stat:contributions welcome type:docs,"[""Thank you for reporting it. Contributions are welcome. It's really easy to break the formatting of the documentation. I wonder if there is a test we could run or some kind of tool to help us check that at each PR. Sorry for the inconvenience.\r\n\r\nIt should be fairly easy to fix by checking what differs from other docstrings."", 'This appears to be a widespread issue, a number of methods are affected. At least, those whose Argument section is the last section of the docstring.\r\n\r\nIt can be ""fixed"" by adding a newline at the end of the docstring. For example, in `optimizers.py` change from\r\n```\r\nclass SGD(Optimizer):\r\n    """"""Stochastic gradient descent optimizer.\r\n\r\n    Includes support for momentum,\r\n    learning rate decay, and Nesterov momentum.\r\n\r\n    # Arguments\r\n        lr: float >= 0. Learning rate.\r\n        momentum: float >= 0. Parameter that accelerates SGD\r\n            in the relevant direction and dampens oscillations.\r\n        decay: float >= 0. Learning rate decay over each update.\r\n        nesterov: boolean. Whether to apply Nesterov momentum.\r\n    """"""\r\n```\r\nto\r\n```\r\nclass SGD(Optimizer):\r\n    """"""Stochastic gradient descent optimizer.\r\n\r\n    Includes support for momentum,\r\n    learning rate decay, and Nesterov momentum.\r\n\r\n    # Arguments\r\n        lr: float >= 0. Learning rate.\r\n        momentum: float >= 0. Parameter that accelerates SGD\r\n            in the relevant direction and dampens oscillations.\r\n        decay: float >= 0. Learning rate decay over each update.\r\n        nesterov: boolean. Whether to apply Nesterov momentum.\r\n\r\n    """"""\r\n```\r\n\r\nBut that\'s probably not the way to proceed. It would require changing a number of existing docstrings.\r\n\r\nAs far as I can tell by generating the docs with various code versions, the root cause is a recent change to `autogen.py` itself:\r\n1) I observe the problem with dcb5e3cf - master from today.\r\n2) I observe the problem with a37b41d5 - master from October 29.\r\n3) I do not observe the problem with aa0f12ce - the parent commit of a37b41d5.\r\n\r\nCertainly I could be wrong, but it seems that #11508 caused this regression.\r\n\r\nI looked at the code a bit, but could not immediately determine how to change it.', 'Related general docs questions: Under what circumstances are the live keras.io docs published? Probably with each new release? Am I missing an indicator of the Keras version corresponding to the live docs?', 'fc327b952f09c9229ee6a6bd1ed1b47d2e0dc3f5 over in my fork has a test that\r\n1) fails for dcb5e3cf\r\n2) fails for a37b41d5\r\n3) passes for aa0f12ce', 'Thank you for the investigation. This is very helpful. Yes, the docs are published after each new release.\r\n\r\nI think our best option would be more tests, and a better algorithm to extract the docstring and put it in markdown. Currently, the algorithm to do this is quite complicated. I believe it could be simplified. Would you want to make a PR? ', ""The root of the problem is more the function `process_docstring` in `autogen.py`. It's very complex, error prone, we can't easily test it. In the end, I believe there must be a better solution. "", 'This issue was fixed by #12007 ']",[],"['ModelCheckpoint', 'ProgbarLogger']",0,0
636,keras,3667,closed,ValueError caused by the BatchNormalization() layer,"Hi there!
Is there a way I could fix this error? it is caused by the **BatchNormalization()** layer..  



It happens when I run the following code:



My machine configuration:
Operating system: openSuse
Theano version: '0.9.0dev2'
Keras version: 1.0.8
cuDNN version: v5.1
CUDA version: 7.5

Thanks!
",,"['It worked when I downgraded **Theano** to **0.8.2**.  :)\n', ""I'm also getting this error on the newest theano/keras\n"", 'I just fixed the issue, see my pull request.\n']","["" python\nmain_model = Sequential()\nmain_model.add(Convolution2D(16, 10, 10, border_mode='valid', \n               input_shape=(3, img_rows, img_cols)))\n\nmain_model.add(Flatten())\nmain_model.add(Dense(128))\nmain_model.add(BatchNormalization())       \nmain_model.add(Activation('relu'))\nmain_model.add(Dense(2))\nmain_model.add(Activation('softmax'))\n\nsgd = keras.optimizers.RMSprop()\nmain_model.compile(loss='categorical_crossentropy', optimizer = sgd)       \n""]",['ValueError: gamma and beta must be of the same dimensionality as inputs; got 1 \nand 1 instead of 2'],0,0
637,keras,8823,closed,Can not get state and memory from LSTM layer when wrap by a Bidirectional layer ?,"I have a model to work with LSTM layer. It works fine :
 
    encoder_inputs = Input(shape=(None, num_encoder_tokens))
    encoder = LSTM(latent_dim, return_state=True)
    encoder_outputs, state_h, state_c = encoder(encoder_inputs)

But when i add a wrap by a Bidirectional layer : 

    encoder_inputs = Input(shape=(None, num_encoder_tokens))
    encoder = Bidirectional(LSTM(latent_dim, return_state=True),merge_mode=""mul"")
    encoder_outputs, state_h, state_c = encoder(encoder_inputs)

It lead me an error : 

    output = y * y_rev
    TypeError: can't multiply sequence by non-int of type 'list'.
So how can i get both output, state and memory with Bidirectional layer ? When i remove  , it also works",,"['[This line](https://github.com/keras-team/keras/blob/master/keras/layers/wrappers.py#L290) takes the output of the layer, which in the case of `return_states=True` , is a list. \r\nAnd you cannot multiply the lists (from forward and backward layer) at [this line](https://github.com/keras-team/keras/blob/master/keras/layers/wrappers.py#L301)\r\n\r\nMy guess is that apart from changing the source code to handle `return_states=True` in the `Bidirectional` wrapper, there is no way around it', 'So should i create 2 LSTM , then merge it by keras,layer.Add() ? But how can i get state and memory from Add layer ? (its result is a tensor ) @mpariente ', '@astrung This is a gist showing [my attempt](https://gist.github.com/mpariente/24f1e3b43e5a999acaa97a42df9a4ed9) to fix your problem.\r\nI create a new class, child of Bidirectional, which overrides the method `call` to handle the argument `return_state=True`, and the method `compute_output_shape` to take into account that the states are returned. \r\n\r\n@fchollet  Would it be worth it to add something like this to the source code or is it too specific? \r\nShould I consider making a PR? (Obviously not adding the class but enabling support for `return_state=True` in the `Bidirectional` wrapper)\r\n', ""@mpariente I also want multiply both states, so i modify your code to multiply both states for forward RNN and backward RNN: \r\n \r\n    ...        \r\n    elif self.merge_mode == 'mul':\r\n        output = y * y_rev\r\n        states_final_h = states_h * states_h_rev\r\n        states_final_c = states_c * states_c_rev\r\n\r\nthen : \r\n\r\n    ....\r\n    if self.return_state:\r\n        # states = [states_h, states_h_rev, states_c, states_c_rev]\r\n        states = [states_final_h,states_final_c]\r\n\r\nOk now i re-use its states for another LSTM : \r\n\r\n    decoder_inputs = Input(shape=(None, num_decoder_tokens))\r\n    decoder_lstm = LSTM(latent_dim, return_state=True)\r\n    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\r\n                               initial_state=encoder_states)  #encoder_states is passed from a Bidirectional encoder\r\n    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\r\n    decoder_outputs = decoder_dense(decoder_outputs)\r\nbut i got an error : \r\n\r\n    ValueError: Error when checking target: expected dense_1 to have 2 dimensions, but got array with shape (10000L, 63L, 92L)\r\n\r\nYou can check my model here : https://gist.github.com/astrung/5b47436f62f91bebd1666677674b837f"", 'If you make an example with random data, I can run it and try to solve the problem.', '@mpariente i updated the gist code for random data with same error. Please check. I am trying to test some modify model from this seq2seq model : https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html', ""I get this error message \r\n```\r\nValueError: Dimensions must be equal, but are 512 and 256 for 'lstm_8/MatMul_4' (op: 'MatMul') with input shapes: [?,512], [256,256].\r\n```\r\nWhich happens at this line : \r\n```\r\ndecoder_outputs, _, _ = decoder_lstm(decoder_inputs,\r\n                                     initial_state=encoder_states)\r\n```\r\nI cannot get the problem, which goes beyond the backend, and I'm not familiar with tensorflow's source code. "", '@mpariente can you comment `initial_state` to see what happen ?. It may be lead to my problem \r\n\r\n    decoder_outputs, _, _ = decoder_lstm(decoder_input)\r\n                                     ## initial_state=encoder_states)', ""This compiles and train :\r\n```\r\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\r\ndecoder_outputs, _, _ = decoder_lstm(decoder_inputs)\r\n```\r\nI didn't work on these architectures yet so I'm not familiar with them. I don't know if it makes sense but this is what is in the tuto.  "", 'I have the same problem (trying to get the states from the bidirectional lstm).\r\nIt seems to be a dimensionality problem:\r\n\r\n```\r\n# Define an input sequence and process it.\r\nencoder_inputs = Input(shape=(None, num_encoder_tokens))\r\n# encoder = LSTM(latent_dim, return_state=True)\r\nencoder = MyBidirectional(LSTM(latent_dim, return_state=True), merge_mode=""sum"")\r\noutputs_and_states = encoder(encoder_inputs)\r\n```\r\n\r\nI think the shapes inside outputs_and_states should be 256 but they are 512:\r\n```\r\n[<tf.Tensor \'my_bidirectional_3/concat:0\' shape=(?, 512) dtype=float32>,\r\n <tf.Tensor \'my_bidirectional_3/concat_1:0\' shape=(?, 512) dtype=float32>,\r\n <tf.Tensor \'my_bidirectional_3/concat_2:0\' shape=(?, 512) dtype=float32>]  \r\n``` \r\n\r\nThe initial_state=encoder_states is required for the seq2seq model has the encoding states are passed to the decoder.', ""I made some change to the Bidirectional class which seems to hold the expected dimensionality and with the expected initial_state=encoder_states:\r\nhttps://gist.github.com/T-B-F/0eaef6b8bb9207a576ffc3e3d3676fc2 \r\n\r\nbut I got an other error regarding the target_output_data dim:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-84-6ed69e21a6f9> in <module>()\r\n      2           batch_size=batch_size,\r\n      3           epochs=epochs,\r\n----> 4           validation_split=0.2)\r\n      5 \r\n\r\n/home/tristanbf/.virtualenvs/pydev3/lib/python3.5/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_ep\r\noch, steps_per_epoch, validation_steps, **kwargs)\r\n   1579             class_weight=class_weight,\r\n   1580             check_batch_axis=False,\r\n-> 1581             batch_size=batch_size)\r\n   1582         # Prepare validation data.\r\n   1583         do_validation = False\r\n\r\n/home/tristanbf/.virtualenvs/pydev3/lib/python3.5/site-packages/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\r\n   1416                                     output_shapes,\r\n   1417                                     check_batch_axis=False,\r\n-> 1418                                     exception_prefix='target')\r\n   1419         sample_weights = _standardize_sample_weights(sample_weight,\r\n   1420                                                      self._feed_output_names)\r\n\r\n/home/tristanbf/.virtualenvs/pydev3/lib/python3.5/site-packages/keras/engine/training.py in _standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)\r\n    139                                  ' to have ' + str(len(shapes[i])) +\r\n    140                                  ' dimensions, but got array with shape ' +\r\n--> 141                                  str(array.shape))\r\n    142             for j, (dim, ref_dim) in enumerate(zip(array.shape, shapes[i])):\r\n    143                 if not j and not check_batch_axis:\r\n\r\nValueError: Error when checking target: expected dense_6 to have 2 dimensions, but got array with shape (100, 12, 32)\r\n```\r\n\r\nEdit: Error, which is also happening when removing the Bidirectional() wrapper. "", 'Ok, need to replace \r\n\r\n```\r\ndecoder_lstm = LSTM(latent_dim, return_state=True)\r\n```\r\nby\r\n\r\n```\r\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\r\n```\r\n\r\n(I updated my gist)', 'You cannot use this implementation of the `Bidirectional` wrapper with `return_state=False`.  \r\nYou need to handle states inside of `if return_states:`\r\n', '@mpariente i fixed and updated it. It works now. But i really want to see if i can use it directly from keras']",[],['return_state=True'],0,0
638,keras,4630,closed,How to allow for preprocessing of image channel values in ImageDataGenerator and flow_from_directory?,"In short we only see rescale option in ImageDataGenerator.  How do you do the following for pertained VGG16 network that needs the channels values preprocessed as follows:
    im = cv2.resize(cv2.imread('cat.jpg'), (224, 224)).astype(np.float32)
    im[:,:,0] -= 103.939
    im[:,:,1] -= 116.779
    im[:,:,2] -= 123.68
    im = im.transpose((2,0,1))
    im = np.expand_dims(im, axis=0)
Where do we specify these options in ImageDataGenerator?

Thanks
Dr 
",stale,"[""There is a tread #3338 and  gives a solution by adding a pipeline with fork of imageGenerator in keras. I think is that your looking for, sorry if I am wrong. \r\n\r\n**A more manual way of doing this is:**\r\n\r\nPass the batches created in the form below trough a method that treats you data as you want it.\r\n\r\n```\r\nfor e in range(nb_epoch):\r\n    print 'Epoch', e\r\n    batches = 0\r\n    for X_batch, Y_batch in datagen.flow(X_train, Y_train, batch_size=32):\r\n        loss = model.train(X_batch, Y_batch)\r\n        batches += 1\r\n        if batches >= len(X_train) / 32:\r\n            # we need to break the loop by hand because\r\n            # the generator loops indefinitely\r\n            break\r\n```"", 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs, but feel free to re-open it if needed.\n']",[],[],0,0
639,keras,1852,closed,"Does the theory of dropout layer in keras refer to (Hinton et al., 2012)?","The theory of dropout layer in keras is the same as ""Hinton G., N. Srivastava, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. 2012. Improving Neural Networks by Preventing Co-adaptation of Feature Detectors. ResearchGate.""?
",,"['@EderSantana @fchollet @dbonadiman Could you spend your valuable time solving this question? Thanks a lot.\n', 'yes, it does!\n', 'Theoretically it does, but the implementation is a little bit different. \n', ""(Also note that the p's in the original paper represent the probability of propagating, while the common current usage, also in Keras, is that p represents the probability of dropping.)\n"", ""@EderSantana @shampool @pasky Thanks! By dropping a unit out, it means removing\nthe unit from the network, along with all its incoming and outgoing edges. However, I print the output of every layer. I find that the dimension of dropout layer output is the same as last output, which does not change. Why? \n\n```\nmodel = Sequential()\n\n model.add(LSTM(lstm_output_dims, input_shape=(1, max_features)))\n\n model.add(Dense(hidden_dims))\n\n model.add(Activation('relu'))\n\n model.add(Dropout(0.5))    \n\n model.add(Dense(1))\n\n model.add(Activation('sigmoid'))\n```\n"", 'It drops the weights by multiplying them with a random matrix consists of 1 and 0, 1 means retain, 0 means dropout.\n']",[],[],0,0
640,keras,12354,closed,UpSampling2D throwing error with keyword 'interpolation',"I'm getting a weird error when I try to create a network using an upsampling layer, when I manually set the interpolate keyword to bilinear. If I leave it out, and go with the default of 'nearest neighbour; it works fine. Does anyone know what's up?

TF = 1.12.0
Keras = 2.2.2
OS = Ubuntu 18.04


Code for model. Error is thrown at layer 'up1'

    chnl4_input = Input(shape=(368, 256, 4))
    chnl3_input = Input(shape=(736, 512, 3))

    conv1 = Conv2D(26, self.kernel_size, activation='relu', padding='same')(chnl4_input)
    conv2 = Conv2D(26, self.kernel_size, strides=(2, 2), activation='relu', padding='same')(conv1)

    conv5 = Conv2D(64, self.kernel_size, activation='relu', padding='same')(conv2)
    conv6 = Conv2D(64, self.kernel_size, activation='relu', padding='same')(conv5)

    up1 = concatenate([UpSampling2D(size=(2, 2), interpolation='bilinear')(conv6), conv1], axis=-1)
    conv7 = Conv2D(64, self.kernel_size, activation='relu', padding='same')(up1)

    conv8 = Conv2D(64, self.kernel_size, activation='relu', padding='same')(conv7)
    conv9 = Conv2D(64, self.kernel_size, activation='relu', padding='same')(conv8)

    conv11 = Conv2D(64, self.kernel_size, activation='relu', padding='same')(conv9)
    conv12 = Conv2D(64, self.kernel_size, activation='relu', padding='same')(conv11)

    up3 = concatenate([UpSampling2D(size=(2, 2), interpolation='bilinear')(conv12), chnl3_input], axis=-1)
    conv13 = Conv2D(67, self.kernel_size, activation='relu', padding='same')(up3)

    conv14 = Conv2D(67, self.kernel_size, activation='relu', padding='same')(conv13)
    conv15 = Conv2D(32, self.kernel_size, activation='relu', padding='same')(conv14)
    conv16 = Conv2D(3, self.kernel_size, activation='relu', padding='same')(conv15)

    out = conv16

    self.model = Model(inputs=[chnl4_input, chnl3_input], outputs=[out])

    self.model.compile(optimizer=self.optimizer_func, loss=self.loss_func)
    self.model.name = 'UNET'

    return self.modele here`

Errror = TypeError: ('Keyword argument not understood:', 'interpolation')",,"[""I think this is because you don't have the latest version of keras. Can you check it?"", 'Are there any news on this? I am facing the same issue as @FCOS  in my virtual environment with (currently) newest versions of Tensorflow and Keras installed:\r\n\r\nTF = 1.13.1\r\nKeras = 2.2.4\r\nPython = 3.6.7\r\nOS = Ubuntu 18.04\r\n\r\n\r\nGreat thanks for any kind of update with this respect in advance, guys!', ""I had a root around the keras code, and it seems to be that 'interpolation' is simply left out of the valid arguments list for the upSampling2D function. You can modify it yourself if you're bothered, but you can bypass the problem altogether by just leaving out the interpolation argument. That way it will default to nearest neighbour interpolation. The performance difference is negligible between the two."", 'Alright, I see.. too bad. The bilinear case works better for my purposes, so I will try to figure an alternative out - thanks a lot, @FCOS ! (:', 'I had the same issue with Keras 2.2.2 but it seemed to be working in 2.2.4\r\n![image](https://user-images.githubusercontent.com/15525366/61409355-af908680-a8a7-11e9-9734-fecc1de227a5.png)\r\n', 'Closing this issue since it works on the master branch. ', 'It works well with ""cuda=9.0, tensorflow=1.11.0, keras=2.2.5""']",[],[],0,0
641,keras,4262,closed,cannot import backend from keras,"I am trying to build my custom function in Keras as shown in:

https://github.com/fchollet/keras/blob/master/keras/objectives.py

However, I get this error:

    $ ipython 
    Python 2.7.12 |Anaconda 4.2.0 (x86_64)| (default, Jul  2 2016, 17:43:17) 
    Type ""copyright"", ""credits"" or ""license"" for more information.
    
    IPython 5.1.0 -- An enhanced Interactive Python.
    ?         -> Introduction and overview of IPython's features.
    %quickref -> Quick reference.
    help      -> Python's own help system.
    object?   -> Details about 'object', use 'object??' for extra details.
    
    In [1]: import keras
    Using TensorFlow backend.
    im
    In [2]: from . import backend as K
    ---------------------------------------------------------------------------
    ValueError                                Traceback (most recent call last)
    <ipython-input-2-b7b972de130c> in <module>()
    ----> 1 from . import backend as K
    
    ValueError: Attempted relative import in non-package

Any help? Thank you very much.",stale,"['Relative imports only work for packages. Why are you importing ""backend"" this way? Where did you define your custom objective?\n', '`from keras import backend as K`\n\nLike your error literally states.\n', 'Thanks. The second issue is that after I save and load the model with custom object, I have this error:\n\n```\n\n    def _save_model(self,net_transfer):\n        net_transfer.save(self._load_nn_location)\n\n    def _load_model(self):\n        net_transfer = keras.models.load_model(self._load_nn_location)\n        return net_transfer\n\n```\n\n```\n\n  File ""/Users/a/Desktop/software/WX-site-packages/Quantomic_new/Quantomic_dev_2016_10_05/Quantomic/strategy/QTNNSimplePredictTensorflowDataGen.py"", line 346, in build\n    self._net = self._load_model()\n  File ""/Users/a/Desktop/software/WX-site-packages/Quantomic_new/Quantomic_dev_2016_10_05/Quantomic/strategy/QTNNSimplePredictTensorflowDataGen.py"", line 657, in _load_model\n    net_transfer = keras.models.load_model(self._load_nn_location)\n  File ""/Users/a/anaconda2/lib/python2.7/site-packages/keras/models.py"", line 155, in load_model\n    sample_weight_mode=sample_weight_mode)\n  File ""/Users/a/anaconda2/lib/python2.7/site-packages/keras/models.py"", line 547, in compile\n    **kwargs)\n  File ""/Users/a/anaconda2/lib/python2.7/site-packages/keras/engine/training.py"", line 537, in compile\n    loss_function = objectives.get(loss)\n  File ""/Users/a/anaconda2/lib/python2.7/site-packages/keras/objectives.py"", line 77, in get\n    return get_from_module(identifier, globals(), \'objective\')\n  File ""/Users/a/anaconda2/lib/python2.7/site-packages/keras/utils/generic_utils.py"", line 16, in get_from_module\n    str(identifier))\nException: Invalid objective: custom_objective\n\n```\n\nDoes i mean that I could not save the custom objective model? (when others load it, I do not want them to build the model first and load the weight.... I just want them to load as is)\n', 'Post the code that includes custom_objective please.\nAre you skipping the definition of custom_objective when you load the model?\n', 'Here\'s the part i define my custom_objective:\n\n``````\n\n    def _generate_model(self):\n        model=keras.models.Sequential()\n\n        for i in range(len(self._hidden_layer_dim)):\n            layer_now=self._hidden_layer_dim[i]\n            print(""i now is "",i)\n            if i==0:\n                model.add(keras.layers.Dense(layer_now,input_dim=self._input_dimension,\n                                             W_regularizer=keras.regularizers.l1(self._weight_decay),\n                                             activity_regularizer=keras.regularizers.activity_l1(self._weight_decay)))\n            else:\n                model.add(keras.layers.Dense(layer_now,W_regularizer=keras.regularizers.l1(self._weight_decay),\n                                             activity_regularizer=keras.regularizers.activity_l1(self._weight_decay)))\n\n            print(""dropout rate is "", self._dropout_rate)\n            if self._activation==""PReLU"":\n                model.add(keras.layers.advanced_activations.PReLU())\n            else:\n                model.add(keras.layers.Activation(self._activation))\n            model.add(keras.layers.Dropout(self._dropout_rate))\n\n        model.add(keras.layers.Dense(self._output_dimension,W_regularizer=keras.regularizers.l1(self._weight_decay)))\n        def custom_objective(y_true, y_pred):\n            from keras import backend as K\n\n            \'\'\'Just another cross entropy\'\'\n            # y_pred_np=np.array(y_pred)\n            # y_true_np=np.array(y_true)\n            # loss_arr = np.sign(y_pred_np)*(y_pred_np-y_true_np)\n            # negative_index = loss_arr < 0\n            # loss_arr [negative_index] = 0\n            # loss = np.mean(loss_arr)\n\n            loss_arr = K.sign(y_pred)*(y_pred-y_true)\n            loss_arr = K.clip(loss_arr,0,np.inf)\n            loss = K.mean(loss_arr)\n\n            return loss\n\n        # loss=\'mean_absolute_error\'\n        loss=custom_objective\n\n        model.compile(optimizer=\'Adam\',loss=loss,metrics=[loss,\'mean_absolute_error\',\'mean_squared_error\',\'mean_squared_logarithmic_error\'])\n        return model\n\n```\'\n\n\nDo I need to define this more globally?\n``````\n', 'i see... when I put the definition in a more global setting (put it on top of  the file), everything works....\n', ""Then I guess custom objectives are not saved, and indeed need to be defined before loading the model.\nYou can also see it here:\nhttps://github.com/fchollet/keras/blob/master/keras/models.py#L64\n\nIt saves the reference to the loss function as a string, and in your case the reference doesn't exist during loading.\n"", '> Relative imports only work for packages. Why are you importing ""backend"" this way? Where did you define your custom objective?\r\n\r\n\r\nSame error arises in official keras/applications example  [https://github.com/fchollet/keras/blob/master/keras/applications/music_tagger_crnn.py](https://github.com/fchollet/keras/blob/master/keras/applications/music_tagger_crnn.py)\r\n\r\n```\r\n~/keras/keras/applications$ python music_tagger_crnn.py \r\nTraceback (most recent call last):\r\n  File ""music_tagger_crnn.py"", line 12, in <module>\r\n    from .. import backend as K\r\nValueError: Attempted relative import in non-package\r\n~/keras/keras/applications$\r\n\r\n```\r\n...So, is there some kind of ""import path"" that I need to set?  The directories are there...\r\n\r\n\r\n```\r\n~/keras/keras/applications$ ls -F ..\r\nactivations.py  backend/      constraints.py  engine/             __init__.py  metrics.py  objectives.py  preprocessing/   utils/\r\napplications/   callbacks.py  datasets/       initializations.py  layers/      models.py   optimizers.py  regularizers.py  wrappers/\r\n```\r\n\r\n...?  Happens to me in two different keras repo clones, on both Mac and Ubuntu 16, running either Python 2.7 or Python 3.5.', ""@drscotthawley It's still the same issue: that file you are referencing:\r\n- Is not an example\r\n- Is a part of the Keras package\r\n- includes no runnable code, just a class\r\n\r\nIf you wish to use the application, you would use it in the same way as any other application:[ import it and then instantiate it](https://keras.io/applications/).\r\n"", '@patyork Thanks.  I misunderstood the use of the word ""applications"", thinking it would be executable.  I just now found & ran the usage example in the docs with no problems. :-)', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],0,0
642,keras,10693,closed,Segmentation fault (core dumped) happened when I use more than one session.,"Hello. I'm new to Keras. I'm using TensorFlow as a backend. I'd like to load many models and use those in different threads. For different threads, I have to use different graphs, so I have to use different sessions. (Am I right?)

Here is a trimmed version of my code that can reproduce this error. What am I doing wrong?

I am using Keras-2.2.0 and tensorflow 1.8.0 without GPU on Ubuntu 16.04.4.

    import tensorflow as tf
    import numpy as np

    from keras.models import Sequential
    from keras.layers import Masking

    def predict():
        model = Sequential()
        layer = Masking(batch_input_shape=[None, 1])
        model.add(layer)

        x = np.array([[0]], dtype='int32')

        model.predict(x, batch_size=1)

    if __name__ == '__main__':
        with tf.Session():
            predict()

        with tf.Session():
            predict()
",,"['The error disappeared using tensorflow 1.9.0.', '> The error disappeared using tensorflow 1.9.0.\r\n\r\nthis solution save my life i ve been puzzled without any idea for so long! thank you so much!']",[],[],0,0
643,keras,13006,closed,Keras training and validation accuracy always converges to 0.5,"**System information**  
- Have I written custom code (as opposed to using example directory): yes
- OS Platform and Distribution: Linux ubuntu 4.15.0-51-generic #16~18.04.1-Ubuntu SMP
- TensorFlow backend: yes
- TensorFlow version: 1.13.1
- Keras version: 2.2.4
- Python version: 3.6.8

I have used a model (provided [here](https://medium.com/@ksusorokina/image-classification-with-convolutional-neural-networks-496815db12a8)] that trains a model on two categories of pictures and then tries to classify them.
Furthermore, I force the network to use the same seeds when training so as to get comparable results. I also create and close the tf sessions as I have read that this may also cause problems.

**Describe the current behaviour** 
Most of the time the test and validation accuracy converge around 0.5 and the loss stays at exactly the same value for every epoch. I can run the same code, without changing it, several times in a row and get this problem and only rarely do I get a run where the neural network is trained properly and rises to and above 0.9 accuracy for training and validation.


**Describe the expected behaviour**  
For the same seeds the run should produce at least very similar results i.e. it should not be stuck around 0.5 accuracy.

**Code to reproduce the issue**  


[Edit]
I've tried a [different tutorial](https://www.geeksforgeeks.org/python-image-classification-using-keras/) and I get the same result. The only changes I have done to the code is to change the image dimension to the size of my images, changed the image directories and to turn off Image Augmentation, as well as changed the number of images.

About half of my runs produced output such as this

while the other half produced 

(the numbers were not always exactly the same).

I though that maybe I have bad images so I tried use images from other tutorials (like [this](https://github.com/perseus784/BvS)) but I got the same results: about half or more of the runs are useless because they converge at 0.5 accuracy. What's going on?",type:support,"['@Randryn0 can you provide a standalone code to reproduce the issue? The above code as it is not working as I found a typo and underfined variables. Thanks!', 'Hello Randryn0 \r\n\r\nEven am facing the same issue now while doing sentimental analysis on movie review data.\r\nis the above issue is resolved for you?\r\nif soo please let us know the solution.\r\n\r\n', 'Closing this issue. Please open a new issue with a standalone code to reproduce the issue. Resolution will be faster with a standalone code. Another comment is, it looks more like a support issue. If you strongly thinks this is a bug, then post here. Otherwise, post it in stackoverflow. thanks!', 'I also have this problem.', 'I have the same problem!', ""One has to change the last layer \r\n```\r\nmodel.add(Dense(1))\r\nmodel.add(Activation('sigmoid'))\r\n```\r\nto \r\n\r\n```\r\nmodel.add(Dense(2))\r\nmodel.add(Activation('softmax'))\r\n```\r\nWhen `tain_datagen.flow_from_directory` generates batches, it uses one-hot encoding, returning a vector of two instead of scalar. \r\n"", 'I also have this problem. Does anyone have a solution yet? I tried different optimizers and learning rates as well but the accuracy always converges to 0.5.', ""I also set the last layer to have two nodes with activation='softmax' but it still doesn't work."", ""When using gen_train.flow_from_directory('../data/train', shuffle=True,  class_mode='binary')\r\nwith  activation='sigmoid'\r\n\r\nbe sure to  add class_mode='binary'  to  gen_train.flow_from_directory"", '@khatbahusain : Thanks for the tip.\r\n\r\nchanging the class_mode to binary helped me. The default seems to be categorical\r\n\r\n\r\n\r\n']","['\r\nEpoch 1/5\r\n20/20 [==============================] - 5s 255ms/step - loss: 8.0572 - acc: 0.4994 - val_loss: 8.0590 - val_acc: 0.5000\r\nEpoch 2/5\r\n20/20 [==============================] - 5s 252ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\r\nEpoch 3/5\r\n20/20 [==============================] - 5s 251ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\r\nEpoch 4/5\r\n20/20 [==============================] - 5s 250ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\r\nEpoch 5/5\r\n20/20 [==============================] - 5s 252ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\r\n', ""\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Conv2D, MaxPooling2D\r\nfrom keras.layers import Activation, Dropout, Flatten, Dense\r\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\r\nimport keras\r\nfrom keras import backend as K\r\n\r\nimport tensorflow as tf\r\n\r\nimport time\r\nimport matplotlib.pyplot as plt\r\nimport sys\r\nimport os\r\nimport numpy\r\nimport random as rn\r\n\r\nos.environ['PYTHONHASHSEED'] = '0'\r\nnumpy.random.seed(11)\r\nrn.seed(11)  \r\ntf.set_random_seed(11)\r\n\r\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads=1, itner_op_parallelism_threads=1)\r\nsess = tf.Session(graph.tf.get_default_graph(), config=session_conf)\r\nK.set_session(sess)\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(32, (3, 3), input_shape=(300, 300, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\nmodel.add(Conv2D(32, (3, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\nmodel.add(Conv2D(64, (3, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\n\r\nmodel.add(Flatten())\r\nmodel.add(Dense(64))\r\nmodel.add(Activation('relu'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(1))\r\nmodel.add(Activation('sigmoid'))\r\n\r\n\r\nkeras.optimizers.Adam(lr=0.001)\r\nmodel.trainable\r\nmodel.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\r\n\r\ntrain_datagen = ImageDataGenerator()\r\ntest_datagen = ImageDataGenerator()\r\n\r\ntrain_generator = tain_datagen.flow_from_directory('train', target_size=(100, 100), batch_size=16, class_mode='binary', shuffle=False)\r\nvalidation_generator = test_datagen.flow_from_directory('validation', target_size=(100, 100), batch_size=16, class_mode='binary', shuffle=False)\r\n\r\nhistory = model.fit_generator(train_generator, steps_per_epoch=320//16, epochs=5, validation_data=validation_generator, validation_steps = 80/16)\r\n\r\nK.clear_session()\r\n"", '\r\n00/100 [==============================] - 11s 115ms/step - loss: 7.9406 - acc: 0.5006 - val_loss: 7.9712 - val_acc: 0.5000\r\nEpoch 2/10\r\n100/100 [==============================] - 11s 111ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\r\nEpoch 3/10\r\n100/100 [==============================] - 11s 110ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\r\nEpoch 4/10\r\n100/100 [==============================] - 11s 111ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\r\nEpoch 5/10\r\n100/100 [==============================] - 11s 110ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\r\nEpoch 6/10\r\n100/100 [==============================] - 11s 109ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\r\nEpoch 7/10\r\n100/100 [==============================] - 11s 110ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\r\nEpoch 8/10\r\n100/100 [==============================] - 11s 109ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\r\nEpoch 9/10\r\n100/100 [==============================] - 11s 106ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\r\nEpoch 10/10\r\n100/100 [==============================] - 11s 107ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\r\n', '\r\n00/100 [==============================] - 11s 114ms/step - loss: 6.7217 - acc: 0.5731 - val_loss: 4.1994 - val_acc: 0.7362\r\nEpoch 2/10\r\n100/100 [==============================] - 11s 110ms/step - loss: 3.4067 - acc: 0.6759 - val_loss: 0.3655 - val_acc: 0.8662\r\nEpoch 3/10\r\n100/100 [==============================] - 11s 109ms/step - loss: 0.4138 - acc: 0.8434 - val_loss: 0.3426 - val_acc: 0.8612\r\nEpoch 4/10\r\n100/100 [==============================] - 11s 110ms/step - loss: 0.3220 - acc: 0.8831 - val_loss: 0.3140 - val_acc: 0.8788\r\nEpoch 5/10\r\n100/100 [==============================] - 11s 110ms/step - loss: 0.2494 - acc: 0.9106 - val_loss: 0.3999 - val_acc: 0.8575\r\nEpoch 6/10\r\n100/100 [==============================] - 11s 110ms/step - loss: 0.2244 - acc: 0.9250 - val_loss: 0.3218 - val_acc: 0.8900\r\nEpoch 7/10                                                                                                                                                                              \r\n100/100 [==============================] - 11s 108ms/step - loss: 0.1907 - acc: 0.9344 - val_loss: 0.6445 - val_acc: 0.8375                                                             \r\nEpoch 8/10                                                                                                                                                                              \r\n100/100 [==============================] - 11s 108ms/step - loss: 0.1743 - acc: 0.9409 - val_loss: 0.4450 - val_acc: 0.8738                                                             \r\nEpoch 9/10                                                                                                                                                                              \r\n100/100 [==============================] - 11s 110ms/step - loss: 0.1382 - acc: 0.9503 - val_loss: 0.4937 - val_acc: 0.8738                                                             \r\nEpoch 10/10                                                                                                                                                                             \r\n100/100 [==============================] - 11s 110ms/step - loss: 0.1301 - acc: 0.9550 - val_loss: 0.4955 - val_acc: 0.8950    \r\n']",[],0,0
644,keras,4696,closed,"AssertionError: Could not compute output Elemwise{add,no_inplace}.0 ","Hi, I am new to keras and trying to implements a model a part of that includes encoding sentences using convolution network.

    # import statements
    from keras.layers.embeddings import Embedding
    from keras.layers.wrappers import TimeDistributed
    from keras.layers.convolutional import Convolution1D
    from keras.layers.pooling import MaxPooling1D
    from keras.engine.topology import Merge
    from keras.layers.core import Dropout, RepeatVector
    from keras.layers import LSTM, Input, Dense
    from keras.models import Model, Sequential
    from IPython.display import SVG


    def conv_encoder(nb_filters, filter_len, in_shape):
        """"""model to encode a sentence with a particular filter size
            nb_filters: number of filters, filter_len: size of filter,
            in_shape: input shape (_, SENT_LEN, WORD_LEN)
        """"""
        model = Sequential()
        model.add(Convolution1D(nb_filters, filter_len, border_mode='same', input_shape=in_shape))
        model.add(MaxPooling1D(pool_length=2, stride=None, border_mode='same'))
        return model

     def make_sentence_encoder(nb_filters, filter_lens, in_shape):
         """"""model to encode a sentence with different filter sizes
             basically, learns different types of representations based
             on filter sizes like unigram, bigram, trigram etc.
         """"""
         models = []
         for flt in filter_lens:
             models.append(conv_encoder(nb_filters, flt, in_shape))
         merged_model = Sequential()
         merged_model.add(Merge(models, mode='sum', concat_axis=1))
         return merged_model

    def encode_all_sentences(vocab_size, dense_emb_dim, doc_maxlen, sent_maxlen, nb_filters,   filter_lens, in_shape):
         """"""A model that encodes each sentences using conv nets.
            Returns encoded set of sentences.
            Input: Document,  SHAPE: (_, DOC_LEN, SENT_LEN)
            Output: SHAPE: (_, DOC_LEN, NEW_SENT_LEN)
         """"""
         # setup a sentence encoder
         sent_encoder = make_sentence_encoder(nb_filters, filter_lens, in_shape)

        # embed the input sequence into a sequence of vectors                     
        sentences_encoder = Sequential()
        # initialize embedding layer with wordvec
        sentences_encoder.add(TimeDistributed(Embedding(input_dim=vocab_size,    output_dim=dense_emb_dim, mask_zero=True),
                    input_shape=(doc_maxlen, sent_maxlen), input_dtype='int32'))
        sentences_encoder.add(TimeDistributed(Dropout(0.3)))
   
        sentences_encoder.add(TimeDistributed(sent_encoder))
        return sentences_encoder



Test code:

    inputs = Input(shape=(50, 170))
    sents_enc = encode_all_sentences(1000, 60, 50, 170, 32, [1, 2, 3], (170, 60))
    print sents_enc.output_shape
    out = sents_enc(inputs)
    model = Model(input=inputs, output=out)


The line  in  method is source of problem. I guess it's because  takes input as list of size equal to len(filter_lens) i.e total number of different filters used. I don't know/am confused how to pass it here.

Help will be highly appreciated. Thanks.

Error thrown : 


",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs, but feel free to re-open it if needed.\n']",[],"['sentences_encoder.add(TimeDistributed(sent_encoder))', 'encode_all_sentences', 'sent_encoder', ""---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-74-e01817c5cc55> in <module>()\r\n      1 inputs = Input(shape=(50, 170))\r\n      2 \r\n----> 3 sents_enc = encode_all_sentences(1000, 60, 50, 170, 32, [1, 2, 3], (170, 60))\r\n      4 print sents_enc.output_shape\r\n      5 \r\n\r\n<ipython-input-73-5e7194fd1614> in encode_all_sentences(vocab_size, dense_emb_dim, doc_maxlen, sent_maxlen, nb_filters, filter_lens, in_shape)\r\n     40     sentences_encoder.add(TimeDistributed(Dropout(0.3)))\r\n     41 \r\n---> 42     sentences_encoder.add(TimeDistributed(sent_encoder))\r\n     43     return sentences_encoder\r\n\r\n/usr/local/lib/python2.7/dist-packages/keras/models.pyc in add(self, layer)\r\n    310                  output_shapes=[self.outputs[0]._keras_shape])\r\n    311         else:\r\n--> 312             output_tensor = layer(self.outputs[0])\r\n    313             if type(output_tensor) is list:\r\n    314                 raise Exception('All layers in a Sequential model '\r\n\r\n/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc in __call__(self, x, mask)\r\n    512         if inbound_layers:\r\n    513             # this will call layer.build() if necessary\r\n--> 514             self.add_inbound_node(inbound_layers, node_indices, tensor_indices)\r\n    515             input_added = True\r\n    516 \r\n\r\n/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc in add_inbound_node(self, inbound_layers, node_indices, tensor_indices)\r\n    570         # creating the node automatically updates self.inbound_nodes\r\n    571         # as well as outbound_nodes on inbound layers.\r\n--> 572         Node.create_node(self, inbound_layers, node_indices, tensor_indices)\r\n    573 \r\n    574     def get_output_shape_for(self, input_shape):\r\n\r\n/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc in create_node(cls, outbound_layer, inbound_layers, node_indices, tensor_indices)\r\n    147 \r\n    148         if len(input_tensors) == 1:\r\n--> 149             output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))\r\n    150             output_masks = to_list(outbound_layer.compute_mask(input_tensors[0], input_masks[0]))\r\n    151             # TODO: try to auto-infer shape if exception is raised by get_output_shape_for\r\n\r\n/usr/local/lib/python2.7/dist-packages/keras/layers/wrappers.pyc in call(self, X, mask)\r\n    139                 input_length = K.shape(X)[1]\r\n    140             X = K.reshape(X, (-1, ) + input_shape[2:])  # (nb_samples * timesteps, ...)\r\n--> 141             y = self.layer.call(X)  # (nb_samples * timesteps, ...)\r\n    142             # (nb_samples, timesteps, ...)\r\n    143             output_shape = self.get_output_shape_for(input_shape)\r\n\r\n/usr/local/lib/python2.7/dist-packages/keras/models.pyc in call(self, x, mask)\r\n    364         if not self.built:\r\n    365             self.build()\r\n--> 366         return self.model.call(x, mask)\r\n    367 \r\n    368     def build(self, input_shape=None):\r\n\r\n/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc in call(self, input, mask)\r\n   2054             return self._output_tensor_cache[cache_key]\r\n   2055         else:\r\n-> 2056             output_tensors, output_masks, output_shapes = self.run_internal_graph(inputs, masks)\r\n   2057             return output_tensors\r\n   2058 \r\n\r\n/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc in run_internal_graph(self, inputs, masks)\r\n   2227         for x in self.outputs:\r\n   2228             # todo: better error msg\r\n-> 2229             assert str(id(x)) in tensor_map, 'Could not compute output ' + str(x)\r\n   2230             tensor, mask = tensor_map[str(id(x))]\r\n   2231             if hasattr(tensor, '_keras_shape') and output_shapes is not None:\r\n\r\nAssertionError: Could not compute output Elemwise{add,no_inplace}.0\r\n\r\n""]",0,0
645,keras,4395,closed,[Question] Graph Machines in keras,"Hello,
I'm not sure if this is the proper place to ask a question about keras. I apologize in advance if it is not.

 I'm currently trying to reproduce this publication which is using recurrent neural nets to predict chemical activities.

> Goulon, A., T. Picot, A. Duprat, and G. Dreyfus. “Predicting Activities without Computing Descriptors: Graph Machines for QSAR.” SAR and QSAR in Environmental Research 18, no. 1–2 (January 1, 2007): 141–53. doi:10.1080/10629360601054313.

In two words : the autors represent a molecule as a directed acyclic graph. Each node of this graph being an atom and each edge being a chemical bond. Then a neural network is applied on the ""exit node"" (called root node in the publication). This neural network calls itself on the parents of the root node and so on to provide a prediction on the whole molecule.

I think I understand how the training works for this graph machine: for each molecule, a gradient of the cost function is calculated by backpropagation in the molecule. The gradient over a batch of molecules is a weighted average of the gradient calculated for each molecule composing the batch. 
A gradient descent algorithm is then used to minimize the cost-function.

My main trouble is : how can I implement this training procedure in Keras ? Usually, you just provide a list of X and a list of corresponding Y and just use model.compile() and then model.fit(). From what I understand, it is not possible here because each molecule should have its own keras.model

Is keras even the best solution here ?

Best regards,
",,"['After some research. The standard name for this architecture is ""Recursive neural network"" (not recurrent). This architecture is described in the chapter 10 at page 400 of: \n\n>  Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. Chapter 10 [http://www.deeplearningbook.org/contents/rnn.html](http://www.deeplearningbook.org/contents/rnn.html).\n\n![Architecture of the recursive Neural Network](http://i.imgur.com/Umz2i0p.png)\n\nConsequentyl, this question is related to #352\nIt should be noted that in my case, the U and W shown in the previous figure share the same weight and that each node is not forced to be bipartite\n', ""Hello @jbDelafosse i kwon it's late, but i have just implemented the same structure shown in PyTorch, if you want to have a look just see my repo."", 'And my response is a bit late too but a very good implementation of this can be found in https://www.dgl.ai/\r\n']",[],[],0,0
646,keras,10179,closed,"feed the lstm weight get by tensorflow to lstm of keras, but I failed to repeat the result! ","    import numpy as np
    import tensorflow as tf
    tf.set_random_seed(49999)

    maxlen = 2
    word_dim = 2
    n_rnn = 3

    x = np.array(range(4), dtype=np.float32).reshape([1, 2, 2])
    x_r = np.flip(x, axis=1)

    inputs_r = tf.constant(x_r, dtype=tf.float32)
    seq_len = tf.constant([2], dtype=tf.int32)

    cell_fw = tf.nn.rnn_cell.LSTMCell(n_rnn, name=""fw"")
    cell_bw = tf.nn.rnn_cell.LSTMCell(n_rnn, name=""bw"")

    # tf forward
    tf_hs_fw, (tf_c_fw, tf_h_fw) = tf.nn.dynamic_rnn(
        cell_fw, inputs_r, seq_len, dtype=tf.float32)

    # tf backward
    tf_hs_bw, (tf_c_bw, tf_h_bw) = tf.nn.dynamic_rnn(
        cell_bw, inputs_r, seq_len, dtype=tf.float32)

    sess = tf.InteractiveSession()
    init = tf.global_variables_initializer()
    sess.run(init)

    print(tf.trainable_variables())

    tf_w = [v.eval() for v in tf.trainable_variables()]

    v_tf_h_fw = tf_h_fw.eval()[0]
    v_tf_h_bw = tf_h_bw.eval()[0]


    def tf2keras(params, reshape_dim=(0, 2, 1, 3), in_dim=2, rnn_dim=3):
        # params: [(5, 12), (12,), (5, 12), (12,)]
        assert in_dim+rnn_dim == params[0].shape[0]
        if len(params) == 2:
            tf_lstm = [params[0][:in_dim, :], params[0][in_dim:, :], params[1]]
        else:  # == 4
            tf_lstm = [params[0][:in_dim, :], params[0][in_dim:, :], params[1],
                       params[2][:in_dim, :], params[2][in_dim:, :], params[3]]

        # tf_lstm: [(2, 12), (3, 12), (12,),
        #          (2, 12), (3, 12), (12,)]

        def dim_recombination(x, reshape_dim):
            # [i, j, f, o]  => [i, f, c, o]  where j is c
            rst = None
            if len(x.shape) == 1:
                # [i, j, f, o]
                tmp = [x[0:rnn_dim * 1],
                       x[rnn_dim * 1:rnn_dim * 2],
                       x[rnn_dim * 2:rnn_dim * 3],
                       x[rnn_dim * 3: rnn_dim * 4]]
                rst = np.hstack([tmp[i] for i in reshape_dim])
            elif len(x.shape) == 2:
                tmp = [x[:, 0:rnn_dim*1],
                       x[:, rnn_dim*1:rnn_dim*2],
                       x[:, rnn_dim*2:rnn_dim*3],
                       x[:, rnn_dim*3:rnn_dim*4]]
                rst = np.hstack([tmp[i] for i in reshape_dim])
            else:
                print(""xxxxx"")

            return rst

        keras_w = [dim_recombination(v, reshape_dim) for v in tf_lstm]
        return keras_w


    k_params = tf2keras(tf_w, (0, 2, 1, 3))


    #  keras
    from keras.layers import Input
    from keras.layers import LSTM
    from keras.models import Model

    keras_input = Input(shape=[maxlen, word_dim], dtype='float32', name='input_layer')


    k_lstm1 = LSTM(n_rnn, recurrent_activation='sigmoid', return_sequences=True, return_state=True,
                   name=""lstm1"")(keras_input, training=False)
    k_lstm2 = LSTM(n_rnn, recurrent_activation='sigmoid', return_sequences=True, return_state=True,
                   name=""lstm2"")(keras_input, training=False)

    m1 = Model(inputs=keras_input, outputs=k_lstm1)
    m1.set_weights(k_params[:3])
    k_hs_fw, k_h_fw, k_c_fw = m1.predict(x_r)
    v_k_h_fw = k_h_fw[0]

    m2 = Model(inputs=keras_input, outputs=k_lstm2)
    m2.set_weights(k_params[3:])

    k_hs_bw, k_h_bw, k_c_bw = m2.predict(x_r)
    v_k_h_bw = k_h_bw[0]

    print(v_k_h_fw - v_tf_h_fw)
    # expect: [0, 0, 0]
    # but get [-0.0370398  -0.02871804  0.00494046]

    print(v_k_h_bw - v_tf_h_bw)
    # expect: [0, 0, 0]
    # but get [-0.03148754 -0.0340828   0.05445436]",,"['fixed it by :\r\n      cell_fw = tf.nn.rnn_cell.LSTMCell(n_rnn, name=""fw"", forget_bias=0.)\r\n      cell_bw = tf.nn.rnn_cell.LSTMCell(n_rnn, name=""bw"", forget_bias=0.)\r\n\r\n\r\nthere are some differents between keras\'s LSTM and Tensorflow\'s LSTM ']",[],[],0,0
647,keras,8139,closed,[Feature Request] Make ImageDataGenerator suitable for test,"Two changes are proposed in this issue to make  more usable for the final testing phase:

### Change 1: ImageDataGenerator can be exhaused

The [documentation of ImageDataGenerator](https://keras.io/preprocessing/image/) says:

> Generate batches of tensor image data with real-time data augmentation. The data will be looped over (in batches) indefinitely.

It seems to me that, this (loops indefinitely) implies that, the image data generator is mainly used for the training process, and not for the final test (because we only need one epoch when testing).

Sure you can achieve the one epoch purpose by using , but this relies on the implementation of  and is not a documented behaviour.

I propose:

1. Add an  keyword argument that controls  whether the iterator yields indefinitely or not.

2. Alternative to 1, we can also make the  mentioned above a documented behaviour, and also document that after the final step, each image is visited exactly once.

### Change 2: Add a new  to yields the filenames of the batch

The current implementation of  allows you to recover the current batch's filenames by using its  and  property, but this relies on the implementation and is not documented.

I propose we add a new  to yields the filenames of the current batch, just like we don't let user slice its  property to get the current batch's class. (The filename is useful at least for making some Kaggle submissions).

Both these changes are backward compatible.",,[],[],"['ImageDataGenerator', 'steps = ceil(total_examples / batch_size)', 'preprocessing.image.Iterator', 'exhaustive = True|False', 'steps = ceil(...)', 'class_mode = ""filename""', 'ImageDataGenerator.flow_from_directory()', 'batch_index', 'filenames', 'class_mode = ""filename""', 'classes']",0,0
648,keras,5267,closed,Modifying categorical accuracy,"I'd like to modify categorical accuracy to take into account non-padded values, the code I've come up with is:









But I keep getting the error message from below.  I've tested the function with some made up examples and it seems to work just fine, but when I pass categorical_accuracy as a metric to my model, it doesn't. Any help? 

Thanks so much in advance!











    metric_result = metric_fn(y_true, y_pred)  File ""../codeswitch_pos_tagger/trainer.py"", line 27, in categorical_accuracy
 

",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs, but feel free to re-open it if needed.\n']",[],"['def categorical_accuracy(y_true, y_pred):', '    y_true_label = K.argmax(y_true, axis=-1)', '    y_pred_label = K.argmax(y_pred, axis=-1)', '   n_e = K.not_equal(y_true_label, 0)', '    indices = tf.where(n_e)', '   return K.mean(K.equal(tf.gather_nd(y_true_label, indices), tf.gather_nd(y_pred_label, indices)))', 'Traceback (most recent call last):', '  File ""../codeswitch_pos_tagger/trainer.py"", line 122, in <module>', '    run_cv_training(configs)', '  File ""../codeswitch_pos_tagger/trainer.py"", line 99, in run_cv_training', '    run_training(cfgs[i])', '  File ""../codeswitch_pos_tagger/trainer.py"", line 70, in run_training', '    metrics=[categorical_accuracy])', '  File ""/rigel/home/vs2411/.local/lib/python2.7/site-packages/keras/models.py"", line 594, in compile\r\n    **kwargs)', '  File ""/rigel/home/vs2411/.local/lib/python2.7/site-packages/keras/engine/training.py"", line 716, in compile', '\r\n', '\r\n    indices = tf.where(n_e)', 'TypeError: select() takes at least 3 arguments (1 given)']",0,0
649,keras,10439,closed,Accidental Submission ,Apologies ,,[],[],[],0,0
650,keras,5760,closed,deep-learning-models :  ImportError: cannot import name '_obtain_input_shape',"Keras 1.2.2

  File ""resnet50.py"", line 39, in <module>
    from keras.applications.imagenet_utils import _obtain_input_shape
ImportError: cannot import name '_obtain_input_shape'

",,"[""I'm having same issue, I think thats because I'm running code implemented over an old keras version. How can I fix it? where do I found `_obtain_input_shape`?\r\n\r\nThanks""]",[],[],0,0
651,keras,1679,closed,Convolutional Autoencoder for temporal sequences,"I am trying to create convolutional autoencoder for temporal sequences using Keras.
Here's the code



I am getting the following error at the 3rd line where decoder is being initialized:



Please help me resolve the issue
",stale,"['Perhaps you need to do the opposite operations in the decoder. Pooling -> upsampling, convolution -> deconvolution.\n', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']","["" python\nae = Sequential()\nencoder = containers.Sequential([Convolution1D(5, 5, border_mode='valid', input_dim=39, activation='tanh', input_length=39), Flatten(), Dense(5)])\ndecoder = containers.Sequential([Convolution1D(5, 5, border_mode='valid', input_dim=5, activation='tanh', input_length=5), Flatten(), Dense(39)])\nae.add(AutoEncoder(encoder=encoder, decoder=decoder,\n                   output_reconstruction=False))\n\nae.compile(loss='mean_squared_error', optimizer=RMSprop())\nae.fit(X_train, X_train, batch_size=32, verbose=1)\n"", '\nValueError: negative dimensions are not allowed\n']",[],0,0
652,keras,9245,closed,Add all the ResNet variants,"@fchollet, I recently wrote an [factorized version](https://github.com/taehoonlee/tensornets/blob/master/tensornets/resnets.py) for all the ResNet variants with pre-trained weights including ResNet, ResNetv2, ResNeXt, and WideResNet (total 11 models). If they are added here Keras, will it be hard to manage and cause more problems? or will it be useful? What do you think about adding the 11 ResNet models?",,"['@fchollet, I would appreciate it if you could give me any comments on this.', '@taehoonlee: this is tensorflow based, not keras based. Am I right? I am finding the resnext implementation in keras but I think it does not support yet', '@John1231983, I am intending to write down the models in Keras, not TensorFlow.']",[],[],0,0
653,keras,4142,closed,using fit_generator with nb_worker > 1 and pickle_safe=True,"Hello All,

I am using fit_generator to train my model, I am trying the batch implementation similar to [test_multiprocessing.py](https://github.com/fchollet/keras/blob/master/tests/keras/test_multiprocessing.py)



And the output is 



It is clear from the above output that multiple process are trying to access the same samples (we can see intersection of (start,end) from different process batches). 

I have also tried implementation suggested in #1638 but no luck, it is also working similarly. 

What should be the batch generator implementation to work correctly(no intersection between the batches as long as possible. In this case always, as samples > samples_per_epoch) with nb_worker > 1 and pickle_safe=True. ?
TIA.
",stale,"['any update on this..?\r\ndid you manage to get it working..?', 'Could it be because the multiprocess spawns multiple copies of your generator and they are all running simultaneously.  You need to have a locked variable to so that each thread keeps track of what its supposed to work on.\r\n\r\nIn your example, when the generator beings, lock a variable called start, save off your range (start:start+length), increment start (start+= length) and then unlock start.', 'Rather than worrying about locks, just make a Sequence object for your dataset and pass that into model.fit_generator instead of a normal generator object ']","[' python\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense\nfrom keras.utils.test_utils import keras_test\nimport multiprocessing\n\narr_data = np.random.randint(0, 256, (500, 2))\narr_labels = np.random.randint(0, 2, 500)\n\ndef myGenerator():\n\n    batch_size = 20\n    n_samples = 500\n    start = 0;\n    while True:\n        end = min(start + batch_size,n_samples)\n        X = arr_data[start: end]\n        y = arr_labels[start: end]\n        print ""\\nProcess: %s, Batch indices (start,end):(%f,%f)""%(multiprocessing.current_process().name,start,end)\n        yield X, y\n        start += batch_size    \n\n# Build a NN\nmodel = Sequential()\nmodel.add(Dense(1, input_shape=(2, )))\nmodel.compile(loss=\'mse\', optimizer=\'adadelta\')\n\nmodel.fit_generator(myGenerator(),\n                    samples_per_epoch=100,\n                    nb_epoch=1,\n                    verbose=2,\n                    max_q_size=10,\n                    nb_worker=4,\n                    pickle_safe=True)\n\n', '\nEpoch 1/1\n\nProcess: Process-1, Batch indices (start,end):(0.000000,20.000000)\n\nProcess: Process-2, Batch indices (start,end):(0.000000,20.000000)\n\nProcess: Process-3, Batch indices (start,end):(0.000000,20.000000)\n\nProcess: Process-4, Batch indices (start,end):(0.000000,20.000000)\n\nProcess: Process-1, Batch indices (start,end):(20.000000,40.000000)\n\nProcess: Process-1, Batch indices (start,end):(40.000000,60.000000)\n\nProcess: Process-1, Batch indices (start,end):(60.000000,80.000000)\n\nProcess: Process-1, Batch indices (start,end):(80.000000,100.000000)\n\nProcess: Process-1, Batch indices (start,end):(100.000000,120.000000)\n\nProcess: Process-1, Batch indices (start,end):(120.000000,140.000000)\n\nProcess: Process-1, Batch indices (start,end):(140.000000,160.000000)\n\nProcess: Process-2, Batch indices (start,end):(20.000000,40.000000)\n\nProcess: Process-1, Batch indices (start,end):(160.000000,180.000000)\n\nProcess: Process-3, Batch indices (start,end):(20.000000,40.000000)\n\nProcess: Process-4, Batch indices (start,end):(20.000000,40.000000)\n\nProcess: Process-2, Batch indices (start,end):(40.000000,60.000000)\n\nProcess: Process-1, Batch indices (start,end):(180.000000,200.000000)\n\nProcess: Process-3, Batch indices (start,end):(40.000000,60.000000)\n\nProcess: Process-4, Batch indices (start,end):(40.000000,60.000000)\n']",[],0,0
654,keras,5700,closed,regarding putting multiple patches for a single image into a single mini-batch,"Regarding patch-wise training for image classification or segmentation, I need to put multiple patches corresponding to a single image into a single mini-batch during training process. How to do that in Keras? Or how can I ensure multiple training patches in a single mini-batch belong to the same training image?",stale,"['This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity  occurs, but feel free to re-open a closed issue if needed.\n']",[],[],0,0
655,keras,5418,closed,Batchwise outer product?,"I see that the backend has an implementation for batchwise dot product.
Can there be a function to do batchwise outer product?

I tried implementing an outer product Layer of my own, but I am getting stuck (dealing with batches) Would appreciate some pointers or help. 

Thanks in advance.",,"[""Basically, all I want to do is take two tensors (coming from different layers) and compute the outer product (which will be passed to further layers).\r\ndim(?, d, 1) x dim(?, 1, d) = dim(?, d, d).\r\nwhere, '?' represents batch size.\r\n\r\nI'm sure this is trivial, but I'm unable to wrap my head around it while implementing the batch version.\r\n\r\n"", 'Hello,\r\n\r\nHave you tried looking at https://keras.io/backend/#backend-functions : batch_dot \r\nYou probably need a few reshapes before but it should do it. (and swap left and right i.e: dim(?, d, 1) x dim(?, 1, d) )', 'Yep, I was implementing something along those lines. Thank you for your quick response. @unrealwill :)\r\nManaged to implement using a Lambda layer.\r\n\r\nYou can find the code here:\r\nhttps://gist.github.com/maximus009/6d8a2e4d6d56d02e5581eb11ab1b2ea2', 'Hi @ maximus009  could you add an example how to use your code']",[],[],0,0
656,keras,1821,closed,How to interprect `.predict_proba()` for multi-label Classification problem?,"Not really an <em>issue</em>, more of a question to ensure for myself (and others) that the output from the  function for a multi-label classification problem is being interpreted correctly.

So here's a toy problem:



, 
                
             
            
             


    
     
    
    d


                  






Using a basic Keras  model:












Which outputs:





So how do I interpret this output? I'm used to using  Classifiers, which return probability output for multi-label problems in the shape of  where  are the number of test instances and  are 2 (0, 1). But I think that the model above is outputting in the shape of  where samples are test instances and classes are the unique number of classes in . 

Could someone clarify this output for me?
",,"['You have an output of 4 binary variables, one for each color. If you take a look at your preprocessed `y` variable, you will have the same shape `(n_samples, n_classes)`. Your output could be seen as `p(color_k=1|x_i)` where `i` is in `{1,2}` (your samples), for all `k` where `k` is in `{1,2,3,4}` (your colors) giving you an output of shape (2, 4). For a binary logistic regression (sigmoid activation), having 2 outputs is redundant since one is the complement of the other. Keras only outputs you `p(color_k=1|x_i)` and not `p(color_k=0|x_i)` like in scikit learn.\n', 'got it thanks!\n', ""@tboquet I don't understand that \n\n> Keras only outputs you p(color_k=1|x_i) and not p(color_k=0|x_i) like in scikit learn.\n\nFor single label, the index of y  is from 0.  Does it different from multi-label because  the index of y is from 1.\nBut when i set the y_label, i also set from 0.\n\n> x is [[1,2,3],[4,5,6],[7,8,9]\n>    y is [[1,2],[1,3],[3]]\n\n Then y need to switch\n\n>  y is [[1,1,0],\n>             [1,0,1],\n>              [0,0,1]]\n"", '@ralston3 Thanks for your share. Do you also use the `predict_proba`.\nhow about the `predict` Thanks.\n']",[],"['.predict_proba()', '# generate some sample data', 'X = np.array([[4, 5, 6, 7, 8]', '[0, 5, 6, 2, 3],', '[1, 2, 6, 5, 8],', '[6, 1, 1, 1, 3],', '[2, 5, 3, 2, 0]])', ""y = [['blue', 'red'],"", ""['red'],"", ""['red', 'green'],"", ""['blue', 'green'],"", ""['orange']]"", 'X_test = np.array([[4, 6, 1, 2, 8],', '[0, 0, 1, 5, 1]])', '# binarize text labels', 'mlb = preprocessing.MultiLabelBinarizer()', 'y = mlb.fit_transform(y)', 'Sequential()', 'model = Sequential()', ""model.add(Dense(output_dim=10, input_dim=5, init='uniform', activation='tanh'))"", ""model.add(Dense(output_dim=10, input_dim=10, init='uniform', activation='tanh'))"", ""model.add(Dense(output_dim=10, input_dim=10, init='uniform', activation='tanh'))"", ""model.add(Dense(output_dim=4, init='uniform', activation='sigmoid'))"", ""model.compile(optimizer='adadelta', loss='binary_crossentropy')"", 'model.fit(X, y)', 'proba = model.predict_proba(X_test)', '»» proba', '[[ 0.39656317  0.41439512  0.03391508  0.90610588]', '[ 0.40581116  0.41944474  0.05669538  0.86803496]]', 'scikit', '[n_samples, n_classes]', 'n_samples', 'n_classes', '[n_samples, n_classes]', 'y']",0,0
657,keras,4271,closed,Semantic Meaning of a sentence,"Not really an issue, but I haven't found any documentation on how to access the hidden state in LSTM RNN.

**Context:** I want to capture the semantic meaning of a sentence, by feeding in the Glove word embedding vectors ( [https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py](url) ) into the RNN and when the sentence ends, the final hidden state captures the so-called sentence embedding. I want to access this vector obtained from the final hidden state.",stale,"['You could try to train your model (like a _sentence2vec_) and then get the intermediate result to see the internal embedding. This is described in the [Keras FAQ](https://keras.io/getting-started/faq/#how-can-i-visualize-the-output-of-an-intermediate-layer)\n', 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs, but feel free to re-open it if needed.\n']",[],[],0,0
658,keras,987,closed,CNMeM is disabled,"I have already installed Keras 2.0 successfully. But, it print ""CNMeM is disabled"", who can tell me what's meaning of this note.
",,"[""See here: http://deeplearning.net/software/theano/library/config.html\n\nYou can change this in the environment variables. I added the following to my .theanorc file and saw significant speed increases during training of a cnn:\n\n[lib]\ncnmem=.75 \n\nBeyond .75 I got out of memory errors, but I'd imagine this varies based on your setup.\n"", 'Thanks very much, jacobzweig.\n', '@jacobzweig, @danieljf24 so how much faster is the model training with cnmem? Is it worth using?\n', 'It is worth using. The speed up depend of many factor, like the shapes and\nthe model itself. The speed up go from 0 to 2x faster.\n\nOn Fri, Jul 1, 2016 at 8:13 AM, Carl Thomé notifications@github.com wrote:\n\n> @jacobzweig https://github.com/jacobzweig, @danieljf24\n> https://github.com/danieljf24 so how much faster is the model training\n> with cnmem? Is it worth using?\n> \n> —\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/fchollet/keras/issues/987#issuecomment-229931441, or mute\n> the thread\n> https://github.com/notifications/unsubscribe/AALC-xuN5Q0fN0G11-NqimL1W878JgYRks5qRQR_gaJpZM4GfN8P\n> .\n']",[],[],0,0
659,keras,4733,closed,ImageDataGenerator does not save label as an image,"Hi guys. I am trying to augment my image data. However, my labels are also images. So i need to do the same augmentation operations on both image and label. I would like to save both as image but somehow, when I use datagen.flow, it does not save the label, only the image. I also insert the code I am  using:
https://github.com/adkoadko/ostruvky/blob/master/Keras_augmentation

Is there a way to save also the label? Or have I missed something?

Thank you",,[],[],[],0,0
660,keras,4220,closed,How to access variables of layers?," If I were to follow the guide and integrate with my TensorFlow workflow (https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html) as with others, you cannot access the weight variable because we **won't be building the model** as shown in the guide. We're merely using the layers. There is no need to compile when we use it as a simplified interface to TensorFlow. **How then do we access the weights (variables)?**

Because if we use with TensorFlow like the guide, we **do not call**  or  but merely use the layers to build. 
",stale,"['If you just want to get the weights, just use model.get_weights(). Of cousre, each keras layer also can use this function, model.layers[0].get_weights().\n', ""@ritchieng did you find an answer to this question?\r\nI observe that: `[v.name for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)]` includes e.g. `dense_1_W:0` but that the following fails:\r\n```\r\nwith tf.variable_scope('', reuse=True):\r\n    W1 = tf.get_variable('dense_1_W')\r\n```\r\nUgly workaround with inverse lookup works\r\n```\r\nvardic = {v.name: v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)}\r\nW1 = vardic['dense_1_W:0']\r\n```\r\nBut what would be the proper way?"", 'I did not actually. I ended up using another framework that gave me more flexibility: https://github.com/zsdonghao/tensorlayer']",[],"['Model', 'Compile']",0,0
661,keras,4727,closed,Weigh multiple losses,"I've built a model with multiple outputs and need to balance the losses against each other.
The theano backend can do this via the parameter in .
Unfortunately I'm using tensorflow.

How does Keras deal with multiple losses:
Do you add them and then take the derivative of the sum ?
Or do you optimize them individually ?

My idea is to just implement the losses I need myself and multiply the weighting factor into the loss manually. But that would only work if you're optimizing the sum of losses.",,"[""> The theano backend can do this via the loss_weights parameter in model.compile().\r\n> Unfortunately I'm using tensorflow.\r\n\r\nThe API is the exact same whether you are using TF or Theano."", 'Well then the docs are wrong\r\n\r\n> kwargs: when using the Theano backend, these arguments are passed into K.function. Ignored for Tensorflow backend.', 'Even comments in the actual source code say that tensorflow ignores this.', 'ping @fchollet . Is he correct? \r\n\r\n@lhk what exactly does this loss_weights parameter actually do?']",[],"['loss_weights ', 'model.compile()']",0,0
662,keras,1926,closed,Learning does not start,"I wanted to reproduce the issues that I cited in [this post](https://github.com/fchollet/keras/issues/1917).
So I wanted to train a neural network using keras, but the training does not start, and it happens in really weird situations.

Here is a portion of my code:



If I run this code in the terminal using , everything goes fine and the training starts.

But if I run this code in the terminal using , , or if I redirect the outputs in my code , then the program stall for the  method, so the learning does not start. The is displayed, but nothing more, no training.

I took a look and it seems that the program keeps calling this function 

Any ideas why it does not start when I redirect the outputs?
",,"['Ok, I got the answer of that.\nThe training starts, but all the outputs are displayed/written only when the training is over, nothing before.\n', '@FiReTiTi can you close this case then?\n']","['\nmodel = Sequential()\nmodel.add(Convolution2D(8, 7, 7, border_mode=\'valid\', input_shape=(1, 31, 31), activation=\'tanh\'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Convolution2D(16, 5, 5, border_mode=\'valid\', activation=\'tanh\'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Convolution2D(32, 3, 3, border_mode=\'valid\', activation=\'relu\'))\nmodel.add(Flatten())\nmodel.add(Dense(23))\nmodel.add(Activation(\'tanh\'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(11))\nmodel.add(Activation(\'sigmoid\'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1))\nmodel.add(Activation(\'sigmoid\'))\n\noptimizer = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nprint(""optimizer done"")\nmodel.compile(loss=\'binary_crossentropy\', optimizer=optimizer)\nprint(""compile done"")\nsys.stdout.flush()\n\nmodel.fit(dataset, labels, batch_size=batch_size, nb_epoch=nb_epoch, shuffle=True, validation_split=0.1, verbose=2)\n']","['python mynn.py', 'python mynn.py &', 'python mynn.py > results.txt', ""sys.stdout = open('Outputs.txt', 'w')"", 'fit', 'compile done', 'clock_gettime(CLOCK_MONOTONIC_RAW, {,}) = 0']",0,0
663,keras,230,closed,Can i add global pooling layer?,"I have added this layer in my local keras (under convolutional layers):

class GlobalPooling(Layer):
    '''
        Apply a global pooling to an output.
    '''
    def **init**(self, pooling_function = T.mean):
        super(GlobalPooling,self).**init**()
        self.pooling_function = pooling_function
    def get_output(self, train):
        X = self.get_input(train)
        return self.pooling_function(X.flatten(3), axis = 2)



It seems to work fine, can you please add this support?

Thanks!
",,"[""Sure, you could open up a pull request and we'll look at it.\n"", 'Is global pooling added?\n', 'You can use the code i wrote in the first message\n', 'thanks! I am thinking to create a pull request for that; it would make things easier.\n', 'Sure, go for it\n\nOn Tue, Aug 11, 2015 at 6:26 PM, Cheng Guo notifications@github.com wrote:\n\n> thanks! I am thinking to create a pull request for that; it would make\n> things easier.\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/fchollet/keras/issues/230#issuecomment-129929339.\n', 'Created a pull request: https://github.com/fchollet/keras/pull/522\n']","['\ndef get_config(self):\n    return {""name"":self.__class__.__name__,\n           ""pooling_function}"":self.pooling_function.__name__}\n']",[],0,0
664,keras,9140,closed,Notebook dying too often.,"I am trying to understand the code given here:
![conv_lstm](https://github.com/keras-team/keras/blob/master/examples/conv_lstm.py). 

I have implemented the whole code in a jupyter python notebook linked here:
![LSTM](https://github.com/Anirudh257/Audio-files-extraction/blob/master/UNDERSTANDING%20LSTM%20.ipynb)

When I start training the network, my notebook becomes unresponsive, my firefox browser freezes and crashes after sometime. The kernel dies too.

What can be the reason for this?
Are these 2 arrays **noisy_movies** and **shifted_movies** too large for the notebook to process?
Is anybody else experiencing the same issue as well?

I would be glad if someone can help.

",,"['I think this question might have a better home at StackOverflow. I would try running the model with a smaller input size and perhaps reduce the number of layers to see if this is the problem. Thanks.', 'Thanks for replying, @anj-s. I have already tried asking the same question on stackoverflow and tried out all their suggestions. But none of them work for me. I will try to decrease the parameters and run the notebook again.']",[],[],0,0
665,keras,3854,closed,A Convolutional Neural Network Model Could not be loaded,"I have saved the model using _model.save('my_model.h5')_ it's working. However, when I try to load the model in a different project I get this error message: ValueError: Tensor(""cond/pred_id:0"", dtype=bool) must be from the same graph as Tensor(""dropout_1/mul_1:0"", shape=(?, 1, 256), dtype=float32).
Could this be a bug? Any idea?
",,"['hi, I happened the same issue, how did you figure this out?\r\nThanks in advance.', ""Hi , I had experienced the same issue as @aykutcayir34 . My project is a web application which I use flask as framework. Also, model loading request comes from the front end. I recognized it was Flask related issue (I still do not figure out the reason). I solved the problem by loading the model before running the flask application and calling `predict ` function immediately after loading model.\r\nI know this sounds silly but it's the way how I solved the issue. \r\n""]",[],[],0,0
666,keras,2372,closed,problem of save/load model,"HI, 

Thanks for making such a wonderful tool!

I'm using Keras 1.0. I want to save and load the model both the arch and the parameters. So I use the method in FAQ. Here is the code.



When I load model and use model.predict(), there is a error:
AttributeError: 'NoneType' object has no attribute 'predict'

Don't know why. If I don't load the model from file, just train a model and use it, everything seems ok.

I checked the issues, most people just need to load the parameters. Is it possible when I load the architecture, I overwrite the old model and loose the model.predict()?

Thanks again for making Keras!

Ben
",,"['You need to compile a Keras model before using it.\nOn Apr 17, 2016 8:31 AM, ""Wu, Bin(Ben)"" notifications@github.com wrote:\n\nHI,\n\nThanks for making such a wonderful tool!\n\nI\'m using Keras 1.0. I want to save and load the model both the arch and\nthe parameters. So I use the method in FAQ. Here is the code.\n\ndef save_model(self, model, options):\n    json_string = model.to_json()\n    open(options[\'file_arch\'], \'w\').write(json_string)\n    model.save_weights(options[\'file_weight\'])\n\ndef load_model(self, options):\n    self.model = model_from_json(open(options[\'file_arch\']).read())\n    self.model.load_weights(options[\'file_weight\'])\n    return self.model\n\nWhen I load model and use model.predict(), there is a error:\nAttributeError: \'NoneType\' object has no attribute \'predict\'\n\nDon\'t know why. If I don\'t load the model from file, just train a model and\nuse it, everything seems ok.\n\nI checked the issues, most people just need to load the parameters. Is it\npossible when I load the architecture, I overwrite the old model and loose\nthe model.predict()?\n\nThanks again for making Keras!\n\nBen\n\n—\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/fchollet/keras/issues/2372\n', ""Thanks @fchollet !\n\nI added the complie function and it worked. The simplified code is here.\n\n```\ndef save_model(model, options):\n    json_string = model.to_json()\n    open(options['file_arch'], 'w').write(json_string)\n    model.save_weights(options['file_weight'])\n\ndef load_model(options):\n    model = model_from_json(open(options['file_arch']).read())\n    model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n    model.load_weights(options['file_weight'])\n    return model\n```\n"", '@fchollet ,\r\n\r\nPlease, I need your help. I try to validate my test data using the already training result from the model but I got the following error:\r\n\r\n##################\r\nAttributeError Traceback (most recent call last)\r\nin ()\r\n1 # load weights into new model\r\n----> 2 model_info.load_weights(save_best_weights)\r\n3 predictions = model.predict([test_q1, test_q2, test_q1, test_q2,test_q1, test_q2], verbose = True)\r\n\r\nAttributeError: \'History\' object has no attribute \'load_weights\'\r\n\r\nIn [ ]:\r\n#####################\r\n\r\nBelow is the snippets of my code:\r\n\r\n###Fitting model\r\n\r\nsave the best weights for predicting the test question pairs\r\n\r\nsave_best_weights = ""weights-pairs1.h5""\r\n\r\ncheckpoint = ModelCheckpoint(filepath, monitor=\'loss\', verbose=1, save_best_only=True, mode=\'min\')\r\n\r\ncallbacks = [ModelCheckpoint(save_best_weights, monitor=\'val_loss\', save_best_only=True),\r\nEarlyStopping(monitor=\'val_loss\', patience=5, verbose=1, mode=\'auto\')]\r\nstart = time.time()\r\nmodel_info=merged_model.fit([x1, x2, x1, x2, x1, x2], y=y, batch_size=64, epochs=3, verbose=True,\r\nvalidation_split=0.33, shuffle=True, callbacks=callbacks)\r\nend = time.time()\r\nprint(""Minutes elapsed: %f"" % ((start - end) / 60.))\r\n\r\n#####evaluting\r\n\r\n#load weights into a new model\r\n\r\nmodel_info.load_weights(save_best_weights)\r\npredictions = model.predict([test_q1, test_q2, test_q1, test_q2,test_q1, test_q2], verbose = True)\r\n\r\nRegards', 'Im trying to load json model with keras from json, but I got error!!: (this worked previously)\r\n\r\njson_file = open( root + ""checkpointsmodel_base_201948-154822.json"" )\r\nloaded_model_json = json_file.read()\r\njson_file.close()\r\nloaded_model = model_from_json(loaded_model_json)\r\n\r\nFull error log:\r\n\r\n    self.do_wait_suspend(thread, frame, event, arg)\r\n  File ""/snap/clion/73/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py"", line 125, in do_wait_suspend\r\n    self._args[0].do_wait_suspend(*args, **kwargs)\r\n  File ""/snap/clion/73/plugins/python/helpers/pydev/pydevd.py"", line 889, in do_wait_suspend\r\n    time.sleep(0.01)\r\nKeyboardInterrupt\r\n\r\nOriginal exception was:\r\nTraceback (most recent call last):\r\n  File ""/snap/clion/73/plugins/python/helpers/pydev/pydevd.py"", line 1758, in <module>\r\n    main()\r\n  File ""/snap/clion/73/plugins/python/helpers/pydev/pydevd.py"", line 1752, in main\r\n    globals = debugger.run(setup[\'file\'], None, None, is_module)\r\n  File ""/snap/clion/73/plugins/python/helpers/pydev/pydevd.py"", line 1147, in run\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File ""/snap/clion/73/plugins/python/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile\r\n    exec(compile(contents+""\\n"", file, \'exec\'), glob, loc)\r\n\r\n    loaded_model = model_from_json(loaded_model_json)\r\n  File ""/python3.5/site-packages/keras/models.py"", line 379, in model_from_json\r\n    return layer_module.deserialize(config, custom_objects=custom_objects)\r\n  File ""python3.5/site-packages/keras/layers/__init__.py"", line 55, in deserialize\r\n    printable_module_name=\'layer\')\r\n  File ""python3.5/site-packages/keras/utils/generic_utils.py"", line 144, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File ""python3.5/site-packages/keras/engine/topology.py"", line 2525, in from_config\r\n    process_layer(layer_data)\r\n  File ""python3.5/site-packages/keras/engine/topology.py"", line 2511, in process_layer\r\n    custom_objects=custom_objects)\r\n  File ""python3.5/site-packages/keras/layers/__init__.py"", line 55, in deserialize\r\n    printable_module_name=\'layer\')\r\n  File ""python3.5/site-packages/keras/utils/generic_utils.py"", line 144, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File ""python3.5/site-packages/keras/layers/core.py"", line 730, in from_config\r\n    printable_module_name=\'function in Lambda layer\')\r\n  File ""python3.5/site-packages/keras/utils/generic_utils.py"", line 161, in deserialize_keras_object\r\n    fn = module_objects.get(function_name)\r\nAttributeError: \'NoneType\' object has no attribute \'get\'\r\n']","[""\ndef save_model(self, model, options):\n    json_string = model.to_json()\n    open(options['file_arch'], 'w').write(json_string)\n    model.save_weights(options['file_weight'])\n\ndef load_model(self, options):\n    self.model = model_from_json(open(options['file_arch']).read())\n    self.model.load_weights(options['file_weight'])\n    return self.model\n""]",[],0,0
667,keras,5191,closed,Fix K.rnn documentation on unroll parameter,"**Edit**

Docs state the below, but unroll is a parameter used by both backends.

> unroll: with TensorFlow the RNN is always unrolled, but with Theano you can use this boolean flag to unroll the RNN.

For theano backend we use scan but for tf we use while_loop, even through tf also supports scan. Also, does anyone know if using scan instead of while_loop has any performance or other benefits?

Cheers,
Ben

Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on [StackOverflow](http://stackoverflow.com/questions/tagged/keras) or [join the Keras Slack channel](https://keras-slack-autojoin.herokuapp.com/) and ask there instead of filing a GitHub issue.

Thank you!

- [x ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps

- [x ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found [here](https://www.tensorflow.org/get_started/os_setup).

- [x ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps

- [ x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
",,"[""I'm pretty sure theano can optimize a scan better than an unrolled RNN. What I don't know is if TF will have different performance between unrolled and scan."", 'That\'s, like, not true. Keras has supported dynamic TF RNNs for as long as\nthey have been available in TF.\n\nOn Jan 26, 2017 10:00, ""Ben"" <notifications@github.com> wrote:\n\n> I\'m pretty sure theano can optimize a scan better than an unrolled RNN.\n> What I don\'t know is if TF will have different performance between unrolled\n> and scan.\n>\n> —\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/fchollet/keras/issues/5191#issuecomment-275461815>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AArWb4mOF7GQRtaAIR2Wf9qmhkPDersyks5rWN8kgaJpZM4Lu8X8>\n> .\n>\n', '@fchollet Then are the docs out of date or am I just misreading them?\r\n\r\n> unroll: with TensorFlow the RNN is always unrolled, but with Theano you can use this boolean flag to unroll the RNN.', ""@fchollet Thanks! I missed it because TF uses while_loop instead of a scan.\r\n\r\nThe docs make it sound like tensorflow `K.rnn` ignores `unroll`, which it doesn't.\r\n\r\nIs there any advantage to `control_flow_ops.while_loop` over `tf.scan`?\r\n\r\nSo my two comments are:\r\n* Should update docs because unrolling is supported in both backends.\r\n* Determine if there is any performance difference with using scan over while_loop. No idea what works best."", 'Updated documentation in this PR:\r\nhttps://github.com/fchollet/keras/pull/5192\r\n\r\nAs to the difference between while_loop and scan in tensorflow, anyone know if there is a performance difference or should I just try it and see?', '@fchollet serves me right for reading the doc instead of just reading the code ;)', ""Another weird inconsistency:\r\n\r\nDocs:\r\n> input_length: not relevant in the TensorFlow implementation. Must be specified if using unrolling with Theano.\r\n\r\nTensorflow implementation gets timesteps from the input shape and ignores the input_length parameter\r\n```\r\ntime_steps = tf.shape(inputs)[0]\r\n```\r\n\r\nTheano implementation requires the input_length parameter.\r\n```\r\n    if unroll:\r\n        if input_length is None:\r\n            raise ValueError('When specifying `unroll=True`, '\r\n                             'an `input_length` '\r\n                             'must be provided to `rnn`.')\r\n```\r\n\r\nWould it be more consistent if both got the input shape the same way? The time_steps parameter could be used by both backends, or could be removed and both could get the shape from the input."", 'Closing the issue. Just a small PR for the documentation.https://github.com/fchollet/keras/pull/5192']",[],[],0,0
