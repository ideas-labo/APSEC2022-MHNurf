,Repository,Number,State,Title,Body,Labels,Comments,Codes,Commands,class,related
0,pytorch,15872,closed,GPU memory usage is very high at the beginning of training.,"## üêõ Bug

At the beginning of the training, the GPU memory usage is high, i.e. about 9GB. After some iterations, the consumption is stable at 1.4 GB. 

So I can't set a larger batch size.

## To Reproduce
Change crop size to  on  https://github.com/pytorch/examples/blob/0.4/imagenet/main.py#L122 can reproduce the problem to some extent, i.e. 7.5GB at beginning and 4.5GB at stable.

My training scripts only has a more complicated dataset process.

## Environment
",,"['This is because you are using cudnn.benchmark, which needs a bit of memory to figure out what the best cudnn algorithm is. Once it has found a good algorithm, it caches it and no longer needs to test for it.', 'I see. Thanks!\r\nDisabling the cudnn.benchmark save a lot GPU memory. However, the performance drops a little as well.\r\nSo can I set the algorithm manually by cache it to disk?', 'Unfortunately there is no way of currently doing this. #4392 may be of interest to you.']",['\r\nCollecting environment information...\r\nPyTorch version: 0.4.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 2.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: TITAN Xp\r\nGPU 1: TITAN Xp\r\nGPU 2: TITAN Xp\r\nGPU 3: TITAN Xp\r\n\r\nNvidia driver version: 384.66\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n'],['334'],1,0
1,pytorch,8710,closed,Incorrect calculation for torch.nn.MSELoss,"## Issue description
torch.nn.MSELoss seems to get inaccurate calculation when size_average=True and reduce=True

## Code example

For tensors of 0.1's and 0's, the desired output should be 0.01. (0.1 - 0)^2 = 0.01.
I can get correct result by code below:


However, if I do something like below, I get inaccurate results:


But when I turned off size_average and reduce and calculate mean manually, I get desired output:

Or:



I kind of know that you cannot get exactly number when you using float datatype, but 0.01 and 0.00983 seems to be too off for me.
I am not quite sure if it is intended to be like that or I am not using it correctly.

## System Info
PyTorch version: 0.4.0
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 16.04.3 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 2.7
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce GTX 1080 Ti
GPU 1: GeForce GTX 1080 Ti
GPU 2: GeForce GTX 1080 Ti

Nvidia driver version: 387.34
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.2
/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.1.4
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn_static.a

Versions of relevant libraries:
[pip] numpy (1.14.5)
[pip] torch (0.4.0)
[pip] torchvision (0.2.1)
[conda] Could not collect
",,"['Actually it is `0.01` and `0.00983`. But still, it should definitely be more accurate. ', 'cc @zou3519 who did reduce for MSE', ""I don't know off of the top of my head why the accuracy would be bad, but we will definitely look into this."", '@SsnL yeah, it is 0.00983, I did a typo and thanks for pointing out.', 'Leaving this here for my reference\r\n```\r\nimport torch\r\npreds = torch.ones(5, 68, 64, 64) / 10\r\nlabels = torch.zeros_like(preds)\r\nres = torch.pow((preds - labels), 2)\r\nexpected = torch.mean(res)\r\ncriterion = torch.nn.MSELoss()\r\noutput = criterion(labels, preds)\r\n```', ""Okay the problem is that MSELoss with reduce=True is accumulating into float instead of double. This may be true with the other losses as well, I am going to take a look. I'll put a fix out soon."", '@zou3519 should we use `accreal` or `double`?', '`accreal`']","['\r\nIn [10]: preds = torch.ones(5, 68, 64, 64) / 10\r\nIn [11]: res = torch.pow((preds - labels), 2)\r\nIn [12]: torch.mean(res)\r\nOut[12]: \r\ntensor(1.00000e-02 *\r\n       1.0000)\r\n', '\r\nIn [16]: criterion = torch.nn.MSELoss()\r\nIn [17]: criterion(labels, preds)\r\nOut[17]: \r\ntensor(1.00000e-03 *\r\n       9.8371)\r\n', '\r\nIn [21]: criterion = torch.nn.MSELoss(False, False)\r\nIn [22]: torch.sum(criterion(labels, preds)) / (5*68*64*64)\r\nOut[22]: \r\ntensor(1.00000e-02 *\r\n       1.0000)\r\n', '\r\nIn [23]: torch.mean(criterion(labels, preds))\r\nOut[23]: \r\ntensor(1.00000e-02 *\r\n       1.0000)\r\n']",[],1,1
2,pytorch,13843,closed,Possible CPU-side memory leak even when fitting on GPU,"## üêõ Bug

A possible CPU-side memory leak even when fitting on the GPU using PyTorch 0.4.1.

## To Reproduce

I am quite new to PyTorch, having used TF/Keras extensively in the past, but am now trying to use PyTorch as a replacement. I decided to start small with a seq2seq Skip-Thought model, cobbled together using the PyTorch NLP tutorials. Everything seems to work fine when I run small scale tests, however, when I use the code to run a large scale fit on 3000 separate paragraphs (each paragraph having a variable number of sentences) I notice that my system RAM usage slowly goes up as the script runs, until eventually it hits 100% and the box becomes unresponsive and has to be force rebooted. The Linux box has 64 GB of RAM, and when the script starts, usage is 4.7% but this climbs steadily over time to 100%, at which point the box becomes unresponsive.

Since I'm new to PyTorch, I'm not sure if perhaps I'm doing something blatantly wrong in my code which would account for this behaviour?

This is my core fitting logic, which runs as a script on linux:



Here are the helper functions which create the tensors passed to the model by converting all text words into indices from a predefined gensim dictionary:



And, finally, here is the SkipThought model itself, with all its helper classes (apologies for the wall of code):



## Expected behavior

Memory usage in Linux should not linearly increase as the fitting script runs, especially not to the point at which the box dies.

## Environment

 - PyTorch Version (e.g., 1.0): 0.4.1
 - OS (e.g., Linux): Linux (Ubuntu 16.04)
 - How you installed PyTorch (, , source): pip install torch
 - Build command you used (if compiling from source): N/A
 - Python version: 3.6.6
 - CUDA/cuDNN version: CUDA 9.0.176 / CuDNN 7.4.1.5
 - GPU models and configuration: Tesla V100
 - Any other relevant information:

",todo,"['This is probably the cudnn memory leak at https://github.com/pytorch/pytorch/issues/1183', ""I thought so too, as I found that post when Googling this issue, however, I\nnote that issue was reported over a year ago, with CuDNN 7.1.0.2, and CuDNN\nis now on 7.4.x, and Nvidia claims to have solved this issue with driver\n390+, which is the driver I have on my box, so am surprised that this is\nstill an issue. I also note that I don't see this issue with Keras when\nusing its CuDNNLSTM layer for big fits on the same box.\n\nI will add code to my script today which disables CuDNN and see if I still\nget the memory leak.\n\nMay I please ask if the code I posted has any red flags in it which could\nbe causing this? I would very much appreciate knowing that this is not\nbeing caused by something in my code so I can stop looking there for a\npotential fix.\n\nOn Tue, 13 Nov 2018, 06:41 Tongzhou Wang <notifications@github.com wrote:\n\n> This is probably the cudnn memory leak at #1183\n> <https://github.com/pytorch/pytorch/issues/1183>\n>\n> ‚Äî\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/13843#issuecomment-438269112>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AYqzNBvpoq0JQdIQeMyTDr5hxCsL5wKkks5uusvwgaJpZM4YaEhb>\n> .\n>\n"", ""Oh actually this is a problem\r\n```\r\nloss.backward(retain_graph=True)\r\n```\r\n\r\nFrom what i see, your model shouldn't need to retain graph. You are never free the graph (which is CPU)! There is probably something wrong in your code, but it is too long and I have a flight to catch so I can't read the details."", ""You are correct, the retain_graph=True is the problem, and I have now fixed\nmy code so that it's not required anymore.\n\nI have now fixed everything and it's all working with no memory leak, so\nplease feel free to close/delete this issue.\n\nOn Tue, Nov 13, 2018 at 7:00 AM Tongzhou Wang <notifications@github.com>\nwrote:\n\n> Oh actually this is a problem\n>\n> loss.backward(retain_graph=True)\n>\n> From what i see, your model shouldn't need to retain graph. You are never\n> free the graph (which is CPU)! There is probably something wrong in your\n> code, but it is too long and I have a flight to catch so I can't read the\n> details.\n>\n> ‚Äî\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/13843#issuecomment-438275502>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AYqzNOdD1G_31X5YnvoIXDjmgAOQz5trks5uutCZgaJpZM4YaEhb>\n> .\n>\n""]","[""\r\ndevice = 'cuda'\r\nmodel = SkipThought(len(text_dictionary.token2id), 128, 256, nn.NLLLoss()).to(device)\r\n\r\n// corpus_orig is a list of lists where each list-element is a text paragraph with multiple sentences\r\nn_iters=len(corpus_orig)\r\n\r\nstart = time.time()\r\nprint_loss_total = 0\r\n\r\noptimizer = optim.SGD(model.parameters(), lr=0.01)\r\n\r\nfor xi in range(1, n_iters + 1):\r\n    x = corpus_orig[xi - 1]\r\n    sents = [x for x in map(str.strip, x.split('. ')) if len(x) > 0]\r\n    for i in range(1, len(sents) - 1):\r\n        input_tensor, prev_tensor, next_tensor = tensorsFromPair( (sents[i], sents[i - 1], sents[i + 1]) )\r\n\r\n        optimizer.zero_grad()\r\n        \r\n        loss, prev_output, next_output = model(input_tensor, prev_tensor, next_tensor, use_teacher_forcing=True)\r\n        \r\n        loss.backward(retain_graph=True)    // pytorch says I need retain_graph=True, otherwise I get an error here\r\n        optimizer.step()\r\n        \r\n        print_loss_total += loss.item()\r\n\r\n    print_loss_avg = print_loss_total\r\n    print_loss_total = 0\r\n    print('%s (%d %d%%) %.4f' % (timeSince(start, xi / n_iters),\r\n                         xi, xi / n_iters * 100, print_loss_avg), flush=True)\r\n"", '\r\ndef indexesFromSentence(sentence):\r\n    return text_dictionary.doc2idx(sentence.split())\r\n\r\ndef tensorFromSentence(sentence):\r\n    indexes = indexesFromSentence(sentence)\r\n    indexes.append(EOS_token)\r\n    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\r\n\r\ndef tensorsFromPair(pair):\r\n    input_tensor = tensorFromSentence(pair[0])\r\n    prev_tensor = tensorFromSentence(pair[1])\r\n    next_tensor = tensorFromSentence(pair[2])\r\n    return (input_tensor, prev_tensor, next_tensor)\r\n', '\r\nclass EncoderRNN(nn.Module):\r\n    def __init__(self, vocab_size, embedding_size, gru_size):\r\n        super(EncoderRNN, self).__init__()\r\n        self.vocab_size = vocab_size\r\n        self.embedding_size = embedding_size\r\n        self.gru_size = gru_size\r\n\r\n        self.embedding = nn.Embedding(vocab_size, embedding_size)\r\n        self.gru = nn.GRU(embedding_size, gru_size)\r\n\r\n    def forward(self, sentence, hidden):\r\n        embeddings = self.embedding(sentence)\r\n        embeddings = F.tanh(embeddings)\r\n        output, hidden = self.gru(embeddings, hidden)\r\n        \r\n        return output, hidden\r\n\r\n    def initHidden(self):\r\n        return torch.zeros(1, 1, self.gru_size, device=device)\r\n    \r\n    def getEmbedding(self, sentence):\r\n        return F.tanh(self.embedding(sentence))\r\n\r\nclass LocalAttention(nn.Module):\r\n    def __init__(self, dim):\r\n        super(LocalAttention, self).__init__()\r\n        self.W = nn.Linear(dim, dim, bias=False)\r\n\r\n    def score(self, decoder_hidden, encoder_out):\r\n        encoder_out = self.W(encoder_out)\r\n        encoder_out = encoder_out.permute(1, 0, 2)\r\n        return encoder_out @ decoder_hidden.permute(1, 2, 0)\r\n\r\n    def forward(self, decoder_hidden, encoder_out):\r\n        energies = self.score(decoder_hidden, encoder_out)\r\n        mask = F.softmax(energies, dim=1)\r\n        context = encoder_out.permute(1, 2, 0) @ mask\r\n        context = context.permute(2, 0, 1)\r\n        mask = mask.permute(2, 0, 1)\r\n        return context, mask\r\n\r\n\r\nclass SkipThought(nn.Module):\r\n    def __init__(self, vocab_size, embedding_size, gru_size, criterion):\r\n        super(SkipThought, self).__init__()\r\n        self.vocab_size = vocab_size\r\n        self.embedding_size = embedding_size\r\n        self.gru_size = gru_size\r\n        self.criterion = criterion\r\n        \r\n        self.encoder = EncoderRNN(vocab_size, embedding_size, gru_size)\r\n        self.prev_gru = nn.GRU(embedding_size + gru_size, gru_size)\r\n        self.next_gru = nn.GRU(embedding_size + gru_size, gru_size)\r\n        self.attention = LocalAttention(gru_size)\r\n        self.worder = nn.Linear(gru_size * 2, vocab_size)\r\n        self.softmax = nn.LogSoftmax(dim=1)\r\n        \r\n        self.encoder_hidden = self.encoder.initHidden()\r\n    \r\n    def forward(self, input_tensor, prev_tensor, next_tensor, use_teacher_forcing=True):        \r\n        encoder_hidden = self.encoder_hidden\r\n        \r\n        prev_length = prev_tensor.size(0)\r\n        next_length = next_tensor.size(0)\r\n    \r\n        loss = 0\r\n        \r\n        encoder_output, encoder_hidden = self.encoder(input_tensor, encoder_hidden)\r\n    \r\n        prev_input = torch.tensor([[SOS_token]], device=device)\r\n        next_input = torch.tensor([[SOS_token]], device=device)\r\n    \r\n        prev_hidden = encoder_hidden\r\n        next_hidden = encoder_hidden\r\n        \r\n        self.encoder_hidden = encoder_hidden\r\n    \r\n        prev_output = []\r\n        next_output = []\r\n    \r\n        if use_teacher_forcing:\r\n            # Teacher forcing: Feed the target as the next input\r\n            for di in range(prev_length):\r\n                embedded = self.encoder.getEmbedding(prev_input)\r\n                context, _ = self.attention(prev_hidden, encoder_output)\r\n                decoder_output, prev_hidden = self.prev_gru(torch.cat([embedded, context], dim=2), prev_hidden)\r\n                decoder_output = self.softmax(self.worder(torch.cat([decoder_output, context], dim=2)[0]))\r\n                loss += self.criterion(decoder_output, prev_tensor[di])\r\n                prev_output.append( decoder_output.topk(1)[1].squeeze().detach().item() )\r\n                prev_input = prev_tensor[di].unsqueeze(0)  # Teacher forcing\r\n            \r\n            for di in range(next_length):\r\n                embedded = self.encoder.getEmbedding(next_input)\r\n                context, _ = self.attention(next_hidden, encoder_output)\r\n                decoder_output, next_hidden = self.next_gru(torch.cat([embedded, context], dim=2), next_hidden)\r\n                decoder_output = self.softmax(self.worder(torch.cat([decoder_output, context], dim=2)[0]))\r\n                loss += self.criterion(decoder_output, next_tensor[di])\r\n                next_output.append( decoder_output.topk(1)[1].squeeze().detach().item() )\r\n                next_input = next_tensor[di].unsqueeze(0)  # Teacher forcing\r\n    \r\n        else:\r\n            # Without teacher forcing: use its own predictions as the next input\r\n            for di in range(prev_length):\r\n                embedded = self.encoder.getEmbedding(prev_input)\r\n                context, _ = self.attention(prev_hidden, encoder_output)\r\n                decoder_output, prev_hidden = self.prev_gru(torch.cat([embedded, context], dim=2), prev_hidden)\r\n                decoder_output = self.softmax(self.worder(torch.cat([decoder_output, context], dim=2)[0]))\r\n                topv, topi = decoder_output.topk(1)\r\n                prev_input = topi.squeeze().detach()  # detach from history as input\r\n                \r\n                loss += self.criterion(decoder_output, prev_tensor[di])\r\n                prev_output.append( prev_input.item() )\r\n                if prev_input.item() == EOS_token:\r\n                    break\r\n                prev_input = prev_input.unsqueeze(0).unsqueeze(0)\r\n            \r\n            for di in range(next_length):\r\n                embedded = self.encoder.getEmbedding(next_input)\r\n                context, _ = self.attention(next_hidden, encoder_output)\r\n                decoder_output, next_hidden = self.next_gru(torch.cat([embedded, context], dim=2), next_hidden)\r\n                decoder_output = self.softmax(self.worder(torch.cat([decoder_output, context], dim=2)[0]))\r\n                topv, topi = decoder_output.topk(1)\r\n                next_input = topi.squeeze().detach()  # detach from history as input\r\n    \r\n                loss += self.criterion(decoder_output, next_tensor[di])\r\n                next_output.append( next_input.item() )\r\n                if next_input.item() == EOS_token:\r\n                    break\r\n                next_input = next_input.unsqueeze(0).unsqueeze(0)\r\n        \r\n        return loss, prev_output, next_output\r\n']","['conda', 'pip']",1,1
3,pytorch,537,closed,Initial call to .cuda() very slow with Titan X,"The first call to .cuda() takes more than one minute when run on a system with a titan X:




Swapping a GTX 980ti on the exact same system results in normal timings (although with a very slight delay for the first call). On my laptop (GTX 960M), it is almost instant. 

",high priority,"['you installed with Conda.\r\nYour Titan-X is Titan-X Pascal.\r\n\r\nI am aware of the issue, i am building Conda binaries which have pre-compiled code fore 6.0 and 6.1 architectures :)', 'Yes, installed with conda, forgot to mention it. So installing from source would get rid of this issue?', '@GPistre installing from source, or installing from pip, both would fix it.', 'The initial slow down is because the CUDA runtime is JIT compiling its PTX\ncode to suit the 6.0 and 6.1 SASS instruction architectures. Soumith is\nfixing this by compiling the SASS for 6.0 and 6.1 beforehand in the conda\npackage.\n\nOn Sat, Jan 21, 2017 at 9:27 AM, Soumith Chintala <notifications@github.com>\nwrote:\n\n> @GPistre <https://github.com/GPistre> installing from source, or\n> installing from pip, both would fix it.\n>\n> ‚Äî\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/537#issuecomment-274275343>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEi_UGJrjxTrEjUdOLt1GCRjA-yi-jKuks5rUkARgaJpZM4LqIID>\n> .\n>\n\n\n\n-- \nDaniel Galvez\n', 'I have the same problem with GTX 1080.\r\nIt takes about 5 minutes to call the method cuda() for the first time\r\n```python\r\nimport torch\r\nx = torch.randn(4,5)\r\nx.cuda()\r\n```\r\n\r\nI use `conda install pytorch torchvision -c soumith` to install the pytorch. Both Python 2.4 and Python 3.5 have the same problem.', 'This should be fixed now if you use the command (website is updated):\r\n\r\n```bash\r\nconda install pytorch torchvision cuda80 -c soumith\r\n```\r\n\r\nThanks for waiting :)', 'I just updated to pytorch 0.11 using conda and this issue appeared again.\r\n\r\nJust to be sure, I created a new environment using\r\n```\r\nconda install pytorch torchvision cuda80 -c soumith\r\n```\r\nand the issue is there too.', 'What is your GPU?', 'TITAN X (Pascal)', 'I got the same problem \r\n\r\nI used the following command provided by the website to install torch\r\n\r\n> pip3 install http://download.pytorch.org/whl/cu80/torch-0.1.11.post4-cp35-cp35m-linux_x86_64.whl \r\n> pip3 install torchvision\r\n\r\nmodel.cuda() is much slower than model with only cpu. I use Python 3.5, GTX 1080 with cuda 8.0\r\n\r\nThank you.', ""I couldn't repro with conda packages, did not try wheels. .cuda() takes a few seconds, but that's expected. "", 'Thank you for your reply. I did not use .cuda() correctly. It works now. Thank you!  ', 'I am affected by this as well, Conda install, x64 Linux, Titan X (Pascal)', '@stepelu is the ""slow"" like 10 seconds-ish, or over a minute?', '@soumith over a minute. I actually interrupted it after that and noticed that the stacktrace was up to `cuda()`, can time it more precisely if needed', '@stepelu are you installing the CUDA8 version? I am suspecting you installed pytorch with:\r\n\r\n```\r\nconda install pytorch torchvision -c soumith\r\n```\r\n\r\nThe correct command for pascal GPUs is:\r\n```\r\nconda install pytorch torchvision cuda80 -c soumith\r\n```', '@soumith  yep, I actually just added you to the channel list and ran `conda update --all`:\r\n\r\n```\r\n(py3env) stepelu@xxx:~/Dropbox/Prj/pytorch/pthwr$ conda install pytorch torchvision cuda80 -c soumith\r\nFetching package metadata ...........\r\nSolving package specifications: .\r\n\r\n# All requested packages already installed.\r\n# packages in environment at /home/stepelu/conda3/envs/py3env:\r\n#\r\n\r\n```\r\n\r\nCUDA info:\r\n\r\n```\r\nNVIDIA-SMI 375.26                 Driver Version: 375.26\r\n```\r\n\r\nSo, `torch.cuda.is_available()` returns True, the call to `model.cuda()` completes in around 4 minutes, if I interrupt it I get:\r\n\r\n```\r\n...\r\n  File ""/home/stepelu/conda3/envs/py3env/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 147, in cuda\r\n    return self._apply(lambda t: t.cuda(device_id))\r\n  File ""/home/stepelu/conda3/envs/py3env/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 118, in _apply\r\n    module._apply(fn)\r\n  File ""/home/stepelu/conda3/envs/py3env/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 118, in _apply\r\n    module._apply(fn)\r\n  File ""/home/stepelu/conda3/envs/py3env/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 124, in _apply\r\n    param.data = fn(param.data)\r\n  File ""/home/stepelu/conda3/envs/py3env/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 147, in <lambda>\r\n    return self._apply(lambda t: t.cuda(device_id))\r\n  File ""/home/stepelu/conda3/envs/py3env/lib/python3.6/site-packages/torch/_utils.py"", line 65, in _cuda\r\n    return new_type(self.size()).copy_(self, async)\r\n  File ""/home/stepelu/conda3/envs/py3env/lib/python3.6/site-packages/torch/cuda/__init__.py"", line 280, in __new__\r\n    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)\r\nKeyboardInterrupt\r\n```\r\n\r\n\r\nHowever everything CUDA relates seems to run very slowly.\r\nFor example I interrupted the code while evaluating for testing and it was way slower than normal and got the following (truncated) stacktrace:\r\n\r\n```\r\n...\r\n  File ""/home/stepelu/conda3/envs/py3env/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 206, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""/home/stepelu/conda3/envs/py3env/lib/python3.6/site-packages/torch/nn/modules/container.py"", line 64, in forward\r\n    input = module(input)\r\n  File ""/home/stepelu/conda3/envs/py3env/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 206, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""/home/stepelu/conda3/envs/py3env/lib/python3.6/site-packages/torch/nn/modules/conv.py"", line 237, in forward\r\n    self.padding, self.dilation, self.groups)\r\n  File ""/home/stepelu/conda3/envs/py3env/lib/python3.6/site-packages/torch/nn/functional.py"", line 39, in conv2d\r\n    return f(input, weight, bias)\r\nKeyboardInterrupt\r\n```\r\n\r\nVersion 0.1.10 doesn\'t have this issue. \r\n\r\nI cannot completely rule out that it\'s a problem with the machine I\'m training on, but is there anything I can do to help with this issue?', '@soumith I found the issue by checking on another computer.\r\n\r\nIt seems the most recent version of PyTorch wants to upgrade to the non-cu80 version:\r\n\r\n```\r\nspeluchetti@xxx2:~$ conda update --all\r\nFetching package metadata ...........\r\nSolving package specifications: .\r\n\r\nPackage plan for installation in environment /home/speluchetti/conda3/envs/py3env:\r\n\r\nThe following packages will be UPDATED:\r\n\r\n    pytorch: 0.1.11-py360_4cu80 soumith [cuda80] --> 0.1.11-py36_5 soumith\r\n```\r\n\r\nAnd indeed on the desktop on which I had the slowdown the non-cu80 was installed.', ""So in the end it's probably an issue with the Conda packages metadata, I managed to install the py36_5-cu80 release via the direct https address but conda80 conflicts with it:\r\n\r\n```\r\nFetching package metadata ...........\r\nSolving package specifications: .\r\n\r\nPackage plan for installation in environment /home/stepelu/conda3/envs/py3env:\r\n\r\nThe following NEW packages will be INSTALLED:\r\n\r\n    cuda80:  1.0-0             soumith\r\n\r\nThe following packages will be DOWNGRADED due to dependency conflicts:\r\n\r\n    pytorch: 0.1.11-py35_5cu80 soumith [cuda80] --> 0.1.11-py360_4cu80 soumith [cuda80]\r\n```\r\n\r\nHopefully that helped to nail down the exact issue."", 'I\'ve also observed a clear disparity in the time taken by the first invocation of `x.cuda()` on Linux x64 with pytorch 0.1.12 and python 2.7.11 on a Titan X (Pascal); I\'ve confirmed that I have the cu80 version of the conda package installed:\r\n\r\n```\r\nconda search pytorch -c soumith|egrep ""0.1.12.*py27_2.*cuda80""\r\n                  *  0.1.12               py27_2cu80  soumith         [cuda80]\r\n```\r\nFor 0.1.12, the first invocation takes about 10 seconds for a 100x100 FloatTensor, while for 0.1.10, the first invocation takes about 2 seconds for the same.', 'I met the same problem. \r\n\r\n```\r\nGPU: GTX 1070\r\npytorch: 0.12.0(should be)\r\n```\r\n\r\nHowever, I can not install the cuda80 version because of the poor network condition.\r\n\r\n**Installing the pytorch from source** fixed it. ', 'Same problem (Pascal, GTX 1060): \r\n```\r\nimport torch\r\nx = torch.randn(4,5)\r\nx.cuda()\r\n```\r\ntakes forever (15 minutes in a real-world script, never seen it end on the test above). \r\n\r\nIt was installed with conda: \r\n_pytorch   0.2.0   py27hc03bea1_4cu80  [cuda80]  soumith_\r\n\r\nCannot install pytorch from source, compilation currently fails. \r\n\r\nConfirmed that `pip` fixes it (\r\n_pip install --upgrade --force-reinstall http://download.pytorch.org/whl/cu80/torch-0.2.0.post3-cp27-cp27mu-manylinux1_x86_64.whl_\r\n)', 'The following line does solved the problem for me.\r\n```\r\nconda install pytorch torchvision cuda80 -c soumith\r\n```\r\nFirst time Tensor.cuda() and Tensor.pin_memory() improves from 5 mins to 5s or less. Since the dataloader in the imagenet example also uses pin_memory(), the unusually long blocking may lead to the suspicion of deadlocks. Finally.', 'Solved the problem by installing with cuda80']","['\r\nimport torch\r\nfrom datetime import datetime\r\n\r\nfor i in range(10):\r\n    x = torch.randn(10, 10, 10, 10) # similar timings regardless of the tensor size\r\n    t1 = datetime.now()\r\n    x.cuda()\r\n    print(i, datetime.now() - t1)\r\n', '\r\n0 0:02:01.451538\r\n1 0:00:00.000205\r\n2 0:00:00.000057\r\n3 0:00:00.000058\r\n4 0:00:00.000053\r\n5 0:00:00.000054\r\n6 0:00:00.000052\r\n7 0:00:00.000051\r\n8 0:00:00.000059\r\n9 0:00:00.000051\r\n']",[],1,1
4,pytorch,7714,closed,Very slow for gradient penalty!,"Gradient penalty (GP) here means we minimize L2 norm of the gradient w.r.t input images.

I work on Ubuntu14.04, Python2.7, CUDA8.0 and CUDNN.
The version of Pytorch is 0.4.0.
I find it is very slow when I apply gradient penalty (GP) for training CIFAR10 with Resnet18.
I test the average running time of each step with and without GP:

4 times slower compared with the standard training!
Here is my code:

Is there better implementation?
Or just as slow as it is? ",,"['Gradient penalty requires computing second order derivatives, and just like computing the first one is 2-3x more expensive than the actual computation, the second one is 2-3x more expensive again, yielding a 4-9x slowdown. Closing as not a bug.']","['\r\nwithout : 0.065s\r\nwith GP: 0.330s\r\n', 'python\r\nimport torch\r\nfrom torch import nn, autograd\r\nfrom torch.autograd import Variable\r\nfrom models import PreActResNet18\r\nimport time\r\n\r\nnet = PreActResNet18()\r\nnet.cuda()\r\nopt = torch.optim.SGD(net.parameters(), 0.01, momentum=0.9, weight_decay=1e-4)\r\nbatchsize = 128\r\n\r\nGP = True\r\nstart = time.time()\r\n\r\nfor i in range(100):\r\n    x = torch.rand((batchsize, 3, 32, 32)).cuda()\r\n    y = torch.randint(0, 10, (batchsize,)).cuda().long()\r\n    x, y = Variable(x, requires_grad=True), Variable(y)\r\n    opt.zero_grad()\r\n    preds = net(x)\r\n    loss_c = nn.CrossEntropyLoss()(preds, y)\r\n\r\n    if GP:\r\n        grad = autograd.grad(loss_c, x, create_graph=True, retain_graph=True,\r\n                             only_inputs=True)[0]\r\n        loss_g = (grad ** 2).mean() * batchsize\r\n        (loss_c + loss_g).backward()\r\n    else:\r\n        loss_c.backward()\r\n    opt.step()\r\n\r\nend = time.time()\r\nprint(""{:.3f}s for each step"".format((end-start)/100))\r\n']",[],1,0
5,pytorch,28198,closed,DLRM performance regression on #26963,"## üêõ DLRM performance regression

<!-- A clear and concise description of what the bug is. -->
This check-in cause a little regression on the [DLRM](https://github.com/facebookresearch/dlrm) benchmark.

Without this check-in the DLRM benchmark result is like:

with this check-in , the result is like:


The profiling data show that there are some time increased in the 
 ,  and  operations

|                 | This      | Before    |
|-----------------|----------:|----------:|
|   | 1559.31ms | 1503.48ms |
|             | 27.14ms   |   14.58ms |
|          | 22.38ms   | 9.79966ms |


## To Reproduce

Steps to reproduce the behavior:

1. Download the DLRM from https://github.com/facebookresearch/dlrm
1. Modify the bench/dlrm_s_benchmark.sh to just run pytorch on cpu version, as
     build=0
     cpu=1
     gpu=0
     pt=1
     c2=0
   And export two KMP variables as
    export KMP_BLOCKTIME=1
    export KMP_AFFINITY=""granularity=fine,compact,1,0""
1. Run bench/dlrm_s_benchmark.sh on SKX8180 machine. performance profiling data is stored at file model1_CPU_PT_28.prof
     'This' is got from commit-id: d0a4b2f586e0901c3c65f1f0e0bae15364e28821
     'Before' is got from commit-id: 42e7eb0426190e07339f03d4e6afb61b7ff5ae9c
<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->
The DLRM performance has no impacted, Thanks 

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


 - PyTorch Version (e.g., 1.0): commit-id: d0a4b2f586e0901c3c65f1f0e0bae15364e28821
 - OS (e.g., Linux): Ubuntu 16.04.5 LTS
 - How you installed PyTorch (, , source):
 - Build command you used (if compiling from source): python setup.py install
 - Python version: 3.7
 - CUDA/cuDNN version: N/A
 - GPU models and configuration: N/A
 - Any other relevant information:
   GCC version: (Ubuntu 8.3.0-16ubuntu3~16.04) 8.3.0
   CMake version: version 3.14.4

    [pip3] numpy==1.16.2
    [pip3] numpydoc==0.8.0
    [conda] blas                      1.0                         mkl
    [conda] mkl                       2019.0                   pypi_0    pypi
    [conda] mkl-devel                 2019.3                      200
    [conda] mkl-include               2019.0                   pypi_0    pypi
    [conda] mkl-service               1.1.2            py37he904b0f_5
    [conda] mkl_fft                   1.0.10           py37ha843d7b_0
    [conda] mkl_random                1.0.2            py37hd81dba3_0

## Additional context

<!-- Add any other context about the problem here. -->


cc @ezyang @gchanan @zou3519 @jerryzh168",high priority triaged,['cc @VitalyFedyunin @ifedan '],"['\r\nnumactl --physcpubind=0-27 -m 0 python dlrm_s_pytorch.py --mini-batch-size=2048 --num-batches=100 --data-generation=random --arch-mlp-bot=512-512-64 --arch-mlp-top=1024-1024-1024-1 --arch-sparse-feature-size=64 --arch-embedding-size=1000000-1000000-1000000-1000000-1000000-1000000-1000000-1000000 --num-indices-per-lookup=100 --arch-interaction-op=dot --numpy-rand-seed=727 --print-freq=100 --print-time --enable-profiling > model1_CPU_PT_28.log\r\nMin time per iteration = 3591.85\r\n', '\r\nnumactl --physcpubind=0-27 -m 0 python dlrm_s_pytorch.py --mini-batch-size=2048 --num-batches=100 --data-generation=random --arch-mlp-bot=512-512-64 --arch-mlp-top=1024-1024-1024-1 --arch-sparse-feature-size=64 --arch-embedding-size=1000000-1000000-1000000-1000000-1000000-1000000-1000000-1000000 --num-indices-per-lookup=100 --arch-interaction-op=dot --numpy-rand-seed=727 --print-freq=100 --print-time --enable-profiling > model1_CPU_PT_28.log\r\nMin time per iteration = 3649.57\r\n', '\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n']","['index_select', 'mm', 'addmm', 'index_select', 'mm', 'addmm', 'conda', 'pip']",1,1
6,pytorch,26165,open,Memory leak in multithreading environment when loading checkpoint,"## üêõ Bug

I have a problem when loading saved checkpoint for pytorch model in seperate thread. CPU memory keeps increasing and is never released.

Multi-threading version of script increases RAM usage with each iteration and ends with 978 RESV memory (htop output).
Single-threading version holds with 374 RESV memory (htop output).
When I randomly initialize model (without state_dict loading) both versions use the same (smaller) amount of memory.

## To Reproduce



## Expected behavior

I expect the RAM to be cleared as no reference is being held to created models.

## Additional info

I have traced the issue to  https://github.com/pytorch/pytorch/blob/33221b19acc3dcacb11c38fdbff65d9a6ce90866/torch/nn/modules/module.py#L775
Commenting out this line makes the multi-threading version behave as single-threading one.

## Environment

 - PyTorch Version: 1.2.0
 - OS: Ubuntu 18.04
 - How you installed PyTorch: pip
 - Python version: 3.5.7


cc @ezyang @gchanan @zou3519 @jerryzh168",high priority module: memory usage module: multiprocessing module: multithreading needs reproduction triaged,"[""I'm not sure that we support `threading`, but we should try to repro this."", '@pietern says: this might be related to TLS (and probably not related to checkpoint.)\r\n\r\nAn even more minimal repro would be great. (marking this issue as hi-pri for someone to repro it).', '@simunovic-antonio Do you mean the resident set size when you say ""RESV""?\r\n\r\nI was not directly able to reproduce the leak with Python 3.7.4 on Ubuntu 18.04.1 using the Pytorch 1.2 (anaconda install).\r\n\r\n```\r\nuname -v\r\n#26~18.04.1-Ubuntu SMP Thu Aug 1 13:51:02 UTC 2019\r\n\r\n>>> torch.__version__\r\n\'1.2.0\'\r\n\r\n>>> torch.version.git_version\r\n\'8554416a199c4cec01c60c7015d8301d2bb39b64\'\r\n\r\npython --version\r\nPython 3.7.4\r\n```\r\n\r\nWithout threading:\r\n\r\n```\r\n$ /usr/bin/time -f ""Max res: %M kb"" python thread_leak.py \r\nCreate model 0\r\nCreate model 1\r\nCreate model 2\r\nCreate model 3\r\nCreate model 4\r\nMax res: 366072 kb\r\n```\r\n\r\nWith threading:\r\n```\r\n$ /usr/bin/time -f ""Max res: %M kb"" python thread_leak.py \r\nCreate model 0\r\nCreate model 1\r\nCreate model 2\r\nCreate model 3\r\nCreate model 4\r\nMax res: 366032 kb\r\n```\r\n\r\nCould you try to run your test-script using `/usr/bin/time -v`?', '@yf225 will try to reproduce this.', 'I got the same result as @andreaskoepf with Python 3.7.2 on Ubuntu 16.04.5 LTS using PyTorch v1.2.0:\r\n\r\nWithout threading:\r\n```\r\n$ /usr/bin/time -v python 26165_thread_leak.py\r\nCreate model 0\r\nCreate model 1\r\nCreate model 2\r\nCreate model 3\r\nCreate model 4\r\n\tCommand being timed: ""python 26165_thread_leak.py""\r\n\tUser time (seconds): 19.13\r\n\tSystem time (seconds): 2.70\r\n\tPercent of CPU this job got: 72%\r\n\tElapsed (wall clock) time (h:mm:ss or m:ss): 0:30.13\r\n\tAverage shared text size (kbytes): 0\r\n\tAverage unshared data size (kbytes): 0\r\n\tAverage stack size (kbytes): 0\r\n\tAverage total size (kbytes): 0\r\n\tMaximum resident set size (kbytes): 316164\r\n\tAverage resident set size (kbytes): 0\r\n\tMajor (requiring I/O) page faults: 0\r\n\tMinor (reclaiming a frame) page faults: 115446\r\n\tVoluntary context switches: 210\r\n\tInvoluntary context switches: 115\r\n\tSwaps: 0\r\n\tFile system inputs: 14391\r\n\tFile system outputs: 1000808\r\n\tSocket messages sent: 0\r\n\tSocket messages received: 0\r\n\tSignals delivered: 0\r\n\tPage size (bytes): 4096\r\n\tExit status: 0\r\n```\r\nWith threading:\r\n```\r\n$ /usr/bin/time -v python 26165_thread_leak.py\r\nCreate model 0\r\nCreate model 1\r\nCreate model 2\r\nCreate model 3\r\nCreate model 4\r\n\tCommand being timed: ""python 26165_thread_leak.py""\r\n\tUser time (seconds): 8.42\r\n\tSystem time (seconds): 29.13\r\n\tPercent of CPU this job got: 123%\r\n\tElapsed (wall clock) time (h:mm:ss or m:ss): 0:30.30\r\n\tAverage shared text size (kbytes): 0\r\n\tAverage unshared data size (kbytes): 0\r\n\tAverage stack size (kbytes): 0\r\n\tAverage total size (kbytes): 0\r\n\tMaximum resident set size (kbytes): 316872\r\n\tAverage resident set size (kbytes): 0\r\n\tMajor (requiring I/O) page faults: 370\r\n\tMinor (reclaiming a frame) page faults: 115678\r\n\tVoluntary context switches: 604\r\n\tInvoluntary context switches: 647635\r\n\tSwaps: 0\r\n\tFile system inputs: 32440\r\n\tFile system outputs: 1000808\r\n\tSocket messages sent: 0\r\n\tSocket messages received: 0\r\n\tSignals delivered: 0\r\n\tPage size (bytes): 4096\r\n\tExit status: 0\r\n```\r\n@simunovic-antonio It would be awesome if you can provide a docker image that can reproduce this issue. Thanks!', ""I have a similar problem. Tested with memory_profiler, it's clear that the memory rises slowly every time the model is invoked.\r\nIs that true that pytorch do not support threading module?\r\nMy envs:\r\npytorch1.4.0 (pytorch1.3.1 also tested, same problem), installed by conda. \r\nwindows10 64bits\r\ntorchvision 0.5\r\npython3.6.9\r\nRun code only in 'cpu' mode.\r\n\r\nTo reproduce, run the code below with: python -m memory_profiler  codefile.py:\r\n```python\r\nimport time\r\nimport torch\r\nimport torchvision as tv\r\nimport threading\r\nfrom memory_profiler import profile\r\nmodel = tv.models.vgg19_bn(pretrained=True)\r\n\r\n@profile\r\ndef model_func():  \r\n    input = torch.randn((1, 3, 224, 224), dtype=torch.float)\r\n    out = model(input)\r\n    return out\r\n\r\nif __name__ == '__main__':\r\n    for i in range(1000):\r\n        th = threading.Thread(target=model_func)\r\n        th.start()\r\n        th.join()\r\n        time.sleep(0.5)\r\n```\r\nIf any answers, appreciated."", 'i have same  issue with cpu device,  but GPU  device is okay, ']","['\r\nimport torch\r\nimport time\r\nimport threading\r\nimport torchvision\r\n\r\ndef create_tensor(index):\r\n    print(""Create model {}"".format(index))\r\n    resnet50 = torchvision.models.resnet50(pretrained=True)\r\n    del resnet50\r\n\r\ndef create_tensor_thread(index):\r\n    x = threading.Thread(target=create_tensor, args=(index,))\r\n    x.start()\r\n    x.join()\r\n\r\ndef run(use_threading=False):\r\n    for index in range(5):\r\n        if use_threading:\r\n            #memory leak observed \r\n            create_tensor_thread(index) \r\n        else:\r\n            #no memory leak observed\r\n            create_tensor(index)\r\n        time.sleep(5)\r\n\r\nrun(True)\r\n']",[],1,0
7,pytorch,13886,closed,Get wrong with Pytorch derivative of division,"I'm trying to compute the gradient of 1/x without using Pytorch's autograd. I use the formula grad(1/x, x) = -1/(x**2) but when I compare my result with the gradient given by Pytorch's autograd, they're different.
Here is my code

The output is

Can anyone explain why ?",,['please use https://discuss.pytorch.org for questions'],"['\r\na = torch.tensor(np.random.randn(), dtype=dtype, requires_grad=True)\r\nloss = 1/a\r\nloss.backward()\r\nprint(a.grad - (-1/(a**2)))\r\n', '\r\ntensor(5.9605e-08, grad_fn=<ThAddBackward>)\r\n']",[],1,0
8,pytorch,31295,closed,nll_loss with weights: reduction 'mean' gives wrong result ,"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:




1. on cpu


2. on gpu

tensor([1, 0, 4], device='cuda:0')

>>> reduction = ['none', 'sum', 'mean']
>>> for r in reduction:
...        m = F.log_softmax(i, dim=1)
...        loss = F.nll_loss(m, target, w, reduction=r)

none :  tensor([-0.0455,  0.1291,  0.8693], device='cuda:0', grad_fn=<NllLossBackward>)
sum :  tensor(0.9530, device='cuda:0', grad_fn=<NllLossBackward>)
mean :  tensor(2.3681, device='cuda:0', grad_fn=<NllLossBackward>)
>> loss = F.nll_loss(m, target, w, reduction='none')>> loss.mean()mean :  tensor(0.8193, grad_fn=<MeanBackward0>)`

<!-- A clear and concise description of what you expected to happen. -->

## Environment

PyTorch version: 1.3.1
Is debug build: No
CUDA used to build PyTorch: 10.1.243

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.0.2

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.1.243
GPU models and configuration: 
GPU 0: TITAN X (Pascal)
GPU 1: TITAN X (Pascal)

Nvidia driver version: 430.50
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip3] numpy==1.13.3
[conda] blas                      1.0                         mkl  
[conda] mkl                       2019.4                      243  
[conda] mkl-service               2.3.0            py36he904b0f_0  
[conda] mkl_fft                   1.0.14           py36ha843d7b_0  
[conda] mkl_random                1.1.0            py36hd6b4f25_0  
[conda] pytorch                   1.3.1           py3.6_cuda10.1.243_cudnn7.6.3_0    pytorch
[conda] torchvision               0.4.2                py36_cu101    pytorch


cc @ezyang @gchanan @zou3519",high priority triage review triaged,"[""Seems bad if there is a correctness issue. I haven't checked the code, so we should first verify that the behavior is indeed correct. nll_loss is an important loss function."", ""This isn't actually a bug -- @anjali411 is going to post why."", ""This is not a bug. The loss in case of mean is calculated by the following formula mentioned in the documentation https://pytorch.org/docs/stable/nn.html#nllloss \r\nAccording to the formula:\r\nFor CPU example:\r\nwhen reduction = mean, loss =  Œ£ ln/(w1+w0+w4) for n=1 to 3 where li's are the elements in the loss tensor for reduction = none\r\nThus loss = (2.0560 +  2.6612 + (-2.2593))/(0.9580+1.3221-0.7506) = 1.6070\r\n\r\nFor GPU example:\r\n\r\nwhen reduction = mean, loss = Œ£ ln/(w1+w0+w4) for n=1 to 3\r\n                                                   = (-0.0455 + 0.1291 + 0.8693)/ (0.1391 -0.1082 + 0.3715) = 2.3681\r\n\r\nClarification for documentation yn = nth element in the target tensor\r\n\r\n"", '@anjali411 Thank you for the explanation']","[""\r\n>>> i = torch.randn(3, 5, requires_grad=True, device='cpu')\r\ntensor([[ 0.3367,  0.1288,  0.2345,  0.2303, -1.1229],\r\n        [-0.1863,  2.2082, -0.6380,  0.4617,  0.2674],\r\n        [ 0.5349,  0.8094,  1.1103, -1.6898, -0.9890]], requires_grad=True)\r\n\r\n>>> w = torch.randn(5, device='cpu')\r\ntensor([ 0.9580,  1.3221,  0.8172, -0.7658, -0.7506])\r\n\r\n>>> target = torch.tensor([1, 0, 4], device='cpu')\r\ntensor([1, 0, 4])\r\n\r\n>>> reduction = ['none', 'sum', 'mean']\r\n>>> for r in reduction:\r\n...        m = F.log_softmax(i, dim=1)\r\n...        loss = F.nll_loss(m, target, w, reduction=r)\r\n\r\nnone :  tensor([ 2.0560,  2.6612, -2.2593], grad_fn=<NllLossBackward>)\r\nsum :  tensor(2.4579, grad_fn=<NllLossBackward>)\r\nmean :  tensor(1.6070, grad_fn=<NllLossBackward>)\r\n""]","['torch.manual_seed(42)', 'torch.cuda.manual_seed(42)', '', ""\r\n>>> i = torch.randn(3, 5, requires_grad=True, device='cuda:0')\r\ntensor([[ 0.1940,  2.1614, -0.1721,  0.8491, -1.9244],\r\n        [ 0.6530, -0.6494, -0.8175,  0.5280, -1.2753],\r\n        [-1.6621, -0.3033, -0.0926,  0.1992, -1.1204]], device='cuda:0',\r\n       requires_grad=True)\r\n\r\n>>> w = torch.randn(5, device='cuda:0')\r\ntensor([ 0.1391, -0.1082, -0.7174,  0.7566,  0.3715], device='cuda:0')\r\n\r\n> >> target = torch.tensor([1, 0, 4], device='cuda:0')"", '', '\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n', '\r\n', '\r\n\r\n']",1,0
9,pytorch,1088,closed,GPU slower than CPU on a simple RNN test code,"Hi,

I wanted to write an RNN from scratch using the pytorch cuda capabilities and I ran some preliminary tests to compare the speed of the CPU vs GPU. The task is very simple and consists of a for loop mimicking the update of the internal state x in an RNN with recurrent weight matrix J. I'm using a Quadro K620 with cuda 8.0.

When the size of x is N=1000 there seems to be a trade-off, with the GPU implementation consistently getting slower when the number of iterations increases (I ran some other tests with different sizes of the J matrix and this behaviour seems pretty systematic).

This is an example of running times I get when running the enclosed script (number of iterations are 100, 1000, 10000, 100000):

cpu: [0.010117292404174805, 0.058980703353881836, 0.45785975456237793, 4.512230634689331]
gpu: [0.0019445419311523438, 0.05474495887756348, 0.7503962516784668, 7.011191129684448]

I'd really appreciate some help on this. Thanks in advance.

The test script is the following:

",,"['if you are looking for help, here is the right place to go and post. Thanks for using PyTorch:\r\n\r\nhttps://discuss.pytorch.org', 'I actually posted there already but I had no reply so far:\r\n\r\nhttps://discuss.pytorch.org/t/gpu-slower-than-cpu-on-a-simple-rnn-test-code/1306', 'I got a different outcome from you. I think it may depend on GPU.\r\n```shell\r\n>>> cputimes = []\r\n>>> for sampl in (100, 1000, 10000, 100000):\r\n...         start = time.time()\r\n...\r\n>>>\r\n>>> for sampl in (100, 1000, 10000, 100000):\r\n...     start = time.time()\r\n...     for i in range(sampl):\r\n...             rn = np.tanh(xn)\r\n...             xn = Jn.dot(xn)\r\n...     end = time.time()\r\n...     cputimes.append(end-start)\r\n...\r\n>>> print(cputimes)\r\n[0.1352827548980713, 1.2355773448944092, 6.053329706192017, 49.56582975387573]\r\n>>> Jc = J.cuda()\r\n>>> xc = x.cuda()\r\n>>> rc = r.cuda()\r\n>>>\r\n>>> gputimes = []\r\n>>> for sampl in (100, 1000, 10000, 100000):\r\n...     start = time.time()\r\n...     for i in range(sampl):\r\n...             rc = tr.tanh(xc)\r\n...             xc = Jc.mv(xc)\r\n...     end = time.time()\r\n...     gputimes.append(end-start)\r\n...\r\n>>> print(gputimes)\r\n[0.626868486404419, 0.01941514015197754, 0.36851024627685547, 3.3849217891693115]\r\n>>>\r\n```', 'or the CPU :)\r\n[0.008335351943969727, 0.040863990783691406, 0.3155703544616699, 3.076826333999634]\r\n[0.6438207626342773, 0.0439152717590332, 0.7193911075592041, 7.180693864822388]']","['python\r\nimport numpy as np\r\nimport torch as tr\r\nimport math\r\nimport time\r\n\r\nGPUID = 0\r\ntr.cuda.set_device(GPUID)\r\n\r\nN = 1000\r\n\r\nJ = tr.randn(N,N)\r\nx = tr.randn(N)\r\nr = tr.randn(N)\r\n\r\nJn = J.numpy()\r\nxn = x.numpy()\r\nrn = r.numpy()\r\n\r\ncputimes = []\r\nfor sampl in (100, 1000, 10000, 100000):\r\n    start = time.time()\r\n    for i in xrange(sampl):\r\n        rn = np.tanh(xn)\r\n        xn = Jn.dot(xn);\r\n    end = time.time()\r\n    cputimes.append(end-start)\r\nprint(cputimes)\r\n\r\nJc = J.cuda()\r\nxc = x.cuda()\r\nrc = r.cuda()\r\n\r\ngputimes = []\r\nfor sampl in (100, 1000, 10000, 100000):\r\n    start = time.time()\r\n    for i in xrange(sampl):\r\n        rc = tr.tanh(xc)\r\n        xc = Jc.mv(xc);\r\n    end = time.time()\r\n    gputimes.append(end-start)\r\nprint(gputimes)\r\n']",[],1,0
10,pytorch,11333,closed,MultivariateNormal and potrf is slow on gpu and seems to have some memory leak,"## Issue description

When I initialize a [MultivariateNormal](https://github.com/pytorch/pytorch/blob/bb7d1837bc164b06e1d0826a20a8f7e8338a44e3/torch/distributions/multivariate_normal.py#L76) object with a covariance matrix provided with batch dimension too, it is very slow on GPU, and it is always slower on GPU then on CPU. 
It even seems to leak the memory with or without the batch dimension and both on CPU and GPU, but with different intensity (check n parameter in code example).
One more strange thing: It uses more CPU RAM when running on GPU.

Digging a bit deeper, I experienced the same with the Tensor's potrf function, so probably this is the root cause.

## Code example


## Outputs

Testing MultivariateNormal









Testing potrf









## System Info
Collecting environment information...
PyTorch version: 0.5.0a0+e9ad743
Is debug build: No
CUDA used to build PyTorch: 9.2.88

OS: Ubuntu 18.04.1 LTS
GCC version: (Ubuntu 7.3.0-16ubuntu3) 7.3.0
CMake version: version 3.12.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.2.148
GPU models and configuration: 
GPU 0: GeForce GTX 1080 Ti
GPU 1: GeForce GTX 1080 Ti
GPU 2: GeForce GTX 1080 Ti

Nvidia driver version: 396.54
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.2.1
/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a

Versions of relevant libraries:
[pip] Could not collect
[conda] magma-cuda91              2.3.0                         1    pytorch
[conda] torch                     0.5.0a0+e9ad743           <pip>
[conda] torchfile                 0.1.0                     <pip>
[conda] torchnet                  0.0.4                     <pip>
[conda] torchvision               0.2.1                     <pip>

Pytorch is used in docker container
pip version:
pip 18.0 from /opt/conda/lib/python3.6/site-packages/pip (python 3.6)
",high priority,"['I think batch potrf #11796 may favourably change the performance on GPU.', ""I could not reproduce the memory leak using `1.0.0.dev20181119`. @koszpe could you please retry this test with the nightly version? Also, batched Cholesky is available now which is extremely faster than looping, so there's no need of loops to compute the Cholesky factors of a batch of positive definite matrix."", ""I checked it, there is no memory leak with the MultivariateNormal, neither with the torch.cholesky. I used 1.0.0.dev20181130 for testing. The potrf still have the issue, but it's not a problem anymore, if we use torch.cholesky instead."", 'Thank you for checking @koszpe . Could you close the issue since it is now resolved?', 'Thanks for the fix', ""@koszpe Sorry to revive an old issue, but do you remember if you ever fixed initialising the distribution being so much slower on GPU than CPU? \r\n\r\nI'm also having similar issues, and reported it here: https://github.com/pytorch/pytorch/issues/23780"", ""@danielcrane Sorry for the late answer. No, I only had to initialize the distribution once at initialization phase, so it wasn't a real problem for me.""]","['\r\nimport resource\r\nimport torch\r\nfrom torch.distributions.multivariate_normal import MultivariateNormal\r\nimport gc\r\nimport time\r\n\r\ndef gpu(item, use_GPU=True):\r\n    if use_GPU:\r\n        return item.cuda()\r\n    else:\r\n        return item\r\n\r\ndef test_multivariate(loc_mat, cov_mat):\r\n    standard_normal_dist = MultivariateNormal(loc=loc_mat,\r\n                                              covariance_matrix=cov_mat)\r\n\r\ndef test_potrf(loc_mat, cov_mat):\r\n    n = cov_mat.size(-1)\r\n    [m.potrf(upper=False) for m in cov_mat.reshape(-1, n, n)]\r\n\r\ndef loop(fn_ref, n=10, size=10000, use_GPU=True, covariance_with_batch_dim=True, print_n=1):\r\n    cov_mat = gpu(torch.zeros((2, 2)) + torch.eye(2), use_GPU)\r\n    if covariance_with_batch_dim:\r\n        cov_mat = cov_mat.unsqueeze(0).repeat(size, 1, 1)\r\n    loc_mat = gpu(torch.zeros((size, 2)), use_GPU)\r\n\r\n    for i in range(n):\r\n        gc.collect()\r\n        fn_ref(loc_mat, cov_mat)\r\n        if i % print_n == 0:\r\n            print(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n\r\n    n = 10\r\n    fn_ref = test_multivariate\r\n    use_GPU = True\r\n    covariance_with_batch_dim = True\r\n    print_n = n / 10\r\n\r\n    print(""test_fn: "", fn_ref.__name__)\r\n    print(""n: "", n)\r\n    print(""use_GPU: "", use_GPU)\r\n    print(""covariance_with_batch_dim: "", covariance_with_batch_dim, ""\\n"")\r\n\r\n    start = time.time()\r\n\r\n    loop(fn_ref=fn_ref, use_GPU=use_GPU, covariance_with_batch_dim=covariance_with_batch_dim, n=n, print_n=print_n)\r\n\r\n    print(""\\ntime: "", (time.time() - start) / n)\r\n', '\r\ntest_fn:  test_multivariate\r\nn:  5\r\nuse_GPU:  False\r\ncovariance_with_batch_dim:  True \r\n\r\n118808\r\n119200\r\n119552\r\n119908\r\n120268\r\n\r\ntime:  0.08764233589172363\r\n', '\r\ntest_fn:  test_multivariate\r\nn:  5\r\nuse_GPU:  True\r\ncovariance_with_batch_dim:  True \r\n\r\n1477808\r\n1479680\r\n1479996\r\n1480312\r\n1480684\r\n\r\ntime:  27.707138347625733\r\n', '\r\ntest_fn:  test_multivariate\r\nn:  5000\r\nuse_GPU:  False\r\ncovariance_with_batch_dim:  False \r\n\r\n107904\r\n107904\r\n109436\r\n109436\r\n109700\r\n\r\ntime:  0.008288921689987183\r\n', '\r\ntest_fn:  test_multivariate\r\nn:  50\r\nuse_GPU:  True\r\ncovariance_with_batch_dim:  False \r\n\r\n1467364\r\n1468676\r\n1468676\r\n1468680\r\n1468680\r\n\r\ntime:  0.037353959083557126\r\n', '\r\ntest_fn:  test_potrf\r\nn:  5\r\nuse_GPU:  False\r\ncovariance_with_batch_dim:  True \r\n\r\n116984\r\n117300\r\n117672\r\n118032\r\n118380\r\n\r\ntime:  0.06480374336242675\r\n', '\r\ntest_fn:  test_potrf\r\nn:  5\r\nuse_GPU:  True\r\ncovariance_with_batch_dim:  True \r\n\r\n1476688\r\n1477008\r\n1477320\r\n1477916\r\n1478168\r\n\r\ntime:  26.757283926010132\r\n', '\r\ntest_fn:  test_potrf\r\nn:  50000\r\nuse_GPU:  False\r\ncovariance_with_batch_dim:  False \r\n\r\n108420\r\n109720\r\n109984\r\n110248\r\n110512\r\n\r\ntime:  0.008067794122695923\r\n', '\r\ntest_fn:  test_potrf\r\nn:  5\r\nuse_GPU:  True\r\ncovariance_with_batch_dim:  False \r\n\r\n1468760\r\n1469664\r\n1469680\r\n1469680\r\n1469680\r\n\r\ntime:  0.2711141109466553\r\n']",[],1,0
11,pytorch,3665,closed,Slight memory leak for LSTM ,"Hi everyone, 
I find there is a slight memory leak during training my lstm networks. 
Here is the running environment:

> torch: 0.2.0.3
> cuda: 7.5
> OS:ubuntu 16.04LTS

This issue is similar to the discuss [here](https://discuss.pytorch.org/t/tracking-down-a-suspected-memory-leak/1130/9). 
Following with the discussion, I found to disable the cuDNN (adding ) can solve this problem but the speed is affected either. 

I am quite confused about the memory leak when using cuDNN. 



Results when cuDNN is enabled, the memory leak occurs in every epoch. Screenshot:
![cudnn_enable](https://user-images.githubusercontent.com/9111828/32723137-558d6d6a-c8a7-11e7-8934-644aeb1aa762.png)

Results when cuDNN is disabled, the memory leak is almost eleminated (although happens in first several epochs). Screenshot:
![cudnn_disable](https://user-images.githubusercontent.com/9111828/32723151-638edee4-c8a7-11e7-825b-f6f604b7aaf2.png)
",,"[""I'm pretty sure this is just the granularity at which the CPU allocator reserves memory chunks when they're needed and the cuDNN path is longer in Python, so it has to create more objects, increasing the allocator pressure. If you ran your script for more than 4 epochs (say 100-200), would it keep allocating memory?"", '@apaszke Thanks for your reply!\r\n\r\nUnfortunately, when I enable the cuDNN, the memory keeps increasing even after 200 epochs. \r\nWhile cuDNN is disabled, the memory keeps constant after several epochs. \r\n\r\nWhen enables cuDNN: (epoch > 240)\r\n![enable](https://user-images.githubusercontent.com/9111828/32726630-8ffcdb4a-c8b4-11e7-8096-82bfbfd2d45b.png)\r\n\r\nWhen disable cuDNN: (epoch > 28)\r\n![disable](https://user-images.githubusercontent.com/9111828/32726638-9a161718-c8b4-11e7-9a51-5a1a6bba7da3.png)\r\n', 'I can reproduce this on 0.3 branch as well.', 'Also reproducible on 0.2.', '@ngimel is this the same cudnn bug that was once discussed? bidirectional cudnn RNN', ""@soumith I just devised a smaller repro example.. All it needs is cuDNN and RNN. Whether it is bidirectional/LSTM doesn't matter."", '@soumith does not look like one, there cudnn was leaking a little bit at every iteration, here one can go for hundreds of iterations without increasing memory. ', ""It's very consistent and seems periodic:\r\n```\r\n     train iterations: 2900, added mem: 0.0M\r\n     train iterations: 3000, added mem: 0.0M\r\n     train iterations: 3100, added mem: 0.0M\r\n     train iterations: 3200, added mem: 0.0M\r\n     train iterations: 3300, added mem: 0.25390625M\r\n     train iterations: 3400, added mem: 0.0M\r\n     train iterations: 3500, added mem: 0.0M\r\n     train iterations: 3600, added mem: 0.0M\r\n     train iterations: 3700, added mem: 0.0M\r\n     train iterations: 3800, added mem: 0.0M\r\n     train iterations: 3900, added mem: 0.0M\r\n     train iterations: 4000, added mem: 0.2578125M\r\n     train iterations: 4100, added mem: 0.0M\r\n     train iterations: 4200, added mem: 0.0M\r\n     train iterations: 4300, added mem: 0.0M\r\n     train iterations: 4400, added mem: 0.0M\r\n     train iterations: 4500, added mem: 0.0M\r\n     train iterations: 4600, added mem: 0.0M\r\n     train iterations: 4700, added mem: 0.2578125M\r\n     train iterations: 4800, added mem: 0.0M\r\n     train iterations: 4900, added mem: 0.0M\r\n     train iterations: 5000, added mem: 0.0M\r\n     train iterations: 5100, added mem: 0.0M\r\n     train iterations: 5200, added mem: 0.0M\r\n     train iterations: 5300, added mem: 0.0M\r\n     train iterations: 5400, added mem: 0.2578125M\r\n     train iterations: 5500, added mem: 0.0M\r\n     train iterations: 5600, added mem: 0.0M\r\n     train iterations: 5700, added mem: 0.0M\r\n     train iterations: 5800, added mem: 0.0M\r\n     train iterations: 5900, added mem: 0.0M\r\n     train iterations: 6000, added mem: 0.0M\r\n     train iterations: 6100, added mem: 0.2578125M\r\n     train iterations: 6200, added mem: 0.0M\r\n     train iterations: 6300, added mem: 0.0M\r\n     train iterations: 6400, added mem: 0.0M\r\n     train iterations: 6500, added mem: 0.0M\r\n     train iterations: 6600, added mem: 0.0M\r\n     train iterations: 6700, added mem: 0.0M\r\n     train iterations: 6800, added mem: 0.2578125M\r\n     train iterations: 6900, added mem: 0.0M\r\n     train iterations: 7000, added mem: 0.0M\r\n     train iterations: 7100, added mem: 0.0M\r\n     train iterations: 7200, added mem: 0.0M\r\n     train iterations: 7300, added mem: 0.0M\r\n     train iterations: 7400, added mem: 0.2578125M\r\n```"", '```\r\nimport torch\r\nimport torch.autograd as autograd\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.optim as optim\r\n\r\ntorch.manual_seed(1)\r\n#torch.backends.cudnn.enabled = False\r\nnet = nn.RNN(3, 3).cuda()\r\ninputs = autograd.Variable(torch.randn(5, 1, 3)).cuda()\r\n\r\nimport os, gc\r\npid = os.getpid()\r\nprev_mem=0\r\n\r\nfor it in range(60000):\r\n    net.zero_grad()\r\n    out, hidden = net(inputs.detach())\r\n    l = out.sum() - hidden.sum()\r\n    l.backward()\r\n    if it % 100 == 0:\r\n        cur_mem = (int(open(\'/proc/%s/statm\'%pid, \'r\').read().split()[1])+0.0)/256\r\n        add_mem = cur_mem - prev_mem\r\n        prev_mem = cur_mem\r\n        print(""     train iterations: %s, added mem: %sM""%(it, add_mem))\r\n```\r\n\r\nconstant amount of memory leak no matter the network size.\r\n\r\nCPU: not leaking\r\nCUDA w/o cuDNN: not leaking\r\nCUDA w/ cuDNN: leaking\r\n\r\nretain_graph=True: not changing anything', '@jiesutd @Gosicfly Thanks for reporting this. After investigating, we have identified this is a bug in cuDNN. NVIDIA is looking into this! ', '@SsnL Thanks!\r\n\r\nHope it can be solved soon.', 'any chance this is fixed in cudnn 7?', 'Unfortunately this isn‚Äôt fixed in cudnn 7.\n\nOn Mon, Dec 11, 2017 at 19:04 Matt <notifications@github.com> wrote:\n\n> any chance this is fixed in cudnn 7?\n>\n> ‚Äî\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/3665#issuecomment-350899805>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AFaWZTf2OyQ0o-iDHjCwTJm-GDlEqxd4ks5s_cMqgaJpZM4QbnY4>\n> .\n>\n', '@SsnL is there a public tracker for that issue on the CuDNN side\u202f?', 'I‚Äôm not sure. Maybe @ngimel can answer this question better.\n\nOn Sat, Mar 3, 2018 at 09:33 Evpok <notifications@github.com> wrote:\n\n> @SsnL <https://github.com/ssnl> is there a public tracker for that issue\n> on the CuDNN side\u202f?\n>\n> ‚Äî\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/3665#issuecomment-370151686>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AFaWZWI9sHKutplyPPGJ2Cv3CdoAdtH2ks5taqm9gaJpZM4QbnY4>\n> .\n>\n', 'Hi, I am encountering the same issue too. I wonder if this has been fixed by now?', 'cc @ngimel What is the status on this from the cudnn team?', ""They claim it's been fixed. "", ""@ngimel Thanks! Do you know if it's in a release yet? If so, which one?"", 'Should be in the release, cudnn 7.1+, driver 384.69+', ""Thanks @ngimel !\r\n\r\nI'll close this issue for now. If you see this issue, please upgrade to cudnn 7.1+, driver 384.69+. \r\n\r\n@Evpok @jiesutd @bangbangjim See above."", 'Issue appears to still be present (cudnn 7.1.4, cuda 9.2, 384.130)', ""I want to share some of my recent logs on the memory leaking issues.\r\n\r\ntesting scipts:\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nprint('torch-version: ', torch.__version__)\r\nprint('cudnn version: ', torch.backends.cudnn.version())\r\nlstm = nn.LSTM(200, 2, batch_first=True).cuda()\r\n\r\n\r\nfake_data = torch.Tensor(96, 200, 200).cuda()\r\n\r\n\r\ndef test():\r\n    import os\r\n    import psutil\r\n    pid = os.getpid()\r\n    py = psutil.Process(pid)\r\n    for i in range(2000):\r\n        loss = lstm(fake_data)[0].mean()\r\n        loss.backward()\r\n        if i % 100 == 0:\r\n            import gc\r\n            gc.collect()\r\n            memoryUse = py.memory_info()[0] / 2.**20  # memory use in MB...I think\r\n            print('iteration {}: memory use: {}MB'.format(i, memoryUse))\r\n\r\n\r\ntest()\r\n\r\ntorch.backends.cudnn.enabled = False\r\nprint('With cudnn disabled')\r\ntest()\r\n```\r\n\r\n## logs:\r\n\r\n### Driver=390\r\n```\r\ntorch-version:  0.4.0\r\ncudnn version:  7102\r\niteration 0: memory use: 1992.01953125MB\r\niteration 100: memory use: 1992.1953125MB\r\niteration 200: memory use: 1992.1953125MB\r\niteration 300: memory use: 1992.3125MB\r\niteration 400: memory use: 1992.34375MB\r\niteration 500: memory use: 1992.53125MB\r\niteration 600: memory use: 1992.55859375MB\r\niteration 700: memory use: 1992.55859375MB\r\niteration 800: memory use: 1992.6328125MB\r\niteration 900: memory use: 1992.6328125MB\r\niteration 1000: memory use: 1992.6328125MB\r\niteration 1100: memory use: 1992.72265625MB\r\niteration 1200: memory use: 1992.72265625MB\r\niteration 1300: memory use: 1992.81640625MB\r\niteration 1400: memory use: 1992.81640625MB\r\niteration 1500: memory use: 1992.890625MB\r\niteration 1600: memory use: 1992.890625MB\r\niteration 1700: memory use: 1992.9453125MB\r\niteration 1800: memory use: 1992.9453125MB\r\nWith cudnn disabled\r\niteration 0: memory use: 1997.7734375MB\r\niteration 100: memory use: 2000.4375MB\r\niteration 200: memory use: 2000.4375MB\r\niteration 300: memory use: 2000.4375MB\r\niteration 400: memory use: 2000.4375MB\r\niteration 500: memory use: 2000.4375MB\r\niteration 600: memory use: 2000.44140625MB\r\niteration 700: memory use: 2000.44140625MB\r\niteration 800: memory use: 2000.4453125MB\r\niteration 900: memory use: 2000.453125MB\r\niteration 1000: memory use: 2000.453125MB\r\niteration 1100: memory use: 2000.46484375MB\r\niteration 1200: memory use: 2000.46875MB\r\niteration 1300: memory use: 2000.4765625MB\r\niteration 1400: memory use: 2000.484375MB\r\niteration 1500: memory use: 2000.48828125MB\r\niteration 1600: memory use: 2000.5078125MB\r\niteration 1700: memory use: 2000.5078125MB\r\niteration 1800: memory use: 2000.5078125MB\r\n\r\n```\r\n\r\n\r\n\r\n### Driver=375.26\r\n\r\n```\r\ntorch-version:  0.4.0\r\ncudnn version:  7102\r\niteration 0: memory use: 2059.859375MB\r\niteration 100: memory use: 2060.09375MB\r\niteration 200: memory use: 2060.1328125MB\r\niteration 300: memory use: 2060.16796875MB\r\niteration 400: memory use: 2060.203125MB\r\niteration 500: memory use: 2060.2421875MB\r\niteration 600: memory use: 2060.2734375MB\r\niteration 700: memory use: 2060.3125MB\r\niteration 800: memory use: 2060.34765625MB\r\niteration 900: memory use: 2060.37890625MB\r\niteration 1000: memory use: 2060.4140625MB\r\niteration 1100: memory use: 2060.453125MB\r\niteration 1200: memory use: 2060.484375MB\r\niteration 1300: memory use: 2060.640625MB\r\niteration 1400: memory use: 2060.671875MB\r\niteration 1500: memory use: 2060.70703125MB\r\niteration 1600: memory use: 2060.7421875MB\r\niteration 1700: memory use: 2060.7734375MB\r\niteration 1800: memory use: 2060.80859375MB\r\niteration 1900: memory use: 2060.83984375MB\r\niteration 2000: memory use: 2060.875MB\r\nWith cudnn disabled\r\niteration 0: memory use: 2064.609375MB\r\niteration 100: memory use: 2064.98046875MB\r\niteration 200: memory use: 2064.98046875MB\r\niteration 300: memory use: 2064.98046875MB\r\niteration 400: memory use: 2064.98046875MB\r\niteration 500: memory use: 2065.16796875MB\r\niteration 600: memory use: 2065.16796875MB\r\niteration 700: memory use: 2065.16796875MB\r\niteration 800: memory use: 2065.16796875MB\r\niteration 900: memory use: 2066.71875MB\r\niteration 1000: memory use: 2068.16015625MB\r\niteration 1100: memory use: 2068.1640625MB\r\niteration 1200: memory use: 2068.16796875MB\r\niteration 1300: memory use: 2068.18359375MB\r\niteration 1400: memory use: 2068.18359375MB\r\niteration 1500: memory use: 2068.18359375MB\r\niteration 1600: memory use: 2068.1953125MB\r\niteration 1700: memory use: 2068.19921875MB\r\niteration 1800: memory use: 2068.19921875MB\r\niteration 1900: memory use: 2068.19921875MB\r\niteration 2000: memory use: 2068.20703125MB\r\n\r\n\r\ntorch-version:  0.5.0a0+4b61760\r\ncudnn version:  7005\r\niteration 0: memory use: 1704.07421875MB\r\niteration 100: memory use: 1704.07421875MB\r\niteration 200: memory use: 1704.07421875MB\r\niteration 300: memory use: 1704.07421875MB\r\niteration 400: memory use: 1704.07421875MB\r\niteration 500: memory use: 1704.5078125MB\r\niteration 600: memory use: 1704.5078125MB\r\niteration 700: memory use: 1704.5078125MB\r\niteration 800: memory use: 1704.5078125MB\r\niteration 900: memory use: 1704.5078125MB\r\niteration 1000: memory use: 1704.5078125MB\r\niteration 1100: memory use: 1704.5078125MB\r\niteration 1200: memory use: 1704.765625MB\r\niteration 1300: memory use: 1704.765625MB\r\niteration 1400: memory use: 1704.765625MB\r\niteration 1500: memory use: 1704.765625MB\r\niteration 1600: memory use: 1704.765625MB\r\niteration 1700: memory use: 1704.765625MB\r\niteration 1800: memory use: 1704.765625MB\r\niteration 1900: memory use: 1704.765625MB\r\niteration 2000: memory use: 1705.0234375MB\r\nWith cudnn disabled\r\niteration 0: memory use: 1881.109375MB\r\niteration 100: memory use: 1881.3671875MB\r\niteration 200: memory use: 1881.3671875MB\r\niteration 300: memory use: 1881.3671875MB\r\niteration 400: memory use: 1881.3671875MB\r\niteration 500: memory use: 1881.3671875MB\r\niteration 600: memory use: 1881.3671875MB\r\niteration 700: memory use: 1881.3671875MB\r\niteration 800: memory use: 1881.3671875MB\r\niteration 900: memory use: 1881.3671875MB\r\niteration 1000: memory use: 1881.3671875MB\r\niteration 1100: memory use: 1881.3671875MB\r\niteration 1200: memory use: 1881.3671875MB\r\niteration 1300: memory use: 1881.3671875MB\r\niteration 1400: memory use: 1881.3671875MB\r\niteration 1500: memory use: 1881.3671875MB\r\niteration 1600: memory use: 1881.3671875MB\r\niteration 1700: memory use: 1881.3671875MB\r\niteration 1800: memory use: 1881.3671875MB\r\niteration 1900: memory use: 1881.3671875MB\r\niteration 2000: memory use: 1881.3671875MB\r\n```""]","['\r\n# -*- coding: utf-8 -*-\r\nimport torch\r\nimport torch.autograd as autograd\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nimport os\r\ntorch.manual_seed(1)\r\n\r\ndef prepare_sequence(seq, to_ix):\r\n    idxs = [to_ix[w] for w in seq]\r\n    tensor = torch.LongTensor(idxs)\r\n    return autograd.Variable(tensor).cuda()\r\n\r\nclass BiLSTM(nn.Module):\r\n    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\r\n        super(BiLSTM, self).__init__()\r\n        self.embedding_dim = embedding_dim\r\n        self.hidden_dim = hidden_dim\r\n        self.vocab_size = vocab_size\r\n        self.tag_to_ix = tag_to_ix\r\n        self.tagset_size = len(tag_to_ix)\r\n        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\r\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\r\n                            num_layers=1, bidirectional=True)\r\n        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\r\n        self.gpu = True \r\n        if self.gpu:\r\n            self.word_embeds = self.word_embeds.cuda()\r\n            self.lstm = self.lstm.cuda()\r\n            self.hidden2tag = self.hidden2tag.cuda() \r\n        self.hidden = self.init_hidden()\r\n\r\n    def init_hidden(self):\r\n        (a,b) = (autograd.Variable(torch.randn(2, 1, self.hidden_dim // 2)),\r\n                autograd.Variable(torch.randn(2, 1, self.hidden_dim // 2)))\r\n        if self.gpu:\r\n            a = a.cuda()\r\n            b = b.cuda()\r\n        return (a,b)\r\n\r\n    def _get_lstm_features(self, sentence):\r\n        self.hidden = self.init_hidden()\r\n        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\r\n        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\r\n        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\r\n        lstm_feats = self.hidden2tag(lstm_out)\r\n        return lstm_feats\r\n\r\n    def neg_log_likelihood(self, sentence, tags):\r\n        loss_function = nn.NLLLoss()\r\n        feats = self._get_lstm_features(sentence)\r\n        score = F.log_softmax(feats)\r\n        loss = loss_function(score, tags)\r\n        return loss\r\n\r\n    def forward(self, sentence):  \r\n        lstm_feats = self._get_lstm_features(sentence)\r\n        score = F.log_softmax(lstm_feats)\r\n        if self.gpu:\r\n            score = score.cpu()\r\n        tag_seq = score.data.numpy().argmax(axis=1).tolist()\r\n        return score, tag_seq\r\n#####################################################################\r\n# Run training\r\n## disable cudnn fixed the mem leak problem\r\n#torch.backends.cudnn.enabled = False\r\nEMBEDDING_DIM = 5\r\nHIDDEN_DIM = 4\r\n# Make up some training data\r\ntraining_data = [(\r\n    ""the wall street journal reported today that apple corporation made money"".split(),\r\n    ""B I I I O O O B I O O"".split()\r\n)] * 500 + [(\r\n    ""georgia tech is a university in georgia"".split(),\r\n    ""B I O O O O B"".split()\r\n)]*500\r\n\r\nword_to_ix = {}\r\nfor sentence, tags in training_data:\r\n    for word in sentence:\r\n        if word not in word_to_ix:\r\n            word_to_ix[word] = len(word_to_ix)\r\ntag_to_ix = {""B"": 0, ""I"": 1, ""O"": 2}\r\nmodel = BiLSTM(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\r\noptimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\r\n## get process id\r\npid = os.getpid()\r\nprev_mem = 0\r\n# Make sure prepare_sequence from earlier in the LSTM section is loaded\r\nfor epoch in range(300):\r\n    print ""Epoch: %s""%epoch\r\n    inst_id = 0\r\n    for sentence, tags in training_data:\r\n        inst_id += 1\r\n        model.zero_grad()\r\n        sentence_in = prepare_sequence(sentence, word_to_ix)\r\n        targets = prepare_sequence(tags, tag_to_ix)\r\n        neg_log_likelihood = model.neg_log_likelihood(sentence_in, targets)\r\n        neg_log_likelihood.backward()\r\n        optimizer.step()\r\n        if inst_id%100 == 0:\r\n            cur_mem = (int(open(\'/proc/%s/statm\'%pid, \'r\').read().split()[1])+0.0)/256\r\n            add_mem = cur_mem - prev_mem\r\n            prev_mem = cur_mem\r\n            print ""     train instances: %s, added mem: %sM""%(inst_id, add_mem)\r\n\r\n# Check predictions after training\r\nprecheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\r\nprint(model(precheck_sent))\r\n']",['torch.backends.cudnn.enabled = False'],1,0
12,pytorch,20053,closed,RFC: accscalar_t for float on CPU,"## Issue
Currently  for  is  on the CPU:

https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/AccumulateType.h#L34

I suggest to have a small discussion whether it'd be better to switch to float.

## Motivation

This is prompted by @wanchaol asking about a [](https://gist.github.com/wanchaol/aceb0a1e8d3a93c8853689730b0f709f) error, but I think there are three possible reasons to consider changing this:
- Consistency with GPU,
- support platforms on which float is faster (e.g. arm32)
- get rid of UB.

I seem to recall @apaszke preferring the current behaviour a year ago or so back (in the context of #6855, which always dispatched a double auxilliary function on CPU or so).

## Pitch

Switch to  generally.

## Alternatives

Switch to  only for specific platforms (e.g. arm32),  somehow get rid of the UB warning.

## Additional context

We (e.g. @ljk53 and me) might be interested in changing this for Android specifically if we don't generally.
",feature module: android module: cpu triaged,"[""this is something we've narrowed on as the right behavior since the torch7 days.\r\nHowever, considering that no-one ever complained that their GPU outputs didn't have enough precision, in these many years.\r\nIt's worth considering, but I would be worried about BC-breaking.\r\n\r\nJust switching it on android is dangerous because a model that you saved on PC will suddenly work differently on Android -- very unexpected."", ""Clearly this won't happen.""]",[],"['accscalar_t', 'float', 'double', 'UndefinedBehaviorSanitizer: float-cast-overflow', 'float', 'float']",1,0
13,pytorch,28761,open,torch.tensor() is very slow when it is passed an h5py Dataset.,"## üêõ Bug

 is very slow when it is passed an h5py Dataset.

## To Reproduce

Create a new HDF5 file with a 1000x1000 float32 dataset:



Then load it back into a Tensor:



It's very slow. However, if the dataset is converted into a NumPy array first, it performs much faster:



The resulting tensors are equal. Perhaps the manual NumPy array conversion avoids an expensive conversion to a Python nested list of floats.


cc @VitalyFedyunin @ngimel",module: performance module: tensor creation triaged,"['Does `torch.as_tensor()` instead of `torch.tensor()` perform as badly?', '> Does torch.as_tensor() instead of torch.tensor() perform as badly?\r\n\r\nYes.', 'Any reason/place anyone thinks we could look at to sort this out?', ""Just to make sure that there is no IO overhead in the first call, the issue holds both if we invert the calls and if we set the same number of loops:\r\n```python\r\nimport h5py\r\nimport numpy as np\r\nimport torch\r\n\r\ntestfile = h5py.File('testfile.h5', 'w')\r\ntestfile['data'] = torch.eye(1000)\r\n```\r\n```\r\n>>> %timeit -n10 torch.as_tensor(np.array(testfile['data']))\r\n1.63 ms ¬± 91 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\r\n```\r\n```\r\n>>> %timeit -n10 torch.as_tensor(testfile['data'])\r\n267 ms ¬± 1.59 ms per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\r\n```"", '`h5py` actually makes very different calls in the two cases (showing just the last 10 calls):\r\n```python\r\nimport cProfile\r\npr = cProfile.Profile()\r\npr.enable()\r\ntorch.as_tensor(testfile[\'data\'])\r\npr.disable()\r\npr.print_stats(sort=""tottime"")\r\n```\r\n```\r\n         189170 function calls (187169 primitive calls) in 0.320 seconds\r\n\r\n   Ordered by: internal time\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n        1    0.080    0.080    0.319    0.319 {built-in method as_tensor}\r\n     2001    0.066    0.000    0.234    0.000 dataset.py:476(__getitem__)\r\n    10008    0.056    0.000    0.057    0.000 dataset.py:282(shape)\r\n     2001    0.017    0.000    0.017    0.000 {method \'reduce\' of \'numpy.ufunc\' objects}\r\n     2001    0.011    0.000    0.031    0.000 selections.py:250(__getitem__)\r\n     2001    0.009    0.000    0.009    0.000 selections.py:147(__init__)\r\n     2001    0.009    0.000    0.009    0.000 {built-in method h5py.h5t.py_create}\r\n     2001    0.008    0.000    0.008    0.000 base.py:85(is_empty_dataspace)\r\n     2001    0.007    0.000    0.019    0.000 selections.py:444(_handle_simple)\r\n     2001    0.007    0.000    0.053    0.000 selections.py:27(select)\r\n```\r\n\r\nAnd when we first create the numpy array:\r\n```python\r\nimport cProfile\r\npr = cProfile.Profile()\r\npr.enable()\r\ntorch.as_tensor(np.array(testfile[\'data\']))\r\npr.disable()\r\npr.print_stats(sort=""tottime"")\r\n```\r\n```\r\n         135 function calls (133 primitive calls) in 0.002 seconds\r\n\r\n   Ordered by: internal time\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n        1    0.001    0.001    0.001    0.001 dataset.py:710(read_direct)\r\n        1    0.000    0.000    0.001    0.001 {built-in method numpy.array}\r\n        1    0.000    0.000    0.000    0.000 group.py:255(__getitem__)\r\n        1    0.000    0.000    0.001    0.001 <ipython-input-30-abba85f68ac2>:4(<module>)\r\n        2    0.000    0.000    0.000    0.000 {built-in method builtins.compile}\r\n        1    0.000    0.000    0.000    0.000 {built-in method as_tensor}\r\n        1    0.000    0.000    0.000    0.000 dataset.py:395(__init__)\r\n        2    0.000    0.000    0.000    0.000 selections.py:272(broadcast)\r\n        2    0.000    0.000    0.000    0.000 {method \'reduce\' of \'numpy.ufunc\' objects}\r\n        3    0.000    0.000    0.000    0.000 dataset.py:282(shape)\r\n```', ""Any updates on this issue? I've spent hours trying to debug this problem on my Dataloader until I stumbled upon this issue thread..."", 'I noticed a similar problem while transforming a list stored in a LMDB to tensor.\r\n\r\n`my_list` is a list of 15 numpy arrays of shape (324, 576, 3).\r\n\r\n```\r\ntorch_tensor = torch.tensor(my_list) # slow\r\ntorch_tensor = torch.tensor(np.array(data)) # fast\r\n# torch_tensor.shape -> torch.Size([15, 324, 576, 3])\r\n```\r\n ']","[""python\r\nimport h5py\r\nimport numpy as np\r\nimport torch\r\n\r\ntestfile = h5py.File('testfile.h5', 'w')\r\ntestfile['data'] = torch.eye(1000)\r\n"", ""\r\n>>> %timeit torch.tensor(testfile['data'])\r\n421 ms ¬± 28.2 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\r\n"", ""\r\n>>> %timeit torch.tensor(np.array(testfile['data']))\r\n2.84 ms ¬± 162 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\r\n""]",['torch.tensor()'],1,0
14,pytorch,23642,closed,Performance Regression of Dataloader,"## üêõ Bug

Latest change to Dataloader (#19228) leads to severe performance regression for large scale training up to 30%. We finally root the cause to theses change: https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py#L889-L891. It causes the exit of each epoch has additional 5 seconds.

## To Reproduce

Steps to reproduce the behavior:

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->



## Expected behavior

The exit is basically free in pytorch 1.1, but it takes 5s in pytorch 1.2.


<!-- A clear and concise description of what you expected to happen. -->

## Environment



## Additional context

<!-- Add any other context about the problem here. -->

The suggest fix is to recover previous lines around https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py#L889-L891. For example, following code will fix the problem:


",module: dataloader module: performance triaged,['cc: @ssnl '],"['python\r\n# regression.py\r\nimport torch\r\nimport time\r\n\r\nfrom torch.utils.data import TensorDataset, DataLoader\r\n\r\ndataset = TensorDataset(torch.randn(10240, 2))\r\nloader = DataLoader(dataset, batch_size=128, num_workers=2, pin_memory=True, drop_last=False)\r\n\r\nfor epoch in range(10):\r\n    for idx, data in enumerate(loader):\r\n        data = data[0].cuda()\r\n        if idx == 10240/128-1:\r\n            ts = time.time()\r\n    print(""Exit epoch {} elapsed {:.2f}s"".format(epoch, time.time()-ts))\r\n', 'console\r\n# 1.2.0a0\r\n$ python regression.py\r\nExit epoch 0 elapsed 5.01s       \r\nExit epoch 1 elapsed 5.05s       \r\nExit epoch 2 elapsed 5.05s       \r\nExit epoch 3 elapsed 5.05s       \r\nExit epoch 4 elapsed 5.05s       \r\nExit epoch 5 elapsed 5.05s       \r\nExit epoch 6 elapsed 5.05s       \r\nExit epoch 7 elapsed 5.05s       \r\nExit epoch 8 elapsed 5.05s       \r\nExit epoch 9 elapsed 5.04s\r\n\r\n# 1.1.0a0\r\n$ python regression.py\r\nExit epoch 0 elapsed 0.01s       \r\nExit epoch 1 elapsed 0.02s       \r\nExit epoch 2 elapsed 0.03s       \r\nExit epoch 3 elapsed 0.02s       \r\nExit epoch 4 elapsed 0.03s       \r\nExit epoch 5 elapsed 0.03s       \r\nExit epoch 6 elapsed 0.03s       \r\nExit epoch 7 elapsed 0.03s       \r\nExit epoch 8 elapsed 0.02s       \r\nExit epoch 9 elapsed 0.02s  \r\n', '\r\nPyTorch version: 1.2.0a0+5b0484d                                          \r\nIs debug build: No                                                        \r\nCUDA used to build PyTorch: 10.1.233                                      \r\n                                                                          \r\nOS: Ubuntu 18.04.2 LTS                                                    \r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0                        \r\nCMake version: version 3.14.0                                             \r\n                                                                          \r\nPython version: 3.6                                                       \r\nIs CUDA available: Yes                                                    \r\nCUDA runtime version: 10.1.241                                            \r\nGPU models and configuration:                                             \r\nGPU 0: Tesla V100-SXM2-16GB                                               \r\nGPU 1: Tesla V100-SXM2-16GB                                               \r\nGPU 2: Tesla V100-SXM2-16GB                                               \r\nGPU 3: Tesla V100-SXM2-16GB                                               \r\nGPU 4: Tesla V100-SXM2-16GB                                               \r\nGPU 5: Tesla V100-SXM2-16GB                                               \r\nGPU 6: Tesla V100-SXM2-16GB                                               \r\nGPU 7: Tesla V100-SXM2-16GB                                               \r\n                                                                          \r\nNvidia driver version: 418.40.04                                          \r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.3                \r\n                                                                          \r\nVersions of relevant libraries:                                           \r\n[pip] msgpack-numpy==0.4.3.2                                              \r\n[pip] numpy==1.16.4                                                       \r\n[pip] torch==1.2.0a0+5b0484d                                              \r\n[pip] torchtext==0.4.0                                                    \r\n[pip] torchvision==0.3.0a0                                                \r\n[conda] magma-cuda100             2.1.0                         5    local\r\n[conda] mkl                       2019.1                      144         \r\n[conda] mkl-include               2019.1                      144         \r\n[conda] nomkl                     3.0                           0         \r\n[conda] torch                     1.2.0a0+5b0484d          pypi_0    pypi \r\n[conda] torchtext                 0.4.0                    pypi_0    pypi \r\n[conda] torchvision               0.3.0a0                  pypi_0    pypi \r\n', 'python\r\nself.worker_result_queue.cancel_join_thread()\r\nself.worker_result_queue.put((0, None))      \r\nself.pin_memory_thread.join()                \r\nself.worker_result_queue.close()   \r\n']",[],1,1
15,pytorch,25690,closed,Dropout behaves differently on different devices,"I have two machines. One has a P100 and another has 4 K80s.
I found the following code output differently on the two machines.
It means seeding doesn't make the output of dropout reproducible on different devices. 
Is it by design?
If we can not make the output reproducible on different devices, it may be a trouble of comparing different models.

The output of P100 machine is always

The output of K80 machine is always


",,"['we dont guarantee that the RNG outputs the same sequence across different GPU models.\r\n\r\nThe guarantees of RNG are in https://pytorch.org/docs/stable/notes/randomness.html?highlight=deterministic', 'Is it possible to make the RNG output same sequence across different GPU. The performance (metrics) of my generation model may change more than 20% on different GPUs.', '> Is it possible to make the RNG output same sequence across different GPU. \r\n\r\nNo it is not in our plans. We take RNG from CUDA API, and CUDA has made the decision of not giving same random sequences across GPUs.']","[""\r\n#!/bin/bash python\r\nimport torch\r\nimport numpy as np \r\nimport random \r\nseed = 0\r\ntorch.manual_seed(seed)\r\ntorch.cuda.manual_seed(seed)\r\ntorch.cuda.manual_seed_all(seed)  \r\nnp.random.seed(seed)  \r\nrandom.seed(seed)  \r\ntorch.manual_seed(seed)\r\ntorch.backends.cudnn.benchmark = False\r\ntorch.backends.cudnn.deterministic = True\r\na = torch.ones(1000,1000).to('cuda:0')\r\ndropout = torch.nn.Dropout(0.5)\r\nb = dropout(a)\r\nprint(torch.sum(torch.abs(b)))\r\n""]","[""\r\ntensor(999826., device='cuda:0')\r\n"", ""\r\ntensor(999100., device='cuda:0')\r\n""]",1,0
16,pytorch,10851,closed,`profiler.table()` too slow,"## Issue description

When processing large amount of output lines(about 100000 lines), profiler.table() can spend several minutes. I think it's because in , new lines are added to the end of a string, and this can result in a copy operation of the whole string. The best practise here may be using .",medium priority (this tag is deprecated),[],[],"['build_table()', 'StringIO']",1,1
17,pytorch,22127,closed,Cpu memory leak when only doing model forwarding in a loop,"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce
Cpu memory leak when only doing model forwarding in a loop
<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

define the model as simple as :

and write the test program as:


## Expected behavior
look into the system monitor, the related python process's memory should be stable. but in fact the memory increase over time.
<!-- A clear and concise description of what you expected to happen. -->

## Environment

 - PyTorch Version ( 1.1):
 - OS (Linux & OSX):
 - How you installed PyTorch ():
 - Python version: 3.6 & 3.7
 - Any other relevant information: when using gc.collect(), the increasing becomes slow, however it won't help since i want to use LSTM layers(quickly out of memory)

## Additional context

<!-- Add any other context about the problem here. -->
I am writing a reinforcement learning program, during every episode(in loops) I will do model forwardings. This memory leak problem is vital. ",module: memory usage triaged,"['This might be related to https://github.com/pytorch/vision/issues/984#issuecomment-498603088, and is due to the implementation of malloc.\r\n\r\nCan you try using `jemalloc` instead?', '@jsyx1994 Another way to check if it is a real memory leak is to see if your program ever OOMs.', ""@jsyx1994 Could you try out @fmassa 's and @zou3519 's suggestions, and let us know the result of your experiments?"", 'I test ```LD_PRELOAD=`jemalloc-config --libdir`/libjemalloc.so.`jemalloc-config --revision` python3 ~/PycharmProjects/Coola_RTS/ServerAI.py``` on ubuntu. It seems no use.', 'I have the same problem here. I tried to solve it using `jemalloc` with no success.\r\n\r\nI would consider that a memory leak on model forwarding should be a high priority. However, this issue seems to be in standby for more than half a year. Any advance so far on this topic?', 'Does the program actually OOM?  Or you only see memory increase?\r\n\r\nIn the last investigation, there was no memory leak in the forward. Just that `malloc` behavior make it buffer more and more memory (but you will never OOM as you can reuse it in this process).', ""I just tried to reproduce the error with the same code that @jsyx1994 posted and I don't see that memory increases in 1000 steps. I have another code similar to this one and increases the memory a lot per step but guess this may be caused by another reason. I'm going to create a different issue for it then."", 'Tentatively closing this as we cannot reproduce. Please re-open if you still see actual OOM errors.']","['\r\nclass TestLeakModel(nn.Module):\r\n    def __init__(self):\r\n        super(TestLeakModel,self).__init__()\r\n        self.conv1 = nn.Conv2d(in_channels=18, out_channels=16, kernel_size=3, padding=1)   # WEIGHTs ARE INITED\r\n        self.conv2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3,\r\n                               padding=1)  # WEIGHTs ARE INITED\r\n        self.conv3 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\r\n        self.conv4 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1)  # WEIGHTs ARE INITED\r\n        self.conv5 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3,\r\n                               padding=1)  # WEIGHTs ARE INITED\r\n        self.fc1 = nn.Linear(in_features=64*16, out_features=128)\r\n        self.fc2 = nn.Linear(in_features=128, out_features=128)\r\n        self.fc3 = nn.Linear(in_features=128, out_features=128)\r\n        self.fc4 = nn.Linear(in_features=128, out_features=128)\r\n        self.fc5 = nn.Linear(in_features=128, out_features=128)\r\n        self.fc6 = nn.Linear(in_features=128, out_features=7)\r\n\r\n    def forward(self, input):\r\n        x = self.conv1(input)\r\n        x = self.conv2(x)\r\n        x = self.conv3(x)\r\n        x = self.conv4(x)\r\n        x = self.conv5(x)\r\n\r\n        x = Flatten()(x)\r\n        x = self.fc1(x)\r\n        x = self.fc2(x)\r\n        x = self.fc3(x)\r\n        x = self.fc4(x)\r\n        x = self.fc5(x)\r\n        x = self.fc6(x)\r\n\r\n        x = F.softmax(x)\r\n        return x\r\n', '\r\ndef test():\r\n    x = torch.randn((1, 18, 8, 8))\r\n    import gc\r\n    for i in range(10000000):\r\n        # gc.collect()\r\n        with torch.no_grad():\r\n            testmodel.forward(x)\r\n\r\n']",['pip'],1,0
18,pytorch,29809,closed,Memory leak with Conv1d on CPU,"## üêõ Bug

When training (on CPU) a simple one-layer CNN (with variable-width batches, as it is standard in text classification), the memory usage increases significantly at each  call, and quickly causes an out of memory.

## To Reproduce



When running , I get this OOM error after 30 iterations:


The [memory profiler output](https://github.com/pytorch/pytorch/files/3846948/memory_profiler_output.txt) clearly shows a steady memory increase (and never decrease) at each  call, of **190Mb per call** on average.


## Expected behavior

I would expect the memory usage to increase slightly at the first step, but then stabilize - not increase linearly.

## Environment

PyTorch version: 1.3.1+cpu
Is debug build: No
CUDA used to build PyTorch: None

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: Could not collect

Python version: 3.6
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip3] numpy==1.17.4
[pip3] torch==1.3.1+cpu
[conda] Could not collect


## Additional context

Here all the things I tested, following advice in the multiples related issues I found (https://github.com/pytorch/vision/issues/984, https://github.com/pytorch/pytorch/issues/5285 and others):

1. Check that there are no references to a model output kept accross iterations (e.g. loss)

2. Remove anything possible to narrow down the issue (data loader, tqdm, backward, optimization, ...). This is why I only kept the forward pass with dummy data.

3. Use  :
 
Same exact issue happens.

4. With random but fixed-size tensors (replacing  with ), the issue disappears.
However, I am doing CNN on text, so unfortunately, I need batches to have variable width...

5. It I remove the  line, the issue disappears as well, so it is not a problem with the data generation itelf.

6. I tried the following Pytorch versions and had the exact same issue with all of them:
*  from pip ()
*  from pip ()
*  from conda ()

cc @ezyang @gchanan @zou3519 @jerryzh168",high priority triage review,"[""Hmmm this sounds like a weird issue (with the additional context 4 & 5). \r\nI'll mark this high priority to reproduce the reported behavior as a first step at least. "", 'Might be #27971 or #25267.', 'Wow, indeed, `LRU_CACHE_CAPACITY=1` (from https://github.com/pytorch/pytorch/issues/27971) solves my issue!\r\n\r\nThanks a lot for pointing out, @ezyang!', ""Thanks, I'm going to close this as a dupe, but this is defo high priority.""]","['\r\nimport random\r\nimport torch\r\nfrom torch import nn\r\nfrom memory_profiler import profile\r\n\r\nclass DummyModel(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.embed = nn.Embedding(500000, 100)\r\n        self.conv = nn.Conv1d(100, 200, 3)\r\n\r\n    def forward(self, x):\r\n        x = self.embed(x)\r\n        x = torch.transpose(x, 1, 2)\r\n        x = self.conv(x)\r\n        return torch.mean(x)\r\n\r\n\r\nmodel = DummyModel()\r\n\r\n@profile\r\ndef run():\r\n    x = torch.randint(0, 500000, (300, random.randint(100, 3000)))\r\n    model(x)\r\n\r\nfor i in range(20):\r\n    run()\r\n', ""\r\nRuntimeError: [enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 633120000 bytes. Error code 12 (Cannot allocate memory)\r\n""]","['forward', 'python example.py', 'model(x)', 'jemalloc', 'LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libjemalloc.so.1 python example.py', 'x = torch.randint(0, 500000, (300, random.randint(100, 3000)))', 'x = torch.randint(0, 500000, (300, 3000))', 'model(x)', '1.3.1+cpu', 'pip install torch==1.3.1+cpu -f https://download.pytorch.org/whl/torch_stable.html', '1.3.1', 'pip install torch', '1.3.1 cpuonly', 'conda install pytorch cpuonly -c pytorch']",1,0
19,pytorch,8818,open,MPI causing job to hang --- unresponsive to external (termination) signals,"## Issue description
When running  with the MPI backend in a cluster, the job fails to exit and hangs unresponsively --- essentially any node in the cluster that the runs the script becomes unresponsive when you try to terminate the job. This varies from run to run, but it happens often enough that it is problematic.

Here is a minimal reproduction script. High level steps:

1.  initialize multiple DataLoader workers
2. communicate model parameters
3. update model parameters
4. run forward-backward pass

## Code example

For a single machine with  GPUs, run



issues.py:



## System Info

PyTorch (built from source) version: 0.5.0a0+f8c18e0
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 16.04.4 LTS
GCC version: (Ubuntu 5.5.0-12ubuntu1~16.04) 5.5.0 20171010
CMake version: version 3.11.1

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.0.176
GPU models and configuration:
GPU[0-8]: V100 (NVIDIA voltas)

Nvidia driver version: 384.81
cuDNN version: cudnn/v7.0-cuda.9.0

Versions of relevant libraries:

openmpi/3.0.0/gcc.5.4.0
NCCL/2.2.12-1-cuda.9.0

[pip] numpy (1.14.3)
[pip] torch (0.5.0a0+f8c18e0)
[pip] torchvision (0.2.1)
[conda] magma-cuda90   2.3.0       1         pytorch
[conda] torch                     0.5.0   <pip>
[conda] torchvision            0.2.1   <pip>

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd",oncall: distributed,"['Hi @ngimel , could you take a look at this when you have time? Basically the job usually fails with CUDNN_INTERNAL_ERROR, and fail to exit gracefully. Thanks! The node has P100 instead of V100 I believe. ']","['\r\nimport os\r\nimport signal\r\n\r\nimport torch\r\nimport torch.backends.cudnn as cudnn\r\nimport torch.distributed as dist\r\nimport torch.utils.data\r\nimport torch.utils.data.distributed\r\nimport torch.multiprocessing as mp\r\nimport torchvision.datasets as datasets\r\nimport torchvision.models as models\r\nimport torchvision.transforms as transforms\r\n\r\n# path to dataset (REPLACE WITH YOUR OWN LOCAL PATH TO DATASET)\r\nTRAIN_DIRECTORY = \'/datasets/imagenet_full_size/train\'\r\n\r\n\r\ndef main():\r\n\r\n    signal.signal(signal.SIGTERM, SIGTERMHandler)\r\n\r\n    # initialize torch distributed mpi backend\r\n    os.environ[\'MASTER_ADDR\'] = \'localhost\'\r\n    os.environ[\'MASTER_PORT\'] = \'40101\'\r\n    dist.init_process_group(backend=\'mpi\')\r\n\r\n    # run each task on a single GPU\r\n    torch.cuda.set_device(dist.get_rank())\r\n\r\n    # seed for reproducibility\r\n    torch.manual_seed(37)\r\n    torch.cuda.manual_seed(37)\r\n    torch.backends.cudnn.deterministic = True\r\n\r\n    model = models.resnet50()\r\n    model.cuda()\r\n    model.train()\r\n    criterion = torch.nn.CrossEntropyLoss().cuda()\r\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9,\r\n                                weight_decay=0.0001, nesterov=True)\r\n\r\n    cudnn.benchmark = True\r\n\r\n    # initialize a dataloader with 4 worker processes\r\n    loader, sampler = make_dataloader(TRAIN_DIRECTORY)\r\n\r\n    for i, (input, target) in enumerate(loader):\r\n\r\n        print(\'itr {}\'.format(i))\r\n\r\n        input = input.cuda(non_blocking=True)\r\n        target = target.cuda(non_blocking=True)\r\n\r\n        # communicate model parameters\r\n        comm_buffer = _flatten_tensors(list(model.parameters())).detach_()\r\n        send_recieve(comm_buffer)\r\n        comm_buffer = _unflatten_tensors(comm_buffer, list(model.parameters()))\r\n        with torch.no_grad():\r\n            for p, e in zip(model.parameters(), comm_buffer):\r\n                p.data.copy_(e)\r\n\r\n        # forward/backward pass\r\n        output = model(input)\r\n        loss = criterion(output, target)\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\r\n\r\n        prec1, prec5 = accuracy(output, target, topk=(1, 5))\r\n        print(\'* @Prec1 {prec1:.3f}\\t@Prec5 {prec5:.3f}\'\r\n              .format(prec1=prec1.item(), prec5=prec5.item()))\r\n\r\n\r\ndef send_recieve(buffer):\r\n\r\n    buffer.mul_(0.5)\r\n\r\n    # ring communication\r\n    destination = (dist.get_rank() + 1) % dist.get_world_size()\r\n    source = dist.get_rank() - 1 if dist.get_rank() != 0 else dist.get_world_size() - 1\r\n\r\n    # non-blocking send message\r\n    send_buffer = buffer.clone()\r\n    req = dist.isend(tensor=send_buffer, dst=destination)\r\n    out_msg = (req, send_buffer)  # keep in scope\r\n\r\n    # blocking receive\r\n    receive = buffer.clone()\r\n    dist.recv(receive, src=source)\r\n\r\n    # update buffer\r\n    buffer.add_(receive)\r\n\r\n    # wait for neighbours to receive message\r\n    out_msg[0].wait()\r\n\r\n\r\ndef make_dataloader(train_directory):\r\n"""""" Create distributed dataloader """"""\r\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\r\n                                     std=[0.229, 0.224, 0.225])\r\n\r\n    train_dataset = datasets.ImageFolder(train_directory, transforms.Compose([\r\n                        transforms.RandomResizedCrop(224),\r\n                        transforms.RandomHorizontalFlip(),\r\n                        transforms.ToTensor(),\r\n                        normalize]))\r\n\r\n    # sampler produces indices used to assign data samples to each agent\r\n    train_sampler = torch.utils.data.distributed.DistributedSampler(\r\n                        dataset=train_dataset,\r\n                        num_replicas=dist.get_world_size(),\r\n                        rank=dist.get_rank())\r\n\r\n    train_loader = torch.utils.data.DataLoader(\r\n        train_dataset, batch_size=32,\r\n        shuffle=False,\r\n        num_workers=4,\r\n        pin_memory=True, sampler=train_sampler)\r\n\r\n    return train_loader, train_sampler\r\n\r\n\r\ndef accuracy(output, target, topk=(1,)):\r\n    """""" Computes the precision@k for the specified values of k """"""\r\n    with torch.no_grad():\r\n        maxk = max(topk)\r\n        batch_size = target.size(0)\r\n\r\n        _, pred = output.topk(maxk, 1, True, True)\r\n        pred = pred.t()\r\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\r\n\r\n        res = []\r\n        for k in topk:\r\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\r\n            res.append(correct_k.mul_(100.0 / batch_size))\r\n        return res\r\n\r\n\r\ndef _flatten_tensors(tensors):\r\n    if len(tensors) == 1:\r\n        return tensors[0].view(-1)\r\n    flat = torch.cat([t.view(-1) for t in tensors], dim=0)\r\n    return flat\r\n\r\n\r\ndef _unflatten_tensors(flat, tensors):\r\n    outputs = []\r\n    offset = 0\r\n    for tensor in tensors:\r\n        numel = tensor.numel()\r\n        outputs.append(flat.narrow(0, offset, numel).view_as(tensor))\r\n        offset += numel\r\n    return tuple(outputs)\r\n\r\n\r\ndef SIGTERMHandler(signum, frame):\r\n    """"""\r\n    Ignore SIGTERM preemption signal (doesn\'t stop preemption);\r\n    instead SIGUSR1 will be moved up and handled accordingly\r\n    """"""\r\n    print(\'Received SIGTERM\')\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    mp.set_start_method(\'forkserver\')\r\n    main()\r\n\r\n']","['torch.distributed', '$world_size', 'mpirun -n $world_size python issues.py']",1,0
20,pytorch,4211,closed,Initial .cuda() slow occur again at the latest cuda9 version PyTorch ,"Similar to #537. the first call of .cuda() take a very long time. It take even more on a model.cuda().





PyTorch install with conda: . 
GPU: GTX 1080 and CUDA9.0.",,"['Interesting, when I try to uninstall and install PyTorch again, I get the following message:\r\n\r\n```bash\r\nconda install pytorch torchvision cuda90 -c pytorch\r\nFetching package metadata ...........\r\nSolving package specifications: .\r\n\r\nPackage plan for installation in environment /home/max/anaconda3/envs/pytorch_env:\r\n\r\nThe following NEW packages will be INSTALLED:\r\n\r\n    pytorch:     0.1.12-py36cuda8.0cudnn6.0_1\r\n    torchvision: 0.1.8-py36_0\r\n```\r\n\r\nWhy would the conda have me installed  the version `0.1.12-py36cuda8.0cudnn6.0_1`? Is this normal?', 'It turns out that I failed to update the PyTorch to the latest version. \r\n\r\nTo update PyTorch in conda, I also need to upgrade conda version outside any conda enviroment. \r\n\r\n```\r\nconda upgrade conda\r\n```\r\n\r\nAnd then source the environment to run update:\r\n\r\n```\r\nconda install pytorch torchvision cuda90 -c pytorch\r\n```\r\n\r\nNow the speed is normal:\r\n```\r\n0 0:00:02.520029\r\n1 0:00:00.000044\r\n2 0:00:00.000028\r\n3 0:00:00.000026\r\n4 0:00:00.000027\r\n5 0:00:00.000025\r\n6 0:00:00.000025\r\n7 0:00:00.000027\r\n8 0:00:00.000025\r\n9 0:00:00.000026\r\n```']","['python\r\nimport torch\r\nfrom datetime import datetime\r\n\r\nfor i in range(10):\r\n    x = torch.randn(10, 10, 10, 10) # similar timings regardless of the tensor size\r\n    t1 = datetime.now()\r\n    x.cuda()\r\n    print(i, datetime.now() - t1)\r\n', '\r\n0 0:03:03.546126\r\n1 0:00:00.000111\r\n2 0:00:00.000065\r\n3 0:00:00.000057\r\n4 0:00:00.000052\r\n5 0:00:00.000051\r\n6 0:00:00.000050\r\n7 0:00:00.000047\r\n8 0:00:00.000046\r\n9 0:00:00.000046\r\n']",['conda install pytorch torchvision cuda90 -c pytorch'],1,0
21,pytorch,23156,open,Pruning off NaN values in the gradient graph still produces NaN gradients.,"## üêõ Bug

This is a niche bug, but it might cause troubles in advanced users who like to use masking to filter out NaN losses. Simply put, when NaN losses are masked out using , performing  on the sum of the losses should produce valid gradients (assuming that the gradient graph is smooth everywhere except for the masked losses). When 

## To Reproduce

Steps to reproduce the behavior:

Define the environment as follows. We will be backpropagating gradients to a very simple  layer.



Performing a forward inference on the linear layer and propagating gradients from there works as intended if no  is involved.



Suppose that  has  in one of its rows.



Performing backwards computation on the sum of the matrix above should
produce  gradients. This is expected.



However, using  or  index slicing on the leaf nodes to mask out problematic gradient graphs should not.


We can check whether masking out final losses works in cases where no  is involved.



The accumulated gradient in the latter case is less than the former because index slicing () on the computation graph prevented the third row in  from affecting the gradient computation.

## Environment

Collecting environment information...
PyTorch version: 1.0.0
Is debug build: No
CUDA used to build PyTorch: None

OS: Mac OSX 10.14.5
GCC version: Could not collect
CMake version: Could not collect

Python version: 3.7
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip] numpy==1.16.3
[pip] torch==1.0.0
[conda] torch                     1.0.0                     <pip>

## Additional context

None.
",module: NaNs and Infs module: autograd module: docs triaged,"[""This isn't a bug. The gradient formula for linear is `weight.grad = x @ grad_output`. You have NaNs in `x`. It doesn't matter that grad_output contains zeros. NaN * 0 is still NaN with IEEE-754 floating point arithmetic."", 'This is a recurrent issue, see #15506 #12986 as an example\r\n\r\nQuoting @ppwwyyxx from one of our previous discussions\r\n\r\n> The fundamental issue is that autograd does not distinguish ""no gradient"" and ""zero gradient"".\r\n\r\nGiven that this has happened very often in the past, I\'m re-opening the issue and tagging it as needs discussion. One possible way of solving this would be to carry an additional (sparse?) tensor that can distinguish between ""no gradient"" and ""zero gradient"".', ""@fmassa This indeed seems like a popular issue that keeps coming back (more examples: #15131 #17455). After reading several closed discussions, I am not sure if it's possible to come up with an elegant solution that can substitute masking with arithmetic multiplication. Passing around sparse flags might be a solution, but it seems like awful lot of work compared to the severity of the bug and could introduce unforeseen consequences like a can of worm. "", 'We should add explanation for this behavior in out FAQ', ""What's the best way to compute the entropy of a vector, i.e. `-(p*p.log()).sum(-1)`, when some of the `p`s are occasionally 0, in such a way that one doesn't get `nan` gradients? Because of this issue, `-torch.where(p > 0, p*p.log(), torch.zeros(p.shape)).sum(-1)` doesn't work since it can work in the forward pass but return `nan`s in the backward pass."", '@carlosgmartin For now, the best way is to apply the mask before subjecting the operand to the operations that result in NaNs. So for your case,\r\n\r\n```\r\np = p.masked_fill(p <= 0, 1)\r\np.mul(p.log()).sum(-1)\r\n```\r\n\r\nthis should do the trick.', 'This can happen in all kinds of situations and should be fixed https://github.com/pytorch/pytorch/issues/36923', ""FWIW I'm guessing that I ran into this too here:\r\nhttps://gist.github.com/EricCousineau-TRI/4d9ec78adf4b566b6d1ce7ab4b643670\r\n\r\nIn a simplified case, I am doing an element-wise multiplication of two vectors (one constant, the other with gradients), where the constant vector may have an entry of `inf` (for depth images). Basically:\r\n```py\r\ndef get_vars():\r\n    a = torch.tensor([inf, 1.0])\r\n    b = torch.tensor([2.0, 3.0], requires_grad=True)\r\n    return a, b\r\n\r\na, b = get_vars()\r\ny1 = a * b\r\nJ1 = jacobian(b, y1)  # Values will have Inf, gradients will have Inf and NaNs.\r\n# v2\r\na, b = get_vars()\r\ny2 = a * b\r\ny2[~torch.isfinite(y2)] = 0.0  # This will zero out values, but gradients will have NaNs.\r\nJ2 = jacobian(b, y2)\r\n# v3\r\na, b = get_vars()\r\na[~torch.isfinite(a)] = 0.0  # This should be OK.\r\ny3 = a * b\r\nJ3 = jacobian(b, y3)\r\n```\r\n\r\nI'm still a bit confused for `v2` (post-masking) and `v3` (pre-masking) for non-finite multiplication. Here's the output for those two cases:\r\nhttps://gist.github.com/EricCousineau-TRI/4d9ec78adf4b566b6d1ce7ab4b643670#file-torch_mask_gradient_nonfinite-py-L98-L108\r\n\r\nCan I ask if someone can confirm if this is indeed the same issue?\r\nI am confused as to why `J[0, 1]` is `nan` for `v2` (post-masking) and `v3` (pre-masking), and why `J[0, 0]` is `nan` for `v2` (post-masking).\r\n\r\nI'm happy to make a separate issue if that simplifies things.\r\n\r\nEDIT: ~And will update to not use `get_analyitcal_jacobian`, just in case that's polluting matters.~ Done in [rev8](https://gist.github.com/EricCousineau-TRI/4d9ec78adf4b566b6d1ce7ab4b643670/revisions#diff-4a77d0effb470897916075a92511b2bc)."", 'It seems like this issue was already fixed at some point. The following snippet taken from OP that was supposed to raise an assertion, but now passes without any when tested on Colab (on both cpu and gpu).\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nx = torch.ones(3, 4, device=\'cuda\')\r\nlinear = nn.Linear(4, 2).to(torch.device(\'cuda\'))\r\ndef has_err(x):\r\n    return bool(((x != x) | (x == float(""inf"")) | (x == float(""-inf""))).any().item())\r\n\r\nlinear.zero_grad()\r\nlinear(x)[:2].sum().backward()\r\nassert not has_err(linear.weight.grad)  # AssertionError raised\r\n```', ""@orsharir Can you post a Gist with `pip freeze` in your Colab env?\r\n\r\nI tried your rendition of the code on my machine (Linux Ubuntu 18.04, CPython 3.6.9, Titan RTX, `torch==1.7.1`), ~but did not get an assertion error. I also have the same behavior on my above snippet.~ Ah, the point was that it didn't raise an assertion error."", ""Hm... to that point, I can't repro OP's error (using Or's consolidated code) on torch 1.0.0, 1.4.0, or 1.7.1...""]","['python\r\nimport torch\r\nimport torch.nn as nn\r\nx = torch.ones(3, 4)\r\nlinear = nn.Linear(4, 2)\r\ndef has_err(x):\r\n    return bool(((x != x) | (x == float(""inf"")) | (x == float(""-inf""))).any().item())\r\n', 'python\r\nlinear(x).sum().backward()\r\nprint(linear.weight.grad)\r\n# tensor([[3., 3., 3., 3.],\r\n#         [3., 3., 3., 3.]])\r\nassert not has_err(linear.weight.grad)\r\n', 'python\r\nx[2].fill_(float(""nan""))\r\nprint(linear(x))\r\n# tensor([[ 0.0570, -1.0066],\r\n#         [ 0.0570, -1.0066],\r\n#         [    nan,     nan]], grad_fn=<AddmmBackward>)\r\n', 'python\r\nlinear.zero_grad()\r\nlinear(x).sum().backward()\r\nprint(linear.weight.grad)\r\n# tensor([[nan, nan, nan, nan],\r\n#         [nan, nan, nan, nan]])\r\nassert has_err(linear.weight.grad)\r\n', 'python\r\nlinear.zero_grad()\r\nlinear(x)[:2].sum().backward()\r\nassert not has_err(linear.weight.grad)  # AssertionError raised\r\n', 'python\r\nx = torch.ones(3, 4)\r\nx[2].fill_(0.5)\r\nlinear.zero_grad()\r\nlinear(x).sum().backward()\r\nprint(linear.weight.grad)\r\n# without masking:\r\n# tensor([[2.5000, 2.5000, 2.5000, 2.5000],\r\n#         [2.5000, 2.5000, 2.5000, 2.5000]])\r\nlinear.zero_grad()\r\nlinear(x)[2:].sum().backward()\r\nprint(linear.weight.grad)\r\n# with masking:\r\n# tensor([[2., 2., 2., 2.],\r\n#         [2., 2., 2., 2.]])\r\n']","['masked_fill', 'backward', 'nn.Linear', 'nan', 'x', 'nan', 'nan', 'masked_fill', 'nan', '[2:]', 'x']",1,0
22,pytorch,15482,closed,GPU allocated memory is not released,"I create CNN to work with images. This network is of the U-Net type for segmentation, which by its architecture saves intermediate calculations. This leads to the conclusion that with a small amount of memory you need to either reduce the size of the image, the number of layers, the size of the batch.
![default](https://user-images.githubusercontent.com/37583380/50356634-ccf4b080-0563-11e9-824a-44f7ef669087.png)
If you give the network a single image of a small size, the memory becomes slightly larger, which is logical.
![default](https://user-images.githubusercontent.com/37583380/50356532-5d7ec100-0563-11e9-95d2-9c7e27bd1558.png)
When you restart with the same image, the amount of allocated memory does not change, only the occupied part of the memory cache increases.
![default](https://user-images.githubusercontent.com/37583380/50356677-ed246f80-0563-11e9-8595-8eb10b651868.png)
Error text:
RuntimeError                              Traceback (most recent call last)
<ipython-input-25-8ba8cee8f3f0> in <module>()
      1 test = np.random.sample([1, 3, 1280, 1280]) * 0.5
----> 2 out = net(torch.FloatTensor(test).to(device))
      3 out.shape

E:\Python\Anaconda\lib\site-packages\torch\nn\modules\module.py in __call__(self, *input, **kwargs)
    487             result = self._slow_forward(*input, **kwargs)
    488         else:
--> 489             result = self.forward(*input, **kwargs)
    490         for hook in self._forward_hooks.values():
    491             hook_result = hook(self, input, result)

<ipython-input-12-552150536d36> in forward(self, x)
     50         x = self.maxpool(d1)
     51 
---> 52         d2 = self.down2(x)
     53         x = self.maxpool(d2)
     54 

E:\Python\Anaconda\lib\site-packages\torch\nn\modules\module.py in __call__(self, *input, **kwargs)
    487             result = self._slow_forward(*input, **kwargs)
    488         else:
--> 489             result = self.forward(*input, **kwargs)
    490         for hook in self._forward_hooks.values():
    491             hook_result = hook(self, input, result)

E:\Python\Anaconda\lib\site-packages\torch\nn\modules\container.py in forward(self, input)
     90     def forward(self, input):
     91         for module in self._modules.values():
---> 92             input = module(input)
     93         return input
     94 

E:\Python\Anaconda\lib\site-packages\torch\nn\modules\module.py in __call__(self, *input, **kwargs)
    487             result = self._slow_forward(*input, **kwargs)
    488         else:
--> 489             result = self.forward(*input, **kwargs)
    490         for hook in self._forward_hooks.values():
    491             hook_result = hook(self, input, result)

E:\Python\Anaconda\lib\site-packages\torch\nn\modules\activation.py in forward(self, input)
     48     @weak_script_method
     49     def forward(self, input):
---> 50         return F.threshold(input, self.threshold, self.value, self.inplace)
     51 
     52     def extra_repr(self):

E:\Python\Anaconda\lib\site-packages\torch\nn\functional.py in threshold(input, threshold, value, inplace)
    838         result = _VF.threshold_(input, threshold, value)
    839     else:
--> 840         result = _VF.threshold(input, threshold, value)
    841     return result
    842 

RuntimeError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 2.00 GiB total capacity; 1.34 GiB already allocated; 5.20 MiB free; 12.75 MiB cached)

After a large image again give the network image smaller and again get the same error:
![default](https://user-images.githubusercontent.com/37583380/50356722-2230c200-0564-11e9-8fba-54855c20de9f.png)
Things with memory become like this:
![default](https://user-images.githubusercontent.com/37583380/50356785-67ed8a80-0564-11e9-8a9c-895b453410ba.png)

Without restarting jupyter notebook in which I work, the memory does not clear out.

",triaged,"[""I also have this annoying problem. The memory just doesn't get cleared out even I `del` every possible object that could live in the GPU memory. The only solution, as stated by @EpicDima , is to restart the Jupyter notebook."", 'This is expected -- we use a memory caching allocator for the GPU, because `cudaFree` implies a synchronization point, which would make the whole program much slower.\r\n\r\nIf you want to free the cached memory, you can call `torch.cuda.empty_cache()`']",[],[],1,0
23,pytorch,2180,open,Discrepancy in BCEWithLogitsLoss and ClassNLLLoss,"In BCEWithLogitsLoss, weights are unnormalized and used, while in ClassNLLLoss weights are normalized and used. 

Some discussion around this can be found [here](https://github.com/pytorch/pytorch/commit/67968cb60b1d3021834594967d4140a36a8213e3#commitcomment-23155134). Is this discrepancy expected? Else if this is an issue I can send a PR to resolve this.",module: cpu module: cuda module: loss triaged,"['this is not an expected discrepancy. Maybe BCEWithLogitsLoss can be fixed.', '@apaszke Your ENet paper used a class weighting scheme that was not normalised if I remember correctly - were they normalised during the  implementation or did you keep them explicitly as-is?', ""@Kaixhin the weights were normalized. [Here's](https://github.com/e-lab/ENet-training/blob/f76214475ace6df38d50fa93a334ed7ffcc93af6/train/models/encoder.lua#L116) the code snippet."", ""@shubhamjain0594 I think comparing BCEWithLogitsLoss (and BCELoss) weights with NLLLoss weights isn't a fair comparision: NLLLoss weights are per-class while BCELoss weights are per batch-element. \r\n\r\nIn this sense I don't think it makes sense to normalize the weights in BCEWithLogitsLoss because they're per-batch element and the use-case for them is weighing the elements in the loss differently: what do you think?""]",[],[],1,0
24,pytorch,9873,open,"Pytorch is slow when only using CPU, and cannot utilize multicore of CPU","When I testing the acceleration effect on CPU by decomposing convolution layers, I found pytorch is slow  and cannot utilize multicores of CPU. 




Output is that while cpu usage is 100%-200%
> 147.48149728775024 16.699654817581177

![pytorch](https://user-images.githubusercontent.com/10665923/43258302-eca30248-9104-11e8-8045-49d8d63819a4.PNG)

However, Keras (TF  back-end)  is faster and multi-threaded.

Output is that while cpu usage is 800%-1600%
> 26.393059253692627 13.783706188201904

![keras](https://user-images.githubusercontent.com/10665923/43258314-f57cf11c-9104-11e8-8cfd-c094a798df48.PNG)

## System Info
Pytorch 0.4
Ubuntu 16.04



cc @VitalyFedyunin @ngimel",module: cpu module: multithreading module: performance triaged,"['@zhangchenkai you will see great improvement if you build from source from these instructions: https://github.com/pytorch/pytorch#from-source\r\n\r\nThe instructions will build and link with mkl-dnn and that will show you remarkable improvements to what you see.', ""Why dosen't PyTorch provide a compiled version directly, like other packages TF, Numpy, Tensorly etc. ?"", 'we are getting there, we plan to ship with MKL-DNN support in the next release binaries.', 'I have been converting from keras to pytorch. Everything similar on GPU but on CPUs pytorch is really slow - 10 times slower with my predictions expected to take 10+ hours versus 1 hour on keras. I have compiled pytorch and installed mkldnn. No difference. ', '@simonm3 that is not normal behavior at all, esp. if you compiled from source with MKLDNN (you have to install MKLDNN first, and compile PyTorch -- make sure PyTorch detected MKLDNN).\r\nWhat kind of model are you running?', ""It is vgg19 with fully connected head. Nothing unusual. Works fine on GPU.\nWill try again. Maybe I installed mkldnn after. can't remember.\n\nWhen will this be included in the binary. Takes hours to compile.\n\nOn 30 July 2018 at 17:16, Soumith Chintala <notifications@github.com> wrote:\n\n> @simonm3 <https://github.com/simonm3> that is not normal behavior at all,\n> esp. if you compiled from source with MKLDNN (you have to install MKLDNN\n> first, and compile PyTorch -- make sure PyTorch detected MKLDNN).\n> What kind of model are you running?\n>\n> ‚Äî\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/9873#issuecomment-408921245>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABJN6fKhCEbq3IAcSfLYzQgvOv_HD6lZks5uLzFXgaJpZM4Vhmgs>\n> .\n>\n"", ""OK that is much better. However there is an anomaly. After a couple of minutes training the expected times are:\r\n\r\n#### 8 processors=> 6.5 hours keras, 3.5 hours pytorch\r\n#### 72 processors=> 1 hour keras, 1'20 pytorch\r\n\r\nSo keras is actually slower on 8 processors but gets a 6 times speedup from 9 times the CPUs which sounds as expected. Pytorch is faster on 8 processors but only gets 2 times speedup from 9 times the CPUs. Hence pytorch is about 30% slower on the 72 processor machine. I tried  torch.set_num_threads but this made no difference. Could be because I compiled pytorch on an 8 CPU machine and I noticed some compile warnings - can't recall exact message but were about number of threads/cpus.\r\n\r\nTypically I would like to compile on cheaper machine but use on 72 processor machine. Is there some compile time flag to allow this? Or some way to speed it up at runtime for 72 processors?"", 'Tried recompiling and the warning error was not about threads/cpus but ""ptxas warning : Too big maxrregcount value specified 96, will be ignored"". So I guess that is not the issue. Maybe there is an environment variable that caps the number of threads or something?\r\n', ""ptxas is an NVIDIA CUDA program, it shouldn't have anything to do with CPU perf."", 'Ah found the problem I think - my fault! Not yet tested but......I have been predicting in chunks using a chunksize=5 so it would work on my laptop with 8GB RAM. That means only 5 rows can be done in parallel. I need to increase this for bigger machine which has more memory.', 'Actually same code for keras and pytorch so does not explain it; and increasing the chunksize makes little difference. So still a mystery', ""@simonm3 you dont have 72 cores or 72 processors. It's likely you have 72 hyperthreads, and actually have 36 cores, spread across 2 process (each processor having 18 cores each).\r\n\r\nTry setting these environment variables and see if this helps:\r\n\r\n```\r\n# in your terminal, before running python / pytorch\r\nNUM_CORES=18\r\nexport MKL_NUM_THREADS=$NUM_CORES OMP_NUM_THREADS=$NUM_CORES\r\npython [your program]\r\n```\r\n\r\nTry the values `9, 18, 36` for `NUM_CORES`"", ""@zhangchenkai and @simonm3, if you compile from source as per the instructions (https://github.com/pytorch/pytorch#from-source) and hence work in a conda environment, un-installing intel-openmp (installed in the process as a dependency) and using openmp instead\r\n\r\n```\r\n# Install all the dependencies and then:\r\nconda remove --force intel-openmp\r\nconda install -c intel openmp\r\n# Install pytorch and then:\r\nconda remove --force openmp\r\n```\r\nin the environment seems to let the `python setup.py install` successfully finish and to enable setting the number of threads using the environmental variables mentioned by @soumith (i.e., you see `$OMP_NUM_THREADS` cores being utilized in `htop`).\r\n\r\nBtw, I am on `Ubuntu 16.04` and used `gcc` for compiling PyTorch. I dont know the exact effect of the installation steps above but may be you need to prevent `openmp` from interfering with `gcc`'s."", '@berkinmalkoc  if I uninstall intel-openmp then it uninstalls a long list of packages that depend on it such as numpy and mkldnn. I guess you have two openmp versions installed so need to uninstall one to avoid clashes; whereas I have just the one.\r\n\r\nI tried setting OMP_NUM_THREADS/MKL_NUM_THREADS but whatever setting I tried seemed to make no difference to the timings on either 8 or 72 vcpus.\r\n\r\nI increased my batch size from 5 to 100 which made pytorch faster but not keras. Now pytorch is 191 minutes on 8 vcpus and 48 minutes on 72 so a multiple of 4. Meanwhile keras is 463 minutes on 8 vcpus and 63 minutes on 72 so a multiple of 7 which still seems significantly different,\r\n\r\nI did struggle a bit getting consistent benchmarks as it is hard to clear all caches and get a stable starting point so all numbers are approximate. Also there may be different optimisation methods between keras and pytorch; and indeed my models are not identical.  So perhaps this kind of difference is to be expected.', '@simonm3,\r\n\r\n> @berkinmalkoc if I uninstall intel-openmp then it uninstalls a long list of packages that depend on it such as numpy and mkldnn.\r\n\r\nSorry, my bad, I updated my previous comment and added the `--force` option to `conda remove` (so that only intel-openmp gets uninstalled and not its dependencies).\r\n\r\n> I tried setting OMP_NUM_THREADS/MKL_NUM_THREADS but whatever setting I tried seemed to make no difference to the timings on either 8 or 72 vcpus.\r\n\r\nDo these vars change the number of cores that get busy when you run your code (as seen with, e.g., `htop`)? Are the timings independent of this?\r\n\r\nFor me, the only way to have the number of busy cores reflect the value in `OMP_NUM_THREADS` was the installation procedure described in my previous post. But the timings were not significantly different across various values of `OMP_NUM_THREADS`.', '@berkinmalkoc For me setting OMP_NUM_THREADS/MKL_NUM_THREADS before starting python does change the number of threads shown in htop. I note that if I set it to 9 htop shows 10 threads; and 36 shows 37 threads. If I set above 36 then it shows 37 despite there being 72 vcpus. I guess there is one main thread added. \r\n\r\nhtop shows the 37 as 100% utilised. Previously I have been using ""mpstat -P ALL"". This shows cpu utilisation starting low (maybe it uses an average over a period) but over time increasing across all 72 vcpus. Not sure what they are showing exactly.\r\n\r\nNo difference in timing whatever values of OMP_NUM_THREADS/MKL_NUM_THREADS.', ""I resolved this issue by running:\r\n```\r\n$ conda install -c pytorch pytorch-nightly-cpu\r\n$ conda install -c fastai torchvision-nightly-cpu\r\n$ conda install -c intel openmp\r\n$ conda env update -f environment-cpu.yml\r\n$ conda activate fastai-cpu\r\nexport NUM_CORES=40\r\n$ export MKL_NUM_THREADS=$NUM_CORES OMP_NUM_THREADS=$NUM_CORES\r\n```\r\n\r\n[How I resolved slow PyTorch on Anaconda](https://pmchojnacki.wordpress.com/2018/10/07/slow-pytorch-cpu-performance/)\r\nNote, I was using fast.ai as a frontend, but it shouldn't matter.\r\n\r\n"", 'May i ask if i use `jupyter notebook`, should i just set `torch.set_num_threads=18` like that ?', '@tsungruihon yes, but it will be applied only to a current notebook. Of course, if you will open your notebook again (or reset kernel) you will have to run this command again.', 'none of export NUM_CORES=12 MKL_NUM_THREADS=12 OMP_NUM_THREADS=12 worked for me on windows/linux\r\nI used torch.set_num_threads(12) in the .py file and it worked :)\r\ntorch 1.8.1', '@Manoa1911, how many logical CPU cores did you have?']","['\r\n# To blind GPU\r\nos.environ[""CUDA_VISIBLE_DEVICES""] = \'  \'\r\n# Generate input data\r\nt = np.random.randn(50,128,128,64).astype(np.float32)\r\n', '\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nx = torch.from_numpy(t).permute(0,3,1,2)\r\n\r\nmodel1 = nn.Sequential(\r\n    nn.Conv2d(64, 128, 3, stride=1, padding=1, dilation=1),\r\n    nn.Conv2d(128, 256, 3, stride=1, padding=1, dilation=1),\r\n)\r\n\r\nmodel2 = nn.Sequential(\r\n    nn.Sequential(\r\n        nn.Conv2d(64, 16, kernel_size=(1, 1)),\r\n        nn.Conv2d(16, 16, kernel_size=(3, 1), padding=(1, 0)),\r\n        nn.Conv2d(16, 16, kernel_size=(1, 3), padding=(0, 1)),\r\n        nn.Conv2d(16, 128, kernel_size=(1, 1)),\r\n    ),\r\n    nn.Sequential(\r\n        nn.Conv2d(128, 32, kernel_size=(1, 1)),\r\n        nn.Conv2d(32, 32, kernel_size=(3, 1), padding=(1, 0)),\r\n        nn.Conv2d(32, 32, kernel_size=(1, 3), padding=(0, 1)),\r\n        nn.Conv2d(32, 256, kernel_size=(1, 1)),\r\n    ),\r\n)\r\n\r\niters = 10\r\n\r\nt1 = time.time()\r\nfor i in range(iters):\r\n    y=model1(x)\r\nt2 = time.time()\r\nm1t = t2-t1\r\n\r\nt1 = time.time()\r\nfor i in range(iters):\r\n    y=model2(x)\r\nt2 = time.time()\r\nm2t = t2-t1\r\n\r\nprint(m1t, m2t)\r\n', '\r\nmodel1 = Sequential([\r\n    Conv2D(128,3,input_shape=(128,128,64)),\r\n    Conv2D(256,3),\r\n])\r\n\r\n\r\nmodel2 = Sequential([\r\n    Sequential([\r\n        Conv2D(16,1,input_shape=(128,128,64)),\r\n        Conv2D(16,(3,1)),\r\n        Conv2D(16,(1,3)),\r\n        Conv2D(128,(1,1)),\r\n    ]),\r\n    Sequential([\r\n        Conv2D(32,1,input_shape=(100,100,128)),\r\n        Conv2D(32,(3,1)),\r\n        Conv2D(32,(1,3)),\r\n        Conv2D(256,1)\r\n    ]),\r\n])\r\n']",[],1,0
25,pytorch,25243,closed,load pretrained model(trained in torch0.4) and got nan loss in torch1.2,"Is it possible to cause Nan loss in pytorch1.2 when i load pretrained model trained in pytorch0.4?
Because pytorch1.2 support sync-bn while pytorch0.4 does not.
I have tested just random init and did not load pretrained model and the nan loss did not appear",,[],[],[],1,0
26,pytorch,30365,open,TorchScript Performance: 150x gap between TorchScript and Native Python,"## üêõ Bug

There's a 150x gap in performance for TorchScript ops versus straight Python / C++.
Looping over 100K numbers takes 2+ seconds instead of 18ms or better. 
Please see the benchmarks here: https://github.com/divyekapoor/ml-op-benchmarks

## To Reproduce
https://github.com/divyekapoor/ml-op-benchmarks

Steps to reproduce the behavior:
1. Clone the repo
1. make torchbench

See related TensorFlow issue for context:
https://github.com/tensorflow/tensorflow/issues/34500

## Expected behavior

FizzBuzz Iteration Counts | 100000 | ¬† | ¬† | ¬†
-- | -- | -- | -- | --
¬† | Raw Latency (ms) | Per Run Latency (usec) | Python Multiplier | C++ Multiplier
PyTorch Python | 4007 | 40.07 | 222.61 | 23851
PyTorch TorchScript Python (from Loaded TorchScript) | 2830 | 28.3 | **157.22** | 16845
PyTorch TorchScript C++ (Native) | 255 | 2.55 | **14.17** | 1518
PyTorch TorchScript C++ (Native + ATen Tensors) | 252 | 2.52 | **14.00** | 1500
Raw Python | 18 | 0.18 | 1.00 | 107
Raw C++ | 0.168 | 0.00168 | 0.01 | 1

Performance similar to raw Python is the expected behavior.

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:




 - PyTorch Version (e.g., 1.0): 1.3
 - OS (e.g., Linux): Mac OS X
 - How you installed PyTorch (, , source): pip
 - Build command you used (if compiling from source): NA
 - Python version: 3.7
 - CUDA/cuDNN version: NA
 - GPU models and configuration: NA
 - Any other relevant information: See performance tables and github repo.

## Additional context

Code: 



cc @suo",oncall: jit triage review triaged,"['To be fair, while it can obviously be done, `forward` having side effects (here setting attributes) is not the most common use case', ""@Evpok Even without the side effects, the performance gap is consistent, just check out:\r\nhttps://github.com/divyekapoor/ml-op-benchmarks\r\nand change the code if you'd prefer:\r\n\r\nOutcomes: \r\n```\r\nTime (PyTorch) (ms):  4097.000675\r\nTime (PyTorch optimized=True) (ms):  3982.672392\r\nTime (PyTorch optimized=False) (ms):  4017.969171\r\nTime (PyTorch from Loaded) (ms):  2879.079591\r\nTime taken (Python3) (ms):  18.797112\r\n```\r\n\r\nCode:\r\n```python3\r\nclass TorchFizzBuzz(torch.nn.Module):\r\n    def __init__(self):\r\n        super(TorchFizzBuzz, self).__init__()\r\n\r\n    def forward(self, n: torch.Tensor):\r\n        i = torch.tensor(0, dtype=torch.int32, requires_grad=False)\r\n        fizz = torch.zeros(1)\r\n        buzz = torch.zeros(1)\r\n        fizzbuzz = torch.zeros(1)\r\n        while i < n:\r\n            if i % 6 == 0:\r\n                fizzbuzz += 1\r\n            elif i % 3 == 0:\r\n                buzz += 1\r\n            elif i % 2 == 0:\r\n                fizz += 1\r\n            i += 1\r\n        return torch.stack([fizz, buzz, fizzbuzz])\r\n```"", '@Evpok From the discussion with the Tensorflow folks (tensorflow/tensorflow#34500), we generated a NumPy baseline if that would be preferable. \r\n\r\nFizzBuzz Iteration Counts | 100000 | \xa0 | \xa0 | \xa0\r\n-- | -- | -- | -- | --\r\n\xa0 | Method Latency (ms) | Iteration Latency (usec) | Python Multiplier | C++ Multiplier\r\nTensorflow Python | 4087 | 40.87 | ***227.06*** | 24327\r\nTensorflow Saved Model Python | 4046 | 40.46 | ***224.78*** | 24083\r\nTensorflow Python no Autograph | 3981 | 39.81 | **221.16** | 23696\r\nPyTorch Python | 4007 | 40.07 | **222.61** | 23851\r\nPyTorch TorchScript Python (from Loaded TorchScript) | 2830 | 28.3 | **157.22** | 16845\r\nNumPy Python | 420 | 4.2 | ***23.3*** | 2500\r\nPyTorch TorchScript C++ (Native) | 255 | 2.55 | 14.17 | 1518\r\nPyTorch TorchScript C++ (Native + ATen Tensors) | 252 | 2.52 | 14.00 | 1500\r\nRaw Python | 18 | 0.18 | 1.00 | 107\r\nRaw C++ | 0.168 | 0.00168 | 0.01 | 1\r\n', ""Why is the numpy version faster than torchscript C++ per iteration but slower for 100,000 iterations?\r\nSeems like there's a factor of 10 out for one of those.\r\nIs iteration meant to be 4.2us?"", 'Yes. Fixed. Thanks for pointing it out. ', 'A colleague also got a Torchscript vectorized implementation set up with an ~8ms baseline (beating the 18ms from Python). So that would be something to think about as a reference implementation. Discussion on the equivalent TF bug is also quite useful - they have some experimental workarounds.\r\n\r\n@xsacha @suo Could you indicate the next steps?', ""Hey, @divyekapoor I'd be interested to know the ultimate use case you're benchmarking for. \r\n\r\nThe reason I ask is that PyTorch is poorly optimized for doing lots of computations on scalar values‚Äîas mentioned on the TF issue, these libraries are typically targeted toward doing operations on large tensors, where the per-op overhead is dwarfed by the operator computation itself. \r\n\r\nAs such, if something fizzbuzz-like is similar to your use case, you're unlikely to get performance comparable to just writing it in C++. In other words, you are paying the cost of using PyTorch (overhead) without benefiting from its core features (autograd, rich tensor library, etc.)\r\n\r\nThat said, a few thoughts on this particular case:\r\n- In cases like this, we typically recommend that you implement the computation in a custom op in C++ and call it from TorchScript/Python ([see our tutorial](https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html)).\r\n- We are working on improved CPU kernel fusion and code generation that would theoretically achieve near-native performance on code that looks like this (cc @jamesr66a @mruberry), but there is no timeline for when that will be available.\r\n"", 'Thanks for the detailed reply @suo !\r\nOur usecase is cross features for some of our online serving models where the features cannot be prematerialized (think UserContext x ImageFeatures where both users and images are large sets O(millions/billions)). Think about these powering something like the Instagram feed / Pinterest feed. \r\n\r\nFor the purposes of our discussion, assume an LR model where Users have some topic affinities and Images have some topic affinities. Both are sparse vectors of the form { topic: weight }. The cross feature is the dot product after some sanitization, thresholding, normalization, boosts. The dot product itself is easy to vectorize but everything around it is regular control flow (eg. Boost feature value by 2x if both sides have more than 3 matches from a given list [a, b, c], if there are more than 7 matches from this other list, reduce by 0.5, one feature might be counts matching a hardcoded feature subset etc.). \r\n\r\nTo be clear, this is a hypothetical illustrative example. The actual cross features are currently in straight custom C++ in our serving binary written by model engineers and are quite varied. However, given the User and Image inputs, we‚Äôd like the serving binary to never know how to generate these crosses (it should all be part of model code). Model engineers can then be more productive (no C++) and the infra simplifies (everyone is dealing with non cross features as inputs even though these inputs may be somewhat complex eg. Maps). Similarly on training, the cross features can be backfilled or tuned (again, no materialized cross features). The end goal is some non trivial cross features produced in-model during execution using some light control flow ops without lots of overhead. \r\n\r\n(Written on mobile, happy to add more context in a bit) ', 'Is using tensor as a loop counter and using tensor for control flow operations necessary/makes benchmark more representative? TorchScript is fine with python numbers, and rewriting the benchmarked function as \r\n```\r\nclass TorchFizzBuzz(torch.nn.Module):\r\n    def __init__(self):\r\n        super(TorchFizzBuzz, self).__init__()\r\n\r\n    def forward(self, n: int):\r\n        i = 0\r\n        fizz = torch.zeros(1)\r\n        buzz = torch.zeros(1)\r\n        fizzbuzz = torch.zeros(1)\r\n        one = torch.ones(1)\r\n        while i < n:\r\n            if i % 6 == 0:\r\n                fizzbuzz += one\r\n            elif i % 3 == 0:\r\n                buzz += one\r\n            elif i % 2 == 0:\r\n                fizz += one\r\n            i += 1\r\n        return torch.stack([fizz, buzz, fizzbuzz])\r\n```\r\ngives \r\n```\r\nTime (PyTorch from Loaded) (ms):  135.470804\r\n```', 'I think the point of this issue is to illustrate how much slower a tensor is so that such bottlenecks can be avoided.\r\n\r\nYes you could replace it with an int, but the point is using a tensor to count should be similar in speed.\r\n\r\nNo one here is using fizzbuzz, we use much more complicated models that exhibit the same issues but are too complex to post here and identify control flow as being a slow down due to the other complexities in the model (such as matrix multiplication).', ""I don't think that using tensor to count should be similar in speed - it would be a nice bonus if it were, but no one is making you use tensors everywhere. Tensors should be used where it makes sense, and not used where it does not. Also, torch native c++ benchmarks listed here don't use tensors for loops and control flow https://github.com/divyekapoor/ml-op-benchmarks/blob/master/torch_fizz.cc#L17-L37, so in that sense it's not an apples-to-apples comparison. "", ""@xsacha @ngimel - I've updated the benchmarks to address @ngimel 's comments on apples to apples.\r\n\r\nThe C++ API now has one setup with a Tensor based loop and the other one with a native loop counter.\r\n\r\nPoint to note: The benchmark is to illustrate that Tensor based ops are hundreds of times slower than just basic Python code. Even at 135ms, the Pytorch version of the program is 10x slower than writing raw Python.\r\n\r\nThe root cause is a slow Tensor class (illustrated by the fact that the torch::Tensor based loop takes 2700 ms to complete but a native counter loop takes just 200ms (and raw C++ takes just usecs)).\r\n\r\nI'm not sure what's driving the slowness with the torch::Tensor class - what would be the best way to investigate?\r\n\r\nApples to Apples links: \r\nhttps://github.com/divyekapoor/ml-op-benchmarks/blob/master/torch_fizz.cc#L17-L37\r\n""]","['\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n', '\r\n$ python3 /tmp/collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 1.3.0.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.14.6\r\nGCC version: Could not collect\r\nCMake version: version 3.15.5\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.4\r\n[pip3] torch==1.3.0.post2\r\n[conda] Could not collect\r\n', 'python\r\nclass TorchFizzBuzz(torch.nn.Module):\r\n    def __init__(self):\r\n        super(TorchFizzBuzz, self).__init__()\r\n        self.fizz = torch.tensor(0, requires_grad=False)\r\n        self.buzz = torch.tensor(0, requires_grad=False)\r\n        self.fizzbuzz = torch.tensor(0, requires_grad=False)\r\n\r\n    def forward(self, n: torch.Tensor):\r\n        i = torch.tensor(0, dtype=torch.int32, requires_grad=False)\r\n        self.fizz = torch.zeros(1)\r\n        self.buzz = torch.zeros(1)\r\n        self.fizzbuzz = torch.zeros(1)\r\n        while i < n:\r\n            if i % 6 == 0:\r\n                self.fizzbuzz += 1\r\n            elif i % 3 == 0:\r\n                self.buzz += 1\r\n            elif i % 2 == 0:\r\n                self.fizz += 1\r\n            i += 1\r\n        return torch.stack([self.fizz, self.buzz, self.fizzbuzz])\r\n']","['conda', 'pip']",1,0
27,pytorch,20125,closed,NCCL hang in PyTorch Distributed Data Parallel for Mixed Precision Training,"## üêõ Bug

On an AWS p3.16xlarge machine, I'm using the mixed precision training from https://github.com/NVIDIA/Megatron-LM checked in at https://github.com/cybertronai/transformer-xl
When I run with [adaptive softmax](https://github.com/cybertronai/transformer-xl/blob/master/mem_transformer.py#L477) with 8 GPUs and [apex mixed precision](https://github.com/cybertronai/transformer-xl/blob/master/fp16_opt.py), NCCL hangs. When I remove , it works as expected (after reducing batch size to avoid OOM). If I remove  it also works fine, so it's definitely a mixed precision issue.

Sorry if this is a dup of #11672 but I haven't been able to figure it out.

## To Reproduce

Steps to reproduce the behavior:
On a p3.16xlarge


In the [logs](https://github.com/NVIDIA/nccl/files/3144052/combined.txt) I found this:

How do I figure out where this is coming from?  Is this something my code should account for or is it a bug in DDP or apex?

## Expected behavior
Doesn't hang silently.

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).


## Additional context

",oncall: distributed,"[""This appears to have been fixed in 1.1, will reopen if it's broken after upgrade."", 'Nope, still broken\r\n```\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: Tesla V100-SXM2-16GB\r\nGPU 1: Tesla V100-SXM2-16GB\r\nGPU 2: Tesla V100-SXM2-16GB\r\nGPU 3: Tesla V100-SXM2-16GB\r\nGPU 4: Tesla V100-SXM2-16GB\r\nGPU 5: Tesla V100-SXM2-16GB\r\nGPU 6: Tesla V100-SXM2-16GB\r\nGPU 7: Tesla V100-SXM2-16GB\r\n\r\nNvidia driver version: 410.104\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.4\r\n[conda] blas                      1.0                         mkl\r\n[conda] cuda100                   1.0                           0    pytorch\r\n[conda] mkl                       2019.3                      199\r\n[conda] mkl-include               2019.3                      199\r\n[conda] mkl-service               1.1.2            py36he904b0f_5\r\n[conda] mkl_fft                   1.0.10           py36ha843d7b_0\r\n[conda] mkl_random                1.0.2            py36hd81dba3_0\r\n[conda] pytorch                   1.0.1           py3.6_cuda10.0.130_cudnn7.4.2_0    pytorch\r\n[conda] pytorch-lamb              0.0.0                     <pip>\r\n[conda] pytorch-pretrained-bert   0.6.2                     <pip>\r\n[conda] torch                     1.1.0                     <pip>\r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n```', 'I ran with n_procs=1 and suddenly a magic new error message appeared telling me to try adding `find_unused_parameters=True` to `DistributedDataParallel`. When I did that, it worked! Thank you for good error messages!!!']","['sh\r\ngit clone https://github.com/cybertronai/transformer-xl\r\ncd transformer-xl\r\npip install -r requirements.txt\r\n# Download dataset to ./data/wikitext-103/ then\r\nNCCL_DEBUG_FILE=log.%h.%p NCCL_DEBUG_SUBSYS=COLL NCCL_DEBUG=INFO NCCL_MIN_NRINGS=16 NCCL_MAX_NRINGS=16  python -m tor\r\nch.distributed.launch --nproc_per_node=8 --nnodes=1 --node_rank=0 --master_addr=172.31.20.228 --master_port=6016 train.py --seed 1111 --data data/wikitext-103\r\n --dataset wt103 --adaptive --log_interval 100 --eval_interval 500 --max_tokens 1500000000 --logdir /ncluster/runs/ben-no-bpe.13 --distributed --lr 0.0005 --b\r\natch_size 4 --eta_min 5e-05 --n_layer 18 --d_model 1024 --n_head 16 --d_head 64 --d_inner 4096 --dropout 0.2 --dropatt 0.2 --optim lamb --warmup_tokens 300000\r\n0 --tgt_len 128 --mem_len 128 --eval_tgt_len 128 --fp16 --dynamic_loss_scale  --init_std 0.005 --div_val 4 \r\n', '\r\nip-172-31-20-228:22326:22707 [3] NCCL INFO AllReduce: opCount 5 sendbuff 0x7fcf6b400000 recvbuff 0x7fcf6b400000 count 1322459 datatype 6 op 0 root 0 comm 0x7f\r\ncfc403a800 [nranks=8] stream 0x563efa445460\r\nip-172-31-20-228:22327:22327 [4] NCCL INFO AllReduce: opCount 5 sendbuff 0x7fb11f71c200 recvbuff 0x7fb11f71c200 count 1 datatype 4 op 0 root 0 comm 0x7fb09c03\r\na800 [nranks=8] stream 0x55c779123600\r\n', '\r\nPyTorch version: 1.0.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: Tesla V100-SXM2-16GB\r\nGPU 1: Tesla V100-SXM2-16GB\r\nGPU 2: Tesla V100-SXM2-16GB\r\nGPU 3: Tesla V100-SXM2-16GB\r\nGPU 4: Tesla V100-SXM2-16GB\r\nGPU 5: Tesla V100-SXM2-16GB\r\nGPU 6: Tesla V100-SXM2-16GB\r\nGPU 7: Tesla V100-SXM2-16GB\r\n\r\nNvidia driver version: 410.104\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.4\r\n[conda] blas                      1.0                         mkl\r\n[conda] cuda100                   1.0                           0    pytorch\r\n[conda] mkl                       2019.3                      199\r\n[conda] mkl-include               2019.3                      199\r\n[conda] mkl-service               1.1.2            py36he904b0f_5\r\n[conda] mkl_fft                   1.0.10           py36ha843d7b_0\r\n[conda] mkl_random                1.0.2            py36hd81dba3_0\r\n[conda] pytorch                   1.0.1           py3.6_cuda10.0.130_cudnn7.4.2_0    pytorch\r\n[conda] pytorch-lamb              0.0.0                     <pip>\r\n[conda] pytorch-pretrained-bert   0.6.2                     <pip>\r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n']","['--adaptive', '--fp16']",1,0
28,pytorch,160,closed,loss functions return wrong values,"The following code outputs:
4, 0.4, 4
instead of
4, 0.4, 0.04

and there seem to be errors for other loss functions as well, like SmoothL1Loss.


",,"[""My bad. It's related to another issue which is that small exponents are not printed. Loss is working fine.\n"", 'As a side note, you should never call `loss.forward`, but use `loss(x, y)` instead.\n']","['\nloss = nn.L1Loss()\ny = Variable(torch.FloatTensor(1).fill_(0))\nprint(loss.forward(Variable(torch.FloatTensor(1).fill_(4.)), y))\nprint(loss.forward(Variable(torch.FloatTensor(1).fill_(.4)), y))\nprint(loss.forward(Variable(torch.FloatTensor(1).fill_(.04)), y))\n']",[],1,1
29,pytorch,5801,closed,KLDivLoss behaves differently on CPU/GPU,"Pytorch Version: '0.3.0'

Code:


Without cuda, both the gradients and the loss are the same:


With cuda the gradients returned by KLDivLoss are scaled down by the number of class, while the loss is the same. 
",,"['Just to confirm I have also found this, but had not yet put together a bug report.\r\n\r\nI have tried it with both cuda 8 and 9, and torch 0.3.1 and the master branch as of a few days ago, compiled with the tricks mentioned in here:\r\n\r\nhttps://github.com/pytorch/pytorch/issues/5136\r\n\r\nOS is ubuntu 17.10, python 3.6.', 'Thanks for the report, @yuandong-tian and the confirmation @reynoldscem. I think I found the bug. Fix incoming.', ""Thanks that's brilliant.""]","['\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\nimport sys\r\n\r\nbatchsize = 8\r\nnclass = 5\r\n\r\nif sys.argv[1] == ""cuda"":\r\n    cuda = True\r\nelse:\r\n    cuda = False\r\n\r\nprint(""Cuda: "" + str(cuda))\r\n\r\ndef make_prob():\r\n    if cuda:\r\n        pr = torch.cuda.FloatTensor(batchsize, nclass)\r\n    else:\r\n        pr = torch.FloatTensor(batchsize, nclass)\r\n    pr.uniform_()\r\n    pr /= pr.sum(dim=1).view(batchsize, 1)\r\n    return pr\r\n\r\ndef loss_kl(pi, targets):\r\n    pi_var = Variable(pi, requires_grad=True)\r\n    logsoftmax = nn.LogSoftmax(dim=1)\r\n    kl = nn.KLDivLoss()\r\n\r\n    if cuda:\r\n        logsoftmax.cuda()\r\n        kl.cuda()\r\n\r\n    logpi = logsoftmax(pi_var)\r\n    return kl(logpi, Variable(targets)) * logpi.size(1), pi_var\r\n\r\ndef loss_plain(pi, targets):\r\n    pi_var = Variable(pi, requires_grad=True)\r\n    targets_var = Variable(targets)\r\n    logsoftmax = nn.LogSoftmax(dim=1)\r\n\r\n    if cuda:\r\n        logsoftmax.cuda()\r\n\r\n    logpi = logsoftmax(pi_var)\r\n    loss = - (logpi * targets_var).sum(dim=1).mean() + (targets_var * targets_var.log()).sum(dim=1).mean()\r\n    return loss, pi_var\r\n\r\ntargets = make_prob()\r\npi = make_prob()\r\n\r\nloss1, pi_var1 = loss_kl(pi, targets)\r\nprint(loss1)\r\nloss1.backward()\r\nprint(pi_var1.grad)\r\n\r\nloss2, pi_var2 = loss_plain(pi, targets)\r\nprint(loss2)\r\nloss2.backward()\r\nprint(pi_var2.grad)\r\n', '\r\n$ python3 ~/test_kl.py nocuda\r\nCuda: False\r\nVariable containing:\r\n 0.1930\r\n[torch.FloatTensor of size 1]\r\n\r\nVariable containing:\r\n1.00000e-02 *\r\n  1.9849 -1.7913 -0.1536  0.1435 -0.1834\r\n -2.1244  2.5148  0.9348  0.5213 -1.8464\r\n  0.3656  0.1773 -0.1752 -0.1085 -0.2591\r\n -1.7945 -0.2471  1.9382  1.0904 -0.9869\r\n  1.6413  2.5043 -1.3458 -3.3194  0.5196\r\n  0.0206 -1.2024  0.4363  0.8926 -0.1471\r\n -0.4564  2.5605  0.2420 -1.0382 -1.3080\r\n  1.9253  1.7758 -1.0304 -0.5174 -2.1532\r\n[torch.FloatTensor of size 8x5]\r\n\r\nVariable containing:\r\n 0.1930\r\n[torch.FloatTensor of size 1]\r\n\r\nVariable containing:\r\n1.00000e-02 *\r\n  1.9849 -1.7913 -0.1536  0.1435 -0.1834\r\n -2.1244  2.5148  0.9348  0.5213 -1.8464\r\n  0.3656  0.1773 -0.1752 -0.1085 -0.2591\r\n -1.7945 -0.2471  1.9382  1.0904 -0.9869\r\n  1.6413  2.5043 -1.3458 -3.3194  0.5196\r\n  0.0206 -1.2024  0.4363  0.8926 -0.1471\r\n -0.4564  2.5605  0.2420 -1.0382 -1.3080\r\n  1.9253  1.7758 -1.0304 -0.5174 -2.1532\r\n[torch.FloatTensor of size 8x5]\r\n', '\r\n$ python3 ~/test_kl.py cuda\r\nCuda: True\r\nVariable containing:\r\n 0.1756\r\n[torch.cuda.FloatTensor of size 1 (GPU 0)]\r\n\r\nVariable containing:\r\n1.00000e-03 *\r\n -1.2566 -3.5917 -0.8016  2.9373  2.7126\r\n -0.0264  2.3321 -1.3892  1.0676 -1.9841\r\n  0.1204  4.5507 -2.2642 -3.7239  1.3170\r\n  0.5667 -2.7765  4.3564 -4.8917  2.7452\r\n  2.3703 -2.4578  2.7945  1.8861 -4.5931\r\n -0.2237 -0.2954  1.7677  1.7757 -3.0243\r\n -3.9952 -7.2011  2.1794  3.3324  5.6845\r\n -0.2814  0.0368 -2.2764 -0.2955  2.8164\r\n[torch.cuda.FloatTensor of size 8x5 (GPU 0)]\r\n\r\nVariable containing:\r\n 0.1756\r\n[torch.cuda.FloatTensor of size 1 (GPU 0)]\r\n\r\nVariable containing:\r\n1.00000e-02 *\r\n -0.6283 -1.7959 -0.4008  1.4686  1.3563\r\n -0.0132  1.1661 -0.6946  0.5338 -0.9920\r\n  0.0602  2.2753 -1.1321 -1.8619  0.6585\r\n  0.2833 -1.3883  2.1782 -2.4459  1.3726\r\n  1.1852 -1.2289  1.3972  0.9430 -2.2966\r\n -0.1119 -0.1477  0.8838  0.8879 -1.5121\r\n -1.9976 -3.6006  1.0897  1.6662  2.8423\r\n -0.1407  0.0184 -1.1382 -0.1477  1.4082\r\n[torch.cuda.FloatTensor of size 8x5 (GPU 0)]\r\n']",[],1,1
30,pytorch,26797,closed,Inconsistent gradient from different backend of CTCLoss,"## üêõ Bug

I'm switching to the cudnn backend of CTCLoss since the other is not fully reproducible,
however, it turns out that the exact same model that used to work with pytorch's cuda backend ctc loss now failed.
With some simple example, I found that there's a huge difference in gradient (both direction and magnitude) between two backends.
I'm not sure if the bug is on pytorch or cudnn, but as far as I know, TensorFlow also used CTC from cudnn and there is no similar issue.
Thanks in advance.

## To Reproduce

## Behavior



## Environment




cc @ezyang @gchanan @zou3519",module: cudnn module: loss triaged,"['Likely related issues:\r\nhttps://github.com/pytorch/pytorch/issues/25833\r\nhttps://github.com/pytorch/pytorch/issues/17798\r\nhttps://github.com/pytorch/pytorch/issues/12201\r\n\r\nI can reproduce this. @t-vi Do you know if this difference is expected?', ""I think this and #25833 are the same, the others I'm not so sure about.""]","['\r\nimport torch\r\ntorch.manual_seed(0)\r\ntorch.backends.cudnn.deterministic = True\r\ntorch.backends.cudnn.benchmark = False\r\n\r\nbatch_size = 16\r\nseq_len = 50\r\ntarget_len = seq_len//2\r\nlatent_dim = 128\r\nvocab_size = 50\r\n\r\nx = torch.randn((seq_len,batch_size,latent_dim)).cuda()\r\ny = torch.randint(1,vocab_size,(batch_size*target_len,),dtype=torch.long).cuda()\r\nx_len = seq_len*torch.ones((batch_size,),dtype=torch.long).cuda()\r\ny_len = target_len*torch.ones((batch_size,),dtype=torch.long).cuda()\r\nw = torch.nn.Linear(latent_dim,vocab_size).cuda()\r\n\r\ndef compute_ctc(x,y,x_len,y_len,use_cudnn):\r\n    for p in w.parameters():\r\n        if p.grad is not None:\r\n            p.grad.zero_()\r\n    # Forward\r\n    output = w(x).log_softmax(dim=-1)\r\n    if use_cudnn:\r\n        loss = torch.nn.functional.ctc_loss(output,\r\n                                            y.to(\'cpu\',torch.int32),\r\n                                            x_len.to(\'cpu\',torch.int32),\r\n                                            y_len.to(\'cpu\',torch.int32))\r\n    else:\r\n        loss = torch.nn.functional.ctc_loss(output,y,x_len,y_len)\r\n    # backward\r\n    loss.backward()\r\n    m, b = w.parameters()\r\n    print(\'loss = {}\\ngrad_norm = {}\'.format(loss, m.grad.view(-1).norm()))\r\n    return m.grad.clone()\r\n\r\nprint(""===== Pytorch CTC ====="")\r\ntorch_grad = compute_ctc(x,y,x_len,y_len,False)\r\nprint(""===== Cudnn CTC ====="")\r\ncudnn_grad = compute_ctc(x,y,x_len,y_len,True)\r\nprint(""===== Grad diff. ====="")\r\nprint(""Cos Sim. = "",torch.nn.functional.cosine_similarity(torch_grad.view(-1),cudnn_grad.view(-1),dim=0))\r\nprint(""Magnitude  = "",cudnn_grad.view(-1).norm() / torch_grad.view(-1).norm())\r\n\r\n', ""\r\n===== Pytorch CTC =====\r\nloss = 6.073373794555664\r\ngrad_norm = 0.4746553599834442\r\n===== Cudnn CTC =====\r\nloss = 6.073373794555664\r\ngrad_norm = 10431.4375\r\n===== Grad diff. =====\r\nCos Sim. =  tensor(0.6508, device='cuda:0')\r\nMagnitude  =  tensor(21976.8672, device='cuda:0')\r\n"", '\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 430.50\r\ncuDNN version: /usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudnn.so.7\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.2\r\n[pip3] torch==1.2.0\r\n[pip3] torchaudio==0.3.0\r\n[pip3] torchvision==0.4.0\r\n[conda] Could not collect\r\n\r\n']",[],1,1
31,pytorch,20146,closed,CPU Memory leak when using weight_decay in libtorch,"## Bug
When using weight_decay in libtorch, CPU Memory usage is slowly increasing.

## To Reproduce
I used docker container ""nvidia/cuda:9.2-cudnn7-devel-ubuntu18.04"", stable libtorch(1.1) for cuda9.0. This phenomenon can be reproduced using the mnist example in the pytorch/example repository.

rewrite examples/cpp/mnist/mnist.cpp l.148



Because the speed of increase is very slow, it may be better to increase the number of epochs. This happens with both CPU learning and GPU learning.

## Environment
- OS: Ubuntu 18.04.2 LTS
- GCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
- CMake version: version 3.10.2
- CUDA runtime version: 9.2.148
- GPU models and configuration: GPU 0: GeForce RTX 2080 Ti
- Nvidia driver version: 410.104
- cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.1

## Additional context
This phenomenon happens also in cuda10.0 and libtorch nightly build for cuda10.0.",high priority module: cpp triaged,"[""this looks worrying on whether it's a cycle (C++ doesn't have a gc either). @yf225 @gchanan "", 'Adam suffers from this issue. I sent the same fix #23125.']","['cpp\r\n- model.parameters(), torch::optim::SGDOptions(0.01).momentum(0.5));\r\n+ model.parameters(), torch::optim::SGDOptions(0.01).momentum(0.5).weight_decay(1e-4));\r\n']",[],1,1
32,pytorch,5812,closed,memory leaky on DataLoader,"- OS: Ubuntu16.04
- PyTorch version: 0.3.1
- PyTorchNet version: 0.0.1
- How you installed PyTorch (conda, pip, source): conda
- Python version: 3.6.4
- CUDA/cuDNN version: CUDA 9.0.176/cuDNN 7.0.5.15

**The script to reproduce the bug:** 

**Error messages**

- The  of  is set to 64, then on epoch 1, before :
![screenshot from 2018-03-15 20-23-31](https://user-images.githubusercontent.com/9991443/37463453-30928920-2890-11e8-93d0-d0dbad043997.png)

- After , you could see the memory is grow up to about 3 times:
![screenshot from 2018-03-15 20-24-00](https://user-images.githubusercontent.com/9991443/37463507-5f8d2992-2890-11e8-9f7d-b6a754314c6a.png)

- Furthermore, if I change the  of  to 1000, then on epoch 1, before , you could see the memory use is just only **715MB**, it's strange:
<img width=""867"" alt=""qq20180315-225225 2x"" src=""https://user-images.githubusercontent.com/9991443/37471232-3815a042-28a4-11e8-89e5-b971a9e3af93.png"">

- After , you could see the memory haven't change:
<img width=""748"" alt=""qq20180315-225238 2x"" src=""https://user-images.githubusercontent.com/9991443/37471400-8e578920-28a4-11e8-8c51-34bb35c80e6c.png"">
",,"[""I don' think this is a bug in the dataloader, because it doesn't send the data to the GPU.\r\nIt's probably a bug either in your code or in torchnet."", ""@fmassa but you could see from `processor` function that I have called `cuda()`, so why I have called this but it don't send data to GPU? And according to  the docs of PyTorch\r\n> Variable API is nearly the same as regular Tensor API (with the exception of a couple in-place methods, that would overwrite inputs required for gradient computation). In most cases Tensors can be safely replaced with Variables and the code will remain to work just fine. Because of this, we‚Äôre not documenting all the operations on variables, and you should refer to `torch.Tensor` docs for this purpose.\r\n\r\nso it's fine by use `data = Variable(data).cuda()`, or it works only by call `data=Variable(data.cuda())`? I also tested this, it still got the problem, and I tested on CUDA8.0, the problem is still there. Further More, I have step through each line of code on `tnt` as it runs, it behaved just fine. So I can't figure out why that happen. "", ""@soumith @fmassa I also remove `tnt` to test, and the problem still there, **The script to reproduce the bug:**\r\n```python\r\nfrom torch import nn\r\nfrom torch.autograd import Variable\r\nfrom torch.optim import Adam\r\nfrom tqdm import tqdm\r\nimport torchvision.transforms as transforms\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision.datasets import MNIST\r\nimport torch.nn.functional as F\r\n\r\n\r\nclass MNISTNet(nn.Module):\r\n    def __init__(self):\r\n        super(MNISTNet, self).__init__()\r\n        self.features = nn.Sequential(\r\n            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, stride=2, padding=3),\r\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1),\r\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\r\n            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1),\r\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\r\n        )\r\n        self.classifier = nn.Linear(in_features=2 * 2 * 512, out_features=10)\r\n\r\n    def forward(self, x):\r\n        out = self.features(x)\r\n        out = out.view(out.size(0), -1)\r\n        classes = self.classifier(out)\r\n        return classes\r\n\r\n\r\nif __name__ == '__main__':\r\n    train_data = MNIST(root='../data/mnist', train=True, transform=transforms.ToTensor(), download=True)\r\n    train_dl = DataLoader(dataset=train_data, batch_size=64, shuffle=True, num_workers=4)\r\n    test_data =  MNIST(root='../data/mnist', train=False, transform=transforms.ToTensor(), download=True)\r\n    test_dl = DataLoader(dataset=test_data, batch_size=64, num_workers=4)\r\n\r\n    model = MNISTNet().cuda()\r\n    optimizer = Adam(model.parameters())\r\n\r\n    for epoch in tqdm(range(2)):\r\n        for data,labels in train_dl:\r\n            data = Variable(data).cuda()\r\n            labels = Variable(labels).cuda()\r\n            classes = model(data)\r\n            loss = F.cross_entropy(classes, labels)\r\n            optimizer.zero_grad()\r\n            loss.backward()\r\n            optimizer.step()\r\n\r\n        for data, labels in test_dl:\r\n            data = Variable(data).cuda()\r\n            labels = Variable(labels).cuda()\r\n            classes = model(data)\r\n```\r\nAnd I also tested the mnist example of PyTorch on [official examples](https://github.com/pytorch/examples/tree/master/mnist), when I change the batch size to 1000, the memory use is grow up a litter than the batch size is 64, it encounter the problem too."", 'The reason in your last snippet is that the loss is still in scope when you do the testing loop, so it holds up the whole computation graph. You can either delete it before the testing starts `del loss` and `del classes`, or put the train loop in a separate function (and thus the scope ends with the function)', ""@fmassa I have tested **put the train loop in a separate function** as you said, it still have the problem! **It is a bug of PyTorch, not my code wrong**. The script to reproduce the bug:\r\n```python\r\nfrom torch import nn\r\nfrom torch.autograd import Variable\r\nfrom torch.optim import Adam\r\nfrom tqdm import tqdm\r\nimport torchvision.transforms as transforms\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision.datasets import MNIST\r\nimport torch.nn.functional as F\r\n\r\n\r\nclass MNISTNet(nn.Module):\r\n    def __init__(self):\r\n        super(MNISTNet, self).__init__()\r\n        self.features = nn.Sequential(\r\n            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, stride=2, padding=3),\r\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1),\r\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\r\n            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1),\r\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\r\n        )\r\n        self.classifier = nn.Linear(in_features=2 * 2 * 512, out_features=10)\r\n\r\n    def forward(self, x):\r\n        out = self.features(x)\r\n        out = out.view(out.size(0), -1)\r\n        classes = self.classifier(out)\r\n        return classes\r\n\r\n\r\ndef train(model, optimizer):\r\n    train_data = MNIST(root='./', train=True, transform=transforms.ToTensor(), download=True)\r\n    train_dl = DataLoader(dataset=train_data, batch_size=64, shuffle=True, num_workers=4)\r\n\r\n    for data, labels in train_dl:\r\n        data = Variable(data).cuda()\r\n        labels = Variable(labels).cuda()\r\n        classes = model(data)\r\n        loss = F.cross_entropy(classes, labels)\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\r\n\r\n\r\ndef test(model):\r\n    test_data = MNIST(root='./', train=False, transform=transforms.ToTensor(), download=True)\r\n    test_dl = DataLoader(dataset=test_data, batch_size=64, num_workers=4)\r\n    for data, labels in test_dl:\r\n        data = Variable(data).cuda()\r\n        model(data)\r\n\r\n\r\nif __name__ == '__main__':\r\n    model = MNISTNet().cuda()\r\n    optimizer = Adam(model.parameters())\r\n    for epoch in tqdm(range(2)):\r\n        train(model, optimizer)\r\n        test(model)\r\n\r\n```"", ""I tried your snippet and I don't see a memory leak, but a small increase in memory usage when the testing part happens, but it then stabilizes after the first epoch.\r\n\r\nWhat I think is happening (but I might be wrong here) is that the caching allocator is not being able to recover part of the memory that was allocated during training, maybe because it got fragmented at a later stage of the network during training (for example during backprop) ?\r\n\r\nIn the end, I think the behavior you are seing is normal, and you'd see expected results if you cleared the caching allocator after training, but that would imply slowdowns on every epoch.\r\n\r\nI think this is not a bug actually."", ""@fmassa But the memory grow up to three times! Does it really normal? And I have asked more than five friends to test this code, we have tested it on PyTorch 0.2.0, 0.3.0, 0.3.1, CUDA8.0,CUDA9.0, NVIDIA GTX 1060, 1070, 1080, TITAN X, they all have saw this issue. The most strange thing is when the `batch_size` set to other number such as 100, it won't encounter this problem, but if `batch_size`  is 32,64,128,256, the problem happened, I don't think it's fine, it's really the natural behavior of PyTorch?"", ""The memory bump that I saw in your example went from something like 780MB to 800MB, so not really significative I'd say.\r\nAbout the difference in behavior on batch sizes, this might be cudnn choosing a different backend for some operations that requires different amount of memory."", ""Also the value you're seeing in `nvidia-smi` is not the amount of memory *used* by PyTorch tensors at any given moment. PyTorch almost never frees the GPU memory because it's expensive and you can often reuse older allocations. As @fmassa you can use `empty_cache()` to get rid of the excessive use after every epoch if this is important for some reason."", '@fmassa  @apaszke I saw a similar problem on [PyTorch discuss](https://discuss.pytorch.org/t/how-to-debug-causes-of-gpu-memory-leaks/6741) today.\r\n<img width=""798"" alt=""qq20180316-224408 2x"" src=""https://user-images.githubusercontent.com/9991443/37526893-9435bd04-296b-11e8-8630-c9d126275d6f.png"">\r\nAs I said before, if the `batch_size` is not 64, the memory won\'t grow up, I mean **won\'t grow up**, not a little, and for 64, it bumps from **581MB** to **1739MB**, it\'s a huge memory usage.', ""I'm sorry, I'm not sure what's going on on your system. I tried your snippet again without modification on PyTorch 0.3.1, and I only saw ~15BM more memory being allocated.\r\nAre you running your latest script, the one that does not use `tnt`?"", '@fmassa I tested the latest script I posted again now, and here are the screenshot, it still have this problem.\r\n<img width=""811"" alt=""qq20180316-232943 2x"" src=""https://user-images.githubusercontent.com/9991443/37530154-ea7d5caa-2973-11e8-89b4-cbea91305124.png"">\r\n<img width=""874"" alt=""qq20180316-232953 2x"" src=""https://user-images.githubusercontent.com/9991443/37530184-f545145c-2973-11e8-8366-5f86af86eba4.png"">\r\nYou could see from that the memory bumps from **581MB** to **1739MB**.', ""@leftthomas I've tried reproducing this on multiple machines with v0.3.1 from conda, cannot reproduce it. Maybe it's a problem on your nvidia driver or something. Can you help reproduce this in nvidia-docker so that we can help you."", ""@soumith , I think maybe it's about network, I have encounter another problem when I use PyTorch `vision` to load `FashionMNIST` data, and it downloads `MNIST`, not `FashionMNIST`, I have proposed this problem [here](https://github.com/pytorch/vision/issues/445), and it been confirmed as the 302 redirection by our university's cache server, solved on [here](https://github.com/zalandoresearch/fashion-mnist/issues/107). So I think maybe when I config my computer's system, CUDA, and PyTorch(or other computers in our university, they all reproduce this memory leaky problem), between this period, some files have to be downloaded is wrong, because of the 302 redirection by our university's cache server?\r\nFurther more, I asked my friends in Shanghai but not our university, to test this code, they all reproduce this problem, so maybe it not the problem of our university's cache server. You could ask some people in Shanghai to test it, I'm sure they will reproduce it."", '@leftthomas that might explain things. what is output of `print(torch.__version__)`', ""@soumith , it's `0.3.1.post2`. But I also tested PyTorch 0.2.0, 0.3.0, 0.3.1 and even build from source on the master 0.4 version."", ""@leftthomas Just saying this, because it hasn't been brought up yet. I had a similiar problem like you. I set my num_workers to 1 and it worked fine, albeit slower of course. But I didn't test this for many epochs yet. Maybe it's still accumulating memory, only slower.\r\n"", '@soumith It not network problem, I have tested it again, and I found the memory leaky problem happened on GTX 1070 and 1080, but not on GTX TITAN X.']","["" python\r\nfrom torch import nn\r\nfrom torch.autograd import Variable\r\nfrom torch.optim import Adam\r\nfrom torchnet.engine import Engine\r\nfrom tqdm import tqdm\r\nimport torchvision.transforms as transforms\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision.datasets import MNIST\r\n\r\nclass MNISTNet(nn.Module):\r\n    def __init__(self):\r\n        super(MNISTNet, self).__init__()\r\n        self.features = nn.Sequential(\r\n            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, stride=2, padding=3),\r\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1),\r\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\r\n            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1),\r\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\r\n        )\r\n        self.classifier = nn.Linear(in_features=2 * 2 * 512, out_features=10)\r\n\r\n    def forward(self, x):\r\n        out = self.features(x)\r\n        out = out.view(out.size(0), -1)\r\n        classes = self.classifier(out)\r\n        return classes\r\n\r\n\r\ndef get_iterator(mode):\r\n    data = MNIST(root='./', train=mode, transform=transforms.ToTensor(), download=True)\r\n    return DataLoader(dataset=data, batch_size=64, shuffle=mode, num_workers=4)\r\n\r\n\r\ndef processor(sample):\r\n    data, labels, training = sample\r\n    data = Variable(data).cuda()\r\n    labels = Variable(labels).cuda()\r\n    classes = model(data)\r\n    loss = loss_criterion(classes, labels)\r\n    return loss, classes\r\n\r\n\r\ndef on_sample(state):\r\n    state['sample'].append(state['train'])\r\n\r\n\r\ndef on_start_epoch(state):\r\n    state['iterator'] = tqdm(state['iterator'])\r\n\r\n\r\ndef on_end_epoch(state):\r\n    engine.test(processor, get_iterator(False))\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    model = MNISTNet().cuda()\r\n    loss_criterion = nn.CrossEntropyLoss().cuda()\r\n\r\n    optimizer = Adam(model.parameters())\r\n    engine = Engine()\r\n    engine.hooks['on_sample'] = on_sample\r\n    engine.hooks['on_start_epoch'] = on_start_epoch\r\n    engine.hooks['on_end_epoch'] = on_end_epoch\r\n\r\n    engine.train(processor, get_iterator(True), maxepoch=100, optimizer=optimizer)\r\n""]","['batch_size', 'DataLoader', 'on_end_epoch', 'on_end_epoch', 'batch_size', 'DataLoader', 'on_end_epoch', 'on_end_epoch']",1,0
33,pytorch,12501,open,C++: Calling Workspace::RunNet for a prediction on a different thread each time causes a GPU memory leak,"## üêõ Bug
Hi, 
I am trying to obtain a model prediction (on GPU) while running another piece of code in parallel (on CPU). Since I am streaming data, I instantiate a separate std::thread (or std::async) every time to call Workspace::RunNet, this causes a GPU memory leak which is not noticeable unless you are streaming data. However, if the thread used is maintained (a worker thread with the same thread_id), the leak does not occur.
Thank you,
<!-- A clear and concise description of what the bug is. -->

## To Reproduce
Steps to reproduce the behavior:
1. Given a pre-trained model, please use the following sample code to reproduce the issue:
<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->


## Expected behavior
Not expecting a memory leak when executing on threads with different thread ids.
<!-- A clear and concise description of what you expected to happen. -->

## Environment

## Additional context

<!-- Add any other context about the problem here. -->
",caffe2,"['I am also experiencing a similar bug on Windows. In my case, the GPU memory usage keeps increasing around 200MB per day for around 2 queries per second traffic. Just want to know how bad is your memory leak? Does it eat up your memory very fast, or increasing in slow process like I do?', 'Indeed, it is the same behavior, a slow process.', 'By running your code here, I found out the reason of my GPU memory leak, which is because I commented out the cudaStreamDestroy calls in ThreadLocalCUDAObject ([link](https://github.com/pytorch/pytorch/blob/49256ddb4a80d5385779c9d60c49004a39535d6a/caffe2/core/context_gpu.h#L118)). The reason I commented it, is that when the program is shutting down, the cudaStreamDestroy call will be after the CUDA driver shutting down, and error will be throw before the program exits. Instead of commenting that, now I use the following patch:\r\n```diff\r\n-      for (auto& stream : cuda_streams_[i]) {\r\n-        if (stream) {\r\n-          CUDA_CHECK(cudaStreamDestroy(stream));\r\n-        }\r\n-      }\r\n+      for (auto& stream : cuda_streams_[i]) {\r\n+        if (stream) {\r\n+          cudaError_t err = cudaStreamDestroy(stream);\r\n+          if (err != cudaErrorCudartUnloading) {\r\n+            CUDA_CHECK(err);\r\n+          }\r\n+        }\r\n+      }\r\n```\r\n\r\n Did you have any modification of caffe2 code, especially related with thread local variables?', 'Tried the code without modification to ThreadLocalCUDAObject. Now GPU memory looks okay, but the CPU memory keeps increasing... Anyway, I cannot reproduce this issue on Windows.', ""@harrysummer I did not modify the caffe2 code, I will try that patch and see if that works out. As far as the host memory is concerned, I haven't seen any problems on Linux so far.""]","['\r\n// Network setup\r\n\r\ncaffe2::NetDef init_net;\r\ncaffe2::NetDef predict_net;\r\nReadProtoFromFile(init_file, &init_net);\r\nReadProtoFromFile(predict_file, &predict_net);\r\nauto  workspace = std::make_unique<caffe2::Workspace>();\r\nconst auto cuda_device = caffe2::TypeToProto(caffe2::CUDA);\r\npredict_net.mutable_device_option()->set_device_type(cuda_device);\r\ninit_net.mutable_device_option()->set_device_type(cuda_device);\r\nworkspace->CreateBlob(predict_net_.external_input(0));\r\nworkspace->RunNetOnce(init_net);\r\nworkspace->CreateNet(predict_net);\r\n\r\n// Tensors setup\r\n\r\nauto input_host_tensor = std::make_unique<caffe2::TensorCPU>(dims, caffe2::CPU);\r\ninput_host_tensor->ShareExternalPointer(some_buffer);  // some_buffer will be filled with the input data\r\nauto input_device_tensor = std::make_unique<caffe2::TensorCUDA>(dims, caffe2::CUDA);\r\nworkspace->GetBlob(predict_net_.external_input(0))->Reset(input_device_tensor.get());\r\n \r\n// Prediction\r\n\r\nwhile(true) {\r\n   FetchData(some_buffer);\r\n   input_device_tensor->CopyFrom(*input_host_tensor);\r\n\r\n  // This is where the leak occurs\r\n   std::thread my_thread(&caffe2::Workspace::RunNet, workspace.get(), predict_net.name());\r\n\r\n  //\r\n  // some host code\r\n  //\r\n\r\n  my_thread.join();\r\n}\r\n\r\n', '\r\nLinux=Ubuntu 16.04.4 LTS\r\ncmake 3.5.1\r\ngcc 5.4.0\r\nNVIDIA CUDA 8.0\r\nNVIDIA cuDNN v6.0\r\nPytorch built from source (version to date is v1.0rc1)\r\n']",[],1,0
34,pytorch,19163,open,LayerNorm is very slow (almost frozen) in CPU of multiprocessing,"The context of the use case is doing ES over small network in multiprocessing. 

It turns out that with  it becomes extremely slow on CPU.

To reproduce the effect, the code is attached below:


The benchmark is:

- **:**

- **:**

**Completely frozen after waiting a few minutes, and  shows all 80 cores CPU are 100% busy**


- **:**


- **:**



",module: cpu triaged,"['Potential solution for pytorch team, repeat the benchmark locally and see if it still happens. Follow up later during the triage meeting.', ""@zuoxingdong this is a very common issue when running **muti-thread** or **muti-process** CPU application which is `OpenMP` paralleled. The issue has a technical name `over subscription`.\r\n\r\nKeep in mind that PyTorch uses `OpenMP` to parallel computation on multi core CPUs, the `OpenMP` thread pool will spawn as many as number of logical cores by default. So when you put PyTorch inside multi-processing environment, in your case `ProcessPoolExecutor`, each worker has its own `OpenMP` thread pool and each pool will spawn threads of number of logical cores. And they compete for CPU resource... Typical disaster for CPU performance...\r\n\r\nthe solution would be fairly simple, regulate how many core you want each worker to use:\r\na few lines will do the job:\r\n```bash\r\n@@ -12,6 +12,10 @@ global use_ln\r\n use_ln = True\r\n global device\r\n device = 'cpu'\r\n+global total_cores\r\n+total_cores = torch.get_num_threads()\r\n+\r\n+total_workers = 16\r\n\r\n class Model(nn.Module):\r\n     def __init__(self, use_ln):\r\n@@ -39,7 +43,9 @@ class Model(nn.Module):\r\n\r\n\r\n def initializer():\r\n-    print(f'Initializing, PID: {os.getpid()}')\r\n+    local_cores = int(total_cores / total_workers)\r\n+    torch.set_num_threads(local_cores)\r\n+    print(f'Initializing, PID: {os.getpid()}, Using: {local_cores} cores')\r\n     t = time.perf_counter()\r\n     global model\r\n     model = Model(use_ln).to(device)\r\n@@ -54,7 +60,7 @@ def fitness(solution):\r\n     print(f'PID: {os.getpid()}, finished')\r\n\r\n\r\n-with ProcessPoolExecutor(max_workers=16, initializer=initializer) as executor:\r\n+with ProcessPoolExecutor(max_workers=total_workers, initializer=initializer) as executor:\r\n     t = time.perf_counter()\r\n     num_param = sum([param.numel() for param in Model(use_ln).parameters() if param.requires_grad])\r\n     solutions = [torch.randn(num_param) for _ in range(16)]\r\n```\r\n\r\nHere is result on my machine `Xeon Skylake 6148: 20x2 cores @2.5GHz`:\r\nWithout the patch (i just get the final output of the log):\r\n```\r\nTotal time: 59.50154646998271\r\n```\r\nWith the patch:\r\n```\r\nTotal time: 2.624945448944345\r\n```\r\n\r\nSo this topic is irrelevant to `LayerNorm`, aside from the topic, your workload or benchmark doesn't have enough computation density to fully utilize CPUs like Xeon. Which means it might not be beneficial to use muti-core for each of the worker at all, because the work is too small...\r\nSo in that case, you run the python script with `OMP_NUM_THREADS=16` (each worker will get 1 core only):\r\n```\r\nOMP_NUM_THREADS=16 python test_layer_norm.py\r\n```\r\nresult is even better:\r\n```\r\nTotal time: 2.012942871078849\r\n```\r\n--------(Hua Li Li de Fen Ge Xian)---------\r\nIndeed i was drawn to the topic by `LayerNorm` because i am optimizing it lately. The raw performance of `LayerNorm` on CPU is not good enough, but this topic has more to complain about for `over subscription`\r\n\r\nJust curious, why you choose `ProcessPoolExecutor` over `torch.multiprocessing`?"", '@mingfeima Thank you so much for your solution, it works perfectly ! Because in evolution strategies, we sample a set of network parameters and evaluate them independently, to my understanding, `torch.multiprocessing` is designed for sharing parameters ? Do you think there could be even further speedup when replacing `ProcessPoolExecutor` ? ', 'Oh, i see, evolution strategies... Well, you probably need to regulate environment variables when running CPU (e.g. how `OpenMP` threads map to physical cores):\r\n```bash\r\nexport OMP_NUM_THREADS=[num_of_physical_cores]\r\nexport KMP_BLOCKTIME=1\r\nexport KMP_AFFINITY=granularity=fine,compact,1,0\r\n```\r\n\r\nSuppose you have enough `jobs` to fill into the multi process pool and no synchronization needed, using sequential thread would be most efficient from computation level. Here I assume your machine has 2x20 physical cores. Using `total_workers=40` (each worker runs sequentially) would probably have the highest overall throughput. You can do a simple scaling on `total_workers` to find out which is the best setting.\r\n\r\nAnd in case sharing parameters is not your intention, essentially there should be no big difference between `torch.multiprocessing` and `ProcessPoolExecutor`. But i am so sure about this, have no solid data here.\r\n\r\nAfter you have fixed the **over subscription**, additional speedup will depend on operator level optimization. e.g. what kind of operators did you use in the model (and also the size of it). This goes back to the original point, `LayerNorm` is indeed slow on CPU. Question is what else?', 'this advice help me,thanks !']","[""python\r\nimport os\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.nn.utils import vector_to_parameters\r\nfrom concurrent.futures import ProcessPoolExecutor\r\nimport time\r\nimport numpy as np\r\n\r\n\r\nglobal use_ln\r\nuse_ln = True\r\nglobal device\r\ndevice = 'cpu'\r\n\r\nclass Model(nn.Module):\r\n    def __init__(self, use_ln):\r\n        super().__init__()\r\n        \r\n        self.use_ln = use_ln\r\n        \r\n        self.fc1 = nn.Linear(17, 64)\r\n        if use_ln:\r\n            self.ln1 = nn.LayerNorm(64)\r\n        self.fc2 = nn.Linear(64, 64)\r\n        if use_ln:\r\n            self.ln2 = nn.LayerNorm(64)\r\n        self.out = nn.Linear(64, 6)\r\n    \r\n    def forward(self, x):\r\n        if self.use_ln:\r\n            x = self.ln1(F.relu(self.fc1(x)))\r\n            x = self.ln2(F.relu(self.fc2(x)))\r\n        else:\r\n            x = F.relu(self.fc1(x))\r\n            x = F.relu(self.fc2(x))\r\n        x = torch.tanh(self.out(x))\r\n        return x\r\n\r\n\r\ndef initializer():\r\n    print(f'Initializing, PID: {os.getpid()}')\r\n    t = time.perf_counter()\r\n    global model\r\n    model = Model(use_ln).to(device)\r\n    print(f'Finish initializing, PID: {os.getpid()}, taken {time.perf_counter() - t} s')\r\n    \r\ndef fitness(solution):\r\n    vector_to_parameters(solution.to(device), model.parameters())\r\n    for i in range(10):\r\n        for t in range(1000):\r\n            observation = np.random.randn(1, 17)\r\n            action = model(torch.from_numpy(observation).float().unsqueeze(0).to(device))\r\n    print(f'PID: {os.getpid()}, finished')\r\n        \r\n        \r\nwith ProcessPoolExecutor(max_workers=16, initializer=initializer) as executor:\r\n    t = time.perf_counter()\r\n    num_param = sum([param.numel() for param in Model(use_ln).parameters() if param.requires_grad])\r\n    solutions = [torch.randn(num_param) for _ in range(16)]\r\n    list(executor.map(fitness, solutions))\r\n    print(f'Total time: {time.perf_counter() - t}')\r\n"", '\r\nInitializing, PID: 2009192\r\nFinish initializing, PID: 2009192, taken 0.001776786521077156 s\r\nInitializing, PID: 2009194\r\nInitializing, PID: 2009196\r\nFinish initializing, PID: 2009194, taken 0.001952311024069786 s\r\nFinish initializing, PID: 2009196, taken 0.0018356721848249435 s\r\nInitializing, PID: 2009203\r\nFinish initializing, PID: 2009203, taken 0.0015176795423030853 s\r\nInitializing, PID: 2009210\r\nFinish initializing, PID: 2009210, taken 0.0021672528237104416 s\r\nInitializing, PID: 2009216\r\nInitializing, PID: 2009226\r\nFinish initializing, PID: 2009216, taken 0.0018634404987096786 s\r\nFinish initializing, PID: 2009226, taken 0.001307731494307518 s\r\nInitializing, PID: 2009232\r\nInitializing, PID: 2009237\r\nFinish initializing, PID: 2009232, taken 0.0021293386816978455 s\r\nFinish initializing, PID: 2009237, taken 0.0014764349907636642 s\r\nInitializing, PID: 2009242\r\nFinish initializing, PID: 2009242, taken 0.0015203002840280533 s\r\nInitializing, PID: 2009253\r\nFinish initializing, PID: 2009253, taken 0.0014950130134820938 s\r\nInitializing, PID: 2009259\r\nInitializing, PID: 2009262\r\nFinish initializing, PID: 2009259, taken 0.002156071364879608 s\r\nInitializing, PID: 2009269\r\nFinish initializing, PID: 2009272, taken 0.0021638404577970505 s\r\nFinish initializing, PID: 2009262, taken 0.0012727994471788406 s\r\nFinish initializing, PID: 2009269, taken 0.0021071340888738632 s\r\nInitializing, PID: 2009272\r\nInitializing, PID: 2009280\r\nFinish initializing, PID: 2009280, taken 0.0012570079416036606 s\r\nPID: 2009259, finished\r\nPID: 2009232, finished\r\nPID: 2009192, finished\r\nPID: 2009242, finished\r\nPID: 2009216, finished\r\nPID: 2009196, finished\r\nPID: 2009280, finished\r\nPID: 2009203, finished\r\nPID: 2009210, finished\r\nPID: 2009262, finished\r\nPID: 2009226, finished\r\nPID: 2009253, finished\r\nPID: 2009272, finished\r\nPID: 2009237, finished\r\nPID: 2009194, finished\r\nPID: 2009269, finished\r\nTotal time: 3.3697756361216307\r\n', '\r\nInitializing, PID: 2013343\r\nInitializing, PID: 2013346\r\nFinish initializing, PID: 2013343, taken 0.002532508224248886 s\r\nInitializing, PID: 2013354\r\nInitializing, PID: 2013359\r\nFinish initializing, PID: 2013346, taken 0.001596800982952118 s\r\nFinish initializing, PID: 2013354, taken 0.002076219767332077 s\r\nFinish initializing, PID: 2013359, taken 0.0015829745680093765 s\r\nInitializing, PID: 2013364\r\nFinish initializing, PID: 2013364, taken 0.0018675383180379868 s\r\nInitializing, PID: 2013369\r\nInitializing, PID: 2013372\r\nFinish initializing, PID: 2013369, taken 0.00201280415058136 s\r\nFinish initializing, PID: 2013372, taken 0.0018042325973510742 s\r\nInitializing, PID: 2013379\r\nFinish initializing, PID: 2013379, taken 0.0017306245863437653 s\r\nInitializing, PID: 2013384\r\nFinish initializing, PID: 2013384, taken 0.0018676463514566422 s\r\nInitializing, PID: 2013389\r\nInitializing, PID: 2013392\r\nFinish initializing, PID: 2013389, taken 0.0027825143188238144 s\r\nFinish initializing, PID: 2013392, taken 0.0016419757157564163 s\r\nInitializing, PID: 2013397\r\nFinish initializing, PID: 2013397, taken 0.0018332768231630325 s\r\nInitializing, PID: 2013402\r\nFinish initializing, PID: 2013402, taken 0.0018747877329587936 s\r\nInitializing, PID: 2013409\r\nFinish initializing, PID: 2013409, taken 0.0016566626727581024 s\r\nInitializing, PID: 2013414\r\nFinish initializing, PID: 2013414, taken 0.0016422048211097717 s\r\nInitializing, PID: 2013419\r\nFinish initializing, PID: 2013419, taken 0.0017681997269392014 s\r\n\r\n...\r\n', '\r\nInitializing, PID: 2010102\r\nInitializing, PID: 2010103\r\nInitializing, PID: 2010108\r\nInitializing, PID: 2010111\r\nInitializing, PID: 2010114\r\nInitializing, PID: 2010117\r\nInitializing, PID: 2010119\r\nInitializing, PID: 2010122\r\nInitializing, PID: 2010126\r\nInitializing, PID: 2010129\r\nInitializing, PID: 2010132\r\nInitializing, PID: 2010135\r\nInitializing, PID: 2010138\r\nInitializing, PID: 2010141\r\nInitializing, PID: 2010144\r\nInitializing, PID: 2010147\r\nFinish initializing, PID: 2010111, taken 17.613565219566226 s\r\nFinish initializing, PID: 2010102, taken 17.802420677617192 s\r\nFinish initializing, PID: 2010114, taken 17.798802657052875 s\r\nFinish initializing, PID: 2010103, taken 17.81860247440636 s\r\nFinish initializing, PID: 2010117, taken 17.89303245767951 s\r\nFinish initializing, PID: 2010108, taken 17.97427663579583 s\r\nFinish initializing, PID: 2010132, taken 18.15790667384863 s\r\nFinish initializing, PID: 2010126, taken 18.16597461514175 s\r\nFinish initializing, PID: 2010141, taken 18.14934244006872 s\r\nFinish initializing, PID: 2010122, taken 18.205005967989564 s\r\nFinish initializing, PID: 2010119, taken 18.209256762638688 s\r\nFinish initializing, PID: 2010129, taken 18.243990190327168 s\r\nFinish initializing, PID: 2010147, taken 18.25442030467093 s\r\nFinish initializing, PID: 2010135, taken 18.283051615580916 s\r\nFinish initializing, PID: 2010138, taken 18.324560744687915 s\r\nFinish initializing, PID: 2010144, taken 18.545221898704767 s\r\nPID: 2010111, finished\r\nPID: 2010114, finished\r\nPID: 2010102, finished\r\nPID: 2010103, finished\r\nPID: 2010117, finished\r\nPID: 2010108, finished\r\nPID: 2010126, finished\r\nPID: 2010122, finished\r\nPID: 2010141, finished\r\nPID: 2010119, finished\r\nPID: 2010132, finished\r\nPID: 2010147, finished\r\nPID: 2010129, finished\r\nPID: 2010135, finished\r\nPID: 2010138, finished\r\nPID: 2010144, finished\r\nTotal time: 35.38426893763244\r\n', '\r\nInitializing, PID: 2010657\r\nInitializing, PID: 2010658\r\nInitializing, PID: 2010663\r\nInitializing, PID: 2010666\r\nInitializing, PID: 2010669\r\nInitializing, PID: 2010670\r\nInitializing, PID: 2010675\r\nInitializing, PID: 2010678\r\nInitializing, PID: 2010679\r\nInitializing, PID: 2010684\r\nInitializing, PID: 2010685\r\nInitializing, PID: 2010690\r\nInitializing, PID: 2010693\r\nInitializing, PID: 2010694\r\nInitializing, PID: 2010698\r\nInitializing, PID: 2010702\r\nFinish initializing, PID: 2010657, taken 16.785767970606685 s\r\nFinish initializing, PID: 2010658, taken 17.48567765392363 s\r\nFinish initializing, PID: 2010663, taken 17.629254495725036 s\r\nFinish initializing, PID: 2010679, taken 17.669835798442364 s\r\nFinish initializing, PID: 2010684, taken 17.806519005447626 s\r\nFinish initializing, PID: 2010666, taken 17.94813333079219 s\r\nFinish initializing, PID: 2010678, taken 17.9285822045058 s\r\nFinish initializing, PID: 2010675, taken 17.933384394273162 s\r\nFinish initializing, PID: 2010685, taken 17.93839507550001 s\r\nFinish initializing, PID: 2010669, taken 17.989747492596507 s\r\nFinish initializing, PID: 2010670, taken 18.010178688913584 s\r\nFinish initializing, PID: 2010690, taken 18.07465230114758 s\r\nFinish initializing, PID: 2010698, taken 18.061042606830597 s\r\nFinish initializing, PID: 2010693, taken 18.082385893911123 s\r\nFinish initializing, PID: 2010702, taken 18.17208438925445 s\r\nFinish initializing, PID: 2010694, taken 18.283266445621848 s\r\nPID: 2010657, finished\r\nPID: 2010658, finished\r\nPID: 2010663, finished\r\nPID: 2010679, finished\r\nPID: 2010684, finished\r\nPID: 2010678, finished\r\nPID: 2010675, finished\r\nPID: 2010685, finished\r\nPID: 2010670, finished\r\nPID: 2010669, finished\r\nPID: 2010666, finished\r\nPID: 2010693, finished\r\nPID: 2010698, finished\r\nPID: 2010690, finished\r\nPID: 2010702, finished\r\nPID: 2010694, finished\r\nTotal time: 39.30256709083915\r\n', '\r\n - PyTorch Version: 1.0.0.dev20190323\r\n - OS: Ubuntu 18.04\r\n - How you installed PyTorch: pip\r\n - Python version: 3.7\r\n']","['LayerNorm', ""use_ln=False, device='cpu'"", ""use_ln=True, device='cpu'"", 'htop', ""use_ln=False, device='cuda'"", ""use_ln=True, device='cuda'""]",1,0
35,pytorch,1253,closed,Memory usage increasing after first batch (possibly not freeing some potentially free memory),"This may be related to issue #1184 but I believe there is something else to it:
I am working on version , using micro batches: split mini-batches into smaller batches and calculate/backprop loss in each micro batch, accumulating it over the mini batch. I noticed multiple times that my code goes out of memory after several micro batches. This seems strange to me as each micro batch should require the same amount of memory. Maybe there is something I don't fully understand here, so if anyone can explain it to me, I'd be happy !

Anyway, I tried to look into it a bit and attached is the smallest code snippet I came up with demonstrating my problem (around 100 lines).
Essentially, I am training a network on images, using micro-batches to reduce memory. However, after the first (and sometimes second) micro-batch, the memory usage increases and then stays the same after further micro-batches.

When using a functional approach for the mini and micro batches, the memory usage does not increase the same way, so I don't believe that this has to do with the fact that there may be 'lazy' memory freeing involved (freeing only when necessary).

[torch_memory.zip](https://github.com/pytorch/pytorch/files/919384/torch_memory.zip)",,"[""Also, is there a utility to see which tensor requires how much memory on GPU and if it can be free'd etc ? That would be really useful to investigate this kind of issue in the future. The actual code I want to debug here is much more complex and I have no idea which part requires how much memory, to know what I could do to optimize memory usage..."", ""It's not related, unless you keep holding on to Variables you won't ever need again (it's better to only save their `.data` once they're not needed). There's no such utility right now. Can you please paste your code into a gist instead of attaching a zip?"", 'gist of his code https://gist.github.com/d5710d24833f6369663438e977b6ff40', ""Thanks for creating the gist. and of course, the input images can be easily replaced by random tensors of shape `(3, 224, 224)` if you don't have a directory of images available. I created a new gist for this, if it's easier for you: https://gist.github.com/MatthiasKohl/45163b8c13c52c495d8c25dc7d22016f"", ""I've looked into this a bit further and first found that simply running my (larger) code in an iPython kernel rather than directly using Python, the memory usage is quite different. So I assume that the garbage collection (or tracking memory allocations) are different in iPython vs Python. Unfortunately, I cannot easily demonstrate this because the smaller code I posted does not behave differently in Python vs iPython.\r\n\r\nI then simply used `gc.collect()` at the end of each micro-batch and my memory consumption dropped significantly (at least 4x) in a way that I don't go out of memory anymore. This is surprising to me since I assumed that any non-referenced Variable/Tensor should be overwritten sooner or later, so `gc.collect()` shouldn't have any impact. Anyway, if I find a more concise example of this behaviour, I will update you."", ""I looked into it and it seems all good to me. Your new gist uses 651MB in the first version and 642MB in the second. The increased memory usage comes from the the way Python uses scopes -- all iterations run in the global one, so when you compute the second one `loss`, `out`, etc. are still alive until they're overwritten, thus increasing the peak memory usage. If you add `del out, loss, train_in, labels_in` after `loss.backward()` the memory usage will go down to 642MB. In the second case, the variables go out of scope once the function returns, so the memory is freed before the second iteration starts.""]",[],['v0.1.11'],1,0
36,pytorch,5351,closed,Wrong automatic gradient for Linear-Layer (Autograd/gradcheck),"Minimal Example:

Output:

My Environment:
",,"['Are we overseeing something essential or is there really a bug in the current release? Sorry for the brevity, I forwarded the issue from a colleague via mobile.\r\n\r\nBeste wishes, \r\nMatthias ', ""You're running grad checks with single precision. If you add `.double()` calls it will pass."", '@apaszke Is there any way that gradcheck passes for float operations. I am working on adversarial attacks and due to this precision loss in adversarial attacks over iterations I am getting different output across iterations. If I use DoubleTensors it works but becomes very slow (3-4 times slower)']","['\r\nimport torch.utils.data\r\nimport torch.autograd\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.optim as optim\r\nfrom torch.autograd import Variable\r\nfrom torch.autograd import gradcheck\r\n\r\ntorch.manual_seed(123)\r\ninput = Variable(torch.rand(1,2), requires_grad = True)\r\ntest = gradcheck(nn.Linear(2,1), (input,), eps=1e-6, atol=1e-3)\r\nprint(test)\r\n', '\r\n numerical:(\r\n-0.3576\r\n 0.2682\r\n[torch.FloatTensor of size 2x1]\r\n,)\r\nanalytical:(\r\n-0.3512\r\n 0.2667\r\n[torch.FloatTensor of size 2x1]\r\n,)\r\n\r\n', '\r\nPython: (3, 5, 4)\r\nCuDNN: 7005\r\nCUDA: 8.0.61\r\nTorch: 0.3.1.post2\r\nNumpy: 1.13.3\r\n\r\n\r\n']",[],1,0
37,pytorch,29429,open,NLLLoss reduce=True returning nan in float16,"## üêõ Bug
**Version**
torch 1.3.1
torchvision 0.4.1

**Notes**
NLLLoss reduce=True doesn't seem to work in float16.
Also, training a model with loss1 in float16 doesn't seem to decrease the loss. (The model trains fine with loss1 in float32)
Possible related to #14878

**Code to reproduce**




",module: nn triaged,"['i think we need to tease out how much of this is due to the extremely limited precision and dynamic range of float16, versus there being an actual bug.', ""That is a good direction. I can look into it once I have time.\r\n\r\nThe following might give some insight to this issue.\r\nI was able to get the model to work by using the Nvidia's APEX Automatic Mixed Precision.\r\nIt **did not** raise any issues like `Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0`"", 'APEX AMP afaik will only do certain operations in fp16, and NLLLoss is not one of them. ']","[""\r\n#Input\r\ninput = torch.rand(32,11,256,256).cuda().half()\r\ntarget =torch.LongTensor(32,256,256).random_(0, 10).cuda()\r\nloss1 = nn.NLLLoss(reduce=False).cuda().half()\r\nloss2 = nn.NLLLoss(reduce=True, reduction='mean').cuda().half()\r\nprint(torch.mean(loss1(input,target)) )\r\nprint(loss2(input,target))\r\n"", ""\r\n#Output\r\ntensor(-0.4995, device='cuda:0', dtype=torch.float16)\r\ntensor(nan, device='cuda:0', dtype=torch.float16)\r\n""]",[],1,0
38,pytorch,13045,closed,dataparallel not working on nvidia gpus and amd cpus,"## üêõ Bug

We have a number of machines with Threadripper CPUs, and 2 NVIDIA GPUs, some have 1070ti cards some 1080 some 1080ti and one with titanXp, they all displayed this behavior, when switching to using data parallel, training would fail, i.e. accuracy would not go up. We first saw this in our code base, but it also happens on the imagnet example from the pytorch examples repo

## To Reproduce

Steps to reproduce the behavior:

1. run the imagnet example for the examples repo in pytorch with dataparallel

these error messages were found in the dmesg log:


[1118468.873266] nvidia 0000:0a:00.0: AMD-Vi: Event logged [IO_PAGE_FAULT domain=0x000f address=0x00000000ea13a000 flags=0x0020]
[1118468.942145] nvidia 0000:0a:00.0: AMD-Vi: Event logged [IO_PAGE_FAULT domain=0x000f address=0x00000000ea139068 flags=0x0020]
[1118468.942189] nvidia 0000:0a:00.0: AMD-Vi: Event logged [IO_PAGE_FAULT domain=0x000f address=0x00000000d0000040 flags=0x0020]
[1118468.942227] nvidia 0000:0a:00.0: AMD-Vi: Event logged [IO_PAGE_FAULT domain=0x000f address=0x00000000d00007c0 flags=0x0020]
[1118468.942265] nvidia 0000:0a:00.0: AMD-Vi: Event logged [IO_PAGE_FAULT domain=0x000f address=0x00000000d0001040 flags=0x0020]
[1118468.942303] nvidia 0000:0a:00.0: AMD-Vi: Event logged [IO_PAGE_FAULT domain=0x000f address=0x00000000d0000f40 flags=0x0020]
[1118468.942340] nvidia 0000:0a:00.0: AMD-Vi: Event logged [IO_PAGE_FAULT domain=0x000f address=0x00000000d00016c0 flags=0x0020]
[1118468.942377] nvidia 0000:0a:00.0: AMD-Vi: Event logged [IO_PAGE_FAULT domain=0x000f address=0x00000000d0002040 flags=0x0020]
[1118468.942414] nvidia 0000:0a:00.0: AMD-Vi: Event logged [IO_PAGE_FAULT domain=0x000f address=0x00000000d0001e40 flags=0x0020]
[1118468.942452] nvidia 0000:0a:00.0: AMD-Vi: Event logged [IO_PAGE_FAULT domain=0x000f address=0x00000000d00025c0 flags=0x0020]
[1118468.942489] AMD-Vi: Event logged [IO_PAGE_FAULT device=0a:00.0 domain=0x000f address=0x00000000d0003040 flags=0x0020]
[1118468.942525] AMD-Vi: Event logged [IO_PAGE_FAULT device=0a:00.0 domain=0x000f address=0x00000000d0002d40 flags=0x0020]
[1118468.942560] AMD-Vi: Event logged [IO_PAGE_FAULT device=0a:00.0 domain=0x000f address=0x00000000d00034c0 flags=0x0020]
[1118468.942596] AMD-Vi: Event logged [IO_PAGE_FAULT device=0a:00.0 domain=0x000f address=0x00000000d0004040 flags=0x0020]
[1118468.942632] AMD-Vi: Event logged [IO_PAGE_FAULT device=0a:00.0 domain=0x000f address=0x00000000d0003c40 flags=0x0020]
[1118468.942667] AMD-Vi: Event logged [IO_PAGE_FAULT device=0a:00.0 domain=0x000f address=0x00000000d00043c0 flags=0x0020]
[1118468.942703] AMD-Vi: Event logged [IO_PAGE_FAULT device=0a:00.0 domain=0x000f address=0x00000000d0005040 flags=0x0020]
[1118468.942739] AMD-Vi: Event logged [IO_PAGE_FAULT device=0a:00.0 domain=0x000f address=0x00000000d0004b40 flags=0x0020]
[1118468.942774] AMD-Vi: Event logged [IO_PAGE_FAULT device=0a:00.0 domain=0x000f address=0x00000000d00052c0 flags=0x0020]

## Expected behavior

accuracy would not pick up in most cases, it never picked up for the validation set. We managed to work around this problem by turning off IOMMU in the bios.

## Environment

PyTorch version: 0.4.1
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 16.04.5 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 2.7
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration:
GPU 0: GeForce GTX 1080 Ti
GPU 1: GeForce GTX 1080 Ti

Nvidia driver version: 384.130
cuDNN version: Probably one of the following:
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.0.5
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn_static.a

",,"[""I believe this is a duplicate of #1637. Specifically, see [this comment](https://github.com/pytorch/pytorch/issues/1637#issuecomment-338268158). I've had success on a threadripper machine by disabling IOMMU or changing the IOMMU setting to 'soft'."", 'cc @petrex from amd and @slayton58 from nvidia - can you guys take a look and make sure this is addressed correctly in the appropriate driver. ', ""@davidmascharka Yes I also figured out that turning off the hardware IOMMU via bios or kernel params solves the problem, It just took me a couple of very annoying weeks to pin down the source. Not the first place you go looking for a simple training session not converging. In my case it was even more confusing, as there was no crash or error messages, just very funky behavior. It's a bit sad that this bug has been known for so long and nobody is addressing it. I found mentions of this issue , i.e. CUDA on AMD platforms going years back."", 'thanks, @jspisak will bring this back to our teams and see what we can do.', 'EVERYONE My friend Jason J. Yu (https://github.com/JasonYuJjyu) got this under control.\r\n\r\nhttps://gph.is/2RXxlPP\r\n\r\nHe will save us. ', 'Closing this issue via https://github.com/pytorch/pytorch/issues/1637#issuecomment-338268158, \r\n@JaeDukSeo if you also another work around for it, please feel free to share it here so that more people can find it. Thanks!']",[],[],1,0
39,pytorch,30968,open,Categorical.sample too slow,"## üêõ Bug

Categorical.sample(shape) calls _shape_ times .
This is slow when we want to get many samples from a large number of classes.

## To Reproduce

Steps to reproduce the behavior:


Outputs : 

## Expected behavior
Shouldn't be slower than a single call to torch.multinomial

which outputs : 

## Environment
PyTorch version: 1.3.1
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 18.04.1 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.10.2

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration:
GPU 0: Quadro GP100
GPU 1: Quadro GP100

Nvidia driver version: 410.79
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] numpy==1.16.4
[pip] numpysane==0.17
[pip] torch==1.3.1
[conda] blas                      1.0                         mkl
[conda] faiss-gpu                 1.6.0            py37h1a5d453_0    pytorch
[conda] mkl                       2019.4                intel_243    intel
[conda] mkl_fft                   1.0.12           py37ha843d7b_0
[conda] mkl_random                1.0.2            py37hd81dba3_0
[conda] pytorch                   1.3.1           py3.7_cuda10.0.130_cudnn7.6.3_0    pytorch

cc @vincentqb @fritzo @neerajprad @alicanb @vishwakftw",module: distributions triaged,"['This appears to be an especially important instance of #11389.\r\nThis refactoring will also be necessary for #18906', 'For a large sample size, I also encountered this problem with the Normal distribution with PyTorch 1.2 on CUDA.\r\n```python\r\ndist = Normal(0,1)\r\nsample_shape = [256,200,20]\r\nslow_sample = dist.sample(sample_shape).to(device)\r\nfast_sample = torch.randn(sample_shape).to(device)\r\n```\r\nslow_sample is 50-100x slower than the other variant. ', 'Seems related, although sampling dimensions explicit in the distribution parameterization:\r\n\r\n```python\r\nstart = time.time()\r\nBinomial(total_count=nSamples.expand([nGenes, *nSamples.shape]), probs=probs).sample()\r\nprint(""final sampling took"", time.time() - start)\r\n```\r\n\r\nnSamples after expansion has shape [20_000, 3, 2], probs is shape [20_000, 3, 2]\r\n\r\nTakes 3 minutes. I would expect generating 120,000 values to take on the order of .1s', 'Both `torch.distributions.Categorical` and `torch.multinomial` are slower than a sampler I wrote (see [this numpy issue](https://github.com/numpy/numpy/issues/15201)):\r\n\r\n```python3\r\nfrom torch import rand, multinomial\r\nfrom torch.distributions import Categorical\r\nfrom timeit import default_timer as timer\r\n\r\ndef cat_1(p):\r\n    return Categorical(p).sample()\r\n\r\ndef cat_2(p):\r\n    return multinomial(p, 1)[:, 0]\r\n\r\ndef cat_3(p):\r\n    return (p.cumsum(-1) >= rand(p.shape[:-1])[..., None]).byte().argmax(-1)\r\n\r\np = rand(100, 100)\r\np /= p.sum(-1, keepdim=True)\r\n\r\nfor method in [cat_1, cat_2, cat_3]:\r\n    start = timer()\r\n    for _ in range(1000):\r\n        method(p)\r\n    end = timer()\r\n    print(end - start)\r\n```\r\n```\r\n0.139183147\r\n0.29912867099999996\r\n0.091817166\r\n```\r\n\r\nHaving to repeatedly instantiate a class just to sample from a distribution seems inefficient. Why not add a function like `torch.rand` and `torch.randn` with the fast sampler implemented above? Perhaps `torch.randcat`?']","['torch.multinomial(probas, 1, True)', 'py\r\nimport torch\r\nsampling = torch.distributions.categorical.Categorical(probs=torch.rand((10_000_000)))\r\n%timeit sampling.sample((100,))\r\n', '3.16 s ¬± 117 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)', 'py\r\nprobas = torch.rand(10_000_000)\r\n%timeit torch.multinomial(probas, 100, True)\r\n', '51.7 ms ¬± 1.31 ms per loop (mean ¬± std. dev. of 7 runs, 10 loops each)']",[],1,0
40,pytorch,27902,open,Slowdown due to thread specific model caching,"## üêõ Bug

PyTorch appears to have an issue with threads that causes it to not properly warm up models if executed by different threads  in succession. This is why when using multi-instances of the model on the same GPU, it does not provide a speedup and in some cases is even slower than a single instance.

## To Reproduce

**Steps to reproduce the behavior:**

I have a simple libtorch code snippet that reproduces this issue. In short all it does is run the same model (Resnet50 model from the torchvision  that was traced to produce a torchscript version of the same). The code was run on a Titian V with a batch size of 1.

1. Create model in main thread
2. Launch thread that runs model N times in a loop, reporting the runtime each time
3. Launch N threads that each run the model once, reporting the runtime each time


## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Is debug build: No
CUDA used to build PyTorch: 10.1.243

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: No
CUDA runtime version: 10.1.243
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.3

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.16.4
[pip] torch==1.2.0a0+afb7a16
[pip] torchtext==0.4.0
[pip] torchvision==0.3.0a0
[conda] magma-cuda100             2.5.0                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] torch                     1.2.0a0+afb7a16          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.3.0a0                  pypi_0    pypi


cc @ezyang @gchanan @zou3519 @suo",high priority oncall: jit triage review triaged,"['this looks like a good repro and a fix that would likely get a lot of long-tail ""serving"" perf unblocked. cc: @wanchaol @suo ', 'Has there been any progress on this?', 'looking into this, marking as high priority so we track it appropriately', 'Bump, looks like other users are also hitting this issue. ', '@suo Has there been any progress on this?', 'Hm, I thought our profiling executor should be smarter about sharing profiling and optimization info between threads. @gmagogsfm, @Krovatkin, @ZolotukhinM any commentary on this?', ""@suo we should, AFAIK. The profiling information is associated with a model (or rather with the GraphExecutor instance of the forward method of the model). It shouldn't matter which thread a model is run on as long as it has been warmed up. However, if we start an unprofiled/unwarmed model on N threads, N-Y (where Y is the number of profiling runs) profiling runs will be wasted and then the threads will probably be blocked on one thread that will compile the optimized version.\r\nI'll try to take a look to see if this is what happens here. \r\n ""]","['\r\n#include <torch/script.h>\r\n\r\n#include <pthread.h>\r\n#include <unistd.h>\r\n#include <chrono>\r\n#include <iostream>\r\n\r\nstruct inferStruct {\r\n  at::Tensor output_;\r\n  torch::jit::script::Module module_;\r\n  std::vector<torch::jit::IValue> inputs_;\r\n};\r\n\r\n// Run model 5 times\r\nvoid*\r\nNForwardPass(void* run)\r\n{\r\n  inferStruct* run_tmp = reinterpret_cast<inferStruct*>(run);\r\n  for (int i = 0; i < 5; i++) {\r\n    auto t1 = std::chrono::high_resolution_clock::now();\r\n    run_tmp->output_ = run_tmp->module_.forward(run_tmp->inputs_).toTensor();\r\n    auto t2 = std::chrono::high_resolution_clock::now();\r\n    auto duration =\r\n        std::chrono::duration_cast<std::chrono::microseconds>(t2 - t1).count();\r\n    std::cout << duration / 1000.0\r\n              << "" ms to run model in process thread (NForwardPass)""\r\n              << std::endl;\r\n  }\r\n}\r\n\r\n// Run model 1 time\r\nvoid*\r\nForwardPass(void* run)\r\n{\r\n  inferStruct* run_tmp = reinterpret_cast<inferStruct*>(run);\r\n  auto t1 = std::chrono::high_resolution_clock::now();\r\n  run_tmp->output_ = run_tmp->module_.forward(run_tmp->inputs_).toTensor();\r\n  auto t2 = std::chrono::high_resolution_clock::now();\r\n  auto duration =\r\n      std::chrono::duration_cast<std::chrono::microseconds>(t2 - t1).count();\r\n  std::cout << duration / 1000.0\r\n            << "" ms to run model in process thread (ForwardPass)"" << std::endl;\r\n}\r\n\r\nint\r\nmain()\r\n{\r\n  // Model loaded in main thread\r\n  torch::Device device_ = torch::Device(torch::kCUDA, 0);\r\n  std::string model_path = ""resnet50_libtorch.pt"";\r\n  torch::jit::script::Module module1 = torch::jit::load(model_path, device_);\r\n  std::vector<torch::jit::IValue> inputs_(1);\r\n  inputs_[0] = torch::zeros({1, 3, 224, 224}).to(device_);\r\n  // pre-warm model\r\n  at::Tensor output = module1.forward(inputs_).toTensor();\r\n\r\n  inferStruct run1;\r\n  pthread_t thread_id1;  //, thread_id2;\r\n\r\n  run1.module_ = module1;\r\n  run1.inputs_ = inputs_;\r\n\r\n  // Run model in main thread N times and report runtime each time\r\n  for (int i = 0; i < 5; i++) {\r\n    auto t1 = std::chrono::high_resolution_clock::now();\r\n    output = module1.forward(inputs_).toTensor();\r\n    auto t2 = std::chrono::high_resolution_clock::now();\r\n    auto duration =\r\n        std::chrono::duration_cast<std::chrono::microseconds>(t2 - t1).count();\r\n    std::cout << duration / 1000.0 << "" ms to run model in main thread""\r\n              << std::endl;\r\n  }\r\n  sleep(1);\r\n\r\n  // Run N threads and in each run model once and report runtime each time\r\n  std::cout << std::endl;\r\n  for (int i = 0; i < 5; i++) {\r\n    pthread_create(&thread_id1, NULL, ForwardPass, &run1);\r\n    pthread_join(thread_id1, NULL);\r\n  }\r\n  sleep(1);\r\n\r\n  // Run 1 thread and in it run the model N and report runtime each time\r\n  std::cout << std::endl;\r\n  pthread_create(&thread_id1, NULL, NForwardPass, &run1);\r\n  pthread_join(thread_id1, NULL);\r\n\r\n  return 0;\r\n}\r\n']",[],1,0
41,pytorch,18175,closed,[Caffe2] Does Caffe2 support distributed training with only CPU?,"I found this sentence on https://caffe2.ai/docs/distributed-training.html:
""We‚Äôre assuming that you‚Äôve successfully built Caffe2 and that you have a system with at least one GPU, but preferably more to test out the distributed features.""

Since I have to do some experiments about CPU performance, I wonder if I can use distributed training with CPU only cluster?",caffe2 oncall: distributed,"['@sth1997 You may use PyTorch `DistributedDataParallel`. See [docs](https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/distributed.py#L174).', 'Closing due to lack of activity']",[],[],1,0
42,pytorch,5388,open,Perf regression: indexing 1-d tensor,"After the Tensor/Variable merge we have a perf regression with indexing a 1-d tensor:

As of 6a2afe3



As of Tensor/Variable merge:



Note that indexing a Python list is even faster:



This affects things like:
https://github.com/pytorch/examples/blob/4ef2d4d0c8524372d0047e050065edcac665ce1a/word_language_model/data.py#L39-L46",in progress module: performance triaged,"[""@colesbury What's the current status of this? I can still see the perf regression at the current master, but this one is crossed out from the 0.4.0 issue tracker: #5673"", ""The issue remains, but won't be fixed until we reduce the overhead for creating Tensors. That won't happen until after the Caffe2 and PyTorch tensor representations are merged."", 'tracking high priority in umbrella issue: https://github.com/pytorch/pytorch/issues/20367.']","['python\r\n>>> import torch\r\n>>> x = torch.randn(1000)\r\n>>> %timeit x[0]\r\n435 ns ¬± 3.34 ns per loop (mean ¬± std. dev. of 7 runs, 1000000 loops each)\r\n>>> %timeit x[0] = 1\r\n449 ns ¬± 1.41 ns per loop (mean ¬± std. dev. of 7 runs, 1000000 loops each)\r\n', 'python\r\n>>> import torch\r\n>>> x = torch.randn(1000)\r\n>>> %timeit x[0]\r\n1000000 loops, best of 3: 1.05 ¬µs per loop\r\n>>> %timeit x[0] = 1\r\n100000 loops, best of 3: 4.02 ¬µs per loop\r\n', 'python\r\n>>> x = [0] * 1000\r\n>>> %timeit x[0]\r\n10000000 loops, best of 3: 36.3 ns per loop\r\n>>> %timeit x[0] = 1\r\n10000000 loops, best of 3: 37.7 ns per loop\r\n']",[],1,0
43,pytorch,26386,closed,pytorch inference fp16,The model is trained with fp32. I try to use .half() to change  layers and inputs to fp16. ActuallyÔºåit indeed accelerate the inference. But its acceleration effect is far away from twice  of fp32. My platform is nvidia tx2. Its compute capability is 6.2. It supports fp16 very well. So what I want to ask is whether fp16 cannot be twice as fast as fp32 in pytorch. Looking for your reply.Thank you.,,['please use https://discuss.pytorch.org for questions'],[],[],1,0
44,pytorch,27926,open,`num_batches_tracked` update in `_BatchNorm` forward should be a single scalar update on host regardless of the residence of the layer,"## üöÄ Feature
<!-- A clear and concise description of the feature proposal -->

 is single scalar that increments by 1 every time  is called on the  layer with both  &  set to true. Our current implementation stores it as a single-element buffer that resides on the same device as with the rest of its parameters/buffers.

We request the update & storage of  to be moved to host, despite the residence of the rest of parameters/buffers.

## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->

When we have a BN layer on accelerators (GPUs), every forward call that updates  triggers a single-element kernel launch, introducing unnecessary host overhead which could hurt our end-2-end perf in cpu bounded workload.

My last attempt to move  to the host from the device gives 0%~11% performance gain across some common problem sizes. #26550

## Pitch

<!-- A clear and concise description of what you want to happen. -->
We need a way that  would resides on device and be backward compatible for save/load modules. This involves relaxing some checks in python tests, which assumes that all values in  are buffers/parameters passed by reference.

## Alternatives

<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->

My implementation #26550 does not give full backward compatibility (failing tests and not loading  by reference) and I don't know how to do that easily without a big hammer (rewriting  in  inherited from ).
But it does support save/load module, as well as allow assignment using tensor/scalar.",enhancement module: cuda module: nn module: performance triaged,"[""Filing the issue to track this problem, as I'm currently not available to work on it.\r\ncc'ing @ngimel @ptrblck @csarofeen for visibility.""]",[],"['num_batches_tracked', 'forward', '_BatchNorm', 'training', 'track_running_states', 'num_batches_tracked', 'num_batches_tracked', 'num_batches_tracked', 'num_batches_tracked', 'state_dict', 'num_batches_tracked', '__setattr__', '_BatchNorm', 'nn.Module']",1,0
45,pytorch,3863,open,Considerable slowdown in Adam.step after a number of epochs with multiple losses,"I have a model with multiple outputs and, therefore, multiple losses. When training I accumulate the losses using retain_graph. Something along the lines of:



where input, output and target are dictionaries with the respective data for the different inputs and losses.

I am using Adam for the optimization. 
I've noticed that after a number of epochs, the running time of an epoch goes suddenly up from 7sec to 34sec.
I also noticed a slowdown of CPU usage in my computer (I haven't test this yet on the GPU). Memory usage doesn't seem to increase.

I profiled the code and I saw this (output from cProfile):

Normal epoch:

Slow epoch:

I've tested with other adaptive losses like Adagrad, and there I can't see the issue.
It seems to be related to this line of code in Adam.step():

Any ideas about why this is happening? It seems like suddenly the size of the accumulated gradient explodes, but  I can't see why.

cc @vincentqb @VitalyFedyunin @ngimel",awaiting response (this tag is deprecated) module: optimizer module: performance needs reproduction triaged,"['Could you share a snippet that can reproduce this? If possible, could you test this on GPU as well?\r\n Btw, which pytorch version are you using? ', 'Is the script start using swap memory after a few epochs? ', 'I also experienced this, with `Adam` and `Adamax` but not with `SGD`, `RMSprop`, or `Adagrad` optimizers, oddly.', 'Also when I add the following lines to the end of `Adamax.step()`:\r\n```\r\n    for k, v in state.items():\r\n        if not isinstance(v, int):\r\n            state[k] = v*0\r\n```\r\nThen (although it obviously makes the optimizer worse) the slowdown does not happen anymore.\r\n(Using CPU)']","['python\r\nself.zero_grad()\r\nfor output_label, output in self(input, target).items():\r\n    loss = self.loss(output, target[output_label])\r\n    loss.backward(retain_graph=True)\r\nself.optimizer.step()\r\n', ""\r\n       52    0.012    0.000    3.538    0.068 adam.py:30(step)\r\n      624    0.646    0.001    0.646    0.001 {method 'addcdiv_' of 'torch._C.FloatTensorBase' objects}\r\n"", ""\r\n       52    0.013    0.000   24.576    0.473 adam.py:30(step)\r\n      624   21.469    0.034   21.469    0.034 {method 'addcdiv_' of 'torch._C.FloatTensorBase' objects}\r\n"", '\r\np.data.addcdiv_(-step_size, exp_avg, denom)\r\n']",[],1,0
46,pytorch,4893,closed,GPU Softmax over last dimension of 3D tensor is slow,"Recently I profiled one of my models with  and was surprised to find that the softmax layers for an attention mechanism were the most expensive entries in the  column.

My original code looked something like this:


After some experimentation, I tried adding some transposes to the code:



This increased my overall model speed by around 10%, and now matrix multiplication is at the top of  (which is what I would expect).

Could this be considered a performance bug? I wonder if there is some way for the softmax cuda code to have comparable speed regardless of the softmax dimension.

(Sorry I don't have sample code at the moment; my actual code is deeply embedded in the current project I'm doing.)

System info
- OS: Linux
- PyTorch version: 0.3.0
- How you installed PyTorch (conda, pip, source): conda
- Python version: 3.6
- CUDA/cuDNN version: CUDA 9
- GPU models and configuration: K80",,"['Can you please give us some example sizes you tried to run on? We need this to do the benchmarks on our side', ""I'm applying softmax to batch_size x length x length tensors, where batch size is 8-150 (typically around 90, and always a multiple of 8) and length is 30-200.\r\n\r\nSome concrete sizes I sampled from running my code:\r\n```\r\ntorch.Size([88, 194, 194])\r\ntorch.Size([104, 160, 160])\r\ntorch.Size([96, 181, 181])\r\ntorch.Size([96, 178, 178])\r\n```"", ""I looked into it and can reproduce the same numbers. Transpose improves the performance 2x for me. I'll look into it, thanks for the report."", 'Thanks for looking into this!', 'fixed via #4973 ']","['\r\nself.softmax = nn.Softmax(dim=-1)\r\n# ...\r\n# attn is a 3D tensor (batch_size x length x length). Length is in the 30-200 range\r\nattn = self.softmax(attn)\r\n', '\r\nself.softmax = nn.Softmax(dim=1)\r\nattn = self.softmax(attn.transpose(1, 2)).transpose(1, 2)\r\n']","['nvprof', 'cuda_total_time', 'nvprof']",1,1
47,pytorch,3869,closed,Why the Dropout2d and BatchNorm2d's model.eval() result is poor,"Recent I'm doing a project using the elegant tool,pytorch. I use the dropout2d and batchnorm2d in my network.The trained model performance well in the test phase if I didn't using the model.eval(). However,when I use the model.eval(), the result is very pool. I really feel  confused.I sincerely hope the developer can help me solving this confusion. Thank you very much!",,"['decrease momentum value in BatchNorm layer to something small like 0.0001', ""use @tofigh- 's solution."", ""> Recent I'm doing a project using the elegant tool,pytorch. I use the dropout2d and batchnorm2d in my network.The trained model performance well in the test phase if I didn't using the model.eval(). However,when I use the model.eval(), the result is very pool. I really feel confused.I sincerely hope the developer can help me solving this confusion. Thank you very much!\r\n\r\nHave you solved this problem by decreasing the momentum in BatchNorm layer?""]",[],[],1,0
48,pytorch,5406,closed,BatchNorm behaves different in train() and eval(),"

Sample output:

Also in master:
",,"['as pointed out by @jakezhaojb, the following lines may answer the discrepancy:\r\n\r\nhttps://github.com/zdevito/ATen/blob/master/src/THNN/generic/BatchNormalization.c#L29-L62\r\n\r\n', 'This is expected. In eval(), BN uses tracked running stats, rather than data stats.', 'this is standard expected behavior. In `eval()` mode, BatchNorm does not rely on batch statistics but uses the `running_mean` and `running_std` estimates that it computed during it\'s training phase.\r\n\r\nThis is documented as well:\r\n<img width=""690"" alt=""screen shot 2018-02-25 at 11 02 13 pm"" src=""https://user-images.githubusercontent.com/1310570/36652947-0ab16f8c-1a80-11e8-9541-935b837b7d27.png"">\r\n', 'Hi, if i want to use batch statistic in inference phase, what should i do?', '@weizequan set your model to `.train()` mode if you want to use batch statistic in inference phase.', ""@soumith you set your model to .eval() mode for inference phase, isn't it ? "", 'yes, but @weizequan wants to use the ""training"" behavoir of using batch statistics rather than running_mean and running_std usage.', '> @weizequan set your model to `.train()` mode if you want to use batch statistic in inference phase.\r\n\r\nIn the testing phase, we cannot set model.train(), right? In practical scenarios, one model accepts one sample as input, therefore batch_size = 1.\r\n\r\n', '> this is standard expected behavior. In `eval()` mode, BatchNorm does not rely on batch statistics but uses the `running_mean` and `running_std` estimates that it computed during it\'s training phase.\r\n> \r\n> This is documented as well:\r\n> <img alt=""screen shot 2018-02-25 at 11 02 13 pm"" width=""690"" src=""https://user-images.githubusercontent.com/1310570/36652947-0ab16f8c-1a80-11e8-9541-935b837b7d27.png"">\r\n\r\nHello.  I can understand there is the difference. But, why is the difference so huge. I train a model with TRAINING_SET data with the setting model.train(). Then, I test the model also with the TRAINING_SET data with the setting model.eval and I get very bad performance. It is very weird. \r\n\r\nCould you please tell me why is the difference so huge, and how can I get that better? Like changing batch size, network structures, etc.? I am very confused and do not know how to solve it. What I want is to see reasonable results with the setting model.eval() enabled', 'See my comment here: https://discuss.pytorch.org/t/model-eval-gives-incorrect-loss-for-model-with-batchnorm-layers/7561/3\r\n', 'does group norm work in the same manner?  ', 'Do running means and var get updated in the eval() mode?', ""@rizhaocai, @soumith : I have never had the same issues using TensorFlow's batch norm layer, and I observe the same thing as you do in PyTorch. I found that **_TensorFlow and PyTorch uses different default parameters for momentum_** and epsilon. After changing to TensorFlow's default momentum value from **0.1 -> 0.01**, my model perform just as good in eval model as it does during training.\r\n\r\nI hope this helps someone, as it was quite annoying for me at the time. I've even trained with batch size 1 and it seems to work in eval mode during inference. Note that TensorFlow's momentum is 1 - PyTorch's in terms of interpretation."", 'Thank you @andreped. I agree with you.\r\nBut there is something really weired in pytorch eval mode with batch normalization. \r\nI set the momentum to 0.01 but it is still not like tensorflow.\r\nAre you setting batch norm to eval mode? Something like:\r\n\r\ndef set_bn_eval(m):\r\n    if isinstance(m, nn.modules.batchnorm._BatchNorm):\r\n        m.eval()\r\n\r\nmodel.apply(set_bn_eval)', ""> Thank you @andreped. I agree with you.\r\n> But there is something really weired in pytorch eval mode with batch normalization.\r\n> I set the momentum to 0.01 but it is still not like tensorflow.\r\n> Are you setting batch norm to eval mode? Something like:\r\n> \r\n> def set_bn_eval(m):\r\n> if isinstance(m, nn.modules.batchnorm._BatchNorm):\r\n> m.eval()\r\n> \r\n> model.apply(set_bn_eval)\r\n\r\nWell, the whole point was to be able to deploy the model in test mode. So yes, I modified the momentum, deployed it in test mode, and then it worked. \r\n\r\nWhen I didn't change the momentum, I was still able to run the model in train mode, and it worked somewhat. However, this is non-ideal for multiple reasons. Simple: model.eval() should be sufficient. \r\n\r\n@Iman1221: But what do you mean like tensorflow? Which model are you attempting to train on which task and data set?"", 'Hi, is anyone experiencing `NaN` values returned by the layer BatchNorm2d?\r\nMy model uses `batch size = 2` because of the size of the GPU memory and it happens in **eval mode** only\r\nThe input data is valid, no `NaN` values\r\n```\r\neps=1e-05\r\nmomentum momentum=0.1\r\n```']","[""python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\nprint(torch.__version__)\r\n\r\ndef get_data(cuda=False):\r\n    if cuda:\r\n        return torch.cuda.FloatTensor(32, 1, 19, 19)\r\n    else:\r\n        return torch.FloatTensor(32, 1, 19, 19)\r\n\r\nm = nn.Sequential(\r\n    nn.Conv2d(1, 3, kernel_size=3, padding=1),\r\n    nn.BatchNorm2d(3),\r\n    nn.ReLU()\r\n)\r\n\r\ncuda = True\r\nif cuda:\r\n    m.cuda()\r\n\r\nm.eval()\r\nprint(m._modules['1'].training)\r\n\r\nx = Variable(get_data(cuda))\r\ny1 = m.forward(x)\r\n\r\nm.train()\r\nprint(m._modules['1'].training)\r\ny2 = m.forward(x)\r\n\r\nerr = (y1 - y2).norm().data[0]\r\nif err > 1e-5:\r\n    print(err)\r\n"", '\r\n0.3.0.post4\r\nFalse\r\nTrue\r\n33.47104263305664\r\n', '\r\n0.4.0a0+0ef1038\r\nFalse\r\nTrue\r\n6.883443355560303\r\n']",[],1,0
49,pytorch,18722,closed,pytorch can't be as good as keras,"I use the same data„ÄÅ same data iter and same vgg16 network, but the pytorch's loss beame 12.3 while the keras's loss became 4.2:
pytorch loss log:
train Loss: 15.2963 val Loss: 8.0108
train Loss: 12.4626 val Loss: 9.5972
train Loss: 12.4143 val Loss: 6.9568
train Loss: 12.3993  val Loss: 8.3055
train Loss: 12.3956 val Loss: 6.8054

keras loss:
Epoch 1/20
402/402 [==============================] - 183s 455ms/step - loss: 11.6960 
Epoch 2/20
402/402 [==============================] - 181s 449ms/step - loss: 6.6525
Epoch 3/20
402/402 [==============================] - 181s 450ms/step - loss: 5.4451
Epoch 4/20
402/402 [==============================] - 186s 463ms/step - loss: 4.9515 
Epoch 5/20
402/402 [==============================] - 186s 462ms/step - loss: 4.6679 
Epoch 6/20
402/402 [==============================] - 186s 462ms/step - loss: 4.4298 

 The main code is as fallows:
pytorch:
model = nn.Sequential(*list(make_model('vgg16', num_classes=1, pretrained=True, input_size=INPUT_SIZE).children())[0][:],\
                      nn.AdaptiveAvgPool2d(1), 
                      Flatten(),nn.Sequential(nn.Linear(512, 1024, bias=True),nn.ReLU(), nn.Linear(1024, 1, bias=True))).cuda()
dataloaders = {'train':train_dl, 'val':valid_dl}
train_model(model, dataloaders, nn.L1Loss(), optim.Adam(model.parameters(),lr=0.001), 21)

keras:
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(input_shape[1], input_shape[0], 3))
    model = GlobalAveragePooling2D()(base_model.output)
    model = Dense(1024, activation='relu')(model)
    model = Dense(1, activation='linear')(model)

    model = Model(inputs=base_model.input, outputs=model)
    # model = base_model

    keras.callbacks.EarlyStopping(
    monitor='val_loss', 
    patience=0, 
    verbose=0, 
    mode='auto')
    model.compile(keras.optimizers.Adam(0.001), 'mape', metrics=['acc', 'mae', 'mse'])
model.fit_generator(train_iter, len_train//BS-1, 20,validation_data=valid_iter, validation_steps=len_valid//BS, callbacks=[early_stop])   ",,"[""Aren't the networks bound to be slightly different and the pretrained models bound to be very different. Not sure what you can take from that.\r\n\r\nAlso, Adam vs fit for optimising? Adam is meant to be fast, right?\r\nhttps://www.reddit.com/r/MachineLearning/comments/auvj3q/r_adabound_an_optimizer_that_trains_as_fast_as/\r\n\r\nToo many variables."", ""what's the code for `train_model`?"", ""There are any number of reasons why the training is diverging, and you haven't given us enough information to actually tell what your problem is. The best place to debug problems like this is in the forums: https://discuss.pytorch.org/""]",[],[],1,0
50,pytorch,17703,open,Training hangs when using DistributedDataParallel in two pod on two nodes ,"<!-- A clear and concise description of what the bug is. -->
Training hangs when using DistributedDataParallel in two pod on nodes.
The problem **occasionally** happens  and can not be reproduced each time. 
The pods were started by k8s using flannel network. The physical nodes are connected by 10Ge ethernet.

Training started normally, but hanged in the middle of training process.
![image](https://user-images.githubusercontent.com/41627739/53853400-4860ec80-4000-11e9-9929-678b7547f630.png)
Then there was no output again. The training process did not exit for servel hours.
We found that the GPU utilization  was always 100% on both two nodes.
![image](https://user-images.githubusercontent.com/41627739/53852922-a096ef00-3ffe-11e9-828c-6d38e8d5dd68.png)
After we added NCCL_DEBUG=WARN before the training command, we found the WARN as follows.
On on pod:
_worker0-0d9gnb:15:91 [3] include/socket.h 398 NCCL WARN Call to **write** failed : Connection reset  by peer.
worker0-0d9gnb:15:91 [3] transport.cu:153 NCCL WARN transport.cu:153 ->2 [Proxy thread error]_
On the other pod:
_worker0-0d9gnb:15:91 [3] include/socket.h 398 NCCL WARN Call to **recv** failed : Connection reset  by peer.
worker0-0d9gnb:15:91 [3] transport.cu:153 NCCL WARN transport.cu:153 ->2 [Proxy thread error]_
<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Environment
Training code is pull from https://github.com/pytorch/examples/tree/master/imagenet.
Model is Resnet. We used 8 V100s to train (4 on each node).
Batch size was set 8, 32 or 128 per GPU.
The network rate is about 500 MB/s to synchronize the gradients.

 - PyTorch Version (e.g., 1.0): both 1.0 and 0.4.1
 - OS (e.g., Linux): Ubunut 16.04
 - How you installed PyTorch (, , source): pip
 - Build command you used (if compiling from source): None
 - Python version: 3.6
 - CUDA/cuDNN version: CUDA 9.0/cuDNN 7.1.2
 - GPU models and configuration: V100 with NVLink
 - Any other relevant information:
",oncall: distributed triaged,"['Tracking this issue.', 'I encountered the same issue when using 1.1.0a0+84b264b. It happens rarely and randomly. I did not test it on the newest version since the newest version runs very slow (at least 2x slower).', 'This is fully reproducible to me', 'I too am able to reproduce this on pytorch 1.1.0(cuda 10, python 3.6, stable via pip) on 2 nodes with worldsize =4, ranks 0,1 on one machine and 2,3 on another connected via direct 10Gbe connection.\r\n\r\nThe same error was posted to NVIDIA/nccl bugtracker: https://github.com/NVIDIA/nccl/issues/193 It has some information that may be valuable to someone looking into this.\r\n\r\n-Paul', '@boxwh1 Have you got it work? Could you throw some light on your solution?']",[],"['conda', 'pip']",1,0
51,pytorch,27946,closed,torch.mean() is imprecise for very large Tensors,"## üêõ Bug

",module: random triaged,"[""Hi,\r\n\r\nI cannot reproduce that locally:\r\n```\r\nIn [1]: import torch\r\n\r\nIn [2]: torch.rand(6000000000).mean()\r\nOut[2]: tensor(0.5000)\r\n\r\nIn [3]: torch.rand(8000000000).mean()\r\nOut[3]: tensor(0.5000)\r\n\r\nIn [4]: torch.__version__\r\nOut[4]: '1.3.0a0+477f474'\r\n```\r\n\r\nAlso this could be accumulation errors, what does the following return on your system: `torch.rand(8000000000).double().mean()` ?"", ""sorry, I don't have enough memory to try the mean in double precision on the 8 billion-element tensor.\r\n64g total memory on my linux box.\r\n\r\nI see the skew on somewhat smaller tensors over on an imac with 32g memory, \r\nbut the mean looks good in double precision.\r\ne.g.\r\n\r\n```\r\n>>> torch.rand(3000000000).mean()\r\ntensor(0.1790)\r\n>>> torch.rand(3000000000).double().mean()\r\ntensor(0.5000, dtype=torch.float64)\r\n\r\n```\r\n\r\n"", 'I would guess that the mean does `t.sum().div(t.nelements())` and so if you have too many elements, you can loose precision when doing this.', '@meganset could you try `torch.rand(3000000000).sum()` and check if it is close to `1500000000`?', ""I would expect `torch.randn()` not to suffer from the same issue, even for `torch.randn(8000000000).mean()` as the sum will remain close to 0 and so there won't be precision loss."", 'Oh sorry, I meant `torch.rand`.', 'Thanks for looking at this.\r\nI retreated to c++ to verify the loss of precision in any long sum of 4-byte floats:\r\n\r\n```\r\nvoid f(int64_t n) {  // length, in millions\r\n n*=1000000;\r\n auto t=torch::rand(n);\r\n double d=0; float f=0,*p=t.data_ptr<float>();\r\n for(size_t i=0; i<n; ++i) d+=p[i], f+=p[i];\r\n std::cerr << ""double  sum: "" <<   d << ""\\n"";\r\n std::cerr << ""float   sum: "" <<   f << ""\\n\\n"";\r\n std::cerr << ""double mean: "" << d/n << ""\\n"";\r\n std::cerr << ""float  mean: "" << f/n << ""\\n"";\r\n std::cerr << ""torch  mean: "" << t.mean().item().toFloat() << ""\\n"";\r\n}\r\n```\r\nthe loss of precision in the sum begins to occur around 33 million elements,\r\nnoticeable by around 40 million, e.g.\r\n\r\n```\r\nf(33);\r\ndouble  sum: 1.64992e+07\r\nfloat   sum: 1.65002e+07\r\n\r\ndouble mean: 0.499975\r\nfloat  mean: 0.500007\r\ntorch  mean: 0.499975\r\n\r\nf(40);\r\ndouble  sum: 2.00009e+07\r\nfloat   sum: 1.67772e+07\r\n\r\ndouble mean: 0.500021\r\nfloat  mean: 0.41943   <<<\r\ntorch  mean: 0.500022\r\n```\r\nBut the torch _t.mean()_ is still correct, I assume it is using double precision in the sum.\r\nThe torch calculated mean will not go astray on my linux box until I try a tensor of around 14 billion elements:\r\n\r\n```\r\nf(14000);\r\ndouble  sum: 7.00001e+09\r\nfloat   sum: 1.67772e+07\r\n\r\ndouble mean: 0.500001\r\nfloat  mean: 0.00119837\r\ntorch  mean: 0.460175       <<<\r\n```\r\nThis boundary is a little mysterious, as it would seem that the double precision accumulation is still arriving at the correct sum & mean.\r\nAlso mysterious is why the python version starts to go wrong earlier:\r\non my linux box, the mean is off when I use _torch.rand(8 billion)_\r\nBut not on albanD\'s box..?\r\n\r\n', ""Interestingly, to speed up testing, I did the following and I see the same results as you in this case:\r\n\r\n```\r\nIn [1]: import torch\r\n\r\nIn [2]: a = torch.zeros(8000000000).add_(0.5)\r\n\r\nIn [3]: a.mean()\r\nOut[3]: tensor(0.4027)\r\n\r\nIn [4]: a.narrow(0, 0, 6500000000).mean()\r\nOut[4]: tensor(0.4956)\r\n\r\nIn [5]: a.narrow(0, 0, 6000000000).mean()\r\nOut[5]: tensor(0.5000)\r\n```\r\n\r\n> This boundary is a little mysterious, as it would seem that the double precision accumulation is still arriving at the correct sum & mean.\r\n\r\nThe answer here is that the mean is actually super fast (<1s) and so you probably don't see it but it uses multithreading ;)\r\n\r\n```\r\nIn [1]: import torch\r\n\r\nIn [2]: a = torch.zeros(int(1e9)).add_(0.5)\r\n\r\nIn [3]: a.mean()\r\nOut[3]: tensor(0.5000)\r\n\r\nIn [4]: torch.get_num_threads()\r\nOut[4]: 12\r\n\r\nIn [5]: torch.set_num_threads(1)\r\n\r\nIn [6]: a.mean()\r\nOut[6]: tensor(0.2684)\r\n```\r\n\r\nBasically, when the workload is big enough, we use multithreading. This has the side effect to perform several sums and then accumulate them. Leading to extra precision. This is why the precision will be different depending on the number of processors you have.\r\n"", 'Ah.\r\nI see now.\r\nThanks.\r\n\r\nMy issue title is misleading, should be something like\r\n""Summary statistics on large float tensors lose precision depending on number of threads used""\r\n\r\nThis also explains how c++ might differ from python if they use a different number of threads.\r\nI guess it\'s still a bug though, to get a different mean depending on how many threads are available.\r\nAnd _t.mean()_ should still be able to sum & divide in double precision.\r\n\r\nThe _torch.mean()_ function anticpates this by allowing a dtype=torch.double option,\r\nbut this seems to convert the whole tensor to double, rather than just the sum\r\n', 'I would argue that this is expected behavior. See https://github.com/pytorch/pytorch/issues/12215 for example for the same problem but with ByteTensor.\r\nThe cost of accumulating in double precision vs single precision is significant. So we prefer to leave it as an opt-in feature for the users that actually need it given that most users do not.', ""I'm going to argue that this is expected behavior as well. We've always given guidance that float32 is imprecise and is expected to be so."", ""I'm going to close base on the comment.\r\nFeel free to re-open if you think there is something more we could do here.""]","[""\r\n>>> torch.__version__\r\n'1.3.0'\r\n>>> torch.rand(6000000000).mean()\r\ntensor(0.5000)\r\n>>> torch.rand(6500000000).mean()\r\ntensor(0.4956)\r\n>>> torch.rand(8000000000).mean()\r\ntensor(0.4027)\r\n""]",[],1,0
52,pytorch,25904,open,Python hang after using torch.exp(),"## üêõ Bug

Using function torch.exp() makes python hang. I test some other functions like torch.sum(), torch.abs(), these functions can return right results.
<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:


Python just hang, python cannot be closed by using CTRL+C under Ubuntu 16.04.

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

Out[2]: Tensor([1, 2])
<!-- A clear and concise description of what you expected to happen. -->

## Environment

Collecting environment information...
PyTorch version: 1.2.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 16.04.5 LTS
GCC version: (Ubuntu 5.3.1-14ubuntu2) 5.3.1 20160413
CMake version: version 3.5.1

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.0.176
GPU models and configuration: 
GPU 0: GeForce GTX 1080 Ti
GPU 1: GeForce GTX 1080 Ti

Nvidia driver version: 410.93
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] numpy==1.16.2
[pip] numpydoc==0.9.1
[pip] torch==1.2.0
[pip] torch-cluster==1.4.2
[pip] torch-geometric==1.2.1
[pip] torch-scatter==1.2.0
[pip] torch-sparse==0.4.0
[pip] torch-spline-conv==1.1.0
[pip] torchsummaryX==1.3.0
[pip] torchvision==0.4.0a0+6b959ee
[conda] blas                      1.0                         mkl  
[conda] cuda90                    1.0                  h6433d27_0    pytorch
[conda] mkl                       2018.0.3                      1  
[conda] mkl-service               1.1.2            py36h90e4bf4_5  
[conda] torch                     1.2.0                    pypi_0    pypi
[conda] torch-cluster             1.4.2                    pypi_0    pypi
[conda] torch-geometric           1.2.1                    pypi_0    pypi
[conda] torch-scatter             1.2.0                    pypi_0    pypi
[conda] torch-sparse              0.4.0                    pypi_0    pypi
[conda] torch-spline-conv         1.1.0                    pypi_0    pypi
[conda] torchsummaryx             1.3.0                    pypi_0    pypi
[conda] torchvision               0.2.2.post2              pypi_0    pypi
",module: deadlock needs reproduction triaged,"['I am unable to reproduce this on PyTorch 1.2. \r\n\r\n![image](https://user-images.githubusercontent.com/29104956/64617682-e0bf9b80-d3ac-11e9-80bf-3a653a2cec42.png)\r\n', '@GanjinZero can you get us a stack trace from gdb when the process hang?\r\nSomething like\r\n```\r\ngdb attach <pid>\r\nthread apply all bt\r\n```\r\nand give back the results?', 'I have no knowledge about gdb, so I only copy the result from it.\r\n```\r\nAttaching to process 7319\r\n[New LWP 7322]\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".\r\n0x00007f670d7de827 in sched_yield ()\r\n    at ../sysdeps/unix/syscall-template.S:84\r\n84\t../sysdeps/unix/syscall-template.S: No such file or directory.\r\n(gdb) thread apply all bt\r\n\r\nThread 2 (Thread 0x7f670a68f700 (LWP 7322)):\r\n#0  0x00007f670dacd827 in futex_abstimed_wait_cancelable (private=0, \r\n    abstime=0x0, expected=0, futex_word=0x7f670403a900)\r\n    at ../sysdeps/unix/sysv/linux/futex-internal.h:205\r\n#1  do_futex_wait (sem=sem@entry=0x7f670403a900, abstime=0x0)\r\n    at sem_waitcommon.c:111\r\n#2  0x00007f670dacd8d4 in __new_sem_wait_slow (sem=0x7f670403a900, \r\n    abstime=0x0) at sem_waitcommon.c:181\r\n#3  0x00007f670dacd97a in __new_sem_wait (sem=<optimized out>)\r\n    at sem_wait.c:29\r\n#4  0x000055e450513f76 in PyThread_acquire_lock_timed ()\r\n    at /tmp/build/80754af9/python_1546130271559/work/Python/thread_pthread.h:386\r\n#5  0x000055e4505a61ac in acquire_timed (timeout=-1000000000, \r\n    lock=0x7f670403a900)\r\n    at /tmp/build/80754af9/python_1546130271559/work/Modules/_threadmodule.c:68\r\n#6  lock_PyThread_acquire_lock ()\r\n    at /tmp/build/80754af9/python_1546130271559/work/Modules/_threadmodule.c:151\r\n---Type <return> to continue, or q <return> to quit---\r\n```', 'I met the same problem with you!\r\nHow did you solve this problem?', '> I met the same problem with you!\r\n> How did you solve this problem?\r\n\r\ntry another version of pytorch', 'The deadlock happens when you try to run python code. Can you give the full code sample of what you were doing when it happens?']","['\r\ntorch.exp(torch.Tensor([0, math.log(2)]))\r\n']",[],1,0
53,pytorch,5426,closed,torch.from_numpy is slow for mmap'ped numpy arrays,"I'm looking at feasibility of using Ray as parameter server combined with single-machine NN implementation. 

The bottleneck for using it with PyTorch is that result of Ray calls come as numpy array created on top of [mmaped memory](https://groups.google.com/forum/#!msg/ray-dev/pNsxWI-iSyI/vhWnjgAfAwAJ), and PyTorch  on these arrays is slow, working at about 2.5 GB/sec, which is the speed of single-threaded memcpy.

Regular numpy array can be turned into PyTorch GPU tensors at 8-11 GB/sec, depending on whether memory has been page locked.

Is this memcpy necessary? Can it be done in multi-threaded way?
Related issue¬†on Ray side: https://github.com/ray-project/ray/issues/1614

https://github.com/diux-dev/cluster/blob/master/yuxin_numpy/tf_numpy_benchmark.py

Pytorch 0.3.0.post4
Repro instructions like in https://github.com/pytorch/pytorch/issues/5412#issuecomment-368658892
",,"[""I can't see any reason why this would happen, and I would suspect it might be caused by ray somehow. What's `f.remote()`? There's no memcpy we're doing when creating PyTorch tensors from numpy ([here's the full source](https://github.com/pytorch/pytorch/blob/master/torch/csrc/utils/tensor_numpy.cpp#L91-L124), it's just 30 lines). They share the same data pointer."", 'I\'m not measuring the time it takes to do `f.remote`, the slowness is in `torch.from_numpy` call\r\n\r\n```\r\narr = ray.get(f.remote())\r\nstart_time = time.time()\r\ntorch.from_numpy(arr)\r\nprint(""%.1f ms ""%((time.time()-start_time)*1000,)) # => 462ms\r\n```\r\nThis takes 462 milliseconds on a 1GB numpy array.\r\n\r\nHere\'s the full [benchmark](https://raw.githubusercontent.com/diux-dev/cluster/master/yuxin_numpy/pytorch_slow_numpy_bug.py). \r\n\r\n```\r\npip install ray\r\nwget https://raw.githubusercontent.com/diux-dev/cluster/master/yuxin_numpy/pytorch_slow_numpy_bug.py\r\npython pytorch_slow_numpy_bug.py\r\n\r\n...\r\n462.9 ms \r\n```\r\n\r\nHere\'s [cpu profile](https://www.dropbox.com/s/5zk9qn6oasfkzve/pytorch.svg?dl=1) of the benchmark (from google-perf-tool)\r\n\r\nNot sure how accurate it is, but it shows 15% of the time in `__nss_passwd_lookup` which usually means memcpy\r\n', 'Oh....it looks like `arr=np.array(arr)` makes things fast so there may be some extra stuff going on on Ray side `__array__?`, will reopen if I can isolate this to PyTorch', ""Yeah, once you have a numpy array the conversion should be pretty much immediate irrespectively of its size (it doesn't depend on it). Last time I measured it a few months ago, it cost around 3us."", 'So `from_numpy` is slow if I do\r\n\r\n`torch.from_numpy(arr)`\r\n\r\nBut `from_numpy` is fast if I do\r\n```\r\narr = np.array(arr)\r\ntorch.from_numpy(arr)\r\n```\r\n\r\nTheory: `np.array(arr)` copies things from mmaped area to process local memory area, so from_numpy becomes fast\r\n\r\nRunning this under cProfile shows that the time is spent under `torch._C.from_numpy`\r\n\r\n![screenshot 2018-02-27 09 54 08](https://user-images.githubusercontent.com/23068/36745383-2d4cb482-1ba4-11e8-8e09-ca95c8d2600a.png)\r\n', '@yaroslavvb, Ray creates a read-only numpy array (since it is backed by shared memory), so\r\n\r\n```python\r\nimport numpy as np\r\nimport ray\r\nimport torch\r\n\r\nray.init()\r\n\r\n@ray.remote\r\ndef f():\r\n    return np.zeros(25 * 10 ** 6, dtype=np.float32)\r\n\r\nx = ray.get(f.remote())  # Now x is read only.\r\nt = torch.from_numpy(x)  # This does a copy because x is read only.\r\n```\r\n\r\nWe can try turning off the read-only flag and see if that fully gets rid of the issue or if there are any remaining slowdowns.', ""`from_numpy` does *nothing* to the tensor data (as can be seen in the linked source). It's possible that the NumPy API we're using to implement the conversion is doing something very expensive under the hood, which would explain why the second case is fast. This is caused by the package implementing `arr`. Let me know if you can think of any improvements, but I don't think there's anything we can do here."", ""BTW, I think `arr` is just regular numpy ndarray in the slow case. IE\r\n\r\n```\r\n(Pdb) arr.__class__\r\n<class 'numpy.ndarray'>\r\n```\r\nHowever, the result of torch.from_numpy(arr) is mutable, whereas the arr's backing memory is marked as read-only, so something must be triggering a copy"", ""Hmm I think it's possible that we're just not respecting the flags in the current implementation. I'm pretty sure we did check that before, but I can't see any conditionals that would depend on this."", 'https://github.com/diux-dev/cluster/blob/master/yuxin_numpy/pytorch_modify_test.py\r\n\r\n```\r\narr = ray.get(f.remote())\r\narr.__class__ # => ""<class \'numpy.ndarray\'>""\r\narr+=1 # fails with  ""ValueError: output array is read-only""\r\narr = torch.from_numpy(arr)\r\narr+=1  # works\r\n\r\n\r\n```', ""I've opened another issue to track this"", 'Different application, I found that processing an array in pytorch using CUDA device is very fast, but displaying the result incurs 200 msec penalty when converting back to numpy array.\r\n\r\nexample:\r\ntorch_array = torch.from_numpy(numpy_array) # less than 1msec\r\ndo processing on torch_array  # less than 1 msec on GPU @ 99%\r\nnumpy_array = np.array(torch_array) # greater than 200 msec\r\n\r\ndoes anyone know a method that avoids the conversion penalty back to numpy. GPU = nvidia on TX1 platform', '> Different application, I found that processing an array in pytorch using CUDA device is very fast, but displaying the result incurs 200 msec penalty when converting back to numpy array.\r\n> \r\n> example:\r\n> torch_array = torch.from_numpy(numpy_array) # less than 1msec\r\n> do processing on torch_array # less than 1 msec on GPU @ 99%\r\n> numpy_array = np.array(torch_array) # greater than 200 msec\r\n> \r\n> does anyone know a method that avoids the conversion penalty back to numpy. GPU = nvidia on TX1 platform\r\n\r\nHas there been any update on this? \r\n\r\nI am tried variable.weight.data.numpy() and this takes 25 seconds. \r\n\r\nnp.array(variable.weight.data) took 0.3 seconds which is better, but I need much more speed. \r\n\r\n']","['\r\n# torch.from_numpy(ray.get(f.remote()))\r\npython tf_numpy_benchmark.py --benchmark=pytorch_from_numpy_noclone --allocator=ray --num-iters=51\r\npytorch_from_numpy            :   2.5 GB/sec, min: 39.86, median: 41.38, mean: 63.55\r\n\r\n# torch.from_numpy(page_locked_numpy).cuda()\r\npython tf_numpy_benchmark.py --benchmark=pytorchgpu_from_numpy --allocator=tfgpu --num-iters=51\r\npytorchgpu_from_numpy         :  11.0 GB/sec, min:  9.05, median:  9.07, mean:  9.07\r\n\r\n# torch.from_numpy(regular_numpy).cuda()\r\npython tf_numpy_benchmark.py --benchmark=pytorchgpu_from_numpy --allocator=numpy --num-iters=51\r\npytorchgpu_from_numpy         :   8.2 GB/sec, min: 12.24, median: 12.42, mean: 12.45\r\n']",['from_numpy'],1,0
54,pytorch,24373,closed,torch.as_tensor is very slow,"## üêõ Bug
## To Reproduce

This code:

prints


I don't expect  to be 100x slower than .

## Environment


",,"['Duplicate of https://github.com/pytorch/pytorch/issues/13918', 'Going to close this because it is a duplicate. @ppwwyyxx please post your findings in  #13918.']","['\r\n\r\nimport time\r\nimport torch\r\nimport numpy as np\r\n\r\ndata = [np.random.rand(8, 800, 1333) > 0.5 for _ in range(2)]\r\ndtype_np = np.bool\r\ndtype_pt = torch.bool\r\n\r\ndef f1():\r\n    return np.asarray(data, dtype=dtype_np)\r\n\r\ndef f2():\r\n    return np.stack(data)\r\n\r\ndef f3():\r\n    return torch.as_tensor(data, dtype=dtype_pt)\r\n\r\ndef f4():\r\n    return torch.stack([torch.from_numpy(x) for x in data])\r\n\r\ndef benchmark(f, iter, warmup):\r\n    for k in range(warmup): f()\r\n    start = time.perf_counter()\r\n    for k in range(iter): f()\r\n    torch.cuda.synchronize()\r\n    return time.perf_counter() - start\r\n\r\nprint(benchmark(f1, 10, 1))\r\nprint(benchmark(f2, 10, 1))\r\nprint(benchmark(f3, 10, 1))\r\nprint(benchmark(f4, 10, 1))\r\n', '\r\n0.2839105408638716\r\n0.013459203764796257\r\n20.43221192806959\r\n0.012655530124902725\r\n', '\r\nPyTorch version: 1.2.0.dev20190805\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04) 7.4.0\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: \r\nGPU 0: Quadro GP100\r\nGPU 1: Quadro GP100\r\n\r\nNvidia driver version: 410.79\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] msgpack-numpy==0.4.4.3\r\n[pip3] numpy==1.16.4\r\n[pip3] numpydoc==0.7.0\r\n[pip3] torch-nightly==1.2.0.dev20190805\r\n[pip3] torchvision==0.4.0a0+6c7189f\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.1                      144  \r\n[conda] mkl-include               2019.1                      144  \r\n[conda] mkl-service               1.1.2            py36he904b0f_5  \r\n[conda] mkl_fft                   1.0.6            py36hd81dba3_0  \r\n[conda] mkl_random                1.0.2            py36hd81dba3_0\r\n']","['torch.as_tensor', 'np.asarray()']",1,0
55,pytorch,23862,closed,nn.functional.conv2d is a factor 5 slower when using specific weight tensor on CPU.,"## üêõ Bug

When using the torch conv2d I have a huge slowdown in speed using a specific weight tensor coming from a pretrained state_dict. By adding 1 and subsequently subtracting 1 from the tensor from the state_dict I get a huge improvement (factor 5 on my machine). 

In the code example below I compare the pretrained weight with itself after adding and subtracting 1. 

I also observed this problem in 1.0.1


generates the problematic tensor (There are many tensors like this in my pretrained state_dict, so it is not unique) 

## To Reproduce

To reproduce run the following code and read the printouts in the console.



## Expected behavior

I would expect the speed of conv2d to be identical whether using w1 or w2. 

## Environment

 - PyTorch Version: 1.1
 - OS: Ubuntu 18.04.2 LTS
 - How you installed PyTorch: 

Python version: 3.7
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
certifi    2019.6.16
numpy      1.17.0   
pip        19.1.1   
setuptools 41.0.1   
torch      1.1.0    
wheel      0.33.4   ",,"['It seems your performances gets a hit by handling denormal values.\r\nTry to set [torch.set_flush_denormal(True)](https://pytorch.org/docs/stable/torch.html#torch.set_flush_denormal) and profile the code again.', 'Alright! This works, thanks a lot! \r\n\r\nIMO allowing denormal numbers by default is not a good idea. The cases where this default behavior effectively acts as a performance bug outnumbers the cases where extreme precision is required.', 'Closing via https://github.com/pytorch/pytorch/issues/23862#issuecomment-518628303']","['create_w()', ' python\r\nimport torch\r\nfrom timeit import timeit\r\n\r\n\r\ndef timing_test():\r\n    from torch.nn.functional import conv2d\r\n\r\n    test_img = torch.ones(10, 12, 100, 100)\r\n    w1 = create_w()\r\n    w2 = (w1 + 1) - 1\r\n\r\n    print(""Torch equal:"", torch.equal(w1, w2))\r\n\r\n    n = 100\r\n    with torch.no_grad():\r\n        t1 = timeit(""conv2d(test_img, w1)"", globals=locals(), number=n) / n\r\n        t2 = timeit(""conv2d(test_img, w2)"", globals=locals(), number=n) / n\r\n\r\n    print(""Time w1:"", t1)\r\n    print(""Time w2:"", t2)\r\n    print(""Factor:"", t1 / t2)\r\n\r\n    print()\r\n    print(""w1 norm:"", w1.norm())\r\n    print(""w2 norm:"", w2.norm())\r\n\r\n    print()\r\n    print(""w1 dtype:"", w1.dtype)\r\n    print(""w2 dtype:"", w2.dtype)\r\n\r\n    print()\r\n    print(""w1 device:"", w1.device)\r\n    print(""w2 device:"", w2.device)\r\n\r\n    print()\r\n    print(""w1 shape:"", w1.shape)\r\n    print(""w2 shape:"", w2.shape)\r\n\r\n\r\ndef create_w():\r\n    return torch.tensor(\r\n        [\r\n            [\r\n                [[2.0536e-03]],\r\n                [[-6.5152e-02]],\r\n                [[7.5900e-02]],\r\n                [[1.9581e-02]],\r\n                [[6.0192e-02]],\r\n                [[1.1083e-01]],\r\n                [[3.5546e-02]],\r\n                [[1.0338e-01]],\r\n                [[7.4448e-02]],\r\n                [[3.8988e-02]],\r\n                [[-1.3289e-02]],\r\n                [[3.5973e-02]],\r\n            ],\r\n            [\r\n                [[-1.8130e-03]],\r\n                [[4.5211e-02]],\r\n                [[-6.3141e-03]],\r\n                [[-2.2000e-02]],\r\n                [[5.5847e-02]],\r\n                [[3.5140e-02]],\r\n                [[5.5567e-02]],\r\n                [[2.0592e-02]],\r\n                [[2.9450e-02]],\r\n                [[2.5826e-02]],\r\n                [[2.6045e-03]],\r\n                [[1.1359e-01]],\r\n            ],\r\n            [\r\n                [[-3.5669e-04]],\r\n                [[-2.0352e-02]],\r\n                [[-1.0962e-02]],\r\n                [[1.2939e-02]],\r\n                [[2.0176e-03]],\r\n                [[4.1796e-02]],\r\n                [[7.6029e-02]],\r\n                [[4.8062e-03]],\r\n                [[2.0193e-02]],\r\n                [[2.5245e-02]],\r\n                [[6.2719e-02]],\r\n                [[1.0437e-01]],\r\n            ],\r\n            [\r\n                [[2.9016e-02]],\r\n                [[3.6604e-02]],\r\n                [[-2.4739e-03]],\r\n                [[1.7465e-02]],\r\n                [[4.5465e-02]],\r\n                [[-2.0248e-02]],\r\n                [[-1.0559e-03]],\r\n                [[-3.3110e-02]],\r\n                [[-2.6498e-02]],\r\n                [[-1.1671e-02]],\r\n                [[2.6471e-02]],\r\n                [[1.4671e-02]],\r\n            ],\r\n            [\r\n                [[3.1644e-02]],\r\n                [[-7.5824e-02]],\r\n                [[-3.2397e-02]],\r\n                [[2.7205e-02]],\r\n                [[-8.0184e-03]],\r\n                [[3.5684e-02]],\r\n                [[-4.3505e-04]],\r\n                [[2.6388e-02]],\r\n                [[4.5333e-04]],\r\n                [[5.5965e-02]],\r\n                [[4.2574e-02]],\r\n                [[1.7094e-02]],\r\n            ],\r\n            [\r\n                [[-1.7368e-02]],\r\n                [[5.3762e-02]],\r\n                [[-4.4826e-02]],\r\n                [[-3.7706e-02]],\r\n                [[1.9211e-03]],\r\n                [[-4.3316e-02]],\r\n                [[6.2763e-02]],\r\n                [[1.0589e-02]],\r\n                [[3.9296e-02]],\r\n                [[-3.6488e-02]],\r\n                [[6.7191e-02]],\r\n                [[2.0684e-02]],\r\n            ],\r\n            [\r\n                [[-4.8351e-02]],\r\n                [[-1.3839e-02]],\r\n                [[5.5983e-02]],\r\n                [[-3.6999e-02]],\r\n                [[-2.1908e-02]],\r\n                [[-1.0006e-02]],\r\n                [[-1.6682e-02]],\r\n                [[6.8055e-02]],\r\n                [[2.5191e-02]],\r\n                [[3.9925e-02]],\r\n                [[-1.5595e-02]],\r\n                [[-3.6358e-02]],\r\n            ],\r\n            [\r\n                [[-3.3860e-41]],\r\n                [[1.3748e-41]],\r\n                [[1.1227e-41]],\r\n                [[5.3964e-42]],\r\n                [[-1.8262e-41]],\r\n                [[3.0524e-41]],\r\n                [[2.0291e-42]],\r\n                [[7.2924e-42]],\r\n                [[-2.7940e-41]],\r\n                [[-1.1212e-41]],\r\n                [[-3.2091e-41]],\r\n                [[-2.9832e-41]],\r\n            ],\r\n            [\r\n                [[-5.4249e-03]],\r\n                [[7.7659e-02]],\r\n                [[4.2960e-02]],\r\n                [[-1.2729e-01]],\r\n                [[-6.1029e-02]],\r\n                [[3.7938e-02]],\r\n                [[7.4308e-03]],\r\n                [[-1.8296e-02]],\r\n                [[4.6577e-02]],\r\n                [[3.3428e-02]],\r\n                [[3.5875e-03]],\r\n                [[1.0800e-02]],\r\n            ],\r\n            [\r\n                [[-3.9751e-02]],\r\n                [[-1.2280e-01]],\r\n                [[4.4217e-02]],\r\n                [[-2.8147e-02]],\r\n                [[9.7940e-03]],\r\n                [[1.0163e-03]],\r\n                [[-5.6101e-02]],\r\n                [[1.6846e-03]],\r\n                [[-3.0928e-02]],\r\n                [[-6.3853e-02]],\r\n                [[-2.0385e-02]],\r\n                [[-4.3640e-02]],\r\n            ],\r\n            [\r\n                [[-5.4244e-02]],\r\n                [[3.3747e-02]],\r\n                [[-2.6432e-02]],\r\n                [[-2.0302e-02]],\r\n                [[3.0961e-02]],\r\n                [[3.0394e-02]],\r\n                [[5.0516e-02]],\r\n                [[-3.3158e-03]],\r\n                [[7.7341e-03]],\r\n                [[-2.7420e-02]],\r\n                [[9.3604e-02]],\r\n                [[-3.1954e-02]],\r\n            ],\r\n            [\r\n                [[-6.5509e-02]],\r\n                [[9.1255e-02]],\r\n                [[-5.3223e-02]],\r\n                [[-9.5710e-03]],\r\n                [[2.3625e-02]],\r\n                [[6.9223e-03]],\r\n                [[2.6769e-03]],\r\n                [[2.8905e-02]],\r\n                [[-7.6544e-03]],\r\n                [[-1.5755e-01]],\r\n                [[2.0315e-02]],\r\n                [[3.1298e-02]],\r\n            ],\r\n            [\r\n                [[-3.9999e-02]],\r\n                [[-6.3408e-02]],\r\n                [[-4.7921e-02]],\r\n                [[5.0635e-02]],\r\n                [[-8.8498e-02]],\r\n                [[3.3315e-02]],\r\n                [[7.9214e-02]],\r\n                [[5.8254e-03]],\r\n                [[-5.9077e-02]],\r\n                [[6.2304e-02]],\r\n                [[5.5565e-02]],\r\n                [[4.1595e-02]],\r\n            ],\r\n            [\r\n                [[3.9259e-02]],\r\n                [[8.9200e-02]],\r\n                [[-4.5643e-03]],\r\n                [[5.1565e-02]],\r\n                [[-1.5232e-02]],\r\n                [[-1.9123e-02]],\r\n                [[-3.0002e-02]],\r\n                [[1.7376e-02]],\r\n                [[5.7346e-02]],\r\n                [[3.7609e-02]],\r\n                [[5.2436e-03]],\r\n                [[-4.0633e-02]],\r\n            ],\r\n            [\r\n                [[1.4893e-02]],\r\n                [[2.9597e-02]],\r\n                [[2.0239e-02]],\r\n                [[-9.2896e-02]],\r\n                [[1.9449e-02]],\r\n                [[-8.1253e-02]],\r\n                [[-4.0866e-02]],\r\n                [[-1.1120e-02]],\r\n                [[-4.7219e-02]],\r\n                [[-3.5702e-02]],\r\n                [[-2.1840e-02]],\r\n                [[6.9138e-02]],\r\n            ],\r\n        ]\r\n    )\r\n\r\n\r\ntiming_test()\r\n', '\r\n~/misc$ conda create -n torch_test python==3.7\r\n~/misc$ conda activate torch_test\r\n~/misc$ pip install torch\r\n']",[],1,0
56,pytorch,5436,closed,Slower performance of eq and fill_ compared to numpy,"




",medium priority (this tag is deprecated),"[""@vadimkantorov I got the following result when running the perf tests on current master:\r\n\r\n```\r\npython -m timeit -n 1000 'import torch; torch.ByteTensor(500, 500, 3).fill_(1) == torch.ByteTensor([1, 2, 3])'\r\npython -m timeit -n 1000 'import numpy; numpy.ones((500, 500, 3), dtype = numpy.uint8) == numpy.array([1, 2, 3], dtype = numpy.uint8)'\r\n```\r\n\r\n```\r\n1000 loops, best of 3: 2.27 msec per loop\r\n1000 loops, best of 3: 1.98 msec per loop\r\n```\r\n\r\nIt's still slower compared to numpy, but the difference is smaller. Could you also try to run the perf comparison again to see if it has been improved?"", ""@yf225 Due to some build issues I can't install a fresh version from source, so can't test the recent improvements.\r\n\r\nThe right hand side array is very small, I hope that proper vectorization is happening (three bytes fit into an int, so vectorized comparisons with a broadcasted rhs should be possible)."", '@vadimkantorov Got it, what are the build issues you are having?\r\n\r\n@cpuhrsch Do you have any insights regarding vectorization?', ""This is currently work in progress as part of [this pull request](https://github.com/pytorch/pytorch/pull/7655). It's definitely on our list. We're also adding a more extensive set of microbenchmarks [to our benchmark repository](https://github.com/pytorch/benchmark) to compare to NumPy."", ""Byte array of size 3 doesn't fit well into 4-byte-aligned vector registers, but maybe some efficient packing is still possible...\r\n\r\n@yf225 It was something related to PyTorch thinking that MKL was required and failing to discover it (instead I had self-built OpenBLAS)"", ""@vadimkantorov - May I also ask what the use-case is? Maybe there's a way in which we could make this more efficient by specializing on something else."", ""Ground truth for image segmentation (e.g. in pascal voc) is often distributed as regular images, with a set of pre-specified RGB-colors, each of which corresponds to a class label. So to recover class-label map I need to perform such comparisons.\r\n\r\nIdeally, I'd like to compare to all colors of the palette at the same time, but not sure if it's achievable in PyTorch."", '@vadimkantorov For the build issues, do you mind opening a new issue with the build command and the CMake logs you saw? We would actually want to fix this use case.', 'I reinstalled PyTorch in a fresh conda environment with MKL. I get:\r\n```\r\n1000 loops, best of 3: 1.92 msec per loop # pytorch\r\n1000 loops, best of 3: 1.26 msec per loop # numpy\r\n```\r\nPyTorch is 50% slower.', ""On master (1.7.0.dev20200709)\r\n\r\n```\r\npython -m timeit -n 1000 'import torch; torch.ByteTensor(500, 500, 3).fill_(1) == torch.ByteTensor([1, 2, 3])'\r\n1000 loops, best of 5: 605 usec per loop\r\n\r\npython -m timeit -n 1000 'import numpy; numpy.ones((500, 500, 3), dtype = numpy.uint8) == numpy.array([1, 2, 3], dtype = numpy.uint8)'\r\n1000 loops, best of 3: 1.88 msec per loop\r\n```\r\n\r\nMission accomplished, let's go home."", '@ezyang A question: when the last dimension is small, does it do comparison with triplets of numbers in one go in a vectorized, contiguous fashion? In theory it could even load the rhs in a vector register (or even scalar register, since 3 bytes fit into one int) once, since broadcasting is dummy', ""Not sure. But eq is going to be used with TensorIterator and it's going to coalesce contiguous dimensions, so my guess is that something reasonable will happen."", 'Small dimension ops (widh size = 2/3/4) may get more used with rise of point cloud/geometric processing in PyTorch. I think it makes sense to add more perf tests about these sizes.']","[""shell\r\npython -m timeit -n 1000 'import torch; torch.ByteTensor(500, 500, 3).fill_(1) == torch.ByteTensor([1, 2, 3])'\r\npython -m timeit -n 1000 'import numpy; numpy.ones((500, 500, 3), dtype = numpy.uint8) == numpy.array([1, 2, 3], dtype = numpy.uint8)'\r\n"", '\r\n1000 loops, best of 3: 4.81 msec per loop\r\n1000 loops, best of 3: 1.45 msec per loop\r\n']","[""torch.__version__ = '0.4.0a0+1fdb392'"", ""numpy.__version__ = '1.14.0'""]",1,0
57,pytorch,23871,closed,Quantized conv is 6x slower than float on CPU,"Repro:
 Run resnext101-32x8d with quantized conv instead of float conv and measure elapsed time.

Possible causes:
 Layout transform before and after quantized conv2d calls.",oncall: quantization quantization_release_1.3 triaged,['Pretty sure all layout issues have been addressed'],[],[],1,1
58,pytorch,21828,open,torch::zeros is slow for small tensors (C++),"## üêõ Bug

For small 1d tensors (smaller than about 4000), setting each element to zero with an accessor is faster than using make_zeros.

Benchmarking code (uses https://github.com/google/benchmark):



And here are the results:


As you can see, for small 1d tensors make_zero is sometime almost 3x slower than using an accessor. For 2d tensors, make_zeros is still slower for very small tensors.

## Environment
PyTorch version: 1.1.0a0+de582e2
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 18.04.2 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration: GPU 0: GeForce GTX TITAN X
Nvidia driver version: 418.56
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.21
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.5.1

Versions of relevant libraries:
[pip3] numpy==1.13.3
[conda] _tflow_select             2.3.0                       mkl  
[conda] blas                      1.0                         mkl  
[conda] magma-cuda10              2.4.0                         1    cpbotha
[conda] mkl                       2019.3                      199  
[conda] mkl-include               2019.3                      199  
[conda] mkl_fft                   1.0.12           py37ha843d7b_0  
[conda] mkl_random                1.0.2            py37hd81dba3_0  
[conda] mkldnn                    0.16.1                        0    mingfeima
[conda] tensorflow                1.13.1          mkl_py37h54b294f_0  
[conda] tensorflow-base           1.13.1          mkl_py37h7ce6ba3_0  
[conda] torch                     1.1.0a0+de582e2          pypi_0    pypi
",module: cpp module: performance triaged,"[""Most of what you're seeing is probably tensor allocation overhead."", 'Why would I see more tensor allocation overhead when using zeros compared to creating exactly the same size tensor with empty and then filling it with accessor?', ""My apologies, I skimmed over your code too quickly. As you pointed out, what you're seeing isn't tensor allocation overhead.\r\n\r\n`at::zeros` is implemented with an `at::empty` followed by a call to `tensor.zero_()`. It looks like the `zero_()` call is the culprit here (you can test this by just running a benchmark of `tensor.zero_()` vs zero-ing via accessor). \r\n\r\n"", ""I'm pretty sure that .zeros() fills in parallel. Maybe this is threading overhead? cc @colesbury "", 'I will take a look. ', 'FYI I see similar behavior with stack_out (and I‚Äôm sure there are others)', 'Tensor.zero_ is using old TH code from the THTensorFill.cpp, we are planning to update this during H2 (hopefully much sooner).', 'I will simplify a bit, but in general (using your numbers): ~500ns is the cost of the tensor function execution (including all type/device dispatching). `zeros()` function (internally) call two more functions `empty` and `zero_` which results with up to 3x function call price. Iterator overhead (universal implementation) is around ~100ns. Iterating with the accessor would always be faster as you are writing custom code for the specific shape, type, memory layout, device. \r\n\r\nWe are working actively on reducing function call price. But it takes time.', 'If this is important, we can speed this up in the short-term by breaking up `zeros` into a `zeros_cpu` and `zeros_cuda`, and `zeros` for other backends.  These functions can call `empty_cpu` or `empty_cuda` or `empty` depending on what the backend is and we can probably do something similar for `zero_` as well.', 'While the benchmarks I provided were for zeros, I want to mention that I have seen similar issues with other functions like cat_out, stack_out, etc. \r\n\r\nIt doesn‚Äôt make sense that iterating with the accessor is always faster due to it being custom code, since the non-accessor version wins in the large tensors\r\n\r\nI think it‚Äôs more likely to be the threading causing the overhead because I do see what seems to be omp-related functions taking up a lot of time when I profile the code with vtune. That would also be consistent with slower performance for small tensors and better performance with large tensors. ', ""@thomasj02 It's a known issue that torch.cat is slow on CPU: https://github.com/pytorch/pytorch/issues/18634.""]","['\r\n#pragma clang diagnostic push\r\n#pragma ide diagnostic ignored ""cert-err58-cpp""\r\n\r\n#include <benchmark/benchmark.h>\r\n#include <torch/torch.h>\r\n\r\nstatic void BM_AccessorZero2d(benchmark::State& state) {\r\n  auto options = torch::TensorOptions().dtype(torch::kFloat32);\r\n\r\n  for(auto _ : state) {\r\n    auto empty = torch::empty({state.range(0), state.range(0)}, options);\r\n    auto accessor = empty.accessor<float, 2>();\r\n    for(int i = 0; i < state.range(0); ++i) {\r\n      for(int j = 0; j < state.range(0); ++j) {\r\n        accessor[i][j] = 0;\r\n      }\r\n    }\r\n  }\r\n}\r\nBENCHMARK(BM_AccessorZero2d)->Range(1, 50000);\r\n\r\nstatic void BM_MakeZeros2d(benchmark::State& state) {\r\n  auto options = torch::TensorOptions().dtype(torch::kFloat32);\r\n\r\n  for (auto _ : state) {\r\n    torch::zeros({state.range(0), state.range(0)}, options);\r\n  }\r\n}\r\nBENCHMARK(BM_MakeZeros2d)->Range(1, 50000);\r\n\r\nstatic void BM_AccessorZero(benchmark::State& state) {\r\n  auto options = torch::TensorOptions().dtype(torch::kFloat32);\r\n\r\n  for (auto _ : state) {\r\n    auto empty = torch::empty({state.range(0)}, options);\r\n    auto accessor = empty.accessor<float, 1>();\r\n    for(int i=0; i<state.range(0); ++i) {\r\n      accessor[i] = 0;\r\n    }\r\n  }\r\n}\r\nBENCHMARK(BM_AccessorZero)->Range(1, 50000);\r\n\r\nstatic void BM_MakeZeros(benchmark::State& state) {\r\n  auto options = torch::TensorOptions().dtype(torch::kFloat32);\r\n\r\n  for (auto _ : state) {\r\n    torch::zeros({state.range(0)}, options);\r\n  }\r\n}\r\nBENCHMARK(BM_MakeZeros)->Range(1, 50000);\r\n\r\nBENCHMARK_MAIN();\r\n#pragma clang diagnostic pop\r\n', '\r\nResults:\r\nRun on (12 X 3600 MHz CPU s)\r\nCPU Caches:\r\n  L1 Data 32K (x6)\r\n  L1 Instruction 32K (x6)\r\n  L2 Unified 256K (x6)\r\n  L3 Unified 15360K (x1)\r\nLoad Average: 1.03, 1.64, 2.00\r\n------------------------------------------------------------------\r\nBenchmark                        Time             CPU   Iterations\r\n------------------------------------------------------------------\r\nBM_AccessorZero2d/1            559 ns          559 ns      1221153\r\nBM_AccessorZero2d/8            614 ns          614 ns      1154875\r\nBM_AccessorZero2d/64          2404 ns         2404 ns       290513\r\nBM_AccessorZero2d/512        88699 ns        88698 ns         7957\r\nBM_AccessorZero2d/4096    27946856 ns     27947340 ns           25\r\nBM_AccessorZero2d/32768 1799065774 ns   1799064893 ns            1\r\nBM_AccessorZero2d/50000 4031161980 ns   4031154712 ns            1\r\nBM_MakeZeros2d/1              1503 ns         1503 ns       466054\r\nBM_MakeZeros2d/8              1492 ns         1492 ns       459766\r\nBM_MakeZeros2d/64             1936 ns         1936 ns       358106\r\nBM_MakeZeros2d/512           11142 ns        10945 ns        71892\r\nBM_MakeZeros2d/4096        7404616 ns      7116580 ns           99\r\nBM_MakeZeros2d/32768     388659886 ns    374048021 ns            2\r\nBM_MakeZeros2d/50000     839755289 ns    818242038 ns            1\r\nBM_AccessorZero/1              578 ns          578 ns      1220794\r\nBM_AccessorZero/8              575 ns          575 ns      1186077\r\nBM_AccessorZero/64             620 ns          620 ns      1113125\r\nBM_AccessorZero/512            781 ns          781 ns       914935\r\nBM_AccessorZero/4096          1853 ns         1853 ns       386530\r\nBM_AccessorZero/32768        10718 ns        10719 ns        66042\r\nBM_AccessorZero/50000        16424 ns        16424 ns        42340\r\nBM_MakeZeros/1                1491 ns         1491 ns       451186\r\nBM_MakeZeros/8                1458 ns         1458 ns       479606\r\nBM_MakeZeros/64               1465 ns         1465 ns       473993\r\nBM_MakeZeros/512              1539 ns         1539 ns       455206\r\nBM_MakeZeros/4096             1933 ns         1933 ns       358675\r\nBM_MakeZeros/32768            5305 ns         5305 ns       134382\r\nBM_MakeZeros/50000            7662 ns         7662 ns        90926\r\n']",[],1,0
59,pytorch,17738,closed,Inconsistent results of torch.argmax with tensors that have duplicated values,"## üêõ Bug
Inconsistent results of torch.argmax with tensors that have duplicated values
<!-- A clear and concise description of what the bug is. -->

## To Reproduce



When I did the same on another machine(pytorch 0.4.1, CUDA 9.0)

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior
torch.argmax returns the same value for tensors that have same data in it, regardless of devices(or other factors).
<!-- A clear and concise description of what you expected to happen. -->

## Environment

PyTorch version: 0.4.1.post2
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 16.04.5 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.0.176
GPU models and configuration:
GPU 0: GeForce GTX 1080 Ti
GPU 1: GeForce GTX 1080 Ti

Nvidia driver version: 410.79
cuDNN version: /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.3.1

Versions of relevant libraries:
[pip3] msgpack-numpy==0.4.4.1
[pip3] numpy==1.15.0
[pip3] torch==0.4.1.post2
[pip3] torchtext==0.3.1
[pip3] torchvision==0.2.1
[conda] blas                      1.0                         mkl
[conda] mkl                       2018.0.3                      1
[conda] mkl_fft                   1.0.4            py36h4414c95_1
[conda] mkl_random                1.0.1            py36h4414c95_1
[conda] mxnet-mkl                 1.3.0.post0               <pip>
[conda] pytorch                   0.4.1           py36_py35_py27__9.0.176_7.1.2_2    pytorch
[conda] torchtext                 0.3.1                     <pip>
[conda] torchvision               0.2.1                    py36_0


## Additional context
It would be helpful if  the documentation describes if torch.argmax returns the last or the first index if a tensor holds duplicated values.
<!-- Add any other context about the problem here. -->
",triage review,"['This is related to https://github.com/pytorch/pytorch/issues/14147. Perhaps we should add [this comment](https://github.com/pytorch/pytorch/issues/14147#issuecomment-439631144) in the docs..', ""@vishwakftw @soumith \r\nI agree with the [comment](https://github.com/pytorch/pytorch/issues/14147#issuecomment-439631144) but, consistent behavior across devices is good to have.\r\n\r\nAnother point is, consistency with NumPy- we are always pushed to have NumPy-like behavior for portability, in such case, returning first occurence is preferable. \r\n[NumPy's argmax](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html)\r\n\r\nWe might not be ready to follow NumPy convention here but we should at least be consistent across devices."", 'There is no easy way to guarantee consistency across device, especially with parallelism-heavy device like GPUs, and many GPUs requiring different launching configs.', 'Thank you for the reference and discussions! \r\n\r\nHow about inconsistency across different machines? In the above example, the argmax results for two cpu tensors are different.\r\nI also report that the results(argmax with same cpu tensor on different machine)were the same for most tensors and I could not find any regularity in the tensors that results in the inconsistency.\r\n\r\nIs there randomness involved in picking a index when there are several answers?', ""the only determinism we aim to have is hashed on device, and for CPU, single-threaded.\r\nEven across different kinds of GPUs, all bets are off.\r\n\r\nWe cannot guarantee cross-device, cross-CPU-type determinism due to severe performance cliffs that'll result from such a constraint.""]","[""\r\ncpu = torch.LongTensor([[2,3,0,3,3], [1,3,1,3,1]])\r\ncpu.argmax(dim=1) # tensor([4, 3])\r\ncuda = torch.cuda.LongTensor([[2,3,0,3,3], [1,3,1,3,1]])\r\ncuda.argmax(dim=1) #tensor([4, 1], device='cuda:0')\r\n"", ""\r\ncpu = torch.LongTensor([[2,3,0,3,0], [1,3,1,3,1]])\r\ncpu.argmax(dim=1) # tensor([4, 1]). \r\ncuda = torch.cuda.LongTensor([[2,3,0,3,0], [1,3,1,3,1]])\r\ncuda.argmax(dim=1) # tensor([1, 1, device='cuda:0')\r\n""]",[],1,0
60,pytorch,847,closed,the loss is not decreasing,"Hi,
I have created a simple model consisting of two 1-layer nn competing each other. So, I have my own loss function based on those nn outputs.  It is very similar to GAN. The problem is that for a very simple test sample case, the loss function is not decreasing. For now I am using non-stochastic optimizer to eliminate randomness.  Here is the pseudo code with explanation

n1_model = Net1(Dimension_in_n1, Dimension_out) # 1-layer nn with sigmoid
n2_model =Net2(Dimension_in_n2, Dimension_out) # 1-layer nn with sigmoid

n1_optimizer = torch.optim.LBFGS(n1_model.parameters(), lr=0.01,max_iter = 50)
n2_optimizer = torch.optim.LBFGS(n2_model.parameters(), lr=0.01, max_iter = 50)

for t in range(iter):
    x_n1 = Variable(torch.from_numpy(...)) #load input of nn1 in batch size
    x_n2 = Variable(torch.from_numpy(...)) #load input of nn2 in batch size

    def closure():
       reset_grad(n1_model.parameters())
       reset_grad(n2.parameters())
        y_n1 = n1_model(x_n1)
        y_n2 = n2_model(x_n2)
        n1_params =getParams(n1_model)
        n2_param =getParams(n2_model)
        loss = my_loss_function(y_n1, y_n2, n1_params, n2_param) # my loss function, mean square error + regularizer
        loss.backward(retain_variables=True)
        return loss

    n1_optimizer.step(closure)


    def clos():
       reset_grad(n1_model.parameters())
       reset_grad(n2_model.parameters())
        y_n1 = n1_model(x_n1)
        y_n2 = n2_model(x_n2)
        n1_params =getParams(n1_model)
        n2_param =getParams(n2_model)
        loss = my_loss_function(y_n1, y_n2, n1_params, n2_param) # my loss function, mean square error + regularizer
        loss.backward()
        return loss

    n2_optimizer.step(clos)


and here is the definition of my loss function:

def my_loss_function(n1_output, n2_output, n1_parm, n2_param):
    sm = torch.pow(n1_output - n2_output, 2)
    reg = torch.norm(n1_parm,2) + torch.norm(n2_param,2)
    y = torch.sum(sm) + 1 * reg
    return y

when I plot loss function, it has oscillation; I expect it to decrease during training.
",,"[""Maybe the model is underfitting or there's something wrong with the training procedure. We're using the GitHub issues only for bug reports and feature requests not for general help. If you have any questions, please ask them [on our forums](discuss.pytorch.org), but we can't help you debug any model you have. There are lots of things that can make training unstable, from data loading to exploding/vanishing gradients and numerical instability.""]",[],[],1,1
61,pytorch,17745,open,"distributed data parallel, gloo backend works, but nccl deadlock","I have run into the same problem as #14870 .  Because I cannot reopen the issue, I opened a new issue. But the GPUs are all empty, except that 11MB memory is used (no process running). 

The code is easy to reproduce:
Nmax_utilizationmax_utilizationmax_memory_usageN_per_processmax_utilizationmax_memory_usage
it hangs.

But it works fine with .",module: deadlock module: nccl oncall: distributed triaged,"['I have encoutered this problem. My problem is caused by different CUDA version.', 'Can you please use https://github.com/pytorch/pytorch/blob/master/torch/utils/collect_env.py to report information about your system', 'Had a similar issue. Mine was due to not issuing `torch.cuda.set_device(args.local_rank)` at the start of each process. I found each proc occupying some memory on the first gpu (gpustat with procids), which went away once I set the device. NCCL requires that everything in one proc be stored on respective GPUs.']",['\r\n\r\nAfter printing \r\n'],"['', ""python\r\nimport torch\r\nimport sys\r\n\r\ndist = '--local_rank' in ''.join(sys.argv)\r\n\r\nif dist:\r\n    torch.distributed.init_process_group(backend='nccl')\r\n\r\ndef get_available_GPUs(N, max_utilization=.5, max_memory_usage=.5):\r\n    '''\r\n    get "", ' available GPU ids with *utilization* less than ', ' and *memory usage* less than max_memory_usage\r\n    Arguments:\r\n        N (int): How many GPUs you want to select\r\n        max_utilization (float): GPU with utilization higher than ', ' is considered as not available.\r\n        max_memory_usage (float): GPU with memory usage higher than ', ' is considered as not available.\r\n\r\n    Returns:\r\n        list containing IDs of available GPUs\r\n    \'\'\'\r\n    from subprocess import Popen, PIPE\r\n    cmd = [""nvidia-smi"",\r\n           ""--query-gpu=index,utilization.gpu,memory.total,memory.used"",\r\n           ""--format=csv,noheader,nounits""]\r\n    p = Popen(cmd, stdout=PIPE)\r\n    output = p.stdout.read().decode(\'UTF-8\')\r\n    gpus = [[int(x) for x in line.split(\',\')] for line in output.splitlines()]\r\n    gpu_ids = []\r\n    for (index, utilization, total, used) in gpus:\r\n        if utilization / 100.0 < max_utilization:\r\n            if used * 1.0 / total < max_memory_usage:\r\n                gpu_ids.append(index)\r\n    if len(gpu_ids) < N:\r\n        raise Exception(""Only %s GPU(s) available but %s GPU(s) are required!"" % (len(gpu_ids), N))\r\n    available = gpu_ids[:N]\r\n    return list(available)\r\n\r\ndef select_GPUs(N_per_process, max_utilization=.5, max_memory_usage=.5):\r\n    \'\'\'\r\n    select ', ' GPUs.\r\n    If distributed training is enabled, GPUs will be assigned properly among different processes.\r\n    Arguments:\r\n        N_per_process (int): How many GPUs you want to select for each process\r\n        max_utilization (float): GPU with utilization higher than ', ' is considered as not available.\r\n        max_memory_usage (float): GPU with memory usage higher than ', "" is considered as not available.\r\n\r\n    Returns:\r\n        list containing IDs of selected GPUs\r\n    '''\r\n    if not dist:\r\n        return get_available_GPUs(N_per_process, max_utilization, max_memory_usage)\r\n    rank = torch.distributed.get_rank()\r\n    world_size = torch.distributed.get_world_size()\r\n    tensor = torch.zeros(world_size * N_per_process, dtype=torch.int).cuda()\r\n    if rank == 0:\r\n        device_ids = get_available_GPUs(world_size * N_per_process)\r\n        tensor = torch.tensor(device_ids, dtype=torch.int).cuda()\r\n    torch.distributed.broadcast(tensor, 0)\r\n    ids = list(tensor.cpu().numpy().tolist())\r\n    return ids[N_per_process * rank : N_per_process * rank + N_per_process]\r\n\r\nimport torch.nn as nn\r\ndevice_ids = select_GPUs(2)\r\noutput_device = torch.device(device_ids[0])\r\n\r\nx = torch.tensor([5.0, -1.0], dtype=torch.float).cuda(output_device).view(-1, 1)\r\n\r\nmodel = nn.Linear(in_features=1, out_features=1, bias=False).cuda(output_device)\r\n\r\nrank = torch.distributed.get_rank()\r\nmodel.weight.data.zero_()\r\nmodel.weight.data.add_(rank)\r\n\r\nprint(f'at rank {rank}, before init, the weight is {model.weight.data.item()}')\r\n\r\nmodel = torch.nn.parallel.DistributedDataParallel(model,device_ids=device_ids, output_device=output_device)\r\n\r\nprint(f'at rank {rank}, after init, the weight is {model.module.weight.data.item()}')\r\n\r\ny = model(x)\r\n\r\nlabel = torch.zeros(2, 1, dtype=torch.float).cuda(output_device)\r\n\r\nloss = torch.sum((y - label)**2)\r\n\r\nloss.backward()\r\n\r\n# print(model.weight.grad)\r\n\r\nat rank 0, before init, the weight is 0.0\r\nat rank 1, before init, the weight is 1.0\r\nat rank 0, after init, the weight is 0.0\r\n"", '', 'gloo']",1,0
62,pytorch,22866,closed,High CPU usage by torch.Tensor,"## üêõ Bug

Pytorch >= 1.0.1 uses a lot of CPU cores for making tensor from numpy array if numpy array was processed by np.transpose. The bug is not appears on pytorch 1.0.0. Nightly build has the same bug.

## To Reproduce

Steps to reproduce the behavior:

1. Install Pytorch >= 1.0.1
2. Run following code 

3. Open htop and enjoy 2500% CPU utilization by the process that is running the code.


This code (without np.transpose) works just fine - not more than 100% CPU utilization


Same abnormal behavior with torch.permute if you call it before tensor.cuda().


torch.permute after tensor.cuda() works just fine  - not more than 100% CPU utilization


## Expected behavior

I'm expecting utilization not more than 100%. 

## Environment



## Additional context

I've tried the same code on other server. On this server CPU usage is not increased so drastically but still shows 200% utilization with np.transpose and not more than 100% without.
",module: performance triaged,"['`torch.Tensor` is the legacy constructor. Use `torch.as_tensor` for copy-free construction from a NumPy array.', ""@SsnL, I think the issue is the `cuda()` call. `torch.Tensor(a)` isn't copying here. Agree that `torch.as_tensor` is the preferred style, but I don't think it affects performance."", '@colesbury Thanks, you are right :). ', ""@movchan74 using more CPU isn't a bug (it's a feature). You can control the CPU parallelization with either `OMP_NUM_THREADS=1` environment variable or `torch.set_num_threads()`.\r\n\r\nIs there a slow down? On multi-socket systems you generally want to limit the number of threads to a single socket. Try `OMP_NUM_THREADS=12`."", ""For future reference, this behavior is independent of using NumPy. It has to do with the fact that a CPU->CUDA copy of a non-contiguous Tensor requires a temporary contiguous copy. This temporary copy (non-contiguous->contiguous) is now parallelized if the Tensor is large enough. It wasn't parallelized in 1.0.0. You can see this behavior with:\r\n\r\n```python\r\nimport torch\r\nx = torch.randn(8, 224, 224, 3).permute(0, 3, 1, 2)\r\nwhile True:\r\n   x.cuda()\r\n```"", '> torch.set_num_threads()\r\n\r\n@colesbury  is there some way to set number of threads while using libtorchÔºü', '@vraivon use `torch::set_num_threads`', ""@colesbury but it will get the error: ‚Äòset_num_threads‚Äô: is not a member of 'torch'"", '@vraivon, oh in PyTorch 1.1.0 you have to use `at::set_num_threads` instead. (In PyTorch 1.2 it will be available as `torch::set_num_threads`)', ""@colesbury in pytorch it's ok, but how to set this in c++ while using libtorch? I have tried to use omp_set_num_threads(), but there‚Äôs no effect."", '@colesbury @vraivon I also have similar [issue](https://github.com/pytorch/pytorch/issues/24244) Did you find any way to fix this? I have full CPU usage although training is on GPU. \r\n\r\nI tried \r\n`set_num_threads()` and\r\n`OMP_NUM_THREADS=1`\r\n\r\n Still CPU usage is full on all 8 cores.\r\n\r\nThanks ', '@AravindChandradoss\r\n  \r\nTry : torch.set_num_threads(1)\r\n```python\r\nimport torch\r\ntorch.set_num_threads(1)\r\nx = torch.randn(8, 224, 224, 3).permute(0, 3, 1, 2)\r\nwhile True:\r\n   x.cuda()\r\n```', ""I don't think the current setting is sensible for running multiple experiments at a machine with 8 GPUs.\r\nI want to start 8 experiments at the same time, but find that the first experiment has already used most of  the CPUs."", ""Another problem is that when I had 8 experiments started, it's hard for me to edit the code as these experiments had occupied all the CPU resources. \r\nWhy tensorflow does not have this issue?\r\n"", 'I had a similar problem of high cpu usage. [This](https://discuss.pytorch.org/t/cpu-usage-extremely-high/52172/5. ) solved it for me. Set pin_memory=False in DataLoader.', '@lastlap the memory pinning bug was fixed in 1.3 btw. ', ""@SsnL Oh ok I'm using 1.1.0. Cpu usage decreased from 2500 to around 100 by setting pin_memory=False!"", 'Still the same problem, and I do not use dataloader. When I set threads to 1, the cpu usage decrease to 100%. But when I want to use multi-threads, the usage of cpu again out of control. ', '> @AravindChandradoss\r\n> \r\n> Try : torch.set_num_threads(1)\r\n> \r\n> ```python\r\n> import torch\r\n> torch.set_num_threads(1)\r\n> x = torch.randn(8, 224, 224, 3).permute(0, 3, 1, 2)\r\n> while True:\r\n>    x.cuda()\r\n> ```\r\n\r\nThis resolves the issue, Thank mate.']","['\r\nimport torch \r\nimport numpy as np\r\ndef preprocessing(batch, transpose=True):\r\n    if transpose:\r\n        batch = batch.transpose(0, 3, 1, 2)\r\n    return batch\r\n\r\nfor i in range(1000):\r\n    a = np.random.rand(8, 224, 224, 3)\r\n    a = preprocessing(a, transpose=True)\r\n    a = a.astype(np.float32)\r\n    a = torch.Tensor(a)\r\n    a = a.cuda()\r\n', '\r\nfor i in range(1000):\r\n    a = np.random.rand(8, 224, 224, 3).astype(np.float32)\r\n    a = preprocessing(a, transpose=False)\r\n    a = a.astype(np.float32)\r\n    a = torch.Tensor(a)\r\n    a = a.cuda()\r\n', '\r\nfor i in range(1000):\r\n    a = np.random.rand(8, 224, 224, 3).astype(np.float32)\r\n    a = preprocessing(a, transpose=False)\r\n    a = a.astype(np.float32)\r\n    a = torch.Tensor(a)\r\n    a = a.permute((0, 3, 1, 2))\r\n    a = a.cuda()\r\n', '\r\nfor i in range(1000):\r\n    a = np.random.rand(8, 224, 224, 3).astype(np.float32)\r\n    a = preprocessing(a, transpose=False)\r\n    a = a.astype(np.float32)\r\n    a = torch.Tensor(a)\r\n    a = a.cuda()\r\n    a = a.permute((0, 3, 1, 2))\r\n', '\r\nPyTorch version: 1.0.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: TITAN RTX\r\n\r\nNvidia driver version: 418.39\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudnn.so.7\r\n/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudnn.so.7.5.0\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.1\r\n[pip3] torch==1.0.1\r\n[pip3] torchvision==0.2.1\r\n[conda] Could not collect\r\n']",[],1,0
63,pytorch,28503,closed,Inconsistent behaviour in einsum with fp16,"## üêõ Bug

torch.einsum seems to have inconsistent behavior with half-precision tensors, as illustrated in the snippet below (minimal non-working example):


As we can see in the example above, the results we get in fp16 can be significantly different from the fp32 (all the 0.000) but are also inconsistent for the same computation as highlighted in the comments.

## To Reproduce

**See snipped above.**

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

Running the same computation multiple times should give the same result everytime for the same input.

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


 - PyTorch Version (e.g., 1.0): 1.3.0
 - OS (e.g., Linux): Ubuntu 18.04
 - How you installed PyTorch (, , source): conda
 - Build command you used (if compiling from source): N/A
 - Python version: 3.7.4
 - CUDA/cuDNN version: 10.2/7.5
 - GPU models and configuration: Code was run on a single 2080 Ti
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->
",,"['I suspect this to be a driver issue, nvidia fp16 support for gaming cards tends to lag behind datacenter versions.\r\n\r\nIt seems to work as expected in Colab (which uses Tesla/K80) \r\nhttps://colab.research.google.com/drive/1MMMRcJkvlrNFPQj1J4AktGuXjWe3vaj2#scrollTo=ZmyKW9pxwViQ', 'Hi Yaroslav\r\n\r\nThank you for your reply.\r\n\r\n### Testing on a Tesla and a Quadro with consumer cards drivers\r\n\r\nI tried on a Tesla V100 in our lab and got the following:\r\n```\r\nIn [19]: lossgh = torch.einsum(""bni,bnij,bnj->bn"", pgh, Mgh, pgh)                                                                        \r\n\r\nIn [20]: lossgh                                                                                                                       \r\nOut[20]: \r\ntensor([[0.3472, 1.6172, 3.1465,  ..., 1.0518, 0.7534, 0.4927],\r\n        [0.5010, 1.9355, 0.6270,  ..., 0.1924, 0.4868, 0.6313],\r\n        [0.5820, 0.4314, 0.3992,  ..., -0.0000, 1.8516, -0.0000],\r\n        ...,\r\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\r\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\r\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\r\n       device=\'cuda:0\', dtype=torch.float16)\r\n\r\nIn [21]: lossgh = torch.einsum(""bni,bnij,bnj->bn"", pgh, Mgh, pgh)                                                                        \r\n\r\nIn [22]: lossgh                                                                                                                       \r\nOut[22]: \r\ntensor([[ 0.3472,  1.6172,  3.1465,  ...,  1.0518,  0.7534,  0.4927],\r\n        [ 0.5010,  1.9355,  0.6270,  ...,  0.1924,  0.4868,  0.6313],\r\n        [ 0.5820,  0.4314,  0.3992,  ..., -0.0000,  1.8516, -0.0000],\r\n        ...,\r\n        [ 1.9775,  0.0000,  2.0000,  ...,  1.9814,  0.0000, -0.0000],\r\n        [ 2.0000,  1.9570,  0.0000,  ..., -0.0000,  1.9883,  0.0000],\r\n        [ 2.0000,  0.0000,  1.9727,  ..., -1.0000,  0.0078,  1.9727]],\r\n       device=\'cuda:0\', dtype=torch.float16)\r\n```\r\nOn a Quadro GV100 on the same machine:\r\n```\r\nIn [12]: lossgh = torch.einsum(""bni,bnij,bnj->bn"", pgh, Mgh, pgh)                                                                        \r\n\r\nIn [13]: lossgh                                                                                                                       \r\nOut[13]: \r\ntensor([[2.2715, 0.7007, 0.7754,  ..., 0.3767, 1.0098, 1.4912],\r\n        [1.7637, 1.4043, 1.7441,  ..., 0.1835, 0.2776, 1.8516],\r\n        [2.9922, 1.2471, 1.9141,  ..., 3.4102, 0.5552, 0.5146],\r\n        ...,\r\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\r\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\r\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\r\n       device=\'cuda:0\', dtype=torch.float16)\r\n\r\nIn [14]: lossgh = torch.einsum(""bni,bnij,bnj->bn"", pgh, Mgh, pgh)                                                                        \r\n\r\nIn [15]: lossgh                                                                                                                       \r\nOut[15]: \r\ntensor([[ 2.2715,  0.7007,  0.7754,  ...,  0.3767,  1.0098,  1.4912],\r\n        [ 1.7637,  1.4043,  1.7441,  ...,  0.1835,  0.2776,  1.8516],\r\n        [ 2.9922,  1.2471,  1.9141,  ...,  3.4102,  0.5552,  0.5146],\r\n        ...,\r\n        [ 1.8809,  0.0078,  2.1270,  ...,  1.7637, -2.0000,  1.7529],\r\n        [ 0.0000,  1.8682,  0.0078,  ..., -2.0000,  1.8867,  0.0078],\r\n        [ 1.4990, -0.0078,  1.7617,  ...,  1.7480,  2.0000,  2.0098]],\r\n       device=\'cuda:0\', dtype=torch.float16)\r\n```\r\nThe other GPUs on that server are older Titan Xp and the driver is a bit old (418.56) and I suppose it\'s a GeForce driver. CUDA is version 10.1.\r\n\r\nSo this means the problem arises on Quadro and Tesla cards and the driver can be the culprit.\r\n\r\n### Testing on AWS\r\n\r\nnvidia-smi:\r\n\r\n```\r\n(env) ubuntu@ip-172-31-33-140:/mnt/data/data$ nvidia-smi\r\nWed Oct 23 18:54:15 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.87.00    Driver Version: 418.87.00    CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\r\n| N/A   33C    P0    28W / 300W |      0MiB / 16130MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nTest:\r\n\r\nAWS p3.2xlarge with the latest deep learning AMI.\r\n\r\n```\r\n(env) ubuntu@ip-172-31-33-140:/mnt/data/data$ ipython\r\nPython 3.7.4 (default, Aug 13 2019, 20:35:49) \r\nType \'copyright\', \'credits\' or \'license\' for more information\r\nIPython 7.8.0 -- An enhanced Interactive Python. Type \'?\' for help.\r\n\r\nIn [1]: import torch                                                                                                                 \r\n\r\nIn [2]: torch.__version__                                                                                                            \r\nOut[2]: \'1.3.0\'\r\n\r\nIn [3]: M = torch.rand(8,29495,3,3) \r\n   ...: p = torch.rand(8,29495,3) \r\n   ...: Mg = M.to(\'cuda:0\') \r\n   ...: pg = p.to(\'cuda:0\') \r\n   ...: loss = torch.einsum(""bni,bnij,bnj->bn"", p, M, p) \r\n   ...: lossg = torch.einsum(""bni,bnij,bnj->bn"", pg, Mg, pg)                                                                         \r\n\r\nIn [4]: loss                                                                                                                         \r\nOut[4]: \r\ntensor([[0.1235, 2.0953, 0.8870,  ..., 1.9333, 1.8819, 0.9468],\r\n        [1.4721, 1.4364, 2.4750,  ..., 1.8208, 2.3497, 0.3136],\r\n        [0.5731, 0.2551, 0.9414,  ..., 1.8437, 1.0861, 0.8178],\r\n        ...,\r\n        [3.0937, 0.8668, 1.3874,  ..., 1.7695, 0.2889, 1.5187],\r\n        [0.8046, 0.9171, 1.5238,  ..., 1.1095, 0.3080, 1.9211],\r\n        [3.5347, 1.2077, 0.8924,  ..., 0.2202, 3.2969, 2.1403]])\r\n\r\nIn [5]: lossg                                                                                                                        \r\nOut[5]: \r\ntensor([[0.1235, 2.0953, 0.8870,  ..., 1.9333, 1.8819, 0.9468],\r\n        [1.4721, 1.4364, 2.4750,  ..., 1.8208, 2.3497, 0.3136],\r\n        [0.5731, 0.2551, 0.9414,  ..., 1.8437, 1.0861, 0.8178],\r\n        ...,\r\n        [3.0937, 0.8668, 1.3874,  ..., 1.7695, 0.2889, 1.5187],\r\n        [0.8046, 0.9171, 1.5238,  ..., 1.1095, 0.3080, 1.9211],\r\n        [3.5347, 1.2077, 0.8924,  ..., 0.2202, 3.2969, 2.1403]],\r\n       device=\'cuda:0\')\r\n\r\nIn [6]: torch.einsum(""bni,bnij,bnj->bn"", pg, Mg, pg)                                                                                 \r\nOut[6]: \r\ntensor([[0.1235, 2.0953, 0.8870,  ..., 1.9333, 1.8819, 0.9468],\r\n        [1.4721, 1.4364, 2.4750,  ..., 1.8208, 2.3497, 0.3136],\r\n        [0.5731, 0.2551, 0.9414,  ..., 1.8437, 1.0861, 0.8178],\r\n        ...,\r\n        [3.0937, 0.8668, 1.3874,  ..., 1.7695, 0.2889, 1.5187],\r\n        [0.8046, 0.9171, 1.5238,  ..., 1.1095, 0.3080, 1.9211],\r\n        [3.5347, 1.2077, 0.8924,  ..., 0.2202, 3.2969, 2.1403]],\r\n       device=\'cuda:0\')\r\n\r\nIn [7]: torch.einsum(""bni,bnij,bnj->bn"", pg, Mg, pg)                                                                                 \r\nOut[7]: \r\ntensor([[0.1235, 2.0953, 0.8870,  ..., 1.9333, 1.8819, 0.9468],\r\n        [1.4721, 1.4364, 2.4750,  ..., 1.8208, 2.3497, 0.3136],\r\n        [0.5731, 0.2551, 0.9414,  ..., 1.8437, 1.0861, 0.8178],\r\n        ...,\r\n        [3.0937, 0.8668, 1.3874,  ..., 1.7695, 0.2889, 1.5187],\r\n        [0.8046, 0.9171, 1.5238,  ..., 1.1095, 0.3080, 1.9211],\r\n        [3.5347, 1.2077, 0.8924,  ..., 0.2202, 3.2969, 2.1403]],\r\n       device=\'cuda:0\')\r\n\r\nIn [8]: Mgh = Mg.half() \r\n   ...: pgh = pg.half()                                                                                                              \r\n\r\nIn [9]: torch.einsum(""bni,bnij,bnj->bn"", pgh, Mgh, pgh)                                                                              \r\nOut[9]: \r\ntensor([[0.1235, 2.0957, 0.8872,  ..., 1.9326, 1.8818, 0.9463],\r\n        [1.4717, 1.4365, 2.4746,  ..., 1.8203, 2.3496, 0.3135],\r\n        [0.5732, 0.2551, 0.9414,  ..., 1.8428, 1.0859, 0.8174],\r\n        ...,\r\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\r\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\r\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\r\n       device=\'cuda:0\', dtype=torch.float16)\r\n\r\nIn [10]: torch.einsum(""bni,bnij,bnj->bn"", pgh, Mgh, pgh)                                                                             \r\nOut[10]: \r\ntensor([[0.1235, 2.0957, 0.8872,  ..., 1.9326, 1.8818, 0.9463],\r\n        [1.4717, 1.4365, 2.4746,  ..., 1.8203, 2.3496, 0.3135],\r\n        [0.5732, 0.2551, 0.9414,  ..., 1.8428, 1.0859, 0.8174],\r\n        ...,\r\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\r\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\r\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\r\n       device=\'cuda:0\', dtype=torch.float16)\r\n\r\nIn [11]: torch.einsum(""bni,bnij,bnj->bn"", pgh, Mgh, pgh)                                                                             \r\nOut[11]: \r\ntensor([[ 1.2354e-01,  2.0957e+00,  8.8721e-01,  ...,  1.9326e+00,\r\n          1.8818e+00,  9.4629e-01],\r\n        [ 1.4717e+00,  1.4365e+00,  2.4746e+00,  ...,  1.8203e+00,\r\n          2.3496e+00,  3.1348e-01],\r\n        [ 5.7324e-01,  2.5513e-01,  9.4141e-01,  ...,  1.8428e+00,\r\n          1.0859e+00,  8.1738e-01],\r\n        ...,\r\n        [ 1.8301e+00,  2.0000e+00,  1.8350e+00,  ...,  1.8857e+00,\r\n          2.0000e+00,  1.8291e+00],\r\n        [-7.8125e-03,  1.8564e+00, -2.0000e+00,  ..., -7.8125e-03,\r\n          1.6777e+00, -2.0000e+00],\r\n        [ 1.9785e+00,  2.0000e+00,  1.8711e+00,  ...,  1.9199e+00,\r\n         -5.1200e+02,  2.0957e+00]], device=\'cuda:0\', dtype=torch.float16)\r\n\r\nIn [12]: torch.einsum(""bni,bnij,bnj->bn"", pgh, Mgh, pgh)                                                                             \r\nOut[12]: \r\ntensor([[ 1.2354e-01,  2.0957e+00,  8.8721e-01,  ...,  1.9326e+00,\r\n          1.8818e+00,  9.4629e-01],\r\n        [ 1.4717e+00,  1.4365e+00,  2.4746e+00,  ...,  1.8203e+00,\r\n          2.3496e+00,  3.1348e-01],\r\n        [ 5.7324e-01,  2.5513e-01,  9.4141e-01,  ...,  1.8428e+00,\r\n          1.0859e+00,  8.1738e-01],\r\n        ...,\r\n        [ 1.8301e+00,  2.0000e+00,  1.8350e+00,  ...,  1.8857e+00,\r\n          2.0000e+00,  1.8291e+00],\r\n        [-7.8125e-03,  1.8564e+00, -2.0000e+00,  ..., -7.8125e-03,\r\n          1.6777e+00, -2.0000e+00],\r\n        [ 1.9785e+00,  2.0000e+00,  1.8711e+00,  ...,  1.9199e+00,\r\n         -5.1200e+02,  2.0957e+00]], device=\'cuda:0\', dtype=torch.float16)\r\n\r\nIn [13]: torch.einsum(""bni,bnij,bnj->bn"", pgh, Mgh, pgh)                                                                             \r\nOut[13]: \r\ntensor([[ 1.2354e-01,  2.0957e+00,  8.8721e-01,  ...,  1.9326e+00,\r\n          1.8818e+00,  9.4629e-01],\r\n        [ 1.4717e+00,  1.4365e+00,  2.4746e+00,  ...,  1.8203e+00,\r\n          2.3496e+00,  3.1348e-01],\r\n        [ 5.7324e-01,  2.5513e-01,  9.4141e-01,  ...,  1.8428e+00,\r\n          1.0859e+00,  8.1738e-01],\r\n        ...,\r\n        [ 1.8301e+00,  2.0000e+00,  1.8350e+00,  ...,  1.8857e+00,\r\n          2.0000e+00,  1.8291e+00],\r\n        [-7.8125e-03,  1.8564e+00, -2.0000e+00,  ..., -7.8125e-03,\r\n          1.6777e+00, -2.0000e+00],\r\n        [ 1.9785e+00,  2.0000e+00,  1.8711e+00,  ...,  1.9199e+00,\r\n         -5.1200e+02,  2.0957e+00]], device=\'cuda:0\', dtype=torch.float16)\r\n```\r\n\r\nI\'m setting up a brand new instance, installing cuda and the drivers from scratch, and will also test with pytorch 1.3.0 outside of any conda environment.', 'First I should take a second to indicate the CUDA version that was actually loaded in the previous tests, as installed by conda and as in the path on the machines, was probably 10.0 in spite of what nvidia-smi was showing.\r\n\r\nBrand new installation of CUDA 10.1 and CUDNN 7.6. System python (3.6.8). GCC 8.3.0\r\n\r\n### New virtualenv\r\n\r\n`ubuntu@ip-172-31-40-147:~$ nvidia-smi\r\nWed Oct 23 19:33:33 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.87.01    Driver Version: 418.87.01    CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\r\n| N/A   41C    P0    39W / 300W |      0MiB / 16130MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+`\r\n\r\nResults:\r\n```\r\nIn [13]: Mgh = Mg.half()                                                                                                             \r\n\r\nIn [14]: pgh = pg.half()                                                                                                             \r\n\r\nIn [15]: torch.einsum(""bni,bnij,bnj->bn"", pgh, Mgh, pgh)                                                                             \r\nOut[15]: \r\ntensor([[0.4292, 1.9844, 1.3896,  ..., 0.2407, 1.8740, 1.8057],\r\n        [0.9395, 3.2715, 1.4238,  ..., 0.4253, 0.4619, 1.1816],\r\n        [1.9424, 0.7715, 1.0586,  ..., 0.3799, 0.6938, 0.2861],\r\n        ...,\r\n        [0.8540, 0.0280, 3.7539,  ..., 3.0566, 0.3833, 4.7695],\r\n        [3.2051, 4.1250, 1.6572,  ..., 1.3770, 0.9238, 1.3057],\r\n        [1.3525, 1.0771, 0.1671,  ..., 5.8398, 1.1250, 0.3889]],\r\n       device=\'cuda:0\', dtype=torch.float16)\r\n\r\nIn [16]: torch.einsum(""bni,bnij,bnj->bn"", pgh, Mgh, pgh)                                                                             \r\nOut[16]: \r\ntensor([[0.4292, 1.9844, 1.3896,  ..., 0.2407, 1.8740, 1.8057],\r\n        [0.9395, 3.2715, 1.4238,  ..., 0.4253, 0.4619, 1.1816],\r\n        [1.9424, 0.7715, 1.0586,  ..., 0.3799, 0.6938, 0.2861],\r\n        ...,\r\n        [0.8540, 0.0280, 3.7539,  ..., 3.0566, 0.3833, 4.7695],\r\n        [3.2051, 4.1250, 1.6572,  ..., 1.3770, 0.9238, 1.3057],\r\n        [1.3525, 1.0771, 0.1671,  ..., 5.8398, 1.1250, 0.3889]],\r\n       device=\'cuda:0\', dtype=torch.float16)\r\n\r\nIn [17]: torch.einsum(""bni,bnij,bnj->bn"", pgh, Mgh, pgh)                                                                             \r\nOut[17]: \r\ntensor([[0.4292, 1.9844, 1.3896,  ..., 0.2407, 1.8740, 1.8057],\r\n        [0.9395, 3.2715, 1.4238,  ..., 0.4253, 0.4619, 1.1816],\r\n        [1.9424, 0.7715, 1.0586,  ..., 0.3799, 0.6938, 0.2861],\r\n        ...,\r\n        [0.8540, 0.0280, 3.7539,  ..., 3.0566, 0.3833, 4.7695],\r\n        [3.2051, 4.1250, 1.6572,  ..., 1.3770, 0.9238, 1.3057],\r\n        [1.3525, 1.0771, 0.1671,  ..., 5.8398, 1.1250, 0.3889]],\r\n       device=\'cuda:0\', dtype=torch.float16)\r\n\r\nIn [18]: torch.einsum(""bni,bnij,bnj->bn"", pgh, Mgh, pgh)                                                                             \r\nOut[18]: \r\ntensor([[0.4292, 1.9844, 1.3896,  ..., 0.2407, 1.8740, 1.8057],\r\n        [0.9395, 3.2715, 1.4238,  ..., 0.4253, 0.4619, 1.1816],\r\n        [1.9424, 0.7715, 1.0586,  ..., 0.3799, 0.6938, 0.2861],\r\n        ...,\r\n        [0.8540, 0.0280, 3.7539,  ..., 3.0566, 0.3833, 4.7695],\r\n        [3.2051, 4.1250, 1.6572,  ..., 1.3770, 0.9238, 1.3057],\r\n        [1.3525, 1.0771, 0.1671,  ..., 5.8398, 1.1250, 0.3889]],\r\n       device=\'cuda:0\', dtype=torch.float16)\r\n\r\nIn [19]: torch.einsum(""bni,bnij,bnj->bn"", pgh, Mgh, pgh)                                                                             \r\nOut[19]: \r\ntensor([[0.4292, 1.9844, 1.3896,  ..., 0.2407, 1.8740, 1.8057],\r\n        [0.9395, 3.2715, 1.4238,  ..., 0.4253, 0.4619, 1.1816],\r\n        [1.9424, 0.7715, 1.0586,  ..., 0.3799, 0.6938, 0.2861],\r\n        ...,\r\n        [0.8540, 0.0280, 3.7539,  ..., 3.0566, 0.3833, 4.7695],\r\n        [3.2051, 4.1250, 1.6572,  ..., 1.3770, 0.9238, 1.3057],\r\n        [1.3525, 1.0771, 0.1671,  ..., 5.8398, 1.1250, 0.3889]],\r\n       device=\'cuda:0\', dtype=torch.float16)\r\n\r\n```\r\n### Old virtualenv exported with conda pack\r\n```\r\n(env) ubuntu@ip-172-31-40-147:~$ ipython\r\nPython 3.7.4 (default, Aug 13 2019, 20:35:49) \r\nType \'copyright\', \'credits\' or \'license\' for more information\r\nIPython 7.8.0 -- An enhanced Interactive Python. Type \'?\' for help.\r\n\r\nIn [1]: In [1]: import torch                                                                                                         \r\n   ...:                                                                                                                              \r\n\r\nIn [2]:                                                                                                                              \r\n\r\nIn [2]: In [2]: torch.__version__                                                                                                    \r\n   ...:                                                                                                                              \r\nOut[2]: \'1.3.0\'\r\n\r\nIn [3]: Out[2]: \'1.3.0\'                                                                                                              \r\n\r\nIn [4]:                                                                                                                              \r\n\r\nIn [4]: In [3]: M = torch.rand(8,29495,3,3)                                                                                          \r\n\r\nIn [5]:    ...: p = torch.rand(8,29495,3)                                                                                            \r\n\r\nIn [6]:    ...: Mg = M.to(\'cuda:0\')                                                                                                  \r\n\r\nIn [7]:    ...: pg = p.to(\'cuda:0\')                                                                                                  \r\n\r\nIn [8]:    ...: loss = torch.einsum(""bni,bnij,bnj->bn"", p, M, p)                                                                     \r\n\r\nIn [9]:    ...: lossg = torch.einsum(""bni,bnij,bnj->bn"", pg, Mg, pg)                                                                 \r\n\r\nIn [10]:                                                                                                                             \r\n\r\nIn [10]: In [8]: Mgh = Mg.half()                                                                                                     \r\n\r\nIn [11]:    ...: pgh = pg.half()                                                                                                     \r\n\r\nIn [12]: torch.einsum(""bni,bnij,bnj->bn"", pgh, Mgh, pgh)                                                                             \r\nOut[12]: \r\ntensor([[0.5703, 0.7681, 1.4316,  ..., 0.1120, 0.7637, 2.1914],\r\n        [1.3281, 1.1641, 0.2944,  ..., 0.9458, 2.2715, 1.2393],\r\n        [2.2520, 0.7188, 2.2969,  ..., 3.1875, 0.7100, 0.2688],\r\n        ...,\r\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\r\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\r\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\r\n       device=\'cuda:0\', dtype=torch.float16)\r\n\r\nIn [13]: torch.einsum(""bni,bnij,bnj->bn"", pgh, Mgh, pgh)                                                                             \r\nOut[13]: \r\ntensor([[0.5703, 0.7681, 1.4316,  ..., 0.1120, 0.7637, 2.1914],\r\n        [1.3281, 1.1641, 0.2944,  ..., 0.9458, 2.2715, 1.2393],\r\n        [2.2520, 0.7188, 2.2969,  ..., 3.1875, 0.7100, 0.2688],\r\n        ...,\r\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\r\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\r\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\r\n       device=\'cuda:0\', dtype=torch.float16)\r\n\r\nIn [14]: torch.einsum(""bni,bnij,bnj->bn"", pgh, Mgh, pgh)                                                                             \r\nOut[14]: \r\ntensor([[ 0.5703,  0.7681,  1.4316,  ...,  0.1120,  0.7637,  2.1914],\r\n        [ 1.3281,  1.1641,  0.2944,  ...,  0.9458,  2.2715,  1.2393],\r\n        [ 2.2520,  0.7188,  2.2969,  ...,  3.1875,  0.7100,  0.2688],\r\n        ...,\r\n        [ 1.8809,  2.0000,  1.6221,  ...,  1.8018, -0.0078,  1.6338],\r\n        [-0.0078,  1.6484,  0.0000,  ..., -0.0078,  1.9238,  2.0000],\r\n        [ 1.9580,  2.0000,  1.8965,  ...,  1.9727, -2.0000,  1.9512]],\r\n       device=\'cuda:0\', dtype=torch.float16)\r\n\r\n```\r\n\r\nBased on these results, it doesn\'t seem like the driver is causing the issue since the second env has the bug on the same machine.\r\n\r\nI will try to look deeper into this to find what the important differences are between the two environments. So far I can see:\r\n- Different CUDA version (10.0 in the conda env, 10.1 on the system)\r\n- Different GCC version (7.3 in the conda env, 8.3 on the system)\r\n- Different python version (3.7.4 in the conda env, 3.6.4 on the system)\r\n\r\nI\'ve disabled cudnn to re-run the test and also had the issue, so I think we can exclude that.', ""I've updated CUDA to 10.1 in the conda environment and now it's working as expected on a 2080 Ti.\r\n\r\nMy apologies for the unnecessary report."", 'Please re-open the issue if you still have a problem.']","['\r\nimport torch\r\nM = torch.rand(8,29495,3,3)\r\np = torch.rand(8,29495,3)\r\nMg = M.to(\'cuda:0\')\r\npg = p.to(\'cuda:0\')\r\nloss = torch.einsum(""bni,bnij,bnj->bn"", p, M, p)\r\nlossg = torch.einsum(""bni,bnij,bnj->bn"", pg, Mg, pg)\r\n\r\nloss\r\nOut[8]:\r\ntensor([[2.1239, 0.9082, 1.4092,  ..., 0.9028, 1.6758, 1.9743],\r\n        [1.6000, 0.0101, 1.7230,  ..., 1.4759, 0.3911, 1.5262],\r\n        [1.0034, 1.1164, 1.5749,  ..., 2.0209, 1.0497, 2.4276],\r\n        ...,\r\n        [0.5796, 2.0528, 0.2507,  ..., 0.0232, 2.2315, 0.5563],\r\n        [0.0189, 2.1756, 1.6934,  ..., 2.5696, 0.6931, 2.1793],\r\n        [1.0421, 0.6458, 1.0652,  ..., 0.2180, 1.9430, 1.5560]])\r\n\r\nlossg\r\nOut[9]:\r\ntensor([[2.1239, 0.9082, 1.4092,  ..., 0.9028, 1.6758, 1.9743],\r\n        [1.6000, 0.0101, 1.7230,  ..., 1.4759, 0.3911, 1.5262],\r\n        [1.0034, 1.1164, 1.5749,  ..., 2.0209, 1.0497, 2.4276],\r\n        ...,\r\n        [0.5796, 2.0528, 0.2507,  ..., 0.0232, 2.2315, 0.5563],\r\n        [0.0189, 2.1756, 1.6934,  ..., 2.5696, 0.6931, 2.1793],\r\n        [1.0421, 0.6458, 1.0652,  ..., 0.2180, 1.9430, 1.5560]],\r\n       device=\'cuda:0\')\r\n\r\n# Recompute the loss in fp32\r\nlossg = torch.einsum(""bni,bnij,bnj->bn"", pg, Mg, pg)\r\nlossg\r\n# Consistent result\r\nOut[11]:\r\ntensor([[2.1239, 0.9082, 1.4092,  ..., 0.9028, 1.6758, 1.9743],\r\n        [1.6000, 0.0101, 1.7230,  ..., 1.4759, 0.3911, 1.5262],\r\n        [1.0034, 1.1164, 1.5749,  ..., 2.0209, 1.0497, 2.4276],\r\n        ...,\r\n        [0.5796, 2.0528, 0.2507,  ..., 0.0232, 2.2315, 0.5563],\r\n        [0.0189, 2.1756, 1.6934,  ..., 2.5696, 0.6931, 2.1793],\r\n        [1.0421, 0.6458, 1.0652,  ..., 0.2180, 1.9430, 1.5560]],\r\n       device=\'cuda:0\')\r\n\r\n# Half-precision\r\nMgh = Mg.half()\r\npgh = pg.half()\r\n\r\nlossgh = torch.einsum(""bni,bnij,bnj->bn"", pgh, Mgh, pgh)\r\nlossgh\r\nOut[15]:\r\ntensor([[2.1250, 0.9087, 1.4092,  ..., 0.9028, 1.6768, 1.9736],\r\n        [1.5996, 0.0101, 1.7236,  ..., 1.4756, 0.3914, 1.5264],\r\n        [1.0029, 1.1162, 1.5742,  ..., 0.0000, 0.0000, 0.0000], # <-- Here\r\n        ...,\r\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\r\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\r\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\r\n       device=\'cuda:0\', dtype=torch.float16)\r\n\r\n# Compute it again\r\nlossgh = torch.einsum(""bni,bnij,bnj->bn"", pgh, Mgh, pgh)\r\nlossgh\r\nOut[17]:\r\ntensor([[ 2.1250,  0.9087,  1.4092,  ...,  0.9028,  1.6768,  1.9736],\r\n        [ 1.5996,  0.0101,  1.7236,  ...,  1.4756,  0.3914,  1.5264],\r\n        [ 1.0029,  1.1162,  1.5742,  ...,  2.0000,  1.7764, -2.0000], # <-- Here\r\n        ...,\r\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\r\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\r\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\r\n       device=\'cuda:0\', dtype=torch.float16)\r\n', '\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n']","['conda', 'pip']",1,0
64,pytorch,14687,open,Advanced indexing slower than numpy,"## üêõ Bug

Advanced indexing is slower than numpy.

## To Reproduce

Steps to reproduce the behavior:



## Expected behavior

PyTorch should be as fast as numpy when doing advanced indexing of tensors that do not require grad.

## Environment

 - PyTorch Version (e.g., 1.0): 1.0.0.dev20181202
 - OS (e.g., Linux): Ubuntu 18.04.1 LTS
 - How you installed PyTorch (, , source): conda install pytorch-nightly -c pytorch
 - Python version: 3.7.0


cc @mruberry @rgommers @heitorschueroff @VitalyFedyunin @ngimel",module: advanced indexing module: numpy module: performance triaged,"['Thanks for filing the issue @jeromerony . @colesbury can you take a look at this?', 'The overhead of the nonzero call is dominating here. This is a pretty small input. If you increase the size to 1024x1024, then PyTorch is much faster than NumPy. (**EDIT**: re-measured timings)\r\n\r\nIf someone wants to speed-up THTensor_(nonzero), then this case will speed up too.', '@colesbury just out of curiosity, does numpy call `nonzero` for masked indexing as well? I suppose so, given that it makes things simpler.', '@fmassa it looks like NumPy has an implementation for `nonzero` specifically for masked indexing:\r\n\r\nhttps://github.com/numpy/numpy/blob/04c2f33843782cd27a16f919477b1b27c4b01e5a/numpy/core/src/multiarray/mapping.c#L2159', ""@colesbury I had an experience that I originally used numpy within an operation in CNN and I thought using torch tensor would be faster. But that wasn't the fact. I had a lot of indexing in the array in that operation. Indexing in torch tensor is at least 10 times slower than numpy array.\r\n\r\nHere is a toy example. Although in my case, the index position is determined during running.\r\n```\r\nIn [3]: x = torch.randn(20,20,20,20)\r\n\r\nIn [4]: x_np = x.numpy()\r\n\r\nIn [5]: %timeit x[10, 10, 10, 10]\r\nThe slowest run took 8.93 times longer than the fastest. This could mean that an intermediate result is being cached.\r\n100000 loops, best of 3: 5.66 ¬µs per loop\r\n\r\nIn [6]: %timeit x_np[10, 10, 10, 10]\r\nThe slowest run took 72.68 times longer than the fastest. This could mean that an intermediate result is being cached.\r\n10000000 loops, best of 3: 180 ns per loop\r\n```\r\n\r\nI think indexing time between numpy array and torch tensor should be almost the same.\r\n"", ""@harryhan618 that's an issue, but it's not advanced indexing. see #5388""]","[""python\r\nPython 3.7.0 (default, Oct  9 2018, 10:31:47) \r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 7.0.1 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: import torch                                                                                                                                            \r\n\r\nIn [2]: torch.__version__                                                                                                                                       \r\nOut[2]: '1.0.0.dev20181202'\r\n\r\nIn [3]: x = torch.randn(1024)                                                                                                                                   \r\n\r\nIn [4]: idx = torch.rand(1024) > 0.5                                                                                                                            \r\n\r\nIn [5]: %%timeit \r\n   ...: x[idx] \r\n   ...:                                                                                                                                                   \r\n35 ¬µs ¬± 63.3 ns per loop (mean ¬± std. dev. of 7 runs, 10000 loops each)\r\n\r\nIn [6]: x_np, idx_np = x.numpy(), idx.numpy()                                                                                                                   \r\n\r\nIn [7]: %%timeit \r\n   ...: x_np[idx_np] \r\n   ...:                                                                                                                                                 \r\n2.36 ¬µs ¬± 8.62 ns per loop (mean ¬± std. dev. of 7 runs, 100000 loops each)\r\n""]","['conda', 'pip']",1,0
65,pytorch,18786,closed,tensor.cuda() is slow after invoking model(input) and loss.backward(),"## ‚ùì Questions and Help

### I'm trying to test the costing of time in the main procedures in train-step, in order to accelerate my training. Then I found that  become slow after executing  and 
Here are the codes and the results with only load batch from dataloader and convert it to cuda:

    for i, (b, target) in enumerate(train_loader):
        start = time.time()
        target_cuda = target.cuda()
        input = b.cuda().half()
        now = time.time()
        cost = now - start
        print(cost)
        # output = model(input)
        # loss = criterion(output[0], target_cuda)
        # loss.backward()model(input)loss.backward()torch.cuda.synchronize().cuda()backward().`
- [Discussion Forum](https://discuss.pytorch.org/)
",,"['After adding `torch.cuda.synchronize()` behind `model(input)` and `loss.backward()`. I can see the real cost of time. So if where is a way to accelerate `loss.backward()`? Does tensor core can do that?', 'The issue tracker is not an appropriate venue to debug slowness in your model. I recommend posting your full model on the forums and letting them help you. If you find there is some specific operation in PyTorch which ought to be faster, file a bug for it.']",[],"['tensor.cuda()', 'inference', 'backward.', ""\r\n  \r\nResults:\r\n0.02122783660888672\r\n0.06698179244995117\r\n0.02122044563293457\r\n0.02604532241821289\r\n0.020719528198242188\r\n0.02075052261352539\r\n0.02080059051513672\r\n0.020093202590942383\r\n0.0195310115814209\r\n0.0453181266784668\r\n0.03698134422302246\r\n0.03193855285644531\r\n0.020868778228759766\r\n0.03483223915100098\r\n0.0204164981842041\r\nIt's really fast.\r\nAnd after I cancel the comments in inference and backward. \r\nResult: with "", ' and ', ""\r\n0.020980358123779297\r\n0.27107763290405273\r\n0.10506486892700195\r\n0.668220043182373\r\n0.6554262638092041\r\n0.6526932716369629\r\n0.6525142192840576\r\n0.6601922512054443\r\n0.6615090370178223\r\n0.6599345207214355\r\n0.6484317779541016\r\n0.6491255760192871\r\n0.6507673263549805\r\n0.651357889175415\r\nIt's really slow, and it takes the most of time in current train-step.\r\nI also add "", '  following the guide in [here](https://discuss.pytorch.org/t/cuda-extremely-slow-after-calling-loss-backward/34371). It do make ', ' faster but slow ']",1,0
66,pytorch,18277,closed,Precision loss of large tensor averaging on CPU,"## üêõ Bug

Taking  of a large CPU tensor results in severe precision loss. 
I would like to know if the following behaviour is known and normal. 

## To Reproduce


<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->
produces 




## Expected behavior

Close match between CPU and GPU versions of mean operation. 

## Environment

PyTorch version: 1.0.1.post2
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 18.04.1 LTS
GCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
CMake version: version 3.10.2

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce GTX 1080 Ti
GPU 1: GeForce GTX 1080 Ti

Nvidia driver version: 390.116
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.16.2
[conda] blas                      1.0                         mkl  
[conda] mkl                       2019.1                      144  
[conda] mkl-service               1.1.2            py37he904b0f_5  
[conda] mkl_fft                   1.0.6            py37hd81dba3_0  
[conda] mkl_random                1.0.2            py37hd81dba3_0  
[conda] pytorch                   1.0.1           py3.7_cuda9.0.176_cudnn7.4.2_2    pytorch
[conda] torchvision               0.2.1                      py_2    pytorch

",,"['this definitely looks like normal behavior. It\'s standard floating point effects at play.\r\nIf you want higher precision, you can try to use double precision:\r\n\r\n```\r\nprint(tr.ones(*shape, dtype=torch.float64).fill_(i).mean(), ""   \\t"",  tr.ones(*shape, dtype=torch.float64).fill_(i).cuda().mean())\r\n```', ""double precision gives the output:\r\n\r\n```\r\ntensor(9991., dtype=torch.float64)       tensor(9991., device='cuda:0', dtype=torch.float64)\r\ntensor(9992., dtype=torch.float64)       tensor(9992., device='cuda:0', dtype=torch.float64)\r\ntensor(9993., dtype=torch.float64)       tensor(9993., device='cuda:0', dtype=torch.float64)\r\ntensor(9994., dtype=torch.float64)       tensor(9994., device='cuda:0', dtype=torch.float64)\r\ntensor(9995., dtype=torch.float64)       tensor(9995., device='cuda:0', dtype=torch.float64)\r\ntensor(9996., dtype=torch.float64)       tensor(9996., device='cuda:0', dtype=torch.float64)\r\ntensor(9997., dtype=torch.float64)       tensor(9997., device='cuda:0', dtype=torch.float64)\r\ntensor(9998., dtype=torch.float64)       tensor(9998., device='cuda:0', dtype=torch.float64)\r\ntensor(9999., dtype=torch.float64)       tensor(9999., device='cuda:0', dtype=torch.float64)\r\n```""]","['\r\nshape = (1000, 1000)\r\nfor i  in range(10000):\r\n    if i > 9990: \r\n        print(tr.ones(*shape).fill_(i).mean(), ""   \\t"",  tr.ones(*shape).fill_(i).cuda().mean())\r\n', ""\r\ntensor(9991.5693)    \t tensor(9991., device='cuda:0')\r\ntensor(9991.9990)    \t tensor(9991.9990, device='cuda:0')\r\ntensor(9992.4297)    \t tensor(9993., device='cuda:0')\r\ntensor(9993.7197)    \t tensor(9994., device='cuda:0')\r\ntensor(9995.5684)    \t tensor(9995., device='cuda:0')\r\ntensor(9996.)    \t tensor(9996., device='cuda:0')\r\ntensor(9996.4287)    \t tensor(9997.0010, device='cuda:0')\r\ntensor(9998.2793)    \t tensor(9998., device='cuda:0')\r\ntensor(9999.5693)    \t tensor(9999., device='cuda:0')\r\n""]",['mean()'],1,0
67,pytorch,19827,closed,Random GPU Operations Hang,"## üêõ Bug

I'm having an issue where certain GPU operations hang. It doesn't appear to be a multi-GPU problem, I can reproduce it on a single GPU. It doesn't seem to reliably effect the same operations but if I change around their order or cache certain results on the GPU to minimize copies it could cause the hang to move to another command. 

## To Reproduce

For my current code I can reproduce the problem fairly reliably with the following command:



e.g. indexing the tensor  which is stored on GPU. I should point out that I was seeing this on other commands, usually copying something onto the GPU, but as I mentioned after changing how i was using the GPU by caching certain results to limit copies and reordering things, it is happening on this line. 

Here is the time taken by this command for several different images:



Here's what the above numbers mean. The first  is telling me which line is printing, so it can be ignored. The next number is how long it took to execute the offending line in seconds. The images are all of comparable size. Newline separates each image.

So you can see the first image executes both lines without problem. The next one, only the first line is slow and it takes over 30s to execute. This trend continues for the rest of the images.


## Environment



## Additional context

My first thought was that this is something to do with garbage collection on the GPU, or something to do with the multi GPU IOMMU problem other people have had, but I'm not so sure anymore, especially since I can reproduce on a single GPU.
",awaiting response (this tag is deprecated) module: cuda module: deadlock triaged,"[""Can you provide a minimal example that enables us to reproduce the problem? The snippet that you shared doesn't contain enough information for us to investigate."", 'Sorry I meant to update this yesterday. I was getting hit with\r\n\r\nhttps://github.com/pytorch/pytorch/issues/15054#\r\n\r\nalong with lazy evaluation, e.g. the tensor was not evaluated until it was needed (the indexing operation in my example) and at that point cudnn was screwing up. So this isnt a new issue.']","['\r\nhorizontal_block_difference = ((im[:, :, :, block_horizontal_positions] - im[:, :, :, block_horizontal_positions + 1])**2).sum(3).sum(2).sum(1)\r\nvertical_block_difference = ((im[:, :, block_vertical_positions, :] - im[:, :, block_vertical_positions + 1, :])**2).sum(3).sum(2).sum(1)\r\n', '\r\n0 0.0010721683502197266\r\n0 0.0008690357208251953\r\n\r\n0 37.040807008743286\r\n0 0.0005404949188232422\r\n\r\n0 37.13724493980408\r\n0 0.0004165172576904297\r\n\r\n0 27.397161722183228\r\n0 0.0003788471221923828\r\n\r\n0 37.17257356643677\r\n0 0.00034737586975097656\r\n\r\n0 37.201823234558105\r\n0 0.0002465248107910156\r\n', '\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No             \r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Red Hat Enterprise Linux Server release 7.6 (Maipo)\r\nGCC version: (GCC) 6.3.0\r\nCMake version: version 2.8.12.2\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: Quadro P6000\r\nGPU 1: Quadro P6000\r\nGPU 2: Quadro P6000\r\nGPU 3: Quadro P6000\r\n\r\nNvidia driver version: 410.93\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.3\r\n[pip3] torch==1.0.1.post2\r\n[pip3] torchjpeg==0.0.0\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] Could not collect\r\n\r\n']","['im', '0']",1,0
68,pytorch,3958,open,10% performance regression on CuDNN convolution dispatch path,"This PR https://github.com/pytorch/pytorch/pull/3666 introduces a 10% performance regression on the CuDNN convolution dispatch path. There is no single smoking gun, but it needs to be as fast as the old CuDNN code was. I spent this morning trying to fix it to no avail, so this is to remind me to fix it before next release.

cc @csarofeen @ptrblck",module: cudnn triaged,"['Does this affect memory usage as well? I saw about 13% increase in GPU memory when training some CNNs.', '@SsnL Were you using group convolution on CuDNN 6? I can believe that this would have increased GPU memory. All the other cases should not have been affected, so if it is happening I need to investigate that too.', '(Is it this one? https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix )', ""Yeah that's the one :) related: #3610 "", ""@ezyang I can no longer run batch 128 / gpu on 2 GPUs with resnet 50. I don't know if it was the cuDNN/aten re-write but will look into it."", ""Opened a new issue for the memory usage https://github.com/pytorch/pytorch/issues/4081  as I'm not sure if it really comes from https://github.com/pytorch/pytorch/pull/3666""]",[],[],1,0
69,pytorch,9592,closed,nn.Conv* incostintent error depending on stride,"## Issue description

Convolutions do not raise an exception for too small input shape if .

## Code example

Suppose we have a following convolution:

This code raises a . Which is quite expected.

Now change the  to 2:

Which gives . Is this expected behavior? 

I guess this is because in the formula for the output shape there is a division by the stride and the check is performed before taking the  operation.

## System Info
- How you installed PyTorch: conda
- OS: Linux
- PyTorch version: 0.4
- Python version: 3.6

The code was run on CPU.",,"['CLosing, looks to be fixed. ']","['python\r\nconv = torch.nn.Conv2d(1, 1, kernel_size=3, dilation=2, stride=1)\r\ntensor = torch.empty(1, 1, 4, 4)\r\nconv(tensor).shape\r\n', 'python\r\nconv = torch.nn.Conv2d(1, 1, kernel_size=3, dilation=2, stride=2)\r\ntensor = torch.empty(1, 1, 4, 4)\r\nconv(tensor).shape\r\n']","['stride > 1', 'RuntimeError: Given input size per channel: (4 x 4). Calculated output size per channel: (0 x 0)', 'stride', 'torch.Size([1, 1, 1, 1])', 'floor']",1,0
70,pytorch,11647,closed,Performance (speed) regression from 0.4.0 to 0.4.1,"## Issue description

I have a fairly optimized cnn-blstm-crf tagger [here](https://github.com/dpressel/baseline/blob/master/python/baseline/pytorch/tagger/model.py) with the crf defined [here](https://github.com/dpressel/baseline/blob/f0204432076c166ce3d6672705826f33aec95dbe/python/baseline/pytorch/torchy.py#L669).

On pytorch  using cuda  and cudnn  I can run a single epoch of the conll 2003 NER task in 21.41 +/- 0.28

When the only thing I change is the version of pytorch to  (the current conda install) a single epoch now takes  27.99 +/-  0.26

These models are run on gpu.

From the pytorch forums I was told to post here https://discuss.pytorch.org/t/large-preformance-regression-on-0-4-1/25037

## Code example

Please try to provide a minimal example to repro the bug.

Assuming pytorch is installed



## System Info

### 0.4.0

Collecting environment information...
PyTorch version: 0.4.0
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 16.04.3 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.0.176
GPU models and configuration: GPU 0: GeForce GTX 1070 with Max-Q Design
Nvidia driver version: 384.130
cuDNN version: Probably one of the following:
/usr/local/cuda-9.0-cudnn-7/lib64/libcudnn.so
/usr/local/cuda-9.0-cudnn-7/lib64/libcudnn.so.7
/usr/local/cuda-9.0-cudnn-7/lib64/libcudnn.so.7.0.5
/usr/local/cuda-9.0-cudnn-7/lib64/libcudnn.so.7.1.2
/usr/local/cuda-9.0-cudnn-7/lib64/libcudnn_static.a
/usr/local/cuda-9.0/lib64/libcudnn.so
/usr/local/cuda-9.0/lib64/libcudnn.so.7
/usr/local/cuda-9.0/lib64/libcudnn.so.7.0.5
/usr/local/cuda-9.0/lib64/libcudnn.so.7.1.2
/usr/local/cuda-9.0/lib64/libcudnn.so.7.2.1
/usr/local/cuda-9.0/lib64/libcudnn_static.a

Versions of relevant libraries:
[pip] numpy (1.14.3)
[pip] torch (0.4.0)
[pip] torchfile (0.1.0)
[pip] torchvision (0.2.1)
[conda] cuda90                    1.0                  h6433d27_0    pytorch
[conda] pytorch                   0.4.0            py36hdf912b8_0  
[conda] torchfile                 0.1.0                     <pip>
[conda] torchvision               0.2.1                    py36_1    pytorch


### 0.4.1
Collecting environment information...
PyTorch version: 0.4.1.post2
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 16.04.3 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.0.176
GPU models and configuration: GPU 0: GeForce GTX 1070 with Max-Q Design
Nvidia driver version: 384.130
cuDNN version: Probably one of the following:
/usr/local/cuda-9.0-cudnn-7/lib64/libcudnn.so
/usr/local/cuda-9.0-cudnn-7/lib64/libcudnn.so.7
/usr/local/cuda-9.0-cudnn-7/lib64/libcudnn.so.7.0.5
/usr/local/cuda-9.0-cudnn-7/lib64/libcudnn.so.7.1.2
/usr/local/cuda-9.0-cudnn-7/lib64/libcudnn_static.a
/usr/local/cuda-9.0/lib64/libcudnn.so
/usr/local/cuda-9.0/lib64/libcudnn.so.7
/usr/local/cuda-9.0/lib64/libcudnn.so.7.0.5
/usr/local/cuda-9.0/lib64/libcudnn.so.7.1.2
/usr/local/cuda-9.0/lib64/libcudnn.so.7.2.1
/usr/local/cuda-9.0/lib64/libcudnn_static.a

Versions of relevant libraries:
[pip] numpy (1.14.3)
[pip] torch (0.4.1.post2)
[pip] torchfile (0.1.0)
[pip] torchvision (0.2.1)
[conda] cuda90                    1.0                  h6433d27_0    pytorch
[conda] pytorch                   0.4.1           py36_py35_py27__9.0.176_7.1.2_2    pytorch
[conda] torchfile                 0.1.0                     <pip>
[conda] torchvision               0.2.1                    py36_1    pytorch",medium priority (this tag is deprecated),"['Thanks for the detailed report Brian!', '@blester125 I am trying to repro this issue via the example you provided, but ran into an error lol would you like to take a look?\r\n```\r\n>>> mead-train --config config/conll-bio.json --time\r\nTask: [tagger]\r\nTraceback (most recent call last):\r\n  File ""/home/weiyang/anaconda2/bin/mead-train"", line 11, in <module>\r\n    load_entry_point(\'deep-baseline\', \'console_scripts\', \'mead-train\')()\r\n  File ""/home/weiyang/baseline/python/mead/trainer.py"", line 27, in main\r\n    task = mead.Task.get_task_specific(task_name, args.logging, args.settings)\r\n  File ""/home/weiyang/baseline/python/mead/tasks.py"", line 69, in get_task_specific\r\n    config = Task.TASK_REGISTRY[task](logging_config, mead_config)\r\n  File ""/home/weiyang/baseline/python/mead/tasks.py"", line 304, in __init__\r\n    super(TaggerTask, self).__init__(logging_config, mead_settings_config, **kwargs)\r\n  File ""/home/weiyang/baseline/python/mead/tasks.py"", line 34, in __init__\r\n    raise Exception(""Expected either a mead settings file or a JSON object"")\r\nException: Expected either a mead settings file or a JSON object\r\n```\r\nSounds like there is lacking a `mead settings file`..', 'My bad, some stuff got swapped out. This should be the steps to run it, I\'ll update the issue too. (These \r\nassume that pytorch is already installed)\r\n```\r\ngit clone https://github.com/blester125/baseline.git\r\ncd baseline\r\ngit checkout speed-test\r\ncd python\r\n./install_dev.sh baseline no-test\r\necho ""{}"" > mead/config/mead-settings.json\r\nmead-train --config config/conll-bio.json\r\n```\r\n\r\nThis should both print timing information to stdout and also to a file called timing-{pid}.log', '@blester125 Thanks! It is running fine now :)', '@blester125 After couple days endeavor on the [Baseline](https://github.com/blester125/baseline.git) project and timing `Modules` and different parts of the model using `torch.cuda.Event()`, finally it gets me closer to the answer... Most likely the perf regression during training is from advanced indexing. Specifically, during the [training loop](https://github.com/blester125/baseline/blob/4edc5dd05bb737dcba543b12c8943ffbae6c9c92/python/baseline/pytorch/tagger/train.py#L109-L131) at `tagger/train.py`, it calls into [compute_loss](https://github.com/blester125/baseline/blob/4edc5dd05bb737dcba543b12c8943ffbae6c9c92/python/baseline/pytorch/tagger/model.py#L192-L216) at `tagger/model.py`, which then calls into [neg_log_loss](https://github.com/blester125/baseline/blob/4edc5dd05bb737dcba543b12c8943ffbae6c9c92/python/baseline/pytorch/torchy.py#L731-L750) at `torchy.py`, then it calls into [score_sentence](https://github.com/blester125/baseline/blob/4edc5dd05bb737dcba543b12c8943ffbae6c9c92/python/baseline/pytorch/torchy.py#L752-L781) at the same file. Inside this function, the biggest difference in runtime between 0.4.0 and 0.4.1 during training is from this [line](https://github.com/blester125/baseline/blob/4edc5dd05bb737dcba543b12c8943ffbae6c9c92/python/baseline/pytorch/torchy.py#L769-L772)\r\n\r\nTime takes for 1 epoch of training on that line of code:\r\n```\r\n0.4.0: 5806.34 ms\r\n0.4.1: 17637.5 ms\r\n```\r\n\r\nHere is a separate test of runtime among different PyTorch versions:\r\n```\r\nimport torch\r\nfrom random import *\r\n\r\n>>> x = torch.randn(2, 5, 10).cuda()\r\n>>> n = 10\r\n>>> I1 = [randint(0, 1) for i in range(n)]\r\n>>> I2 = [randint(0, 4) for i in range(n)]\r\n>>> I3 = [randint(0, 9) for i in range(n)]\r\n\r\n>>> %timeit -r 100 torch.cuda.synchronize(); x[I1, I2, I3]; torch.cuda.synchronize()\r\n\r\n========= 0.4.0 ========\r\n10000 loops, best of 100: 163 ¬µs per loop\r\n\r\n========= 0.4.1 ========\r\n1000 loops, best of 100: 610 ¬µs per loop\r\n\r\n========= master ========\r\n1000 loops, best of 100: 739 ¬µs per loop\r\n```\r\n\r\n@colesbury is going to push a patch to improve performance of advanced indexing, and it should fix this issue.', 'Thanks for looking into it. Impressed by the thoroughness and quick fix.', 'Advanced indexing also had a perf hit on CPU as well:\r\n\r\nUsing the same indexing test:\r\n\r\n```bash\r\n======= pytorch (0.4.1) =======\r\n23.6 ¬µs ¬± 46.8 ns per loop (mean ¬± std. dev. of 100 runs, 10000 loops each)\r\n\r\n======= pytorch-nightly =======\r\n54.6 ¬µs ¬± 323 ns per loop (mean ¬± std. dev. of 100 runs, 10000 loops each)\r\n```', '@gpleiss yes, this is in part due to the wrapping of negative indices I think.\r\n@colesbury is working on a PR that will completely revamp advanced indexing, and should bring the performance back to what it was (or even faster in some cases)', 'cc @weiyangfb on advanced indexing slow down', 'probably good to re-time again after this PR lands: https://github.com/pytorch/pytorch/pull/13420', 'closing this since #13420 landed', ""Awesome!!! Can't wait to test this out. Thanks so much, all!""]","['\r\ngit clone https://github.com/blester125/baseline.git\r\ncd baseline\r\ngit checkout speed-test\r\ncd python\r\n./install_dev.sh baseline no-test\r\necho ""{}"" > mead/config/mead-settings.json\r\nmead-train --config config/conll-bio.json\r\n']","['0.4.0', '9.0', '7102', '0.4.1']",1,1
71,pytorch,23425,open,Hanging on when one gpu node return zero as loss in the context of distributed data parallel training,"## üêõ Bug

Hello.
I am training a multi task network on a cluster using 8 gpu with .
On every iteration, I process one image per gpu.
Suppose I have  and  with  and  respectively.
In some case, the  could be zero on one gpu, which means that there is no loss computed.
Then I just return the zero tensor.
When this situation happens, the code just hang on.

 - PyTorch Version: 1.1.0 
 - OS (e.g., Linux): Ubuntu 
 - How you installed PyTorch (, , source): docker conda
 - Python version: 3.6
 - CUDA/cuDNN version:  CUDA 9.0
 - GPU models and configuration: 1080ti
 - Any other relevant information:

",oncall: distributed triaged,"['Hi @chenchr, the task has to return the computed output/loss instead of manually create a new zero tensor even if the loss is indeed zero. Because, that loss tensor/variable not only stores the loss value, but also the autograd graph. DDP will use that loss (or output) tensor to find all participating parameters and do the backward reduction. \r\n\r\nIf returning the real loss tensor does not solve the problem, please share more details and some code snippet of your trainer.', ""@mrshenli  Hi, thanks for your reply.\r\nI am training a detection network.\r\nUsually detection's loss consists of ```box regression loss``` and ```classification loss```.\r\nSuppose you have one image that has no box annotation on it, then when calculating the loss, only ```classification loss``` can be conducted, and the ```box regression loss``` has nonsense.  \r\nTherefore, I just return a zero tensor for ```box regression loss```.\r\nHow can I deal with this situation ?\r\nThanks."", ""Hi @chenchr, let me try to better understand the requirement. Are you doing something like the following?\r\n\r\n```python\r\nclassification_output, box_output = ddp_model(one_image)\r\n\r\nif one_image.has_box_annotation:\r\n  box_regression_loss = box_regression_loss_fn(box_output, label)\r\nelse:\r\n  box_regression_loss = torch.zeros(...)\r\n\r\nclassification_loss = classification_loss(classification_output, label)\r\n(classification_loss + box_regression_loss).backward()\r\n```\r\n\r\nThere are two problems with the above code:\r\n\r\n1. Not all DDP outputs contribute to loss: `box_output` was discarded after the forward pass, but DDP backward reduction would still expect gradients for params involved in computing that loss. This will lead to backward hang.\r\n2. If some model params are not used in some iterations, you need to set [find_unused_parameters](https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/distributed.py#L196) to true.\r\n\r\nThe solution to the above problem would be: 1) set `find_unused_parameters`, 2) move the `if-else` clause into `ddp_model`'s `forward` method, and make sure that all ddp outputs contribute to loss.\r\n\r\nIf your implementation is different from the above, please post some pseudo code to help debugging.\r\n\r\n"", 'Hi, @mrshenli, exactly it\'s almost the same, at a little difference.\r\nAs the ```box_regression_loss ``` and ```classification_loss ``` is used for statisstics of loss during training.\r\nTherefore, in a network module, I return them as a dict like:\r\n```\r\ndef forward(self, x):\r\n    box_regression_loss = ......\r\n    classification_loss = .....\r\n    return_dict = {\r\n        ""loss_reg"": box_regression_loss,\r\n        ""loss_cls"": classification_loss ,\r\n    }\r\n    return return_dict\r\n```\r\n\r\nThe solution you provide seems to work only when I add these loss into on tensor before return.\r\n\r\nUpdated:\r\nMaybe I can return scalar zero when no ```loss_reg``` computated, and then do some ```if-else``` conditioning to deal with the loss ```reduced``` from multi gpu.\r\nThanks.', ""> Maybe I can return scalar zero when no loss_reg computated, and then do some if-else conditioning to deal with the loss reduced from multi gpu.\r\n\r\nYep, doing that (or don't include `box_regression_loss` in the `return_dict` if there is no annotation) together with setting `find_unused_parameters` would work I think. "", '@mrshenli  A more complicated situation is when I train on multiple gpu using ```distributed data parallel```. The ```loss_reg``` may not be computed on a specific gpu instead of all gpus. \r\nThen during backward, when the backend begin to sync the graident from all nodes, will this introduces  other problems ? \r\nThanks.\r\n\r\nAnyway, I will test the solution first and check whether it will introduce other problems.', ""> The loss_reg may not be computed on a specific gpu instead of all gpus.\r\nThen during backward, when the backend begin to sync the gradient from all nodes, will this introduces other problems ?\r\n\r\n@chenchr That should be fine, because DDP would still reduce gradients for all params even if there are unused ones on some gpu. The only difference is that when DDP detects some unused params in the forward pass (by walking the autograd graph from the forward output), it would mark those params as backward ready and won't expect their gradients during backward pass. ""]",[],"['Distributed Data Parallel', 'task_a', 'task_b', 'loss_a', 'loss_b', 'loss_a', 'conda', 'pip']",1,0
72,pytorch,16258,closed,Inconsistent Behavior of permute() in different calling order,"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->
I am trying to convert the format between cv::Mat and torch::Tensor using c++(libtorch), one crucial step of which is to permute the shape of the Tensor. However I found that in some cases the permute() function failed to manage the memory of the image.

## To Reproduce

I provide some code to reproduce the bug:

1. Image Version


1. A Simpler Matrix Version
this code:

would output (wrong):


but change the permute line from 

to 


will output (correct):


Note that when the permute() failed to managed the memory, it always returned the ""correct"" shape of the permuted tensor.

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->
The two different usage of permute() should behave consistently( both should  permute the memory not only the shape)

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


 - PyTorch Version (e.g., 1.0): v1.0.0
 - OS (e.g., Linux): Linux Arm
 - How you installed PyTorch (, , source): From source, and following the step of [issue 15138](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
 - Build command you used (if compiling from source): python setup.py build
 - Python version: 3.5.2
 - CUDA/cuDNN version: libcudnn.so.7.0.5
 - GPU models and configuration:
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->
",,"['This behavior is expected.\r\n\r\n`permute`/`transpose` only change the underlying sizes / strides, and not the underlying memory.\r\nThis is for performance reasons.\r\n\r\nThe second case worked for you because moving the tensor to the GPU makes the tensor contiguous.\r\n\r\nIf you want to share the data with another program that requires contiguous data, you can use a `.contiguous` in the tensor.', 'Thanks a lot! @fmassa ']","['c++\r\n#include <opencv2/opencv.hpp>\r\n#include <torch/script.h>\r\n#include <cstdint>\r\n#include <cstdio>\r\n#include <iostream>\r\n\r\nconst char* shape_of(torch::Tensor const & tensor) {\r\n  const int kBufferSize = 64;\r\n  static char buffer[kBufferSize];\r\n  memset(buffer, kBufferSize, 0);\r\n  int offset = 0;\r\n  offset += snprintf(buffer+offset, kBufferSize-offset, ""("");\r\n  for(int i : tensor.sizes()) {\r\n    offset += snprintf(buffer+offset, kBufferSize-offset, ""%d,"", i);\r\n  }\r\n  offset += snprintf(buffer+offset-1, kBufferSize-offset, "")"");\r\n  return buffer;\r\n}\r\n\r\nint main(){\r\n    cv::VideoCapture cap(""video.mp4"");\r\n    cv::Mat frame, show;\r\n    torch::Tensor tframe, tres;\r\n    cap >> frame;\r\n    tframe = torch::from_blob(frame.data, {1, frame.rows, frame.cols, 3}, torch::kByte);\r\n    tframe = tframe.permute({0,3,1,2}).toType(torch::kFloat).div_(255.0f).to(torch::kCUDA);\r\n    printf(""%s,\\n"", shape_of(tframe));\r\n    tres = tframe.to(torch::kCPU).mul_(255.0f).toType(torch::kByte).permute({0, 2, 3, 1}); // this would not work\r\n    //tres = tframe.permute({0, 2, 3, 1}).to(torch::kCPU).mul_(255.0f).toType(torch::kByte); // this would work!!!\r\n    printf(""%s,\\n"", shape_of(tres));\r\n    show = cv::Mat(frame.rows, frame.cols, CV_8UC3, tres.data<uint8_t>(), frame.cols);\r\n    imshow(""show"", show);\r\n    imshow(""frame"", frame);\r\n    cv::waitKey(0);\r\n    return 0;\r\n}\r\n', 'c++\r\n#include <opencv2/opencv.hpp>\r\n#include <torch/script.h>\r\n#include <cstdint>\r\n#include <cstdio>\r\n#include <iostream>\r\n\r\nconst char* shape_of(torch::Tensor const & tensor) {\r\n  const int kBufferSize = 64;\r\n  static char buffer[kBufferSize];\r\n  memset(buffer, kBufferSize, 0);\r\n  int offset = 0;\r\n  offset += snprintf(buffer+offset, kBufferSize-offset, ""("");\r\n  for(int i : tensor.sizes()) {\r\n    offset += snprintf(buffer+offset, kBufferSize-offset, ""%d,"", i);\r\n  }\r\n  offset += snprintf(buffer+offset-1, kBufferSize-offset, "")"");\r\n  return buffer;\r\n}\r\n\r\nint main(){\r\n    cv::VideoCapture cap(""video.mp4"");\r\n    cv::Mat frame, show;\r\n    torch::Tensor tframe, tres;\r\n    char data[] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12};\r\n    tframe = torch::from_blob(data, {1, 2, 2, 3}, torch::kByte);     printf(""shape of tframe is %s,\\n"", shape_of(tframe));\r\n    frame = cv::Mat(2, 2, CV_8UC3, data);                            std::cout << ""cout frame(data) is "" << frame << std::endl;\r\n    frame = cv::Mat(2, 2, CV_8UC3, tframe.data<uint8_t>());          std::cout << ""cout frame(tframe.data) is "" << frame << std::endl;\r\n    tframe = tframe.toType(torch::kFloat).div_(255.0f).to(torch::kCUDA).permute({0,3,1,2}); /* this would fail */    printf(""shape of permuted tframe is %s,\\n"", shape_of(tframe));\r\n    tres = tframe.permute({0, 2, 3, 1}).to(torch::kCPU).mul_(255.0f).toType(torch::kByte);     printf(""shape of tres is %s,\\n"", shape_of(tres));\r\n    show = cv::Mat(frame.rows, frame.cols, CV_8UC3, tres.data<uint8_t>());    std::cout << ""cout show(tres.data) is "" << show << std::endl;\r\n    return 0;\r\n}\r\n', '\r\nshape of tframe is (1,2,2,3),\r\ncout frame(data) is [  1,   2,   3,   4,   5,   6;   7,   8,   9,  10,  11,  12]\r\ncout frame(tframe.data) is [  1,   2,   3,   4,   5,   6;   7,   8,   9,  10,  11,  12]\r\nshape of permuted tframe is (1,3,2,2),\r\nshape of tres is (1,2,2,3),\r\ncout show(tres.data) is [  1,   4,   7,  10,   2,   5;   8,  11,   3,   6,   9,  12]\r\n', 'c++\r\ntframe = tframe.toType(torch::kFloat).div_(255.0f).to(torch::kCUDA).permute({0,3,1,2}); /* this would fail */ \r\n', 'c++\r\ntframe = tframe.permute({0,3,1,2}).toType(torch::kFloat).div_(255.0f).to(torch::kCUDA); /* this would work */ \r\n', '\r\nshape of tframe is (1,2,2,3),\r\ncout frame(data) is [  1,   2,   3,   4,   5,   6;   7,   8,   9,  10,  11,  12]\r\ncout frame(tframe.data) is [  1,   2,   3,   4,   5,   6;   7,   8,   9,  10,  11,  12]\r\nshape of permuted tframe is (1,3,2,2),\r\nshape of tres is (1,2,2,3),\r\ncout show(tres.data) is [  1,   2,   3,   4,   5,   6;   7,   8,   9,  10,  11,  12]\r\n\r\n', '\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n']","['conda', 'pip']",1,0
73,pytorch,20355,closed,Backwards hangs,"Training is hanging during a call to backwards - I might be wrong but it looks like it hangs while one of the threads tries to acquire a lock.

The setup is the following:
 - pytorch version is 1.0.0post2
 - start 4 python processes on same machine (using multiprocessing spawn, but same happens with Popen)
 - each process uses a different gpu and different data (all data is loaded into memory prior to start training)
 - use torch.distributed (nccl) to synchronise training; all communication between different processes happens via nccl

What I observe is that at random times, one of the processes hangs - and the others then wait for it in the call to all_reduce.

Unfortunately, it is hard to reproduce, and it can take an arbitrary amount of time to hang - sometimes it hangs after 10min, sometimes it hangs after 10h+.

Any help is appreciated - thanks.

At the moment where it hangs, the threads of the frozen process are in this state:
> (gdb) info threads
  Id   Target Id         Frame
  16   Thread 0x7f22711b9700 (LWP 23578) ""python"" 0x00007f22d64fff0d in poll () from /lib64/libc.so.6
  15   Thread 0x7f2270943700 (LWP 23614) ""python"" 0x00007f22d650c03f in accept4 () from /lib64/libc.so.6
  14   Thread 0x7f2265fff700 (LWP 23644) ""python"" 0x00007f22d64fff0d in poll () from /lib64/libc.so.6
  13   Thread 0x7f22617fe700 (LWP 30293) ""python"" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
  12   Thread 0x7f22609ed800 (LWP 30294) ""python"" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
  11   Thread 0x7f22605eb880 (LWP 30295) ""python"" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
  10   Thread 0x7f224dffe900 (LWP 30296) ""python"" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
  9    Thread 0x7f224dbfc980 (LWP 30297) ""python"" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
  8    Thread 0x7f224d7faa00 (LWP 30298) ""python"" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
  7    Thread 0x7f224d3f8a80 (LWP 30299) ""python"" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
  6    Thread 0x7f221aee5700 (LWP 68788) ""python"" 0x00007f22d70f851d in __lll_lock_wait () from /lib64/libpthread.so.0
  5    Thread 0x7f221a6e4700 (LWP 68789) ""python"" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
  4    Thread 0x7f2219ee3700 (LWP 68790) ""python"" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
  3    Thread 0x7f22196e2700 (LWP 68791) ""python"" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
  2    Thread 0x7f2218ee1700 (LWP 68792) ""python"" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
  1    Thread 0x7f22d750f740 (LWP 23478) ""python"" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0

The stack trace per thread is below.

Thread 1:
>#0  0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f22bfb9656f in __gthread_cond_wait (__mutex=<optimized out>, __cond=<optimized out>)
    at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/build/build-cc-gcc-final/x86_64-conda_cos6-linux-gnu/libstdc++-v3/include/x86_64-conda_cos6-linux-gnu/bits/gthr-default.h:877
#2  std::condition_variable::wait (this=<optimized out>, __lock=...) at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/src/c++11/condition_variable.cc:53
#3  0x00007f2278eb08e3 in torch::autograd::Engine::execute(std::vector<torch::autograd::Edge, std::allocator<torch::autograd::Edge> > const&, std::vector<torch::autograd::Variable, std::allocator<torch::autograd::Variable> > const&, bool, bool, std::vector<torch::autograd::Edge, std::allocator<torch::autograd::Edge> > const&) () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch.so.1
#4  0x00007f22bc696a0c in torch::autograd::python::PythonEngine::execute(std::vector<torch::autograd::Edge, std::allocator<torch::autograd::Edge> > const&, std::vector<torch::autograd::Variable, std::allocator<torch::autograd::Variable> > const&, bool, bool, std::vector<torch::autograd::Edge, std::allocator<torch::autograd::Edge> > const&) () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so
#5  0x00007f22bc69722c in THPEngine_run_backward(THPEngine*, _object*, _object*) () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so
...

Threads 2-5:
>#0  0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f22bfb9656f in __gthread_cond_wait (__mutex=<optimized out>, __cond=<optimized out>)
    at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/build/build-cc-gcc-final/x86_64-conda_cos6-linux-gnu/libstdc++-v3/include/x86_64-conda_cos6-linux-gnu/bits/gthr-default.h:877
#2  std::condition_variable::wait (this=<optimized out>, __lock=...) at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/src/c++11/condition_variable.cc:53
#3  0x00007f2278eac92b in torch::autograd::ReadyQueue::pop() () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch.so.1
#4  0x00007f2278eaf083 in torch::autograd::Engine::thread_main(torch::autograd::GraphTask*) () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch.so.1
#5  0x00007f2278eaba77 in torch::autograd::Engine::thread_init(int) () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch.so.1
#6  0x00007f22bc6968aa in torch::autograd::python::PythonEngine::thread_init(int) () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so
#7  0x00007f22bfb9a678 in std::execute_native_thread_routine_compat (__p=<optimized out>) at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/src/c++11/thread.cc:94
#8  0x00007f22d70f1e25 in start_thread () from /lib64/libpthread.so.0
#9  0x00007f22d650abad in clone () from /lib64/libc.so.6

Thread 6:
>#0  0x00007f22d70f851d in __lll_lock_wait () from /lib64/libpthread.so.0
#1  0x00007f22d70f61a0 in pthread_cond_broadcast@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#2  0x00007f22bfb96594 in __gthread_cond_broadcast (__cond=<optimized out>)
    at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/build/build-cc-gcc-final/x86_64-conda_cos6-linux-gnu/libstdc++-v3/include/x86_64-conda_cos6-linux-gnu/bits/gthr-default.h:865
#3  std::condition_variable::notify_all (this=<optimized out>) at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/src/c++11/condition_variable.cc:73
#4  0x00007f2278eaf167 in torch::autograd::Engine::thread_main(torch::autograd::GraphTask*) () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch.so.1
#5  0x00007f2278eaba77 in torch::autograd::Engine::thread_init(int) () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch.so.1
#6  0x00007f22bc6968aa in torch::autograd::python::PythonEngine::thread_init(int) () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so
#7  0x00007f22bfb9a678 in std::execute_native_thread_routine_compat (__p=<optimized out>) at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/src/c++11/thread.cc:94
#8  0x00007f22d70f1e25 in start_thread () from /lib64/libpthread.so.0
#9  0x00007f22d650abad in clone () from /lib64/libc.so.6

Thread 7-13:
>#0  0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f22cb364725 in __kmp_suspend_64 () from /opt/anaconda3/lib/python3.7/site-packages/numpy/../../../libiomp5.so
#2  0x00007f22cb2d5e4e in bool _INTERNAL_25_______src_kmp_barrier_cpp_3dc39ea5::__kmp_wait_template<kmp_flag_64, 1, false, true>(kmp_info*, kmp_flag_64*, void*) ()
   from /opt/anaconda3/lib/python3.7/site-packages/numpy/../../../libiomp5.so
#3  0x00007f22cb2d9398 in _INTERNAL_25_______src_kmp_barrier_cpp_3dc39ea5::__kmp_hyper_barrier_release(barrier_type, kmp_info*, int, int, int, void*) () from /opt/anaconda3/lib/python3.7/site-packages/numpy/../../../libiomp5.so
#4  0x00007f22cb2df4d2 in __kmp_fork_barrier(int, int) () from /opt/anaconda3/lib/python3.7/site-packages/numpy/../../../libiomp5.so
#5  0x00007f22cb320d8e in __kmp_launch_thread () from /opt/anaconda3/lib/python3.7/site-packages/numpy/../../../libiomp5.so
#6  0x00007f22cb360571 in _INTERNAL_26_______src_z_Linux_util_cpp_51eec780::__kmp_launch_worker(void*) () from /opt/anaconda3/lib/python3.7/site-packages/numpy/../../../libiomp5.so
#7  0x00007f22d70f1e25 in start_thread () from /lib64/libpthread.so.0
#8  0x00007f22d650abad in clone () from /lib64/libc.so.6

Thread 14:
>#0  0x00007f22d64fff0d in poll () from /lib64/libc.so.6
#1  0x00007f22beb8a323 in ?? () from /lib64/libcuda.so.1
#2  0x00007f22bebecacd in ?? () from /lib64/libcuda.so.1
#3  0x00007f22beb8c988 in ?? () from /lib64/libcuda.so.1
#4  0x00007f22d70f1e25 in start_thread () from /lib64/libpthread.so.0
#5  0x00007f22d650abad in clone () from /lib64/libc.so.6

Thread 15:
>#0  0x00007f22d650c03f in accept4 () from /lib64/libc.so.6
#1  0x00007f22beb8b2ca in ?? () from /lib64/libcuda.so.1
#2  0x00007f22beb7d8dd in ?? () from /lib64/libcuda.so.1
#3  0x00007f22beb8c988 in ?? () from /lib64/libcuda.so.1
#4  0x00007f22d70f1e25 in start_thread () from /lib64/libpthread.so.0
#5  0x00007f22d650abad in clone () from /lib64/libc.so.6

Thread 16:
>#0  0x00007f22d64fff0d in poll () from /lib64/libc.so.6
#1  0x00007f22bca13879 in c10d::TCPStoreDaemon::run() () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so
#2  0x00007f22bfb9a678 in std::execute_native_thread_routine_compat (__p=<optimized out>) at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/src/c++11/thread.cc:94
#3  0x00007f22d70f1e25 in start_thread () from /lib64/libpthread.so.0
#4  0x00007f22d650abad in clone () from /lib64/libc.so.6",module: autograd oncall: distributed triaged,"['@mrshenli @albanD do you have any ideas on why it might be hanging?', 'Is it possible to get the reproducer script (even if it takes multiple hours to repro)? What happens on master?', '@vpedro87 would I be correct to assume that you are not using `DistributedDataParallel`, and instead manually synchronize training using `torch.distributed.all_reduce`, `torch.distributed.all_reduce`, etc.? ', ""Afraid it's not possible to upload the reproducer script. I'm trying to write a smaller version which also hangs - but testing for this is tricky since the original script may take hours to hang.\r\n\r\n@mrshenli that's correct I'm not using DistributedDataParallel and instead calling torch.distributed.all_reduce in each process separately at the end of each batch."", ""Don't know if it's helpful but I didn't kill the processes yet - they're still stuck."", ""> that's correct I'm not using DistributedDataParallel and instead calling torch.distributed.all_reduce in each machine separately at the end of each batch.\r\n\r\nAre you passing `async_op=False` or `async_op=True` to `all_reduce` (it defaults to False)? Is it something like the following:\r\n\r\n```python\r\nfor input in inputs:\r\n  output = model(input)\r\n  loss_func(output, label).backward()\r\n  all_reduce(model.parameters(), ..., async_op=True) # only use all reduce to achieve model averaging.\r\n```\r\n\r\nIn the above case, both forward and backward are local activity. But it could be sth got messed up in the previous `all_reduce`. Could you please try two things:\r\n\r\n1. set `async_op=False` to force synchronization after every `all_reduce`.\r\n2. run you code with latest PyTorch master. We fixed some sync bugs in NCCL `all_reduce` recently, and I would like to check if that plays a role here."", ""> Afraid it's not possible to upload the reproducer script. \r\n\r\nIt will be helpful if you could post your script even if it's hard to reproduce. We can help to check if there is any unexpected configurations."", '@ezyang if a tensor content got modified during a backward call, will it cause backward to hang or just produce wrong results?', ""@mrshenli I'm using the default in all_reduce, i.e. async_op=False.\r\nCode is something along these lines:\r\n```python\r\nfor i in range(n_batches):\r\n  optim.zero_grad()\r\n  input = prepare_batch(i) #heaviest operation is torch.index_select, which is multithreaded\r\n  for j in range(len(input)):  #whole input doesnt fit in gpu, and loss function is additive\r\n    output = model(input[j])\r\n    loss_func(output, label).backward()\r\n  average_gradients(optim, async_op=False)\r\n  optim.step()\r\n```"", ""will try to run in latest PyTorch master and report back - that will take a few days.\r\n\r\nReally cannot share the script I'm afraid - need to write a simplified version of it and check it also hangs."", 'BTW, any reason for not using `DistributedDataParallel`?', ""Main reason is that I haven't looked into it, tbh."", '[Here](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html) is a tutorial.', 'I meet the problem when using fairseq\r\nhttps://github.com/pytorch/fairseq/blob/master/fairseq/distributed_utils.py#L121\r\nusing 1 node with 8 gpus\r\nthe program hangs with high gpu usage and low power.\r\n\r\nafter export NCCL_LL_THRESHOLD=0, there is an error and the program exits:\r\n> Traceback (most recent call last):\r\n>   File ""train.py"", line 306, in <module>\r\n>     cli_main()\r\n>   File ""train.py"", line 298, in cli_main\r\n>     nprocs=args.distributed_world_size,\r\n>   File ""/usr/local/lib/python3.5/dist-packages/torch/multiprocessing/spawn.py"", line 167, in spawn\r\n>     while not spawn_context.join():\r\n>   File ""/usr/local/lib/python3.5/dist-packages/torch/multiprocessing/spawn.py"", line 114, in join\r\n>     raise Exception(msg)\r\n> Exception: \r\n> \r\n> -- Process 2 terminated with the following error:\r\n> Traceback (most recent call last):\r\n>   File ""/opt/tiger/bunmt/fairseq/distributed_utils.py"", line 169, in all_gather_list\r\n>     result.append(pickle.loads(bytes(out_buffer[2 : size + 2].tolist())))\r\n> _pickle.UnpicklingError: invalid load key, \'4\'.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File ""/usr/local/lib/python3.5/dist-packages/torch/multiprocessing/spawn.py"", line 19, in _wrap\r\n>     fn(i, *args)\r\n>   File ""/opt/tiger/bunmt/train.py"", line 265, in distributed_main\r\n>     main(args, init_distributed=True)\r\n>   File ""/opt/tiger/bunmt/train.py"", line 80, in main\r\n>     train(args, trainer, task, epoch_itr)\r\n>   File ""/opt/tiger/bunmt/train.py"", line 121, in train\r\n>     log_output = trainer.train_step(samples)\r\n>   File ""/opt/tiger/bunmt/fairseq/trainer.py"", line 301, in train_step\r\n>     [logging_outputs, sample_sizes, ooms, self._prev_grad_norm],\r\n>   File ""/opt/tiger/bunmt/fairseq/distributed_utils.py"", line 173, in all_gather_list\r\n>     \'Unable to unpickle data from other workers. all_gather_list requires all \'\r\n> Exception: Unable to unpickle data from other workers. all_gather_list requires all workers to enter the function together, so this error usually indicates that the workers have fallen out of sync somehow. Workers can fall out of sync if one of them runs out of memory, or if there are other conditions in your training script that can cause one worker to finish an epoch while other workers are still iterating over their portions of the data.\r\n\r\nor\r\n\r\n> Traceback (most recent call last):\r\n>   File ""train.py"", line 306, in <module>\r\n>     cli_main()\r\n>   File ""train.py"", line 298, in cli_main\r\n>     nprocs=args.distributed_world_size,\r\n>   File ""/usr/local/lib/python3.5/dist-packages/torch/multiprocessing/spawn.py"", line 167, in spawn\r\n>     while not spawn_context.join():\r\n>   File ""/usr/local/lib/python3.5/dist-packages/torch/multiprocessing/spawn.py"", line 114, in join\r\n>     raise Exception(msg)\r\n> Exception: \r\n> \r\n> -- Process 2 terminated with the following error:\r\n> Traceback (most recent call last):\r\n>   File ""/opt/tiger/bunmt/fairseq/distributed_utils.py"", line 169, in all_gather_list\r\n>     result.append(pickle.loads(bytes(out_buffer[2 : size + 2].tolist())))\r\n> _pickle.UnpicklingError: invalid load key, \'H\'.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File ""/usr/local/lib/python3.5/dist-packages/torch/multiprocessing/spawn.py"", line 19, in _wrap\r\n>     fn(i, *args)\r\n>   File ""/opt/tiger/bunmt/train.py"", line 265, in distributed_main\r\n>     main(args, init_distributed=True)\r\n>   File ""/opt/tiger/bunmt/train.py"", line 80, in main\r\n>     train(args, trainer, task, epoch_itr)\r\n>   File ""/opt/tiger/bunmt/train.py"", line 121, in train\r\n>     log_output = trainer.train_step(samples)\r\n>   File ""/opt/tiger/bunmt/fairseq/trainer.py"", line 302, in train_step\r\n>     [logging_outputs, sample_sizes, ooms, self._prev_grad_norm],\r\n>   File ""/opt/tiger/bunmt/fairseq/distributed_utils.py"", line 173, in all_gather_list\r\n>     \'Unable to unpickle data from other workers. all_gather_list requires all \'\r\n> Exception: Unable to unpickle data from other workers. all_gather_list requires all workers to enter the function together, so this error usually indicates that the workers have fallen out of sync somehow. Workers can fall out of sync if one of them runs out of memory, or if there are other conditions in your training script that can cause one worker to finish an epoch while other workers are still iterating over their portions of the data.\r\n> \r\n> Exception ignored in: <function WeakValueDictionary.__init__.<locals>.remove at 0x7f8e992138c8>\r\n> Traceback (most recent call last):\r\n>   File ""/usr/lib/python3.5/weakref.py"", line 117, in remove\r\n> TypeError: \'NoneType\' object is not callable\r\n\r\nwhen changed to gloo backend, the error is\r\n\r\n> terminate called after throwing an instance of \'gloo::EnforceNotMet\'\r\n>   what():  [enforce fail at /pytorch_src/third_party/gloo/gloo/transport/tcp/pair.cc:486] op.preamble.length <= op.nbytes. 1048576 vs 8192\r\n\r\nAfter updating the pytorch on this https://github.com/pytorch/pytorch/pull/20882, the problem is the same.\r\n@mrshenli ', ""My training is also hanging using DistributedDataParallel. Any ideia how to debug? It doesn't show error messages. I've tried a lot of different things, nothing worked so far. Hanging appears to be stoachastic, it happens in different iterations, sometimes it takes a long time to happen, sometimes it doesn't."", 'Hi @dougsouza does it hang for both NCCL and GLOO backend?', ""@mrshenli, GLOO backend is painfully slow but it hangs the same way, no error. Things I noticed:\r\nthe more GPUs I use, the quicker the hanging happens.\r\n\r\nThings I think it is worth mentioning:\r\n* I am training GANs using sync batchnorm in G\r\n* I load the entire dataset in memory\r\n* I'm using DistributedDataParellel in 3 GPUs using NCCL backend\r\n* I'm using the nightly version of Pytorch (reinstalled today)\r\n* CUDA 10, cudnn 7.5\r\n* When it hangs, I can't even kill the GPU processes, I have to restart the machine"", 'Setting `NCCL_DEBUG=INFO` I get:\r\n\r\nteia-shellby:7778:7778 [0] NCCL INFO NET/Socket : Using [0]enp5s0:192.168.0.220<0>                   \r\nteia-shellby:7778:7778 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\n\r\nteia-shellby:7778:7778 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\r\nNCCL version 2.4.6+cuda10.0\r\nteia-shellby:7778:8002 [0] NCCL INFO Setting affinity for GPU 0 to 3f00003f\r\nloaded checkpoint from epoch 35...\r\nteia-shellby:7780:7780 [2] NCCL INFO NET/Socket : Using [0]enp5s0:192.168.0.220<0>\r\nteia-shellby:7780:7780 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\n\r\nteia-shellby:7780:7780 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\r\nteia-shellby:7780:8003 [2] NCCL INFO Setting affinity for GPU 2 to 0f,c0000fc0\r\nloaded checkpoint from epoch 35...\r\nteia-shellby:7779:7779 [1] NCCL INFO NET/Socket : Using [0]enp5s0:192.168.0.220<0>\r\nteia-shellby:7779:7779 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\n\r\nteia-shellby:7779:7779 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\r\nteia-shellby:7779:8007 [1] NCCL INFO Setting affinity for GPU 1 to 3f00003f\r\nteia-shellby:7778:8002 [0] NCCL INFO Channel 00 :    0   1   2\r\nteia-shellby:7780:8003 [2] NCCL INFO Ring 00 : 2[2] -> 0[0] via direct shared memory\r\nteia-shellby:7778:8002 [0] NCCL INFO Ring 00 : 0[0] -> 1[1] via direct shared memory\r\nteia-shellby:7779:8007 [1] NCCL INFO Ring 00 : 1[1] -> 2[2] via direct shared memory\r\nteia-shellby:7778:8002 [0] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees disabled\r\nteia-shellby:7780:8003 [2] NCCL INFO comm 0x7fc9f0002330 rank 2 nranks 3 cudaDev 2 nvmlDev 2 - Init C\r\nOMPLETE\r\nteia-shellby:7778:8002 [0] NCCL INFO comm 0x7fbb78002330 rank 0 nranks 3 cudaDev 0 nvmlDev 0 - Init C\r\nOMPLETE\r\nteia-shellby:7778:7778 [0] NCCL INFO Launch mode Parallel\r\nteia-shellby:7779:8007 [1] NCCL INFO comm 0x7f399c002330 rank 1 nranks 3 cudaDev 1 nvmlDev 1 - Init C\r\nOMPLETE', 'Another strange thing. I ran out of space in my hard drive. I found that kern.log and syslog were storing about 200GB each. When I checked them, most logs were relate to `pcieport` logs. In fact, when I train using distributed data parallel, ubuntu keeps writing a lot of stuff to the kern.log and syslog. Example attached.\r\n[kern.log](https://github.com/pytorch/pytorch/files/3356596/kern.log)\r\n', 'In my case I solved it by disabling **PCIe Active State Power Management**. No more hanging and no more frenetic logging from the OS.\r\n\r\nHeres how to do it: [https://askubuntu.com/questions/863150/pcie-bus-error-severity-corrected-type-physical-layer-id-00e5receiver-id](https://askubuntu.com/questions/863150/pcie-bus-error-severity-corrected-type-physical-layer-id-00e5receiver-id)\r\n\r\nHope it helps', 'Guys, remember to set the correct network interface, that is really important for training work properly.\r\n\r\nFor example: export `NCCL_SOCKET_IFNAME=eth0`', 'cc @pietern', '@dougsouza is this solved? The root cause was that the pci power management was causing issues with nccl? ', '@albanD, no, not really. I tried a million of different things, but none worked completely. In the end I believe it was a hardware failure in my case. I replaced the card and frenetic logging went away.', 'Optimistically closing this then.\r\nFeel free to re-open if you find a repro that is not caused by hardware.']",[],[],1,0
74,pytorch,27016,closed,inconsistent of einsum and torch.mm,"
## Issue description
When comparing the outcomes of torch.mm and torch.einsum for matrix multiplication, the results is not consistent. The same result is also produced by numpy. 

## Code example
A minimal example is down here. 


[[ True  True  True  True]
 [ True  True  True  True]
 [ True  True  True  True]
 [ True  True  True  True]]
[[ True  True  True  True]
 [ True  True  True  True]
 [ True  True  True  True]
 [ True  True  True  True]]
tensor([[ True,  True,  True,  True],
        [False,  True, False,  True],
        [ True, False,  True,  True],
        [False,  True,  True,  True]])
[[ True  True  True  True]
 [False  True False  True]
 [ True False  True  True]
 [False  True  True  True]]

While of course we should expect all trues.

## System Info
PyTorch version: 1.2.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Red Hat Enterprise Linux release 8.0 (Ootpa)
GCC version: (GCC) 8.2.1 20180905 (Red Hat 8.2.1-3)
CMake version: version 3.11.4

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.1.168
GPU models and configuration: GPU 0: GeForce RTX 2080 Ti
Nvidia driver version: 430.14
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] numpy==1.16.2
[pip] numpydoc==0.8.0
[pip] torch==1.2.0
[conda] _pytorch_select           0.2                       gpu_0  
[conda] blas                      1.0                         mkl  
[conda] mkl                       2019.4                      243  
[conda] mkl-service               2.3.0            py37he904b0f_0  
[conda] mkl_fft                   1.0.10           py37ha843d7b_0  
[conda] mkl_random                1.0.2            py37hd81dba3_0  
[conda] pytorch                   1.2.0           cuda100py37h938c94c_0

",,"[""Please don't compare numerical computation results with `==`, but check with a certain tolerance. Floating point calculation results are only well-defined up to numerical precision."", 'The error of your floating point computation may be affected by various factors, such as [other programs](https://www.nag.com/content/wandering-precision) running on the machine.\r\n\r\nHowever, the error is still bounded, so whether exact equality testing will succeed depends on the specific values. \r\n \r\nFirst of all, note that two results are off by exactly 1 ulp in 4 places\r\n\r\n```\r\nimport numpy.testing._private.utils as nputil\r\nnputil.nulp_diff(c_torch_mm.numpy(), c_np_einsum) \r\n```\r\ngives\r\n```\r\narray([[0., 0., 0., 0.],\r\n       [1., 0., 1., 0.],\r\n       [0., 1., 0., 0.],\r\n       [1., 0., 0., 0.]], dtype=float32)\r\n```\r\n\r\nIs this acceptable error for a standard matrix multiplication algorithm? You can use approach from numerical analysis to check, ie, Higham 3.5 ""Accuracy and Stability of Numerical Algorithms""\r\n\r\n<img width=""539"" alt=""Screenshot 2019-09-28 10 19 22"" src=""https://user-images.githubusercontent.com/23068/65820064-79da1900-e1d9-11e9-8199-fd58a0a60149.png"">\r\n<img width=""516"" alt=""Screenshot 2019-09-28 10 19 28"" src=""https://user-images.githubusercontent.com/23068/65820067-7e9ecd00-e1d9-11e9-9c46-f0cc7cd05c92.png"">\r\n\r\nTo apply this analysis to your case, \r\n```\r\ndef float32_matmul_error(a, b):\r\n    a = a.type(torch.float64)\r\n    b = b.type(torch.float64)\r\n    n = max(a.shape+b.shape)\r\n    u = np.finfo(np.dtype(\'float32\')).eps\r\n    gamma=(n*u)/(1-n*u)\r\n    error=torch.norm(a.flatten())*torch.norm(b.flatten())*gamma\r\n    return error\r\n\r\nerr = float32_matmul_error(a, b)\r\nscipy.linalg.norm(c_torch_mm.numpy()-c_np_einsum, \'fro\')/err  #=> 23.83\r\n```\r\n\r\nIn other words the discrepancy you are seeing is about 23 times less than worst case error of standard matrix multiplication.', 'Thanks you guys so much, it really bothers me a lot.', 'Here\'s the utility I personally for equality testing, it\'s `check_equal` below\r\n```\r\ndef pytorch_dtype_to_floating_numpy_dtype(dtype):\r\n    """"""Converts PyTorch dtype to numpy floating point dtype, defaulting to np.float32 for non-floating point types.""""""\r\n    if dtype == torch.float64:\r\n        dtype = np.float64\r\n    elif dtype == torch.float32:\r\n        dtype = np.float32\r\n    elif dtype == torch.float16:\r\n        dtype = np.float16\r\n    else:\r\n        dtype = np.float32\r\n    return dtype\r\n\r\ndef to_numpy(x, dtype: np.dtype = None) -> np.ndarray:\r\n    """"""\r\n    Convert numeric object to floating point numpy array. If dtype is not specified, use PyTorch default dtype.\r\n\r\n    Args:\r\n        x: numeric object\r\n        dtype: numpy dtype, must be floating point\r\n\r\n    Returns:\r\n        floating point numpy array\r\n    """"""\r\n\r\n    assert np.issubdtype(dtype, np.floating), ""dtype must be real-valued floating point""\r\n\r\n    # Convert to normal_form expression from a special form (https://reference.wolfram.com/language/ref/Normal.html)\r\n    if hasattr(x, \'normal_form\'):\r\n        x = x.normal_form()\r\n\r\n    if type(x) == np.ndarray:\r\n        assert np.issubdtype(x.dtype, np.floating), f""numpy type promotion not implemented for {x.dtype}""\r\n\r\n    if type(x) == torch.Tensor:\r\n        dtype = pytorch_dtype_to_floating_numpy_dtype(x.dtype)\r\n        return x.detach().cpu().numpy().astype(dtype)\r\n\r\n    # list or tuple, iterate inside to convert PyTorch arrrays\r\n    if type(x) in [list, tuple]:\r\n        x = [to_numpy(r) for r in x]\r\n\r\n    # Some Python type, use numpy conversion\r\n    result = np.array(x, dtype=dtype)\r\n    assert np.issubdtype(result.dtype, np.number), f""Provided object ({result}) is not numeric, has type {result.dtype}""\r\n    if dtype is None:\r\n        return result.astype(pytorch_dtype_to_floating_numpy_dtype(torch.get_default_dtype()))\r\n    return result\r\n\r\n\r\ndef check_close(a0, b0, rtol=1e-5, atol=1e-8, label: str= \'\') -> None:\r\n    """"""Convenience method for check_equal with tolerances defaulting to typical errors observed in neural network\r\n    ops in float32 precision.""""""\r\n    return check_equal(a0, b0, rtol=rtol, atol=atol, label=label)\r\n\r\n\r\ndef check_equal(observed, truth, rtol=1e-9, atol=1e-12, label: str= \'\') -> None:\r\n    """"""\r\n    Assert fail any entries in two arrays are not close to each to desired tolerance. See np.allclose for meaning of rtol, atol\r\n\r\n    """"""\r\n\r\n    # special handling for lists, which could contain\r\n    #if type(observed) == List and type(truth) == List:\r\n    #    for a, b in zip(observed, truth):\r\n    #        check_equal(a, b)\r\n\r\n    truth = to_numpy(truth)\r\n    observed = to_numpy(observed)\r\n\r\n    assert truth.shape == observed.shape, f""Observed shape {observed.shape}, expected shape {truth.shape}""\r\n    # run np.testing.assert_allclose for extra info on discrepancies\r\n    if not np.allclose(observed, truth, rtol=rtol, atol=atol, equal_nan=True):\r\n        print(f\'Numerical testing failed for {label}\')\r\n        np.testing.assert_allclose(truth, observed, rtol=rtol, atol=atol, equal_nan=True)\r\n\r\n```']",[],"[""import torch\r\nimport numpy as np\r\ntorch.manual_seed(2)\r\na = torch.randn(4, 4)\r\nb = torch.randn(4, 4)\r\nc_torch_mm = torch.mm(a, b)\r\nc_torch_einsum = torch.einsum('ik,kj -> ij', a, b)\r\na_ = a.numpy()\r\nb_ = b.numpy()\r\nc_np_matmul = np.matmul(a_, b_)\r\nc_np_einsum = np.einsum('ik, kj -> ij', a_, b_)\r\n\r\nprint(c_torch_mm.numpy() == c_np_matmul)\r\nprint(c_torch_einsum.numpy() == c_np_einsum)\r\nprint(c_torch_mm == c_torch_einsum)\r\nprint(c_torch_mm.numpy() == c_np_einsum)""]",1,0
75,pytorch,12683,open,CPU triangular solve is slow,"It would be nice to have a GPU version of triangular solve. It's needed for preconditioned stochastic gradient optimizer ([paper](https://ieeexplore.ieee.org/document/7875097/)), and it's possibly the reason why TF implementation runs 3x faster than PyTorch version

Here's a microbenchmark in milliseconds of 8000x8000 triangular solve:
https://github.com/yaroslavvb/newton/blob/master/benchmark_triangular.py



cc @VitalyFedyunin @jianyuh @nikitaved @pearu @mruberry @heitorschueroff @walterddr @IvanYashchuk @xwang233 @ngimel",module: cpu module: linear algebra module: performance triaged,"[""I just realized docs are out of date and there's indeed a gpu implementation"", ""I think CPU triangular solve being much slower than numpy/TF's is worth having a bug open for it."", '@yaroslavvb I believe the CUDA threads are not synchronized in your benchmark, could you please check?', ""I've updated numbers with sync points. TensorFlow CPU version probably benefits from being optimized for AWS by the DLAMI team (probably compiled with MKL and env vars like OMP affinity are properly set for p3.2xlarge machine)\r\n\r\n```\r\nBenchmarking n=10000\r\nnumpy\r\nTimes: min: 203.06, median: 203.48, mean: 204.37\r\nPytorch GPU\r\nTimes: min: 6.47, median: 6.53, mean: 6.64\r\nPytorch CPU\r\nTimes: min: 254.77, median: 263.34, mean: 268.56\r\nTF GPU\r\nTimes: min: 11.65, median: 11.70, mean: 11.69\r\nTF CPU\r\nTimes: min: 62.13, median: 63.75, mean: 63.86\r\n```""]","['\r\nnumpy: Times: min: 127.86, median: 128.12, mean: 128.31\r\ntensorflow: Times: min: 1.20, median: 1.20, mean: 1.20\r\npytorch: Times: min: 162.65, median: 162.65, mean: 162.65\r\n']",[],1,0
76,pytorch,13716,open,depthwise convolution are slow on cpu ,"I try to use depthwise convolution to reduce parameters of my model. However I found depthwise convolutions are slow on cpu, just 4x~5x than normal 3x3 convolution, while input_channel and output channel are 256. Is there any way to speed up the process? My version is pytorch 0.4.1.



cc @VitalyFedyunin @ngimel",module: convolution module: cpu module: performance triaged,"['What are some sample input sizes?', ""I think this is probably because we're using the crappy THNN implementation and not NNPACK"", 'Any news for this?', ""Hi,\r\n\r\nI don't think there is any update no.\r\nDo you have a specific use case that is problematic? (sample input sizes + conv params)"", ""We‚Äôre trying to use MobileNetV2 on the CPU and are seeing big slowdown compared to standard conv nets like e.g. AlexNet. So we were just hoping there would be a new implementation on the way :)\r\nOn 9. Jun 2020, 17:51 +0200, albanD <notifications@github.com>, wrote:\r\n\r\nHi,\r\n\r\nI don't think there is any update no.\r\nDo you have a specific use case that is problematic? (sample input sizes + conv params)\r\n\r\n‚Äî\r\nYou are receiving this because you commented.\r\nReply to this email directly, view it on GitHub<https://github.com/pytorch/pytorch/issues/13716#issuecomment-641394316>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AEOWKJDDUCYLK2AMVU2WTHLRVZLABANCNFSM4GCRKN7A>.\r\n"", 'I think that mkldnn now supports groups in convolution and we should be letting them do the heavy lifting in this case.\r\nDo you use mkldnn on CPU? If so you can use `MKLDNN_VERBOSE=1` to have a better idea of what actually runs with it.']",[],[],1,0
77,pytorch,14231,closed,"`index_select` on flat tensor faster than integer array indexing, even including reshaping","## üêõ Bug

Integer array indexing appears to be slower than applying  to the flattened tensor, using flattened indexes, and then reshaping to the expected output size.

## To Reproduce

Reproduced in this gist: https://gist.github.com/gngdb/3d4f5aa27ee5199b0d4b997ffe21a6b4

## Expected behavior

If this is a faster way to implement integer array indexing, it should be the default. It might not be though, there are other uses for integer array indexing where this speed difference might not hold.

## Environment

PyTorch version: 0.4.1.post2
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Scientific Linux release 7.5 (Nitrogen)
GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)
CMake version: version 2.8.12.2

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 8.0.44
GPU models and configuration: 
GPU 0: TITAN X (Pascal)
GPU 1: TITAN X (Pascal)
GPU 2: TITAN X (Pascal)
GPU 3: TITAN X (Pascal)

Nvidia driver version: 390.87
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] numpy (1.15.2)
[pip] torch (0.4.1.post2)
[pip] torchvision (0.2.1)
[conda] pytorch                   0.4.1           py36_py35_py27__9.0.176_7.1.2_2    pytorch
[conda] torchvision               0.2.1                    py36_1    pytorch",,"['this should be fixed after https://github.com/pytorch/pytorch/pull/13420 is merged', '#13420 has been merged, and most of the slowdown is over (at least for forward).']",[],['index_select'],1,1
78,pytorch,13722,closed,Extremely high GPU memory usage for a simple architecture,"## üêõ Bug

When using a particular architecture, pytorch is throwing CUDA OOME much faster (with ""batch_size"" of 10k) than tensorflow (which runs smoothly with ""batch_size"" of 200k-500k).

## To Reproduce

Steps to reproduce the behavior:

I wrote two almost identical implementations of a problematic NN architecture in both pytorch and tensorflow.

Here is PyTorch version:


And tensorflow:

If you run both scripts on 11GB GPU you should see CUDA OOME in pytorch (on loss.backward() line), but not in tensorflow script.

## Expected behavior

I would expect adequate GPU memory usage for such a simple architecture and small input size (10k numbers), which is the case in tensorflow, but not in pytorch.

## Environment

(This is the environment of Google Colaboratory, but the same issue on two other machines)

PyTorch version: 0.4.1
Is debug build: No
CUDA used to build PyTorch: 9.2.148

OS: Ubuntu 18.04.1 LTS
GCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
CMake version: Could not collect

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.2.148
GPU models and configuration: GPU 0: Tesla K80
Nvidia driver version: 396.44
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.3.1

Versions of relevant libraries:
[pip] Could not collect
[conda] Could not collect",,"['We have fixed this significantly on master; please try building from source.', 'Thanks, it worked!']","[""\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nclass Model(nn.Module):\r\n    def __init__(self, **params):\r\n        super(Model, self).__init__()\r\n        self.windows = [1, 2, 3]\r\n\r\n        self.region_emb_layers = nn.ModuleList([nn.Conv2d(1, 150,\r\n                                                kernel_size=(window, 300), stride=1) \\\r\n                                                for window in self.windows])\r\n        self.beg_conv_layers = nn.ModuleList([nn.Conv2d(450, 450, \\\r\n                                              kernel_size=(3, 1), stride=1) \\\r\n                                              for _ in range(2)])                                   \r\n\r\n        self.pooling = nn.MaxPool2d(kernel_size=(3, 1), stride=2)\r\n        self.padding_conv = nn.ZeroPad2d((0, 0, 0, 2))\r\n        self.act_fun = nn.ReLU()\r\n        self.linear_out = nn.Linear(450, 2)\r\n\r\n    def forward(self, out, emb):\r\n        outs = [None]*len(self.windows)\r\n        out = emb(out)\r\n        out = out.unsqueeze(1)\r\n        \r\n        for i, (window, region_emb_layer) in enumerate(zip(self.windows, self.region_emb_layers)):\r\n            out_ = region_emb_layer(out)\r\n            out_ = nn.ZeroPad2d((0, 0, 0, window - 1))(out_)\r\n            outs[i] = out_\r\n\r\n        out = torch.cat(outs, dim=1)\r\n        \r\n        for beg_conv_layer in self.beg_conv_layers:\r\n            out = self.padding_conv(out)\r\n            out = beg_conv_layer(out)\r\n            out = self.act_fun(out)\r\n            \r\n        out = F.adaptive_max_pool2d(out, (1, 1))\r\n        out = out.view(out.size(0), -1)\r\n        out = self.linear_out(out)\r\n        \r\n        return out\r\n\r\nimport numpy as np\r\n\r\ndef gen(dataset_size, batch_size):\r\n    for i in range(dataset_size // batch_size):\r\n        b = np.random.choice([10, 30, 70, 200, 500, 1000])\r\n        a = batch_size // b\r\n\r\n        x = np.random.randint(0, 42000, (a, b))\r\n        y = np.random.randint(0, 2, (a,))\r\n        \r\n        yield x, y\r\n\r\n\r\ndevice = torch.device('cuda:0')\r\n\r\nmodel = Model().to(device)\r\nmodel = model.to(device)\r\n\r\nprint('Model parameters cnt: {}'.format(sum(p.numel() for p in model.parameters())))\r\n\r\nemb = nn.Embedding(42000, 300).to(device)\r\nemb.weight.requires_grad_(False)\r\n\r\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\r\n\r\ndataset_size = int(1e6)\r\nbatch_size = 10000\r\n\r\nfor i, (x, y) in enumerate(gen(dataset_size, batch_size)):\r\n    if i % 10 == 0:\r\n        print(i)\r\n\r\n    x = torch.from_numpy(x).to(device)\r\n    y = torch.from_numpy(y).to(device)\r\n\r\n    out = model(x, emb=emb)\r\n\r\n    crit = nn.CrossEntropyLoss()\r\n    loss = crit(out, y)\r\n    \r\n    optimizer.zero_grad()\r\n    loss.backward()\r\n    optimizer.step()\r\n"", ""\r\nimport tensorflow\r\nimport numpy as np\r\nimport keras\r\nfrom keras.models import Model\r\nfrom keras.layers import Lambda, Input, Conv2D, Dense, Flatten, \\\r\nEmbedding, Concatenate, ZeroPadding2D\r\nfrom keras.optimizers import Adam\r\nimport keras.backend as K\r\n\r\n\r\nx_in = Input((None,))\r\n\r\nx = x_in\r\n\r\nx = Embedding(42000, 300, trainable=False)(x)\r\nx = Lambda(lambda x: K.expand_dims(x, axis=3))(x)\r\n\r\nx1 = ZeroPadding2D(((0, 2), (0, 0)))(x)\r\nx1 = Conv2D(150, (3, 300), activation='relu')(x1)\r\nx2 = ZeroPadding2D(((0, 1), (0, 0)))(x)\r\nx2 = Conv2D(150, (2, 300), activation='relu')(x2)\r\nx3 = Conv2D(150, (1, 300), activation='relu')(x)\r\n\r\nx = Concatenate(axis=3)([x1, x2, x3])\r\n\r\nx = Conv2D(450, (3, 1), activation='relu', padding='same')(x)\r\nx = Conv2D(450, (3, 1), activation='relu', padding='same')(x)\r\n\r\nx = Lambda(lambda x: K.max(x, axis=1))(x)\r\n\r\nx = Flatten()(x)\r\nx = Dense(1, activation='sigmoid')(x)\r\n\r\nx_out = x\r\n\r\nmodel = Model(inputs=x_in, outputs=x_out)\r\nopt = Adam(lr=1e-3)\r\nmodel.compile(loss='binary_crossentropy', optimizer=opt)\r\n\r\nprint(model.summary())\r\n\r\ndef gen(batch_size):\r\n  while True:\r\n    b = np.random.choice([10, 30, 70, 200, 500, 1000])\r\n    a = batch_size // b\r\n    x = np.random.randint(0, 42000, (a, b))\r\n    y = np.random.randint(0, 2, (a,))\r\n    \r\n    yield x, y\r\n\r\ndataset_size = int(1e6)\r\nbatch_size = 200000\r\n\r\nmodel.fit_generator(gen(batch_size), steps_per_epoch=dataset_size//batch_size)\r\n""]",[],1,1
79,pytorch,2973,closed,broadcasting inconsistency?,"when we multiply (or any binary operator) two tensor variables, i've noticed the weird behaviour depending on the shapes of those variables:

(a) when x.size()=(1,10) and y.size()=(10), (x\*y).size()=(1,10) (expected)
(b) when x.size()=(10) and y.size()=(1,10), (x\*y).size()=(1,10) (expected)
(c) when x.size()=(10,1) and y.size()=(10), (x\*y).size()=(10,10) (**unexpected**)
(d) when x.size()=(10) and y.size()=(10,1), (x\*y).size()=(10,10) (**unexpected**)
(e) when x.size()=(10,1) and y.size()=(1,10), (x\*y).size()=(10,10) (expected)

the cases (c) and (d) really easily throw off many (or perhaps only me) from debugging, when such cases happen in a loss function that reduces the output into a single scalar. 

my expectation is that broadcasting should first find a matching axis, and i wonder if the current implementation/behaviour is expected and was designed with a certain goal in mind.

",,"['@gchanan ', ""Hi @kyunghyuncho: these are expected behavior and consistent with NumPy broadcasting, see: http://pytorch.org/docs/master/notes/broadcasting.html for more information, but basically, the dimensions are matched in rightmost order, there is no inference for a matching axis size.\r\n\r\nI'm not sure I completely understand your scalar point about (c) and (d) -- could you provide an example?  But if you are doing reductions, you can control whether the (1) in the reduced dimension is preserved via the `keepdim` parameter, for example in `torch.sum`."", 'i meant the following scenario:\r\n\r\nloss = torch.mean((x - y) ** 2)\r\n\r\nloss is a scalar regardless of whether (x-y).size() is (10,10) or (10,1). this makes debugging pretty much impossible :(', 'i wonder if we could at least put some kind of warning or automatic ``squeeze``ing in loss functions. after all, loss functions should work with two input tensors of the same size.', 'You can always add `assert x.size() == y.size()` in your code if you have your own loss function', 'yes, i can. though, many people (including many students in my lab) heavily rely on the loss functions available from pytorch, and having such an assertion expression in pytorch by default would be tremendously helpful.', 'I think most of them should already have fairly strict requirements for sizes. Do you have any specific ones in mind?', ""We do also have `torch.utils.backcompat.broadcast_warning.enabled=True` (see the documentation above).  Turning that on won't be limited to loss functions but it could help in debugging."", 'You can turn it on only periodically (e.g. using a context manager) to limit it to certain scopes you want to debug.', ""that's helpful. thanks!""]",[],[],1,0
80,pytorch,21926,closed,Bug in saving indexed torch tensors makes it much slower than numpy.,"## üêõ Bug

I am indexing tensors and storing each element of the batch separately so that they can be read individually from disk. The output of the model is a tensor of size (40,100,256,256). When I index one element from it and store the (100,256,256) dimensional tensor using torch.save() it takes the exact same time as it takes to save the whole (40,100,256,256) object.

On the other hand, if I do a .numpy() and do the same process in numpy it is much faster.

May be some thing to do with how torch handles indexed sub-tensors. Is it a separate object, or the same one? 

Dummy code below reproduces the issue with PyTorch 1.0.1.

## To Reproduce


<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior
I'd expect torch and numpy to not be SO different.

## Environment
PyTorch version: 1.0.1
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 14.04.6 LTS
GCC version: (GCC) 5.2.0
CMake version: version 3.10.20171205-gd06b8

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.1.105
GPU models and configuration: 
GPU 0: TITAN Xp
GPU 1: TITAN Xp
GPU 2: TITAN Xp
GPU 3: TITAN Xp

Nvidia driver version: 418.56
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.15.2
[conda] blas                      1.0                         mkl  
[conda] mkl                       2019.1                      144  
[conda] mkl_fft                   1.0.10           py36ha843d7b_0  
[conda] mkl_random                1.0.2            py36hd81dba3_0  
[conda] pytorch                   1.0.1           cuda100py36he554f03_0  
[conda] torchfile                 0.1.0                     <pip>
[conda] torchnet                  0.0.4                     <pip>
[conda] torchvision               0.2.1                    py36_0

## Additional context

Saving indexed sub-tensors can be important when storing features of images etc. Due to large dataset sizes, things are usually done in batches, but it may be important to extract each element out of the batch it was processed in.",,"['this is not a bug, it\'s actually by design.\r\n\r\nwhen you index individual elements into a Tensor, then you are holding a view into the original Tensor.\r\n\r\n```\r\na = torch.randn(10, 20)\r\nb = a[0]\r\nb.add_(10) # changes `a`\r\n```\r\n\r\nIf you actually do `torch.save([a, b], \'foo.pt\')` then PyTorch makes sure that this ""view"" property is actually preserved across serialization / deserialization.\r\nSo, \r\n\r\n```\r\na = torch.randn(10, 20)\r\nb = a[0]\r\nb.add_(10) # changes `a`\r\n\r\ntorch.save((a, b), \'foo.pt\')\r\na, b = torch.save(\'foo.pt\')\r\n\r\nb.add_(10) # still changes `a`\r\n```\r\n\r\nSo, what you are seeing is that even when you are indexing, the whole original Storage is saved.\r\n\r\nYou can make pytorch only save the particular element with `torch.save(a[0].clone()`', 'Interesting, what was the reason behind doing this?\r\n\r\nThanks,\r\nSpandan', 'if you do a `torch.save(my_model, ...)` where your model has parameters that are views, or if you do `torch.save([my_model, my_optimizer], ...)`, when you load the model back, you want the preserving property that these both continue to work as expected', 'Thanks!']","[""\r\nimport time\r\nimport torch\r\nimport numpy as np\r\n\r\na = torch.rand((40,100,256,256))\r\nb = np.ones((40,100,256,256))\r\nc = a.cpu()\r\n\r\nt1 = time.time()\r\ntorch.save(a[0],open('torch1.p','wb'))\r\nt2 = time.time()\r\nprint(t2 - t1)\r\n\r\na = a.cuda()\r\ntorch.save(a[0],open('torch2.p','wb'))\r\nt3 = time.time()\r\nprint(t3 - t2)\r\n\r\nnp.save(open('numpy1.p','wb'),c[0])\r\nt4 = time.time()\r\nprint(t4 - t3)\r\n\r\nnp.save(open('numpy2.p','wb'),b[0])\r\nt5= time.time()\r\nprint(t5 - t4)\r\n""]",[],1,0
81,pytorch,22961,open,performance much worse on 2080ti than 1080ti,"## üêõ Bug

I have a model that I have historically trained on 1080ti, and recently I discovered that the training speed is much worse (almost 2x slower) on 2080ti. The rest of the setup (nvidia driver  + cpu + networking) is the same between the two.

I profiled my script using , and discovered that on the 2080ti, way too much time (~70%) is spent in this function:


Any ideas what the problem could be?
I have attached the two profiles in case they are helpful.
[1080ti.log](https://github.com/pytorch/pytorch/files/3400744/1080ti.log)
[2080ti.log](https://github.com/pytorch/pytorch/files/3400745/2080ti.log)

 - PyTorch Version (e.g., 1.0): 1.1.0
 - OS (e.g., Linux): Ubuntu 16.04
 - How you installed PyTorch (, , source): pip
 - Build command you used (if compiling from source):
 - Python version: 3.6
 - CUDA/cuDNN version: 10.0 / 7.4
 - GPU models and configuration: 1080ti + 2080ti, nvidia driver 410.78
 - Any other relevant information:
",module: cuda module: performance triaged,"['My guess is that PyTorch prebuild binaries are compiled with CUDA Compute Capabilities up to 70 (including 61 targeting GTX 1080Ti).\r\n\r\nRTX 2080Ti is CUDA Compute Capabilities 75 and thus doesn\'t benefit from the optimized kernels ... The solution would be to rebuild PyTorch especially targeting 6.1 and 7.5 as follow:\r\n\r\n`TORCH_CUDA_ARCH_LIST=""6.1;7.5"" python setup.py bdist_wheel`\r\n\r\n@soumith Do you know if compute_75,sm_75 is supported in the distribution wheels ?', '@ngimel any ideas on what this could be?', ""It looks like cudnn is not picking the right algorithm. Are  you using `torch.backends.cudnn.benchmark=True` ?\r\nThank you, profiles are very helpful, can you also please collect cudnn call logs (https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#api-logging) or alternatively give us a small repro script?\r\nAlso, is it possible to try cudnn 7.6? There were some heuristics updates that went into it, so the problem might have been fixed.\r\n@mfuntowicz when pytorch binaries are built, cudnn pruning still leaves 7.5 architecture kernels in. https://github.com/pytorch/builder/blob/59ad166ce23abcad030d922aa0331530a7dc7eda/manywheel/Dockerfile_100#L100, so cudnn should still be picking the right kernels for Turing, if it does not, it is a cudnn bug. For pytorch itself, whether 7.5 architecture is included in compilation flags or not (I don't know off the top of my head if it is), it should not matter much."", 'Thanks for the quick response!\r\n\r\n@ngimel I am not setting `torch.backends.cudnn.benchmark`.\r\n\r\nHere are the cudnn logs:\r\n[1080ti_cudnn.log](https://github.com/pytorch/pytorch/files/3403238/1080ti_cudnn.log)\r\n[2080ti_cudnn.log](https://github.com/pytorch/pytorch/files/3403239/2080ti_cudnn.log)\r\n\r\nIt might be difficult to give a repro script, but my model is mainly conv2d, conv3d, batch norm, relu. I tried my benchmark using a subset of the model (no conv3d) and there is no performance discrepancy between 1080ti and 2080ti.\r\n\r\nI can try building from source against cudnn 7.6.', '@ngimel Thanks for the link, I was looking exactly for this one early today ! \r\n\r\nThus, if removing conv3d leads to no discrepancy you might be in the following: \r\n\r\n[CuDNN 7.5.0 Release notes](https://docs.nvidia.com/deeplearning/sdk/cudnn-release-notes/rel_750.html#rel_750):\r\n\r\n> In cuDNN 7.4.2, for some cases the 3D convolution resulted in a reduced performance on Turing GPUs, compared to the previous cuDNN releases. This is fixed.', 'update: rebuilding with cudnn 7.6 had no effect', 'Same problem for me.', 'Same problem', 'From the latest (7.6.1) release notes, it seems this is a known issue in all versions of CUDNN:\r\n\r\n> In cuDNN 7.6.1, on Volta architecture only, there may be a performance degradation when the function cudnnConvolutionBackwardFilter() is used for 3D convolutions with CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1.\r\n> In cuDNN 7.6.1, on Turing and Pascal architectures, performance may be degraded for cudnnConvolutionBackwardData(), when used with the following conditions:\r\nCUDNN_CONVOLUTION_BWD_DATA_ALGO_0 for 3D convolutions\r\nwDesc, dyDesc and dxDesc are all in NCDHW\r\nData type configuration is FLOAT_CONFIG (i.e., single precision data and compute)\r\n\r\nAlthough it only mentions single precision on Turing. It also says it applies to Pascal too.', 'interesting -- from pytorch, is there any way to choose an alternative algorithm? or any other suggested workaround?', 'Please see this link https://github.com/hyperfraise/Apex-bench with reproductible code. Not exactly what you guys talk about, but related (maybe ?)', 'After profiling via torch.autograd.profiler.profile, I observed the following issue, a significant amount of time is spent on the CPU side during CudnnConvolutionBackward, cudnn_convolution_backward,CudnnBatchNormBackward,cudnn_batch_norm_backward. Note that I am using half precision (via apex), and my network use 3D convolution operations. I use cuDNN 7.6.1, CUDA 10.0, and pytorch 1.1.0. The GPU is RTX 2080 ti.\r\n\r\nIn contrast, a dumb approach which uses .half() only spends a tiny fraction of this time on the CPU side.\r\n\r\n* RTX 2080 ti with torch half\r\n\r\n```\r\n------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------\r\nName                                  Self CPU total %   Self CPU total      CPU total %        CPU total     CPU time avg     CUDA total %       CUDA total    CUDA time avg  Number of Calls\r\n------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------\r\ntorch::autograd::GraphRoot                      0.01%         30.060us            0.01%         30.060us         30.060us            0.00%          8.320us          8.320us                1\r\nNllLossBackward                                 0.08%        253.392us            0.08%        253.392us        253.392us            0.00%        246.368us        246.368us                1\r\nnll_loss_backward                               0.06%        177.542us            0.06%        177.542us        177.542us            0.00%        176.064us        176.064us                1\r\nLogSoftmaxBackward                              0.03%         92.631us            0.03%         92.631us         92.631us            0.00%         92.160us         92.160us                1\r\n_log_softmax_backward_data                      0.02%         75.321us            0.02%         75.321us         75.321us            0.00%         77.152us         77.152us                1\r\nAddmmBackward                                   0.09%        272.563us            0.09%        272.563us        272.563us            0.01%        272.544us        272.544us                1\r\nunsigned short                                  0.01%         19.150us            0.01%         19.150us         19.150us            0.00%         18.592us         18.592us                1\r\nmm                                              0.04%        123.522us            0.04%        123.522us        123.522us            0.00%        125.408us        125.408us                1\r\nunsigned short                                  0.00%         12.120us            0.00%         12.120us         12.120us            0.00%         12.288us         12.288us                1\r\nmm                                              0.02%         56.040us            0.02%         56.040us         56.040us            0.00%         57.376us         57.376us                1\r\nunsigned short                                  0.00%          7.751us            0.00%          7.751us          7.751us            0.00%          7.168us          7.168us                1\r\nsum                                             0.03%         89.521us            0.03%         89.521us         89.521us            0.00%         90.368us         90.368us                1\r\nview                                            0.00%         15.110us            0.00%         15.110us         15.110us            0.00%         15.488us         15.488us                1\r\ntorch::autograd::AccumulateGrad                 0.01%         20.210us            0.01%         20.210us         20.210us            0.00%         20.352us         20.352us                1\r\nTBackward                                       0.01%         16.341us            0.01%         16.341us         16.341us            0.00%         16.096us         16.096us                1\r\nunsigned short                                  0.00%          7.851us            0.00%          7.851us          7.851us            0.00%          7.712us          7.712us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          5.730us            0.00%          5.730us          5.730us            0.00%          4.960us          4.960us                1\r\nViewBackward                                    0.01%         36.970us            0.01%         36.970us         36.970us            0.00%         36.576us         36.576us                1\r\nreshape                                         0.01%         28.080us            0.01%         28.080us         28.080us            0.00%         28.512us         28.512us                1\r\nas_strided                                      0.00%          7.000us            0.00%          7.000us          7.000us            0.00%          7.680us          7.680us                1\r\nAdaptiveAvgPool3DBackward                       0.02%         77.891us            0.02%         77.891us         77.891us            0.02%        808.960us        808.960us                1\r\nadaptive_avg_pool3d_backward                    0.02%         64.461us            0.02%         64.461us         64.461us            0.02%        800.512us        800.512us                1\r\nReluBackward1                                   0.02%         59.111us            0.02%         59.111us         59.111us            0.00%         40.960us         40.960us                1\r\nthreshold_backward                              0.01%         42.751us            0.01%         42.751us         42.751us            0.00%         38.304us         38.304us                1\r\nAddBackward0                                    0.00%          4.440us            0.00%          4.440us          4.440us            0.00%          1.632us          1.632us                1\r\nNativeBatchNormBackward                         0.03%        103.371us            0.03%        103.371us        103.371us            0.00%         74.496us         74.496us                1\r\nnative_batch_norm_backward                      0.02%         75.431us            0.02%         75.431us         75.431us            0.00%         71.680us         71.680us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          6.361us            0.00%          6.361us          6.361us            0.00%          0.704us          0.704us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          4.970us            0.00%          4.970us          4.970us            0.00%          1.824us          1.824us                1\r\nCudnnConvolutionBackward                        0.69%          2.191ms            0.69%          2.191ms          2.191ms            0.04%          2.274ms          2.274ms                1\r\ncudnn_convolution_backward                      0.68%          2.171ms            0.68%          2.171ms          2.171ms            0.04%          2.271ms          2.271ms                1\r\ntorch::autograd::AccumulateGrad                 0.00%          6.710us            0.00%          6.710us          6.710us            0.00%          0.929us          0.929us                1\r\nReluBackward1                                   0.01%         46.211us            0.01%         46.211us         46.211us            0.00%         26.592us         26.592us                1\r\nthreshold_backward                              0.01%         33.381us            0.01%         33.381us         33.381us            0.00%         22.880us         22.880us                1\r\nNativeBatchNormBackward                         0.02%         65.761us            0.02%         65.761us         65.761us            0.00%         43.584us         43.584us                1\r\nnative_batch_norm_backward                      0.01%         46.851us            0.01%         46.851us         46.851us            0.00%         40.960us         40.960us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          6.090us            0.00%          6.090us          6.090us            0.00%          1.729us          1.729us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          4.590us            0.00%          4.590us          4.590us            0.00%          0.832us          0.832us                1\r\nCudnnConvolutionBackward                        0.47%          1.495ms            0.47%          1.495ms          1.495ms            0.03%          1.626ms          1.626ms                1\r\ncudnn_convolution_backward                      0.46%          1.479ms            0.46%          1.479ms          1.479ms            0.03%          1.622ms          1.622ms                1\r\ntorch::autograd::AccumulateGrad                 0.00%          6.580us            0.00%          6.580us          6.580us            0.00%          2.048us          2.048us                1\r\nReluBackward1                                   0.01%         43.341us            0.01%         43.341us         43.341us            0.00%         22.688us         22.688us                1\r\nthreshold_backward                              0.01%         31.021us            0.01%         31.021us         31.021us            0.00%         19.136us         19.136us                1\r\nNativeBatchNormBackward                         0.02%         64.161us            0.02%         64.161us         64.161us            0.00%         40.320us         40.320us                1\r\nnative_batch_norm_backward                      0.01%         45.981us            0.01%         45.981us         45.981us            0.00%         37.312us         37.312us                1\r\ntorch::autograd::AccumulateGrad                 0.00%         10.750us            0.00%         10.750us         10.750us            0.00%          2.048us          2.048us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          4.750us            0.00%          4.750us          4.750us            0.00%          1.504us          1.504us                1\r\nCudnnConvolutionBackward                        0.06%        187.662us            0.06%        187.662us        187.662us            0.03%          1.384ms          1.384ms                1\r\ncudnn_convolution_backward                      0.05%        173.032us            0.05%        173.032us        173.032us            0.03%          1.381ms          1.381ms                1\r\nadd                                             0.01%         40.201us            0.01%         40.201us         40.201us            0.00%         34.528us         34.528us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          6.110us            0.00%          6.110us          6.110us            0.00%          0.832us          0.832us                1\r\nReluBackward1                                   0.01%         37.130us            0.01%         37.130us         37.130us            0.00%         45.057us         45.057us                1\r\nthreshold_backward                              0.01%         25.600us            0.01%         25.600us         25.600us            0.00%         42.592us         42.592us                1\r\nAddBackward0                                    0.00%          4.000us            0.00%          4.000us          4.000us            0.00%          1.761us          1.761us                1\r\nNativeBatchNormBackward                         0.02%         57.550us            0.02%         57.550us         57.550us            0.00%         76.607us         76.607us                1\r\nnative_batch_norm_backward                      0.01%         39.830us            0.01%         39.830us         39.830us            0.00%         75.008us         75.008us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          6.060us            0.00%          6.060us          6.060us            0.00%          1.695us          1.695us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          4.720us            0.00%          4.720us          4.720us            0.00%          0.736us          0.736us                1\r\nCudnnConvolutionBackward                        0.05%        153.481us            0.05%        153.481us        153.481us            0.03%          1.411ms          1.411ms                1\r\ncudnn_convolution_backward                      0.04%        134.891us            0.04%        134.891us        134.891us            0.03%          1.408ms          1.408ms                1\r\ntorch::autograd::AccumulateGrad                 0.00%          6.150us            0.00%          6.150us          6.150us            0.00%          1.568us          1.568us                1\r\nReluBackward1                                   0.01%         46.971us            0.01%         46.971us         46.971us            0.00%         27.487us         27.487us                1\r\nthreshold_backward                              0.01%         31.490us            0.01%         31.490us         31.490us            0.00%         26.111us         26.111us                1\r\nNativeBatchNormBackward                         0.02%         64.061us            0.02%         64.061us         64.061us            0.00%         47.104us         47.104us                1\r\nnative_batch_norm_backward                      0.01%         38.801us            0.01%         38.801us         38.801us            0.00%         44.353us         44.353us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          5.890us            0.00%          5.890us          5.890us            0.00%          1.695us          1.695us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          4.540us            0.00%          4.540us          4.540us            0.00%          0.896us          0.896us                1\r\nCudnnConvolutionBackward                        0.43%          1.358ms            0.43%          1.358ms          1.358ms            0.03%          1.624ms          1.624ms                1\r\ncudnn_convolution_backward                      0.42%          1.343ms            0.42%          1.343ms          1.343ms            0.03%          1.620ms          1.620ms                1\r\ntorch::autograd::AccumulateGrad                 0.00%          6.400us            0.00%          6.400us          6.400us            0.00%          1.663us          1.663us                1\r\nReluBackward1                                   0.02%         49.950us            0.02%         49.950us         49.950us            0.00%         27.553us         27.553us                1\r\nthreshold_backward                              0.01%         37.140us            0.01%         37.140us         37.140us            0.00%         24.575us         24.575us                1\r\nNativeBatchNormBackward                         0.02%         63.521us            0.02%         63.521us         63.521us            0.00%         43.391us         43.391us                1\r\nnative_batch_norm_backward                      0.01%         45.331us            0.01%         45.331us         45.331us            0.00%         41.119us         41.119us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          6.310us            0.00%          6.310us          6.310us            0.00%          1.664us          1.664us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          4.830us            0.00%          4.830us          4.830us            0.00%          0.896us          0.896us                1\r\nCudnnConvolutionBackward                        0.04%        135.992us            0.04%        135.992us        135.992us            0.03%          1.393ms          1.393ms                1\r\ncudnn_convolution_backward                      0.04%        118.831us            0.04%        118.831us        118.831us            0.03%          1.389ms          1.389ms                1\r\nadd                                             0.01%         28.780us            0.01%         28.780us         28.780us            0.00%         35.008us         35.008us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          6.130us            0.00%          6.130us          6.130us            0.00%          1.951us          1.951us                1\r\nReluBackward1                                   0.01%         42.411us            0.01%         42.411us         42.411us            0.00%         46.943us         46.943us                1\r\nthreshold_backward                              0.01%         30.281us            0.01%         30.281us         30.281us            0.00%         44.770us         44.770us                1\r\nAddBackward0                                    0.00%          4.210us            0.00%          4.210us          4.210us            0.00%          2.049us          2.049us                1\r\nNativeBatchNormBackward                         0.02%         62.710us            0.02%         62.710us         62.710us            0.00%         80.287us         80.287us                1\r\nnative_batch_norm_backward                      0.01%         44.850us            0.01%         44.850us         44.850us            0.00%         78.209us         78.209us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          5.570us            0.00%          5.570us          5.570us            0.00%          1.920us          1.920us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          4.750us            0.00%          4.750us          4.750us            0.00%          0.672us          0.672us                1\r\nCudnnConvolutionBackward                        0.17%        544.115us            0.17%        544.115us        544.115us            0.11%          5.790ms          5.790ms                1\r\ncudnn_convolution_backward                      0.17%        528.815us            0.17%        528.815us        528.815us            0.11%          5.787ms          5.787ms                1\r\ntorch::autograd::AccumulateGrad                 0.00%         15.000us            0.00%         15.000us         15.000us            0.00%          1.822us          1.822us                1\r\nNativeBatchNormBackward                         0.02%         66.350us            0.02%         66.350us         66.350us            0.00%         76.543us         76.543us                1\r\nnative_batch_norm_backward                      0.01%         46.760us            0.01%         46.760us         46.760us            0.00%         74.848us         74.848us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          5.750us            0.00%          5.750us          5.750us            0.00%          1.537us          1.537us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          5.000us            0.00%          5.000us          5.000us            0.00%          2.049us          2.049us                1\r\nCudnnConvolutionBackward                        0.04%        130.121us            0.04%        130.121us        130.121us            0.03%          1.412ms          1.412ms                1\r\ncudnn_convolution_backward                      0.04%        115.561us            0.04%        115.561us        115.561us            0.03%          1.409ms          1.409ms                1\r\ntorch::autograd::AccumulateGrad                 0.00%          5.880us            0.00%          5.880us          5.880us            0.00%          2.049us          2.049us                1\r\nReluBackward1                                   0.01%         46.741us            0.01%         46.741us         46.741us            0.00%         31.264us         31.264us                1\r\nthreshold_backward                              0.01%         35.161us            0.01%         35.161us         35.161us            0.00%         29.727us         29.727us                1\r\nNativeBatchNormBackward                         0.02%         60.841us            0.02%         60.841us         60.841us            0.00%         46.176us         46.176us                1\r\n------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------\r\nSelf CPU time total: 318.945ms\r\nCUDA time total: 5.090s\r\n\r\n```\r\n\r\n* RTX 2080 ti with apex half\r\n\r\n```\r\n\r\n------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------\r\nName                                  Self CPU total %   Self CPU total      CPU total %        CPU total     CPU time avg     CUDA total %       CUDA total    CUDA time avg  Number of Calls\r\n------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------\r\nto                                              0.00%          4.650us            0.00%          4.650us          4.650us            0.00%          3.904us          3.904us                1\r\nis_floating_point                               0.00%          2.270us            0.00%          2.270us          2.270us            0.00%          2.048us          2.048us                1\r\nmul                                             0.00%         41.471us            0.00%         41.471us         41.471us            0.00%         41.568us         41.568us                1\r\ntorch::autograd::GraphRoot                      0.00%         29.271us            0.00%         29.271us         29.271us            0.00%          7.616us          7.616us                1\r\nMulBackward0                                    0.00%        162.214us            0.00%        162.214us        162.214us            0.00%        157.504us        157.504us                1\r\nmul                                             0.00%        107.863us            0.00%        107.863us        107.863us            0.00%        108.768us        108.768us                1\r\nNllLossBackward                                 0.00%        159.613us            0.00%        159.613us        159.613us            0.00%        159.744us        159.744us                1\r\nnll_loss_backward                               0.00%        129.183us            0.00%        129.183us        129.183us            0.00%        128.640us        128.640us                1\r\nLogSoftmaxBackward                              0.00%         78.661us            0.00%         78.661us         78.661us            0.00%         77.856us         77.856us                1\r\n_log_softmax_backward_data                      0.00%         64.331us            0.00%         64.331us         64.331us            0.00%         65.536us         65.536us                1\r\ntorch::autograd::CopyBackwards                  0.00%         85.042us            0.00%         85.042us         85.042us            0.00%         85.376us         85.376us                1\r\nto                                              0.00%         63.722us            0.00%         63.722us         63.722us            0.00%         64.833us         64.833us                1\r\nempty                                           0.00%          8.841us            0.00%          8.841us          8.841us            0.00%          9.376us          9.376us                1\r\nAddmmBackward                                   0.00%        233.895us            0.00%        233.895us        233.895us            0.00%        233.792us        233.792us                1\r\nunsigned short                                  0.00%         21.271us            0.00%         21.271us         21.271us            0.00%         21.792us         21.792us                1\r\nmm                                              0.00%         95.212us            0.00%         95.212us         95.212us            0.00%         98.176us         98.176us                1\r\nunsigned short                                  0.00%          8.100us            0.00%          8.100us          8.100us            0.00%          8.160us          8.160us                1\r\nmm                                              0.00%         52.021us            0.00%         52.021us         52.021us            0.00%         53.152us         53.152us                1\r\nunsigned short                                  0.00%         12.400us            0.00%         12.400us         12.400us            0.00%         12.288us         12.288us                1\r\nsum                                             0.00%         88.282us            0.00%         88.282us         88.282us            0.00%         88.736us         88.736us                1\r\nview                                            0.00%         14.390us            0.00%         14.390us         14.390us            0.00%         14.336us         14.336us                1\r\nTBackward                                       0.00%         20.000us            0.00%         20.000us         20.000us            0.00%         18.976us         18.976us                1\r\nunsigned short                                  0.00%         10.590us            0.00%         10.590us         10.590us            0.00%         10.240us         10.240us                1\r\ntorch::autograd::CopyBackwards                  0.00%         60.421us            0.00%         60.421us         60.421us            0.00%         59.520us         59.520us                1\r\nto                                              0.00%         49.261us            0.00%         49.261us         49.261us            0.00%         50.176us         50.176us                1\r\nempty                                           0.00%          8.110us            0.00%          8.110us          8.110us            0.00%          8.192us          8.192us                1\r\ntorch::autograd::AccumulateGrad                 0.00%         11.930us            0.00%         11.930us         11.930us            0.00%         12.000us         12.000us                1\r\ntorch::autograd::CopyBackwards                  0.00%         51.741us            0.00%         51.741us         51.741us            0.00%         52.512us         52.512us                1\r\nto                                              0.00%         37.550us            0.00%         37.550us         37.550us            0.00%         38.400us         38.400us                1\r\nempty                                           0.00%          9.070us            0.00%          9.070us          9.070us            0.00%          8.705us          8.705us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          6.170us            0.00%          6.170us          6.170us            0.00%          6.144us          6.144us                1\r\nViewBackward                                    0.00%         44.671us            0.00%         44.671us         44.671us            0.00%         43.840us         43.840us                1\r\nreshape                                         0.00%         31.741us            0.00%         31.741us         31.741us            0.00%         32.320us         32.320us                1\r\nas_strided                                      0.00%          9.930us            0.00%          9.930us          9.930us            0.00%          8.128us          8.128us                1\r\nAdaptiveAvgPool3DBackward                       0.00%         85.322us            0.00%         85.322us         85.322us            0.02%        810.496us        810.496us                1\r\nadaptive_avg_pool3d_backward                    0.00%         71.082us            0.00%         71.082us         71.082us            0.02%        800.768us        800.768us                1\r\nReluBackward1                                   0.00%         60.452us            0.00%         60.452us         60.452us            0.00%         42.432us         42.432us                1\r\nthreshold_backward                              0.00%         38.450us            0.00%         38.450us         38.450us            0.00%         38.880us         38.880us                1\r\nAddBackward0                                    0.00%          4.560us            0.00%          4.560us          4.560us            0.00%          2.048us          2.048us                1\r\nCudnnBatchNormBackward                          0.03%          1.231ms            0.03%          1.231ms          1.231ms            0.01%        579.008us        579.008us                1\r\ncontiguous                                      0.00%          5.170us            0.00%          5.170us          5.170us            0.00%          0.640us          0.640us                1\r\ncudnn_batch_norm_backward                       0.02%          1.190ms            0.02%          1.190ms          1.190ms            0.01%        573.280us        573.280us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          7.030us            0.00%          7.030us          7.030us            0.00%          6.176us          6.176us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          5.290us            0.00%          5.290us          5.290us            0.00%          5.408us          5.408us                1\r\nCudnnConvolutionBackward                        0.02%          1.134ms            0.02%          1.134ms          1.134ms            0.04%          1.837ms          1.837ms                1\r\ncudnn_convolution_backward                      0.02%          1.116ms            0.02%          1.116ms          1.116ms            0.04%          1.825ms          1.825ms                1\r\ntorch::autograd::CopyBackwards                  0.00%         64.342us            0.00%         64.342us         64.342us            0.00%         35.457us         35.457us                1\r\nto                                              0.00%         52.031us            0.00%         52.031us         52.031us            0.00%         32.769us         32.769us                1\r\nempty                                           0.00%         10.860us            0.00%         10.860us         10.860us            0.00%          1.760us          1.760us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          6.470us            0.00%          6.470us          6.470us            0.00%          0.640us          0.640us                1\r\nReluBackward1                                   0.00%         47.741us            0.00%         47.741us         47.741us            0.00%         26.272us         26.272us                1\r\nthreshold_backward                              0.00%         35.231us            0.00%         35.231us         35.231us            0.00%         22.752us         22.752us                1\r\nCudnnBatchNormBackward                          0.00%         79.771us            0.00%         79.771us         79.771us            0.00%         34.911us         34.911us                1\r\ncontiguous                                      0.00%          4.840us            0.00%          4.840us          4.840us            0.00%          2.049us          2.049us                1\r\ncudnn_batch_norm_backward                       0.00%         51.131us            0.00%         51.131us         51.131us            0.00%         28.673us         28.673us                1\r\ntorch::autograd::AccumulateGrad                 0.00%         10.820us            0.00%         10.820us         10.820us            0.00%          2.048us          2.048us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          5.210us            0.00%          5.210us          5.210us            0.00%          1.504us          1.504us                1\r\nCudnnConvolutionBackward                        0.03%          1.502ms            0.03%          1.502ms          1.502ms            0.03%          1.608ms          1.608ms                1\r\ncudnn_convolution_backward                      0.03%          1.486ms            0.03%          1.486ms          1.486ms            0.03%          1.605ms          1.605ms                1\r\ntorch::autograd::CopyBackwards                  0.00%         60.001us            0.00%         60.001us         60.001us            0.00%         16.384us         16.384us                1\r\nto                                              0.00%         48.501us            0.00%         48.501us         48.501us            0.00%         13.409us         13.409us                1\r\nempty                                           0.00%          9.650us            0.00%          9.650us          9.650us            0.00%          0.576us          0.576us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          6.470us            0.00%          6.470us          6.470us            0.00%          1.504us          1.504us                1\r\nReluBackward1                                   0.00%         51.271us            0.00%         51.271us         51.271us            0.00%         25.887us         25.887us                1\r\nthreshold_backward                              0.00%         35.340us            0.00%         35.340us         35.340us            0.00%         22.369us         22.369us                1\r\nCudnnBatchNormBackward                          0.00%         78.302us            0.00%         78.302us         78.302us            0.00%         32.385us         32.385us                1\r\ncontiguous                                      0.00%          4.910us            0.00%          4.910us          4.910us            0.00%          2.048us          2.048us                1\r\ncudnn_batch_norm_backward                       0.00%         49.581us            0.00%         49.581us         49.581us            0.00%         25.150us         25.150us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          6.150us            0.00%          6.150us          6.150us            0.00%          0.960us          0.960us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          8.380us            0.00%          8.380us          8.380us            0.00%          1.792us          1.792us                1\r\nCudnnConvolutionBackward                        0.00%        180.934us            0.00%        180.934us        180.934us            0.03%          1.368ms          1.368ms                1\r\ncudnn_convolution_backward                      0.00%        167.003us            0.00%        167.003us        167.003us            0.03%          1.365ms          1.365ms                1\r\nadd                                             0.00%         33.070us            0.00%         33.070us         33.070us            0.00%         37.184us         37.184us                1\r\ntorch::autograd::CopyBackwards                  0.01%        534.221us            0.01%        534.221us        534.221us            0.00%         37.280us         37.280us                1\r\nto                                              0.01%        522.121us            0.01%        522.121us        522.121us            0.00%         34.816us         34.816us                1\r\nempty                                           0.01%        469.580us            0.01%        469.580us        469.580us            0.00%          2.048us          2.048us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          6.831us            0.00%          6.831us          6.831us            0.00%          0.800us          0.800us                1\r\nReluBackward1                                   0.00%         41.371us            0.00%         41.371us         41.371us            0.00%         47.104us         47.104us                1\r\nthreshold_backward                              0.00%         29.670us            0.00%         29.670us         29.670us            0.00%         43.393us         43.393us                1\r\nAddBackward0                                    0.00%          4.590us            0.00%          4.590us          4.590us            0.00%          1.471us          1.471us                1\r\nCudnnBatchNormBackward                          0.00%         86.972us            0.00%         86.972us         86.972us            0.00%         56.735us         56.735us                1\r\ncontiguous                                      0.00%          4.840us            0.00%          4.840us          4.840us            0.00%          2.048us          2.048us                1\r\ncudnn_batch_norm_backward                       0.00%         51.351us            0.00%         51.351us         51.351us            0.00%         49.632us         49.632us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          6.170us            0.00%          6.170us          6.170us            0.00%          2.048us          2.048us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          8.880us            0.00%          8.880us          8.880us            0.00%          1.504us          1.504us                1\r\nCudnnConvolutionBackward                        0.00%        144.583us            0.00%        144.583us        144.583us            0.03%          1.407ms          1.407ms                1\r\ncudnn_convolution_backward                      0.00%        131.113us            0.00%        131.113us        131.113us            0.03%          1.404ms          1.404ms                1\r\ntorch::autograd::CopyBackwards                  0.00%         58.572us            0.00%         58.572us         58.572us            0.00%         34.880us         34.880us                1\r\nto                                              0.00%         47.011us            0.00%         47.011us         47.011us            0.00%         32.544us         32.544us                1\r\nempty                                           0.00%         14.510us            0.00%         14.510us         14.510us            0.00%          2.049us          2.049us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          6.430us            0.00%          6.430us          6.430us            0.00%          1.888us          1.888us                1\r\nReluBackward1                                   0.00%         39.851us            0.00%         39.851us         39.851us            0.00%         28.960us         28.960us                1\r\nthreshold_backward                              0.00%         28.581us            0.00%         28.581us         28.581us            0.00%         26.623us         26.623us                1\r\nCudnnBatchNormBackward                          0.00%         76.212us            0.00%         76.212us         76.212us            0.00%         35.744us         35.744us                1\r\ncontiguous                                      0.00%          4.850us            0.00%          4.850us          4.850us            0.00%          2.047us          2.047us                1\r\ncudnn_batch_norm_backward                       0.00%         48.611us            0.00%         48.611us         48.611us            0.00%         30.016us         30.016us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          5.940us            0.00%          5.940us          5.940us            0.00%          1.504us          1.504us                1\r\ntorch::autograd::AccumulateGrad                 0.00%          8.201us            0.00%          8.201us          8.201us            0.00%          2.049us          2.049us                1\r\nCudnnConvolutionBackward                        0.03%          1.334ms            0.03%          1.334ms          1.334ms            0.03%          1.620ms          1.620ms                1\r\ncudnn_convolution_backward                      0.03%          1.317ms            0.03%          1.317ms          1.317ms            0.03%          1.617ms          1.617ms                1\r\n------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------\r\nSelf CPU time total: 4.869s\r\nCUDA time total: 5.038s\r\n```', 'I also clearly observed this problem, where a RTX 2080 is slower than GTX 1080.\r\nThis is a minimal script that already exhibit the difference:\r\n```python\r\nimport numpy as np\r\nimport time\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\ndevice = torch.device(""cuda:0"")\r\nprint(""torch "", torch.__version__)\r\nprint(""cudnn "", torch.backends.cudnn.version())\r\nprint(torch.cuda.get_device_name(device.index) if device.type == ""cuda"" else ""cpu"")\r\n\r\nclass Model(nn.Module):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.conv1 = nn.Conv3d(1, 8, 3)\r\n        self.conv2 = nn.Conv3d(8, 8, 3)\r\n        self.conv3 = nn.Conv3d(8, 8, 3)\r\n        self.conv4 = nn.Conv3d(8, 1, 3)\r\n\r\n    def forward(self, x):\r\n        x = F.relu(self.conv1(x))\r\n        x = F.relu(self.conv2(x))\r\n        x = F.relu(self.conv3(x))\r\n        x = F.relu(self.conv4(x))\r\n        return x.mean()\r\n\r\nnet = Model()\r\nnet.to(device)\r\n\r\ninput_data = np.zeros((1,1,64,64,64), np.float32)\r\ninputs = torch.from_numpy(input_data).to(device, dtype=torch.float32)\r\n\r\nfor it in range(1, 161):\r\n\r\n    loss = net(inputs[:,:1])\r\n    loss.backward()\r\n\r\n    if it == 20: # ignore warmup\r\n        start_time = time.time()\r\n    if it % 40 == 0:\r\n        print( ""%d (%.2f s)"" % (it, time.time() - start_time))\r\n```', 'Running the script above on RTX 2080ti (centos-7 machine, cuda 10.1):\r\n\r\nGeForce RTX 2080 Ti|GeForce RTX 2080 Ti|GeForce RTX 2080 Ti \r\n-|-|-\r\ntorch  1.0.0|torch  1.1.0|torch  1.3.0 \r\ncudnn  7401|cudnn  7501|cudnn  7603\r\n40 (3.87 s)|40 (3.30 s)|40 (2.77 s)\r\n80 (11.65 s)|80 (10.89 s)|80 (9.10 s)\r\n120 (19.42 s)|120 (18.51 s)|120 (15.44 s)\r\n160 (27.20 s)|160 (26.15 s)|160 (21.78 s)\r\n\r\nWith the older Quadro P4000 (roughly equal to a GTX 1070), on the same machine/config:\r\n\r\nQuadro P4000|Quadro P4000|Quadro P4000\r\n-|-|-\r\ntorch  1.0.0|torch  1.1.0|torch  1.3.0 \r\ncudnn  7401|cudnn  7501|cudnn  7603\r\n40 (0.82 s)|40 (0.80 s)|40 (2.50 s)\r\n80 (2.35 s)|80 (2.34 s)|80 (7.51 s)\r\n120 (3.89 s)|120 (3.88 s)|120 (12.51 s)\r\n160 (5.42 s)|160 (5.42 s)|160 (17.51 s)\r\n\r\nNote that related to the last column of the Quadro above, another GTX 1080, centos-7 ( different machine), running the script under different torch+cudnn settings exhibit large performance difference. Older torch+cudnn had better perfs. Although not directly related to the issue here.\r\nAll use conda binaries, except last one which use pip binaries.\r\n\r\nGeForce GTX 1080|GeForce GTX 1080|GeForce GTX 1080|GeForce GTX 1080|GeForce GTX 1080\r\n-|-|-|-|-\r\ntorch  1.0.0|torch  1.1.0|torch  1.2.0|torch  1.3.0|torch  1.3.0+cu100 pip\r\ncudnn  7401|cudnn  7501|cudnn  7602|cudnn  7603|cudnn  7603     \r\n40 (0.68 s)|40 (0.69 s)|40 (2.01 s)|40 (2.18 s)|40 (2.01 s)\r\n80 (2.04 s)|80 (2.05 s)|80 (6.04 s)|80 (6.55 s)|80 (6.02 s)     \r\n120 (3.40 s)|120 (3.42 s)|120 (10.06 s)|120 (10.92 s)|120 (10.04 s)\r\n160 (4.76 s)|160 (4.78 s)|160 (14.08 s)|160 (15.28 s)|160 (14.05 s) \r\n\r\n', 'As for the script just above, the default algorithm was apparently inadequate.\r\nUsing torch.backends.cudnn.benchmark=True improved the behavior drastically.\r\n\r\nGeForce RTX 2080 Ti|GeForce RTX 2080 Ti benchmark=True\r\n-|-\r\ntorch  1.3.0|torch  1.3.0\r\ncudnn  7603|cudnn  7603\r\n40 (2.77 s)|40 (0.18 s)\r\n80 (9.10 s)|80 (0.68 s)\r\n120 (15.44 s)|120 (1.17 s)\r\n160 (21.78 s)|160 (1.66 s)\r\n\r\n```\r\nIn the logs, these calls are only performed in the slower (benchmark=True) version:\r\n...\r\nI! CuDNN (v7604) function cudnnGetConvolutionForwardAlgorithm_v7() called:\r\nI! CuDNN (v7604) function cudnnGetConvolutionForwardAlgorithmMaxCount() called:\r\n...\r\nI! CuDNN (v7604) function cudnnGetConvolutionBackwardDataAlgorithm_v7() called:\r\nI! CuDNN (v7604) function cudnnGetConvolutionBackwardDataAlgorithmMaxCount() called:\r\n```', 'Hi, I ve found comparable performance discrepancy when running inference o mask rcnn model with RTX2070 vs TITAN RTX.\r\n\r\nRTX2070 shows significantly faster run especially on batch_norm. \r\nNote that `torch.backends.cudnn.benchmark=True` did not improve the behaviour. \r\n\r\nTested on Win10, Pytorch 1.4, CUDA 10.1 just by exchanging GPU on same system.\r\n\r\n\r\nProfile for RTX2070\r\n\r\n```\r\n-------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------\r\nName                             Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     CUDA total %     CUDA total       CUDA time avg    Number of Calls  Input Shapes\r\n\r\n-------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------\r\nto                               0.00%            7.600us          0.00%            7.600us          7.600us          0.00%            2.367us          2.367us          1                []\r\n\r\nto                               1.63%            3.473ms          1.63%            3.473ms          3.473ms          0.57%            3.401ms          3.401ms          1                []\r\n\r\nempty                            0.03%            53.400us         0.03%            53.400us         53.400us         0.30%            1.749ms          1.749ms          1                []\r\n\r\nsub                              0.03%            71.500us         0.03%            71.500us         71.500us         0.02%            102.914us        102.914us        1                []\r\n\r\ndiv                              0.02%            38.300us         0.02%            38.300us         38.300us         0.02%            129.410us        129.410us        1                []\r\n\r\nunsqueeze                        0.00%            10.500us         0.00%            10.500us         10.500us         0.00%            2.047us          2.047us          1                []\r\n\r\nconv2d                           0.08%            177.900us        0.08%            177.900us        177.900us        0.10%            613.344us        613.344us        1                []\r\n\r\nconvolution                      0.07%            146.400us        0.07%            146.400us        146.400us        0.10%            576.160us        576.160us        1                []\r\n\r\n_convolution                     0.06%            133.400us        0.06%            133.400us        133.400us        0.09%            557.055us        557.055us        1                []\r\n\r\ncontiguous                       0.01%            15.500us         0.01%            15.500us         15.500us         0.00%            12.352us         12.352us         1                []\r\n\r\ncudnn_convolution                0.04%            94.100us         0.04%            94.100us         94.100us         0.08%            495.617us        495.617us        1                []\r\n\r\nbatch_norm                       0.07%            145.300us        0.07%            145.300us        145.300us        0.08%            481.211us        481.211us        1                []\r\n\r\n_batch_norm_impl_index           0.06%            131.500us        0.06%            131.500us        131.500us        0.08%            461.086us        461.086us        1                []\r\n\r\ncontiguous                       0.01%            12.000us         0.01%            12.000us         12.000us         0.00%            7.867us          7.867us          1                []\r\n\r\ncontiguous                       0.00%            8.900us          0.00%            8.900us          8.900us          0.01%            41.566us         41.566us         1                []\r\n\r\ncontiguous                       0.00%            9.200us          0.00%            9.200us          9.200us          0.00%            14.531us         14.531us         1                []\r\n\r\ncontiguous                       0.00%            8.700us          0.00%            8.700us          8.700us          0.01%            34.816us         34.816us         1                []\r\n\r\ncontiguous                       0.00%            8.900us          0.00%            8.900us          8.900us          0.00%            15.359us         15.359us         1                []\r\n\r\ncudnn_batch_norm                 0.03%            62.000us         0.03%            62.000us         62.000us         0.05%            294.914us        294.914us        1                []\r\n\r\nrelu_                            0.01%            26.700us         0.01%            26.700us         26.700us         0.04%            226.113us        226.113us        1                []\r\n\r\nmax_pool2d                       0.03%            69.500us         0.03%            69.500us         69.500us         0.04%            264.191us        264.191us        1                []\r\n\r\nmax_pool2d_with_indices          0.03%            53.500us         0.03%            53.500us         53.500us         0.04%            231.332us        231.332us        1                []\r\n\r\nconv2d                           0.06%            122.800us        0.06%            122.800us        122.800us        0.04%            232.254us        232.254us        1                []\r\n\r\nconvolution                      0.05%            108.700us        0.05%            108.700us        108.700us        0.03%            192.484us        192.484us        1                []\r\n\r\n_convolution                     0.04%            95.600us         0.04%            95.600us         95.600us         0.03%            174.305us        174.305us        1                []\r\n\r\ncontiguous                       0.01%            13.800us         0.01%            13.800us         13.800us         0.00%            16.387us         16.387us         1                []\r\n\r\ncudnn_convolution                0.03%            62.900us         0.03%            62.900us         62.900us         0.02%            116.730us        116.730us        1                []\r\n\r\nbatch_norm                       0.06%            137.400us        0.06%            137.400us        137.400us        0.05%            318.402us        318.402us        1                []\r\n\r\n_batch_norm_impl_index           0.06%            124.400us        0.06%            124.400us        124.400us        0.05%            299.648us        299.648us        1                []\r\n\r\ncontiguous                       0.01%            11.600us         0.01%            11.600us         11.600us         0.00%            9.660us          9.660us          1                []\r\n\r\ncontiguous                       0.01%            11.200us         0.01%            11.200us         11.200us         0.01%            41.504us         41.504us         1                []\r\n\r\ncontiguous                       0.00%            9.600us          0.00%            9.600us          9.600us          0.00%            15.359us         15.359us         1                []\r\n\r\ncontiguous                       0.00%            8.500us          0.00%            8.500us          8.500us          0.01%            65.664us         65.664us         1                []\r\n\r\ncontiguous                       0.01%            10.900us         0.01%            10.900us         10.900us         0.00%            15.230us         15.230us         1                []\r\n\r\ncudnn_batch_norm                 0.02%            52.700us         0.02%            52.700us         52.700us         0.02%            109.504us        109.504us        1                []\r\n\r\nrelu_                            0.02%            40.000us         0.02%            40.000us         40.000us         0.01%            63.969us         63.969us         1                []\r\n\r\nconv2d                           0.07%            141.300us        0.07%            141.300us        141.300us        0.08%            473.887us        473.887us        1                []\r\n\r\nconvolution                      0.06%            129.100us        0.06%            129.100us        129.100us        0.07%            437.277us        437.277us        1                []\r\n\r\n_convolution                     0.05%            117.300us        0.05%            117.300us        117.300us        0.07%            418.082us        418.082us        1                []\r\n\r\ncontiguous                       0.01%            16.300us         0.01%            16.300us         16.300us         0.00%            28.676us         28.676us         1                []\r\n\r\ncudnn_convolution                0.04%            81.800us         0.04%            81.800us         81.800us         0.06%            355.391us        355.391us        1                []\r\n\r\nbatch_norm                       0.07%            149.300us        0.07%            149.300us        149.300us        0.05%            298.688us        298.688us        1                []\r\n\r\n_batch_norm_impl_index           0.06%            136.500us        0.06%            136.500us        136.500us        0.05%            278.531us        278.531us        1                []\r\n\r\ncontiguous                       0.01%            11.700us         0.01%            11.700us         11.700us         0.00%            9.027us          9.027us          1                []\r\n\r\ncontiguous                       0.00%            10.100us         0.00%            10.100us         10.100us         0.01%            44.156us         44.156us         1                []\r\n\r\ncontiguous                       0.00%            9.500us          0.00%            9.500us          9.500us          0.00%            14.016us         14.016us         1                []\r\n\r\ncontiguous                       0.00%            9.500us          0.00%            9.500us          9.500us          0.01%            36.125us         36.125us         1                []\r\n\r\ncontiguous                       0.01%            28.100us         0.01%            28.100us         28.100us         0.00%            14.750us         14.750us         1                []\r\n\r\ncudnn_batch_norm                 0.02%            48.700us         0.02%            48.700us         48.700us         0.02%            114.879us        114.879us        1                []\r\n\r\nrelu_                            0.01%            22.200us         0.01%            22.200us         22.200us         0.01%            63.488us         63.488us         1                []\r\n\r\nconv2d                           0.05%            108.400us        0.05%            108.400us        108.400us        0.06%            378.590us        378.590us        1                []\r\n\r\nconvolution                      0.05%            97.000us         0.05%            97.000us         97.000us         0.06%            342.559us        342.559us        1                []\r\n\r\n_convolution                     0.04%            85.100us         0.04%            85.100us         85.100us         0.05%            323.582us        323.582us        1                []\r\n\r\ncontiguous                       0.01%            12.900us         0.01%            12.900us         12.900us         0.00%            28.254us         28.254us         1                []\r\n\r\ncudnn_convolution                0.03%            55.800us         0.03%            55.800us         55.800us         0.04%            252.383us        252.383us        1                []\r\n\r\nbatch_norm                       0.06%            128.200us        0.06%            128.200us        128.200us        0.16%            938.336us        938.336us        1                []\r\n\r\n_batch_norm_impl_index           0.05%            115.800us        0.05%            115.800us        115.800us        0.16%            919.582us        919.582us        1                []\r\n\r\ncontiguous                       0.01%            11.800us         0.01%            11.800us         11.800us         0.00%            9.504us          9.504us          1                []\r\n\r\ncontiguous                       0.00%            9.000us          0.00%            9.000us          9.000us          0.01%            39.262us         39.262us         1                []\r\n\r\ncontiguous                       0.00%            9.100us          0.00%            9.100us          9.100us          0.00%            14.336us         14.336us         1                []\r\n\r\ncontiguous                       0.00%            9.100us          0.00%            9.100us          9.100us          0.09%            503.805us        503.805us        1                []\r\n\r\ncontiguous                       0.00%            8.900us          0.00%            8.900us          8.900us          0.00%            15.391us         15.391us         1                []\r\n\r\ncudnn_batch_norm                 0.02%            48.200us         0.02%            48.200us         48.200us         0.05%            281.664us        281.664us        1                []\r\n\r\nconv2d                           0.05%            104.700us        0.05%            104.700us        104.700us        0.06%            383.297us        383.297us        1                []\r\n\r\nconvolution                      0.04%            93.800us         0.04%            93.800us         93.800us         0.06%            346.113us        346.113us        1                []\r\n\r\n_convolution                     0.04%            82.200us         0.04%            82.200us         82.200us         0.06%            327.777us        327.777us        1                []\r\n\r\ncontiguous                       0.01%            12.000us         0.01%            12.000us         12.000us         0.00%            29.277us         29.277us         1                []\r\n\r\ncudnn_convolution                0.03%            55.000us         0.03%            55.000us         55.000us         0.04%            253.027us        253.027us        1                []\r\n\r\nbatch_norm                       0.06%            129.100us        0.06%            129.100us        129.100us        0.08%            466.945us        466.945us        1                []\r\n\r\n_batch_norm_impl_index           0.05%            116.400us        0.05%            116.400us        116.400us        0.08%            446.340us        446.340us        1                []\r\n\r\ncontiguous                       0.01%            11.600us         0.01%            11.600us         11.600us         0.00%            8.668us          8.668us          1                []\r\n\r\ncontiguous                       0.00%            9.700us          0.00%            9.700us          9.700us          0.01%            39.137us         39.137us         1                []\r\n\r\ncontiguous                       0.00%            9.400us          0.00%            9.400us          9.400us          0.00%            14.496us         14.496us         1                []\r\n\r\ncontiguous                       0.00%            9.300us          0.00%            9.300us          9.300us          0.01%            35.168us         35.168us         1                []\r\n\r\ncontiguous                       0.00%            8.100us          0.00%            8.100us          8.100us          0.00%            14.625us         14.625us         1                []\r\n\r\ncudnn_batch_norm                 0.02%            48.800us         0.02%            48.800us         48.800us         0.05%            280.832us        280.832us        1                []\r\n\r\nadd_                             0.01%            21.700us         0.01%            21.700us         21.700us         0.05%            318.305us        318.305us        1                []\r\n\r\nrelu_                            0.01%            20.700us         0.01%            20.700us         20.700us         0.04%            229.379us        229.379us        1                []\r\n\r\nconv2d                           0.05%            107.700us        0.05%            107.700us        107.700us        0.06%            338.812us        338.812us        1                []\r\n\r\nconvolution                      0.04%            95.600us         0.04%            95.600us         95.600us         0.05%            304.641us        304.641us        1                []\r\n\r\n_convolution                     0.04%            84.700us         0.04%            84.700us         84.700us         0.05%            286.270us        286.270us        1                []\r\n\r\ncontiguous                       0.01%            13.500us         0.01%            13.500us         13.500us         0.00%            28.578us         28.578us         1                []\r\n\r\ncudnn_convolution                0.03%            54.800us         0.03%            54.800us         54.800us         0.04%            212.191us        212.191us        1                []\r\n\r\nbatch_norm                       0.06%            127.500us        0.06%            127.500us        127.500us        0.05%            290.977us        290.977us        1                []\r\n\r\n_batch_norm_impl_index           0.05%            116.200us        0.05%            116.200us        116.200us        0.05%            272.031us        272.031us        1                []\r\n\r\ncontiguous                       0.01%            11.800us         0.01%            11.800us         11.800us         0.00%            9.664us          9.664us          1                []\r\n\r\ncontiguous                       0.00%            9.000us          0.00%            9.000us          9.000us          0.01%            36.801us         36.801us         1                []\r\n\r\ncontiguous                       0.00%            8.900us          0.00%            8.900us          8.900us          0.00%            14.531us         14.531us         1                []\r\n\r\ncontiguous                       0.00%            8.700us          0.00%            8.700us          8.700us          0.01%            38.879us         38.879us         1                []\r\n\r\ncontiguous                       0.00%            9.100us          0.00%            9.100us          9.100us          0.00%            14.594us         14.594us         1                []\r\n\r\ncudnn_batch_norm                 0.02%            49.400us         0.02%            49.400us         49.400us         0.02%            106.016us        106.016us        1                []\r\n\r\nrelu_                            0.01%            22.300us         0.01%            22.300us         22.300us         0.01%            71.680us         71.680us         1                []\r\n\r\nconv2d                           0.05%            113.900us        0.05%            113.900us        113.900us        0.08%            471.969us        471.969us        1                []\r\n\r\nconvolution                      0.05%            103.100us        0.05%            103.100us        103.100us        0.07%            436.219us        436.219us        1                []\r\n\r\n_convolution                     0.04%            92.600us         0.04%            92.600us         92.600us         0.07%            415.840us        415.840us        1                []\r\n\r\ncontiguous                       0.01%            11.200us         0.01%            11.200us         11.200us         0.00%            26.367us         26.367us         1                []\r\n\r\ncudnn_convolution                0.03%            66.000us         0.03%            66.000us         66.000us         0.06%            354.305us        354.305us        1                []\r\n\r\nbatch_norm                       0.06%            126.800us        0.06%            126.800us        126.800us        0.05%            290.754us        290.754us        1                []\r\n\r\n_batch_norm_impl_index           0.05%            114.600us        0.05%            114.600us        114.600us        0.05%            269.090us        269.090us        1                []\r\n\r\ncontiguous                       0.01%            11.600us         0.01%            11.600us         11.600us         0.00%            9.848us          9.848us          1                []\r\n\r\n-------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------\r\nSelf CPU time total: 213.563ms\r\nCUDA time total: 592.358ms\r\n```\r\nand for TITAN RTX:\r\n\r\n```\r\n---------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------\r\nName                         Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     CUDA total %     CUDA total       CUDA time avg    Number of Calls  Input Shapes\r\n\r\n---------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------\r\nto                           0.00%            25.375us         0.00%            25.375us         25.375us         0.00%            2.910us          2.910us          1                []\r\n\r\nto                           0.56%            4.238ms          0.56%            4.238ms          4.238ms          0.28%            4.259ms          4.259ms          1                []\r\n\r\nempty                        0.01%            59.625us         0.01%            59.625us         59.625us         0.10%            1.493ms          1.493ms          1                []\r\n\r\nsub                          0.02%            141.208us        0.02%            141.208us        141.208us        0.03%            448.801us        448.801us        1                []\r\n\r\ndiv                          0.01%            67.916us         0.01%            67.916us         67.916us         0.04%            642.594us        642.594us        1                []\r\n\r\nunsqueeze                    0.00%            19.917us         0.00%            19.917us         19.917us         0.00%            2.496us          2.496us          1                []\r\n\r\nconv2d                       0.06%            477.917us        0.06%            477.917us        477.917us        0.12%            1.827ms          1.827ms          1                []\r\n\r\nconvolution                  0.06%            431.625us        0.06%            431.625us        431.625us        0.11%            1.656ms          1.656ms          1                []\r\n\r\n_convolution                 0.04%            290.125us        0.04%            290.125us        290.125us        0.09%            1.326ms          1.326ms          1                []\r\n\r\ncontiguous                   0.00%            20.000us         0.00%            20.000us         20.000us         0.03%            398.238us        398.238us        1                []\r\n\r\ncudnn_convolution            0.03%            194.833us        0.03%            194.833us        194.833us        0.05%            723.902us        723.902us        1                []\r\n\r\nbatch_norm                   0.09%            664.875us        0.09%            664.875us        664.875us        0.20%            3.006ms          3.006ms          1                []\r\n\r\n_batch_norm_impl_index       0.08%            592.292us        0.08%            592.292us        592.292us        0.18%            2.676ms          2.676ms          1                []\r\n\r\ncontiguous                   0.00%            34.958us         0.00%            34.958us         34.958us         0.03%            446.047us        446.047us        1                []\r\n\r\ncontiguous                   0.00%            25.041us         0.00%            25.041us         25.041us         0.02%            231.422us        231.422us        1                []\r\n\r\ncontiguous                   0.00%            16.375us         0.00%            16.375us         16.375us         0.01%            148.223us        148.223us        1                []\r\n\r\ncontiguous                   0.01%            45.292us         0.01%            45.292us         45.292us         0.02%            373.477us        373.477us        1                []\r\n\r\ncontiguous                   0.01%            79.292us         0.01%            79.292us         79.292us         0.01%            181.309us        181.309us        1                []\r\n\r\ncudnn_batch_norm             0.02%            140.666us        0.02%            140.666us        140.666us        0.05%            755.199us        755.199us        1                []\r\n\r\nrelu_                        0.01%            66.250us         0.01%            66.250us         66.250us         0.01%            163.395us        163.395us        1                []\r\n\r\nmax_pool2d                   0.03%            192.458us        0.03%            192.458us        192.458us        0.06%            953.824us        953.824us        1                []\r\n\r\nmax_pool2d_with_indices      0.02%            135.875us        0.02%            135.875us        135.875us        0.03%            520.383us        520.383us        1                []\r\n\r\nconv2d                       0.06%            441.666us        0.06%            441.666us        441.666us        0.12%            1.899ms          1.899ms          1                []\r\n\r\nconvolution                  0.05%            400.291us        0.05%            400.291us        400.291us        0.11%            1.645ms          1.645ms          1                []\r\n\r\n_convolution                 0.04%            329.000us        0.04%            329.000us        329.000us        0.09%            1.421ms          1.421ms          1                []\r\n\r\ncontiguous                   0.01%            62.625us         0.01%            62.625us         62.625us         0.01%            214.309us        214.309us        1                []\r\n\r\ncudnn_convolution            0.03%            211.542us        0.03%            211.542us        211.542us        0.04%            637.664us        637.664us        1                []\r\n\r\nbatch_norm                   0.10%            773.042us        0.10%            773.042us        773.042us        0.15%            2.366ms          2.366ms          1                []\r\n\r\n_batch_norm_impl_index       0.09%            700.083us        0.09%            700.083us        700.083us        0.14%            2.136ms          2.136ms          1                []\r\n\r\ncontiguous                   0.00%            34.000us         0.00%            34.000us         34.000us         0.01%            176.125us        176.125us        1                []\r\n\r\ncontiguous                   0.01%            86.291us         0.01%            86.291us         86.291us         0.02%            364.000us        364.000us        1                []\r\n\r\ncontiguous                   0.01%            46.292us         0.01%            46.292us         46.292us         0.01%            167.547us        167.547us        1                []\r\n\r\ncontiguous                   0.00%            29.958us         0.00%            29.958us         29.958us         0.03%            432.320us        432.320us        1                []\r\n\r\ncontiguous                   0.00%            25.000us         0.00%            25.000us         25.000us         0.01%            129.121us        129.121us        1                []\r\n\r\ncudnn_batch_norm             0.02%            164.541us        0.02%            164.541us        164.541us        0.03%            493.570us        493.570us        1                []\r\n\r\nrelu_                        0.01%            71.292us         0.01%            71.292us         71.292us         0.03%            489.469us        489.469us        1                []\r\n\r\nconv2d                       0.07%            561.167us        0.07%            561.167us        561.167us        0.12%            1.872ms          1.872ms          1                []\r\n\r\nconvolution                  0.06%            461.292us        0.06%            461.292us        461.292us        0.10%            1.460ms          1.460ms          1                []\r\n\r\n_convolution                 0.04%            312.375us        0.04%            312.375us        312.375us        0.08%            1.257ms          1.257ms          1                []\r\n\r\ncontiguous                   0.00%            20.333us         0.00%            20.333us         20.333us         0.02%            359.012us        359.012us        1                []\r\n\r\ncudnn_convolution            0.02%            184.500us        0.02%            184.500us        184.500us        0.04%            542.113us        542.113us        1                []\r\n\r\nbatch_norm                   0.10%            735.417us        0.10%            735.417us        735.417us        0.18%            2.713ms          2.713ms          1                []\r\n\r\n_batch_norm_impl_index       0.09%            678.833us        0.09%            678.833us        678.833us        0.15%            2.297ms          2.297ms          1                []\r\n\r\ncontiguous                   0.02%            113.916us        0.02%            113.916us        113.916us        0.01%            184.164us        184.164us        1                []\r\n\r\ncontiguous                   0.00%            31.292us         0.00%            31.292us         31.292us         0.02%            334.496us        334.496us        1                []\r\n\r\ncontiguous                   0.01%            59.958us         0.01%            59.958us         59.958us         0.01%            192.512us        192.512us        1                []\r\n\r\ncontiguous                   0.01%            48.583us         0.01%            48.583us         48.583us         0.03%            489.469us        489.469us        1                []\r\n\r\ncontiguous                   0.00%            34.000us         0.00%            34.000us         34.000us         0.02%            253.797us        253.797us        1                []\r\n\r\ncudnn_batch_norm             0.01%            84.250us         0.01%            84.250us         84.250us         0.05%            693.406us        693.406us        1                []\r\n\r\nrelu_                        0.01%            82.958us         0.01%            82.958us         82.958us         0.01%            149.566us        149.566us        1                []\r\n\r\nconv2d                       0.04%            316.042us        0.04%            316.042us        316.042us        0.12%            1.810ms          1.810ms          1                []\r\n\r\nconvolution                  0.03%            231.125us        0.03%            231.125us        231.125us        0.11%            1.609ms          1.609ms          1                []\r\n\r\n_convolution                 0.02%            169.792us        0.02%            169.792us        169.792us        0.09%            1.319ms          1.319ms          1                []\r\n\r\ncontiguous                   0.00%            24.959us         0.00%            24.959us         24.959us         0.03%            402.141us        402.141us        1                []\r\n\r\ncudnn_convolution            0.01%            69.958us         0.01%            69.958us         69.958us         0.03%            387.078us        387.078us        1                []\r\n\r\nbatch_norm                   0.07%            548.917us        0.07%            548.917us        548.917us        0.20%            3.110ms          3.110ms          1                []\r\n\r\n_batch_norm_impl_index       0.07%            518.917us        0.07%            518.917us        518.917us        0.18%            2.728ms          2.728ms          1                []\r\n\r\ncontiguous                   0.01%            60.291us         0.01%            60.291us         60.291us         0.03%            515.742us        515.742us        1                []\r\n\r\ncontiguous                   0.01%            90.583us         0.01%            90.583us         90.583us         0.02%            327.617us        327.617us        1                []\r\n\r\ncontiguous                   0.00%            16.333us         0.00%            16.333us         16.333us         0.03%            382.055us        382.055us        1                []\r\n\r\ncontiguous                   0.00%            24.959us         0.00%            24.959us         24.959us         0.02%            256.477us        256.477us        1                []\r\n\r\ncontiguous                   0.00%            30.000us         0.00%            30.000us         30.000us         0.02%            229.367us        229.367us        1                []\r\n\r\ncudnn_batch_norm             0.01%            101.292us        0.01%            101.292us        101.292us        0.03%            417.789us        417.789us        1                []\r\n\r\nconv2d                       0.05%            397.084us        0.05%            397.084us        397.084us        0.11%            1.615ms          1.615ms          1                []\r\n\r\nconvolution                  0.05%            359.459us        0.05%            359.459us        359.459us        0.08%            1.224ms          1.224ms          1                []\r\n\r\n_convolution                 0.04%            285.459us        0.04%            285.459us        285.459us        0.07%            999.172us        999.172us        1                []\r\n\r\ncontiguous                   0.01%            90.250us         0.01%            90.250us         90.250us         0.02%            235.938us        235.938us        1                []\r\n\r\ncudnn_convolution            0.01%            82.625us         0.01%            82.625us         82.625us         0.04%            577.281us        577.281us        1                []\r\n\r\nbatch_norm                   0.11%            799.334us        0.11%            799.334us        799.334us        0.21%            3.154ms          3.154ms          1                []\r\n\r\n_batch_norm_impl_index       0.09%            710.417us        0.09%            710.417us        710.417us        0.20%            2.988ms          2.988ms          1                []\r\n\r\ncontiguous                   0.02%            154.250us        0.02%            154.250us        154.250us        0.02%            376.828us        376.828us        1                []\r\n\r\ncontiguous                   0.00%            19.959us         0.00%            19.959us         19.959us         0.02%            260.000us        260.000us        1                []\r\n\r\ncontiguous                   0.01%            40.000us         0.01%            40.000us         40.000us         0.04%            579.070us        579.070us        1                []\r\n\r\ncontiguous                   0.00%            26.292us         0.00%            26.292us         26.292us         0.02%            345.945us        345.945us        1                []\r\n\r\ncontiguous                   0.00%            34.958us         0.00%            34.958us         34.958us         0.02%            257.508us        257.508us        1                []\r\n\r\ncudnn_batch_norm             0.01%            68.958us         0.01%            68.958us         68.958us         0.04%            639.430us        639.430us        1                []\r\n\r\nadd_                         0.01%            39.916us         0.01%            39.916us         39.916us         0.03%            409.945us        409.945us        1                []\r\n\r\nrelu_                        0.01%            52.292us         0.01%            52.292us         52.292us         0.02%            364.023us        364.023us        1                []\r\n\r\nconv2d                       0.08%            587.125us        0.08%            587.125us        587.125us        0.10%            1.552ms          1.552ms          1                []\r\n\r\nconvolution                  0.07%            523.208us        0.07%            523.208us        523.208us        0.08%            1.280ms          1.280ms          1                []\r\n\r\n_convolution                 0.06%            416.625us        0.06%            416.625us        416.625us        0.07%            1.098ms          1.098ms          1                []\r\n\r\ncontiguous                   0.01%            45.334us         0.01%            45.334us         45.334us         0.01%            213.594us        213.594us        1                []\r\n\r\ncudnn_convolution            0.03%            243.458us        0.03%            243.458us        243.458us        0.03%            499.328us        499.328us        1                []\r\n\r\nbatch_norm                   0.13%            966.208us        0.13%            966.208us        966.208us        0.23%            3.443ms          3.443ms          1                []\r\n\r\n_batch_norm_impl_index       0.12%            872.292us        0.12%            872.292us        872.292us        0.21%            3.229ms          3.229ms          1                []\r\n\r\ncontiguous                   0.01%            45.292us         0.01%            45.292us         45.292us         0.06%            864.508us        864.508us        1                []\r\n\r\ncontiguous                   0.01%            84.250us         0.01%            84.250us         84.250us         0.02%            329.438us        329.438us        1                []\r\n\r\ncontiguous                   0.01%            107.916us        0.01%            107.916us        107.916us        0.02%            337.922us        337.922us        1                []\r\n\r\ncontiguous                   0.02%            151.833us        0.02%            151.833us        151.833us        0.02%            234.305us        234.305us        1                []\r\n\r\ncontiguous                   0.01%            39.959us         0.01%            39.959us         39.959us         0.03%            430.078us        430.078us        1                []\r\n\r\ncudnn_batch_norm             0.02%            113.875us        0.02%            113.875us        113.875us        0.03%            530.914us        530.914us        1                []\r\n\r\nrelu_                        0.02%            123.792us        0.02%            123.792us        123.792us        0.01%            131.070us        131.070us        1                []\r\n\r\nconv2d                       0.06%            455.375us        0.06%            455.375us        455.375us        0.15%            2.238ms          2.238ms          1                []\r\n\r\nconvolution                  0.05%            359.083us        0.05%            359.083us        359.083us        0.13%            1.995ms          1.995ms          1                []\r\n\r\n_convolution                 0.04%            292.458us        0.04%            292.458us        292.458us        0.09%            1.411ms          1.411ms          1                []\r\n\r\ncontiguous                   0.01%            41.292us         0.01%            41.292us         41.292us         0.02%            361.891us        361.891us        1                []\r\n\r\ncudnn_convolution            0.01%            110.625us        0.01%            110.625us        110.625us        0.06%            846.078us        846.078us        1                []\r\n\r\nbatch_norm                   0.10%            740.459us        0.10%            740.459us        740.459us        0.15%            2.271ms          2.271ms          1                []\r\n\r\n_batch_norm_impl_index       0.09%            675.541us        0.09%            675.541us        675.541us        0.14%            2.148ms          2.148ms          1                []\r\n\r\ncontiguous                   0.01%            49.917us         0.01%            49.917us         49.917us         0.02%            262.750us        262.750us        1                []\r\n\r\n---------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------\r\nSelf CPU time total: 751.832ms\r\nCUDA time total: 1.527s\r\n```\r\n\r\n\r\n', 'Any progress on this?', ""This is a pretty old issue comparing performance using now old versions of pytorch and cudnn, tbh, I don't think it's going to be looked at in detail. If you have perf issues with current pytorch/cudnn versions, please file a new issue with a script that demonstrates performance problems. Don't forget to use torch.backends.cudnn.benchmark=True and use a few warmup iterations, and benchmark and profile over several tens of iterations. Also, the reproducer should be reflective of a real usecase, and not just try to find problematic configurations. Convolution search space is very wide, there always will be convolution parameters that are unoptimized for and give poor performance.\r\ncc @ptrblck. "", ""In my case, I am facing a 10 times increase in iteration time when I ported my code from Tensorflow to Pytorch. Upon profiling my code with both torch.utils.bottleneck.profiler and torch.cuda.profiler I observed that the issue I face is on the backward pass. More precisely on the update on the convolutions, even more precisely it is the following kernel call:\r\n```\r\nvoid cudnn::detail::wgrad_alg1_nd_float_engine<float, int=3, int=0, int=5, int=7, int=4, int=3, int=5, bool=1, bool=1>(int, int, int, float const *, int, cudnn::detail::wgrad_alg1_nd_float_engine<float, int=3, int=0, int=5, int=7, int=4, int=3, int=5, bool=1, bool=1>*, float const , kernel_gradNd_params, int, float, float, int, int, int*, kernel_gradNd_params)\r\n```\r\nI can't publicly release my code at the moment, as soon as my publication is accepted I will. But I can continuously reproduce it on my machine which has a TitanV and on a DGX1 which has a V100. If I use benchmark=True I get even worse performance than with benchmark=False and nothing in my network is dynamic in size."", ""I'm sorry, the information you've provided is not actionable. With proper benchmarking benchmark=True does not give worse performance, unless your input sizes change at each iteration. You can try using pyprof https://github.com/NVIDIA/apex/tree/master/apex/pyprof to identify problematic operation, but that's again subject to proper benchmarking, enough warm-up iterations, enough benchmarking iterations. "", 'Thank you for responding @ngimel. For me to give you the required data I have the following questions:\r\n- Do you recommend PyProf over torch.cuda.profiler + torch.bottleneck.profiler.emit_nvtx + NvProf?\r\n- Do you want me to run with only torch.backends.cudnn.benchmark=True or with both True and False?\r\n- How many warm-up iterations are enough?\r\n- How many profile iterations are enough?\r\n- What files do you want me to send to you?\r\n- Can I share my code with the contributors purely for reproducibility?', ""pyprof provides more detailed information than torch.autograd.profiler (e.g. torch.autograd.profiler does not record all the necessary shapes). If you are certain that your input sizes don't change every iteration, just running with benchmark=True is enough, otherwise, run with both benchmark=True and benchmark=False.  There should be say 20 warmup iterations (with a synchronization after them) and 100 benchmarking iterations, you can change those numbers if you see that your results are flaky. \r\nIdeal case for files you want to share is a small self-contained reproducer with no dependencies that runs on random data - that would allow whoever is working on the problem to make their own tweaks to runs and benchmarking and try say newer builds of pytorch or cudnn. If that's not possible, hopefully pyprof should provide you with enough information to identify where the slow wgrad kernel is coming from (one would need all the convolution parameters such as kernel size, padding, stride, dilation, and the input sizes). \r\nAs for sharing your code, since there are no formal NDA agreements here, it's up to you, but really, what we are interested in is a small snippet demonstrating poor performance. "", '@ngimel Thank you very much for the swift response! \r\n\r\nMy network is a particular implementation of the VQ-VAE 2 paper and besides the quantization process, it is fully convolutional with a standard input size of [3,1,192,256,192]. \r\n\r\nI observed that PyProf does not cover transpose convolutions which I use heavily in my network. Will that be a big problem?\r\n\r\nShould I post the links to the files here after I generate them?', ""Hm, then I'm surprised that cudnn.benchmark hurts - it's possible that it would not help, but it should not help. I've verified with pyprof author that transposed convolution is supported in a sense that it will capture all the relevant shapes and parameters, but they won't compute achieved flops/bandwidth. If some convolutions are egregiously slow though, just looking at the timing should be enough. Yes, please post the links to the files here (including sql file generated by pyprof)"", '@danieltudosiu\r\n\r\n(wrt PyProf): I didn\'t yet add bytes, flops calculation for transposed convolution. But all the other useful information e.g. the call stack (file name, line number), tensor shape, datatype and attributes (strides, padding, dilation), kernel duration etc. are captured. The information is visible in the intermediate human readable dictionary (every line of output corresponds to the same line in the dictionary). The final output is not polished, I agree. As a stop gap, if you can post sql file, I can help.\r\n\r\ne.g. if you had\r\n```python\r\nm = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2)).cuda()\r\ninput = torch.randn(20, 16, 50, 100).cuda()\r\noutput = m(input)\r\n```\r\nthe intermediate file will have the following information (Its uglier so I pretty printed here). Look at the information contained in `args`. Its signature is the same as `torch.nn.functional.conv_transpose2d`\r\n```bash\r\n{\r\n\'kShortName\': \'cudnn::detail::dgrad2d_alg1_1\',\r\n\'kDuration\': 1849853, \r\n\'layer\': [],\r\n\'trace\': [\'conv2d_transpose.py:17\'],\r\n\'reprMarkers\': [],\r\n\'marker\': [""{\'mod\': \'torch.nn.functional\', \'op\': \'conv_transpose2d\',\r\n   \'args\': [{\'name\': \'\', \'type\': \'tensor\', \'shape\': (20, 16, 50, 100), \'dtype\': \'float32\'},\r\n              {\'name\': \'\', \'type\': \'tensor\', \'shape\': (16, 33, 3, 5), \'dtype\': \'float32\'},\r\n              {\'name\': \'\', \'type\': \'tensor\', \'shape\': (33,), \'dtype\': \'float32\'},\r\n              {\'name\': \'\', \'type\': \'tuple\', \'value\': (2, 1)},\r\n              {\'name\': \'\', \'type\': \'tuple\', \'value\': (4, 2)},\r\n              {\'name\': \'\', \'type\': \'tuple\', \'value\': (0, 0)},\r\n              {\'name\': \'\', \'type\': \'int\', \'value\': 1},\r\n              {\'name\': \'\', \'type\': \'tuple\', \'value\': (1, 1)}]}""],\r\n\'seqMarker\': [],\r\n\'seqId\': [],\r\n\'subSeqId\': 0,\r\n\'altSeqId\': [],\r\n\'dir\': \'fprop\',\r\n\'mod\': [\'torch.nn.functional\'],\r\n\'op\': [\'conv_transpose2d\'],\r\n\'tid\': 2833028928, \'device\': 0, \'stream\': 7, \'grid\': (9, 20, 1), \'block\': (16, 32, 1),\r\n\'kLongName\': \'void cudnn::detail::dgrad2d_alg1_1<float, 0, 6, 7, 5, 4, 5, true, true>\r\n(int, int, int, float const*, int, float const*, int, float*, kernel_grad_params, int, int, float, int, int)\'}\r\n```', '@ngimel @adityaiitb Thank you for your support. \r\n\r\nI am working now on installing Apex and running a full-blown profiling. \r\n\r\nWill come back to you with more details tomorrow morning London time.', ""Cool! You don't need to install apex with cpp extensions, for profiling just python installation will do. "", '@ngimel I have made public my git repo and you can find it here https://github.com/danieltudosiu/nmpevqvae\r\n\r\nIn it you can find the net.sql.gz which is the profiling that I have done.\r\n\r\nTo generate it I used the profiling.py script and the command line `nvprof -f -o net.sql --profile-from-start off -- python3 profiling.py`. \r\n\r\nThe python environment that I use can be found in the requirements.txt (besides apex which is just for profiling). \r\n\r\nThe profiling has been run inside a Docker container that inherits from `10.1-cudnn7-devel-ubuntu18.04` [NVIDIA image](https://hub.docker.com/layers/nvidia/cuda/10.1-cudnn7-devel-ubuntu18.04/images/sha256-557de4ba2cb674029ffb602bed8f748d44d59bb7db9daa746ea72a102406d3ec?context=explore) the Dockerfile can be found in my repo. \r\n\r\nThe hardware used was a V100 from a DGX1 server.\r\n\r\nPlease let me know if I can help in any other way.', 'A couple of things:\r\n\r\n1. I looked at the SQL file. It does not have a single GPU kernel. Looks like for some reason your application ran on the CPU or for some reason the profiler did not capture any GPU activity.\r\n\r\n2. I tried running your code but couldn\'t because there is a hard coded path ""/raid/danieltudosiu/datasets/neuro_morphology/healthy/train_192"". If you can modify that to feed random data, that would help.\r\n\r\n3. The SQL file is big because NVprof captured a lot of NVTX annotations as those always happen on the CPU (which is fine). I did see 10 calls to `conv_transpose3d`. Attached is the file which has all the parameters for each of those calls.\r\n[conv_transpose3d.txt](https://github.com/pytorch/pytorch/files/4360807/conv_transpose3d.txt)\r\n\r\nThe command I used to look for the convolutions in the SQL file was\r\n`sqlite3 net.sql ""select * from StringTable"" | grep -i conv_transpose3d | grep functional`', '@adityaiitb Interesting! \r\n\r\nI will modify the script now and push it to git. \r\n\r\n10 calls to conv_transpose3d sounds about right. \r\n\r\nI need to go and move my workstation from office to home covid19 style. Let me know if you need anything else. \r\n\r\nThere are NVTX annotation for each nn.Module that I create so I could debug it.', '@adityaiitb @ngimel \r\n\r\nI modified the profiling script to be independent of my data but respect the data shape. \r\n\r\nI did two more profilings one that tried to capture between start and stop called net_independent.sql.gz and another one which captured everything and in this one I was able to pars and you find it as net_independent_all.dict.gz since the sql was too big. \r\n\r\nLet me know if I can be of any further help!', '@adityaiitb \r\n\r\nI did some personal parsing of the dict hoping to find something useful, but the most useful thing I found is this but I can\'t make heads or tails of it.\r\n\r\n```\r\n                                                             min      mean           std       max      total   count\r\nkShortName                                                                                                           \r\ncudnn::detail::wgrad_alg1_nd_float_engine           1.622400e-05  0.012991  2.407226e-02  0.086074  70.593751    5434\r\ncudnn::detail::implicit_convolveNd_sgemm            3.795100e-05  0.005770  8.815605e-03  0.033361  23.240455    4028\r\nvolta_scudnn_128x64_stridedB_splitK_small_nn_v1     3.494400e-05  0.004681  1.163572e-02  0.046304  21.624084    4620\r\ncudnn::detail::convolveNd_wgrad_engine              1.369600e-05  0.009364  1.378996e-01  2.297072  20.919318    2234\r\ncudnn::detail::convolveNd_dgrad_float_engine        1.257600e-05  0.002649  5.326334e-03  0.046430  14.804468    5588\r\nelementwise_kernel                                  9.280000e-07  0.000041  1.653503e-04  0.001723  11.703452  283920\r\navg_pool3d_cuda_update_output                       3.328000e-06  0.000666  1.345273e-03  0.005348   3.355386    5040\r\navg_pool3d_single_backward_out_frame_stride1        3.104000e-06  0.000558  1.115472e-03  0.003855   2.812430    5040\r\nreduce_kernel                                       1.824000e-06  0.000083  1.556771e-04  0.000613   1.691088   20280\r\nsetTensor5d_kernel                                  1.248000e-06  0.000229  4.797258e-04  0.001454   1.279915    5588\r\nkernelPointwiseApply3                               1.408000e-06  0.000251  5.185644e-04  0.001856   1.264417    5040\r\nvolta_scudnn_128x32_stridedB_splitK_xregs_large_nn  4.360067e-02  0.118861  6.979165e-02  0.176072   0.832024       7\r\nkernelPointwiseApply2                               1.120000e-06  0.000139  3.165819e-04  0.001157   0.816679    5880\r\ncudnn::detail::dgrad_alg1_nd_float_engine           3.263350e-04  0.047277  1.015612e-01  0.385441   0.756425      16\r\ncudnn::gemm::setOutputKernel                        9.280000e-07  0.000131  3.467400e-04  0.001468   0.751137    5720\r\nvolta_gcgemm_32x32_tn                               2.160000e-05  0.000025  2.614317e-05  0.002271   0.418831   16426\r\nvolta_scudnn_128x128_stridedB_splitK_small_nn_v1    7.689500e-05  0.000220  8.117861e-05  0.000495   0.238825    1088\r\nfft3d_r2c_16x16x16                                  4.224000e-06  0.000006  1.294814e-05  0.001555   0.198598   31358\r\nfft3d_c2r_16x16x16                                  4.896000e-06  0.000006  1.387327e-06  0.000041   0.188108   31322\r\ncudnn::gemm::computeOffsetsKernel                   1.216000e-06  0.000031  8.746352e-05  0.000370   0.174604    5692\r\nsgemm_largek_lds64                                  1.195738e-03  0.001297  3.165635e-05  0.001402   0.155656     120\r\ntranspose_readWrite_alignment_kernel                1.503000e-06  0.000002  1.791118e-05  0.003037   0.144902   62682\r\nvolta_sgemm_32x32_sliced1x4_nn                      1.225600e-05  0.000269  3.576056e-04  0.000811   0.096964     360\r\ngemv2T_kernel_val                                   6.335000e-06  0.000006  2.950772e-07  0.000016   0.095937   14896\r\nvolta_scudnn_128x64_stridedB_splitK_medium_nn_v1    1.886720e-02  0.019069  1.830427e-04  0.019249   0.095347       5\r\nvolta_sgemm_128x32_tn                               1.484800e-05  0.000283  2.680396e-04  0.000553   0.067963     240\r\nTHCudaTensor_scatterFillKernel                      2.272000e-06  0.000076  1.031451e-04  0.000230   0.027233     360\r\ncudnn::gemm::computeBOffsetsKernel                  9.590000e-07  0.000001  2.185772e-07  0.000002   0.006775    5720\r\nvolta_sgemm_32x32_sliced1x4_nt                      1.078400e-05  0.000028  1.699259e-05  0.000057   0.006748     240\r\nvolta_sgemm_32x32_sliced1x4_tn                      9.055000e-06  0.000009  2.104289e-07  0.000011   0.001140     120\r\ncudnn::gemm::computeWgradOffsetsKernel              1.696000e-06  0.000015  2.410338e-05  0.000072   0.000415      28\r\nscal_kernel                                         1.504000e-06  0.000002  9.983884e-08  0.000002   0.000191     120\r\n```\r\n\r\nAlso, the inbuilt parser crashes due to unusual return of layers (VectorQuantizerEMA, Quantization, Encoder, VectorQuantizedVAE). \r\n\r\nWhat would you recommend that I do next?\r\n\r\nI found out that PyTorch ""they ship with their own CUDA, cudnn etc."" as per @ptrblck comments. Does this mean that PyTorch is agnostic to the installed CUDA, CUDNN and Drivers?', '> I found out that PyTorch ""they ship with their own CUDA, cudnn etc."" as per @ptrblck comments. Does this mean that PyTorch is agnostic to the installed CUDA, CUDNN and Drivers?\r\n\r\nIf you installed PyTorch from binaries (pip or conda), then yes. It will be agnostic to system\'s CUDA/CUDNN', '@soumith yes I pip installed it. Then my problem is purely a PyTorch one? ', '@ngimel @adityaiitb\r\n\r\nI found out the reason. In my experiment.py I was missing the line of code `torch.backends.cudnn.benchmark = True`. ', '@danieltudosiu it kicks off an internal auto-tuner in CuDNN which picks the best convolution codepath manually after benchmarking every single available codepath (algorithm) manually for each input shape', '@danieltudosiu @ngimel \r\n\r\nI have quite a few updates.\r\n\r\n1. The profile context manager captures only the current device. That\'s why I did not observe any GPU kernels when I ran your code. There is a very small fix as follows (thanks to Natalia).\r\n\r\n```python\r\ntorch.cuda.set_device(1)\r\ndevice=""cuda""  # instead of ""cuda:1""\r\n```\r\n\r\n2. The PyProf in APEX is in a state of flux. For the time being can you run `parse.py` and `prof.py` using my repo at https://github.com/adityaiitb/pyprof2. To repeat, you can still use `from apex import pyprof` but for the remaining steps please use my repo.\r\n\r\n```sh\r\npyprof2/parse/parse.py net.sql > net.dict\r\npyprof2/prof/prof.py -w 150 net.dict\r\nOR\r\npyprof2/prof/prof.py --csv -c idx,dir,sub,mod,op,kernel,params,sil,tc,flops,bytes net.dict > net.csv\r\n```\r\n\r\n3. After warmup, I profiled 1 step of your code. Attached is the detailed csv file. Please change the extension to `.csv`. For transpose convolution, the output of `prof.py` shows the size of image and filter. All other arguments like stride, padding, dilation are in the net.dict file (on the same line). Sorry its still a WIP.\r\n[nmpevqvae.txt](https://github.com/pytorch/pytorch/files/4371381/nmpevqvae.txt)\r\n\r\n4. The first thing that jumped out to me was that you are not using fp16. If you don\'t use fp16, you are not making use of the tensor cores on Turing and losing out on a lot of performance for both compute kernels like convolution and also for streaming kernels like `c = a + b`. I think you can speed up your training by ~ 1.5x.']","['void cudnn::detail::convolveNd_wgrad_engine<float, int=3, int=512, int=6, int=5, int=3, int=3, int=3, bool=1>(int, int, int, float const *, int, cudnn::detail::convolveNd_wgrad_engine<float, int=3, int=512, int=6, int=5, int=3, int=3, int=3, bool=1>*, float const , kernel_gradNd_params, int, float, int)']","['nvprof python my_script.py', 'conda', 'pip']",1,0
82,pytorch,10681,closed,nn.DataParallel hangs with Pytorch 0.4.1 and CUDA 9.1.85 on TITAN V,"Hi guys,
I tested a simple example with nn.DataParallel() to use multiple GPUs, but got a hang.


It hangs when I try to forward the data to the model. nvidia-smi gives


I have tried the solution in [this](https://github.com/pytorch/pytorch/issues/1637#issuecomment-338268158), but it didn‚Äôt work.

I use

- CUDA 9.1.85
- Pytorch 0.4.1 (installed by pip)
- Python 2.7.13
- Debian 4.9.110-3+deb9u1 (2018-08-03) x86_64 GNU/Linux
- TITAN V cards

Any ideas to solve this issue? Or I should let NVIDIA‚Äôs folks see this issue?",,"['If I do \r\n`export NCCL_P2P_DISABLE=1`\r\nthe code can run without hangs. Then I thought it would work, and I ran another script (which is really what I need to run instead of this toy script). But after some epochs, it hung again. I killed that process, and tried to re-ran that script, it hung from the beginning, and the utility status of all GPUs is 0%.', 'Actually, this issue comes up after the upgrading of our system. It upgraded from Debian kernel 4.9.0-6-amd64 to 4.9.0-7-amd64, and cuda 8 to cuda 9.\r\n\r\nBefore the upgrading, we have\r\n- CUDA 8.0\r\n- Pytorch 0.3.0 post 4\r\n- Installed by pip install cu90/torch-0.3.0.post4-cp27-cp27mu-linux_x86_64.whl\r\n\r\nThough we installed pytorch compiled with cuda 9.0, it worked with our cuda 8 environment, and there was no hangs.', ""@liren2515  have you tried adding\r\n```\r\nimport multiprocessing as mp\r\nmp.set_start_method('spawn')\r\n```"", ""@ailzhang I got \r\n`AttributeError: 'module' object has no attribute 'set_start_method'`\r\nI use python 2.7"", '@liren2515 Could you give us a complete script to repro? Does it happen 100% or randomly? Does switching to py3 help? ', ""Here is the [repro](https://github.com/liren2515/test.git). The hang happens 100%. It's not my personal computer, so I don't have the root to install py3. I'll try to compile py3 locally tomorrow."", ""@ailzhang Here is the [repro](https://github.com/liren2515/test.git). The hang happens 100%. Switching to py3 doesn't help, and the following way doesn't help.\r\n```\r\nimport multiprocessing as mp\r\nmp.set_start_method('spawn')\r\n```\r\n"", '@ailzhang Hi, did you replicate the issue? Are there any advise? Thanks.', 'Hi @liren2515 , sorry for the late reply, I tried your repo, although I got OOM on backward step, I can get through the forward() pass with two gpus successfully. \r\n\r\nPlease let me know if you have further questions. ', '@ailzhang thanks for your replying. Disabling ACS solved this issue.']","['\r\nepochs  = 2000\r\nlr = 1e-3\r\nmomentum = 0\r\nw_decay = 1e-5\r\n\r\ntrain_data = torch.randn(288,2)\r\ntrain_label = torch.zeros([288], dtype=torch.long)\r\n\r\nnum_gpu = list(range(torch.cuda.device_count()))\r\nmodel = nn.DataParallel(MyNet().cuda(0), device_ids = num_gpu)#.cuda()\r\ncriterion = nn.CrossEntropyLoss()\r\noptimizer = optim.Adam(model.parameters(), lr = lr, weight_decay = w_decay)\r\n\r\nprint ""Starting training""\r\n\r\nmodel.train()\r\nfor epoch in range(epochs):\r\n\toptimizer.zero_grad()\r\n\tinputs = Variable(train_data.cuda(0))\r\n\tlabels = Variable(train_label.cuda(0))\r\n\toutputs = model(inputs)\r\n\tloss = criterion(outputs, labels)\r\n\tloss.backward()\r\n\toptimizer.step()\r\n\tprint(""epoch{}, loss: {}"".format(epoch, loss.data.item()))\r\n', '\r\nThu Aug 16 09:56:56 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 390.67                 Driver Version: 390.67                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  TITAN V             Off  | 00000000:1B:00.0 Off |                  N/A |\r\n| 28%   39C    P8    25W / 250W |   1087MiB / 12066MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  TITAN V             Off  | 00000000:1C:00.0 Off |                  N/A |\r\n| 28%   41C    P2    39W / 250W |   1087MiB / 12066MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  TITAN V             Off  | 00000000:1D:00.0 Off |                  N/A |\r\n| 31%   45C    P2    41W / 250W |   1087MiB / 12066MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  TITAN V             Off  | 00000000:1E:00.0 Off |                  N/A |\r\n| 31%   45C    P2    40W / 250W |   1087MiB / 12066MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   4  TITAN V             Off  | 00000000:3D:00.0 Off |                  N/A |\r\n| 28%   39C    P2    38W / 250W |   1087MiB / 12066MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   5  TITAN V             Off  | 00000000:3E:00.0 Off |                  N/A |\r\n| 28%   41C    P2    40W / 250W |   1087MiB / 12066MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   6  TITAN V             Off  | 00000000:3F:00.0 Off |                  N/A |\r\n| 28%   40C    P2    38W / 250W |   1087MiB / 12066MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   7  TITAN V             Off  | 00000000:40:00.0 Off |                  N/A |\r\n| 31%   45C    P2    40W / 250W |   1087MiB / 12066MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   8  TITAN V             Off  | 00000000:41:00.0 Off |                  N/A |\r\n| 29%   43C    P2    41W / 250W |   1087MiB / 12066MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0    131331      C   python                                      1076MiB |\r\n|    1    131331      C   python                                      1076MiB |\r\n|    2    131331      C   python                                      1076MiB |\r\n|    3    131331      C   python                                      1076MiB |\r\n|    4    131331      C   python                                      1076MiB |\r\n|    5    131331      C   python                                      1076MiB |\r\n|    6    131331      C   python                                      1076MiB |\r\n|    7    131331      C   python                                      1076MiB |\r\n|    8    131331      C   python                                      1076MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n']",[],1,0
83,pytorch,17350,open,"torch.nn.CrossEntropyLoss with ""reduction"" sum/mean is not deterministic on segmentation outputs / labels","## üêõ Bug
torch.nn.CrossEntropyLoss doesn't output deterministic results on segmentation outputs / labels, when using reduction other than 'none'.
Happens only on GPU. CPU does give a consistent behavior. 

## To Reproduce

## Output


## Expected behavior
I believe the expected behavior of the reduction='sum' and 'mean' should be as consistent as the 'none' option (where I use numpy for reduction).

## Environment

## Additional context
Ran on amazon K80 instance (p2.xlarge).
I know it does seem like a very tiny error but the result of this is that two training sessions of my segmentation network (with identical parameters, initialization, order of image-batches, random seed, etc') doesn't produce identical results. That is problematic when I want to investigate a specific training session.
",module: cuda triaged,"[""Thank you for reporting this.\r\n\r\nThis is a case where atomicAdd bites that is not yet documented in [our non-determinism note](https://pytorch.org/docs/stable/notes/randomness.html).\r\n\r\nSpecifically, when using a reduction, this dispatches to [THCUNN's `cunn_SpatialClassNLLCriterion_updateOutput_kernel`, which contains the non-deterministic atomic add.](https://github.com/pytorch/pytorch/blob/master/aten/src/THCUNN/SpatialClassNLLCriterion.cu#L115)\r\n\r\nIt should be relatively easy to fix by not using blocks and atomic add - or by saying the savings of fusing don't matter enough in the grand scheme of things and just split the reduction off, which, to be honest, would make total sense to me.\r\n\r\nNote that we don't consider non-determinism a bug per se. #15359 might change that."", ""I'll move the NLLLoss to ATen and then follow up with a patch regarding determinism."", 'Thanks Thomas!', ""Turns out I'm not good at attending to this, sorry."", ""removing high priority label because no one is actively working on this and we aren't actively working on https://github.com/pytorch/pytorch/issues/15359.""]","[""\r\nimport numpy as np\r\nimport torch\r\n\r\noutputs = np.random.rand(16, 1, 256, 256)\r\noutputs = np.hstack((outputs, 1.0 - outputs))\r\ntargets = np.random.randint(2, size=(16, 256, 256))\r\n\r\nseed = 0\r\ntorch.backends.cudnn.deterministic = True\r\ntorch.backends.cudnn.benchmark = False\r\n\r\nfor reduction in ['none', 'sum', 'mean']:\r\n    print(reduction)\r\n\r\n    for i in range(10):\r\n        torch.manual_seed(seed)\r\n        np.random.seed(seed)\r\n\r\n        outputs_t, targets_t = torch.from_numpy(outputs), torch.from_numpy(targets)\r\n        outputs_t, targets_t = outputs_t.cuda(0), targets_t.cuda(0)\r\n\r\n        loss_fn = torch.nn.CrossEntropyLoss(reduction=reduction)\r\n        loss_fn = loss_fn.cuda(0)\r\n\r\n        loss = loss_fn(outputs_t, targets_t)\r\n        loss = loss.detach().cpu().numpy()\r\n        print(i, outputs.sum(), targets.sum(), outputs.mean(), targets.mean(), loss.sum(), loss.mean())\r\n"", '\r\nnone\r\n0 1048576.0 524341 0.5 0.5000505447387695 769533.4950007759 0.7338843297965774\r\n1 1048576.0 524341 0.5 0.5000505447387695 769533.4950007759 0.7338843297965774\r\n2 1048576.0 524341 0.5 0.5000505447387695 769533.4950007759 0.7338843297965774\r\n3 1048576.0 524341 0.5 0.5000505447387695 769533.4950007759 0.7338843297965774\r\n4 1048576.0 524341 0.5 0.5000505447387695 769533.4950007759 0.7338843297965774\r\n5 1048576.0 524341 0.5 0.5000505447387695 769533.4950007759 0.7338843297965774\r\n6 1048576.0 524341 0.5 0.5000505447387695 769533.4950007759 0.7338843297965774\r\n7 1048576.0 524341 0.5 0.5000505447387695 769533.4950007759 0.7338843297965774\r\n8 1048576.0 524341 0.5 0.5000505447387695 769533.4950007759 0.7338843297965774\r\n9 1048576.0 524341 0.5 0.5000505447387695 769533.4950007759 0.7338843297965774\r\nsum\r\n0 1048576.0 524341 0.5 0.5000505447387695 769533.4950007756 769533.4950007756\r\n1 1048576.0 524341 0.5 0.5000505447387695 769533.4950007756 769533.4950007756\r\n2 1048576.0 524341 0.5 0.5000505447387695 769533.4950007757 769533.4950007757\r\n3 1048576.0 524341 0.5 0.5000505447387695 769533.4950007756 769533.4950007756\r\n4 1048576.0 524341 0.5 0.5000505447387695 769533.4950007756 769533.4950007756\r\n5 1048576.0 524341 0.5 0.5000505447387695 769533.4950007756 769533.4950007756\r\n6 1048576.0 524341 0.5 0.5000505447387695 769533.4950007756 769533.4950007756\r\n7 1048576.0 524341 0.5 0.5000505447387695 769533.4950007754 769533.4950007754\r\n8 1048576.0 524341 0.5 0.5000505447387695 769533.4950007756 769533.4950007756\r\n9 1048576.0 524341 0.5 0.5000505447387695 769533.4950007756 769533.4950007756\r\nmean\r\n0 1048576.0 524341 0.5 0.5000505447387695 0.733884329796577 0.733884329796577\r\n1 1048576.0 524341 0.5 0.5000505447387695 0.733884329796577 0.733884329796577\r\n2 1048576.0 524341 0.5 0.5000505447387695 0.733884329796577 0.733884329796577\r\n3 1048576.0 524341 0.5 0.5000505447387695 0.733884329796577 0.733884329796577\r\n4 1048576.0 524341 0.5 0.5000505447387695 0.733884329796577 0.733884329796577\r\n5 1048576.0 524341 0.5 0.5000505447387695 0.7338843297965769 0.7338843297965769\r\n6 1048576.0 524341 0.5 0.5000505447387695 0.733884329796577 0.733884329796577\r\n7 1048576.0 524341 0.5 0.5000505447387695 0.733884329796577 0.733884329796577\r\n8 1048576.0 524341 0.5 0.5000505447387695 0.733884329796577 0.733884329796577\r\n9 1048576.0 524341 0.5 0.5000505447387695 0.733884329796577 0.733884329796577\r\n', '\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: GPU 0: Tesla K80\r\nNvidia driver version: 384.111\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.1\r\n[pip3] torch==1.0.1.post2\r\n[conda] mkl                       2018.0.0             hb491cac_4\r\n[conda] mkl-service               1.1.2            py36h17a0993_4\r\n[conda] torch                     0.4.0                     <pip>\r\n']",[],1,0
84,pytorch,3018,closed,CUDA initialization hangs on CUDA 9,"I just installed pytorch HEAD from source with cuda 9 and cudnn 7 using gcc 5.4 on ubuntu 16.04. It seemed to build and install without issue, however the python process hangs if I try to move a tensor to gpu using the cuda method. The following snippet also hangs.



There are 2 Maxwell Titan X and 2 Pascal 1080 Ti cards attached to the machine.",,"['Did you build with magma conda package? If so, try building without it, magma conda package is built with cuda 8 and conflicts with pytorch when it is built with cuda 9. ', 'Yes, that was the problem. Thanks!', 'Will not installing the magma package affect computational performance?', ""Magma is not used to boost the performance. It's only that without it some linear algebra functions (linear system solvers, Cholesky decomposition, and similar) will not work. You won't find any difference unless you use those."", '@dogancan  Hi, can I know how to built without magma?']",['python\r\nimport torch\r\n\r\ntorch._C._cuda_init()\r\n'],[],1,0
85,pytorch,3021,closed,CPU scaling issue,"Platform: c4.4xlarge EC2 instance (16 vCPUs)
OS: Fedora cloud 26-1.5
Version: PyTorch v0.2.0

Description:
Running 4 workloads on all cores is x8 slower than pinning each workload to 4 different cpus.

How-to reproduce:


The following yields 220 ms/sequence (while not utilizing the CPUs fully):


The following yields 1850 ms/sequence:

And moreover, all the CPUs are 100% utilized...

Motivation:
I wanted to scale training on multi-core systems, a single workload not utilized the cpus fully, so I thought about utilizing the rest of the resources by running multiple workloads.",,"[""It's probably just a cache issue. If you restrict programs to a subset of CPUs, they won't stomp on each other caches. Additionally, OMP might be detecting this and reducing paralellism. Can you try setting `OMP_NUM_THREADS=4 MKL_NUM_THREADS=4` and run these 4 processes without pinning them to any cores?"", 'I executed 4 workloads on the first 4 cores and it worked fine (performance as in a single workload).\r\n```\r\ntaskset -c 0-3 ./train.py --seed 5 --checkpoint-interval 0\r\ntaskset -c 0-3 ./train.py --seed 50 --checkpoint-interval 0\r\ntaskset -c 0-3 ./train.py --seed 500 --checkpoint-interval 0\r\ntaskset -c 0-3 ./train.py --seed 5000 --checkpoint-interval 0\r\n```\r\nThe scaling issue happens when running the workloads on all 16 cores.\r\nAttached are the histograms of ""perf top"" during both tests.\r\n\r\n[perf.hist.4cores.txt](https://github.com/pytorch/pytorch/files/1368315/perf.hist.4cores.txt)\r\n[perf.hist.16cores.txt](https://github.com/pytorch/pytorch/files/1368317/perf.hist.16cores.txt)\r\n\r\nThat\'s the output for the 16 cores execution:\r\n\r\n```\r\n  18.56%  libgomp-ae56ecdc.so.1.0.0                   [.] 0x000000000000f062\r\n  18.04%  libgomp-ae56ecdc.so.1.0.0                   [.] 0x000000000000f05e\r\n  15.30%  libgomp-ae56ecdc.so.1.0.0                   [.] 0x000000000000f3a2\r\n  14.88%  libgomp-ae56ecdc.so.1.0.0                   [.] 0x000000000000f39e\r\n  12.09%  libgomp-ae56ecdc.so.1.0.0                   [.] 0x000000000000f05c\r\n  10.16%  libgomp-ae56ecdc.so.1.0.0                   [.] 0x000000000000f39c\r\n   3.64%  libgomp-ae56ecdc.so.1.0.0                   [.] 0x000000000000f058\r\n   2.94%  libgomp-ae56ecdc.so.1.0.0                   [.] 0x000000000000f398\r\n   0.23%  libpython3.6m.so.1.0                        [.] _PyEval_EvalFrameDefault\r\n   0.16%  libc-2.25.so                                [.] _int_malloc\r\n   0.13%  libc-2.25.so                                [.] _int_free\r\n   0.13%  [kernel]                                    [k] finish_task_switch\r\n   0.12%  libc-2.25.so                                [.] malloc\r\n   0.10%  libpython3.6m.so.1.0                        [.] _PyType_Lookup\r\n   0.10%  [kernel]                                    [k] _raw_spin_unlock_irqrestore\r\n   0.06%  libpython3.6m.so.1.0                        [.] _PyObject_GenericGetAttrWithDict\r\n   0.05%  libpython3.6m.so.1.0                        [.] PyObject_Malloc\r\n   0.05%  libc-2.25.so                                [.] malloc_consolidate.part.1\r\n   0.05%  libpython3.6m.so.1.0                        [.] PyTuple_New\r\n   0.05%  libpython3.6m.so.1.0                        [.] _init\r\n   0.04%  libc-2.25.so                                [.] free\r\n```\r\n', ""Closing due to age. @loudinthecloud, please file a new issue if you're still seeing this behavior. ""]","['\r\ngit clone git@github.com:loudinthecloud/pytorch-ntm.git\r\ngit checkout d08291fac4dccaa3c8d081bad06802d9e4d737ea\r\nsudo pip3 install -r requirements\r\n', '\r\ntaskset -c 0-3 ./train.py --seed 5 --checkpoint-interval 0\r\ntaskset -c 4-7 ./train.py --seed 50 --checkpoint-interval 0\r\ntaskset -c 8-11 ./train.py --seed 500 --checkpoint-interval 0\r\ntaskset -c 12-15 ./train.py --seed 5000 --checkpoint-interval 0\r\n', '\r\n./train.py --seed 5 --checkpoint-interval 0\r\n./train.py --seed 50 --checkpoint-interval 0\r\n./train.py --seed 500 --checkpoint-interval 0\r\n./train.py --seed 5000 --checkpoint-interval 0\r\n']",[],1,0
86,pytorch,12238,open,Caffe2 is compiled without optimization passes.,"## ‚ùì Questions and Help

### Please note that this issue tracker is not a help form and this issue will be closed.

Hi , I've compiled caffe2 for cpu inference on python 3, but I get the following warning  : 


so my question is how can I compile caffe2 with optimization passes ? 

~I've also noticed that during inference the model uses only one core of my cpu for inference , but I've compiled caffe2 with NNPACK enabled wich should make it use multi core~ (my bad i didn't set the correct engine to use for the convolution ops , that's why NNPACK wasn't used) 

The model used is faster-rcnn-FPN-101 converted to .pb from detectron, inference time reported on  cpu  for image of size 800x800 is 5.8 seconds 

thanks :)
",caffe2,[],[],['W1001 22:03:57.518604     1 predictor_config.cc:81] Caffe2 is compiled without optimization passes.'],1,0
87,pytorch,19407,closed,torch.max slow for some inputs,"## Issue description

It seems that  can be quite slow depending on input. For example, sorting a tensor before calling  can make it miserably slow. Note that this issue exists even for non-pathological cases where input is not fully sorted. This was originally observed in my beam search implementation that seemed to run faster than greedy decoding at times.

## Code example

Here is a minimal example highlighting the issue. It's framed in terms of a greedy decoding algorithm:



The following are timing results from a number of different GPU configurations. Note that when the input to  is sorted it is always slower than , otherwise it is faster. The speed difference is definitely architecture dependent. Additionally,  is much more stable in terms of speed regardless of sorting.

Timings for a Titan X Pascal:


Timings for a 1080ti:


And finally timings when running on Google Colab (Tesla T4):


## System Info
This is from the Google Colab system:

",module: cuda module: performance triaged,['Please disregard. Explicitly putting in calls to `torch.cuda.synchronize()` shows the slow down is actually from previous kernels.'],"[""python\r\nfrom collections import namedtuple\r\nfrom types import SimpleNamespace\r\nimport time\r\n\r\nimport torch\r\n\r\nHypothesis = namedtuple('Hypothesis', ['sequence', 'score'])\r\n\r\nSOS = 1\r\nSPAN = 1\r\nBEAM_WIDTH = 1\r\nMAX_SEQ_LEN = 23\r\nBATCH_SIZE = 1000\r\nVOCAB_SIZE = 37000\r\nBEAMS = [\r\n    SimpleNamespace(hypotheses=[Hypothesis([SOS], 0.) for _ in range(BEAM_WIDTH)])\r\n    for _ in range(BATCH_SIZE)\r\n]\r\n\r\ndef update_greedy(scores, indices, beam, batch_idx):\r\n    ''' Update a particular beam in a greedy fashion '''\r\n    new_hypotheses = []\r\n    for hypothesis in beam.hypotheses:\r\n        new_score = hypothesis.score + scores[batch_idx]\r\n        new_sequence = hypothesis.sequence + indices[batch_idx].tolist()\r\n        new_hypotheses.append(Hypothesis(new_sequence, new_score))\r\n\r\n    beam.hypotheses = new_hypotheses\r\n\r\ndef update_beams(log_prob, beams, use_max=True):\r\n    ''' Update the beam batch '''\r\n    if use_max:\r\n        # originally max was used when BEAM_WIDTH == 1\r\n        scores, indices = torch.max(log_prob, 1)\r\n    else:\r\n        scores, indices = torch.topk(log_prob, 1, 1)\r\n        scores, indices = scores[:, 0], indices[:, 0]\r\n\r\n    for idx, beam in enumerate(beams):\r\n        update_greedy(scores, indices, beam, idx)\r\n        # If BEAM_WIDTH > 1 use different update approach\r\n\r\nfor sort in (True, False):\r\n  print(f'******sorting={sort}******')\r\n  for name, k, dim in [('batch', BATCH_SIZE, 0), ('vocab', VOCAB_SIZE, 1), ('span', SPAN, 2)]:\r\n    max_time = 0.\r\n    topk_time = 0.\r\n    for _ in range(MAX_SEQ_LEN):\r\n      log_prob = torch.rand(BATCH_SIZE, VOCAB_SIZE, SPAN).cuda()\r\n      if sort:\r\n        log_prob, _ = torch.topk(log_prob, k, dim, sorted=True)\r\n      start = time.time()\r\n      update_beams(log_prob, BEAMS, use_max=True)\r\n      max_time += time.time() - start\r\n\r\n      start = time.time()\r\n      update_beams(log_prob, BEAMS, use_max=False)\r\n      topk_time += time.time() - start\r\n\r\n    print(f'{name} dim: max={max_time / MAX_SEQ_LEN:.3f}s, topk={topk_time / MAX_SEQ_LEN:.3f}s')\r\n"", '\r\n******sorting=True******\r\nbatch dim: max=0.189s, topk=0.058s\r\nvocab dim: max=0.081s, topk=0.070s\r\nspan dim: max=0.262s, topk=0.075s\r\n\r\n******sorting=False******\r\nbatch dim: max=0.060s, topk=0.063s\r\nvocab dim: max=0.065s, topk=0.071s\r\nspan dim: max=0.069s, topk=0.073s\r\n', '\r\n******sorting=True******\r\nbatch dim: max=0.075s, topk=0.040s\r\nvocab dim: max=0.049s, topk=0.042s\r\nspan dim: max=0.150s, topk=0.043s\r\n\r\n******sorting=False******\r\nbatch dim: max=0.043s, topk=0.045s\r\nvocab dim: max=0.044s, topk=0.046s\r\nspan dim: max=0.045s, topk=0.047s\r\n', '\r\n******sorting=True******\r\nbatch dim: max=0.101s, topk=0.045s\r\nvocab dim: max=0.059s, topk=0.048s\r\nspan dim: max=0.214s, topk=0.050s\r\n\r\n******sorting=False******\r\nbatch dim: max=0.051s, topk=0.058s\r\nvocab dim: max=0.056s, topk=0.064s\r\nspan dim: max=0.055s, topk=0.063s\r\n', '\r\nCollecting environment information...\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.12.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: Tesla T4\r\nNvidia driver version: 410.79\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.5.0\r\n\r\nVersions of relevant libraries:\r\n[pip3] msgpack-numpy==0.4.3.2\r\n[pip3] numpy==1.16.2\r\n[pip3] torch==1.0.1.post2\r\n[pip3] torchsummary==1.5.1\r\n[pip3] torchtext==0.3.1\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] Could not collect\r\n']","['torch.max', 'torch.max', 'torch.max', 'torch.topk', 'torch.topk']",1,0
88,pytorch,21462,open,"Slow convolution with large kernels, should be using FFT","## üêõ Bug

When using  with a large kernel size (1024 for instance) on gpu, the cudnn implementation is very slow and gets slower as I increase the kernel size. I thought it was using FFT but apparently not. If it were using FFT, the computation time should be independent of the kernel size, because the kernel is anyway padded to the length of the input.

I have tried benchmarking with both  set to  and . My implementation using the FFT is significantly faster especially when using a stride of 1. You will find hereafter the code both for the FFT based convolution implementation I use and the profiling. My implementation is within ~5e-5 of the reference implementation for random weights and input. For a kernel size of 1024, with 64 channels, a stride of 1 and an input of length 64000, the default implementation is about 20 times slower than the FFT based one. When using a kernel size of 2048, it is 40 times slower.


## To Reproduce

Steps to reproduce the behavior:

1. Copy the code below in 
2. Run 

torch.cuda.synchronize()

## Expected behavior

When using a stride of 1 and large kernel size, the FFT implementation is much faster than the default one. The FFT one takes 160ms whatever the size of the kernel, versus 3.3 seconds (resp 6.7) for the default one with a kernel size of 1024 (resp 2048).  For large strides, the cudnn implementation is competitive or faster as expected (the FFT only has an interest if we want the convolution for all positions).
I would expect cudnn to provide a fast implementation for large kernels with low stride, which can be especially useful in audio (filters implementation). When talking about this around me, most people were surprised as it has been announced that an FFT based implementation was added to cudnn.

## Environment

Collecting environment information...
PyTorch version: 1.1.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 18.04.1 LTS
GCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
CMake version: version 3.13.4

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration:
GPU 0: Quadro GP100
GPU 1: Quadro GP100

Nvidia driver version: 410.79
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.15.4
[pip3] torch==1.1.0
[pip3] torchvision==0.2.2
[conda] blas                      1.0                         mkl
[conda] mkl                       2018.0.3                      1
[conda] mkl_fft                   1.0.6            py37h7dd41cf_0
[conda] mkl_random                1.0.1            py37h4414c95_1
[conda] pytorch                   1.1.0           py3.7_cuda10.0.130_cudnn7.5.1_0    pytorch
[conda] torchvision               0.2.2                      py_3    pytorch


cc @csarofeen @ptrblck @mruberry @peterbell10 @VitalyFedyunin @ngimel",module: convolution module: cudnn module: fft module: performance triaged,"['Thanks a lot for the detailed report!\r\n\r\nWhile this is something that should definitely be improved on the cudnn side, in the meantime we can also dispatch to your implementation for the configurations that you mentioned.\r\n\r\nWe would be happy to accept a PR implementing this feature.', ""I can probably take this if @adefossez doesn't want to."", '@Chillee go for it!', ""> When using Conv1d with a large kernel size (1024 for instance) on gpu, the cudnn implementation is very slow and gets slower as I increase the kernel size. I thought it was using FFT but apparently not. If it were using FFT, the computation time should be independent of the kernel size, because the kernel is anyway padded to the length of the input.\r\n\r\nJust FYI, the FFT Convolutions in CuDNN, and from our FBFFT paper dont actually expand and do a FFT with the kernel padded all the way up to the input. They do FFT with small windows of a tile-size. In our paper, we did tile size of 8, 16, 32 based on the size of the convolution kernel, and we parallelized all of these smaller FFT calls with dedicated CUDA kernels. I'm pretty sure CuDNN didn't implement a full traditional FFT convolution either.\r\n\r\nThat being said, if you do have a kernel size of 1024, or something larger than 128, it's probably worth doing ConvFFT like what you did above.\r\nThe exercise / work here is to not have CuDNN be forced to use the FFT algorithm (it wont be any faster, at worse the workload shape wont even be qualified to use that implementation).\r\nThe work here is to write @adefossez 's code deep into PyTorch routines, when we encounter larger kernel sizes.\r\n"", ""Thanks for the quick replies everyone!\r\n@Chillee I'm happy to let you do it, I don't think I know Pytorch internals well enough to do it cleanly, but I would be curious to see how you do it.\r\n@soumith, thanks for the clarification. This is going to be especially useful when using something like Neil Zeghidour's filterbanks for audio classification [1], which uses a kernel size of 400 with a stride of 1 for the first layer.\r\n\r\nOne danger of my implementation though is the memory usage if there is too many channels and the stride is > 1, because it always computes the full convolution and then discard the values not needed if the stride is > 1 (but small enough to make it worth using the FFT). If the end user does not know that, he might be surprised at the memory usage. I don't know if there is a clean solution to that problem.\r\n\r\n[1] https://arxiv.org/abs/1711.01161"", ""I don't think I'm gonna be able to take this one - having some issues with building CUDA pytorch, and too much inertia to figure out how to fix it :("", ""No worries, I can take care of it. @fmassa, I'll probably bug you to get some help."", '@adefossez sure, no worries!', 'Did this ever get implemented? Do you need help?', 'For alternatives, check out the FFT-based convolution in julius and pyro:\r\n\r\nhttps://adefossez.github.io/julius/julius/fftconv.html\r\n\r\nhttps://github.com/pyro-ppl/pyro/blob/ae55140acfdc6d4eade08b434195234e5ae8c261/pyro/ops/tensor_utils.py#L187', 'I implemented it outside of pytorch first, it is on my todo list to integrate it with PyTorch, but it will require a bit of experimenting to choose the right implementation automatically on all devices. Hopefully I get that done in January. In the mean time you can use the implementation in julius that @iver56 linked to, it is exactly the same thing.', ""I'm happy to at least PR the pyro implementation into torch.functional as a first effort? ""]",['\r\n\r\nOutput from the above code in my environment:\r\n'],"['Conv1d', 'torch.backends.cudnn.benchmark', 'False', 'True', 'profile_conv.py', 'python3 profile_conv.py', '', 'python\r\nimport time\r\nfrom functools import partial\r\nfrom itertools import product\r\n\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\n\r\ndef compl_mul(a, b):\r\n    """"""\r\n    Given a and b two tensors of dimension 4\r\n    with the last dimension being the real and imaginary part, \r\n    returns a multiplied by the conjugate of b, the multiplication\r\n    being with respect to the second dimension.\r\n    """"""\r\n    op = partial(torch.einsum, ""bct,dct->bdt"")\r\n    return torch.stack([\r\n        op(a[..., 0], b[..., 0]) + op(a[..., 1], b[..., 1]),\r\n        op(a[..., 1], b[..., 0]) - op(a[..., 0], b[..., 1])\r\n    ],\r\n                       dim=-1)\r\n\r\n\r\nclass FastConv(nn.Module):\r\n    """"""\r\n    Convoluton based on FFT, faster for large kernels and small strides.\r\n    """"""\r\n\r\n    def __init__(self,\r\n                 in_channels,\r\n                 out_channels,\r\n                 kernel_size,\r\n                 stride=1,\r\n                 bias=True):\r\n        super().__init__()\r\n\r\n        if bias:\r\n            self.bias = nn.Parameter(torch.zeros(out_channels, 1))\r\n        else:\r\n            self.bias = None\r\n        self.weight = nn.Parameter(\r\n            torch.zeros(out_channels, in_channels, kernel_size))\r\n\r\n        self.kernel_size = kernel_size\r\n        self.stride = stride\r\n\r\n    def forward(self, signal):\r\n        padded = F.pad(self.weight,\r\n                       (0, signal.size(-1) - self.weight.size(-1)))\r\n        signal_fr = torch.rfft(signal, 1)\r\n        weight_fr = torch.rfft(padded, 1)\r\n        output_fr = compl_mul(signal_fr, weight_fr)\r\n        output = torch.irfft(output_fr, 1, signal_sizes=(signal.size(-1), ))\r\n        output = output[..., ::self.stride]\r\n        target_length = (signal.size(-1) - self.kernel_size) // self.stride + 1\r\n        output = output[..., :target_length].contiguous()\r\n        if self.bias is not None:\r\n            output += self.bias\r\n        return output\r\n\r\n\r\ndef profile(module, *args, repetitions=10, warmup=1):\r\n    """"""\r\n    Given a module and args, apply repeatedly the module to the args,\r\n    calling ', ' in between. Return the time per\r\n    repetition. Not perfect profiling but gives a rough idea.\r\n    """"""\r\n    module(*args)\r\n    begin = time.time()\r\n    for _ in range(repetitions):\r\n        module(*args)\r\n        torch.cuda.synchronize()\r\n    return (time.time() - begin) / repetitions\r\n\r\n\r\ndef human_seconds(seconds, display=\'.2f\'):\r\n    """"""\r\n    Human readable string from a number of seconds.\r\n    """"""\r\n    value = seconds * 1e6\r\n    ratios = [1e3, 1e3, 60, 60, 24]\r\n    names = [\'us\', \'ms\', \'s\', \'min\', \'hrs\', \'days\']\r\n    last = names.pop(0)\r\n    for name, ratio in zip(names, ratios):\r\n        if value / ratio < 0.3:\r\n            break\r\n        value /= ratio\r\n        last = name\r\n    return f""{format(value, display)} {last}""\r\n\r\n\r\ndef test_one(kernel_size=1024,\r\n             channels=8,\r\n             batch_size=16,\r\n             length=16000 * 5,\r\n             stride=1):\r\n    print(f""Benchmark for kernel_size={kernel_size} ""\r\n          f""stride={stride} channels={channels}"")\r\n    device = ""cuda""\r\n    signal = torch.randn(batch_size, channels, length, device=device)\r\n    conv = nn.Conv1d(\r\n        channels, channels, kernel_size, stride=stride, bias=False).to(device)\r\n    fft_conv = FastConv(\r\n        channels, channels, kernel_size, stride=stride, bias=False).to(device)\r\n    fft_conv.weight = conv.weight\r\n\r\n    conv_output = conv(signal)\r\n    fft_output = fft_conv(signal)\r\n    error = torch.abs(conv_output - fft_output)\r\n    print(""\\tMean error={:.2g}, max error={:.2g}"".format(\r\n        error.mean(), error.max()))\r\n    torch.backends.cudnn.benchmark = False\r\n    print(""\\tCudnn benchmark = False: {}"".format(\r\n        human_seconds(profile(conv, signal))))\r\n    torch.backends.cudnn.benchmark = True\r\n    print(""\\tCudnn benchmark = True: {}"".format(\r\n        human_seconds(profile(conv, signal))))\r\n    print(""\\tFFT Conv: {}"".format(human_seconds(profile(fft_conv, signal))))\r\n\r\n\r\ndef test():\r\n    print(""torch.backends.cudnn.is_available(): "",\r\n          torch.backends.cudnn.is_available(),\r\n          ""\\ntorch.backends.cudnn.version(): "", torch.backends.cudnn.version())\r\n    grid = product([256, 1024, 2048], [64], [1, 16])\r\n    for kernel_size, channels, stride in grid:\r\n        test_one(kernel_size=kernel_size, channels=channels, stride=stride)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    test()\r\n\r\ntorch.backends.cudnn.is_available():  True\r\ntorch.backends.cudnn.version():  7501\r\nBenchmark for kernel_size=256 stride=1 channels=64\r\n        Mean error=9.5e-07, max error=2.5e-05\r\n        Cudnn benchmark = False: 0.33 s\r\n        Cudnn benchmark = True: 0.33 s\r\n        FFT Conv: 160.82 ms\r\nBenchmark for kernel_size=256 stride=16 channels=64\r\n        Mean error=9.5e-07, max error=1.8e-05\r\n        Cudnn benchmark = False: 23.70 ms\r\n        Cudnn benchmark = True: 23.12 ms\r\n        FFT Conv: 160.16 ms\r\nBenchmark for kernel_size=1024 stride=1 channels=64\r\n        Mean error=1.9e-06, max error=4.1e-05\r\n        Cudnn benchmark = False: 3.28 s\r\n        Cudnn benchmark = True: 3.31 s\r\n        FFT Conv: 160.77 ms\r\nBenchmark for kernel_size=1024 stride=16 channels=64\r\n        Mean error=1.9e-06, max error=3.5e-05\r\n        Cudnn benchmark = False: 213.10 ms\r\n        Cudnn benchmark = True: 213.29 ms\r\n        FFT Conv: 158.74 ms\r\nBenchmark for kernel_size=2048 stride=1 channels=64\r\n        Mean error=2.6e-06, max error=6e-05\r\n        Cudnn benchmark = False: 6.68 s\r\n        Cudnn benchmark = True: 6.68 s\r\n        FFT Conv: 160.56 ms\r\nBenchmark for kernel_size=2048 stride=16 channels=64\r\n        Mean error=2.6e-06, max error=4.8e-05\r\n        Cudnn benchmark = False: 0.43 s\r\n        Cudnn benchmark = True: 0.43 s\r\n        FFT Conv: 160.41 ms\r\n', '']",1,0
89,pytorch,2520,closed,DistributedDataParallel doesn't exit properly; Leaves reduction threads running,"
When using the DDP module the process always hangs. I believe because the reduction threads are in a  loop.

https://github.com/pytorch/pytorch/blob/4c69697d2acbbe8e418a0921464c09aaf09da82a/torch/nn/parallel/distributed.py#L313-L315

Repro Code:

",,['I believe we need something like below. Let me know if I should submit a PR.\r\n\r\nhttps://github.com/pytorch/pytorch/blob/f09027bc2913d8ab010085040c189eecd5681c12/torch/utils/data/dataloader.py#L156-L158'],"[' python\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.distributed as dist\r\nimport ctypes\r\n\r\nlib = ctypes.cdll.LoadLibrary(None)\r\n\r\n\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.conv1 = nn.Conv2d(3, 6, 5)\r\n        self.pool = nn.MaxPool2d(2, 2)\r\n        self.conv2 = nn.Conv2d(6, 16, 5)\r\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\r\n        self.fc2 = nn.Linear(120, 84)\r\n        self.fc3 = nn.Linear(84, 10)\r\n#        self.bn1 = torch.nn.BatchNorm2d(6)\r\n        self.bn2 = torch.nn.BatchNorm2d(16)\r\n        \r\n    def forward(self, x):\r\n        x = self.pool(F.relu((self.conv1(x))))\r\n        x = self.pool(F.relu(self.bn2(self.conv2(x))))\r\n        x = x.view(-1, 16 * 5 * 5)\r\n        x = F.relu(self.fc1(x))\r\n        x = F.relu(self.fc2(x))\r\n#        x = self.fc3(x)\r\n        return x\r\n    \r\n    \r\nnet = Net().cuda()\r\nworld_size=2\r\ndist_url = \'tcp://224.66.41.62:23456\'\r\ndist_backend = \'gloo\'\r\n\r\ndist.init_process_group(backend=dist_backend, init_method=dist_url, world_size=world_size)\r\ntorch.cuda.synchronize()\r\nprint(""initialized"")\r\nnet = torch.nn.parallel.DistributedDataParallel(net)\r\ntorch.cuda.synchronize()\r\nprint(""network created"")\r\ninp = Variable(torch.randn(64, 3, 32,32))\r\ntorch.cuda.synchronize()\r\nprint(""input created"")\r\n\r\n\r\nfor i in range(3):\r\n    out = net.forward(inp)\r\n    loss = out.sum()\r\n    loss.backward()\r\n    torch.cuda.synchronize()\r\n\r\nlib.cudaProfilerStop()\r\n']",['while True:'],1,1
90,pytorch,8154,closed,Performance drop on small models trained on CPU between 0.3.1 and 0.4,"## Issue description

Updating from Pytorch 0.3.1 to 0.4 yields a significant performance drop on small feedforward networks, trained **on CPU**. This effect happens on Linux (in a virtual machine) and on Windows.

The attached code was executed on a Windows 10 machine with an Intel i7-7700HQ.
The Linux test was done in a virtual machine. The computer is equipped with an Intel i7-4702MQ.

The average runtime over 10 repetitions on Linux is 26.141655 s for Pytorch 0.3.1 and 32.092158 s on 0.4  (~23 % increase)
The average runtime over 10 repetitions on Windows is 12.577887 s for Pytorch 0.3.1 and 17.759720 s on 0.4  (~41 % increase)

Observations: 
- The time difference between 0.3.1 and 0.4 seems to be more or less constant on both Windows and Linux. 
- Also profiling suggests the DataLoader __next__ takes 8 s instead of 4 s previously, a major part of the increased time
- Weirdly, in different calls of the script, the early stopping if-clause activates sometimes despire setting the pytorch seed as to avoid effects due to randomness

## Code example

See my results and the data/script used here:
[PytorchSpeedTest.zip](https://github.com/pytorch/pytorch/files/2071136/PytorchSpeedTest.zip)

## System Info

(the collect_env is not yet adapted to Windows, therefore information is missing. I am using pip 10.x and the latest conda)

PyTorch version: 0.4.0
Is debug build: No
CUDA used to build PyTorch: 9.1

OS: Microsoft Windows 10 Home
GCC version: Could not collect
CMake version: Could not collect

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.1.85
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] Could not collect
[conda] Could not collect

- PyTorch or Caffe2: PyTorch
- How you installed PyTorch (conda, pip, source): all conda (Windows 0.3.1. via conda install -c peterjc123 pytorch-cpu)
- OS: Windows 10 / Ubuntu 16.04 
- PyTorch version: 0.3.1 to 0.4
- Python version: 3.6

## Bottleneck Profiling
0.4


### cProfile
0.3.1

 ",high priority todo,"['Thank you for reporting this separately @jhmenke.', 'cc @peterjc123 ', ""@jhmenke @cpuhrsch  Well, I don't think it is Windows specific. Although I have changed `THAllocator` that may be related to this, it should not be the case according to your test results. The difference in time seems constant, so it must be somewhere else."", ""Since the Linux version also shows decreased performance i also don't think there is any connection to the build process of peterjc123.\r\nI personally am unclear how i would debug this further.."", 'This issue could be related to (and fixed by) #8255 \r\nI will test the performance if i find a quiet hour this week.', '_edit: To make sure the performance problem is not caused by the missing NNPack in my compilation i just built v0.3.1. from source. It still sits at 26 s compared to 45 s in the commit below._\r\n\r\nIt is now way worse:\r\nI just cloned and compiled PyTorch (torch-0.5.0a0+e31ab99, e31ab99) on Linux. v0.31 average was ~26 s, v0.4 average ~ 32 s. \r\nNow the average time is: 45.49 s\r\n\r\ntorch.bottleneck runs and after a few minutes exits with the message ""killed"" without giving any output. here is the manual cProfile log:\r\n\r\n```\r\n12393230 function calls (12099688 primitive calls) in 42.208 seconds\r\n\r\n   Ordered by: internal time\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n    30872    7.610    0.000    7.610    0.000 {method \'run_backward\' of \'torch._C._EngineBase\' objects}\r\n    82416    4.467    0.000    4.467    0.000 {built-in method stack}\r\n  2962080    4.061    0.000    4.061    0.000 dataset.py:40(<genexpr>)\r\n   123624    3.209    0.000    3.209    0.000 {built-in method addmm}\r\n    30872    2.495    0.000    7.598    0.000 adam.py:48(step)\r\n   987360    1.904    0.000    5.964    0.000 dataset.py:39(__getitem__)\r\n    82416    1.306    0.000    1.306    0.000 {built-in method torch._C._nn.threshold}\r\n288456/82416    1.256    0.000    8.898    0.000 module.py:462(__call__)\r\n   185232    1.136    0.000    1.136    0.000 {method \'sqrt\' of \'torch._C._TensorBase\' objects}\r\n   370464    1.106    0.000    1.106    0.000 {method \'add_\' of \'torch._C._TensorBase\' objects}\r\n        1    1.047    1.047   42.208   42.208 pytorch_debug.0.4.py:1(<module>)\r\n    41208    0.989    0.000    6.953    0.000 dataloader.py:314(<listcomp>)\r\n   370464    0.965    0.000    0.965    0.000 {method \'mul_\' of \'torch._C._TensorBase\' objects}\r\n    41344    0.958    0.000   14.878    0.000 dataloader.py:311(__next__)\r\n   185232    0.937    0.000    0.937    0.000 {method \'addcdiv_\' of \'torch._C._TensorBase\' objects}\r\n    41344    0.927    0.000    1.252    0.000 sampler.py:136(__iter__)\r\n   185232    0.667    0.000    0.667    0.000 {method \'addcmul_\' of \'torch._C._TensorBase\' objects}\r\n   123624    0.620    0.000    4.994    0.000 linear.py:54(forward)\r\n   123624    0.604    0.000    0.604    0.000 {method \'t\' of \'torch._C._TensorBase\' objects}\r\n123624/41208    0.557    0.000    5.679    0.000 dataloader.py:151(default_collate)\r\n    41208    0.441    0.000    8.015    0.000 container.py:89(forward)\r\n    41208    0.403    0.000    0.403    0.000 {built-in method torch._C._nn.smooth_l1_loss}\r\n   185226    0.369    0.000    0.369    0.000 {method \'zero_\' of \'torch._C._TensorBase\' objects}\r\n    30872    0.324    0.000    0.734    0.000 optimizer.py:150(zero_grad)\r\n   123624    0.308    0.000    4.171    0.000 functional.py:995(linear)\r\n    30872    0.256    0.000    0.256    0.000 {built-in method ones_like}\r\n1497619/1497435    0.248    0.000    0.249    0.000 {built-in method builtins.len}\r\n```', ""We've recently added a few performance improvements. Could you try again @jhmenke?"", ""Yes, we are back at the 0.4 average:\r\n>Average time: 31.492700 s (10 runs)\r\n\r\nIf that's what can be improved in a reasonable amount of time, i can live with it. thanks for the work you have done!"", '@jhmenke - Can we consider this issue resolved?', 'i guess so, thanks for your time!']","[""\r\n--------------------------------------------------------------------------------\r\n  cProfile output\r\n--------------------------------------------------------------------------------\r\n         12550526 function calls (12252338 primitive calls) in 21.593 seconds\r\n\r\n   Ordered by: internal time\r\n   List reduced from 334 to 15 due to restriction <15>\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n    31780    4.498    0.000    4.498    0.000 {method 'run_backward' of 'torch._C._EngineBase' objects}\r\n    84840    2.774    0.000    2.774    0.000 {built-in method stack}\r\n  3049200    2.219    0.000    2.219    0.000 C:\\Users\\Jan\\Miniconda3\\lib\\site-packages\\torch\\utils\\data\\dataset.py:40(<genexpr>)\r\n    31780    1.347    0.000    3.899    0.000 C:\\Users\\Jan\\Miniconda3\\lib\\site-packages\\torch\\optim\\adam.py:48(step)\r\n   127260    1.217    0.000    1.217    0.000 {built-in method addmm}\r\n  1016400    0.866    0.000    3.085    0.000 C:\\Users\\Jan\\Miniconda3\\lib\\site-packages\\torch\\utils\\data\\dataset.py:39(__getitem__)\r\n   190680    0.593    0.000    0.593    0.000 {method 'addcdiv_' of 'torch._C._TensorBase' objects}\r\n296940/84840    0.566    0.000    3.832    0.000 C:\\Users\\Jan\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:485(__call__)\r\n   381360    0.557    0.000    0.557    0.000 {method 'mul_' of 'torch._C._TensorBase' objects}\r\n    84840    0.553    0.000    0.553    0.000 {built-in method torch._C._nn.threshold}\r\n    42560    0.532    0.000    8.012    0.000 C:\\Users\\Jan\\Miniconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:261(__next__)\r\n        1    0.511    0.511   21.593   21.593 .\\pytorch.0.4.py:1(<module>)\r\n   381360    0.470    0.000    0.470    0.000 {method 'add_' of 'torch._C._TensorBase' objects}\r\n    42560    0.438    0.000    0.603    0.000 C:\\Users\\Jan\\Miniconda3\\lib\\site-packages\\torch\\utils\\data\\sampler.py:136(__iter__)\r\n   190680    0.421    0.000    0.421    0.000 {method 'sqrt' of 'torch._C._TensorBase' objects}\r\n\r\n\r\n--------------------------------------------------------------------------------\r\n  autograd profiler output (CPU mode)\r\n--------------------------------------------------------------------------------\r\n        top 15 events sorted by cpu_time_total\r\n\r\n----------------------  ---------------  ---------------  ---------------  ---------------  ---------------\r\nName                           CPU time        CUDA time            Calls        CPU total       CUDA total\r\n----------------------  ---------------  ---------------  ---------------  ---------------  ---------------\r\nstack                         475.169us          0.000us                1        475.169us          0.000us\r\nstack                         415.362us          0.000us                1        415.362us          0.000us\r\nstack                         388.741us          0.000us                1        388.741us          0.000us\r\nstack                         381.812us          0.000us                1        381.812us          0.000us\r\nstack                         378.531us          0.000us                1        378.531us          0.000us\r\nstack                         371.966us          0.000us                1        371.966us          0.000us\r\nThresholdBackward0            370.507us          0.000us                1        370.507us          0.000us\r\nthreshold_backward            369.049us          0.000us                1        369.049us          0.000us\r\nAddmmBackward                 358.473us          0.000us                1        358.473us          0.000us\r\nstack                         350.815us          0.000us                1        350.815us          0.000us\r\nmm                            347.169us          0.000us                1        347.169us          0.000us\r\nselect                        343.887us          0.000us                1        343.887us          0.000us\r\nadd_                          340.239us          0.000us                1        340.239us          0.000us\r\nunsqueeze                     339.511us          0.000us                1        339.511us          0.000us\r\naddmm                         335.864us          0.000us                1        335.864us          0.000us\r\n"", "" \r\n 12819090 function calls (12546756 primitive calls) in 14.661 seconds\r\n\r\n   Ordered by: internal time\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n    28602    3.038    0.000    3.038    0.000 {method 'run_backward' of 'torch._C._EngineBase' objects}\r\n  1829520    1.041    0.000    1.041    0.000 {method 'unsqueeze' of 'torch._C.FloatTensorBase' objects}\r\n    28602    0.855    0.000    2.765    0.000 adam.py:31(step)\r\n   114534    0.854    0.000    0.854    0.000 {built-in method torch._C.addmm}\r\n   914760    0.616    0.000    0.616    0.000 dataset.py:42(__getitem__)\r\n   171612    0.504    0.000    0.504    0.000 {method 'sqrt' of 'torch._C.FloatTensorBase' objects}\r\n   171612    0.468    0.000    0.468    0.000 {method 'addcdiv_' of 'torch._C.FloatTensorBase' objects}\r\n    38304    0.466    0.000    0.917    0.000 sampler.py:117(__iter__)\r\n        1    0.454    0.454   14.661   14.661 pytorch.0.3.1.py:1(<module>)\r\n267246/76356    0.438    0.000    3.045    0.000 module.py:351(__call__)\r\n   343224    0.437    0.000    0.437    0.000 {method 'mul_' of 'torch._C.FloatTensorBase' objects}\r\n    76356    0.431    0.000    0.431    0.000 {built-in method torch._C._nn.threshold}\r\n114534/38178    0.421    0.000    2.340    0.000 dataloader.py:99(default_collate)\r\n    38304    0.272    0.000    4.413    0.000 dataloader.py:256(__next__)\r\n    76356    0.270    0.000    0.270    0.000 {built-in method torch._C.cat}\r\n""]",[],1,1
91,pytorch,1509,closed,GPU memory consumption increases while training,"Hello, all
I am new to Pytorch and I meet a strange GPU memory behavior while training a CNN model for semantic segmentation. Batchsize = 1, and there are totally 100 image-label pairs in trainset, thus 100 iterations per epoch. However the **GPU memory consumption increases a lot at the first several iterations while training**.

[Platform] GTX TITAN X (12G), CUDA-7.5, cuDNN-5.0

> torch.backends.cudnn.enabled = False
> torch.backends.cudnn.benchmark = False

Then GPU memory consumption is **2934M -- 4413M -- 4433M -- 4537M -- 4537M -- 4537M** at the first six iterations.

> torch.backends.cudnn.enabled = True
> torch.backends.cudnn.benchmark = True

Then GPU memory consumption is **1686M -- 1791M -- 1791M -- 1791M -- 1791M -- 1791M** at the first six iterations.

**Why GPU memory consumption increases while training, especially, increases so largely while no cuDNN? (In my opinion, GPU memory consumption won't increase while the CNN has been build and starts training)** 

Does anyone meet the same problem? Or could anyone give some help?

This is the code snippet:

",,['https://discuss.pytorch.org'],"[""Python\r\ndef train(train_loader, model, criterion, optimizer, epoch):\r\n    batch_time = AverageMeter()\r\n    data_time = AverageMeter()\r\n    losses = AverageMeter()\r\n\r\n    # switch to train mode\r\n    model.train()\r\n\r\n    end = time.time()\r\n    for i, (input, target) in enumerate(train_loader):\r\n        # measure data loading time\r\n        data_time.update(time.time() - end)\r\n\r\n        target = target.long()\r\n        input = input.cuda(async=True)\r\n        target = target.cuda(async=True)\r\n        input_var = torch.autograd.Variable(input)\r\n        target_var = torch.autograd.Variable(target)\r\n\r\n        # compute output\r\n        output = model(input_var)\r\n        loss = criterion(output, target_var)\r\n\r\n        # record loss\r\n        losses.update(loss.data[0], input.size(0))\r\n\r\n        # compute gradient and do SGD step\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\r\n\r\n        # measure elapsed time\r\n        batch_time.update(time.time() - end)\r\n        end = time.time()\r\n\r\n        if i % args.print_freq == 0:\r\n            print('Epoch: [{0}][{1}/{2}]\\t'\r\n                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\r\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\r\n                  'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(\r\n                   epoch, i+1, len(train_loader),\r\n                   batch_time=batch_time,\r\n                   data_time=data_time,\r\n                   loss=losses))\r\n""]",[],1,0
92,pytorch,6126,closed,Inf and nan loss ,"Here is my network:

Then, I created the network and the training loops as :


The initial output was very large, then it became Inf and finally Nan. Why is this happening?",,"['I read on the forum that it maybe because of the loss function I am using. But I get the same result even after changing it to MSELoss() or NLLLoss().', ""It's likely an error in your data pipeline or an initialization problem. If you suspect it's a bug in a framework then please isolate the part that concerns you and let us know. If it's just a general difficulty in training your model, then you should use the forums.""]","['\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\nclass RNN(nn.Module):\r\n    def __init__(self, input_size, hidden_size, output_size):\r\n        super().__init__()\r\n        self.hidden_size = hidden_size\r\n        self.i2h = nn.Linear(input_size , hidden_size)\r\n        self.h2o = nn.Linear(hidden_size, output_size)\r\n        self.h2h = nn.Linear(hidden_size, hidden_size)\r\n        self.softmax = nn.LogSoftmax(dim = 1)\r\n\r\n    def forward(self, input, hidden):\r\n        h = self.softmax(self.h2h(hidden)+  self.i2h(input))\r\n        o = self.softmax(self.h2o(h))\r\n        return o, h\r\n\r\n    def init_hidden(self):\r\n        return Variable(torch.zeros(1, self.hidden_size))\r\n\r\n', ""\r\n\r\nrnn = RNN(n_chars, 90, n_chars)\r\ncriterion = nn.KLDivLoss()\r\nlearning_rate = 0.05\r\noptimizer = torch.optim.Adam(rnn.parameters(), lr = learning_rate)\r\nhidden = rnn.init_hidden()\r\nepochs = 5\r\n\r\nfor epoch in range(epochs):\r\n    for i in range(len(X)):\r\n        for ele in X[i]:\r\n            output, hidden = rnn(Variable(ele.t()), hidden)\r\n        loss = criterion(output, Y[i])\r\n        \r\n        loss.backward(retain_graph=True)\r\n        optimizer.zero_grad()\r\n        optimizer.step()\r\n        print('Current loss is: ', loss)\r\n""]",[],1,0
93,pytorch,4081,closed,20% more gpu memory usage on resnet,"Been testing with resnet 50 when I noticed that a batch of 128 / gpu no longer fits in multi-gpu. I'm seeing around 20% more gpu memory being used now (see below).

'old version' commit hash https://github.com/pytorch/pytorch/commit/50009144c02155be6afd4570e93f453e73904a8e
'new version' commit hash https://github.com/pytorch/pytorch/commit/7ddcb91c7f84c3da8cc9f7fba28a3ae9ecd6cc45

",,"[""I'm traveling this week and haven't had time to repro this, but despite your comment on the other issue, #3666 seems like the most high risk commit here. Do you think you might have a chance to bisect? If not, I will look at it next week."", 'result of git bisect:\r\n\r\n```9cb8b437786648dfcf1e6a60dced70b6ee983311 is the first bad commit\r\ncommit 9cb8b437786648dfcf1e6a60dced70b6ee983311\r\nAuthor: Sam Gross <colesbury@gmail.com>\r\nDate:   Tue Nov 14 12:59:06 2017 -0500\r\n\r\n    Split off in-place NN functions (#3683)\r\n    \r\n    For example, this splits threshold into threshold(), which is now\r\n    never in-place, and threshold_() which is always in-place.\r\n    \r\n    This simplifies the in-place vs. non-in-place logic in\r\n    gen_variable_type.py, which was bug-prone.\r\n\r\n:040000 040000 41e00599aec0edd4ca83831b70b5bea6548f93e2 5dcb50f14bd811935e98e2286a1046944a80c015 M\taten\r\n:040000 040000 7fe0482203d52e0739324180a4d7e2ea84be9e42 7b53a7090d7d0e567ec507ff45f544f260971482 M\tdocs\r\n:040000 040000 1cce0769b77847d40e644bc69d34d690e05c411c de0d3f1b2fb41e3e9f7a0896e8f19038ff55b3a6 M\ttools\r\n:040000 040000 c7fc37d284a99153972a910558fbd95902fcb5bf 65483bdccdeb25bb9cac709ba5adaa8b4759e8d2 M\ttorch\r\n```\r\n\r\n@colesbury ^^ any ideas?', ""Thanks for bisecting it. I'll take a look."", ""OK, so the problem is that the auto-generated derivatives for the in-place functions are constructed such that they use the input (which has to be cloned, so it defeats the memory savings). They should be using the output instead.\r\n\r\nI'll fix `gen_variable_type.py`.\r\n\r\n@lantiga, when you implement #3874 you'll want to make sure that the derivative formulas for in-place functions use output/result instead of self. For example,\r\n\r\nhttps://github.com/lantiga/pytorch/commit/eb14fa6d2fe578bb91f461516ddee163742a3145#diff-8026a1a389bfcf92e484ae272675c336R702\r\n\r\nshould be\r\n\r\n```\r\nself: threshold_backward(grad, output, threshold, value)\r\n```"", 'Ok, so\r\n```\r\nself: elu_backward(grad, alpha, output)\r\n```\r\nis correct, while these change:\r\n```\r\n< self: hardtanh_backward(grad, self, min_val, max_val)\r\n> self: hardtanh_backward(grad, output, min_val, max_val)\r\n\r\n< self: leaky_relu_backward(grad, self, negative_slope)\r\n> self: leaky_relu_backward(grad, output, negative_slope)\r\n\r\n< self: rrelu_backward(grad, self, lower, upper, training, noise)\r\n> self: rrelu_backward(grad, output, lower, upper, training, noise)\r\n\r\n< self: threshold_backward(grad, self, threshold, value)\r\n> self: threshold_backward(grad, output, threshold, value)\r\n```', '@lantiga yeah, that looks right. Just make sure `test_nn.py` still passes.', ""There are still memcopies for inplace relu's, even though #4096 is merged."", '@ngimel is this in forward or in backward?', 'Memcopies are in forward, and generated code for backward still uses ""self"" rather than ""output"", I guess changes to ```gen_variable_type.py``` are necessary, as @colesbury was saying.\r\n```\r\nTensor VariableType::threshold_backward(const Tensor & grad_output, const Tensor & self, Scalar threshold, Scalar value) const {\r\n    profiler::RecordFunction profiler(""threshold_backward"");\r\n    auto& grad_output_ = unpack(grad_output, ""grad_output"", 0);\r\n    auto& self_ = unpack(self, ""self"", 1);\r\n    std::shared_ptr<ThresholdBackwardBackward> grad_fn;\r\n    auto flags = compute_flags({ grad_output, self });\r\n    if (flags.requires_grad) {\r\n      grad_fn = std::make_shared<ThresholdBackwardBackward>();\r\n      grad_fn->next_functions = compute_next_functions({ grad_output, self });\r\n      grad_fn->self_ = SavedVariable(self, false);\r\n      grad_fn->threshold = threshold;\r\n      grad_fn->value = value;\r\n    }\r\n    auto ret = as_variable(baseType->threshold_backward(grad_output_, self_, threshold, value));\r\n    set_flags(ret, flags, grad_fn);\r\n    if (jit::tracer::isTracing({ grad_output, self })) {\r\n      jit::Node *n = jit::tracer::recordTrace( ""threshold_backward"", { grad_output, self }, { ret } );\r\n      setattr(n, jit::stringToSymbol(""threshold""), threshold);\r\n      setattr(n, jit::stringToSymbol(""value""), value);\r\n    }\r\n    return Tensor(std::move(ret));\r\n}\r\nTensor VariableType::threshold_(Tensor & self, Scalar threshold, Scalar value) const {\r\n    profiler::RecordFunction profiler(""threshold_"");\r\n    auto& self_ = unpack(self, ""self"", 0);\r\n    check_inplace(self);\r\n    std::shared_ptr<ThresholdBackward> grad_fn;\r\n    auto flags = compute_flags({ self });\r\n    if (flags.requires_grad) {\r\n      grad_fn = std::make_shared<ThresholdBackward>();\r\n      grad_fn->next_functions = compute_next_functions({ self });\r\n      grad_fn->self_ = SavedVariable(self.clone(), false);\r\n      grad_fn->threshold = threshold;\r\n      grad_fn->value = value;\r\n    }\r\n    baseType->threshold_forward_(self_, threshold, value);\r\n    increment_version(self);\r\n    set_flags(static_cast<Variable&>(self), flags, grad_fn, true);\r\n    if (jit::tracer::isTracing({ self })) {\r\n      jit::Node *n = jit::tracer::recordTrace( ""threshold"", { self }, { self } );\r\n      setattr(n, jit::stringToSymbol(""threshold""), threshold);\r\n      setattr(n, jit::stringToSymbol(""value""), value);\r\n    }\r\n    return self;\r\n}\r\n```', ""If we want the `backward` to get `output` as the argument just for `inplace` functions, shouldn't we give the inplace backward function a different name, like `threshold__backward` (two underscores)?"", ""I don't see problems with all (not just inplace) backward variants getting ```output``` as argument, but may be I'm missing something?"", ""Doesn't the backward of elu/sigmoid/... actually run a lot faster if you have the output (the derivatives are simple to express in terms of outputs)?"", 'Exactly, sigmoid/tanh require outputs for derivative computation (and yaml actually lists that). Generate sigmoid_ does not clone input, and saves output:\r\n```\r\nTensor & VariableType::sigmoid_(Tensor & self) const {\r\n    profiler::RecordFunction profiler(""sigmoid_"");\r\n    auto& self_ = unpack(self, ""self"", 0);\r\n    check_inplace(self);\r\n    std::shared_ptr<SigmoidBackward> grad_fn;\r\n    auto flags = compute_flags({ self });\r\n    if (flags.requires_grad) {\r\n      grad_fn = std::make_shared<SigmoidBackward>();\r\n      grad_fn->next_functions = compute_next_functions({ self });\r\n    }\r\n    baseType->sigmoid_(self_);\r\n    increment_version(self);\r\n    set_flags(static_cast<Variable&>(self), flags, grad_fn, true);\r\n    if (jit::tracer::isTracing({ self })) {\r\n      jit::Node *n = jit::tracer::recordTrace( ""sigmoid"", { self }, { self } );\r\n      (void)n;\r\n    }\r\n    if (grad_fn) {\r\n      grad_fn->result_ = SavedVariable(self, true);\r\n    }\r\n    return self;\r\n}\r\n```', ""I think I've got a fix for this."", ""I think all that's needed is replace ```self``` with ```output``` here https://github.com/pytorch/pytorch/blob/8199edf5c1fb7073694e18954caa87602fbcce64/tools/autograd/derivatives.yaml#L734 and in similar places. Testing it now."", ""I think there's a change in `gen_variable_type.py` for inplace to work properly, i.e. independently from the non-inplace derivative definition (you should be able to use `self` for the non inplace and `output` for the inplace):\r\n```\r\n--- a/tools/autograd/gen_variable_type.py\r\n+++ b/tools/autograd/gen_variable_type.py\r\n@@ -414,12 +414,7 @@ def load_derivatives(path, declarations_by_signature, declarations_by_name):\r\n \r\n         declaration = declarations_by_name[defn_name][0]\r\n         base_name = defn_name if not declaration['inplace'] else defn_name[:-1]\r\n-        fwd_name = base_name + '_forward'\r\n-\r\n-        if declaration['inplace']:\r\n-            declaration['base_name'] = fwd_name + '_'\r\n-            declaration['derivative'] = declarations_by_name[base_name][0]['derivative']\r\n-            return None\r\n+        fwd_name = base_name + ('_forward' if not declaration['inplace'] else '_forward_')\r\n \r\n         assert len(declarations_by_name[fwd_name]) == 1\r\n```"", ""But a single _backward function is generated regardless of there being 2 declarations in the yaml file? I don't know how generation works. "", 'So, the _backward you see is actually the ""forward"" of the double backward. The actual backwards for, say, `threshold` and `threshold_` are set up when creating and configuring `grad_fn` inside the forward function.\r\nTests pass for me after the change above (will submit a PR in a minute): `clone` is gone from forward and `output` is saved after the call\xa0for all inplace functions, so we should be good.\r\nDouble backwards are out of scope I believe.', 'Are all the fixes here merged to master now?', '#4184 is merged, #4199 is superceded by #4395, so this issue can be closed, and #4199 can be closed too? @lantiga?', '@ngimel yep, correct. Thank you for following up.']",['\r\nresnet50    | batch 16/gpu | batch 32/gpu | batch 64/gpu\r\nold version | 2.96 GiB     | 4.52 GiB     | 7.97 GiB\r\nnew version | 3.55 GiB     | 5.60 GiB     | 9.81 GiB\r\n'],[],1,1
94,pytorch,17914,closed,Incorrect behaviour of min() and argmin(),"## üêõ Bug

PyTorch's implementation of the argmin() function returns incorrect (maybe rather unexpected results) when using the argmin() and the min() function on tensors with dimensions where the minimal value appears multiple times. PyTorch picks the last(!) index with said value, whereas one would assume that the very first occurrence is reported. This is inconsistent with numpy's, Eigen's, C++ STL etc.

## To Reproduce

PyTorch



## Expected behavior

numpy



## Environment

PyTorch version: 1.0.0
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Fedora release 27 (Twenty Seven)
GCC version: (GCC) 7.3.1 20180712 (Red Hat 7.3.1-6)
CMake version: version 3.11.2

Python version: 3.6
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip3] numpy==1.14.6
[pip3] torch==1.0.0
[conda] Could not collect

## Additional context

See numpy's argmin() documentation: https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmin.html
",,"[""> PyTorch picks the last(!) index with said value, \r\n\r\nThis isn't guaranteed\r\n\r\n> whereas one would assume that the very first occurrence is reported. \r\n\r\nThe assumption isn't reasonable. No where did pytorch ever make promise about which one it returns. It computes the correct argmin/max according to mathematical definition after all."", 'PyTorch does indeed not make any promise about this behaviour and it is indeed mathematically correct as it works now. \r\n\r\nHowever, I am arguing it should behave as suggested. Several frameworks implement it in the way described and therefore users may silently assume that Torch also does.', '@SsnL maybe we should consider adding a note to the docs about this non-determinism to clear any ambiguity.', ""@Markus-Goetz repasting my comment from https://github.com/pytorch/pytorch/issues/17738#issuecomment-471365962\r\n\r\nthe only determinism we aim to have is hashed on device, and for CPU, single-threaded.\r\nEven across different kinds of GPUs, all bets are off.\r\n\r\nWe cannot guarantee cross-device, cross-CPU-type determinism due to severe performance cliffs that'll result from such a constraint.\r\n\r\nFor example, to guarantee that we pick the first argmax (or argmin) element, we have to add an additional pass in our CUDA kernel to sort or order the results, which costs performance.\r\n\r\n> This is inconsistent with numpy's, Eigen's, C++ STL etc.\r\n\r\nIf you see, you have given examples of CPU kernels, where this is easy to guarantee without significant performance regression."", 'Okay I can understand the reasoning behind this. Would it still be possible to maybe document the above assumption in the docstrings as @vishwakftw suggested?', 'yes, documenting in the docstrings as a Note sounds okay.']","['\r\nIn [80]: a = torch.triu(torch.ones((5,5,)))\r\n\r\nIn [81]: a\r\nOut[81]: \r\ntensor([[1., 1., 1., 1., 1.],\r\n        [0., 1., 1., 1., 1.],\r\n        [0., 0., 1., 1., 1.],\r\n        [0., 0., 0., 1., 1.],\r\n        [0., 0., 0., 0., 1.]])\r\n\r\nIn [82]: a.min(dim=0)\r\nOut[82]: (tensor([0., 0., 0., 0., 1.]), tensor([4, 4, 4, 4, 4]))\r\n', '\r\nIn [83]: b = np.triu(np.ones((5,5,)))\r\n\r\nIn [84]: b\r\nOut[84]: \r\narray([[1., 1., 1., 1., 1.],\r\n       [0., 1., 1., 1., 1.],\r\n       [0., 0., 1., 1., 1.],\r\n       [0., 0., 0., 1., 1.],\r\n       [0., 0., 0., 0., 1.]])\r\n\r\nIn [85]: b.min(0), b.argmin(0)\r\nOut[85]: (array([0., 0., 0., 0., 1.]), array([1, 2, 3, 4, 0]))\r\n']",[],1,0
95,pytorch,25878,open,Model weights silently fail to load when model is on different gpu than when it was saved,"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->
 silently fails when the model is saved and loaded on different gpu-devices. As a result, the model remains in its randomly-initialized state.

This does not occur in PyTorch 1.1 but does occur in PyTorch 1.2.
 
## To Reproduce

Steps to reproduce the behavior:

The following code demonstrates this bug:

1. Define functions for creating a simple model. For loading the model and asserting that its loaded weights reflect the weights in the checkpoint



2. Create a model, place on gpu-0 and save its state-dictionary


3. Demonstrate that the model loads as-expected on gpu-0


4. Demonstrate that the model loads as-expected on cpu


5. Demonstrate that the model **fails to load** on gpu-1, and retains its randomly-initialized weights.


## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


PyTorch version: 1.2.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 16.04.6 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: Tesla V100-PCIE-16GB
GPU 1: Tesla V100-PCIE-16GB
GPU 2: Tesla V100-PCIE-16GB
GPU 3: Tesla V100-PCIE-16GB
GPU 4: Tesla V100-PCIE-16GB
GPU 5: Tesla V100-PCIE-16GB
GPU 6: Tesla V100-PCIE-16GB
GPU 7: Tesla V100-PCIE-16GB

Nvidia driver version: 418.67
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.1

Versions of relevant libraries:
[pip] numpy==1.16.4
[pip] torch==1.2.0
[pip] torchvision==0.4.0a0+6b959ee
[conda] blas                      1.0                         mkl  
[conda] mkl                       2019.4                      243  
[conda] mkl-service               2.0.2            py37h7b6447c_0  
[conda] mkl_fft                   1.0.14           py37ha843d7b_0  
[conda] mkl_random                1.0.2            py37hd81dba3_0  
[conda] pytorch                   1.2.0           py3.7_cuda10.0.130_cudnn7.6.2_0    pytorch
[conda] torchvision               0.4.0                    pypi_0    pypi
:

## Additional context

<!-- Add any other context about the problem here. -->


cc @ezyang @gchanan @zou3519",module: serialization needs reproduction triaged,"['Is there a status update on this nefarious bug?', ""i'm bumping the priority on this cc: @gchanan "", 'Could not reproduce on:\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.2.0a0+8554416\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: Quadro GP100\r\nGPU 1: Quadro GP100\r\n\r\nNvidia driver version: 410.79\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] torch==1.2.0a0+8554416\r\n[pip] torchvision==0.4.0\r\n[conda] blas                      1.0                         mkl\r\n[conda] magma-cuda100             2.5.1                         1    pytorch\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-include               2019.4                      243\r\n[conda] mkl-service               2.0.2            py37h7b6447c_0\r\n[conda] mkl_fft                   1.0.14           py37ha843d7b_0\r\n[conda] mkl_random                1.0.2            py37hd81dba3_0\r\n[conda] torch                     1.2.0a0+8554416           dev_0    <develop>\r\n[conda] torchvision               0.4.0                    pypi_0    pypi\r\n```', 'If it is any help, here is a picture of a notebook that I executed in sequence to reproduce this error:\r\n\r\n![image](https://user-images.githubusercontent.com/29104956/68159662-34b7ae00-ff20-11e9-8419-ce67fcd57af4.png)\r\n']","['python\r\nimport torch as tr\r\nimport torch.nn as nn\r\n\r\n\r\ndef make_model():\r\n    return nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)\r\n\r\n\r\ndef load_on_device_and_check(checkpoint, device, verbose=False):\r\n    model = make_model()\r\n    model.to(device)\r\n\r\n    saved_weight = checkpoint[""model_state_dict""][""weight""]\r\n    assert not tr.allclose(saved_weight.cpu(), model.weight.cpu())\r\n    \r\n    if verbose:\r\n        print(f""pre-load:\\n{model.weight.cpu()}"")\r\n        \r\n    model.load_state_dict(checkpoint[""model_state_dict""])\r\n    \r\n    if verbose:\r\n        print(f""post-load:\\n{model.weight.cpu()}"")\r\n    assert tr.allclose(saved_weight.cpu(), model.weight.cpu())\r\n', 'python\r\ndevice = tr.device(""cuda:0"")\r\nmodel = make_model()\r\nmodel.to(device)\r\ntr.save({""model_state_dict"": model.state_dict()}, ""dummy_model.p"")\r\n', 'python\r\n>>> checkpoint = tr.load(""dummy_model.p"")\r\n>>> load_on_device_and_check(checkpoint, tr.device(""cuda:0""))\r\n', 'python\r\n>>> checkpoint = tr.load(""dummy_model.p"")\r\n>>> load_on_device_and_check(checkpoint, tr.device(""cpu""), verbose=True)\r\npre-load:\r\ntensor([[[[-0.1033, -0.3067,  0.0443],\r\n          [-0.1475, -0.1446, -0.0476],\r\n          [ 0.1484,  0.0619, -0.3134]]]], grad_fn=<CopyBackwards>)\r\npost-load:\r\ntensor([[[[ 0.0477,  0.0031, -0.0045],\r\n          [-0.1170, -0.0351,  0.1652],\r\n          [ 0.1713, -0.1888,  0.2063]]]], grad_fn=<CopyBackwards>)\r\n', 'python\r\n>>> checkpoint = tr.load(""dummy_model.p"")\r\n>>> load_on_device_and_check(checkpoint, tr.device(""cuda:1""), verbose=True)\r\npre-load:\r\ntensor([[[[ 0.0328, -0.1487,  0.0571],\r\n          [ 0.0730, -0.1790, -0.1707],\r\n          [-0.1375,  0.2665,  0.2011]]]], grad_fn=<CopyBackwards>)\r\npost-load:\r\ntensor([[[[ 0.0328, -0.1487,  0.0571],\r\n          [ 0.0730, -0.1790, -0.1707],\r\n          [-0.1375,  0.2665,  0.2011]]]], grad_fn=<CopyBackwards>)\r\n\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-7-aa2957eac0e5> in <module>\r\n      1 checkpoint = tr.load(""dummy_model.p"")\r\n----> 2 load_on_device_and_check(checkpoint, tr.device(""cuda:1""), verbose=True)\r\n\r\n<ipython-input-6-eb34bf821c07> in load_on_device_and_check(checkpoint, device, verbose)\r\n     21     if verbose:\r\n     22         print(f""post-load:\\n{model.weight.cpu()}"")\r\n---> 23     assert tr.allclose(saved_weight.cpu(), model.weight.cpu())\r\n\r\nAssertionError: \r\n\r\n\r\n', '\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n']",['model.load_state_dict'],0,0
96,pytorch,27094,open,[jit] Document what types can be traced,"Our  docs don't mention that  can be traced, nor that NamedTuples cannot be traced

cc @suo",jit-backlog oncall: jit triaged,[],[],"['torch.jit.trace', 'Dict[str, Tensor]']",0,0
97,pytorch,20230,open,[jit] torch.tensor doesn't support list of tuples,"## üêõ Bug

Cannot create a tensor using  from a list of tuples.

## To Reproduce

Steps to reproduce the behavior:



gives the following error:


## Expected behavior

Should not give an error. Works in pure python.

## Environment
Pytorch 1.1.0

## Additional context

A workaround is to use a list of lists instead.
",low priority oncall: jit triaged,"[""`list(tup)` is an easy workaround. For context, torch.tensor has a separate compilation mechanism since it's not schematizable""]","['python\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef test():\r\n  li = [(1, 2, 3, 4), (5, 6, 7, 8), (9, 10, 11, 12)]\r\n  return torch.tensor(li)\r\n', '\r\nRuntimeError: \r\nInput list to torch.tensor must be of ints, floats, or bools, got (int, int, int, int):\r\n@torch.jit.script\r\ndef test():\r\n  li = [(1, 2, 3, 4), (5, 6, 7, 8), (9, 10, 11, 12)]\r\n  return torch.tensor(li)\r\n         ~~~~~~~~~~~~ <--- HERE\r\n']",['torch.tensor'],0,0
98,pytorch,18053,open,cuDNN error when using 3d convolutions,"## üêõ Bug

The following simple script: https://gist.github.com/vlasenkov/b3aa7c12570fe0056fca3421453470ca crashes with the following traceback:




## To Reproduce

Run the code above on single GPU.

## Expected behavior

The script just successfully finishes.

## Environment

",module: cudnn module: dependency bug triaged,"[""This is probably not an interpolate bug, because interpolate with trilinear doesn't call cuDNN. This looks like a Conv3d bug, cc @mruberry \r\n\r\nIt would be really helpful to get a minimal reproduction. Would it be possible for you to remove layers from your network while keeping the bug reproducing? Otherwise one of us can try."", 'I have updated the gist. I removed as much as was possible. Removing more layers makes the error disappear.', 'We should be able to take a look at this next week (GTC is this week). ', 'A workaround is to use `torch.backends.cudnn.benchmark=True`', 'Ahah)) When I do `torch.backends.cudnn.benchmark=True` I get:\r\n```\r\nRuntimeError: no deterministic convolution algorithms available in CuDNN\r\n```\r\nI have to use the deterministic mode because memory consumption is large otherwise. Guys, why CuDNN is so buggy?', 'Thanks for the report. We are reviewing this issue. ', '@vlasenkov I can repro the error with binary packages that use cudnn 7.4.2, but I cannot repro it with source builds with cudnn 7.5. Can you try building from source with cudnn 7.5 and see if it fixes your error? Unfortunately, binary packages statically link to cudnn, so you cannot drop and replace cudnn version in binaries. ', ""Once we update the nightlies to 7.5 and confirm it doesn't repro, we'll close the bug.""]","['\r\nTraceback (most recent call last):\r\n  File ""models/unet/test.py"", line 126, in <module>\r\n    loss.backward()\r\n  File ""/opt/conda/lib/python3.6/site-packages/torch/tensor.py"", line 102, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File ""/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py"", line 90, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED\r\n', '\r\nCollecting environment information...\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: \r\nGPU 0: Tesla V100-DGXS-32GB\r\nGPU 1: Tesla V100-DGXS-32GB\r\nGPU 2: Tesla V100-DGXS-32GB\r\nGPU 3: Tesla V100-DGXS-32GB\r\n\r\nNvidia driver version: 410.79\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.1\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] blas                      1.0                         mkl  \r\n[conda] magma-cuda100             2.1.0                         5    local\r\n[conda] mkl                       2019.0                      118  \r\n[conda] mkl-include               2019.0                      118  \r\n[conda] mkl_fft                   1.0.6            py36h7dd41cf_0  \r\n[conda] mkl_random                1.0.1            py36h4414c95_1  \r\n[conda] pytorch                   1.0.1           py3.6_cuda10.0.130_cudnn7.4.2_2    pytorch\r\n[conda] torch                     1.0.0a0                  pypi_0    pypi\r\n[conda] torchtext                 0.3.0                    pypi_0    pypi\r\n[conda] torchvision               0.2.1                    pypi_0    pypi\r\n']",[],0,0
99,pytorch,25150,open,Problems with install python from source,"## üêõ Bug

For Pytorch 1.3.0 a from source installation is necessary, if one only have cuda 9.0. I cant update the version, because I do not have root access.

## To Reproduce
I have done everything what they say in the tutorial. The result is the following output of CMakerError:
main':
CheckSymbolExists.c:(.text+0x1b): undefined reference to 

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

PyTorch version: From Source
Is debug build: N/A
CUDA used to build PyTorch: 9.0

OS: Debian GNU/Linux 9.9 (stretch)
GCC version: (Debian 6.3.0-18+deb9u1) 6.3.0 20170516
CMake version: version 3.14.0

Python version: 3.7
Is CUDA available: yes
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce GTX 1080
GPU 1: NVS 310

Nvidia driver version: 384.111
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.17.0
[pip3] torch==1.2.0
[pip3] torchvision==0.4.0
[conda] blas                      1.0                         mkl  
[conda] magma-cuda90              2.5.0                         1    pytorch
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] mkl-service               2.0.2            py37h7b6447c_0  
[conda] mkl_fft                   1.0.14           py37ha843d7b_0  
[conda] mkl_random                1.0.2            py37hd81dba3_0

## Additional context

<!-- Add any other context about the problem here. -->
",module: build triaged,"['Hey @BioFreak95, which tutorial are you following? And when you say v1.3.0 you mean the master branch?\r\n\r\nCC @ezyang ', 'Hey. The tutorial on pytorch.org for a ""From source"" installation. And yes, the master branch.', ""This is the `-lpthread` versus `-pthread` thing. I can't find the preexisting cmake issue, but see https://stackoverflow.com/questions/1662909/undefined-reference-to-pthread-create-in-linux Try updating your cmake version."", 'Updating does not help. Get the same Error. \r\nDid you have an another idea?', ""Can you read broukema's comment at https://github.com/pytorch/pytorch/issues/9310#issuecomment-524055527 and see if that works for you."", '`make 2>&1 |tee log.1` does not show any missing package. \r\n\r\nNevertheless. Additionally I have in conda cmake 3.14 and locally I have installed cmake 3.15.2 -> also does not help.\r\n\r\n`aptitude install libtbb-dev` does not work, because I do not have root access.', '@BioFreak95 were you able to remedy this?', 'Getting the same error with PyTorch 1.6 in Ubuntu 18.04 x86-64 using latest cmake from the kitware repo (3.18) and the dependencies listed [here](https://github.com/pytorch/pytorch/blob/v1.6.0/.circleci/docker/common/install_base.sh).  Trying to build outside of conda with python3.\r\n\r\nUpdate:  I deployed the dependencies listed [here](https://github.com/pytorch/pytorch/blob/v1.6.0/.circleci/docker/common/install_base.sh#L23), did a fresh checkout, and it appears to have built.\r\n\r\nUpdate2:  This seems to be some sort of race condition, for I get it sometimes, but not others, eventually I can clear it by cleaning the development environment and checking out a fresh copy of the source.', '@BioFreak95 I am seeing the same thing as well, did you figure out how to resolve this?', ""It seems that the clang settings are seeping into gcc. That's what the error messages seem to indicate.""]",[],"['', '\r\nPerforming C++ SOURCE FILE Test CAFFE2_NEED_TO_TURN_OFF_DEPRECATION_WARNING failed with the following output:\r\nChange Dir: /group/ag_cmb/scratch/msalomon#/pytorch/build/CMakeFiles/CMakeTmp\r\n\r\nRun Build Command(s):/group/ag_cmb/scratch/msalomon#/anaconda3/envs/pytorch-build/bin/ninja cmTC_b8375 \r\n[1/2] Building CXX object CMakeFiles/cmTC_b8375.dir/src.cxx.o\r\nFAILED: CMakeFiles/cmTC_b8375.dir/src.cxx.o \r\n/usr/bin/c++    -DCAFFE2_NEED_TO_TURN_OFF_DEPRECATION_WARNING -std=c++11 -o CMakeFiles/cmTC_b8375.dir/src.cxx.o -c src.cxx\r\nsrc.cxx:1:30: fatal error: glog/stl_logging.h: Datei oder Verzeichnis nicht gefunden\r\n #include <glog/stl_logging.h>\r\n                              ^\r\ncompilation terminated.\r\nninja: build stopped: subcommand failed.\r\n\r\nSource file was:\r\n#include <glog/stl_logging.h>\r\n    int main(int argc, char** argv) {\r\n      return 0;\r\n    }\r\nDetermining if the pthread_create exist failed with the following output:\r\nChange Dir: /group/ag_cmb/scratch/msalomon#/pytorch/build/CMakeFiles/CMakeTmp\r\n\r\nRun Build Command(s):/group/ag_cmb/scratch/msalomon#/anaconda3/envs/pytorch-build/bin/ninja cmTC_7fcd6 \r\n[1/2] Building C object CMakeFiles/cmTC_7fcd6.dir/CheckSymbolExists.c.o\r\n[2/2] Linking C executable cmTC_7fcd6\r\nFAILED: cmTC_7fcd6 \r\n: && /usr/bin/gcc   -rdynamic CMakeFiles/cmTC_7fcd6.dir/CheckSymbolExists.c.o  -o cmTC_7fcd6   && :\r\nCMakeFiles/cmTC_7fcd6.dir/CheckSymbolExists.c.o: In function ', 'pthread_create\'\r\ncollect2: error: ld returned 1 exit status\r\nninja: build stopped: subcommand failed.\r\n\r\nFile /group/ag_cmb/scratch/msalomon#/pytorch/build/CMakeFiles/CMakeTmp/CheckSymbolExists.c:\r\n/* */\r\n#include <pthread.h>\r\n\r\nint main(int argc, char** argv)\r\n{\r\n  (void)argv;\r\n#ifndef pthread_create\r\n  return ((int*)(&pthread_create))[argc];\r\n#else\r\n  (void)argc;\r\n  return 0;\r\n#endif\r\n}\r\n\r\nDetermining if the function pthread_create exists in the pthreads failed with the following output:\r\nChange Dir: /group/ag_cmb/scratch/msalomon#/pytorch/build/CMakeFiles/CMakeTmp\r\n\r\nRun Build Command(s):/group/ag_cmb/scratch/msalomon#/anaconda3/envs/pytorch-build/bin/ninja cmTC_95667 \r\n[1/2] Building C object CMakeFiles/cmTC_95667.dir/CheckFunctionExists.c.o\r\n[2/2] Linking C executable cmTC_95667\r\nFAILED: cmTC_95667 \r\n: && /usr/bin/gcc -DCHECK_FUNCTION_EXISTS=pthread_create  -rdynamic CMakeFiles/cmTC_95667.dir/CheckFunctionExists.c.o  -o cmTC_95667  -lpthreads && :\r\n/usr/bin/ld: cannot find -lpthreads\r\ncollect2: error: ld returned 1 exit status\r\nninja: build stopped: subcommand failed.\r\n\r\n\r\nPerforming C SOURCE FILE Test NNPACK_ARCH_IS_X86_32 failed with the following output:\r\nChange Dir: /group/ag_cmb/scratch/msalomon#/pytorch/build/CMakeFiles/CMakeTmp\r\n\r\nRun Build Command(s):/group/ag_cmb/scratch/msalomon#/anaconda3/envs/pytorch-build/bin/ninja cmTC_94e41 \r\n[1/2] Building C object CMakeFiles/cmTC_94e41.dir/src.c.o\r\nFAILED: CMakeFiles/cmTC_94e41.dir/src.c.o \r\n/usr/bin/gcc   -DNNPACK_ARCH_IS_X86_32 -o CMakeFiles/cmTC_94e41.dir/src.c.o   -c src.c\r\nsrc.c:3:10: error: #error AVX only on x86_64\r\n         #error AVX only on x86_64\r\n          ^~~~~\r\nninja: build stopped: subcommand failed.\r\n\r\nSource file was:\r\n\r\n      #if ! (defined(__i386) || defined(_M_IX86))\r\n        #error AVX only on x86_64\r\n      #endif\r\n      int main() {\r\n        return 0;\r\n      }\r\nPerforming C++ SOURCE FILE Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32 failed with the following output:\r\nChange Dir: /group/ag_cmb/scratch/msalomon#/pytorch/build/CMakeFiles/CMakeTmp\r\n\r\nRun Build Command(s):/group/ag_cmb/scratch/msalomon#/anaconda3/envs/pytorch-build/bin/ninja cmTC_c20e9 \r\n[1/2] Building CXX object CMakeFiles/cmTC_c20e9.dir/src.cxx.o\r\nFAILED: CMakeFiles/cmTC_c20e9.dir/src.cxx.o \r\n/usr/bin/c++    -Wno-deprecated -fvisibility-inlines-hidden  -std=c++11  -Wall  -Wextra  -Wshadow  -pedantic  -pedantic-errors -DHAVE_CXX_FLAG_WSHORTEN_64_TO_32  -Wshorten-64-to-32   -Wshorten-64-to-32 -o CMakeFiles/cmTC_c20e9.dir/src.cxx.o -c src.cxx\r\nc++: error: unrecognized command line option \'-Wshorten-64-to-32\'\r\nc++: error: unrecognized command line option \'-Wshorten-64-to-32\'\r\nninja: build stopped: subcommand failed.\r\n\r\nSource file was:\r\nint main() { return 0; }\r\nPerforming C++ SOURCE FILE Test HAVE_CXX_FLAG_WD654 failed with the following output:\r\nChange Dir: /group/ag_cmb/scratch/msalomon#/pytorch/build/CMakeFiles/CMakeTmp\r\n\r\nRun Build Command(s):/group/ag_cmb/scratch/msalomon#/anaconda3/envs/pytorch-build/bin/ninja cmTC_bfc93 \r\n[1/2] Building CXX object CMakeFiles/cmTC_bfc93.dir/src.cxx.o\r\nFAILED: CMakeFiles/cmTC_bfc93.dir/src.cxx.o \r\n/usr/bin/c++    -Wno-deprecated -fvisibility-inlines-hidden  -std=c++11  -Wall  -Wextra  -Wshadow  -pedantic  -pedantic-errors  -Wfloat-equal  -fstrict-aliasing  -Wno-deprecated-declarations  -Wstrict-aliasing -DHAVE_CXX_FLAG_WD654  -wd654   -wd654 -o CMakeFiles/cmTC_bfc93.dir/src.cxx.o -c src.cxx\r\nc++: error: unrecognized command line option \'-wd654\'\r\nc++: error: unrecognized command line option \'-wd654\'\r\nninja: build stopped: subcommand failed.\r\n\r\nSource file was:\r\nint main() { return 0; }\r\nPerforming C++ SOURCE FILE Test HAVE_CXX_FLAG_WTHREAD_SAFETY failed with the following output:\r\nChange Dir: /group/ag_cmb/scratch/msalomon#/pytorch/build/CMakeFiles/CMakeTmp\r\n\r\nRun Build Command(s):/group/ag_cmb/scratch/msalomon#/anaconda3/envs/pytorch-build/bin/ninja cmTC_9f5d6 \r\n[1/2] Building CXX object CMakeFiles/cmTC_9f5d6.dir/src.cxx.o\r\nFAILED: CMakeFiles/cmTC_9f5d6.dir/src.cxx.o \r\n/usr/bin/c++    -Wno-deprecated -fvisibility-inlines-hidden  -std=c++11  -Wall  -Wextra  -Wshadow  -pedantic  -pedantic-errors  -Wfloat-equal  -fstrict-aliasing  -Wno-deprecated-declarations  -Wstrict-aliasing -DHAVE_CXX_FLAG_WTHREAD_SAFETY  -Wthread-safety   -Wthread-safety -o CMakeFiles/cmTC_9f5d6.dir/src.cxx.o -c src.cxx\r\nc++: error: unrecognized command line option \'-Wthread-safety\'; did you mean \'-fthread-jumps\'?\r\nc++: error: unrecognized command line option \'-Wthread-safety\'; did you mean \'-fthread-jumps\'?\r\nninja: build stopped: subcommand failed.\r\n\r\nSource file was:\r\nint main() { return 0; }\r\nPerforming C SOURCE FILE Test C_HAS_AVX_1 failed with the following output:\r\nChange Dir: /group/ag_cmb/scratch/msalomon#/pytorch/build/CMakeFiles/CMakeTmp\r\n\r\nRun Build Command(s):/group/ag_cmb/scratch/msalomon#/anaconda3/envs/pytorch-build/bin/ninja cmTC_b6753 \r\n[1/2] Building C object CMakeFiles/cmTC_b6753.dir/src.c.o\r\nFAILED: CMakeFiles/cmTC_b6753.dir/src.c.o \r\n/usr/bin/gcc  -I""/group/ag_cmb/scratch/msalomon#/anaconda3/envs/pytorch-build/include"" -I/usr/local/cuda/include -fopenmp -DC_HAS_AVX_1 -fPIE -o CMakeFiles/cmTC_b6753.dir/src.c.o   -c src.c\r\nsrc.c: In function ‚Äòmain‚Äô:\r\nsrc.c:7:7: warning: AVX vector return without AVX enabled changes the ABI [-Wpsabi]\r\n     a = _mm256_set1_ps(0);\r\n     ~~^~~~~~~~~~~~~~~~~~~\r\nIn file included from /usr/lib/gcc/x86_64-linux-gnu/6/include/immintrin.h:41:0,\r\n                 from src.c:2:\r\n/usr/lib/gcc/x86_64-linux-gnu/6/include/avxintrin.h:1285:1: error: inlining failed in call to always_inline ‚Äò_mm256_set1_ps‚Äô: target specific option mismatch\r\n _mm256_set1_ps (float __A)\r\n ^~~~~~~~~~~~~~\r\nsrc.c:7:7: note: called from here\r\n     a = _mm256_set1_ps(0);\r\n     ~~^~~~~~~~~~~~~~~~~~~\r\nninja: build stopped: subcommand failed.\r\n\r\nSource file was:\r\n\r\n  #include <immintrin.h>\r\n\r\n  int main()\r\n  {\r\n    __m256 a;\r\n    a = _mm256_set1_ps(0);\r\n    return 0;\r\n  }\r\n\r\nPerforming C SOURCE FILE Test C_HAS_AVX2_1 failed with the following output:\r\nChange Dir: /group/ag_cmb/scratch/msalomon#/pytorch/build/CMakeFiles/CMakeTmp\r\n\r\nRun Build Command(s):/group/ag_cmb/scratch/msalomon#/anaconda3/envs/pytorch-build/bin/ninja cmTC_0bc98 \r\n[1/2] Building C object CMakeFiles/cmTC_0bc98.dir/src.c.o\r\nFAILED: CMakeFiles/cmTC_0bc98.dir/src.c.o \r\n/usr/bin/gcc  -I""/group/ag_cmb/scratch/msalomon#/anaconda3/envs/pytorch-build/include"" -I/usr/local/cuda/include -fopenmp -DC_HAS_AVX2_1 -fPIE -o CMakeFiles/cmTC_0bc98.dir/src.c.o   -c src.c\r\nsrc.c: In function ‚Äòmain‚Äô:\r\nsrc.c:7:7: warning: AVX vector return without AVX enabled changes the ABI [-Wpsabi]\r\n     a = _mm256_abs_epi16(a);\r\n     ~~^~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /usr/lib/gcc/x86_64-linux-gnu/6/include/immintrin.h:43:0,\r\n                 from src.c:2:\r\n/usr/lib/gcc/x86_64-linux-gnu/6/include/avx2intrin.h:63:1: error: inlining failed in call to always_inline ‚Äò_mm256_abs_epi16‚Äô: target specific option mismatch\r\n _mm256_abs_epi16 (__m256i __A)\r\n ^~~~~~~~~~~~~~~~\r\nsrc.c:7:7: note: called from here\r\n     a = _mm256_abs_epi16(a);\r\n     ~~^~~~~~~~~~~~~~~~~~~~~\r\nninja: build stopped: subcommand failed.\r\n\r\nSource file was:\r\n\r\n  #include <immintrin.h>\r\n\r\n  int main()\r\n  {\r\n    __m256i a = {0};\r\n    a = _mm256_abs_epi16(a);\r\n    __m256i x;\r\n    _mm256_extract_epi64(x, 0); // we rely on this in our AVX2 code\r\n    return 0;\r\n  }\r\n\r\nPerforming C SOURCE FILE Test CXX_HAS_AVX_1 failed with the following output:\r\nChange Dir: /group/ag_cmb/scratch/msalomon#/pytorch/build/CMakeFiles/CMakeTmp\r\n\r\nRun Build Command(s):/group/ag_cmb/scratch/msalomon#/anaconda3/envs/pytorch-build/bin/ninja cmTC_dcabd \r\n[1/2] Building C object CMakeFiles/cmTC_dcabd.dir/src.c.o\r\nFAILED: CMakeFiles/cmTC_dcabd.dir/src.c.o \r\n/usr/bin/gcc  -I""/group/ag_cmb/scratch/msalomon#/anaconda3/envs/pytorch-build/include"" -I/usr/local/cuda/include -fopenmp -DCXX_HAS_AVX_1 -fPIE -o CMakeFiles/cmTC_dcabd.dir/src.c.o   -c src.c\r\nsrc.c: In function ‚Äòmain‚Äô:\r\nsrc.c:7:7: warning: AVX vector return without AVX enabled changes the ABI [-Wpsabi]\r\n     a = _mm256_set1_ps(0);\r\n     ~~^~~~~~~~~~~~~~~~~~~\r\nIn file included from /usr/lib/gcc/x86_64-linux-gnu/6/include/immintrin.h:41:0,\r\n                 from src.c:2:\r\n/usr/lib/gcc/x86_64-linux-gnu/6/include/avxintrin.h:1285:1: error: inlining failed in call to always_inline ‚Äò_mm256_set1_ps‚Äô: target specific option mismatch\r\n _mm256_set1_ps (float __A)\r\n ^~~~~~~~~~~~~~\r\nsrc.c:7:7: note: called from here\r\n     a = _mm256_set1_ps(0);\r\n     ~~^~~~~~~~~~~~~~~~~~~\r\nninja: build stopped: subcommand failed.\r\n\r\nSource file was:\r\n\r\n  #include <immintrin.h>\r\n\r\n  int main()\r\n  {\r\n    __m256 a;\r\n    a = _mm256_set1_ps(0);\r\n    return 0;\r\n  }\r\n\r\nPerforming C SOURCE FILE Test CXX_HAS_AVX2_1 failed with the following output:\r\nChange Dir: /group/ag_cmb/scratch/msalomon#/pytorch/build/CMakeFiles/CMakeTmp\r\n\r\nRun Build Command(s):/group/ag_cmb/scratch/msalomon#/anaconda3/envs/pytorch-build/bin/ninja cmTC_b0cad \r\n[1/2] Building C object CMakeFiles/cmTC_b0cad.dir/src.c.o\r\nFAILED: CMakeFiles/cmTC_b0cad.dir/src.c.o \r\n/usr/bin/gcc  -I""/group/ag_cmb/scratch/msalomon#/anaconda3/envs/pytorch-build/include"" -I/usr/local/cuda/include -fopenmp -DCXX_HAS_AVX2_1 -fPIE -o CMakeFiles/cmTC_b0cad.dir/src.c.o   -c src.c\r\nsrc.c: In function ‚Äòmain‚Äô:\r\nsrc.c:7:7: warning: AVX vector return without AVX enabled changes the ABI [-Wpsabi]\r\n     a = _mm256_abs_epi16(a);\r\n     ~~^~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /usr/lib/gcc/x86_64-linux-gnu/6/include/immintrin.h:43:0,\r\n                 from src.c:2:\r\n/usr/lib/gcc/x86_64-linux-gnu/6/include/avx2intrin.h:63:1: error: inlining failed in call to always_inline ‚Äò_mm256_abs_epi16‚Äô: target specific option mismatch\r\n _mm256_abs_epi16 (__m256i __A)\r\n ^~~~~~~~~~~~~~~~\r\nsrc.c:7:7: note: called from here\r\n     a = _mm256_abs_epi16(a);\r\n     ~~^~~~~~~~~~~~~~~~~~~~~\r\nninja: build stopped: subcommand failed.\r\n\r\nSource file was:\r\n\r\n  #include <immintrin.h>\r\n\r\n  int main()\r\n  {\r\n    __m256i a = {0};\r\n    a = _mm256_abs_epi16(a);\r\n    __m256i x;\r\n    _mm256_extract_epi64(x, 0); // we rely on this in our AVX2 code\r\n    return 0;\r\n  }\r\n\r\nPerforming C SOURCE FILE Test BLAS_F2C_DOUBLE_WORKS failed with the following compile output:\r\nChange Dir: /group/ag_cmb/scratch/msalomon#/pytorch/build/CMakeFiles/CMakeTmp\r\n\r\nRun Build Command(s):/group/ag_cmb/scratch/msalomon#/anaconda3/envs/pytorch-build/bin/ninja cmTC_6dd89 \r\n[1/2] Building C object CMakeFiles/cmTC_6dd89.dir/src.c.o\r\n[2/2] Linking C executable cmTC_6dd89\r\n\r\n...and run output:\r\n\r\nReturn value: 1\r\nSource file was:\r\n\r\n#include <stdlib.h>\r\n#include <stdio.h>\r\nfloat x[4] = { 1, 2, 3, 4 };\r\nfloat y[4] = { .1, .01, .001, .0001 };\r\nint four = 4;\r\nint one = 1;\r\nextern double sdot_();\r\nint main() {\r\n  int i;\r\n  double r = sdot_(&four, x, &one, y, &one);\r\n  exit((float)r != (float).1234);\r\n}\r\nPerforming C SOURCE FILE Test COMPILER_SUPPORTS_SVE failed with the following output:\r\nChange Dir: /group/ag_cmb/scratch/msalomon#/pytorch/build/CMakeFiles/CMakeTmp\r\n\r\nRun Build Command(s):/group/ag_cmb/scratch/msalomon#/anaconda3/envs/pytorch-build/bin/ninja cmTC_feaf1 \r\n[1/2] Building C object CMakeFiles/cmTC_feaf1.dir/src.c.o\r\nFAILED: CMakeFiles/cmTC_feaf1.dir/src.c.o \r\n/usr/bin/gcc  -I""/group/ag_cmb/scratch/msalomon#/anaconda3/envs/pytorch-build/include"" -I/usr/local/cuda/include -fopenmp -Wno-ignored-qualifiers -Wno-absolute-value -DCOMPILER_SUPPORTS_SVE -march=armv8-a+sve -fPIE -o CMakeFiles/cmTC_feaf1.dir/src.c.o   -c src.c\r\nsrc.c:1:0: error: bad value (armv8-a+sve) for -march= switch\r\n \r\n \r\ncc1: warning: unrecognized command line option ‚Äò-Wno-absolute-value‚Äô\r\nninja: build stopped: subcommand failed.\r\n\r\nSource file was:\r\n\r\n  #include <arm_sve.h>\r\n  int main() {\r\n    svint32_t r = svdup_n_s32(1); }\r\nPerforming C SOURCE FILE Test COMPILER_SUPPORTS_AVX512F failed with the following output:\r\nChange Dir: /group/ag_cmb/scratch/msalomon#/pytorch/build/CMakeFiles/CMakeTmp\r\n\r\nRun Build Command(s):/group/ag_cmb/scratch/msalomon#/anaconda3/envs/pytorch-build/bin/ninja cmTC_1782d \r\n[1/2] Building C object CMakeFiles/cmTC_1782d.dir/src.c.o\r\nFAILED: CMakeFiles/cmTC_1782d.dir/src.c.o \r\n/usr/bin/gcc  -I""/group/ag_cmb/scratch/msalomon#/anaconda3/envs/pytorch-build/include"" -I/usr/local/cuda/include -fopenmp -Wno-ignored-qualifiers -Wno-absolute-value -DCOMPILER_SUPPORTS_AVX512F -mavx512f -fPIE -o CMakeFiles/cmTC_1782d.dir/src.c.o   -c src.c\r\nsrc.c: In function ‚Äòmain‚Äô:\r\nsrc.c:14:5: error: unrecognizable insn:\r\n     __m512i r = _mm512_andnot_si512(a, a); }\r\n     ^~~~~~~\r\n(insn 22 21 23 2 (set (mem/c:V4DI (plus:DI (reg/f:DI 82 virtual-stack-vars)\r\n                (const_int -96 [0xffffffffffffffa0])) [3 ymm+0 S32 A256])\r\n        (vec_merge:V4DI (vec_select:V4DI (reg:V8DI 103)\r\n                (parallel [\r\n                        (const_int 0 [0])\r\n                        (const_int 1 [0x1])\r\n                        (const_int 2 [0x2])\r\n                        (const_int 3 [0x3])\r\n                    ]))\r\n            (reg:V4DI 104)\r\n            (reg:QI 105))) src.c:12 -1\r\n     (nil))\r\nsrc.c:14:5: internal compiler error: in extract_insn, at recog.c:2287\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-6/README.Bugs> for instructions.\r\nninja: build stopped: subcommand failed.\r\n\r\nSource file was:\r\n\r\n  #if defined(_MSC_VER)\r\n  #include <intrin.h>\r\n  #else\r\n  #include <x86intrin.h>\r\n  #endif\r\n  __m512 addConstant(__m512 arg) {\r\n    return _mm512_add_ps(arg, _mm512_set1_ps(1.f));\r\n  }\r\n  int main() {\r\n    __m512i a = _mm512_set1_epi32(1);\r\n    __m256i ymm = _mm512_extracti64x4_epi64(a, 0);\r\n    __mmask16 m = _mm512_cmp_epi32_mask(a, a, _MM_CMPINT_EQ);\r\n    __m512i r = _mm512_andnot_si512(a, a); }\r\n\r\n', '']",0,0
100,pytorch,19149,open,Make operators like logsumexp and cumsum operate over dimension 0 by default (or at least for 1D arrays),"It's pretty ridiculous that, for a 1D input like ,  still requires you to specify you're operating over dimension 0.

In particular  will error, saying that it needs a dim argument.  works.

My suggestion is that, at the very least for 1D inputs, the default dim you operate over is set to 0. This is a problem for  and , just to name two.

cc @jlin27 @mruberry @rgommers",enhancement module: docs module: numpy module: reductions triaged,"['this is fair, we should do it.', 'We should remember to update our documentation for this change.', ""Any thoughts on, for non-1D tensors, what the default dimension should be (if there is one)? There's arguments for 0 and -1 as the defaults. For the record, numpy and scipy default to `None`, which indicate operating over every last entry in every last dimension. For `cumsum`, this means operating over the flattened array."", ""NumPy's behavior seems reasonable.""]",[],"['x = torch.rand(5)', 'torch.logsumexp', 'torch.logsumexp(x)', 'torch.logsumexp(x, 0)', 'logsumexp', 'cumsum']",0,0
101,pytorch,16527,open,Delete _cudart from torch/cuda/__init__.py,"_cudart is the Python ctypes binding to the libcudart.so library. It was previously used for Streams and Events but is now unused. The initialization (in _load_cudart) can sometimes fail. For example, @zou3519 ran into:

",,"['btw, `_cudart` is used by `test_cuda_primary_ctx.py` https://github.com/pytorch/pytorch/blob/ffed8bff6a1df334e371957009dd77af0c5140dd/test/test_cuda_primary_ctx.py.', ""[this is a better fix](https://github.com/pytorch/pytorch/commit/f748654e0e070f9e4c635ffc295d694530bb45cd), that I'll cherry-pick onto master, if we want to keep cudart around.\r\nBut if we can indeed delete it, that's double awesome!""]","[""\r\nRuntimeError: couldn't find libcudart. Make sure CUDA libraries are installed in a default location,\r\nor that they're in LD_LIBRARY_PATH.\r\n""]",[],0,0
102,pytorch,7342,open,[feature request] [PyTorch] More flexible optimizer API,"Currently, there is no easy way to change the decay / momentum / lr for a parameter after constructing the optimizer. For example, it is hard to stop a parameter from being decayed in an iteration.

Maybe we should support a getter/setter API like .

cc @vincentqb",enhancement module: optimizer triaged,"['Does it set it in all parameter groups? What if you want to keep them different, but e.g. scale them? Or modify each differently?', 'Another point that was raised to me by @ir413 : enabling to selectively change the variables that are optimized at every `step`, while retaining momentum etc.\r\nA possibility for that would be to let the `step` accept a list of parameters as well, and use the `id` of the parameter to get the corresponding momentum buffer in the optimizer.', ""@apaszke `optim.set(X, 'decay', 0)` only sets the `decay` of `X`, where `X` can be a parameter or a collection of parameters."", 'Ok, I think that if we had an invariant that we would always match `X` to the existing parameter groups then it would be ok. A slightly nicer syntax might be `optimizer[params].decay = 0`', 'Could we also allow if `X` is only a subset of an existing parameter group? Not many people use more than 1 parameter groups afaik. ', 'That would need to split an existing parameter group, and we definitely shouldn‚Äôt allow that']",[],"[""optim.set(my_parameter, 'decay', 0)""]",0,0
103,pytorch,13618,closed,test_dist_broadcast_coalesced_nccl timeout on CI,,,['Closing due to age'],"['\r\nNov 06 05:00:37 ======================================================================\r\nNov 06 05:00:37 ERROR: test_dist_broadcast_coalesced_nccl (__main__.DistributedDataParallelTest)\r\nNov 06 05:00:37 ----------------------------------------------------------------------\r\nNov 06 05:00:37 Traceback (most recent call last):\r\nNov 06 05:00:37   File ""test_c10d.py"", line 349, in wrapper\r\nNov 06 05:00:37     self._join_processes(fn)\r\nNov 06 05:00:37   File ""test_c10d.py"", line 395, in _join_processes\r\nNov 06 05:00:37     self._check_return_codes(elapsed_time)\r\nNov 06 05:00:37   File ""test_c10d.py"", line 405, in _check_return_codes\r\nNov 06 05:00:37     raise RuntimeError(\'Process {} terminated or timed out after {} seconds\'.format(i, elapsed_time))\r\nNov 06 05:00:37 RuntimeError: Process 0 terminated or timed out after 30.030466556549072 seconds\r\n']",[],0,0
104,pytorch,27421,open,[dataloader] Sampler abstract constructor API minor proposal,"Currently the base abstract Sampler class [has](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/sampler.py#L17):


The  is declared as a common argument but not saved to . Many samplers that do take  as argument indeed do that. And there exist samplers that do not even take  as argument. Two proposed suggestions: 1) add  to the abstract class; 2) remove  method from the base abstract class completely

cc @SsnL",module: dataloader triaged,"['I think it may be more general to change it to \r\n```py\r\ndef __init__(self, data_source=None):\r\n  if data_source is not None:\r\n    self.data_source = data_source\r\n```\r\n\r\nWhat do you think?', ""I think it's quite strange to create a field only if the passed object is not None. If the derived class does not want to have any data_source passed, they will bypass the base class constructor entirely (or do you want to have them call it anyway, but passing no data_source?)\r\n\r\nAlso most frequently the data_source object is of type Dataset, so discrepancy between a more generic `data_source` and a most frequent impl `dataset` is mildly confusing. This is to discuss a bit naming `data_source` with a proposal to shift it to `dataset`, but probably this is a bc-breaking change.""]","['python\r\ndef __init__(self, data_source):\r\n  pass\r\n']","['data_source', 'self.data_source', 'data_source', 'data_source', 'self.data_source = data_source', '__init__']",0,0
105,pytorch,22120,open,convert pth to onnx,"## üêõ Bug


print(torch.__version__)
1.1.0

print(torch.version.cuda)
9.0.176

OS: linux 18.04

 - How you installed PyTorch (, , source): i try both of them, and try to compile from source.
 - Python version: 3.6
 - CUDA/cuDNN version: 7.6.0

Hi, im try to convert .pth weights from yolact project to onnx, im running the following code:



and get the following error:
",module: onnx oncall: jit triaged,"['Seems to be JIT tracing related', 'cc: @houseroad ', '@VitalyFedyunin @ailzhang @sdimantsd Were you guys able to solve the .pth to onnx conversion error which @sdimantsd was facing?\r\nI am also facing the same issue. Could you please help?']","['python\r\nweights_path = \'/home/ws/DL/yolact/weights/yolact_im700_54_800000.pth\'\r\n\r\nimport torch\r\nimport torch.onnx\r\nimport yolact\r\nimport torchvision\r\n\r\nmodel = yolact.Yolact()\r\n\r\n# state_dict = torch.load(weights_path)\r\n# model.load_state_dict(state_dict)\r\n\r\nmodel.load_weights(weights_path)\r\n\r\ndummy_input = torch.randn(1, 3, 640, 480)\r\n\r\ntorch.onnx.export(model, dummy_input, ""onnx_model_name.onnx"")\r\n', '\r\n/home/ws/DL/yolact/yolact.py:256: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can\'t record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  for j, i in product(range(conv_h), range(conv_w)):\r\n/home/ws/DL/yolact/yolact.py:279: TracerWarning: torch.Tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\r\n  self.priors = torch.Tensor(prior_data).view(-1, 4)\r\n/home/ws/DL/yolact/yolact.py:279: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can\'t record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  self.priors = torch.Tensor(prior_data).view(-1, 4)\r\n/home/ws/DL/yolact/layers/functions/detection.py:74: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can\'t record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  for batch_idx in range(batch_size):\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-2-a796dc0eef97> in <module>\r\n     13 dummy_input = torch.randn(1, 3, 700, 700)\r\n     14 \r\n---> 15 torch.onnx.export(model, dummy_input, ""onnx_model_name.onnx"")\r\n\r\n~/.local/lib/python3.6/site-packages/torch/onnx/__init__.py in export(*args, **kwargs)\r\n     23 def export(*args, **kwargs):\r\n     24     from torch.onnx import utils\r\n---> 25     return utils.export(*args, **kwargs)\r\n     26 \r\n     27 \r\n\r\n~/.local/lib/python3.6/site-packages/torch/onnx/utils.py in export(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, strip_doc_string)\r\n    129             operator_export_type=operator_export_type, opset_version=opset_version,\r\n    130             _retain_param_name=_retain_param_name, do_constant_folding=do_constant_folding,\r\n--> 131             strip_doc_string=strip_doc_string)\r\n    132 \r\n    133 \r\n\r\n~/.local/lib/python3.6/site-packages/torch/onnx/utils.py in _export(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, example_outputs, propagate, opset_version, _retain_param_name, do_constant_folding, strip_doc_string)\r\n    361                                                         output_names, operator_export_type,\r\n    362                                                         example_outputs, propagate,\r\n--> 363                                                         _retain_param_name, do_constant_folding)\r\n    364 \r\n    365         # TODO: Don\'t allocate a in-memory string for the protobuf\r\n\r\n~/.local/lib/python3.6/site-packages/torch/onnx/utils.py in _model_to_graph(model, args, verbose, training, input_names, output_names, operator_export_type, example_outputs, propagate, _retain_param_name, do_constant_folding, _disable_torch_constant_prop)\r\n    264             model.graph, tuple(args), example_outputs, False, propagate)\r\n    265     else:\r\n--> 266         graph, torch_out = _trace_and_get_graph_from_model(model, args, training)\r\n    267         state_dict = _unique_state_dict(model)\r\n    268         params = list(state_dict.values())\r\n\r\n~/.local/lib/python3.6/site-packages/torch/onnx/utils.py in _trace_and_get_graph_from_model(model, args, training)\r\n    223     # training mode was.)\r\n    224     with set_training(model, training):\r\n--> 225         trace, torch_out = torch.jit.get_trace_graph(model, args, _force_outplace=True)\r\n    226 \r\n    227     if orig_state_dict_keys != _unique_state_dict(model).keys():\r\n\r\n~/.local/lib/python3.6/site-packages/torch/jit/__init__.py in get_trace_graph(f, args, kwargs, _force_outplace, return_inputs)\r\n    229     if not isinstance(args, tuple):\r\n    230         args = (args,)\r\n--> 231     return LegacyTracedModule(f, _force_outplace, return_inputs)(*args, **kwargs)\r\n    232 \r\n    233 \r\n\r\n~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    491             result = self._slow_forward(*input, **kwargs)\r\n    492         else:\r\n--> 493             result = self.forward(*input, **kwargs)\r\n    494         for hook in self._forward_hooks.values():\r\n    495             hook_result = hook(self, input, result)\r\n\r\n~/.local/lib/python3.6/site-packages/torch/jit/__init__.py in forward(self, *args)\r\n    292         try:\r\n    293             trace_inputs = _unflatten(all_trace_inputs[:len(in_vars)], in_desc)\r\n--> 294             out = self.inner(*trace_inputs)\r\n    295             out_vars, _ = _flatten(out)\r\n    296             torch._C._tracer_exit(tuple(out_vars))\r\n\r\n~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    489             hook(self, input)\r\n    490         if torch._C._get_tracing_state():\r\n--> 491             result = self._slow_forward(*input, **kwargs)\r\n    492         else:\r\n    493             result = self.forward(*input, **kwargs)\r\n\r\n~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py in _slow_forward(self, *input, **kwargs)\r\n    479         tracing_state._traced_module_stack.append(self)\r\n    480         try:\r\n--> 481             result = self.forward(*input, **kwargs)\r\n    482         finally:\r\n    483             tracing_state.pop_scope()\r\n\r\n~/DL/yolact/yolact.py in forward(self, x)\r\n    615                 pred_outs[\'conf\'] = F.softmax(pred_outs[\'conf\'], -1)\r\n    616 \r\n--> 617             return self.detect(pred_outs)\r\n    618 \r\n    619 \r\n\r\n~/DL/yolact/layers/functions/detection.py in __call__(self, predictions)\r\n     73 \r\n     74             for batch_idx in range(batch_size):\r\n---> 75                 decoded_boxes = decode(loc_data[batch_idx], prior_data)\r\n     76                 result = self.detect(batch_idx, conf_preds, decoded_boxes, mask_data, inst_data)\r\n     77 \r\n\r\nRuntimeError: isTensor() ASSERT FAILED at /pytorch/aten/src/ATen/core/ivalue.h:209, please report a bug to PyTorch. (toTensor at /pytorch/aten/src/ATen/core/ivalue.h:209)\r\nframe #0: std::function<std::string ()>::operator()() const + 0x11 (0x7f721e0ac441 in /home/ws/.local/lib/python3.6/site-packages/torch/lib/libc10.so)\r\nframe #1: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x2a (0x7f721e0abd7a in /home/ws/.local/lib/python3.6/site-packages/torch/lib/libc10.so)\r\nframe #2: <unknown function> + 0x979ad2 (0x7f721d130ad2 in /home/ws/.local/lib/python3.6/site-packages/torch/lib/libtorch.so.1)\r\nframe #3: torch::jit::tracer::getNestedValueTrace(c10::IValue const&) + 0x41 (0x7f721d3939a1 in /home/ws/.local/lib/python3.6/site-packages/torch/lib/libtorch.so.1)\r\nframe #4: <unknown function> + 0xa7651b (0x7f721d22d51b in /home/ws/.local/lib/python3.6/site-packages/torch/lib/libtorch.so.1)\r\nframe #5: <unknown function> + 0xa766db (0x7f721d22d6db in /home/ws/.local/lib/python3.6/site-packages/torch/lib/libtorch.so.1)\r\nframe #6: <unknown function> + 0x457942 (0x7f725d6d2942 in /home/ws/.local/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\r\nframe #7: <unknown function> + 0x130cfc (0x7f725d3abcfc in /home/ws/.local/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\r\nframe #8: _PyCFunction_FastCallDict + 0x35c (0x56204c in /usr/bin/python3)\r\nframe #9: /usr/bin/python3() [0x5a1501]\r\nframe #10: PyObject_Call + 0x3e (0x57c2fe in /usr/bin/python3)\r\nframe #11: /usr/bin/python3() [0x5136c6]\r\nframe #12: _PyObject_FastCallKeywords + 0x19c (0x57ec0c in /usr/bin/python3)\r\nframe #13: /usr/bin/python3() [0x4f88ba]\r\nframe #14: _PyEval_EvalFrameDefault + 0x467 (0x4f98c7 in /usr/bin/python3)\r\nframe #15: _PyFunction_FastCallDict + 0xf5 (0x4f4065 in /usr/bin/python3)\r\nframe #16: /usr/bin/python3() [0x5a1481]\r\nframe #17: PyObject_Call + 0x3e (0x57c2fe in /usr/bin/python3)\r\nframe #18: /usr/bin/python3() [0x513601]\r\nframe #19: _PyObject_FastCallKeywords + 0x19c (0x57ec0c in /usr/bin/python3)\r\nframe #20: /usr/bin/python3() [0x4f88ba]\r\nframe #21: _PyEval_EvalFrameDefault + 0x467 (0x4f98c7 in /usr/bin/python3)\r\nframe #22: /usr/bin/python3() [0x4f6128]\r\nframe #23: _PyFunction_FastCallDict + 0x2fe (0x4f426e in /usr/bin/python3)\r\nframe #24: /usr/bin/python3() [0x5a1481]\r\nframe #25: PyObject_Call + 0x3e (0x57c2fe in /usr/bin/python3)\r\nframe #26: _PyEval_EvalFrameDefault + 0x1851 (0x4facb1 in /usr/bin/python3)\r\nframe #27: /usr/bin/python3() [0x4f6128]\r\nframe #28: _PyFunction_FastCallDict + 0x2fe (0x4f426e in /usr/bin/python3)\r\nframe #29: /usr/bin/python3() [0x5a1481]\r\nframe #30: PyObject_Call + 0x3e (0x57c2fe in /usr/bin/python3)\r\nframe #31: _PyEval_EvalFrameDefault + 0x1851 (0x4facb1 in /usr/bin/python3)\r\nframe #32: /usr/bin/python3() [0x4f6128]\r\nframe #33: _PyFunction_FastCallDict + 0x2fe (0x4f426e in /usr/bin/python3)\r\nframe #34: /usr/bin/python3() [0x5a1481]\r\nframe #35: PyObject_Call + 0x3e (0x57c2fe in /usr/bin/python3)\r\nframe #36: /usr/bin/python3() [0x513601]\r\nframe #37: PyObject_Call + 0x3e (0x57c2fe in /usr/bin/python3)\r\nframe #38: _PyEval_EvalFrameDefault + 0x1851 (0x4facb1 in /usr/bin/python3)\r\nframe #39: /usr/bin/python3() [0x4f6128]\r\nframe #40: _PyFunction_FastCallDict + 0x2fe (0x4f426e in /usr/bin/python3)\r\nframe #41: /usr/bin/python3() [0x5a1481]\r\nframe #42: PyObject_Call + 0x3e (0x57c2fe in /usr/bin/python3)\r\nframe #43: _PyEval_EvalFrameDefault + 0x1851 (0x4facb1 in /usr/bin/python3)\r\nframe #44: /usr/bin/python3() [0x4f6128]\r\nframe #45: _PyFunction_FastCallDict + 0x2fe (0x4f426e in /usr/bin/python3)\r\nframe #46: /usr/bin/python3() [0x5a1481]\r\nframe #47: PyObject_Call + 0x3e (0x57c2fe in /usr/bin/python3)\r\nframe #48: /usr/bin/python3() [0x513601]\r\nframe #49: PyObject_Call + 0x3e (0x57c2fe in /usr/bin/python3)\r\nframe #50: _PyEval_EvalFrameDefault + 0x1851 (0x4facb1 in /usr/bin/python3)\r\nframe #51: /usr/bin/python3() [0x4f6128]\r\nframe #52: /usr/bin/python3() [0x4f7d60]\r\nframe #53: /usr/bin/python3() [0x4f876d]\r\nframe #54: _PyEval_EvalFrameDefault + 0x1260 (0x4fa6c0 in /usr/bin/python3)\r\nframe #55: /usr/bin/python3() [0x4f7a28]\r\nframe #56: /usr/bin/python3() [0x4f876d]\r\nframe #57: _PyEval_EvalFrameDefault + 0x467 (0x4f98c7 in /usr/bin/python3)\r\nframe #58: /usr/bin/python3() [0x4f6128]\r\nframe #59: /usr/bin/python3() [0x4f7d60]\r\nframe #60: /usr/bin/python3() [0x4f876d]\r\nframe #61: _PyEval_EvalFrameDefault + 0x467 (0x4f98c7 in /usr/bin/python3)\r\nframe #62: /usr/bin/python3() [0x4f6128]\r\nframe #63: /usr/bin/python3() [0x4f7d60]\r\n']","['conda', 'pip']",0,0
106,pytorch,2576,open,CUDA multinomial is limited to 2^24 categories,"As reported by @Moustapha6C. The following fails, because there are too many categories:



cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @ngimel @fritzo @neerajprad @alicanb @vishwakftw @nikitaved @vincentqb",function request high priority module: 64-bit module: cuda module: distributions triaged,"['This still applies on master\r\n\r\n```\r\n>>> import torch\r\n>>> x = torch.randn(9000 * 5000).cuda().abs()\r\n>>> out = torch.multinomial(x, 10, replacement=True)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\nRuntimeError: number of categories cannot exceed 2^24\r\n```', 'I am facing the same issue when using `WeightedRandomSampler` with weights of size larger than 2^24. As sampler uses `torch.multinomial` underneath to sample data. ', ""Is there any resolution for this issue?\r\nI am using WeightedRandomSampler as follows:\r\ntrain_sampler = WeightedRandomSampler(sampler_weights, num_samples=500000,replacement=False)\r\nwhere sampler_weights has size 20000000. \r\nThe statement doesn't throw any error but if I convert the train_sampler object to a list then I get the same error: \r\nRuntimeError: number of categories cannot exceed 2^24"", 'For anyone stumbling upon this, you can use the following snippet.\r\n\r\n```python\r\nclass CustomWeightedRandomSampler(WeightedRandomSampler):\r\n    """"""WeightedRandomSampler except allows for more than 2^24 samples to be sampled""""""\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n\r\n    def __iter__(self):\r\n        rand_tensor = np.random.choice(range(0, len(self.weights)),\r\n                                       size=self.num_samples,\r\n                                       p=self.weights.numpy() / torch.sum(self.weights).numpy(),\r\n                                       replace=self.replacement)\r\n        rand_tensor = torch.from_numpy(rand_tensor)\r\n        return iter(rand_tensor.tolist())\r\n```', 'Bumping priority based on user activity', 'We can provide a non-performant workaround for large number of categories, making it performant is hard. ']","['python\r\nx = torch.randn(9000 * 5000).cuda().abs()\r\nout = torch.multinomial(x, 10, replacement=True)\r\n']",[],0,0
107,pytorch,14320,open,[caffe2] Fails to build with fbgemm enabled,"## üêõ Bug

Caffe2 git master fails to build with fbgemm enabled (), giving the following error:

This error seems to be caused by the recent commit https://github.com/pytorch/pytorch/commit/fb8c3d62feacf5c3d39f6fb034db944a89a0bcf4. It was building fine some commits before it.

## To Reproduce

Steps to reproduce the behavior:

1. mkdir build
1. cd build
1. [cmake options](https://bpaste.net/show/ca5a7eb99368)
1. make -j1

## Expected behavior

A succesful build with fbgemm enabled.

## Environment

 - **PyTorch Version:** git master (currently at https://github.com/pytorch/pytorch/commit/f79fb58744ba70970de652e46ea039b03e9ce9ff)
 - **OS:** Arch Linux x86_64
 - **How you installed PyTorch:** source
 - **Build command you used (if compiling from source):** already shown above
 - **Python version:** 3.7.1
 - **CUDA/cuDNN version:** not enabled in this example
 - **GPU models and configuration:** not enabled in this example
 - **Compiler:** gcc 8.2.1
 - **Any other relevant information:** the same error occurs when enabling cuda and using gcc 7.3.1

## Additional context

It builds fine when disabling fbgemm ().",caffe2,[],"['\r\n[ 45%] Building CXX object caffe2/quantization/server/CMakeFiles/caffe2_dnnlowp_avx2_ops.dir/elementwise_sum_dnnlowp_op_avx2.cc.o\r\nIn file included from /storage/linux/abs/caffe2-git/src/pytorch/caffe2/quantization/server/dnnlowp.h:11,\r\n                 from /storage/linux/abs/caffe2-git/src/pytorch/caffe2/quantization/server/caffe2_dnnlowp_utils.h:4,\r\n                 from /storage/linux/abs/caffe2-git/src/pytorch/caffe2/quantization/server/utility_dnnlowp_ops.h:4,\r\n                 from /storage/linux/abs/caffe2-git/src/pytorch/caffe2/quantization/server/elementwise_sum_dnnlowp_op_avx2.cc:1:\r\n/storage/linux/abs/caffe2-git/src/pytorch/third_party/fbgemm/include/fbgemm/QuantUtils.h:9:10: fatal error: cpuinfo.h: No such file or directory\r\n #include <cpuinfo.h>\r\n          ^~~~~~~~~~~\r\ncompilation terminated.\r\nmake[2]: *** [caffe2/quantization/server/CMakeFiles/caffe2_dnnlowp_avx2_ops.dir/build.make:63: caffe2/quantization/server/CMakeFiles/caffe2_dnnlowp_avx2_ops.dir/elementwise_sum_dnnlowp_op_avx2.cc.o] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:4460: caffe2/quantization/server/CMakeFiles/caffe2_dnnlowp_avx2_ops.dir/all] Error 2\r\nmake: *** [Makefile:141: all] Error 2\r\n']","[""USE_FBGEMM:BOOL='ON'"", ""USE_FBGEMM:BOOL='OFF'""]",0,0
108,pytorch,15617,open,Feature request: transposed locally connected layer,"## üöÄ Feature

transposed version of locally connected layer, similar to the transposed version of convolution layer but without weight sharing.

## Motivation

Similar to the necessity of the transposed convolution layer,  the locally connected layer (issue #499, PR #1583) should also have a transposed version.



cc @albanD @mruberry @jbschlosser",enhancement module: nn todo triaged,[],[],[],0,0
109,pytorch,7480,open,[caffe2] How to use multiple CPUs?,"I want to run a caffe2 model on multiple CPUs (evaluation).

Is there a way to do that directly? Maybe it requires OpenBLAS?

I don't think is it possible to use multiprocessing and call RunNet on different workspaces.",caffe2,[],[],[],0,0
110,pytorch,20009,open,issue while exporting torch model to onnx format,"## üêõ Bug

i am getting error while converting my saved torch model to onnx format

torch.onnx.export(trained_model, dummy_input, ""sentiment.onnx"")
TypeError: forward() missing 1 required positional argument: 'hidden' 

torch version - '1.0.1.post2'

## To Reproduce

Steps to reproduce the behavior:
from torch.autograd import Variable
import torch.onnx
# Load the trained model from file
net_save = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)
trained_model = net_save
trained_model.load_state_dict(torch.load('sentiment.pth'))

# Export the trained model to ONNX
dummy_input = Variable(torch.randn(1, 1, 28, 28)) # one black and white 28 x 28 picture will be the input to the model
torch.onnx.export(trained_model, dummy_input, ""sentiment.onnx"")

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

i should get the onnx format of my model but getting error

",module: onnx triaged,"['Me too: https://github.com/pytorch/pytorch/issues/20031', 'same here...any updates on this?']",[],[],0,0
111,pytorch,30139,open,"RuntimeError: isTensor() INTERNAL ASSERT FAILED at /pytorch/aten/src/ATen/core/ivalue_inl.h:90, please report a bug to PyTorch. Expected Tensor but got Int (toTensor at /pytorch/aten/src/ATen/core/ivalue_inl.h:90)","So, what I'm trying to do is to to convert torchscript model into onnx. Both scripting and tracing works during the creation of graph, but fails when the model is converted into onnx

Here's the error:

> 
Traceback (most recent call last):
  File ""yolo2script.py"", line 8, in <module>
    torch.onnx.export(model_scripted, dummy_input, ""yolov3.onnx"", example_outputs=model(dummy_input))
  File ""/usr/local/lib/python3.6/dist-packages/torch/onnx/__init__.py"", line 143, in export
    strip_doc_string, dynamic_axes, keep_initializers_as_inputs)
  File ""/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py"", line 66, in export
    dynamic_axes=dynamic_axes, keep_initializers_as_inputs=keep_initializers_as_inputs)
  File ""/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py"", line 382, in _export
    fixed_batch_size=fixed_batch_size)
  File ""/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py"", line 235, in _model_to_graph
    method_graph, params = model.forward._lowered_graph()
RuntimeError: isTensor() INTERNAL ASSERT FAILED at /pytorch/aten/src/ATen/core/ivalue_inl.h:90, please report a bug to PyTorch. Expected Tensor but got Int (toTensor at /pytorch/aten/src/ATen/core/ivalue_inl.h:90)
frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x33 (0x7f83210f0813 in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1876251 (0x7f82bdc7d251 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch.so)
frame #2: torch::jit::script::Method::_lowered_graph() + 0x161 (0x7f82c0420771 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x5af698 (0x7f832229b698 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_python.so)
frame #4: <unknown function> + 0x2110f4 (0x7f8321efd0f4 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_python.so)
<omitting python frames>
frame #6: python3() [0x4f88ba]
frame #8: python3() [0x4f6128]
frame #9: python3() [0x4f7d60]
frame #10: python3() [0x4f876d]
frame #12: python3() [0x4f6128]
frame #13: python3() [0x4f7d60]
frame #14: python3() [0x4f876d]
frame #16: python3() [0x4f6128]
frame #17: python3() [0x4f7d60]
frame #18: python3() [0x4f876d]
frame #20: python3() [0x4f6128]
frame #21: python3() [0x4f7d60]
frame #22: python3() [0x4f876d]
frame #24: python3() [0x4f6128]
frame #26: python3() [0x6415b2]
frame #31: __libc_start_main + 0xe7 (0x7f83267afb97 in /lib/x86_64-linux-gnu/libc.so.6)

Here's my system:

Collecting environment information...
PyTorch version: 1.3.1
Is debug build: No
CUDA used to build PyTorch: 10.1.243

OS: Ubuntu 18.04.2 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.16.0-rc3

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration: GPU 0: GeForce GTX 1060 with Max-Q Design
Nvidia driver version: 418.87.00
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.2

Versions of relevant libraries:
[pip3] efficientnet-pytorch==0.4.0
[pip3] numpy==1.17.4
[pip3] torch==1.3.1
[pip3] torch2trt==0.0.2
[pip3] torchetl==0.3.9
[pip3] torchvision==0.4.2
[conda] Could not collect




If you need the model source code, I will provide it to you asap. Thank you !

cc @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof",module: onnx triaged,"['I met the same error', 'I have the same error - did you find a workaround?', 'I have the same problem', 'any solution on this? having the same problem!', ""I'm also having this problem. Found no solution yet.\r\nMy code worked several days ago, it failed after a connection break while testing."", ""Got similar problems using PyTorch '1.5.0a0+8f84ded'"", ""I'm not sure, but I think I solved this error by rerunning the code. \r\nI used a Google Compute Engine and sometimes it gave an error.\r\nIf this does not work, maybe upgrade the memory you're using or use more gpu's."", ""> I'm not sure, but I think I solved this error by rerunning the code.\r\n> I used a Google Compute Engine and sometimes it gave an error.\r\n> If this does not work, maybe upgrade the memory you're using or use more gpu's.\r\n\r\nThis is peculiar. My model is extremely small (10MB), and I was using V100 GPU. In principle, it should not be a GPU problem."", 'This issue maybe helps you: https://github.com/pytorch/pytorch/issues/35293', 'Indeed @leimao . That cannot be the problem in that case. Hope the comment above helps you further.', ""I tried making similar changes to my models as #35293 but it didn't help :("", 'I have the same error', 'I too have the same error.', ""I'm seeing this error when running a traced model with LibTorch on iOS.""]",[],[],0,0
112,pytorch,24608,open,Migrate `nll_loss2d_forward` from the TH to Aten (CUDA),"Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review.",better-engineering module: loss module: porting triaged,[],[],[],0,0
113,pytorch,17632,open,Caffe2 C++ script for classification/object_detection with CMakeLists.txt ,"## ‚ùì Questions and Help

I was looking for caffe2 c++ script that supports latest version. But I couldn't find any good resource. Can anyone provide caffe2 c++ example script for classification. 

Thanks. ",caffe2,[],[],[],0,0
114,pytorch,11982,open,[feature request] Publish wheels with debug symbols,"It would be great if wheels of pytorch with debug symbols were published (or at least made available via https://ci.pytorch.org), so that users can investigate native crashes without going through the whole (error prone) process of building pytorch themselves.

For reference, I asked [the question on the forums first, but got no answer](https://discuss.pytorch.org/t/pytorch-wheel-with-debug-symbols/25169).

cc @ezyang @seemethere @malfet @walterddr",module: binaries triaged,"['cc @pjh5 what do you think?', 'Linking to #6391 as I think it is related', 'I built a set of wheels with debug symbols once for some internal debugging. IIRC it added about 8 hours on each of our nightly machines and generated another 40 GB. Given the wide array of possible build configurations and that debug symbols are probably most useful when built with your particular environment, I think that building from source for debug symbols is actually the better option here. ', ""Closing via @pjh5's comments"", 'I respectfully disagree, my use case is investigating a native crash with pytorch\'s ""official"" pip wheel => if I build it on my system with debug symbols, the crash is not reproduced. The point is having a debug wheel built in the exact same conditions as the non-debug one. But I understand your reasons.', '@pjh5 What is the difference between a wheel and a build from source? I remember there was some distinction (static linking of cuda bits?)', ""> debug symbols are probably most useful when built with your particular environment\r\n\r\nThis is not true. Debug symbols are always useful, even if you're running a binary on another environment.\r\n\r\nSOP in Linux distro land is to distribute debug symbols separately from executable binaries, since they are pretty big. We might have trouble setting infrastructure for this. Maybe if we can make it easy for end users to build wheels, we can make this self service, which will be easier for us.\r\n\r\nReopening."", 'Right now the wheels are built with the pytorch/builder repo, which contains Dockerfiles and bash scripts that call setup.py and then do some linking and path munging. It comes down to setting a few environment variables in a Docker https://github.com/pytorch/builder/blob/master/cron/build_docker.sh#L142 (DESIRED_PYTHON and DESIRED_CUDA and DEBUG are all that should matter) and then calling https://github.com/pytorch/builder/blob/master/manywheel/build.sh (or build_cpu.sh). The docker can be pulled from soumith/manylinux-cuda80 or cuda90 or cuda92 (cpus are built on cuda80). You should be able to produce a debug wheel with this in the exact same conditions, except perhaps for differences in nvidia-docker versions or CUDA drivers.', ""@pjh5 Thanks, I could easily build with debug symbols using:\r\n```\r\nexport PACKAGE_TYPE=manywheel\r\nexport DESIRED_PYTHON=3.6m\r\nexport DESIRED_CUDA=cu90\r\nexport PYTORCH_BUILD_VERSION=0.4.1\r\nexport DEBUG=1\r\nexport NIGHTLIES_EMAIL_LIST=''\r\nexport NIGHTLIES_FOLDER=$(mktemp -d)\r\n./cron/build_docker.sh\r\n```\r\nThe scripts are not really intented to do this (notifications and such) but it  works. As @ezyang said, maybe all is needed is to make these build scripts a bit more accessible or documented."", ""cron/build_docker.sh is quite coupled to the nightlies setup. But https://github.com/pytorch/builder/blob/master/manywheel/build.sh will build a wheel without needing to set most of those environment variables if you run it in a soumith/manylinux-cuda80 (you'll have to checkout the pytorch/builder repo but it comes with git). I think you'd only have to set DESIRED_PYTHON and DESIRED_CUDA. That way it won't require a nightlies folder to store all the nightlies metadata and logs"", 'This issue is still valid but IDK if anyone is actually going to get around to doing it.', 'Actually, we do plan to do something soon in that area.\r\ncc: @driazati']",[],[],0,0
115,pytorch,24625,closed,Migrate `sigmoid_backward` from the TH to Aten (CUDA),"Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review.",better-engineering module: porting triaged,[],[],[],0,0
116,pytorch,7764,closed,Incorrect compilation instructions for Mac,"The instructions at https://github.com/pytorch/pytorch#from-source for compiling from source say to use the following command on macOS:



Why I try to compile with that command, it fails with the following error:



I was able to get it to compile by using the following command instead:



## System Info

- PyTorch or Caffe2: PyTorch
- How you installed PyTorch (conda, pip, source): Source
- Build command you used (if compiling from source): See above
- OS: macOS 10.13.4
- PyTorch version: Latest code from repository
- Python version: 3.6.1
",,['Closing due to age. '],"['\r\nMACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py install\r\n', ""\r\ntorch/csrc/distributed/Module.cpp:4:10: fatal error: 'unordered_map' file not\r\n      found\r\n#include <unordered_map>\r\n"", '\r\nMACOS_DEPOYMENT_TARGET=10.9 CC=clang CXX=clang++ CPPFLAGS=""-stdlib=libc++"" python setup.py install\r\n']",[],0,0
117,pytorch,1434,closed,no member named 'shared_ptr' in namespace 'std',"When building from source, I got this error. My system is OSX 10.12.4, conda, CUDA 8.0. Any thoughts? Thanks!

",,"[""can you try adding `#include <memory>` to the file `pytorch/torch/csrc/autograd/function_hook.h` and see if that fixes things? if so i'll commit it."", 'actually, that file already has memory included.\r\n\r\nAre you following instructions from https://github.com/pytorch/pytorch#from-source\r\nEspecially: \r\n```\r\nMACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py install\r\n```\r\n\r\nThis part is important: `MACOSX_DEPLOYMENT_TARGET=10.9 `', '`MACOSX_DEPLOYMENT_TARGET=10.9` works nicely! Thanks a lot!']","[""\r\ntorch/csrc/serialization.cpp:2:10: fatal error: 'system_error' file not found\r\n#include <system_error>\r\n         ^\r\nIn file included from torch/csrc/autograd/engine.cpp:1:\r\n"", ""\r\npytorch/torch/csrc/autograd/function_hook.h:11:40: error: no member named 'shared_ptr' in namespace 'std'\r\nusing variable_list = std::vector<std::shared_ptr<Variable>>;\r\n                                  ~~~~~^\r\n/Users/hangzhang/pytorch/torch/csrc/autograd/function_hook.h:11:51: error: 'Variable' does not refer to a value\r\nusing variable_list = std::vector<std::shared_ptr<Variable>>;\r\n""]",[],0,0
118,pytorch,22764,closed,[feature request] torch.hypot," returns real and imaginary components as the last dimension. Two very frequent functions to consume it are:  and  (in NumPy). Abs is similar to regular , so meanwhile complex tensor support is not developed,  could do 's job.

NumPy also has a , although it accepts two arrays. We may have two versions: one accepting two arrays, another accepting only one array and a dim. Essentially,  is a version of , specialized for two element vectors only.",enhancement module: numpy triaged,"['This could be quite fast for real/imaginary being contiguous in memory (for impl one may or may not use the precision-enhancement as found in existing hypot implementations, maybe not such a big deal)', 'Probably the same kernel could be used for complex abs and hypot', '@muthuArivoli  It would be interesting to know perf for:\r\n```python\r\nX = torch.rand(128, 128, 2)\r\nA = X.norm(dim = -1)\r\nB = torch.hypot(X[..., 0], X[..., 1])\r\nC = X.view_as_complex().abs()\r\nD = torch.add(*(X ** 2).unbind(dim = -1)).sqrt()\r\n```', ""@vadimkantorov Using the script\r\n```\r\nimport torch\r\nimport time\r\n\r\nX = torch.rand(2**7, 2**7, 2)\r\nstart = time.time()\r\nA = X.norm(dim = -1)\r\nend = time.time()\r\nprint(end-start)\r\n\r\nstart = time.time()\r\nB = torch.hypot(X[..., 0], X[..., 1])\r\nend = time.time()\r\nprint(end-start)\r\n\r\nstart = time.time()\r\nC = torch.view_as_complex(X).abs()\r\nend = time.time()\r\nprint(end-start)\r\n\r\nstart = time.time()\r\nD = torch.add(*(X ** 2).unbind(dim = -1)).sqrt()\r\nend = time.time()\r\nprint(end-start)\r\n```\r\nI got consistent results around\r\n```\r\n0.0018534660339355469\r\n0.0002155303955078125\r\n0.0001697540283203125\r\n0.00030803680419921875\r\n```\r\n\r\nWhen I increased the size of the tensor to something large, `X = torch.rand(2**14, 2**14, 2)`, I got results consistently around:\r\n```\r\n25.421497344970703\r\n0.5420243740081787\r\n0.6201455593109131\r\n0.7890973091125488\r\n```\r\nThese tests were done on a core i7-6770HQ. I don't have a CUDA enabled computer with me, so I won't be able to benchmark on GPU.\r\nHope this helps! Let me know if I should benchmark this in a different way or if you need anything else."", ""@mruberry What is the proper way of measuring CPU perf? I'm having troubles to compute wall-clock time in  https://github.com/pytorch/pytorch/issues/42959\r\n\r\nIf torch.hypot is indeed better than other methods (and definitely better than norm), maybe they all could delegate to hypot?\r\n\r\nI also wonder if hypot makes use of chunked/consecutive data loading for the case when first and second arguments are actually interleaved in memory as in this example @colesbury "", ""@muthuArivoli The large number for norm may actually be the same reason. Could you please try `torch.set_num_threads(1)`? If it decreases, it's related to multiple threads times summing."", '@ngimel has a reference for how we benchmark, I think', ""> @muthuArivoli The large number for norm may actually be the same reason. Could you please try `torch.set_num_threads(1)`? If it decreases, it's related to multiple threads times summing.\r\n\r\n@vadimkantorov  With input `X = torch.rand(2**7, 2**7, 2)`, I got\r\n```\r\n0.0036995410919189453\r\n0.00023674964904785156\r\n0.00020194053649902344\r\n0.00036525726318359375\r\n```\r\n\r\nWith large input `X = torch.rand(2**14, 2**14, 2)`, I got\r\n```\r\n55.82693123817444\r\n2.1235761642456055\r\n1.628692388534546\r\n2.3243396282196045\r\n```"", ""@ngimel shouldn't `norm` then delegate to some other methods for super small dimensions?""]",[],"['torch.stft', 'abs', 'angle', 'np.hypot', 'hypot', 'abs', 'hypot', 'hypot', 'norm']",0,0
119,pytorch,13491,closed,Numerical ODE solvers,"## üöÄ Feature
It would be useful to have a library of numerical ODE solvers that are compatible with PyTorch.

## Motivation
Neural networks are increasingly being combined with ODEs. For example, when modelling ODEs that form part of a probabilistic model, or when defining a derivative with a neural network (neural ODEs).

You may want to get a speedup by using the GPU with PyTorch for this, and it's useful to be able to differentiate through these algorithms too.

## Pitch
I'm proposing to create a  module that contains half a dozen common ODE solvers. For instance, you could call  to calculate  points from  to  for the equation 
",,"['Neural ODE would be great! :) I would suggest submitting to contrib repo: https://github.com/pytorch/contrib/pulls', ""I was intending to contribute some general ODE solvers first, which are more widely applicable than to Neural ODEs. Would that be better suited for the `contrib` repo? It's not so much implementing a specific deep learning paper\r\n\r\nHowever, I would like to understand the paper more fully and write an implementation of neural ODEs after that...\r\n\r\nI think in general it would be good to have basic numerical algorithms in PyTorch so you can get the GPU speedup! And some like numerical differentiation would be useful for testing PyTorch components"", ""if it's generalized ODE solvers, I put them similar to our `spectral` ops like FFT, they can go into pytorch core as long as we can iterate on the design / API.\r\n\r\nIf it's Neural ODEs / a particular paper, we should put them in `contrib`"", '@stefanwebb have you seen this? https://github.com/rtqichen/torchdiffeq', ""Yes, thanks! I'm going to ask the author if he would consider merging it into PyTorch...""]",[],"['torch.ode', 'torch.ode.forward_euler(f, t0, tn, x0, n)', 'n', 't0', 'tn', 'dx/dt = f(t, x)']",0,0
120,pytorch,6894,closed,Maybe a bug in torch.nn.Upsample,"## Issue description

An unfriendly exception output when the input data size (the last two dims) and Upsample param 'size' have the same value

## Code example


and the output is:
/home/USER/.pyenv/versions/3.6.4/envs/shinerio-python3.6.4-env/bin/python': free(): invalid pointer: 0x00007fbea6fdfd40 ***
======= Backtrace: =========
/lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7fbed7b017e5]
/lib/x86_64-linux-gnu/libc.so.6(+0x8037a)[0x7fbed7b0a37a]
/lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7fbed7b0e53c]
/home/USER/.pyenv/versions/3.6.4/envs/shinerio-python3.6.4-env/lib/python3.6/site-packages/torch/_thnn/_THNN.cpython-36m-x86_64-linux-gnu.so(+0x2f367)[0x7fbe764f6367]
/home/USER/.pyenv/versions/3.6.4/envs/shinerio-python3.6.4-env/bin/python(_PyCFunction_FastCallKeywords+0x279)[0x4aefe9]
/home/USER/.pyenv/versions/3.6.4/envs/shinerio-python3.6.4-env/bin/python[0x54060e]
/home/USER/.pyenv/versions/3.6.4/envs/shinerio-python3.6.4-env/bin/python(_PyEval_EvalFrameDefault+0x102d)[0x54268d]
/home/USER/.pyenv/versions/3.6.4/envs/shinerio-python3.6.4-env/bin/python[0x540275]
/home/USER/.pyenv/versions/3.6.4/envs/shinerio-python3.6.4-env/bin/python(PyEval_EvalCodeEx+0x3e)[0x54118e]
/home/USER/.pyenv/versions/3.6.4/envs/shinerio-python3.6.4-env/bin/python[0x485828]
/home/USER/.pyenv/versions/3.6.4/envs/shinerio-python3.6.4-env/bin/python(PyObject_Call+0x5c)[0x45322c]
/home/USER/.pyenv/versions/3.6.4/envs/shinerio-python3.6.4-env/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_Z17THPFunction_applyP7_objectS0_+0x271)[0x7fbea5ae7ac1]

... ...

7fbed899f000-7fbed89a6000 r--s 00000000 08:02 17172406                   /usr/lib/x86_64-linux-gnu/gconv/gconv-modules.cache
7fbed89a6000-7fbed89a7000 r--p 00025000 08:02 8912913                    /lib/x86_64-linux-gnu/ld-2.23.so
7fbed89a7000-7fbed89a8000 rw-p 00026000 08:02 8912913                    /lib/x86_64-linux-gnu/ld-2.23.so
7fbed89a8000-7fbed89a9000 rw-p 00000000 00:00 0 
7ffc0354d000-7ffc0356e000 rw-p 00000000 00:00 0                          [stack]
7ffc03575000-7ffc03578000 r--p 00000000 00:00 0                          [vvar]
7ffc03578000-7ffc0357a000 r-xp 00000000 00:00 0                          [vdso]
ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]

Process finished with exit code -1

`


## System Info

-   PyTorch version: 0.3.1
    Is debug build: No
    CUDA used to build PyTorch: 8.0.61
-   OS: Ubuntu 16.04.2 LTS
    GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
    CMake version: version 3.5.1

-   Python version: 3.6
    Is CUDA available: Yes
    CUDA runtime version: Could not collect
    GPU models and configuration: GPU 0: Tesla M40 24GB
    Nvidia driver version: 384.111
    cuDNN version: Probably one of the following:
    /usr/local/cuda-9.0/lib64/libcudnn.so
    /usr/local/cuda-9.0/lib64/libcudnn.so.7
    /usr/local/cuda-9.0/lib64/libcudnn.so.7.0.5
    /usr/local/cuda-9.0/lib64/libcudnn_static.a

-   Versions of relevant libraries:
    [pip3] numpy (1.14.2)
    [conda] Could not collect
",,"['Same issue found here!\r\nIt took me a whole day to locate the problem üòÇ ', 'Sorry for this bug. Here is the true error `RuntimeError: invalid argument 4: scale_factor must be greater than 1, but got: 1`. The issue is that the binaries sometimes throw invalid free when printing error message... See https://github.com/pytorch/pytorch/issues/5400. \r\n\r\nFrom https://github.com/pytorch/pytorch/issues/5673, it seems that this is solved. Is this correct, @soumith ?', '@SsnL Got it, thank you very much üòÉ ', 'solved in 0.4']","[""python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\nx = Variable(torch.rand(1, 1, 2, 2))\r\nm = nn.Upsample(size=(2, 2), mode='nearest') \r\nprint(x)\r\nprint(m(x))\r\n""]","['', '\r\nssh://USER@xxx.xxx.xxx.xxx:22/home/USER/.pyenv/versions/3.6.4/envs/shinerio-python3.6.4-env/bin/python -u /home/USER/tomxie/hpe/pytorch-pose-hg-3d/src/test/test.py\r\nVariable containing:\r\n(0 ,0 ,.,.) = \r\n  0.9584  0.4462\r\n  0.3099  0.4113\r\n[torch.FloatTensor of size 1x1x2x2]\r\n\r\n*** Error in ', '']",0,0
121,pytorch,6064,closed,[jit]][script] handling of 'void' returns,"Methods returning no values generate weird error messages:

For instance:



produces 


What is expected?

* omitting a return statement should be valid and just return 0 values (someone might just be print debugging at this point and hasn't written the return yet)
*  with no value should work.",oncall: jit,"[""I'm on this""]","['\r\n    def test_script_module_for(self):\r\n        class M(torch.jit.ScriptModule):\r\n            def __init__(self):\r\n                super(M, self).__init__(False)\r\n               \r\n            @torch.jit.script_method\r\n            def forward(self):\r\n                return\r\n', '\r\nTraceback (most recent call last):\r\n  File ""test/test_jit.py"", line 2059, in test_script_module_for\r\n    class M(torch.jit.ScriptModule):\r\n  File ""test/test_jit.py"", line 2064, in M\r\n    @torch.jit.script_method\r\n  File ""/data/users/zdevito/pytorch/torch/jit/__init__.py"", line 539, in script_method\r\n    return ScriptMethodStub(createResolutionCallback(), get_jit_ast(fn))\r\n  File ""/data/users/zdevito/pytorch/torch/jit/frontend.py"", line 127, in get_jit_ast\r\n    return build_def(SourceRangeFactory(source), py_ast.body[0])\r\n  File ""/data/users/zdevito/pytorch/torch/jit/frontend.py"", line 146, in build_def\r\n    [build_stmt(ctx, stmt) for stmt in body])\r\n  File ""/data/users/zdevito/pytorch/torch/jit/frontend.py"", line 135, in __call__\r\n    return method(ctx, node)\r\n  File ""/data/users/zdevito/pytorch/torch/jit/frontend.py"", line 207, in build_Return\r\n    return Return(r, [build_expr(ctx, val) for val in values])\r\n  File ""/data/users/zdevito/pytorch/torch/jit/frontend.py"", line 134, in __call__\r\n    raise UnsupportedNodeError(ctx, node)\r\n  File ""/data/users/zdevito/pytorch/torch/jit/frontend.py"", line 110, in __init__\r\n    source_range = ctx.make_range(offending_node.lineno,\r\nAttributeError: \'NoneType\' object has no attribute \'lineno\'\r\n']",['return'],0,0
122,pytorch,17484,closed,Parameter not registering if .to(device) is used,"## üêõ Bug

During instantiation of a custom module, parameters do not register if they are initialized with a .to('cuda') function call on the parameter level. 

## To Reproduce



On the other hand, if the .to('cuda') is called on the wrapped tensor, everything works as expected.




OS: Ubuntu 16.04.5 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CMake version: Could not collect

Python version: 3.5
Is CUDA available: N/A
CUDA runtime version: 7.5.17
GPU models and configuration: GPU 0: GeForce GTX 1070
Nvidia driver version: 384.130
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.1
[pip] numpy==1.13.3
[pip] torch==1.0.0
[pip] torchvision==0.2.1
[conda] Could not collect

",,"[""this is totally expected.\r\n\r\nParameters can only be leaf Tensors, not Tensors that are a result of an operation on another Tensor.\r\n\r\nWhat you want is:\r\n\r\n```\r\nself.par = torch.nn.Parameter(torch.rand(5, device='cuda'))\r\n```""]","[""\r\nclass A(torch.nn.Module):\r\n    def __init__(self):\r\n        super(A, self).__init__()\r\n        self.par = torch.nn.Parameter(torch.rand(5)).to('cuda')\r\n    def forward(self):\r\n        pass\r\n    \r\na= A()\r\na.state_dict()\r\nOut[8]: OrderedDict()\r\n"", ""\r\nclass A(torch.nn.Module):\r\n    def __init__(self):\r\n        super(A, self).__init__()\r\n        self.par = torch.nn.Parameter(torch.rand(5, device='cuda'))\r\n    def forward(self):\r\n        pass\r\n    \r\na = A()\r\na.state_dict()\r\nOut[13]: \r\nOrderedDict([('par',\r\n              tensor([0.0247, 0.8373, 0.2540, 0.9849, 0.5181], device='cuda:0'))])\r\n""]",[],0,0
123,pytorch,2523,closed,cudnn bindings not respecting current stream,"currently, some cudnnSetStream calls are missing in the bindings, fix them.
Context: https://discuss.pytorch.org/t/torch-nn-in-context-of-a-cuda-stream/6304/3",high priority,['fixed via #2984 '],[],[],0,0
124,pytorch,19201,closed,How do I edit a .t7 file?,"I currently have a .t7 file which contains some arrays and raw text, but i want to reduce the number of entries in there. How do I edit the file?

So far, I can only display the file's contents using
",,['torchfile is not maintained by us. ask in https://github.com/bshillingford/python-torchfile'],[],"[""torchfile.load('val_captions.t7')""]",0,0
125,pytorch,15028,closed,License unavailable in the installed package,"Hi. I noticed that, after pytorch is installed, the PKG-INFO file says the license is unknown. Our tools rely on this file to obtain the correct license information. Is it possible to put correct license information in it?

For example,  has:
",,"['Metadata for the `1.7.0` release seems correct, so closing this. Not sure when it got fixed, probably a long time ago.\r\n\r\nThanks for the report @hanyucui ']",['\r\nMetadata-Version: 1.0\r\nName: torch\r\nVersion: 0.4.1\r\nSummary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\r\nHome-page: UNKNOWN\r\nAuthor: UNKNOWN\r\nAuthor-email: UNKNOWN\r\nLicense: UNKNOWN\r\nDescription: UNKNOWN\r\nPlatform: UNKNOWN\r\n'],['lib/python3.6/site-packages/torch-0.4.1-py3.6.egg-info/PKG-INFO'],0,0
126,pytorch,4115,closed,Feature Request : Alias Multinomial,"Would you be interested in extending the alias multinomial sampling to pytorch. The C and the CUDA version are already present in [](https://github.com/torch/torch7/blob/aed31711c6b8846b8337a263a7f9f998697994e7/doc/maths.md#res-torchmultinomialaliasoutput-state) (linked)
I believe it could be very useful for people who want to use multinomial sampling from a static distribution.",todo,"['Yeah we should get it too! I think adding a flag to categorical distribution that makes it use aliasing would be the way to go, but I‚Äôd like to hear what @fritzo thinks.', 'There is however a [setup step](https://github.com/torch/torch7/blob/aed31711c6b8846b8337a263a7f9f998697994e7/doc/maths.md#state-torchmultinomialaliassetupprobs)  in alias multinomial. It can be hidden using a single function that does the setup first time and then simply draws from then on but would probably be more useful to leave it open.\r\n', 'Yes I know. You would do the setup in the distribution constructor', ""That's a really clever method. It sounds reasonable to add an argument `alias_sampler=False` to the `distributions.Categorical` constructor. It would be even nicer if we could automatically decide whether to use the alias sampler, e.g. use it in `.sample((n,))` whenever there are sufficiently many `n` samples to make the computation cheaper, and cache the alias table when we do compute it."", ""It would be nicer, but I'm not sure how to properly implement a heuristic for when to build the aliasing table üòï "", 'Can we do a benchmarking step at some point to compare them ?\n\nOn Wed, Dec 13, 2017, 9:23 AM Adam Paszke <notifications@github.com> wrote:\n\n> It would be nicer, but I\'m not sure how to properly implement a heuristic\n> for when to build the aliasing table üòï\n>\n> ‚Äî\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/4115#issuecomment-351332249>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHWRc8_PLhu1neG2c-r-szbeSVdhM7FCks5s_5d0gaJpZM4Q9yuB>\n> .\n>\n-- \n======================================================\nAmartya Sanyal\nFourth Year Undergraduate\nDept. of Computer Science And Engineering\nIIT Kanpur\n\n*""You sort of start thinking anything‚Äôs possible if you‚Äôve got enough\nnerve."" - Ginny Weasley*\n', 'Not sure how would that solve it. Aliasing will be faster in most (all?) cases, so you pretty much know the result already. The only question is when do you choose to pay the setup cost (i.e. when do you expect that it will get amortized)?', 'Yeah, I think if we can find out for what n the amortised cost of setup +\r\nalias sampling is better than sampling, we can choose to do alias sampling\r\nfor all n greater than than that n. Does this sound reasonable or am I\r\nmissing something?\r\n\r\n', ""We might figure this out automatically if someone requests a large `sample_shape`, but otherwise there's no way to say how many small requests will you get."", ""I see what you mean. Yes, we can only figure out which multinomial sampler\r\nto use depending on the `sample_shape` of one call but can't account for\r\nmultiple future calls.\r\n"", 'Any other ideas ? @apaszke  Or is putting a flag in and letting the user decide the best idea ? I guess we could  have a default flag set to `alias_multinomial` if `n_sample` is too large and not otherwise and the user would still have the chance to change it .', 'I think that we can take a mixed approach: let people use a flag to force the setup in the constructor, or delay it until we see a `sample_shape` for which we can be sure that the setup will already amortize (and future calls will only benefit from this).', 'Yeah that makes sense. ', 'Any progress on if we should go ahead with this ?', ""Yeah let's do that. It's not a super high priority so no one is working on this as far as I know"", ""I can do it. I haven't made a contribution to pytorch before but I can do it, I guess."", 'Hello, is there any progress with this request?', 'Is there anyone still working on this thread?', '@neerajprad said this was already available in aten and that it might not be much work to start using that version in `torch.distributions.Categorical`.', ""> @neerajprad said this was already available in aten and that it might not be much work to start using that version in `torch.distributions.Categorical`.\r\n\r\nThanks. I found torch.distributions.categorical.Categorical, but seems there isn't a flag to turn on the AliasMethod option. Besides, how does this relates with aten? "", ""> Thanks. I found torch.distributions.categorical.Categorical, but seems there isn't a flag to turn on the AliasMethod option. Besides, how does this relates with aten?\r\n\r\nThe class in `distributions.categorical` is only a Python wrapper that calls `torch.multinomial` when asked to sample. Currently, `torch.multinomial` would delegate to the c++ implementation in aten's [THTensorRandom.cpp](https://github.com/pytorch/pytorch/blob/5737c5259c6d6b3e74c002c584149c718c0fc823/aten/src/TH/generic/THTensorRandom.cpp#L247), but there are also [multinomialAliasSetup](https://github.com/pytorch/pytorch/blob/5737c5259c6d6b3e74c002c584149c718c0fc823/aten/src/TH/generic/THTensorRandom.cpp#L136) and `multinomialAliasDraw` methods in the same file. I think all we need to do is to expose them in `native_functions.yaml`, call `multinomialAliasSetup` in the constructor of `distributions.Categorical` based on a flag and then call `multinomialAliasDraw` in `.sample`. I can take a stab at it, in case there is interest. "", ""> > Thanks. I found torch.distributions.categorical.Categorical, but seems there isn't a flag to turn on the AliasMethod option. Besides, how does this relates with aten?\r\n> \r\n> The class in `distributions.categorical` is only a Python wrapper that calls `torch.multinomial` when asked to sample. Currently, `torch.multinomial` would delegate to the c++ implementation in aten's [THTensorRandom.cpp](https://github.com/pytorch/pytorch/blob/5737c5259c6d6b3e74c002c584149c718c0fc823/aten/src/TH/generic/THTensorRandom.cpp#L247), but there are also [multinomialAliasSetup](https://github.com/pytorch/pytorch/blob/5737c5259c6d6b3e74c002c584149c718c0fc823/aten/src/TH/generic/THTensorRandom.cpp#L136) and `multinomialAliasDraw` methods in the same file. I think all we need to do is to expose them in `native_functions.yaml`, call `multinomialAliasSetup` in the constructor of `distributions.Categorical` based on a flag and then call `multinomialAliasDraw` in `.sample`. I can take a stab at it, in case there is interest.\r\n\r\nNice, thanks for your explanation. Already got this idea, but not sure how to get it actually work. If there is an example, that would be tremendously helpful."", ""@HobbitLong - I don't have a ready example, and this is my best guess as to what changes might be needed. You could probably grep for `multinomial` and see what plumbing will be required to set this up. I'll take another look at this tonight, and throw something up or let you know if I run into any roadblocks."", ""I tried adding the function metadata to `Declarations.cwrap` and `LegacyDefinitions.cpp`, but I haven't been able to successfully compile it. I have been having issues with trying to designate the 2nd and 3rd arguments of `multinomialAliasSetup` as return args, but the generated source is different. I am very likely doing something wrong, and will need help from someone who is more intimately familiar with torch internals. \r\n\r\n@vishwakftw - would you have any ideas or pointers on how to get this to work?"", ""I'll look into this, and get back to you by next morning if that works for you."", '@neerajprad I have a patch that compiles, and works something like this:\r\n```\r\nimport torch\r\na, b = torch.multinomial_alias_setup(torch.rand(3,))\r\nres = torch.multinomial_alias_draw(a, b, 3)\r\n```\r\n\r\nIs this what you are expecting?', ""That's awesome, thanks for helping out with this! Could you push a PR with the patch? I think we can get that merged first, and then @HobbitLong or I could integrate that into our categorical sampler with some additional tests and benchmarking. You have already done the important bit here! üòÑ "", 'Just one question on the patch:\r\n\r\n```python\r\nres = torch.multinomial_alias_draw(a, b, 3)\r\n```\r\n\r\nWhat does 3 here indicate? Is that the number of samples, in which case I am guessing it can be any positive integer?\r\n', ""Sorry about the delay, it got late back here then. I'll send in a PR by morning.\r\n\r\nRegarding the number in `multinomial_alias_draw`, 3 indicates the number of samples which can be any positive integer."", ""Hoping this will be available soon!\r\n\r\nBut until y'all get it released, for anyone else finding they need this right away (like I did) you could use this https://github.com/enewe101/pytorch-categorical"", 'Alias multinomial is available in PyTorch as a couple of internal methods in the nightlies as `torch_multinomial_alias_setup` and `torch._multinomial_alias_draw`.', '@vishwakftw @neerajprad should we close this issue then and open a new one for leveraging `_multinomial_alias` in `torch.distributions`?', '@fmassa - I am fine tracking this as part of a separate issue. One caveat is that the alias method currently only accepts 1-D tensor of probability values when building the alias table, which prevents batching with distinct probability distribution tables. As such the alias method cannot be swapped out with the current implementation in all cases. For now, I think it is fine if we raise an exception or a warning and switch to the default method, in such cases.']",[],['lua torch/torch7'],0,0
127,pytorch,3462,closed,sort() and  topk() behave weirdly when given large numbers,"

In the above, a matrix of large numbers, where every element is the same, is added to a matrix of small numbers (this shouldn't affect the ordering of the small matrix). Hence  should all be the same. However,  is different from the rest, meaning adding a large numbers to the input makes  behave weirdly. So does , presumably for the same reason.

I've been getting weird results with my beam search implementation in PyTorch, and this was the culprit. In beam search you have to sort a list of large negative numbers (log probabilities of candidate translation sentences). Can anyone suggest a quick workaround? Thanks!",,"[""This is a floating point accuracy problem; at large numbers, float can't distinguish small differences. If your numbers are not too big, increasing precision to double can help solve your problem:\r\n\r\n```\r\nimport torch\r\n\r\nbatch_size = 3\r\ndim = 3\r\n\r\n### pytorch\r\na1 = torch.DoubleTensor(batch_size, dim).normal_()\r\nv1, r1 = a1.sort(dim=1)\r\n\r\ninc = torch.DoubleTensor(batch_size).fill_(1)[:,None]*99999999\r\na2 = inc + a1\r\nv2, r2 = a2.sort(dim=1)\r\n\r\nprint(r1)\r\nprint(r2)\r\n```\r\n\r\nNumpy does not have this problem because it defaults to double precision.\r\n\r\nIt might also help if you can scale your log probabilities so that they fall within a range that can be accurately represented with floats.""]","['python\r\nimport torch\r\nimport numpy as np\r\n\r\nbatch_size = 5\r\ndim = 10\r\n\r\n### pytorch\r\na1 = torch.randn(batch_size, dim)\r\nv1, r1 = a1.sort(dim=1)\r\n\r\ninc = torch.ones(batch_size)[:,None]*99999999\r\na2 = inc + a1\r\nv2, r2 = a2.sort(dim=1)\r\n\r\n### numpy\r\na3 = a1.numpy()\r\nr3 = np.argsort(a3, axis=1)\r\n\r\ninc2 = np.ones(batch_size)[:,None]*99999999\r\na4 = inc2 + a3\r\nr4 = np.argsort(a4, axis=1)\r\n']","['r1, r2, r3, r4', 'r2', 'torch.sort()', 'torch.topk()']",0,0
128,pytorch,15620,closed,cuda, cuda runtime error(11):invalid argumen at /pytorch/torch/lib/THC/generic/THCTensorCopy.c:70,,[],[],[],0,0
129,pytorch,3131,closed,cuda() methods on Module and Tensor/Variable are inconsistent,"On , the keyword argument is named , but on Tensor/Variable, the keyword argument is just .",,[],[],"['Module', 'device_id', 'device']",0,0
130,pytorch,11504,closed,[JIT] Tracer throws runtime exception for torch.normal,"## Issue description

The normal distribution (and hence all distributions that use the  sampler) is throwing a runtime exception under JIT (this seems to be a recent regression). I have isolated the issue to the following code snippet which throws an exception on PyTorch master, and is breaking many PyTorch models. cc. @zou3519, @apaszke, @fritzo. 

## Code example

The following code that returns a sample from a normal distribution,



throws a Runtime Exception:


## System Info


",oncall: jit,"[""I'm fixing this right now."", ""@apaszke - Just FYI, it seems like this could be a bigger issue and I'm noticing this in `torch._standard_gamma` as well.""]","['python\r\ndef fn(mean, std):\r\n    return torch.normal(mean, std)\r\n\r\ncompiled_fn = torch.jit.trace(fn, (torch.zeros(2, 3), torch.ones(2, 3)))\r\n', '\r\nTraceback (most recent call last):\r\n  File ""/Users/npradhan/workspace/pyro_dev/pyro/examples/ex1.py"", line 10, in <module>\r\n    compiled_fn = torch.jit.trace(fn, (torch.zeros(2, 3), torch.ones(2, 3)))\r\n  File ""/Users/npradhan/miniconda2/envs/pytorch-master/lib/python3.6/site-packages/torch/jit/__init__.py"", line 501, in trace\r\n    module._create_method_from_trace(\'forward\', func, example_inputs)\r\n  File ""/Users/npradhan/workspace/pyro_dev/pyro/examples/ex1.py"", line 7, in fn\r\n    return d.sample()\r\n  File ""/Users/npradhan/miniconda2/envs/pytorch-master/lib/python3.6/site-packages/torch/distributions/normal.py"", line 54, in sample\r\n    return torch.normal(self.loc.expand(shape), self.scale.expand(shape))\r\nRuntimeError: Found an unsupported argument type in the JIT tracer: at::Generator*. File a bug report.\r\n', '\r\n  $ python collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 0.5.0a0+f2f43ad\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.13.3\r\nGCC version: Could not collect\r\nCMake version: version 3.12.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.15.0)\r\n[pip] torch (0.5.0a0+f2f43ad)\r\n[pip] torchfile (0.1.0)\r\n[pip] torchvision (0.2.1)\r\n[conda] torch                     0.5.0a0+f2f43ad           <pip>\r\n[conda] torch                     0.5.0a0+2158f4a           <pip>\r\n[conda] torchfile                 0.1.0                     <pip>\r\n[conda] torchvision               0.2.1                     <pip>\r\n']",['torch.normal'],0,0
131,pytorch,260,closed,define default GPU device,"I think it would be useful to have a cuda.set_default_device in pytorch, so that the GPU 0 is not always the default one.",high priority,"['The only problem with setting the default device are slightly unclear semantics of this function, when used inside the `with` block. e.g.\r\n\r\n```python\r\nwith torch.cuda.device(1):\r\n    torch.cuda.set_default_device(2)\r\n    y = torch.cuda.FloatTensor(2)\r\n    # does the default device selection affect the block?\r\n    # y.get_device() == 2 or y.get_device() == 1?\r\nx = torch.cuda.FloatTensor(2)\r\n# after all with clauses do we return to the new default device or to the old one?\r\n# x.get_device() == 2 or x.get_device() == 0?\r\n```', ""What about having `set_default_device` setting a global variable? So that it's doing the same thing wherever it's executed (inside of a `with` or not). And I guess the `with` should prevail upon the default GPU, so in the example `y` would be on device 1 and `x` on device 2."", ""Just expose a function `torch.cuda.set_device()`. The notion of a default device is more complicated than setting the current device (as you point out) and doesn't buy much over just setting the device.\r\n\r\nI'm still not entirely convinced this is worthwhile. `CUDA_VISIBLE_DEVICES` seems like a better method."", ""@colesbury i think it's a convenience function that's probably needed. For workflows that are not: python [myscript.py]. When you have long running ipython notebooks etc., it gets useful."", ""Yeah I think `set_device` would be better. It has much clearer semantics. With `set_default_device` it might sometimes work inside a nested function, and sometimes silently have no effect.\r\n\r\n@colesbury I also don't like this very much, but as @soumith says, I think it's useful for notebooks. However, we should clearly discourage its usage in the docs, except for these situations."", 'Implemented in #282.', ""What do you think about introducing a function `torch.set_default_device` working with the new device semantics ?\r\n\r\n```\r\ntorch.set_default_device(torch.device('cpu'))\r\n\r\ntorch.set_default_device(torch.device('cuda:0'))\r\n```""]",[],[],0,0
132,pytorch,13494,closed,Throws error when backward through sum() twice,"## üêõ Bug
 is special that we can backward() through it multiple times since it doesn't have saved variable, so nothing was really destroyed after the first backward.

This behavior is bad as we might silently backward through it multiple times without noticing, but the grad accumulates.

## To Reproduce


@zdevito told me briefly how to fix, I will send a PR for it soon. 
",,"[""hmm wait. I don't think this should be a problem. sum backward doesn't need a buffer and for graph that doesn't need a buffer, users should be able to backprop however many times they want."", '@ailzhang I\'m not sure this should be ""fixed"". if the engine can `.backward()` without affecting correctness, it should be doing so.', 'But if this was [EDIT: not] done intentionally, the accumulated grad in `a` might not be what the user expected right? ', ""if it was not done intentionally, it's a user-side bug. But it should be possible to do it intentionally because it is a valid thing to do."", 'Yea I understand this is valid from the memory effiency perspective, that doing so is totally fine.\r\nBut this behavior is not consistent with all other operators, I thought it was clearer if we throw an error for this special case and no confusion is made for users. What do you think @soumith @SsnL  @zdevito  ?', 'The other operators error because they need the buffer that is freed, not because that users backprop more than once.', '@ailzhang I actually think this should be the behavior that happens, so I dont think we should throw an error.\r\nMaybe we can empathize with your perspective better if you explain in what case you had to deal with this.', 'Ailing brought up a good point in person: typically when you accidentally have a gradient graph lying around that you have already used, you will get an error because you cannot backward twice.  This is a good error! You were most likely doing something wrong and if silently works, your grads will accumulate in unexpected cases. But in this particular case, the error never appears for basically implementation-defined reasons (at least, reasons that are more complicated than we expect our users to understand).', ""I don't think we can expect our users to necessarily know when we will be and won't be saving buffers, so being inconsistent based on whether we actually have buffers can lead to surprising results."", 'While I do agree the behavior is implementation dependent, this code is sipmly incorrect in the vast majority of cases, and I don‚Äôt think anyone will ever be hit by it. The ‚Äúif you backward multiple times, grads get accumulated‚Äù invariant is still true, no matter if you ask us to retain buffers or not. It‚Äôs just that without those flags (which is not 100% correct) we can still satisfy your request and will proceed without an error. There‚Äôs nothin unexpected in it I‚Äôd say, and I don‚Äôt think spending time to do extra checks when they‚Äôre unnecessary is a good idea', ""I'd expect that most people think that retain_graph=True is the only valid way to be allowed to call backward twice, and this behavior is an exception. If it were frequently occurring it would be really bad: someone would write a model relying on backwards to work multiple times, everything would work and then someone else would come along, replace one op for another, and get an error someone else in the code. \r\n\r\nHowever, since it requires you to only use things like sum and add to cause this corner case to happen, it is probably pretty rare for anyone to ever get into this situation in the first place. So it is probably not worth the added complexity of making the API consistent."", 'Thanks all for the discussion! :D \r\n\r\nTo add a little context here, we discovered this behavior mismatch while comparing the sparse tensor backward to the dense one(cc: @weiyangfb ), and by using `%timeit` we were surprised to see that `sum` is allowed to backprop unlimited times but sparse sum throws an error, while all other operators don\'t have this mismatch.  We figured out the specialness of dense `sum` with help of @zdevito  and @goldsborough. We agreed that this might be a common sense for users that ""Pytorch destroys something after the first backward, to keep the graph I have to do retain_graph"", and this mismatched behavior could potentially cause some confusion to users. \r\n\r\nBut I totally agree that this case is too rare to happen in real user code, and it doesn\'t worth the added complexity. Closing the issue. ']","['\r\na = torch.randn(2, 2, 2, 2).requires_grad_(True)\r\na_sum = a.sum()\r\na_sum.backward()\r\na_sum.backward()\r\n']",['sum()'],0,0
133,pytorch,11737,closed,torch.utils.cpp_extension.load doesn't change device after moving the model,"## Issue description
After using torch.utils.cpp_extension.load, returned module always puts tensors at GPU 0 ('cuda:0')
It causes problems when model is moved to another GPU. 
In my example I used code from https://github.com/mapillary/inplace_abn
See also mapillary/inplace_abn#52

## Code example

## System Info
Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).
PyTorch version: 0.4.0
Is debug build: No
CUDA used to build PyTorch: 8.0.61

OS: Ubuntu 16.04.4 LTS
GCC version: (Ubuntu 4.9.4-2ubuntu1~16.04) 4.9.4
CMake version: version 3.12.0

Python version: 3.5
Is CUDA available: Yes
CUDA runtime version: 9.0.176
GPU models and configuration: 
GPU 0: GeForce GTX 1080 Ti
GPU 1: GeForce GTX 1080 Ti
GPU 2: GeForce GTX 1080 Ti
GPU 3: GeForce GTX 1080 Ti

Nvidia driver version: 384.130
cuDNN version: Probably one of the following:
/usr/local/cuda-8.0/lib64/libcudnn.so
/usr/local/cuda-8.0/lib64/libcudnn.so.6
/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21
/usr/local/cuda-8.0/lib64/libcudnn_static.a
/usr/local/cuda-9.0/lib64/libcudnn.so.7.0.5
/usr/local/cuda-9.0/lib64/libcudnn_static.a
",,"[""It's a bug in `inplace_abn`. I'll comment on the issue there.""]","['\r\n""""""\r\nexample from https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/502_GPU.py\r\nDependencies:\r\ntorch: 0.4\r\ntorchvision\r\n""""""\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.utils.data as Data\r\nimport torchvision\r\n\r\nfrom modules.bn import InPlaceABN \r\n\r\n#===================================#\r\n# Change to cuda:0 to make it work  #\r\ndevice = torch.device(\'cuda:1\')     #\r\nprint(device)                       #\r\n#===================================#\r\n\r\nEPOCH = 1\r\nBATCH_SIZE = 50\r\nLR = 0.001\r\nDOWNLOAD_MNIST = True\r\n\r\ntrain_data = torchvision.datasets.MNIST(root=\'./mnist/\', train=True, transform=torchvision.transforms.ToTensor(), download=DOWNLOAD_MNIST,)\r\ntrain_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\r\n\r\n\r\nclass CNN(nn.Module):\r\n    def __init__(self):\r\n        super(CNN, self).__init__()\r\n        self.conv1 = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2,),\r\n                                   nn.ReLU(), nn.MaxPool2d(kernel_size=2),)\r\n        self.conv2 = nn.Sequential(nn.Conv2d(16, 32, 5, 1, 2), nn.ReLU(), nn.MaxPool2d(2),)\r\n        \r\n        self.bn = InPlaceABN(32)\r\n        self.out = nn.Linear(32 * 7 * 7, 10)\r\n\r\n    def forward(self, x):\r\n        x = self.conv1(x)\r\n        x = self.conv2(x)\r\n        x = self.bn(x)\r\n        x = x.view(x.size(0), -1)\r\n        output = self.out(x)\r\n        return output\r\n\r\ncnn = CNN()\r\n\r\ncnn.to(device)      # Moves all model parameters and buffers to the GPU\r\n\r\noptimizer = torch.optim.Adam(cnn.parameters(), lr=LR)\r\nloss_func = nn.CrossEntropyLoss().cuda(device)\r\n\r\nfor epoch in range(EPOCH):\r\n    for step, (x, y) in enumerate(train_loader):\r\n        \r\n        b_x = x.to(device)     # Tensor on GPU\r\n        b_y = y.to(device)     # Tensor on GPU\r\n        \r\n        # This is where error happens\r\n        output = cnn(b_x)\r\n        loss = loss_func(output, b_y)\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\r\n\r\n        if step % 50 == 0:\r\n            print(""Test passed"")\r\n            break\r\n\r\n\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-2-c7d32883c675> in <module>()\r\n     60         b_y = y.to(device)     # Tensor on GPU\r\n     61 \r\n---> 62         output = cnn(b_x)\r\n     63         loss = loss_func(output, b_y)\r\n     64         optimizer.zero_grad()\r\n\r\n~/torch-py3/lib/python3.5/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    489             result = self._slow_forward(*input, **kwargs)\r\n    490         else:\r\n--> 491             result = self.forward(*input, **kwargs)\r\n    492         for hook in self._forward_hooks.values():\r\n    493             hook_result = hook(self, input, result)\r\n\r\n<ipython-input-2-c7d32883c675> in forward(self, x)\r\n     41         x = self.conv1(x)\r\n     42         x = self.conv2(x)\r\n---> 43         x = self.bn(x)\r\n     44         x = x.view(x.size(0), -1)\r\n     45         output = self.out(x)\r\n\r\n~/torch-py3/lib/python3.5/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    489             result = self._slow_forward(*input, **kwargs)\r\n    490         else:\r\n--> 491             result = self.forward(*input, **kwargs)\r\n    492         for hook in self._forward_hooks.values():\r\n    493             hook_result = hook(self, input, result)\r\n\r\n~/kaggle_salt/modules/bn.py in forward(self, x)\r\n    106     def forward(self, x):\r\n    107         return inplace_abn(x, self.weight, self.bias, self.running_mean, self.running_var,\r\n--> 108                            self.training, self.momentum, self.eps, self.activation, self.slope)\r\n    109 \r\n    110 \r\n\r\n~/kaggle_salt/modules/functions.py in forward(ctx, x, weight, bias, running_mean, running_var, training, momentum, eps, activation, slope)\r\n     97 \r\n     98             # Update running stats\r\n---> 99             running_mean.mul_((1 - ctx.momentum)).add_(ctx.momentum * mean)\r\n    100             running_var.mul_((1 - ctx.momentum)).add_(ctx.momentum * var * count / (count - 1))\r\n    101 \r\n\r\nRuntimeError: arguments are located on different GPUs at /pytorch/aten/src/THC/generated/../generic/THCTensorMathPointwise.cu:233\r\n']",[],0,0
134,pytorch,3124,closed,Does pytorch have Softmax3D Ôºü,"I am doing a 3d project and I found that pytorch has conv3d, pooling3d and batchnorm3d, but I didn't find the softmax3d. 
Does anyone have softmax3d layer? ",,"[""We don't have a layer, but the functional version works:\r\n```python\r\nimport torch.nn.functional as F\r\nF.softmax(Variable(torch.randn(1, 3, 5, 5, 5)))\r\n```"", '@apaszke Thanks for your kind hearted reply. However, `F.softmax` is only support  1D, 2D, 3D or 4D tensor, not support 5D tensor (batch, channel, Width, Height, Length ) yet. So I come here to encounter someone who has implemented the  5D tensor supported softmax.\r\n\r\nThe followings is the output of the code you shared.\r\n```\r\n>>> import torch\r\n>>> from torch.autograd import Variable                                                                               \r\n>>> import torch.nn.functional as F                                                                                   \r\n>>> a=F.softmax(Variable(torch.randn(1, 3, 5, 5, 5)))                                                                 \r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/jsptgpu/anaconda2/lib/python2.7/site-packages/torch/nn/functional.py"", line 529, in softmax\r\n    return _functions.thnn.auto.Softmax.apply(input)\r\n  File ""/home/jsptgpu/anaconda2/lib/python2.7/site-packages/torch/nn/_functions/thnn/auto.py"", line 174, in forward\r\n    getattr(ctx._backend, update_output.name)(ctx._backend.library_state, input, output, *args)\r\nRuntimeError: invalid argument 2: 1D, 2D, 3D or 4D tensor expected at /pytorch/torch/lib/THNN/generic/SoftMax.c:40\r\n```\r\n\r\n', 'Ah my bad! That will be fixed once #2899 lands. I just happened to have that change installed locally', 'Wow, Great! Thank you. When and how that version would be available? ', ""Once it's merged it will be in master, so you can use it as long as you install from source. After this, it will be in the next binaries, but I don't know when we'll release them"", 'OK, got it, thank you very much.', 'Hello I am wondering do we have the merged Softmax3D in master? There seems to be no document description for Softmax3D yet.\r\n\r\nThanks!', 'yes it is there. now Softmax can take `dim` argument that will allow you to do Softmax3D']",[],[],0,0
135,pytorch,12510,closed,[feature request] fix Windows cmake scripts to deal with \backlashes in paths ,"## Issue description

Getting a bunch of Invalid escape sequence \N while compiling from source.
Coming from paths with unescaped backslashes in them.
Trying to resolve by myself in the meantime but its a real pain

## Code example

Invalid escape sequence \N                                                                                                                                                                                                                                                                                                                                                                                    Policy CMP0010 is not set: Bad variable reference syntax is an error.  Run                                                                                                                             ""cmake --help-policy CMP0010"" for policy details.  Use the cmake_policy                                                                                                                                command to set the policy and suppress this warning.                                                                                                                                                 This warning is for project developers.  Use -Wno-dev to suppress it.                                                                                                                                                                                                                                                                                                                                         CMake Warning (dev) at caffe2_gpu_generated_THCStorageCopy.cu.obj.Release.cmake:178 (execute_process):                                                                                                   Syntax error in cmake code at                                                                                                                                                                                                                                                                                                                                                                                   C:/Users/user/pytorchrc1/pytorch/build/caffe2/CMakeFiles/caffe2_gpu.dir/__/aten/src/THC/caffe2_gpu_generated_THCStorageCopy.cu.obj.Release.cmake:178                                                                                                                                                                                                                                                      when parsing string                                                                                                                                                                                                                                                                                                                                                                                             C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0/bin/nvcc.exe;-M;-D__CUDACC__;C:/Users/user/pytorchrc1/pytorch/aten/src/THC/THCStorageCopy.cu;-o;C:/Users/user/pytorchrc1/pytorch/bui   ld/caffe2/CMakeFiles/caffe2_gpu.dir/__/aten/src/THC/caffe2_gpu_generated_THCStorageCopy.cu.obj.NVCC-depend;-ccbin;C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.11.   25503/bin/Hostx64/x64;-m64;-Dcaffe2_gpu_EXPORTS;-DCAFFE2_BUILD_MAIN_LIB;-DNOMINMAX;-D_CRT_SECURE_NO_DEPRECATE=1;-DUSE_MSC_ATOMICS=1;-DTH_BLAS_MKL;-D_OPENMP_NOFORCE_MANIFEST;-Xcompiler;,""/wd4267"",""   /wd4251"",""/wd4522"",""/wd4522"",""/wd4838"",""/wd4305"",""/wd4244"",""/wd4190"",""/wd4101"",""/wd4996"",""/wd4275"",""/EHa"",""-DONNX_NAMESPACE=onnx_torch"",""-openmp"",""/MP"",""/bigobj"",""-DHAVE_AVX_CPU_DEFINITION"",""-DHAV   E_AVX2_CPU_DEFINITION"",""/MD"",""/O2"",""/Ob2"",""/MP"",""/bigobj"";-DONNX_NAMESPACE=onnx_torch;-gencode;arch=compute_30,code=sm_30;-Xcudafe;--diag_suppress=cc_clobber_ignored;-Xcudafe;--diag_suppress=integ   er_sign_change;-Xcudafe;--diag_suppress=useless_using_declaration;-Xcudafe;--diag_suppress=set_but_not_used;-Xcompiler;-MD;--expt-relaxed-constexpr;--expt-extended-lambda;-Xcompiler;/wd4819;-Xcomp   iler;/wd4503;-Xcompiler;/wd4190;-Xcompiler;/wd4244;-Xcompiler;/wd4251;-Xcompiler;/wd4275;-Xcompiler;/wd4522;-Wno-deprecated-gpu-targets;--expt-extended-lambda;-gencode;arch=compute_30,code=sm_30;-   DCUDA_HAS_FP16=1;-D__CUDA_NO_HALF_OPERATORS__;-D__CUDA_NO_HALF_CONVERSIONS__;-D__CUDA_NO_HALF2_OPERATORS__;-DNVCC;-IC:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0/include;-IC:/Users/yacin   e/pytorchrc1/pytorch/aten/src;-IC:/Users/user/pytorchrc1/pytorch/build;-IC:/Users/user/pytorchrc1/pytorch;-IC:/Users/user/pytorchrc1/pytorch/third_party/protobuf/src;-IC:/Program Files (x86)   /IntelSWTools/compilers_and_libraries/windows/mkl/include;-IC:/Users/user/pytorchrc1/pytorch/cmake/../third_party/eigen;-IC:/Users/user/pytorchrc1/pytorch/cmake/../third_party/pybind11/include   ;-IC:/Users/user/pytorchrc1/pytorch/cmake/../third_party/cub;-IC:/Users/user/pytorchrc1/pytorch/third_party/onnx;-IC:/Users/user/pytorchrc1/pytorch/build/third_party/onnx;-IC:/Users/user/p   ytorchrc1/pytorch/build/caffe2/aten/src/TH;-IC:/Users/user/pytorchrc1/pytorch/aten/src/TH;-IC:/Users/user/pytorchrc1/pytorch/aten/src/THC;-IC:/Users/user/pytorchrc1/pytorch/build/caffe2/aten   /src/THC;-IC:/Users/user/pytorchrc1/pytorch/aten/src/THCUNN;-IC:/Users/user/pytorchrc1/pytorch/aten/src/ATen/cuda;-IC:/Users/user/pytorchrc1/pytorch/build/caffe2/aten/src;-IC:/Users/user/p   ytorchrc1/pytorch/build/aten/src;-IC:/Users/user/pytorchrc1/pytorch/aten/src/THNN;-IC:/Users/user/pytorchrc1/pytorch/aten/../third_party/catch/single_include;-IC:/Users/user/pytorchrc1/pytor   ch/build/caffe2/aten/src/ATen;-IC:/Users/user/pytorchrc1/pytorch/aten/src/ATen/..;-IC:\NVIDIA\CUDA\9.0\cudnn-v7.3\include                                                                                                                                                                                                                                                                                     Invalid escape sequence \N                                                                                                                                                                                                                                                                                                                                                                                    Policy CMP0010 is not set: Bad variable reference syntax is an error.  Run                                                                                                                             ""cmake --help-policy CMP0010"" for policy details.  Use the cmake_policy                                                                                                                                command to set the policy and suppress this warning.                                                                                                                                                 Call Stack (most recent call first):                                                                                                                                                                     caffe2_gpu_generated_THCStorageCopy.cu.obj.Release.cmake:203 (cuda_execute_process)                                                                                                                  This warning is for project developers.  Use -Wno-dev to suppress it.                                                  

## System Info

- PyTorch or Caffe2: PyTorch
- How you installed PyTorch (conda, pip, source): source
- Build command you used (if compiling from source): python setup.py build -f
- OS: Microsoft Windows 10 Pro x64 Version	10.0.17134 Build 17134
- PyTorch version: 
- Python version: 3.6.5
- CUDA/cuDNN version: 9.0patch4/7.3
- GPU models and configuration: Nvidia Quadro K2100M
- GCC version (if compiling from source): N/A
- CMake version: the one from VS as described in compilation instruction
- Versions of any other relevant libraries:
",module: windows,[],[],[],0,0
136,pytorch,26142,closed,TorchScript fails to compile methods with misindented comments,"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Reproducer:




cc @suo",oncall: jit,['Duplicate of https://github.com/pytorch/pytorch/issues/25043'],"['python\r\nimport torch\r\n\r\nclass Bug(torch.nn.Module):\r\n    def forward(self, x):\r\n        # no bug\r\n#       bug: ""IndentationError: unexpected indent""\r\n        return x\r\n\r\nf = torch.jit.script(Bug())\r\n', 'bash\r\n$ python bug.py\r\n\r\nTraceback (most recent call last):\r\n  File ""bug.py"", line 9, in <module>\r\n    f = torch.jit.script(Bug())\r\n  File ""/home/mvz/pytorch/torch/jit/__init__.py"", line 1188, in script\r\n    return torch.jit.torch.jit._recursive.recursive_script(obj)\r\n  File ""/home/mvz/pytorch/torch/jit/_recursive.py"", line 166, in recursive_script\r\n    stubs = list(map(make_stub, filtered_methods))\r\n  File ""/home/mvz/pytorch/torch/jit/_recursive.py"", line 163, in make_stub\r\n    return torch.jit.script_method(func, _jit_internal.createResolutionCallbackFromClosure(func))\r\n  File ""/home/mvz/pytorch/torch/jit/__init__.py"", line 1265, in script_method\r\n    ast = get_jit_def(fn, self_name=""ScriptModule"")\r\n  File ""/home/mvz/pytorch/torch/jit/frontend.py"", line 164, in get_jit_def\r\n    py_ast = ast.parse(dedent_src)\r\n  File ""/home/mvz/local/anaconda3/envs/ve3/lib/python3.7/ast.py"", line 35, in parse\r\n    return compile(source, filename, mode, PyCF_ONLY_AST)\r\n  File ""<unknown>"", line 1\r\n    def forward(self, x):\r\n    ^\r\nIndentationError: unexpected indent\r\n']",[],0,0
137,pytorch,14954,closed,undefined reference to `vmdLog2'  ,"

## Issue description

/data1/bryanleoliu/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp: In member function ‚Äòc10d::AlgorithmEntry* c10d::ProcessGroupGloo::checkout(const c10d::AlgorithmKey&)‚Äô:
/data1/bryanleoliu/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp:445:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (vec.size() != cacheNumAlgorithmEntries_) {
                     ^
[ 42%] Linking CXX static library libc10d.a
[ 42%] Built target c10d
[ 47%] Building NVCC (Device) object test/CMakeFiles/c10d_cuda_test.dir/c10d_cuda_test_generated_CUDATest.cu.o
Scanning dependencies of target ProcessGroupMPITest
Scanning dependencies of target TCPStoreTest
Scanning dependencies of target FileStoreTest
[ 52%] Building CXX object test/CMakeFiles/FileStoreTest.dir/FileStoreTest.cpp.o
[ 57%] Building CXX object test/CMakeFiles/TCPStoreTest.dir/TCPStoreTest.cpp.o
[ 61%] Building CXX object test/CMakeFiles/ProcessGroupMPITest.dir/ProcessGroupMPITest.cpp.o
[ 66%] Linking CXX executable FileStoreTest
/data1/bryanleoliu/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp: In function ‚Äòvoid testAllreduce(int)‚Äô:
/data1/bryanleoliu/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp:19:57: warning: ‚Äòat::Tensor at::ones(const at::Type&, at::IntList)‚Äô is deprecated (declared at /data1/bryanleoliu/pytorch/torch/lib/tmp_install/include/ATen/Functions.h:3936) [-Wdeprecated-declarations]
     auto tensor = at::ones(at::CPU(at::kFloat), {16, 16}) * i;
                                                         ^
/data1/bryanleoliu/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp: In function ‚Äòvoid testBroadcast(int)‚Äô:
/data1/bryanleoliu/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp:61:59: warning: ‚Äòat::Tensor at::ones(const at::Type&, at::IntList)‚Äô is deprecated (declared at /data1/bryanleoliu/pytorch/torch/lib/tmp_install/include/ATen/Functions.h:3936) [-Wdeprecated-declarations]
       auto tensor = at::ones(at::CPU(at::kFloat), {16, 16}) * i;
                                                           ^
/data1/bryanleoliu/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp:64:60: warning: ‚Äòat::Tensor at::zeros(const at::Type&, at::IntList)‚Äô is deprecated (declared at /data1/bryanleoliu/pytorch/torch/lib/tmp_install/include/ATen/Functions.h:4374) [-Wdeprecated-declarations]
       auto tensor = at::zeros(at::CPU(at::kFloat), {16, 16});
                                                            ^
/data1/bryanleoliu/pytorch/torch/lib/tmp_install/lib/libcaffe2.so: undefined reference to vmsLog2'
collect2: error: ld returned 1 exit status
make[2]: *** [test/FileStoreTest] Error 1
make[1]: *** [test/CMakeFiles/FileStoreTest.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
[ 71%] Linking CXX executable TCPStoreTest
[ 76%] Linking CXX executable ProcessGroupMPITest
/data1/bryanleoliu/pytorch/torch/lib/tmp_install/lib/libcaffe2.so: undefined reference to vmsLog2'
collect2: error: ld returned 1 exit status
make[2]: *** [test/TCPStoreTest] Error 1
make[1]: *** [test/CMakeFiles/TCPStoreTest.dir/all] Error 2
/data1/bryanleoliu/pytorch/torch/lib/tmp_install/lib/libcaffe2.so: undefined reference to vmsLog2'
collect2: error: ld returned 1 exit status
make[2]: *** [test/ProcessGroupMPITest] Error 1
make[1]: *** [test/CMakeFiles/ProcessGroupMPITest.dir/all] Error 2
Scanning dependencies of target c10d_cuda_test
[ 80%] Linking CXX static library libc10d_cuda_test.a
[ 80%] Built target c10d_cuda_test
make: *** [all] Error 2
Failed to run 'bash tools/build_pytorch_libs.sh --use-cuda --use-nnpack --use-mkldnn nccl caffe2 nanopb libshm gloo THD c10d'




## System Info

`

- PyTorch or Caffe2:Pytorch
- How you installed PyTorch (conda, pip, source): source 
- Build command you used (if compiling from source):python setup.py install
- OS: centos 7
- PyTorch version:0.4.1
- Python version:3.6
- CUDA/cuDNN version: 9.0/7.3
- GPU models and configuration: 8 nvidia p40
- GCC version (if compiling from source):4.8.5
- CMake version:3.12.0
- Versions of any other relevant libraries: 


cc @malfet",module: build triaged,"['I reinstall mkl by conda install mkl & conda install mkl-include , but it doesnt work all the time \r\ncan anyone help me?\r\n', ""i have the same issue.\r\n\r\n### System Info\r\n```\r\n- PyTorch or Caffe2:Pytorch\r\n- How you installed PyTorch (conda, pip, source): source \r\n- Build command you used (if compiling from source):python setup.py install\r\n- OS: Ubuntu 16.04\r\n- PyTorch version:0.4.1\r\n- Python version:3.6.5\r\n- CUDA/cuDNN version: 8.0/6.0.21\r\n- GPU models and configuration: 4 nvidia titan-x\r\n- GCC version (if compiling from source): 4.9.2\r\n- CMake version: 3.6.3\r\n- Versions of any other relevant libraries: nvcc=2.2.13\r\n```\r\n\r\n### Error info:\r\n```\r\n/home/user/pytorch/torch/lib/tmp_install/lib/libcaffe2.so: undefined reference to `vmdLog2'\r\n/home/user/pytorch/torch/lib/tmp_install/lib/libcaffe2.so: undefined reference to `vmsLog2'\r\ncollect2: error: ld returned 1 exit status\r\ntest/CMakeFiles/FileStoreTest.dir/build.make:118: recipe for target 'test/FileStoreTest' failed\r\nmake[2]: *** [test/FileStoreTest] Error 1\r\nCMakeFiles/Makefile2:246: recipe for target 'test/CMakeFiles/FileStoreTest.dir/all' failed\r\nmake[1]: *** [test/CMakeFiles/FileStoreTest.dir/all] Error 2\r\nmake[1]: *** Waiting for unfinished jobs....\r\n/home/user/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp: In function ‚Äòvoid testAllreduce(int)‚Äô:\r\n/home/user/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp:19:57: warning: ‚Äòat::Tensor at::ones(const at::Type&, at::IntList)‚Äô is deprecated (declared at /home/user/pytorch/torch/lib/tmp_install/include/ATen/Functions.h:3936) [-Wdeprecated-declarations]\r\n     auto tensor = at::ones(at::CPU(at::kFloat), {16, 16}) * i;\r\n                                                         ^\r\n/home/user/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp: In function ‚Äòvoid testBroadcast(int)‚Äô:\r\n/home/user/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp:61:59: warning: ‚Äòat::Tensor at::ones(const at::Type&, at::IntList)‚Äô is deprecated (declared at /home/user/pytorch/torch/lib/tmp_install/include/ATen/Functions.h:3936) [-Wdeprecated-declarations]\r\n       auto tensor = at::ones(at::CPU(at::kFloat), {16, 16}) * i;\r\n                                                           ^\r\n/home/user/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp:64:60: warning: ‚Äòat::Tensor at::zeros(const at::Type&, at::IntList)‚Äô is deprecated (declared at /home/user/pytorch/torch/lib/tmp_install/include/ATen/Functions.h:4374) [-Wdeprecated-declarations]\r\n       auto tensor = at::zeros(at::CPU(at::kFloat), {16, 16});\r\n                                                            ^\r\n[ 66%] Linking CXX executable TCPStoreTest\r\n/home/user/pytorch/torch/lib/tmp_install/lib/libcaffe2.so: undefined reference to `vmdLog2'\r\n/home/user/pytorch/torch/lib/tmp_install/lib/libcaffe2.so: undefined reference to `vmsLog2'\r\ncollect2: error: ld returned 1 exit status\r\ntest/CMakeFiles/TCPStoreTest.dir/build.make:118: recipe for target 'test/TCPStoreTest' failed\r\nmake[2]: *** [test/TCPStoreTest] Error 1\r\nCMakeFiles/Makefile2:209: recipe for target 'test/CMakeFiles/TCPStoreTest.dir/all' failed\r\nmake[1]: *** [test/CMakeFiles/TCPStoreTest.dir/all] Error 2\r\n[ 70%] Linking CXX executable ProcessGroupMPITest\r\n/home/user/pytorch/torch/lib/tmp_install/lib/libcaffe2.so: undefined reference to `vmdLog2'\r\n/home/user/pytorch/torch/lib/tmp_install/lib/libcaffe2.so: undefined reference to `vmsLog2'\r\ncollect2: error: ld returned 1 exit status\r\ntest/CMakeFiles/ProcessGroupMPITest.dir/build.make:118: recipe for target 'test/ProcessGroupMPITest' failed\r\nmake[2]: *** [test/ProcessGroupMPITest] Error 1\r\nCMakeFiles/Makefile2:172: recipe for target 'test/CMakeFiles/ProcessGroupMPITest.dir/all' failed\r\nmake[1]: *** [test/CMakeFiles/ProcessGroupMPITest.dir/all] Error 2\r\nScanning dependencies of target c10d_cuda_test\r\n[ 75%] Linking CXX static library libc10d_cuda_test.a\r\n[ 75%] Built target c10d_cuda_test\r\nMakefile:138: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\nFailed to run 'bash tools/build_pytorch_libs.sh --use-cuda --use-nnpack caffe2 nanopb libshm gloo THD c10d'\r\n```\r\n"", ""@hitlxm \r\nI'm pretty sure this is caused by some newly updated packages installed by conda.\r\nI copied a miniconda3 installation folder (which was proven to build 0.4.1 pytorch successfully 3 weeks ago) from another machine. Then it went through the build steps smoothly."", 'I have the same problem', '@hitlxm @frankang If you were using conda virtual environments, you may have to give this a try.\r\n#15548 \r\n> export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-""$(dirname $(which conda))/../""}\r\n\r\nIf you are using a virtual env, `CMAKE_PREFIX_PATH` should be `CONDA_PREFIX` not conda root. Otherwise, cmake gets confused about where to find dependencies.\r\n', 'Update the mkl, mkl-include, ninja to the newest version and it can be build successfully.', 'Looks like its an old issue. Please reopen if the problem reproduces. ']",[],"[""vmdLog2'\r\n/data1/bryanleoliu/pytorch/torch/lib/tmp_install/lib/libcaffe2.so: undefined reference to "", ""vmdLog2'\r\n/data1/bryanleoliu/pytorch/torch/lib/tmp_install/lib/libcaffe2.so: undefined reference to "", ""vmdLog2'\r\n/data1/bryanleoliu/pytorch/torch/lib/tmp_install/lib/libcaffe2.so: undefined reference to "", '']",0,0
138,pytorch,31095,closed,pin_memory stuck in DDP/Reducer constructor,"### Problem

@baobablyh  @pietern and I are working on PR #28883. We are trying to create pinned memory tensors in Reducer constructor:


https://github.com/pytorch/pytorch/blob/4902a08e44e1e8dbcced99b9c18321cf9bb644d5/torch/csrc/distributed/c10d/reducer.cpp#L113-L116

However, we observed that, one of the process created the pinned memory tensor successfully, but another process stuck at the line above. More specifically, it stuck when creating the pinned memory storage (stack traces are attached at the end)

https://github.com/pytorch/pytorch/blob/4902a08e44e1e8dbcced99b9c18321cf9bb644d5/aten/src/ATen/native/Memory.cpp#L24

@baobablyh also discovered that, when adding 3 seconds sleep time after the Reducer constructioin, this problem disappear. So, the symptom likes like something is not ready. It might be caused by invoking async copy in one process while another process has an unready the pinned memory tensor (not sure).

@ngimel @mruberry Do you know what could cause this problem? Or is there any way to synchronize until the pinned memory is ready?

### Reproduce

1. fetch #28883
2. comment out the  in 
 https://github.com/pytorch/pytorch/blob/4902a08e44e1e8dbcced99b9c18321cf9bb644d5/torch/nn/parallel/distributed.py#L308-L310
3. run 

### Trace

Process 1



Process 2



### Environment



cc @ezyang @gchanan @zou3519 @ngimel @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528",high priority module: cuda oncall: distributed triaged,"[""Right before the point where you suggest adding a `time.sleep(3)` the model parameters are broadcast from rank 0 to all other ranks. This runs NCCL kernels (see `_broadcast_coalesced`).\r\n\r\nI won't be surprised that this is what's interfering with pinned memory allocation.\r\n\r\nCan you run this with `NCCL_DEBUG=INFO` to see if anything stands out?"", '@pietern \r\n\r\nWithout `time.sleep(3)`, which hangs:\r\n\r\n```\r\ntest/test_c10d.py::DistributedDataParallelTest::test_find_unused_parameters_kwarg devgpu217:2442156:2442156 [0] NCCL INFO Bootstrap : Using [0]eth0:2803:6080:c838:a003::1<0>\r\ndevgpu217:2442156:2442156 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\n\r\ndevgpu217:2442156:2442156 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\r\ndevgpu217:2442156:2442156 [0] NCCL INFO NET/Socket : Using [0]eth0:2803:6080:c838:a003::1<0>\r\nNCCL version 2.4.8+cuda9.2\r\ndevgpu217:2442156:2442270 [0] NCCL INFO Setting affinity for GPU 0 to 0f,ff000fff\r\ndevgpu217:2442157:2442157 [1] NCCL INFO Bootstrap : Using [0]eth0:2803:6080:c838:a003::1<0>\r\ndevgpu217:2442157:2442157 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\n\r\ndevgpu217:2442157:2442157 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\r\ndevgpu217:2442157:2442157 [1] NCCL INFO NET/Socket : Using [0]eth0:2803:6080:c838:a003::1<0>\r\ndevgpu217:2442157:2442271 [1] NCCL INFO Setting affinity for GPU 1 to 0f,ff000fff\r\ndevgpu217:2442156:2442270 [0] NCCL INFO Channel 00 :    0   1\r\ndevgpu217:2442157:2442271 [1] NCCL INFO Ring 00 : 1[1] -> 0[0] via P2P/IPC\r\ndevgpu217:2442156:2442270 [0] NCCL INFO Ring 00 : 0[0] -> 1[1] via P2P/IPC\r\ndevgpu217:2442156:2442270 [0] NCCL INFO Using 256 threads, Min Comp Cap 5, Trees disabled\r\ndevgpu217:2442157:2442271 [1] NCCL INFO comm 0x7f0380001980 rank 1 nranks 2 cudaDev 1 nvmlDev 1 - Init COMPLETE\r\ndevgpu217:2442156:2442270 [0] NCCL INFO comm 0x7f0378001980 rank 0 nranks 2 cudaDev 0 nvmlDev 0 - Init COMPLETE\r\ndevgpu217:2442156:2442156 [0] NCCL INFO Launch mode Parallel\r\n```\r\n\r\nWith `time.sleep(3)` which does not hang:\r\n\r\n```\r\ntest/test_c10d.py::DistributedDataParallelTest::test_find_unused_parameters_kwarg devgpu217:2434724:2434724 [0] NCCL INFO Bootstrap : Using [0]eth0:2803:6080:c838:a003::1<0>                                                                                                                                                                 \r\ndevgpu217:2434724:2434724 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).     \r\n                                                                                                                                                                      \r\ndevgpu217:2434724:2434724 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\r\ndevgpu217:2434724:2434724 [0] NCCL INFO NET/Socket : Using [0]eth0:2803:6080:c838:a003::1<0>                                                                          \r\nNCCL version 2.4.8+cuda9.2                                                                                                                                            \r\ndevgpu217:2434724:2435858 [0] NCCL INFO Setting affinity for GPU 0 to 0f,ff000fff\r\ndevgpu217:2434725:2434725 [1] NCCL INFO Bootstrap : Using [0]eth0:2803:6080:c838:a003::1<0>                    \r\ndevgpu217:2434725:2434725 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).                                                                                \r\n                                                                                                                                                                      \r\ndevgpu217:2434725:2434725 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]                                                                            \r\ndevgpu217:2434725:2434725 [1] NCCL INFO NET/Socket : Using [0]eth0:2803:6080:c838:a003::1<0>                                                                          \r\ndevgpu217:2434725:2435860 [1] NCCL INFO Setting affinity for GPU 1 to 0f,ff000fff                                                                                      \r\ndevgpu217:2434724:2435858 [0] NCCL INFO Channel 00 :    0   1                                                   \r\ndevgpu217:2434724:2435858 [0] NCCL INFO Ring 00 : 0[0] -> 1[1] via P2P/IPC               \r\ndevgpu217:2434725:2435860 [1] NCCL INFO Ring 00 : 1[1] -> 0[0] via P2P/IPC                                     \r\ndevgpu217:2434724:2435858 [0] NCCL INFO Using 256 threads, Min Comp Cap 5, Trees disabled\r\ndevgpu217:2434725:2435860 [1] NCCL INFO comm 0x7f29f4001980 rank 1 nranks 2 cudaDev 1 nvmlDev 1 - Init COMPLETE                                                       \r\ndevgpu217:2434724:2435858 [0] NCCL INFO comm 0x7f29ec001980 rank 0 nranks 2 cudaDev 0 nvmlDev 0 - Init COMPLETE                                                       \r\ndevgpu217:2434724:2434724 [0] NCCL INFO Launch mode Parallel  \r\n```\r\n\r\nLooks the same', 'Thanks. Yeah, nothing stands out...', 'I tried to reproduce it on our machines with CUDA10.2.89 (with and without the `time.sleep`) and both tests passed.\r\n\r\n@mrsalehi Were you able to reproduce it only using this setup (i.e. CUDA9.2, M40, driver 396.69) or on any other setups as well?\r\nI would like to narrow down the possible root cause of this issue.', 'Just some data point: I could not reproduce it in our CUDA 10.0.130 env either.\r\n', ""@ptrblck I haven't try other environments yet. [This](https://app.circleci.com/github/pytorch/pytorch/pipelines/93c0752c-2dc9-4d93-aee1-e9da39b83d25/workflows/24948a1a-d95e-4703-8450-abe2a5adad41) is the failed CI run, all failures occur in envs with CUDA9. Though [this](https://app.circleci.com/jobs/github/pytorch/pytorch/3824068) CUDA9 test has passed.\r\n\r\nLet me try a different environment."", 'Explicitly setting the device before creating DDP can make the test pass: `torch.cuda.set_device(device_id) `\r\n\r\nThe issue seems that forked processes work on the same device messing up the states and causing the deadlock. \r\n\r\n', ""I can confirm that @baobablyh's solution with `set_device` works for my local env as well. We are testing using `DeviceGuard` to wrap pinned memory creation (DeviceGuard works for my local env as well). \r\n\r\n@ngimel is it true that when calling pinned memory, there will be some side effect on the default CUDA device?"", '@mrshenli does gh-31261 replace this? Are you still working on it?', ""@mrshenli, I second @mattip's question"", 'Hey @mattip @Baranowski thanks, yes, we can close this PR and use #31261 to track and #28883 has already been merged. \r\n\r\nClosing now.']","['\r\n#0  0x00007ffc203cf9dd in clock_gettime ()\r\n#1  0x00007f9191fffc6d in clock_gettime () from /lib64/libc.so.6\r\n#2  0x00007f9152050ace in ?? () from /usr/lib64/nvidia/libcuda.so.1\r\n#3  0x00007f91520e5b73 in ?? () from /usr/lib64/nvidia/libcuda.so.1\r\n#4  0x00007f915203ac93 in ?? () from /usr/lib64/nvidia/libcuda.so.1\r\n#5  0x00007f915203ade9 in ?? () from /usr/lib64/nvidia/libcuda.so.1\r\n#6  0x00007f9151f52707 in ?? () from /usr/lib64/nvidia/libcuda.so.1\r\n#7  0x00007f915209a232 in cuStreamSynchronize () from /usr/lib64/nvidia/libcuda.so.1\r\n#8  0x00007f9164fa4dc0 in ?? () from /usr/local/cuda/lib64/libcudart.so.9.2\r\n#9  0x00007f9164fdcb1d in cudaStreamSynchronize () from /usr/local/cuda/lib64/libcudart.so.9.2\r\n#10 0x00007f9168fb9050 in at::native::copy_kernel_cuda(at::TensorIterator&, bool) () from /data/users/shenli/pytorch/torch/lib/libtorch.so\r\n#11 0x00007f9169dc863d in void at::native::DispatchStub<void (*)(at::TensorIterator&, bool), at::native::copy_stub>::operator()<at::TensorIterator&, bool&>(c10::DeviceType, at::TensorIterator&, bool&) () from /data/users/shenli/pytorch/torch/lib/libtorch.so\r\n#12 0x00007f9169dc593d in at::native::copy_impl(at::Tensor&, at::Tensor const&, bool) () from /data/users/shenli/pytorch/torch/lib/libtorch.so\r\n#13 0x00007f9169dc7744 in at::native::copy_(at::Tensor&, at::Tensor const&, bool) () from /data/users/shenli/pytorch/torch/lib/libtorch.so\r\n#14 0x00007f916c5afe9b in torch::autograd::VariableType::copy_(at::Tensor&, at::Tensor const&, bool) () from /data/users/shenli/pytorch/torch/lib/libtorch.so\r\n#15 0x00007f9168b294e5 in at::Tensor::copy_(at::Tensor const&, bool) const () from /data/users/shenli/pytorch/torch/lib/libtorch.so\r\n#16 0x00007f916a028d42 in at::native::to_impl(at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>) ()\r\n   from /data/users/shenli/pytorch/torch/lib/libtorch.so\r\n#17 0x00007f916a02999e in at::native::to(at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>) ()\r\n   from /data/users/shenli/pytorch/torch/lib/libtorch.so\r\n#18 0x00007f916a33c2ba in at::TypeDefault::to(at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>) ()\r\n   from /data/users/shenli/pytorch/torch/lib/libtorch.so\r\n#19 0x00007f916bf1d77c in torch::autograd::VariableType::(anonymous namespace)::to(at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>) () from /data/users/shenli/pytorch/torch/lib/libtorch.so\r\n#20 0x00007f916a3864d2 in c10::detail::wrap_kernel_functor_unboxed_<c10::detail::WrapRuntimeKernelFunctor_<at::Tensor (*)(at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat> > >, at::Tensor (at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>)>::call(c10::OperatorKernel*, at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>) () from /data/users/shenli/pytorch/torch/lib/libtorch.so\r\n#21 0x00007f917c0eb3de in torch::autograd::dispatch_to(at::Tensor const&, c10::Device, bool, bool, c10::optional<c10::MemoryFormat>) ()\r\n   from /data/users/shenli/pytorch/torch/lib/libtorch_python.so\r\n#22 0x00007f917c20bb2c in torch::autograd::THPVariable_to(_object*, _object*, _object*) () from /data/users/shenli/pytorch/torch/lib/libtorch_python.so\r\n#23 0x000056527b9716e4 in _PyMethodDef_RawFastCallKeywords (method=0x56527d110a60, self=0x7f9152df0678, args=0x7f9152e3acc0, nargs=<optimized out>, \r\n    kwnames=<optimized out>) at /tmp/build/80754af9/python_1553721932202/work/Objects/call.c:690\r\n#24 0x000056527b97186f in _PyMethodDescr_FastCallKeywords (descrobj=0x7f91536d8438, args=0x7f9152e3acb8, nargs=4, kwnames=0x0)\r\n    at /tmp/build/80754af9/python_1553721932202/work/Objects/descrobject.c:288\r\n#25 0x000056527b9cd07c in call_function (kwnames=0x0, oparg=4, pp_stack=<synthetic pointer>) at /tmp/build/80754af9/python_1553721932202/work/Python/ceval.c:4593\r\n#26 _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at /tmp/build/80754af9/python_1553721932202/work/Python/ceval.c:3110\r\n#27 0x000056527b90edb9 in _PyEval_EvalCodeWithName (_co=0x7f91534aac90, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, \r\n    argcount=<optimized out>, kwnames=0x0, kwargs=0x5652aea29f70, kwcount=0, kwstep=1, defs=0x0, defcount=0, kwdefs=0x0, closure=0x7f9152ddaf30, name=0x7f918a95fa78, \r\n    qualname=0x7f91534a92b0) at /tmp/build/80754af9/python_1553721932202/work/Python/ceval.c:3930\r\n#28 0x000056527b970a27 in _PyFunction_FastCallKeywords (func=<optimized out>, stack=0x5652aea29f68, nargs=1, kwnames=<optimized out>)\r\n    at /tmp/build/80754af9/python_1553721932202/work/Objects/call.c:433\r\n#29 0x000056527b9c8846 in call_function (kwnames=0x0, oparg=<optimized out>, pp_stack=<synthetic pointer>)\r\n    at /tmp/build/80754af9/python_1553721932202/work/Python/ceval.c:4616\r\n', '\r\n#0  0x00007f9151f5b8f9 in ?? () from /usr/lib64/nvidia/libcuda.so.1\r\n#1  0x00007f91520a1e1b in ?? () from /usr/lib64/nvidia/libcuda.so.1\r\n#2  0x00007f91520a2947 in ?? () from /usr/lib64/nvidia/libcuda.so.1\r\n#3  0x00007f9151f5c29a in ?? () from /usr/lib64/nvidia/libcuda.so.1\r\n#4  0x00007f9151f5cbb3 in ?? () from /usr/lib64/nvidia/libcuda.so.1\r\n#5  0x00007f9152186a9a in ?? () from /usr/lib64/nvidia/libcuda.so.1\r\n#6  0x00007f9152186c52 in ?? () from /usr/lib64/nvidia/libcuda.so.1\r\n#7  0x00007f9152186cf7 in ?? () from /usr/lib64/nvidia/libcuda.so.1\r\n#8  0x00007f91520178dd in ?? () from /usr/lib64/nvidia/libcuda.so.1\r\n#9  0x00007f91520186e8 in ?? () from /usr/lib64/nvidia/libcuda.so.1\r\n#10 0x00007f91520152a8 in ?? () from /usr/lib64/nvidia/libcuda.so.1\r\n#11 0x00007f9152032349 in ?? () from /usr/lib64/nvidia/libcuda.so.1\r\n#12 0x00007f9151f7a15d in ?? () from /usr/lib64/nvidia/libcuda.so.1\r\n#13 0x00007f9151f7a470 in ?? () from /usr/lib64/nvidia/libcuda.so.1\r\n#14 0x00007f9164fbc1e7 in ?? () from /usr/local/cuda/lib64/libcudart.so.9.2\r\n#15 0x00007f9164fb62a0 in ?? () from /usr/local/cuda/lib64/libcudart.so.9.2\r\n#16 0x00007f9164fc2616 in ?? () from /usr/local/cuda/lib64/libcudart.so.9.2\r\n#17 0x00007f9164fc5031 in ?? () from /usr/local/cuda/lib64/libcudart.so.9.2\r\n#18 0x00007f9164fb823e in ?? () from /usr/local/cuda/lib64/libcudart.so.9.2\r\n#19 0x00007f9164fa47de in ?? () from /usr/local/cuda/lib64/libcudart.so.9.2\r\n#20 0x00007f9164fd9684 in cudaEventDestroy () from /usr/local/cuda/lib64/libcudart.so.9.2\r\n#21 0x00007f916c88cfff in THCCachingHostAllocator::allocate(unsigned long) const () from /data/users/shenli/pytorch/torch/lib/libtorch.so\r\n#22 0x00007f916a02d96a in at::native::empty_cpu(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) ()\r\n   from /data/users/shenli/pytorch/torch/lib/libtorch.so\r\n#23 0x00007f916a24c6bb in at::CPUType::(anonymous namespace)::empty(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) ()\r\n   from /data/users/shenli/pytorch/torch/lib/libtorch.so\r\n#24 0x00007f916a2944b7 in c10::detail::wrap_kernel_functor_unboxed_<c10::detail::WrapRuntimeKernelFunctor_<at::Tensor (*)(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>), at::Tensor, c10::guts::typelist::typelist<c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat> > >, at::Tensor (c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>)>::call(c10::OperatorKernel*, c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) () from /data/users/shenli/pytorch/torch/lib/libtorch.so\r\n#25 0x00007f916c236778 in torch::autograd::VariableType::(anonymous namespace)::empty(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) () from /data/users/shenli/pytorch/torch/lib/libtorch.so\r\n#26 0x00007f916a2944b7 in c10::detail::wrap_kernel_functor_unboxed_<c10::detail::WrapRuntimeKernelFunctor_<at::Tensor (*)(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>), at::Tensor, c10::guts::typelist::typelist<c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat> > >, at::Tensor (c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>)>::call(c10::OperatorKernel*, c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) () from /data/users/shenli/pytorch/torch/lib/libtorch.so\r\n#27 0x00007f916a035731 in at::empty () from /data/users/shenli/pytorch/torch/lib/libtorch.so\r\n#28 0x00007f916a03a755 in at::native::zeros(c10::ArrayRef<long>, c10::TensorOptions const&) () from /data/users/shenli/pytorch/torch/lib/libtorch.so\r\n#29 0x00007f916a342093 in at::TypeDefault::zeros(c10::ArrayRef<long>, c10::TensorOptions const&) () from /data/users/shenli/pytorch/torch/lib/libtorch.so\r\n#30 0x00007f916c02c026 in torch::autograd::VariableType::(anonymous namespace)::zeros(c10::ArrayRef<long>, c10::TensorOptions const&) ()\r\n   from /data/users/shenli/pytorch/torch/lib/libtorch.so\r\n#31 0x00007f916a3855f9 in c10::detail::wrap_kernel_functor_unboxed_<c10::detail::WrapRuntimeKernelFunctor_<at::Tensor (*)(c10::ArrayRef<long>, c10::TensorOptions const&), at::Tensor, c10::guts::typelist::typelist<c10::ArrayRef<long>, c10::TensorOptions const&> >, at::Tensor (c10::ArrayRef<long>, c10::TensorOptions const&)>::call(c10::OperatorKernel*, c10::ArrayRef<long>, c10::TensorOptions const&) () from /data/users/shenli/pytorch/torch/lib/libtorch.so\r\n', '\r\nPyTorch version: 1.4.0a0+cbf7f8b\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.2\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 7.3.1 20180303 (Red Hat 7.3.1-5)\r\nCMake version: version 3.14.2\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.88\r\nGPU models and configuration:\r\nGPU 0: Tesla M40\r\nGPU 1: Tesla M40\r\nNvidia driver version: 396.69\r\ncuDNN version: Could not collect\r\nVersions of relevant libraries:\r\n[pip] numpy==1.17.2\r\n[pip] torch==1.4.0a0+cbf7f8b\r\n[conda] blas                      1.0                         mkl\r\n[conda] magma-cuda90              2.5.0                         1    pytorch\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-include               2019.4                      243\r\n[conda] mkl-service               2.3.0            py37he904b0f_0\r\n[conda] mkl_fft                   1.0.14           py37ha843d7b_0\r\n[conda] mkl_random                1.1.0            py37hd6b4f25_0\r\n[conda] torch                     1.4.0a0+cbf7f8b           dev_0    <develop>\r\n']","['time.sleep(3)', 'distributed.py', 'py.test test/test_c10d.py -k test_find_unused_parameters_kwarg -vs']",0,0
139,pytorch,31410,closed,can't close torch.utils.tensorboard.SummaryWriter in __del__,"The program can not exit (print 'closed') when I try to close the  in .

The program can exit if the  is not in .

Python 3.7.5

Pytorch 1.3.1

tensorboard 2.0.0

OS: Ubuntu and Win10",,"['Hi,\r\n\r\nThe `__del__` function has many caveats in python. In particular there are not that many guarantees that it will always be called.\r\nIf you want to ensure this is called, you should use a `try:` `finally:` statement.\r\n\r\nAlso we use github issues only for bug reports, please use the forum for further questions: https://discuss.pytorch.org/']","[""\r\nfrom torch.utils.tensorboard import SummaryWriter\r\n\r\n\r\nclass Test:\r\n    def __init__(self):\r\n        self.tblog = SummaryWriter('exp/tb/exit_test')\r\n        # self.tblog.close()\r\n\r\n    def __del__(self):\r\n        self.tblog.close()\r\n        print('closed')\r\n\r\n\r\nif __name__ == '__main__':\r\n    tblog = Test()\r\n""]","['SummaryWriter', '__del__()', 'close()', '__del__()']",0,0
140,pytorch,23641,closed,"Hi, could you please tell me the different about deconvtranspose between pytorch and caffe?","## ‚ùì Questions and Help

### Please note that this issue tracker is not a help form and this issue will be closed.

We have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:

- [Discussion Forum](https://discuss.pytorch.org/)
",,['please use https://discuss.pytorch.org for questions'],[],[],0,0
141,pytorch,3652,closed,Fix far-fetched Boolean ambiguity for byte tensor with one value in it.,"Testing non-empty tensor in an if statement will unconditionally raise a RuntimeError stating that ""bool value of non-empty torch.ByteTensor objects is ambiguous"". But when there's only one value, it's bool value is pretty straightforward.
What do you think?",,"[""Yes, that makes sense. I thought we support this, but apparently we don't. NumPy does so it's better to do it as well for consistency"", 'Ok, will do a pull request in a few hours.']",[],[],0,0
142,pytorch,8716,closed,[JIT] Traced and Script modules are not properly inlined into script functions,,oncall: jit,[],"[""\r\n+    @unittest.skip('TODO: Incorrect behavior')\r\n+    def test_call_traced_mod_from_script_fn(self):\r\n+        class TracedModule(torch.nn.Module):\r\n+            def __init__(self):\r\n+                super(TracedModule, self).__init__()\r\n+                self.param = torch.nn.Parameter(torch.rand(4, 3))\r\n+\r\n+            def forward(self, x):\r\n+                return torch.mm(x, self.param)\r\n+\r\n+        tm = torch.jit.trace(torch.rand(3, 4))(TracedModule())\r\n+\r\n+        @torch.jit.script\r\n+        def script_fn(x):\r\n+            return tm(x) + 1\r\n+\r\n+        # Note: At the time of writing this produces the following graph:\r\n+        # graph(%x : Dynamic) {\r\n+        #   %1 : Dynamic = ^<python_value>()(%x)\r\n+        #   %2 : Long() = prim::Constant[value={1}]()\r\n+        #   %3 : Dynamic = aten::add[alpha={1}](%1, %2)\r\n+        #   return (%3);\r\n+        # }\r\n+        # This seems incorrect. Similarly to calling a traced module from a\r\n+        # traced function, the behavior here should likely be that we inline\r\n+        # the parameter from the traced module as a Constant node and we inline\r\n+        # the ops into the graph of the script function. TODO: fix\r\n+        self.assertExpected(str(script_fn.graph))\r\n+\r\n+    def test_call_script_mod_from_script_fn(self):\r\n+        class ScriptMod(torch.jit.ScriptModule):\r\n+            def __init__(self):\r\n+                super(ScriptMod, self).__init__()\r\n+                self.param = torch.nn.Parameter(torch.rand(4, 3))\r\n+\r\n+            @torch.jit.script_method\r\n+            def forward(self, x):\r\n+                return torch.mm(x, self.param)\r\n+\r\n+        sm = ScriptMod()\r\n+\r\n+        @torch.jit.script\r\n+        def script_fn(x):\r\n+            return sm(x) + 1\r\n+\r\n+        # Note: At the time of writing this produces the following graph:\r\n+        # graph(%x : Dynamic) {\r\n+        #   %1 : Dynamic = ^<python_value>()(%x)\r\n+        #   %2 : Long() = prim::Constant[value={1}]()\r\n+        #   %3 : Dynamic = aten::add[alpha={1}](%1, %2)\r\n+        #   return (%3);\r\n+        # }\r\n+        # This seems incorrect. Similarly to calling a script module from a\r\n+        # traced function, the behavior here should likely be that we inline\r\n+        # the parameter from the traced module as a Constant node and we inline\r\n+        # the ops into the graph of the script function. TODO: fix\r\n+        self.assertExpected(str(script_fn.graph))\r\n""]",[],0,0
143,pytorch,26726,closed,[jit] Recursive script doesn't pick up TorchScript class attributes,"

cc @suo",jit-backlog oncall: jit triaged,"['Fixed by https://github.com/pytorch/pytorch/pull/32594 but it has some weird issues', 'I think this is fixed']","[""python\r\n@torch.jit.script\r\nclass X(object)\r\n\t...\r\n\r\nclass M(nn.Module):\r\n\tdef __init__(self):\r\n\t\tsuper().__init__()\r\n\r\n\t\t# The type is not inferred correctly here, so this doesn't get added as an attribute\r\n\t\tself.x = X()\r\n""]",[],0,0
144,pytorch,30015,closed,JIT fails for multihead attention,"## üêõ Bug

I can run the model before torchscript while the torchscripted model fails in running time for the multihead attention @driazati

## To Reproduce

The following code sample:


## Expected behavior



## Environment

bento classyvision env


cc @suo",oncall: jit triaged,"['Can you include the `_lengths2mask` function?', '@driazati Opps, added that func. Thanks!', 'Fixed on master with #31076']","['python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\ndef _lengths2mask(lengths: torch.Tensor, seq_len: int, lt=torch.tensor(True)) -> torch.Tensor:\r\n    """"""\r\n    Input lengths is a tensor of shape (batch_size) with value in [0, seq_len],\r\n    Return a tensor of shape (batch_size x seq_len) with binary value\r\n    """"""\r\n    if lt:\r\n        return torch.lt(\r\n            torch.arange(seq_len, device=lengths.device)[None, :],\r\n            lengths[:, None].long(),\r\n        )\r\n    else:\r\n        return torch.ge(\r\n            torch.arange(seq_len, device=lengths.device)[None, :],\r\n            lengths[:, None].long(),\r\n        )\r\n\r\nclass AttentionMultihead(nn.Module):\r\n    def __init__(self, dim_in, method, use_variable_lengths=False, num_heads=1):\r\n        super(AttentionMultihead, self).__init__()\r\n\r\n        # choose from supported attention methods\r\n        assert method in (""multihead"")\r\n        self.method = method\r\n        self.use_variable_length = use_variable_lengths\r\n        \r\n        self.attention = nn.MultiheadAttention(\r\n            embed_dim=dim_in, num_heads=num_heads\r\n        )\r\n\r\n        # record output dim\r\n        self.dim_out = dim_in\r\n\r\n    def forward(self, data: torch.Tensor, lengths: torch.Tensor):\r\n        assert data.dim() == 3, ""Require input shape (batch_size x seq_len x embed_dim)""\r\n        \r\n        if self.use_variable_length is True:\r\n            mask = _lengths2mask(lengths.clamp(min=1), data.size(1), torch.tensor(False))\r\n        else:\r\n            mask = None\r\n\r\n        data = data.transpose(0, 1)\r\n        attn_output, attn_weights = self.attention(\r\n            data, data, data, key_padding_mask=mask, need_weights=True, attn_mask=None\r\n        )\r\n        # transpose output data to (batch_size, seq_length, embed_dim)\r\n        attn_output = attn_output.transpose(0, 1)\r\n        \r\n        return attn_output, lengths, attn_weights\r\n\r\n    \r\nclass DeployAttentionMultihead(torch.nn.Module):\r\n    def __init__(self, dim_in=100, length=torch.tensor([300])[0]):\r\n        super(DeployAttentionMultihead, self).__init__()\r\n        self.length = length\r\n        self.use_variable_length = True\r\n        self.attention = AttentionMultihead(dim_in=dim_in, method=""multihead"", use_variable_lengths=True, num_heads=5)\r\n        \r\n    def forward(self, data: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\r\n        attn_output, lengths, attn_weights = self.attention(data, lengths)\r\n        return attn_output\r\n    \r\nmodel = DeployAttentionMultihead()\r\n\r\nts_model_multihead = torch.jit.script(model)\r\nprint(ts_model_multihead.code)\r\n\r\n# run ts model of multihead\r\n\r\nvue_clip_emb = [0.0840825,5.65534,2.23576,2.21367,1.5883,0.493749,0,1.49034,1.54749,1.15795,0.118591,2.54162,2.70971,2.10865,0.115242,0.0115912,5.39253,2.83803,1.96323,0.935772,1.33328,1.16219,0.15678,1.33033,0,2.80253,0.258661,1.19186,0.429935,1.14404,4.58615,0.11558,2.31151,3.45441,0.117317,2.62973,2.57802,3.12865,0.632287,0.280647,1.08333,4.14262,3.5503,0.67525,0.889924,0.128302,3.943,0.825888,3.11752,2.46894,4.05448,3.0315,5.9825,2.54857,0.390271,0.105513,1.76064,3.53628,0.496927,0.850136,3.52684,1.92267,1.50376,3.43571,2.36993,0.0011008,0.101535,0.324544,0.875401,1.6104,0.497361,1.72176,4.01326,1.31662,0.262011,0.837993,0.622429,2.66528,1.13911,1.21707,1.56516,6.33717,1.5675,0.136495,0.355789,1.48718,0.108055,3.47766,1.0657,0.00356666,1.77421,3.86285,1.42049,0.819946,0.340235,0.109702,0.867129,2.71539,0.206769,2.02203,0.0292637,5.49686,1.74569,2.19925,1.51534,0.46074,0.0161071,1.24292,1.44666,0.562175,0.104977,2.05992,2.41354,2.51859,0.165397,0.011803,4.64355,4.19187,2.1993,1.15172,1.72702,1.62523,0.343265,1.45464,0,2.05193,0.619562,0.983955,0.432762,1.09673,4.58744,0.209633,2.02799,3.54795,0.156958,2.19384,2.25872,2.84524,0.120264,0.432246,0.800701,3.83577,3.56383,0.501572,0.458101,0.0500416,3.77406,0.618967,2.51559,2.57312,3.63771,4.04265,5.3371,2.427,0.555011,0.393777,1.45245,4.48355,0.642099,1.48505,3.90371,2.12527,1.10275,3.13394,2.88107,0,0.0938793,0.432923,1.40834,1.47035,0.199206,0.899871,4.19796,1.37373,0.108242,0.541182,0.60768,2.56032,1.05987,1.05486,2.31946,6.57515,1.71218,0.191091,0.523715,0.562181,0.163343,3.41128,1.90795,0.103194,2.18321,4.01785,1.40121,0.712813,0.40542,0.0130574,0.482167,3.61689,0.541964,1.54621,0.00721372,3.53875,1.99706,2.16941,2.30858,0.139537,0.0292894,0.59817,1.67502,1.06869,0.15484,1.32557,2.84414,1.81534,0.0359029,0.155307,4.85468,3.37413,1.90302,1.30877,0.734729,1.76238,0.150899,0.547858,0.0230488,2.15477,0.665912,2.1411,0.486219,0.981247,4.02336,0.340909,2.64124,1.94704,0.127592,2.22661,3.51697,2.28332,0.23371,0.0208469,0.71869,3.26233,3.37803,0.440662,0.510871,0.00753081,5.16184,0.492385,2.8764,2.53306,2.73119,3.92737,4.78175,3.25056,0.738502,0.171556,1.14462,4.28251,0.983006,1.13122,3.53485,1.87881,0.758899,3.47704,3.11202,0.00754782,0.223614,0.554179,2.12652,1.15958,0.225096,1.4953,3.90192,1.51579,0.0205165,1.09822,0.339162,3.14576,2.58463,0.766622,2.68842,5.81371,1.30858,0.0567763,0.362448,0.427235,0.103554,1.55612,1.40707,0.327143,2.06921,3.1817,1.11314,0.293359,0.491569,0.0237787,0.335975,1.49355,0.080589,0.798883,0.0042814,4.00296,1.46194,1.97508,2.93223,0.369621,0.114972,1.43744,0.953024,1.51951,0.0664168,1.36838,1.12008,2.37583,0.0656217,0.0550487,5.11532,3.41184,2.20438,1.50039,1.4365,1.97503,0.0544394,1.24929,0.00753079,1.44055,0.312355,1.65135,0.778141,0.717496,3.53737,0.872259,1.93065,2.75865,0.0802059,2.52429,2.25681,2.45514,0.0332121,0.272185,1.78893,4.1729,3.72535,0.296955,0.977887,0.0917793,5.16411,0.227299,2.63355,2.93526,3.32066,3.49133,3.54922,3.74034,0.651864,0.379803,1.49085,4.07919,0.729488,0.412124,3.82536,0.844314,0.661833,2.39382,2.20183,0.0319286,0.20516,0.6255,1.46891,1.14456,0.44578,1.54798,4.63112,1.32129,0,0.868121,0.800939,4.08268,1.32296,0.520707,3.65064,6.09133,0.723082,0.158022,0.261037,0.397029,0.0556116,2.45283,1.20808,0.051552,1.35081,3.32664,0.930835,0.830775,0.637906,0.00174865,0.0430316,1.45825,0.147698,1.924,0.00532853,3.77304,1.63318,1.94748,2.129,0.0999379,0.0187992,0.764959,1.12453,0.710663,0.221404,1.04146,2.84609,2.1028,0.0512187,0.0633501,4.60868,4.72056,1.81119,1.71785,1.46575,1.1067,0.22531,1.22568,0.0114571,1.80884,0.322822,1.30182,0.343267,0.780105,4.02635,0.468035,2.11109,2.30884,0.225344,1.73228,3.32626,2.14921,0.0652682,0.179686,0.794495,4.04693,3.94471,0.479574,0.372132,0.0392945,5.12383,0.643861,3.05057,2.2246,3.45714,4.82734,4.22932,3.33674,0.602254,0.160737,1.20358,5.0604,0.894768,1.28898,4.45525,1.44087,0.816369,1.96161,3.89633,0.0192901,0.130743,0.526546,0.911684,0.790611,0.290481,0.535833,3.96858,0.660851,0.183099,1.0221,0.527058,2.76179,2.45929,0.418104,1.77804,6.29833,1.67065,0.412998,0.493972,0.239688,0.143934,2.45849,2.14727,0.0778733,2.5178,3.35156,1.2997,0.506005,0.775292,0.0188194,0.447015,2.64902,0.5688,1.19909,0.0417836,3.4061,2.37674,1.87274,1.59598,0.440209,0.0390666,0.459501,1.47975,2.99328,0.293728,3.44896,2.98324,0.377933,0.136621,0.0858318,3.94416,2.30151,1.82907,2.10093,0.403424,1.47967,0.348227,1.58975,0.0860919,1.47215,0.111787,3.21831,2.28628,1.27021,2.6389,0.099854,2.80626,1.31564,0.256794,2.7791,3.04503,3.35439,0.637234,0.873176,1.78056,1.96819,3.70617,0.573749,0.86824,0.0227969,4.80532,1.19482,4.30919,3.18403,2.90808,3.55253,5.32479,2.20803,0.292511,0.917928,0.917466,3.6687,1.65847,1.24795,2.66524,0.583895,1.36363,3.3847,3.8937,0.104704,0.242911,1.92325,1.26514,1.42388,0.702984,2.26055,4.22127,0.507518,0.104398,1.00863,1.40142,2.62522,3.89506,0.304073,1.87341,4.88558,0.57671,0.101682,0.376967,0.151231,0.302362,0.964678,0.314604,0.357943,2.24616,3.35078,0.934013,0.87141,0.291341,0.00906326,0.116968,0.5981,0.0642974,0.59632,0.070716,3.52079,2.46395,2.44951,2.9654,0.663779,0.298566,0.532495,0.605621,1.02062,1.23134,2.30953,2.70189,0.311687,0.250861,0.157656,2.54278,1.19941,1.6287,1.38798,0.378133,1.65116,0.462127,2.0553,0.183922,1.911,0.26869,2.17268,1.65513,1.10857,1.4878,0.285458,3.24843,1.04712,1.4921,2.49815,3.25619,2.89996,0.615719,1.52029,1.29669,0.594365,3.33981,0.7246,0.386914,0.18093,3.73587,1.15124,4.20692,2.63089,4.41739,4.75388,4.79367,1.61047,0.177007,1.5846,2.15091,2.80859,2.03466,1.22181,1.88013,0.966551,1.62572,2.95286,3.22257,0.171248,0.0488555,1.50401,0.564802,1.11984,1.97094,1.09832,4.79174,1.00213,0.0561535,1.06735,2.31487,3.13783,3.00876,0.280711,1.51538,4.03358,0.577055,0.256786,0.520065,0.197432,1.37471,1.67807,0.111243,1.28862,3.27064,3.1356,0.810865,0.226676,0.090544,0.923369,0.273451,0.463472,0.740315,0.466818,0.0792522,3.41677,1.65213,1.87478,2.9709,0.649614,0.287107,0.742373,2.44696,1.98299,0.362439,3.38517,3.0017,1.15386,0.123824,0.71546,3.2347,2.01491,1.11258,2.1084,1.17668,1.4841,0.585114,2.00054,0.0496953,1.25047,0.316964,2.6968,1.75643,0.580133,1.54092,0.0531415,2.72435,1.84557,0.622192,2.4177,3.2615,2.47776,0.355572,1.18969,2.80585,1.48045,2.73792,1.27207,1.31757,0.00647418,5.56059,0.929273,2.86031,2.55737,3.45009,1.33754,4.2221,2.44413,0.265876,1.38801,0.652557,2.74723,2.27915,1.15483,2.48536,0.862942,1.00924,2.87263,2.60592,0.341337,0.789075,2.61353,1.51038,1.42327,0.692666,1.54161,4.74268,2.04178,0.52153,1.57625,1.82753,2.29875,2.95661,0.794998,2.05961,4.72058,1.10073,0.154321,0.499115,0.0518899,0.51693,0.409915,0.405275,0.53946,2.23644,2.72192,0.94019,1.03312,0.339981,0.0105235,0.0286632,0.641563,0.176992,1.62913,0.173662,2.71067,1.12456,0.980614,1.26853,0.502585,0.0405315,0.0835886,0.801681,2.17712,0.600408,2.7253,3.26336,1.98236,0.19047,0.160637,4.27236,4.15545,0.692021,2.13214,1.37671,1.35437,1.51932,0.632685,0,2.57451,0.0496026,3.45484,0.800954,1.08736,2.65716,0.288336,3.76113,0.877107,0.343407,1.93358,3.87112,3.22765,0.771835,0.544815,2.16071,3.16342,1.80419,0.929744,2.37628,0.0166744,5.67011,0.584775,4.03636,2.40968,1.96826,2.96728,4.39782,2.26301,1.22459,0.56253,0.861167,3.46662,0.947368,1.0488,3.56452,1.12366,1.08122,3.01743,2.8018,0.545803,0.861587,0.610711,1.44031,0.526162,0.620491,1.9175,3.08667,0.663501,0.35154,1.79684,0.938363,2.92849,2.60115,0.880397,2.2393,5.30998,0.792989,0.532533,0.38656,0.475731,0.0857692,0.487966,1.6554,0.17406,3.03141,2.42888,2.00751,0.879513,0.731286,0.102841,0.637661,0.50266,0,1.66406,0.0403251,2.60657,1.94436,0.987926,2.60567,0.490878,0.00202899,0.331029,2.03788,2.20193,1.82084,3.72021,3.44797,0.278736,0.0147163,0.169737,3.7098,2.93405,1.24039,2.8275,0.505545,1.41782,0.345053,1.34328,0.0294194,2.0961,0.0848584,3.29406,1.32525,2.41475,3.38123,0.0721113,2.92625,0.847645,0.420569,1.33231,3.7629,2.30329,0.388465,1.12179,2.35953,1.66306,2.70488,0.503543,0.822229,0,4.34317,0.605498,5.77145,1.83385,1.98237,3.45013,5.29985,2.02454,0.0475114,0.395792,2.11374,3.54273,0.528591,1.56819,3.8756,0.558113,0.941327,2.41993,3.57062,0.187795,0.31701,1.55603,0.604092,0.648071,0.516138,1.93505,3.75648,0.439664,0.0350492,1.06238,2.00205,3.3981,4.43062,0.284624,1.01425,4.71788,0.761708,0.0611976,1.14644,0.125763,0.42916,0.839016,0.331764,0.328137,3.00843,3.59455,1.00545,0.717833,0.128019,0.00645771,0.328582,0.54553,0.188525,0.895525,0.0408872,3.24783,2.71351,2.49815,2.20143,0.676868,0.0287524,0.661264,1.41991,1.15123,0.467025,1.38601,2.40322,0.770031,0.00989118,0.386181,3.90587,4.04182,1.06631,1.77359,1.07598,1.38183,0.577455,1.14628,0.0020014,2.77807,0,2.90588,0.480905,1.61046,3.46837,0.355569,0.968912,1.83375,0.248112,2.19061,3.11325,2.65638,0.0367746,0.0603686,1.67572,2.31816,3.60933,1.29633,0.11909,0.0600984,5.61985,0.720068,2.48464,2.86905,2.13853,1.98833,3.15317,2.22118,0.639354,1.25785,0.281851,2.72698,0.921203,2.52241,1.82059,0.669081,0.100118,1.23264,3.06482,0.0135905,0.641079,0.403843,0.999941,1.04065,2.18396,1.00285,1.85417,0.877147,0.144436,2.53481,0.977721,1.64681,3.45587,0.818207,2.53121,4.49658,0.887782,0.234846,1.14279,0.659315,0.109713,1.36542,0.899438,0.175104,2.23193,2.60381,1.32821,0.915714,0.445052,0.00288133,0.163356,0.982402,0.0125229,0.666454,0.0555231,2.31373,1.38932,2.6543,1.35873,0.143295,0,0.226475,1.68297,0.660947,0.487611,1.4304,4.92722,1.75121,0,0.143175,3.02567,3.35532,0.272801,1.61775,0.645526,0.531874,0.614343,1.03124,0.0053275,3.25492,0.00264338,2.77116,0.430088,0.649697,2.29188,0.722446,2.50275,0.544916,0.204628,1.21149,3.88391,2.39757,0.000352105,0.212974,0.0384491,4.97835,2.7165,1.15199,0.413043,0.0146025,4.46414,0.0112177,2.61585,0.861757,2.28923,3.35354,5.43353,1.99584,1.30369,0.10671,0.500256,5.71459,0.963861,3.3774,3.52604,0.217327,0.0892254,3.19477,4.90951,0.10896,0.394621,0.600351,0.810254,0.513302,0.580245,1.14567,1.09709,0.362707,0.198018,1.85646,0.661291,3.34353,3.2775,0.385556,0.675552,6.33126,1.56004,0.641665,0.847008,1.38089,0.0555137,1.03894,1.94796,0.00552486,3.4941,3.26301,1.9274,0.253463,1.10076,0.0425154,0.904492,2.41034,0.0463368,1.52589,0.0862676,2.75297,1.88521,2.06296,1.15305,0.212366,0,0.394245,1.78298,0.888924,0.672979,1.35533,4.45294,1.48784,0.0605392,0.176757,4.75529,3.85554,0.844591,2.16973,0.272653,1.32838,0.688873,1.4805,0.00044684,2.74095,0.0685647,2.77912,1.24349,1.33388,3.78668,0.0939944,2.98083,1.01641,0.336904,2.41045,3.56117,2.63229,0.0388405,0.126486,0.872781,4.13501,2.94543,1.10456,0.801741,0,4.4144,0.153719,4.29423,1.82199,2.28633,3.094,5.33176,2.21881,0.693673,0.156806,1.09274,4.26824,0.379416,2.21975,3.53672,1.08108,0.89244,3.1856,4.76257,0.0451743,0.0949684,0.228956,0.373762,0.289384,1.2531,1.81112,2.30036,0.625152,0.00985754,1.77277,0.283007,3.30557,3.30533,0.619986,1.35398,6.38698,1.73076,0.126091,0.871414,0.356507,0.174712,1.66457,1.17135,0.0491385,2.67154,4.05349,2.17985,0.523685,0.576262,0.157504,0.308839,1.56192,0.0512652,0.95207,0.0805058,2.48621,1.37535,2.45468,1.87533,0.468265,0.122033,0.321863,1.33016,0.549338,0.494591,0.874955,2.63495,0.740472,0.0438563,0.141534,3.29394,3.68166,1.18674,1.08292,0.244218,1.11172,0.828344,1.42215,0.00554828,1.77962,0.189551,2.12145,0.546716,1.04505,3.95568,0.3655,2.37688,1.71594,0.224391,2.74793,3.33626,2.97977,0.122376,0.0632366,0.972671,2.11059,2.25442,0.85038,0.124168,0.0187264,4.50107,0.534927,3.1567,2.6244,2.49565,3.01174,4.25579,1.62496,0.592326,1.05417,0.562298,3.77764,1.02897,1.90008,2.77085,1.05659,0.747173,2.30213,3.18592,0.0147369,0.023573,0.207013,1.03249,0.925702,0.932221,1.22497,2.04939,0.744914,0.328359,1.31802,0.387632,1.70093,2.47217,1.20024,1.79619,5.47078,1.69118,0.165774,0.794053,0.341791,0.129126,1.44495,1.19076,0.0475776,2.66236,3.06191,2.4638,0.453825,0.488893,0.0543923,0.0634928,1.52558,0.22285,0.553228,0.221582,2.48304,1.022,1.56536,2.29826,0.209171,0.138548,0.309978,1.81818,1.4621,1.09224,2.08636,2.86538,0.603495,0.366823,0.329206,3.08912,2.58224,0.700127,2.0108,0.0986592,2.0601,0.895608,1.33536,0.101057,1.51155,0.290049,2.36485,1.46373,1.06817,3.15438,0.0736844,2.95347,1.35243,0.867988,2.56145,3.61033,1.72861,0.562523,0.60729,1.96837,1.20481,4.01519,1.00243,1.17049,0.0626663,4.00606,0.959066,4.73986,3.9961,2.07039,1.85597,4.51874,2.15147,0.0866293,0.160415,0.776263,3.42877,1.5827,1.208,3.27742,1.12566,1.38087,2.50217,2.11579,0.168926,0.47073,0.789089,1.18347,0.692235,0.859697,2.53243,2.11629,0.874951,0,1.84621,1.05134,2.79519,4.17778,2.43341,1.46079,4.95973,0.990473,0.00451488,0.498543,0.136919,0.747515,1.02114,0.164363,0.20047,2.40017,2.5119,2.07164,1.25656,0.222824,0.0637718,0.0677823,0.407466,0.0305444,1.12243,0.191263,2.95274,0.788903,2.04562,0.821009,0.390528,0.106878,0.290616,0.74644,1.30808,0.7783,2.74388,3.67038,3.43589,0.0119235,0.327381,4.70921,3.99718,0.309318,1.34281,0.799294,0.795629,2.50191,0.502497,0.0563869,3.90016,0.16275,2.25545,0.906753,0.884097,3.51065,0.653698,3.58473,1.43594,0.368113,3.07226,3.27478,2.61307,0.0926574,0.20586,1.01696,3.96841,1.73393,1.60189,1.61205,0.332741,5.6585,0.151749,3.11497,1.19252,2.06011,2.88685,5.18958,1.93425,1.74641,0.143624,0.980521,3.32435,0.240595,2.76726,3.43182,0.829861,0.513481,3.03082,3.14267,0.450467,0.384188,0.203941,1.57999,0.0863299,1.05067,1.62805,2.49849,0.624522,0.0210923,3.38103,0.0787131,3.06627,3.05254,0.724761,2.51799,5.99396,1.44315,0.48246,0.460933,1.27126,0.178611,0.842834,2.41634,0.0746336,2.28044,1.60962,1.77188,0.850597,1.19109,0.296519,0.604338,1.35025,0,0.855787,0.24927,2.99181,2.2486,2.37542,1.3126,0.350791,0.024723,0.201999,1.45714,1.48941,1.53887,2.55214,5.06456,0.994865,0.0734257,0.083538,3.51118,2.41169,0.631832,2.0792,0.115151,2.3912,0.772408,2.02382,0,3.3883,0,2.83265,1.07762,1.06044,3.32062,0.152621,3.1467,0.999532,0.691353,2.18066,4.07433,3.76381,0.564451,0.288695,1.46631,1.80245,3.96539,0.75673,1.25008,0.209399,4.06395,0.238434,5.02981,2.71844,2.61667,2.93738,5.24963,1.91154,0.472515,0.478448,1.34589,4.0873,1.01529,1.9998,4.08209,1.06896,1.46918,4.13979,3.46283,0.222857,0.165366,0.435843,1.03564,0.934988,0.827622,2.30924,2.56759,1.11041,0.339095,2.17876,0.246572,3.56566,2.52839,1.91746,1.52709,7.18276,1.46865,0.0681596,0.578342,0.243158,0.224508,1.52765,0.580382,0.030582,2.47407,4.58363,2.31677,0.465353,0.39472,0.587289,0.213973,0.600313,0.120027,0.89201,0.185326,3.94036,1.52176,2.62566,1.78063,0.208037,0,0.558397,1.04916,0.900976,0.483306,1.48558,3.60931,1.59509,0.0638275,0.0647773,3.8862,3.17611,0.8943,0.841184,0.323702,1.04602,0.258091,1.67474,0,1.82776,0.0908958,1.57279,0.929668,1.49998,4.81248,0.228259,2.91749,2.00505,0.0470934,2.84799,3.22272,3.68171,0.0594875,0.0733745,0.54319,3.34397,2.55439,0.507932,0.75085,0.108581,4.24962,0.145146,3.65904,2.06542,2.62613,3.56953,5.05274,1.96623,0.779482,0.634575,0.948498,4.4603,0.50401,1.81869,3.3978,1.04111,0.447499,2.99711,3.06425,0.0197619,0.0390933,0.276516,0.752591,0.459888,0.672086,2.00618,2.43392,0.363539,0.0871918,1.12805,0.738331,2.49416,1.82035,1.00751,2.02311,6.0054,1.90343,0.143459,0.798544,0.837159,0.103327,1.35308,1.02193,0.0155476,2.08821,4.37539,1.69243,0.628235,0.612107,0.0970055,0.407801,1.79539,0.123628,0.747089,0.163579,2.35004,0.852999,0.910148,1.80315,0.59716,0,0.851198,0.778011,2.19988,0.402101,0.933004,2.09489,3.05333,0.0252962,0.0977166,2.9703,1.6115,1.0476,0.400364,0.66365,0.440661,0.829402,1.43691,0.796235,1.38266,0.161254,0.934144,0.411609,1.29833,2.81847,0.583717,1.70186,2.00263,0.344199,3.56151,1.28074,1.83579,0.0387559,0.0243303,1.23755,4.48576,1.36396,2.05456,0.473,0,3.95411,0.00469383,1.32001,0.745603,1.49426,1.90903,2.37102,0.920934,2.45122,0.736415,0.664711,3.55602,2.10916,0.70585,2.62348,1.12938,0.956224,1.17342,2.17879,1.02419,0.309579,0.363046,1.38165,0.74992,0.428322,1.58538,1.77151,0.166842,1.10562,1.36608,0.289836,1.93779,1.48751,0.142338,1.41565,4.64051,1.12845,1.44624,0.618646,1.53882,0.55646,0.11152,1.63253,0.13781,3.06238,1.53316,2.18355,0.44066,1.92378,0.46227,0.37104,1.70604,0.23205,1.86809,0.102674,3.19703,2.5658,1.93422,2.27114,0.657241,0.0382914,0.290383,1.35741,1.49755,0.741304,2.09531,2.14157,1.03596,0.0978403,0.203721,4.29982,4.3951,1.17579,2.35823,0.722298,1.5228,1.03373,1.08911,0.0532115,1.63872,0.00680868,1.80044,0.4208,2.52569,3.74694,0.2112,1.57353,2.23563,0.457424,3.61216,4.27956,1.83665,0.107581,0.134045,1.37063,2.4452,2.30221,1.81058,1.01463,0.0911855,5.20949,0.789669,4.47411,2.57445,1.41898,1.25783,3.56169,1.19806,0.501017,0.323389,0.956781,1.86047,0.423411,2.34943,2.48705,0.592364,0.878147,1.79065,2.40255,0.415444,0.630449,1.11744,0.658741,0.73657,1.44287,1.21421,2.56012,1.30955,0.0461261,2.81246,0.494368,2.01135,3.04064,1.15881,1.88349,3.73473,0.919362,0.107721,1.49636,0.617467,0.305044,0.784687,1.16902,0.0493862,2.54482,2.76247,1.1756,1.09797,1.07768,0.0691202,0.0770139,0.696618,0.0298994,0.735747,0.0129393,1.88335,0.905986,1.03496,2.101,0.597856,0,0.19022,1.55955,1.9505,1.45855,2.44006,3.13068,0.374842,0.109704,0.0564849,4.41647,3.26477,0.885778,2.43465,0.350469,1.09961,0.447687,1.88259,0.0782037,1.40383,0.0368739,2.70133,1.89626,1.92466,3.58989,0.122527,3.11725,1.05007,0.51717,2.40146,3.33289,1.93742,0.252541,1.02291,2.0183,1.3077,2.65496,1.15508,1.23102,0.000266209,4.37571,1.61635,5.65003,1.84072,1.86486,3.17868,5.39525,1.75413,0.261276,0.306518,0.943247,3.54729,0.883223,1.44945,3.65783,0.709348,1.23866,3.18681,3.92327,0.106278,0.093275,1.28177,0.542254,0.68957,0.528313,1.24936,2.35226,0.303834,0.0594064,1.31486,1.1578,2.43625,4.4125,0.904337,1.00147,4.81197,1.31189,0.0296455,1.11495,0.724461,0.520235,1.25262,0.250807,0.0281229,2.57911,2.92775,2.22562,0.75527,0.242219,0.118131,0.363648,0.497668,0.0814463,1.30973,0.0366483,3.00148,1.25068,1.63096,1.52441,0.621856,0.00641068,0.230657,1.73529,1.45144,0.646578,2.25007,3.25211,0.563205,0.0751726,0.0163225,3.54842,3.84219,1.02355,1.92965,0.817386,1.16032,0.510203,1.44918,0.094062,2.12093,0.18292,2.18219,0.247555,1.35479,3.45437,0.0852659,2.62705,0.895785,0.287165,2.27176,3.76671,2.21876,0.34165,1.02308,1.78778,1.66692,2.17194,0.78184,0.405094,0.0208693,4.54682,0.971125,3.88979,2.00045,2.23424,2.78853,5.28522,2.13774,0.237709,0.150786,0.456535,3.60444,0.698361,1.24292,3.25507,1.02311,0.797662,2.94663,3.29284,0.155182,0.0697566,0.439491,0.730536,0.809481,0.477256,1.35965,2.22005,0.560803,0.0364263,1.49128,0.700289,2.63805,3.72576,0.408458,0.932113,4.24421,1.17882,0.134419,0.657352,0.368504,0.184188,1.30552,0.460816,0.028444,2.41779,2.85773,2.01615,0.72293,0.287015,0.0462368,0.160295,0.856651,0.098313,0.130786,0.046435,2.05407,1.54281,2.34828,2.53102,0.442987,0.0561916,0.268094,1.58142,0.8853,1.28698,2.50667,3.04347,0.423114,0.0185859,0.262836,3.83576,3.57948,0.914873,2.57641,0.271825,1.59572,0.423181,1.84506,0.0146318,1.08426,0.0235161,3.13197,0.697155,1.7896,3.38426,0.0901369,2.18545,1.48666,0.392883,2.32617,3.22082,2.32532,0.0902337,0.145137,1.08237,1.66407,2.63171,1.45994,0.131882,0.0723601,5.2952,0.436551,4.01131,2.07321,1.4268,2.99351,4.44962,1.59591,0.584692,0.999917,0.741395,3.61155,1.21488,2.08004,3.2875,1.06792,0.701226,2.57786,3.9834,0.0356573,0.0402862,0.258593,0.57421,1.35641,0.611022,1.26718,1.16932,1.30904,0.0148628,1.97011,0.861008,1.83741,3.52019,0.66894,1.38401,4.86006,0.991134,0.0453428,0.595723,0.376409,0.243376,1.10378,0.772661,0.104803,2.20183,2.70189,1.8724,0.874469,0.276714,0.0854222,0.109846,0.469759,0.0341609,0.822008,0.0663278,2.82431,1.98922,2.58916,2.33014,0.36916,0.000405256,0.440166,1.42747,2.06229,0.839913,2.56837,3.54221,0.806237,0.0238713,0.0986123,4.16513,3.2629,1.11632,2.24042,0.744196,1.76337,0.456146,1.41652,0.0272024,2.58901,0.167701,2.60603,1.09688,1.16621,3.30133,0.272464,2.55796,1.90589,0.180866,2.41701,3.38609,3.11584,0.191306,0.749476,1.46686,1.90682,2.87457,0.694,0.343411,0.0147392,4.91275,0.464997,3.85381,1.65685,1.67556,3.0329,4.73798,2.09576,0.312937,0.517346,1.04992,3.59476,1.26293,1.085,3.32468,1.59134,1.01859,3.35532,3.53533,0.084303,0.0628613,0.648484,1.09515,1.50538,0.17028,1.40541,2.64826,1.06457,0.100404,2.33836,0.694368,2.53434,3.55765,0.68602,1.77681,5.59932,1.25979,0.0556799,0.505265,0.470065,0.221458,1.14363,0.721429,0.0939817,2.04274,3.13907,1.29704,0.464239,0.875146,0.118063,0.167729,0.691054,0.0293266,0.611965,0.0768707,2.41301,1.51695,1.93647,2.18609,0.344321,0.0389794,0.454889,1.37358,1.48123,0.780079,2.11202,3.10542,0.999507,0.0394063,0.204987,4.05465,3.69151,0.976356,1.60781,0.698079,1.13738,0.537282,1.42179,0.00870474,1.36949,0.023588,2.30823,1.14589,1.14334,3.29495,0.192186,2.55375,1.98119,0.272244,2.98279,3.39615,2.60981,0.0805932,0.603192,1.68799,1.85609,2.74516,0.505839,0.325397,0.0708398,5.28361,0.430745,3.53773,2.20675,1.66348,2.58096,4.62437,2.74623,0.222936,0.392539,1.03132,3.74876,1.04139,1.11657,3.41723,0.882675,0.70088,2.92961,3.14528,0.0404496,0.197252,0.654779,1.25583,0.600679,0.254489,1.76305,2.48888,0.957629,0.0304134,1.98809,0.68718,2.4288,3.55978,0.906977,2.39657,4.87962,1.07763,0.0113303,0.629885,0.284732,0.307247,0.693691,0.863577,0.0309877,1.49964,2.85926,1.40856,0.633111,0.401133,0.0182732,0.0368716,0.753268,0.00673428,0.534558,0.00958632,2.73017,2.39266,2.17597,2.33269,0.836123,0.0222678,0.178695,1.06635,1.69794,1.20227,2.49672,4.13729,0.191926,0.0308835,0.0363431,4.61121,3.23564,1.00583,2.72539,0.298151,2.01082,0.769828,1.96608,0.0858546,2.23029,0.0122647,2.23588,0.633349,1.20338,3.37658,0.0692665,2.29815,1.05922,0.480622,2.08249,4.0234,2.28867,0.415034,0.983109,1.92329,1.10093,1.73856,1.04592,0.487978,0.0242384,4.47649,0.681652,5.14066,1.34459,2.59875,3.29062,4.8603,1.86614,0.227459,0.307617,1.15525,2.771,0.254383,2.10122,3.77412,0.778152,1.06289,3.55692,4.12524,0.0866851,0.039943,0.282242,0.490172,1.49075,0.340438,1.89202,3.13522,0.571204,0.0840356,1.08281,0.748803,2.67685,3.2724,0.322296,1.01903,5.3433,1.32265,0.079348,0.697845,0.370772,0.0963784,1.51029,0.731301,0.0519211,3.12523,3.5217,1.55013,0.490105,0.309106,0.152689,0.126233,0.900529,0.0508995,0.470179,0.0570541,2.93815,1.03723,1.31152,1.88639,0.53211,0.0120982,0.19104,1.59915,0.789835,1.25294,2.21089,3.76776,0.442896,0.0824799,0.0142063,4.1578,3.41276,0.872557,2.83429,0.41081,1.12326,0.520284,2.29224,0.034131,2.31664,0.0527101,1.93802,0.23192,1.07679,3.58917,0.103749,2.88465,0.943151,0.274026,1.80946,4.21325,1.81319,0.25273,0.727642,1.31808,1.68266,2.21464,0.861174,0.387389,0.0198225,3.8402,0.379294,4.60951,1.26635,1.76706,3.3753,4.86082,1.96672,0.269316,0.196363,1.33864,3.62479,0.627859,2.09711,3.85491,0.609686,0.881531,2.71234,3.77832,0.0278493,0.0952197,0.350846,0.639852,0.512732,0.316809,1.29115,2.19078,0.408391,0.0650802,1.27545,0.848432,2.89082,3.46501,0.429315,0.532971,4.65582,1.2727,0.0419573,0.771441,0.57798,0.189393,1.16264,0.666373,0.011883,2.61725,2.62028,2.10185,0.367404,0.307311,0.375864,0.115727,1.16286,0.145557,0.639334,0.0189169,2.85532,1.43457,1.51701,1.08727,0.655423,0,0.39513,1.46804,1.47166,0.837551,2.10679,3.85416,0.534801,0.0470078,0.124029,3.15903,3.37361,0.818388,2.83651,0.451094,0.913231,0.778017,1.86247,0.0487507,2.85288,0.0583686,2.2793,0.732785,1.39704,2.60797,0.122074,2.77161,1.1196,0.590158,2.21509,3.73356,2.73063,0.499842,0.927976,1.42161,2.06318,2.38936,0.891494,0.883829,0.209371,4.37902,0.608832,3.81845,2.51543,2.54147,3.3838,4.15177,1.59683,0.192216,0.537215,0.455513,3.93832,1.0829,1.92547,2.39969,1.00714,0.802947,2.15678,3.95803,0.20991,0.124253,0.479957,1.0012,0.444251,0.969963,1.81434,2.18901,0.321689,0.0278072,1.64864,0.651427,2.12086,4.46436,0.998611,1.1613,4.08807,1.39377,0.127706,0.700906,0.552711,0.515313,1.00938,1.48769,0.0935584,3.28137,2.79706,1.85369,0.296472,0.487268,0.0317496,0.19594,1.23039,0.0388062,0.67017,0.0934113,2.86187,1.86251,1.88909,1.87347,0.596602,0.00377345,0.192182,1.35662,1.60304,1.6355,2.39665,4.20327,0.978054,0.0436493,0.0764731,3.90868,3.21256,0.552857,2.17574,0.720971,1.57867,1.00832,1.66714,0.0796119,1.50929,0.0632518,3.0211,0.78774,1.44251,3.49814,0.0800848,2.00075,1.36371,0.743488,2.33431,2.60721,2.96996,0.278091,0.484778,1.58918,2.23393,1.81321,0.76124,1.05518,0.180481,5.67809,1.19608,4.3703,2.42004,1.21932,3.44784,4.91669,1.31593,0.246808,0.215084,0.619059,3.59653,0.725094,1.33354,2.42733,1.78544,1.59301,2.10466,3.41312,0.251837,0.0452664,0.893701,0.983479,0.592334,0.906095,1.89089,2.11333,0.918553,0,1.45872,0.963036,1.41516,4.55745,0.836861,2.22682,4.22401,0.894384,0.0436534,1.00384,0.145187,0.258983,1.36973,1.11032,0.156456,2.41609,2.70471,2.1322,0.526215,0.252542,0.0258043,0.349993,0.35663,0.104863,0.225369,0.24749,3.35109,0.963801,1.10813,0.800809,0.467626,0.107116,0.0552091,2.47139,2.62246,1.78614,3.4856,5.1379,2.94626,0.0129506,0.137714,4.38971,4.69919,0.188566,2.24813,0.520747,0.79837,1.95592,0.550285,0.00599656,3.57328,0.080133,2.81194,0.483219,1.56046,3.52163,0.658864,3.26069,1.05765,1.0118,2.94407,4.80952,1.89636,0.259329,0.0739332,1.2724,3.2216,1.86089,1.15171,1.37013,0.828185,6.12156,0.314778,3.56917,0.780318,1.08123,4.30628,4.93581,2.55729,1.14319,0.0388253,0.890784,3.77139,0.236146,3.92972,3.29398,1.21922,0.728362,2.22584,3.33834,0.835542,0.443234,0.266086,1.38535,0.148491,1.23911,1.60022,1.91213,0.431745,0.0798277,3.92146,0.100696,2.10039,5.02391,0.322355,2.43659,4.82507,0.601774,0.46191,0.594377,1.27198,0.325763,0.846408,2.22323,0.0822376,3.12431,1.4243,2.1481,1.00837,0.604639,0.100744,0.806879,0.668582,0,0.35246,0.0494527,2.93895,0.883624,1.23582,1.33622,0.579971,0.00303413,0.12075,1.26554,0.868126,0.683931,2.13174,3.34147,0.507017,0.034596,0.407799,3.14334,3.45889,0.514382,2.67469,0.30144,0.566191,0.578803,1.58267,0.0177247,1.72527,0.00918701,2.09903,0.249594,1.94982,3.16641,0.119972,2.91085,1.27837,0.211139,2.1288,3.62004,1.90441,0.0502565,0.314473,1.26084,2.53492,2.32336,1.24451,0.584151,0.09295,3.83199,0.583287,4.35442,2.26039,2.16336,3.10987,4.73173,1.53667,0.242611,0.268393,0.520367,3.39473,0.656522,2.33191,2.41795,0.737193,0.88631,1.90878,3.36651,0.160806,0.30254,0.598468,1.03497,0.637035,1.40763,1.29467,1.22339,0.127561,0.0349597,1.86225,0.668291,2.26833,4.25181,0.515916,0.231848,3.73252,1.58088,0.390363,1.09529,0.602876,0.307385,1.07234,1.07695,0.00450348,3.96513,2.2456,2.13224,0.583686,0.3779,0.0323832,1.12609,0.944197,0.0601499,0.516288,0.0505858,3.97665,1.66554,1.54787,2.25417,0.843666,0.109033,0.411015,0.34539,0.718933,0.611431,2.28845,2.05871,0.923705,0.216646,0.0154038,3.44349,2.49335,0.979974,1.59057,0.453513,1.40971,0.303805,1.07996,0.0104552,2.15765,0.575598,1.24831,0.146673,0.647196,3.51472,0.223764,2.36423,2.38342,0.979161,2.10142,3.73741,2.76451,0.864586,0.278941,1.18661,2.21592,2.22132,1.20689,1.51193,0.020477,3.30165,0.550168,2.83984,2.15062,4.65036,4.78363,3.84807,1.75357,0.539525,1.41835,1.5899,3.9677,0.71811,1.18531,4.55276,0.925562,1.40166,2.73467,3.21768,0.090259,0.000825712,0.866169,2.0545,0.279414,0.582492,1.44524,3.59893,0.470387,0.00659904,0.221317,1.14851,2.84489,2.55217,0.285621,1.01121,5.44992,1.04718,0.384958,0.209347,0.269595,0.214903,1.98371,0.982846,0.446999,3.39738,2.89421,1.69827,0.18654,0.0606241,0.572584,0.845817,0.83125,0.142304,0.511877,0.101747,3.0588,1.25144,1.19462,1.74543,0.190779,0.00891978,0.442403,0.66589,0.917971,0.111052,1.06662,2.67585,1.32598,0.0668443,0.0650987,3.3711,3.52265,1.85074,1.12053,0.144353,0.746337,0.826837,2.03384,0.0279713,1.60881,0.160879,1.02235,1.06167,1.20357,4.12033,0.136176,2.46433,2.25091,0.329999,3.34333,2.89288,2.42422,0.0518891,0.154113,0.650573,3.72507,2.40731,0.693673,0.657181,0.0400677,4.00169,0.610979,3.20096,2.30301,3.29098,3.8978,4.15155,2.06398,0.739552,0.580522,0.728652,4.44951,1.04893,2.03725,3.53141,0.726454,0.643724,1.8834,3.04606,0.0857924,0.340497,0.160281,1.16203,0.133217,0.954035,1.35878,1.93417,0.0825962,0.0565659,0.919205,0.37644,2.29047,3.81102,0.682563,1.71354,6.10332,1.62584,0.381732,0.857583,0.478908,0.310714,1.69634,1.53788,0.093128,2.65451,3.03056,2.00864,0.622983,0.242817,0.0446907,0.31917,2.12676,0.584698,0.508028,0.312854,3.57535,1.01957,1.76515,2.32122,0.309599,0.0135759,0.426861,0.866511,1.5532,0.567531,1.69692,3.17837,1.28109,0.138402,0.0340985,4.34716,3.71548,1.77937,2.15931,0.173799,0.726289,0.202157,2.69981,0.0611559,1.09466,0.21049,1.1993,1.28511,1.93591,5.0003,0.160311,3.53014,1.60281,0.552014,2.56209,3.43234,3.58124,1.18037,0.927042,2.41542,2.06958,1.71808,1.0145,0.981867,0,4.76112,1.02332,4.90183,2.32996,3.75912,4.43126,3.90499,1.79408,0.407988,0.975094,1.80331,3.76273,0.680769,1.02908,4.06174,1.33209,1.37562,3.4585,3.2734,0.0544218,0.048771,0.182318,0.83412,0.339921,1.93744,1.76138,3.12507,0.333666,0.603001,0.910121,1.11202,1.64939,2.83296,0.878977,1.53331,5.16968,2.19303,0.26282,0.56399,0.559927,0.339195,1.69228,0.502092,0.0285549,2.2546,4.01991,1.49154,0.449443,0.216892,0.285517,0.116664,1.16992,0.968088,0.554891,0.372601,4.17086,0.611138,1.43107,0.835574,0.497291,0.0541932,0.0318258,1.75179,3.45455,2.30135,4.44617,5.07075,4.09356,0.168165,0.118873,3.71805,5.38558,0.359948,1.17537,0.401529,1.74139,3.03928,0.641873,0.0452245,3.51162,0.225924,1.38501,0.975901,1.95064,3.55211,0.409476,3.11961,2.09502,0.283407,3.57552,3.27555,2.12664,0.34929,0.474149,2.16918,3.38105,0.957036,1.66978,2.51291,1.40494,5.48474,0.223458,5.07802,0.565896,1.52222,3.37747,5.36741,2.40689,1.75742,0.177935,1.56984,2.99916,0.108959,3.20956,2.76626,1.30163,0.704933,4.24807,1.97075,2.02187,0.711818,0.606899,0.713849,0.580595,1.98833,1.63021,3.92626,1.18241,0.0354319,4.58759,0.332903,2.27938,5.06359,0.173173,3.92383,4.6605,1.06388,0.556589,0.171921,1.86755,0.878197,1.36928,0.658514,0.0190993,3.09929,2.0158,2.27263,1.96783,0.792823,0.213694,1.07014,0.38616,0.0420136,0.121149,0.419845,2.73868,1.23557,1.6829,1.17198,0.422408,0,0.212392,2.15687,1.06676,0.841352,3.33135,5.16731,2.13337,0.220289,0.325544,2.79493,2.97439,0.37262,1.64285,0.181783,1.72529,1.00698,1.78506,0.0236844,2.2001,0.10197,1.8321,1.11312,1.18017,2.91984,0.108672,3.7586,0.671367,0.93762,3.10085,3.06066,1.9627,0.179142,0.317111,1.18516,2.57359,2.41962,1.47094,1.01224,0.000268379,4.35446,0.484724,4.49661,1.5371,1.99019,2.96908,6.3332,0.812003,1.23299,0.243958,0.250655,3.52692,1.45084,2.45594,2.48154,0.441547,0.970346,4.3034,3.72199,0.833116,0.0537236,0.949839,1.43612,0.538444,0.225747,1.98101,2.63878,0.72098,0.148604,2.18236,0.28725,2.9508,3.88237,1.00137,0.888707,6.01463,2.11074,0.386278,0.708657,0.847301,0.235339,1.14889,0.734926,0.0110711,4.25795,3.09755,2.88656,0.214374,0.615552,0.0784606,0.794541,1.4357,0.00623513,0.514883,0.03413,3.33517,0.812137,1.2248,1.74864,0.315777,0.013393,0.210911,1.90697,1.91836,0.767752,1.65758,3.16942,1.14282,0.0459913,0.220999,4.17401,4.52362,0.804498,2.51992,0.268469,0.642483,0.851914,1.99326,0,2.50734,0.0381192,2.03101,0.542559,2.1662,4.10502,0.0503313,2.32466,1.89728,0.167142,3.0341,3.65051,1.81492,0.173916,0.377965,0.68109,2.95184,1.96136,1.56334,0.696108,0.00445108,4.07859,0.114465,4.67323,1.05034,1.61055,3.35983,4.21886,1.91939,0.436332,0.254994,0.68775,2.84131,0.521681,2.10881,3.26495,1.07813,0.664651,1.33711,3.38636,0.0436841,0.419039,0.227035,0.94118,0.196337,1.36326,1.60736,1.10145,0.0341857,0,2.58226,0.202405,2.20619,4.68477,0.452925,0.957097,4.00319,1.80225,0.122734,1.26034,0.979403,0.635432,0.928647,1.13046,0.0393081,2.84849,2.48059,2.28962,1.31035,0.412608,0.0703817,0.296971,0.897599,0.0375367,0.294765,0.160068,2.55941,1.3605,1.34182,3.14562,0.427241,0.0111013,0.150922,1.19132,1.69642,1.51754,3.49422,2.81836,0.123653,0.0371704,0.242947,3.68749,3.71244,1.78137,3.2637,0.947374,1.67937,0.796561,2.51693,0.134468,1.68809,0.116397,1.39079,1.24835,1.9665,3.59129,0.0179134,2.62591,0.670484,0.865052,3.07211,3.2161,1.61743,0.771014,0.855325,1.22421,0.375836,1.87587,1.42771,0.939178,0.00956868,4.43019,1.83368,6.28362,2.02937,2.98059,3.77094,4.369,0.936155,0.152694,1.12217,1.54499,2.27825,0.381153,1.99007,2.66481,0.41185,1.30618,2.36695,3.4729,0.175739,0.0572974,0.920726,0.575249,1.12185,0.886291,1.77656,3.09869,0.271949,0.42442,0.827352,1.61262,2.53005,4.59954,0.966383,0.271037,3.76151,1.69681,0.196308,0.92964,0.292832,1.10383,0.979889,0.0910277,0.456767,4.02827,3.03481,0.945839,0.690891,0.159811,0.335634,0.407737,0.668719,0.211333,0.512295,0.0159793,3.84225,0.861139,1.66746,2.88913,1.05076,0.017604,0.28309,1.60989,1.0752,0.338429,1.99783,1.83507,0.453097,0.152038,0.072195,3.54355,4.65087,1.40336,1.6848,0.314609,1.67955,0.824161,2.18883,0.240514,1.77434,0.279308,1.42163,1.27125,1.40641,4.79524,0.0708822,2.12524,1.796,0.135306,1.86181,3.34326,2.62422,0.273251,0.466707,1.28652,1.4584,1.91578,1.2838,0.758845,0.0115877,4.82758,1.02023,4.63398,2.57347,3.50164,3.95303,3.70157,1.64042,0.440699,0.761368,0.495789,1.64867,0.70766,1.93985,2.99937,0.903021,0.935193,1.87404,3.39784,0.0792058,0.0470492,0.257649,0.60916,0.915243,1.18901,1.08066,1.89791,0.142627,0.186183,1.73326,1.11931,2.12438,3.13803,0.993114,1.108,4.62309,2.45593,0.0847078,1.31495,0.32682,0.559858,1.30904,0.349134,0.0207973,2.69449,2.65408,1.76183,0.526198,0.360282,0.0162377,0.452695,1.67969,0.786978,0.738353,0.0760599,2.98748,0.974157,1.96799,1.90436,0.93187,0.0125455,0.144638,1.44335,0.501522,0.782217,1.49275,3.55478,0.51408,0.0272756,0.00305018,3.47825,4.65626,0.926729,1.82176,0.461906,0.700576,1.0538,1.82246,0.0129438,3.10561,0.129999,2.23367,0.193773,1.35113,4.37293,0.408169,2.48598,1.12841,0.26154,2.01435,3.85373,3.28516,0.033117,0.569275,1.23854,1.83767,1.66214,1.18243,0.607491,0.127385,4.84697,0.817407,3.33022,2.42144,2.63031,3.68531,4.49616,1.40106,0.68475,0.911053,0.511081,3.96689,0.497705,2.66518,3.59399,0.693997,0.303797,2.3589,3.17979,0.120173,0.0840141,0.0773957,1.22968,0.873476,0.766364,0.745777,1.11064,0.114887,0.175923,2.15218,0.573491,1.95948,3.04424,0.916047,1.46429,4.72096,1.51816,0.327772,1.1236,0.827795,0.180154,0.970699,0.992361,0.0485862,3.01936,2.58218,1.8066,0.349257,0.142384,0.0494521,0.425016,1.76525,0.178625,0.188875,0.0699539,4.07327,0.691051,1.58693,2.37945,0.216532,0,1.81234,1.15422,0.581613,0.271597,0.473457,2.50652,2.63144,0.181997,0.0424526,2.76629,2.03107,0.867031,0.152904,0.0139542,0.236293,0.0618427,2.29404,0.0216631,1.18051,0.0467634,0.860408,0.57037,0.724652,4.19656,0.0281417,2.31023,1.71416,0.147774,3.36904,2.19917,2.46426,0.0893236,0.077964,1.17619,3.83219,2.2351,0.284899,0.717847,0.00192396,4.0828,0.0221865,2.47903,1.85622,2.79813,3.13349,3.78653,2.18643,1.42026,0.848296,1.66738,4.36045,2.32377,0.300671,2.82945,0.368193,0.19111,1.17597,2.81472,0.231004,0.0665675,0.404295,1.22482,0.0443175,0.790282,1.99073,2.7893,0.349134,0.273168,0.396511,0.449771,2.04238,1.17213,0.0075335,1.75189,4.58506,1.10772,0.211991,0.926428,0.222057,0.110032,0.107369,0.978691,0.0444489,1.09031,3.1166,1.84598,0.23379,0.0729769,0.087661,0.16739,2.09003,0.564941,0.553636,0.98773,3.2317,2.84104,1.344,1.61252,0.924333,0.390287,0.523503,0.745252,1.81386,0.3759,0.961302,0.74361,1.92746,0.0367375,0.254657,4.22352,3.2171,0.928905,0.551574,1.87886,0.820247,1.85763,1.40116,0.460056,1.42202,0.0111928,0.189119,0.220919,1.58414,4.81054,1.16823,0.408528,2.59343,0.0531654,2.90222,2.50481,0.482151,0.0355503,1.4243,0.97523,5.23268,1.0987,1.421,1.03101,0.0672638,3.65836,0.717889,2.82239,2.06995,2.53594,1.60664,1.07846,2.54499,0.898502,0.0471799,0.700152,1.64357,0.0399906,0.482931,2.05672,0.797772,1.13379,0.274555,0.707143,0.234836,1.07659,0.178635,0.615385,0.297467,0.860431,0.110594,2.59657,0.600154,0.479032,0.296133,0.993862,2.26152,1.45347,0.364953,0.894827,2.17797,2.07657,0.611407,0.822876,1.5875,0.144002,1.58942,1.88468,0.114159,1.42093,0.818327,1.9349,0.802866,0.243312,0.301177,0.429983,1.43563,0.656302,0.0938108,0.430553,4.81325,0.0807951,1.46824,2.15321,1.30366,0.0564281,0.156204,2.13588,1.46877,0.472374,0.611697,1.42164,1.90515,0.231403,0.129641,2.6591,2.61479,0.637518,0.904208,2.13806,0.372701,1.13394,1.90304,0.58043,1.33959,0.553732,1.06007,0.985018,0.543808,3.25897,0.206441,1.39186,0.804308,0.384461,0.844792,2.90573,0.300399,0.213936,0.998085,1.63261,3.31909,1.73901,1.18092,0.349687,0.109669,3.74661,1.23421,2.04492,2.26541,2.26596,1.53822,2.52296,2.75313,0.220269,0.019697,0.46049,1.82769,0.388736,1.27916,3.24384,0.741025,0.487426,0.531523,2.76641,0.0748161,0.547243,0.837052,1.31265,1.02772,1.34277,1.34924,1.00636,0.103381,0.0574457,0.980765,0.820282,0.887506,3.20173,0.046969,0.528082,2.77855,1.84989,1.00946,1.86239,0.608829,0.516535,2.2498,0.342212,0.0634859,1.29077,1.86591,2.0693,0.417948,0.261626,0.13938,0.409343,2.22424,1.9599,0.663104,0.11887,6.45383,0.38252,4.04982,1.77296,0.789522,0.227368,0.340924,3.34547,2.93424,1.29059,1.17446,1.97037,1.24899,0.574112,0.254942,2.74412,2.74275,0.509385,0.330373,1.52191,0.451901,2.00786,2.16086,1.21977,1.45518,0.822138,1.62224,2.1564,0.630753,2.69039,0.0497901,1.52492,1.13065,0.222982,1.43297,1.86954,0.621498,0.639015,1.63963,2.85328,2.43762,1.38037,0.973973,1.20674,0.148073,4.70571,3.20529,2.91054,0.694522,3.6054,2.29621,3.79459,0.683008,0.239141,0.2356,0.30097,1.11056,1.80002,1.15592,2.84758,0.432422,0.857427,1.14139,1.32843,0.153965,0.989956,0.686348,0.316861,0.115731,1.02845,3.41758,2.12211,1.2383,0.485448,2.75098,1.07798,1.46718,3.1013,0.249525,1.36975,3.21075,1.94395,0.38638,1.62883,0.523149,2.01228,1.84046,0.501834,0.593366,1.46181,3.40543,1.10476,1.44816,1.1251,0.0295201,0.133602,3.52492,2.39285,0.594473,0.67307,7.33662,0.312076,2.17701,1.45173,0.573155,0.34617,0.487049,3.08279,2.93026,0.649512,1.89446,1.756,0.609289,1.22079,0.398642,2.5188,1.94528,0.986859,0.457446,0.847528,0.412541,0.976149,1.2117,0.705849,1.9876,0.722498,1.24018,2.55111,0.489519,2.72867,0.0291009,2.35459,0.235688,0.316869,1.69132,1.35021,1.12745,0.435903,1.41336,2.33648,2.03596,1.68084,0.584443,1.15148,0.0612399,4.65578,1.55388,4.58667,0.963235,4.25159,2.54535,3.56805,0.892973,0.33514,0.181146,1.26509,1.56171,1.0552,1.02058,2.56878,0.821836,0.81548,2.27766,2.08943,0.648175,0.884804,0.96172,1.02797,0.711759,1.21395,2.56594,3.44538,0.760028,0.0488547,1.48262,0.799258,1.08438,3.27935,0.0382538,1.45882,3.31775,2.27778,0.715653,1.53316,0.778512,1.63186,1.1993,0.220446,0.778951,2.07157,3.78488,0.823545,0.834017,0.654665,0.0800226,0.337342,1.8533,2.51725,0.702393,0.102364,7.71211,0.286745,2.43409,2.67686,0.619262,0.335597,0.0531147,3.60267,2.38416,0.416732,2.09647,1.71686,0.757883,0.930876,0.0560811,1.97866,3.51794,0.56192,0.535995,0.996723,0.851703,0.930008,2.42682,0.615996,1.27864,1.03836,1.36461,1.88232,0.572575,1.65992,0.166636,1.97775,0.721565,0.134376,1.03473,1.5387,0.73856,0.193663,1.30164,2.81109,2.61607,2.13191,1.14167,1.21546,0,4.85786,1.71485,3.11446,0.752463,4.40693,1.40171,4.11217,1.04395,0.399484,0.80766,0.345108,1.13839,0.892735,1.10168,3.1794,0.556764,0.76763,1.86924,1.66639,0.354613,0.812082,1.13795,0.466964,0.466748,2.14874,3.45943,3.79036,0.872065,0.592503,2.39201,2.14517,1.32333,3.32057,0.065606,1.8166,3.61239,1.98733,1.4363,1.76843,0.745389,1.04846,1.55632,0.433596,0.657589,2.06524,3.66612,0.766329,1.20404,0.482788,0.039347,0.568658,3.09411,3.26962,1.34813,0.10506,5.60434,0.0889655,2.88713,2.08657,1.16381,0,0.320336,3.15232,2.11339,0.581482,0.599109,1.64684,0.136905,0.509498,0.0594996,3.93309,3.1175,0.723839,1.43198,1.64233,1.06164,1.62169,1.82454,1.04357,1.74279,0.442203,2.06128,2.01575,0.252877,3.43121,0.0215486,1.07699,0.155145,0.312614,0.946017,1.93533,0.400249,0.157762,2.43273,2.4283,2.45243,2.85849,1.08282,0.64019,0.0445874,4.83998,2.06011,3.49755,0.916848,3.19959,1.67709,2.80862,2.02959,0.462559,0.40393,0.567347,1.60233,0.82948,1.03516,4.08745,0.758877,0.558853,0.81449,2.77149,0.020525,0.424165,0.548064,1.4836,1.51395,1.17532,2.14801,2.16371,0.536734,0.20805,2.26247,0.393927,1.15546,3.80855,0.062539,0.560503,3.57414,1.74579,0.403144,1.8321,0.521171,0.281189,2.58205,0.165068,0.21593,1.33553,2.64503,1.22495,0.95273,0.758475,0.0459459,0.0536204,2.62278,3.09329,0.743081,0.355141,3.07225,0.99825,0.949002,1.12931,1.44038,0.223063,0.0110416,2.39146,1.64444,0.935833,1.60383,0.529734,1.10079,0.362805,0.828244,2.59053,4.22452,0.686707,0.93297,3.42002,0.0990215,0.910626,0.453031,0.468966,2.71115,0.0979038,0.878076,0.580586,1.66404,2.94335,1.24273,1.59951,1.76682,0.486193,2.11952,2.20781,0.287483,0.00329437,3.36734,0.745545,4.66653,0.703305,3.2894,3.1307,0.324362,2.55482,0.777873,1.86771,1.24968,1.76596,0.422246,2.00153,3.43406,0.229743,0.130489,1.09743,2.68829,0.149078,0.925941,3.05519,2.28496,0.148349,1.08481,0.311612,0.871743,1.85163,0.623889,2.28469,1.59312,2.77352,0.738557,1.06193,0.625584,0.341742,2.5183,2.62624,2.89115,3.69003,0.861826,0.267442,1.87515,2.30547,0.962704,0.719618,1.73405,0.0303122,1.83939,2.17335,0.257229,1.65153,1.09087,0.755912,1.62221,1.2047,0.0661021,0.732646,2.63772,0.807986,0.478311,1.44167,3.3315,1.92262,1.10396,2.1373,2.94314,0.605313,0.589089,3.41747,1.81504,1.43408,0.91407,1.51313,1.78559,0.543597,0.0665963,0.848016,4.45425,0.375745,2.11154,2.0208,0.331684,0.872671,0.837342,1.35013,3.21238,0.285412,0.122489,0.0346207,1.26419,2.99989,0.387741,0.275004,0.779306,1.11022,0.949502,1.27925,0.00285642,0.139194,3.4054,0.815279,1.51069,0.314028,2.58728,0.931934,0.0288647,0.85859,1.04216,2.46264,0.431734,0.340156,0.22502,2.04931,2.61658,0.785478,0.0418276,2.72806,0.686128,0.114332,0.593864,0.881908,1.11965,0.0711848,0.935166,0.3465,1.27528,0.173925,0.981481,3.06613,1.73727,2.21345,0.102727,0.694715,0.258337,0.0755219,1.85269,3.1383,2.03729,2.63149,0.560199,0.010192,0.823945,1.67051,2.69149,0.669242,1.35486,1.68324,0.539925,1.1993,0.0525289,0.577206,0.48674,1.16002,0.269629,0.6042,0.215148,0.47513,1.01106,0.912632,0.722549,0.141901,6.56804,0.332258,1.45423,1.84598,1.82717,0.607185,0.573186,2.45562,1.12138,0.721186,1.19955,2.16764,2.1298,0.463881,0.178236,1.82724,3.83526,0.128018,0.762716,0.459628,0.192741,1.12438,1.60296,0.963624,0.888673,0.645426,0.701608,0.815011,0.682288,1.60415,0.127339,1.19198,0.86594,0.288694,1.62697,1.99353,0.0730802,0.0582204,1.01377,1.65823,1.42085,3.36974,1.82254,0.886563,0.110572,3.70404,1.13102,2.06875,2.42856,1.66822,0.726381,3.11729,1.03275,0.200753,0.104538,0.586497,0.485197,0.89011,1.04055,3.76498,0.289018,0.173404,0.913941,0.603713,0.771254,1.16629,1.70345,0.877945,0.199141,1.21614,2.2545,1.42328,0.443216,0.0665258,2.377,1.73775,2.94043,4.8777,0.528893,0.166291,2.38448,2.42238,1.55654,0.459104,0.535974,1.50026,1.24673,0.8198,0.0119287,1.99609,3.31431,2.55657,0.570893,1.3003,0.0539923,0.52299,1.41024,1.28207,0.630163,0.105739,5.2302,0.227859,1.18988,1.75374,2.91697,0.386402,0.277785,2.83866,2.86109,0.331147,1.23632,2.39778,1.80306,0.259275,0.0913708,2.5227,3.94372,0.183673,1.56848,2.52969,0.532227,1.35426,1.27724,0.66081,1.88493,0.0528896,0.384378,0.398594,1.99346,2.60009,0.319524,0.562004,0.493854,0.701511,1.39409,3.22114,0.00298254,0.000888258,2.58557,1.696,3.47152,1.68618,1.24888,1.71971,0.0914841,3.61332,1.02518,2.04624,0.821979,1.69475,0.152026,2.5869,3.31264,0.179382,0.226419,0.27463,0.567847,0.686924,0.527543,2.87715,0.480118,0.376092,0.896157,0.540858,0.364544,1.60616,1.842,2.10057,0.225728,1.059,1.49925,1.6298,0.26304,0.268784,3.2952,1.58951,2.43822,4.05653,0.189399,0.416057,1.68345,1.14149,0.173652,1.17457,1.47797,0.139466,1.87393,1.34337,0.0421757,1.9166,2.95541,1.46175,1.41858,0.866783,0,0.0498921,2.34725,2.31223,1.62391,0.375117,4.67734,1.96945,1.1104,0.41031,2.23528,0.0241686,0.108533,3.11117,1.68433,0.227382,1.86509,2.15314,0.75385,0.14333,0.0874208,2.50929,3.98529,0.110356,1.13896,1.43417,0.313859,1.31874,0.309791,1.05315,1.39793,0.283745,0.876908,0.435468,1.45572,4.32967,0.356555,1.59968,0.916453,0.208693,0.985789,2.3605,0.577597,0.0338435,2.86926,1.3007,4.79799,0.899941,0.919981,1.93226,0.287895,4.67877,1.19555,2.69553,1.09768,1.7825,1.0552,3.7481,1.68595,1.15395,0.121583,0.967309,1.42674,0.24203,1.80173,2.21328,0.455632,0.0445802,2.03796,0.194335,0.623538,0.673932,3.03886,1.03398,0.341118,0.814646,1.06452,2.10121,0.458083,0.307486,2.18301,2.82394,2.79605,4.71776,0.0169554,0.514094,1.82997,0.227555,1.4776,1.54838,1.73153,0.134113,0.53203,1.91166,1.06839,2.55417,2.87113,0.928262,0.717707,1.47587,0.0368642,0.395106,2.57313,2.36082,0.21963,0.323391,5.03808,0.082217,1.79875,1.31236,3.24022,0.0172295,0.314153,2.91945,3.53399,0.424727,2.75348,2.03163,2.58717,0.19609,0.163325,1.67908,4.54266,0.128016,1.19594,4.72996,0.151521,2.57415,0.585961,0.137761,3.29643,0.257186,0.867165,0.450505,1.81,2.83961,0.772029,0.758587,1.41232,0.355646,1.14016,2.61898,0.0082014,0,2.93451,1.90219,3.77763,1.32164,1.4126,2.36852,0.146113,2.66697,1.39908,2.6182,0.160965,1.32218,0.233872,4.31484,3.86704,0.212293,0.0289287,0.211306,0.948793,0.0361328,2.27068,2.3723,0.391514,0.132712,1.97043,0.661183,0.168071,1.03141,1.16468,1.77397,0.436936,1.85067,1.08954,1.75647,0.943836,0.312196,3.22857,2.07949,2.32178,4.79149,0.109242,1.39368,1.99084,0.881735,0.252822,1.66833,2.07736,0.235665,1.47536,2.16852,0.126876,1.65064,1.7142,1.02378,1.67983,0.853408,0.000578408,0.429056,1.95147,2.63166,0.514809,0.292234,6.86733,0.708982,1.07439,2.01243,1.70541,0.0322334,0.125035,2.63003,3.71278,0.252346,1.74815,1.72679,2.06691,0.478085,0.00483701,3.22095,4.07958,0.0830733,0.856439,1.46784,0.161831,1.89061,0.873866,0.556582,2.05569,0.800068,0.749068,1.25719,1.60501,1.70926,0.275427,1.74127,1.1998,0.342481,1.02666,2.29009,0.0140432,0.00846901,1.14974,2.9381,2.94817,1.66574,0.967391,2.07723,0.00166173,4.34336,1.6463,4.26598,0.295507,2.75264,0.581208,2.98656,1.74961,0.267475,0.00369199,0.515228,0.689112,0.215311,2.00409,1.85591,0.100661,0.215568,1.28994,0.443553,0.628079,1.4706,1.66099,0.469097,0.0821688,2.06333,1.43894,3.49428,1.17021,0.0637752,2.23233,1.96668,1.91407,4.57395,0.201747,1.19394,1.94423,1.59399,0.4392,1.26067,1.52161,0.432527,0.68314,0.536641,0.0698201,2.35767,2.47381,1.03315,0.914498,0.835524,0.00987089,1.05986,2.03397,2.89699,0.325294,0.00119871,6.21996,0.00835778,2.5288,2.36303,0.318735,0.496379,0.934812,3.41399,3.82237,0.600501,1.72741,1.0386,1.41984,0.81225,0.117677,3.28885,4.08034,1.41401,1.32952,2.14254,0.730038,0.948184,2.88913,1.35151,1.5385,1.24999,1.30351,2.03615,0.644665,1.51591,0.0244568,0.981655,0.301641,0.643772,1.83247,1.88254,0.108825,0.131312,1.1616,1.51908,2.31524,2.62664,1.71108,1.12073,0.00699734,4.42007,0.975613,2.27487,0.766412,2.54045,0.633935,3.36548,2.42673,1.16776,0.927902,0.710297,1.08234,2.17219,1.42854,3.42324,0.489319,0.301125,0.260754,1.54756,0.0837248,1.38027,0.686394,0.481088,0.567501,1.99992,3.4116,2.68451,1.27092,0.627177,2.89755,1.2995,0.600107,3.28343,0.0155647,0.63508,3.19827,3.25178,0.778291,1.28901,1.18572,1.21674,1.48707,0.20439,0.127491,2.70604,3.09741,1.14486,1.68527,0.38197,0.0627786,0.381863,2.5112,3.71608,1.92274,0.573895,3.72506,2.36487,0.619444,3.19856,3.11657,0.189383,0.180706,3.70595,0.91928,1.20836,1.04514,2.23861,0.245261,0.174829,0.00154734,1.3084,5.75153,1.18626,2.96441,1.93846,0.390841,0.375997,1.0069,0.364289,2.65606,0.102186,0.147101,0.00865398,2.66011,5.33546,1.41967,0.488059,0.293369,1.84573,2.50777,3.20392,0,0.0109018,1.09469,1.82154,2.8126,2.0021,1.60885,0.630014,0,2.09139,1.30937,3.7491,0.664126,0.123373,0.11209,1.19551,1.35644,0.606804,0.187993,1.10878,0.810724,0.0113032,1.7821,1.91432,0.149821,0.293472,0.222547,0.215981,0.556262,0.414154,2.70669,3.63663,2.35668,2.95834,0.267524,1.63889,0.187753,0.12711,3.3947,1.63793,1.35076,2.79242,0.86852,0,0.567694,0.799586,1.22088,2.3433,1.85411,0.88546,0.820162,0.647504,0.123085,0.758855,0.639487,0.812279,0.49055,0.598673,0.366934,0.603286,1.46246,0.445609,0.898841,0.586868,8.73982,1.02971,1.20077,1.94829,1.83092,0.0220182,0.261601,3.15512,3.30329,0.551034,2.94538,1.72373,1.97993,0.779542,0.0822677,2.9852,3.75193,0.144997,0.836988,1.69875,0.504745,2.41732,1.30603,0.608154,1.05779,1.73576,0.36456,1.38593,2.02989,2.08777,0.164489,1.48814,1.37225,0.809428,0.972248,2.35817,0.251876,0.0687024,0.744842,3.6117,3.38709,1.54124,2.41988,2.61364,0.361874,6.00888,1.67688,4.14727,0.158881,2.55793,0.111752,2.84868,1.40215,0.489402,0.271234,0.513271,0.497959,0.11324,3.00346,1.56173,0.205529,0.497307,2.00518,0.614836,0.908684,1.41282,3.29453,0.969603,0.547435,2.81178,2.94911,4.6194,1.27235,0.716387,3.19781,1.2433,0.992348,3.85399,0.407022,1.57019,2.38822,0.998639,0.221525,2.03134,1.33124,0.617507,1.30361,0.708646,0.759568,2.45856,3.54255,0.812994,0.204393,0.436802,0.0270976,1.53677,1.46702,3.65913,0.466971,0.251133,6.95959,0.0333794,1.40907,2.13019,1.31999,0.292266,0.181486,3.14676,2.28961,0.490431,1.35744,1.39209,0.084761,0.816483,0.0671694,2.58854,3.1498,0.61749,1.58415,1.30812,0.149177,1.0595,2.15829,0.968012,2.30158,1.45327,1.81182,2.29565,0.689341,2.11324,0.112973,1.12802,0.219814,0.877145,0.537364,2.30229,0.153106,0.00261487,0.789265,2.10999,1.36101,1.68151,1.41636,1.05213,0.0044397,4.59925,0.991876,3.37134,0.220099,3.13481,1.31909,2.52578,1.33094,0.627478,1.09398,0.353905,0.799904,1.02119,2.29109,3.5297,0.463236,0.436492,1.10343,2.8353,0.123138,0.790711,1.18252,0.823587,1.04403,1.73025,2.1707,3.33867,1.07313,1.18701,3.20581,1.10758,0.958429,3.31983,0.0549896,1.10189,2.74074,2.45557,1.44002,2.93435,0.970824,0.895067,0.71077,0.124661,0.765065,1.71309,3.49349,0.65441,0.672074,0.354003,0.0919241,0.506844,1.93992,3.61502,0.446271,0.3745,4.5924,1.23532,0.241716,1.43527,1.75488,0.539591,1.15005,2.69045,1.74025,0.110312,1.61856,1.47826,0.826395,0.502828,0.422565,3.59079,3.1942,1.72556,0.845413,0.360287,0.274238,0.797396,1.93063,1.06382,1.0984,1.65286,0.27872,0.481458,2.84992,2.61509,0.0257836,0.255934,1.07027,0.218782,3.23025,3.41362,0.0452738,0.0335229,0.955382,1.47124,4.39593,1.03073,2.08899,0.991484,0,2.82603,0.44292,2.68303,1.31588,0.534263,0.0979332,1.1765,1.88004,1.33126,0.51734,1.27743,0.82657,0.43469,1.03023,1.61289,0.637126,0.246193,0.66784,0.261125,0.202319,2.98351,1.77656,1.7038,0.869062,2.82035,0.667508,1.84661,0.584493,0.0369572,2.16519,1.93956,0.818924,2.80943,0.160816,0.135175,0.51689,0.825145,1.074,3.13664,1.27932,0.117167,0.531846,0.91422,0.00876264,0.795256,2.56697,1.44093,1.00984,0.455143,0.0774129,0.100801,1.35025,1.91794,0.257685,0.174504,6.99856,0.132885,2.00133,1.88682,1.80335,0.785178,1.18023,3.76684,3.1939,0.565615,2.31022,2.75415,1.73442,0.41269,0.291212,3.30776,3.32774,0.126696,0.606345,1.4245,0.384673,1.15666,1.35154,0.979975,0.766484,2.00923,2.16857,2.27555,0.398939,1.1732,0.138618,1.23992,0.447373,0.616605,1.00978,3.01147,0.0924396,0.0674492,1.66365,4.12297,1.79219,1.82543,1.29482,2.48605,0.0176754,5.19097,0.986877,2.57266,1.57903,3.02494,0.390604,4.12094,1.05613,0.470683,1.23008,0.247726,1.68997,1.28396,1.19812,3.29264,0.0823708,0.131314,1.13449,1.04483,0.462526,1.5643,2.42756,0.356194,0.295758,2.51568,4.19365,3.52803,1.47537,0.420603,3.22312,2.51929,3.17259,3.80346,1.02889,0.487563,3.3617,1.2777,0.312199,1.22358,0.259978,0.480599,1.02987,0.188222,0.151817,1.71614,3.593,2.10357,0.849569,0.942012,0.042104,0.509642,1.40164,3.83614,0.184156,0.221195,5.11221,1.0615,1.43651,1.90652,1.32315,0.0532484,0.439429,1.16011,1.90052,0.532259,1.15372,1.07451,0.656496,0.354654,0.384508,4.31573,2.85546,0.325853,0.566785,2.11056,1.70249,2.32448,0.330881,0.140268,1.50035,0.0365847,0.707865,1.58583,0.280018,3.56375,0.158765,2.68258,0.408648,1.22888,1.0269,1.02123,0.407549,0.233926,1.51785,3.37066,2.94239,1.27334,1.87556,3.5836,0.133146,5.41036,1.91804,3.84347,0.619772,2.98138,0.331498,2.28954,1.83127,0.710477,0.604542,1.88388,1.43014,0.224193,0.667574,3.3583,0.680597,0.304905,1.52853,0.296768,0.588059,1.01646,3.68636,2.01778,0.569689,0.464187,2.63521,5.09287,0.647101,0.151575,1.0164,0.403723,0.650813,1.84017,0.0552277,2.31462,2.00682,1.79648,0.236092,1.40605,2.18151,0.082038,0.504618,0.31107,0.398998,1.36783,3.16728,1.43395,0.365569,1.51801,0.0374257,0.849416,0.351533,2.94804,0.974565,0.861189,5.25728,1.02628,0.873751,2.13586,2.61117,0.703581,0.0308287,2.81073,1.29858,1.12471,0.25401,1.83754,1.22727,1.3554,0.0133957,0.857975,3.37658,0.100097,0.995698,0.856568,0.0534877,1.25937,0.299307,0.51188,1.93843,0.844398,0.459178,0.082004,1.25152,1.87234,0.852074,1.10209,0.0674482,1.17181,0.702495,1.02526,0.0325532,0.026385,2.39219,0.415488,1.07483,0.60647,1.98251,0.304612,0.022602,2.08917,1.07849,1.73861,0.589407,0.532113,0.741026,2.9143,1.63904,0.408422,0.097312,0.769933,0.521111,0.45975,0.5108,0.961989,0.21067,0.00781904,0.431027,1.33048,1.45045,0.258897,0.41484,2.11862,1.19518,1.78641,0.192141,0.124088,0.00156289,0.125198,1.91346,2.21974,1.95588,3.44095,0.166916,0.00183741,1.91274,1.71632,2.48322,0.294634,0.865751,2.8646,0.642169,1.80255,0.098533,0.583954,0.872587,1.32031,0.517327,0.975735,0.0171696,0.60458,0.940533,1.24504,0.663863,0.111222,3.36156,0.589114,0.958669,1.90041,2.92993,0.261462,0.312009,2.1037,2.22552,0.867818,2.59185,1.03615,1.82114,0.324595,0.457084,2.42399,4.60721,0.708082,1.73009,4.23178,0.44775,2.33453,0.453969,0.344531,2.91467,0.121897,0.114587,0.0283162,2.26917,3.26856,0.847829,0.408003,1.85722,0.238222,2.36447,2.69791,0.0400725,0.0149142,2.89335,0.929881,4.48752,0.794999,2.79169,2.13796,0.270315,2.84102,0.44938,1.6643,0.504644,0.379103,0.457211,2.70984,4.60011,0.656926,0.192714,0.40539,1.16158,0.0783707,2.54728,3.43145,1.0045,0.1937,1.56081,1.80243,0.372579,1.26293,1.02895,2.1259,2.32369,2.18291,0.625517,1.5817,0.675317,0.897016,3.24683,1.99096,2.37221,3.13066,0.00317319,0.286411,2.29131,1.21139,1.66276,0.589232,1.81656,0.144804,2.01761,2.55133,0.0883723,0.728125,0.771948,0.999863,1.90504,1.11528,0.352635,1.09363,2.05708,1.93355,0.253309,0.116556,4.68634,0.223883,1.67129,1.93171,1.97933,0.00383753,0.280216,1.40586,3.09517,0.213545,1.66106,2.13006,2.98515,0.271883,0.0522383,3.09997,3.51699,0.141877,0.651085,2.91886,0.343161,2.02151,1.13365,1.30164,2.875,1.17437,1.1995,0.855122,1.28774,2.12559,0.458248,2.22088,1.93161,0.952154,2.20498,3.26444,0.250488,0.00407381,1.85257,2.29864,2.46343,0.561894,1.62252,2.49955,0.309508,2.61178,1.33307,3.72915,0.643366,3.51015,0.562023,3.57493,3.10522,0.328935,0.119799,0.897191,1.89314,0.119459,2.02567,2.35492,1.34613,0.383373,2.32709,1.71177,0.134538,0.375472,0.889519,1.85584,1.17967,1.46707,1.70745,2.37053,0.720302,0.472549,2.28538,2.25924,3.50832,5.28705,0.84084,0.875018,3.52472,1.67576,0.387816,0.449753,2.36395,0.0993472,1.84713,1.42916,0.0308633,2.50824,1.53989,1.30368,0.730894,0.759442,0.111097,1.70707,1.80198,2.37097,0.812136,0.120098,4.86855,1.24059,2.86626,2.44454,0.765356,0.204651,1.52859,1.3326,0.527706,0.962032,0.919145,2.30383,1.64707,0.193598,0.293331,2.66928,2.69068,0.708493,0.227696,1.3636,1.16759,0.13128,2.24242,0.647674,1.27019,0.198226,0.660604,0.416808,1.01276,3.12247,0.387053,1.73044,2.10827,0.6178,0.565228,1.56219,0.447435,0.4185,0.961366,2.13676,2.59692,2.04515,0.937313,0.196836,0.00168212,2.53829,0.734472,2.94557,0.865219,3.53086,4.39869,3.41571,3.60505,0.38702,0.440112,2.58771,2.84441,0.811425,2.02781,3.80813,0.565964,0.126715,0.61499,2.82767,0.0637746,0.335573,0.393531,0.976523,0.75281,0.915117,1.70388,1.42754,0.468946,0.0694357,1.1903,2.43293,2.69881,2.03315,0.0570462,1.02948,6.27247,3.65036,0.71304,1.27246,0.271345,0.530685,1.8314,0.463416,0.141136,2.70229,2.4328,1.15523,0.550393,0.0891806,0.0283122,0.325709,2.4861,2.15164,0.898232,0.210259,3.1975,0.997738,0.191175,1.55085,1.71338,0.0577143,2.03634,0.873271,1.88994,0.713991,0.665725,0.109941,1.71841,0.186993,0.268377,3.74585,2.38165,0.328973,0.538873,2.25382,1.36053,4.35583,0.320201,0.641422,1.44763,0.416863,1.08291,0.267269,0.679639,1.50867,0.542548,0.490702,1.41823,0.432038,1.9593,1.10792,0.105017,0.0269861,4.09409,0.96845,6.05413,0.186122,3.53427,1.90091,0.421887,1.95093,0.348475,1.01732,2.2948,1.17767,0.642385,1.31366,2.82119,1.42115,0.539733,2.13342,2.80341,1.1287,0.10518,1.32477,1.47408,0.525935,0.423997,1.4702,0.819589,0.996782,0.507418,0.792482,0.117183,1.16909,0.60062,0.798,1.19358,0.211437,2.14669,0.834304,2.43454,2.85052,0.211237,0.51729,2.33352,0.254928,0.25645,0.38028,1.65891,0.0968816,1.5055,1.407,0.0182651,1.11253,0.616646,1.46619,1.64317,0.659965,0.00739392,0.163936,1.33386,1.29004,2.11317,1.03964,3.65167,1.30471,2.29612,1.38089,2.01531,0.0406192,0.331822,1.60388,1.93104,0.812058,1.03308,1.75459,0.86162,0.152441,0.2124,1.42098,2.72777,0.214831,0.677203,2.25576,0.731401,2.00064,1.31908,1.27204,1.79025,0.149946,0.0791397,0.266467,1.14631,3.07066,1.06992,0.606958,0.239376,0.956309,2.76435,1.11536,0.000791357,0.0550667,2.86583,1.17624,6.24452,1.18306,1.69543,1.13793,0.320872,3.57882,1.89732,0.935178,0.783058,1.5818,1.3382,2.74709,1.92389,2.03014,0.229265,1.39986,1.77446,0.403916,1.45285,2.01894,0.317412,0.105496,1.13236,0.569124,0.535925,1.0535,1.46998,1.68174,0.253648,0.816185,1.05546,1.41445,1.19722,0.0818693,2.06625,0.997922,0.737934,2.35428,0.0058167,0.145901,1.48014,1.18936,1.02932,1.85306,1.51759,0.428515,1.52003,0.803402,0,1.38276,1.61682,1.30886,1.18338,0.586477,0.0902724,0.831467,3.56661,1.31524,0.876652,0.0631791,4.7749,1.07338,1.69869,1.58828,0.25187,0,0.947262,1.35063,1.7544,0.335397,0.602412,1.81719,0.83498,0.0757302,0.0206605,1.9908,2.08454,0.59629,0.108872,0.0560465,0.0712629,0.71132,1.64435,0.262862,1.37295,0.0695136,0.742151,0.55555,1.31163,4.53364,0.144293,1.48394,0.706466,0.220706,0.92508,2.26944,0.495825,0.0551027,0.185759,1.27448,4.2655,2.43653,0.900594,0.135071,0.0394816,4.60703,0.308671,2.70279,1.00552,2.895,2.59958,2.97339,3.17499,1.61685,0.646072,0.582307,3.176,1.11632,0.462667,2.5227,0.720804,0.0654785,1.10443,3.22618,0.166501,0.0845012,0.269973,0.851177,0.141087,1.16259,0.60383,1.48998,0.0792487,0.130158,0.53897,0.779274,1.28965,1.09309,0.00114253,0.760716,3.85795,0.544777,0.670001,1.39091,1.41372,0.537886,1.09387,1.16429,0.00110852,1.36474,1.96186,1.17676,1.34232,0.392489,0.0028248,0.820288,3.33081,1.9707,0.141534,1.32752,4.32412,1.36519,0.7822,1.24212,1.29921,0.00449213,2.96646,0.890678,0.947418,0.524281,1.01502,1.19485,2.00927,0.140004,0.0336756,3.82305,2.6648,0.808076,0.175849,0.379751,1.27281,1.64596,0.85119,0.304467,1.05965,0.88702,0.673684,0.412961,1.16726,5.23773,0.181942,1.30786,2.51306,0.0140293,1.52968,1.47284,1.31953,0.115669,1.00374,2.17982,7.58773,1.60782,2.21347,1.31729,0.489387,2.94165,0.132097,1.60515,0.845589,1.8837,2.59501,1.74942,3.93922,2.10887,0.885373,1.38633,3.0048,0.850073,0.156651,2.41793,1.78144,0.102717,1.45141,2.25859,0.553741,0.352278,1.39596,0.605364,0.722566,2.26311,0.989234,3.1654,1.32028,0.131032,0.430185,0.63604,1.89814,0.840887,0.0145833,1.12403,3.63153,0.461552,0.338114,1.41944,1.2121,0.0547536,1.15151,2.13569,0.183726,1.35507,2.37558,0.880525,2.10045,0.313574,0.158706,0.819517,2.97787,1.5779,0.617671,0.468382,4.2573,2.68184,0.405576,1.66272,2.19618,0.454411,0.112507,2.0079,0.605678,0.0785262,0.808339,0.763635,0.514089,0.251851,0.199175,2.23691,3.86687,1.66287,1.84195,1.37314,0.902038,0.777063,1.49601,0.37944,1.16528,0.189918,0.0748914,0.0220248,2.78947,5.58867,1.3001,0.0293565,1.03021,0.822084,2.649,3.1179,0.0243542,0.0170031,2.31794,2.32811,4.56341,1.56396,2.14732,0.507619,0.0942021,1.91325,0.484128,3.22408,2.23196,0.548829,0.251454,0.504564,1.64646,1.11299,0.448078,0.00928187,1.99006,0.222099,0.00566369,2.7882,0.721351,0.716473,1.48018,0.348745,0.0468219,1.15817,1.77556,1.69238,1.53411,2.84495,0.437648,1.22891,0.149386,0.50927,1.36986,0.939152,4.01539,1.81847,0.116702,0.0480713,0.592695,0.635974,0.522637,1.95381,1.00631,0.00983327,2.25001,0.568138,0,0.0477794,0.946258,1.89782,2.10002,0.680816,0.118034,0.289923,0.534844,1.14276,0.17787,0.0671842,3.74189,0.598143,1.46102,1.59229,0.573945,0.0248033,0.18219,2.10823,0.628571,0.277635,0.55642,1.37147,0.189093,0.304289,0.021872,0.766755,3.72452,0.849935,0.742728,0.336274,0.727936,0.875328,1.5026,0.148357,1.73686,0.0130925,1.14383,0.142981,0.537176,3.29533,0.0895149,2.22682,0.590893,0.123592,0.956116,1.90591,0.335271,0.144899,0.965926,0.810877,3.06673,2.55084,0.898227,0.188399,0.218642,4.21891,0.949401,2.19536,2.56585,2.28743,1.50976,4.43636,2.14383,1.21491,0.238991,0.119743,3.13956,0.7063,0.698713,1.79337,0.713203,0.239497,1.12453,2.11674,0.215012,0.148944,0.371673,0.688459,0.584677,0.668979,1.1467,1.24969,0.365664,0.00402623,0.913581,1.31169,0.679478,3.68138,0.128535,0.452051,2.87009,0.942447,0.473775,0.990575,0.495187,0.543496,1.7609,0.644544,0.352128,2.7088,1.69498,2.96797,0.55632,0.00166314,0.037931,0.867557,2.67301,1.17689,0.590685,0.0582472,4.19662,0.179836,1.69836,1.31358,1.49582,0.406852,1.29723,1.39667,0.477145,0.447816,1.19232,2.0995,1.4763,0.0372801,0.558062,2.76918,3.73471,0.308949,1.57176,0.553801,0.365138,1.15407,2.44019,0.297839,0.90976,0.615724,0.813148,0.463448,0.654772,2.71199,0.237753,0.909471,1.26044,0.74914,1.39616,2.38322,0.167203,0.0126121,0.225377,1.5502,2.26877,2.24333,0.973906,0.709005,0.0451798,3.05034,0.189501,2.54107,1.02928,2.93998,1.85342,3.96938,2.19005,0.321954,0.162484,0.813151,2.76814,1.26534,2.03445,3.33698,0.651145,0.120352,1.04319,2.61028,0.264721,0.305187,0.897256,1.02103,0.58373,1.0037,1.71082,1.52767,0.668524,0.0208841,1.75831,1.83232,3.35193,4.0909,0.294792,0.394428,3.74717,3.01426,0.839781,1.40738,0.364597,0.526923,1.33506,0.342368,0.0127373,2.4178,2.62937,2.66246,0.298979,0.372313,0.149647,0.286726,2.90082,1.18625,0.520868,0.100791,3.63484,0.177705,1.7658,2.12389,0.915643,0.0530187,0.868138,1.60942,0.807279,0.734285,1.02882,2.66665,1.09878,0.207759,0.0152958,1.39496,2.98445,0.345051,1.99615,1.6004,0.905284,0.662348,2.13196,0.395938,2.09453,0.133119,2.15142,0.367175,0.262221,2.86103,0.118603,1.45757,0.690362,0.78626,2.20591,2.73903,0.103179,0.00699327,0.469433,1.13614,1.86629,2.94935,0.47088,0.0390566,0.00386581,4.85439,0.915615,1.80378,1.58185,2.70266,1.99958,3.65516,2.39768,0.321273,0.287234,0.443654,3.43264,0.878042,1.2525,4.4268,0.624393,0.499974,0.693653,3.4787,0.0673157,0.22458,1.16722,2.52149,0.631295,0.267162,1.50803,0.86045,0.36575,0.0676028,1.27223,0.49721,1.87069,4.34851,0.0029013,0.405826,3.7857,3.4135,0.674171,1.74218,0.42268,0.708406,1.79269,0.464328,0.139114,2.36464,1.41014,2.57096,0.355134,0.604991,0.391504,0.319167,2.55726,0.739781,1.02642,0.205901,3.9757,1.18295,1.24104,1.63107,1.51492,0.0216242,0.0775006,2.32664,2.68503,0.487947,1.85133,1.7975,1.55172,0.089643,0.0868945,2.00716,3.74157,0.401673,1.4517,2.23622,0.503565,0.864392,0.640068,0.360182,2.73337,0.0533497,0.796302,0.0789252,0.493384,3.57012,0.750832,0.837013,1.6331,0.361186,0.937966,3.20455,0.354754,0,1.34457,2.48876,3.36057,1.98883,0.920533,0.856483,0.0131143,3.56402,0.398756,3.64401,0.859795,2.60172,0.820675,3.51612,3.01152,0.256439,0.0793038,0.217291,2.4773,0.0733517,1.59288,2.78761,0.257391,0.0951554,0.812235,1.26376,0.108545,0.287873,1.1668,1.39608,1.13534,1.01982,1.83664,2.25875,0.785398,0.145803,0.978086,1.04681,3.51331,4.46406,0.338991,0.440636,2.46854,0.93062,0.387732,0.645399,2.80796,0.110147,2.04517,2.49583,0.257262,0.930653,0.876773,1.16229,1.00568,0.579815,0.02377,0.9081,1.58965,1.33536,0.390522,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\r\ndata = torch.Tensor(vue_clip_emb).view(1, -1, 100)\r\nlengths = torch.tensor([74])\r\noutput = ts_model_multihead.forward(data=data, lengths=lengths)\r\nprint(output)\r\n', '\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-66-6d725915459d> in <module>\r\n     53 data = torch.Tensor(vue_clip_emb).view(1, -1, 100)\r\n     54 lengths = torch.tensor([74])\r\n---> 55 output = ts_model_multihead.forward(data=data, lengths=lengths)\r\n     56 print(output)\r\n\r\nRuntimeError: \r\n\r\n\r\nnew_type INTERNAL ASSERT FAILED at caffe2/torch/csrc/jit/passes/shape_analysis.cpp:280, please report a bug to PyTorch. \r\nThe above operation failed shape propagation in this context:\r\nat /mnt/xarfuse/uid-156246/70539c06-ns-4026531840/torch/nn/functional.py:3312:12\r\n                                                  dtype=attn_mask.dtype,\r\n                                                  device=attn_mask.device)], dim=1)\r\n            if key_padding_mask is not None:\r\n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...  <--- HERE\r\n                key_padding_mask = torch.cat(\r\n                    [key_padding_mask, torch.zeros((key_padding_mask.size(0), 1),\r\n\r\nThe above operation failed shape propagation in this context:\r\nat /mnt/xarfuse/uid-156246/70539c06-ns-4026531840/torch/nn/functional.py:3303:4\r\n    q = q * scaling\r\n\r\n    if bias_k is not None and bias_v is not None:\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...  <--- HERE\r\n        if static_k is None and static_v is None:\r\n            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\r\n\r\nThe above operation failed shape propagation in this context:\r\nat <ipython-input-60-415a6b53c56b>:127:8\r\n          L is the target sequence length, S is the source sequence length.\r\n        """"""\r\n        if not self._qkv_same_embed_dim:\r\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...  <--- HERE\r\n            return nn.functional.multi_head_attention_forward(\r\n                query, key, value, self.embed_dim, self.num_heads,\r\n']",[],0,0
145,pytorch,3195,closed,PyTorch throws an exception on import when denormals are flushed to zero,"This happens, for example, when you import DyNet before importing PyTorch. The code from #3113 tries to take the log(0) which throws an exception.

My guess is that DyNet is setting FTZ or DAZ (or both). See:
https://software.intel.com/en-us/node/523328",,"['cc @SsnL ', 'I plan to look into this after dealing with the sparse stuff. How urgent is this?', ""Since it seems easy to work around, it doesn't sound super urgent to me. (But it would be good to fix... as with all of the bugs :)"", ""We need to fix this before the next release, but it can definitely wait until after the sparse stuff\r\n\r\nThere's an easy fix: just replace the computation in `__MIN_LOG_SCALE` with `-323`. The minimum denormalized number for IEEE float64s is unlikely to change.""]",[],[],0,0
146,pytorch,31560,closed,What should I do if my model has multiple inputs and multiple outputs?,"Here are the functions to export the model using onnx:
torch.onnx.export(model, dummy_input, ""alexnet.onnx"", verbose=True, input_names=input_names,
output_names=output_names)
What should I do if my model has multiple inputs and multiple outputs? How can I export a model?
My model interface is roughly as follows:
 model = YOLO_SEGONE()
 model_initial(model, model_name)
 if is_cuda == True:
        model.cuda()
 model.eval()
 mask_out1Ôºåmask_out2 = model.seg_sub_net3D(conv9_roiA, conv7_roiA, conv5_roiA, conv3_roiA)
        ",,[],[],[],0,0
147,pytorch,15916,closed,Where to download pytorch=0.2.0_4,"I specifically need pytorch=0.2.0_4 to test a project built on this version (https://github.com/oawiles/X2Face) but can't find this anywhere on https://pytorch.org/get-started/previous-versions/, nor does  work. Is there any way I can install this specific version?",,"[""I'd recommend to use the latest 0.2 version from the page you link and see if that works.\r\n"", '0.2.0_4 or 0.2.0 have the same exact code. _4 is just the build number, you can ignore it.']",[],['conda install pytorch=0.2.0_4'],0,0
148,pytorch,22296,closed,Support data parallel for heterogeneous GPU,"## üöÄ Feature
<!-- A clear and concise description of the feature proposal -->
Hopefully,  can support data parallel on multiple heterogeneous GPU.


## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->
I'm currently working on a big model and would like to use  to speed up training. However, I use two different GPU on my computer, i.e, GTX 1060 and GTX 1080, when I train the model, 1060 becomes a bottleneck and slows down the whole training process.


## Pitch
It will be great that  can automatically assign tasks according to computation power or support manually specifying mini-batch size on each GPU.
<!-- A clear and concise description of what you want to happen. -->

",feature module: multi-gpu module: nn triaged,"['@pietern @fmassa and @soumith think we should not do this.', ""@KaitoHH Thanks for creating the issue. To add a bit more context, using heterogeneous GPUs is not that common, and therefore I think we shouldn't overspecialize `nn.DataParallel` for this use case. You know the performance imbalance for your particular setup, so you could try to do manual or automatic load balancing yourself by tuning the batch size per GPU, for example. I would start with a copy of `nn.DataParallel` and scatter an input batch based on throughput (or another characteristic).""]",[],"['torch.nn.DataParallel', 'torch.nn.DataParallel', 'nn.Dataparallel']",0,0
149,pytorch,22780,closed,Build Pytorch/Libtorch with TBB support is failing,"## üêõ Bug

Build Pytorch/Libtorch with TBB support is failing

## To Reproduce

Steps to reproduce the behavior:

1. Set environment variables  to use TBB:

2. Build: 


## Expected behavior

It builds successfully. Building with OpenMP is working in the same environment.

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


 Collecting environment information...
PyTorch version: N/A
Is debug build: N/A
CUDA used to build PyTorch: N/A

OS: Ubuntu 14.04.6 LTS
GCC version: (Ubuntu 4.8.4-2ubuntu1~14.04.4) 4.8.4
CMake version: version 3.11.1

Python version: 2.7
Is CUDA available: N/A
CUDA runtime version: 10.0.130
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.5
/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.21

Versions of relevant libraries:
[pip] numpy==1.8.2
[conda] Could not collect
## Additional context

Attached CMakeOutput and CMakeError files.
[CMakeError.log](https://github.com/pytorch/pytorch/files/3384330/CMakeError.log)
[CMakeOutput.log](https://github.com/pytorch/pytorch/files/3384331/CMakeOutput.log)


",module: build triaged,"['cc: @ilia-cher ', ""@abonnet, a few comments/questions:\r\n\r\n1. The build options interface for parallelism is not stabilized yet, in the trunk we changed the MKL threading option name to MKL_THREADING to mimic MKLDNN_THREADING, please use MKL_THREADING=TBB\r\n\r\n2. I wonder why is PARALLEL_BACKEND=NATIVE? if you'd want to have a TBB build, then I'd recommend PARALLEL_BACKEND=NATIVE_TBB. Please note that we'll simplify this further and will post about available build options\r\n"", '@ilia-cher Yes, I tried both for PARALLEL_BACKEND but the goal is to get a TBB build. Switching the flag MKL_THREADING is not solving the problem. Do you have a commit known to be working from master? I tried multiple commits but no luck.', ""I've just tried these build settings on the latest trunk and it built successfully.\r\nIs there any chance you could share the full log? e.g.\r\n`USE_CUDA=0 PARALLEL_BACKEND=NATIVE USE_OPENMP=0  USE_TBB=1  MKL_THREADING=TBB BLAS=MKL  USE_MKLDNN=1  MKLDNN_THREADING=TBB  BUILD_BINARY=1 python setup.py develop --cmake 2>&1 | tee ~/output.txt`"", 'I ran your command with latest trunk and still got an issue. Attached is the output file\r\n[output.txt](https://github.com/pytorch/pytorch/files/3407759/output.txt)\r\n\r\n\r\n', 'Looks like the actual error you\'re experiencing is:\r\n```\r\nCMake Error at cmake/Dependencies.cmake:92 (install):\r\n  install TARGETS given target ""tbb"" which does not exist in this directory.\r\n```\r\n\r\nwill investigate\r\n', ""Yes, that's the only suspicious output I can think of"", 'I managed to reproduce the issue by downgrading cmake to 3.11.0, originally I used 3.13.3 and it worked with it', 'From https://cmake.org/cmake/help/v3.13/release/3.13.html:\r\n""The install(TARGETS) command learned to install targets created outside the current directory.""\r\nPossibly related, will check further', 'Ok, I can upgrade to 3.13.3.', ""I'll update this cmake rule to make it compatible with older cmake versions"", 'Confirmed that upgrading to Cmake 3.13.3 is fixing the issue! Thanks a lot @ilia-cher ']","['\r\nexport USE_CUDA=0\r\nexport PARALLEL_BACKEND=NATIVE\r\nexport USE_OPENMP=0 \r\nexport USE_TBB=1 \r\nexport MKL_TBB=1\r\nexport BLAS=MKL \r\nexport USE_MKLDNN=1 \r\nexport MKLDNN_THREADING=TBB \r\nexport BUILD_BINARY=1\r\nexport USE_EIGEN_THREADPOOL=1\r\n', '\r\n...\r\n-- ******** Summary ********\r\n-- General:\r\n--   CMake version         : 3.11.1\r\n--   CMake command         : /usr/local/bin/cmake\r\n--   System                : Linux\r\n--   C++ compiler          : /usr/bin/c++\r\n--   C++ compiler id       : GNU\r\n--   C++ compiler version  : 4.8.4\r\n--   BLAS                  : MKL\r\n--   CXX flags             :   -Wno-deprecated -fvisibility-inlines-hidden -O2 -fPIC -Wno-narrowing -Wall -Wextra -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math\r\n--   Build type            : Release\r\n--   Compile definitions   : TH_BLAS_MKL;ONNX_NAMESPACE=onnx_torch;USE_GCC_ATOMICS=1;MKLDNN_THR=MKLDNN_THR_TBB;HAVE_MMAP=1;_FILE_OFFSET_BITS=64;HAVE_SHM_OPEN=1;HAVE_SHM_UNLINK=1;HAVE_MALLOC_USABLE_SIZE=1\r\n--   CMAKE_PREFIX_PATH     : /usr/lib/python2.7/dist-packages\r\n--   CMAKE_INSTALL_PREFIX  : /torch_build/pytorch/torch\r\n-- \r\n--   TORCH_VERSION         : 1.2.0\r\n--   CAFFE2_VERSION        : 1.2.0\r\n--   BUILD_CAFFE2_MOBILE   : ON\r\n--   BUILD_ATEN_ONLY       : OFF\r\n--   BUILD_BINARY          : True\r\n--   BUILD_CUSTOM_PROTOBUF : ON\r\n--     Link local protobuf : ON\r\n--   BUILD_DOCS            : OFF\r\n--   BUILD_PYTHON          : True\r\n--     Python version      : 2.7.6\r\n--     Python executable   : /usr/bin/python\r\n--     Pythonlibs version  : 2.7.6\r\n--     Python library      : /usr/lib/libpython2.7.so.1.0\r\n--     Python includes     : /usr/include/python2.7\r\n--     Python site-packages: lib/python2.7/dist-packages\r\n--   BUILD_CAFFE2_OPS      : True\r\n--   BUILD_SHARED_LIBS     : ON\r\n--   BUILD_TEST            : True\r\n--   INTERN_BUILD_MOBILE   : \r\n--   USE_ASAN              : False\r\n--   USE_CUDA              : False\r\n--   USE_ROCM              : False\r\n--   USE_EIGEN_FOR_BLAS    : \r\n--   USE_FBGEMM            : OFF\r\n--   USE_FFMPEG            : False\r\n--   USE_GFLAGS            : OFF\r\n--   USE_GLOG              : OFF\r\n--   USE_LEVELDB           : False\r\n--   USE_LITE_PROTO        : OFF\r\n--   USE_LMDB              : False\r\n--   USE_METAL             : OFF\r\n--   USE_MKL               : ON\r\n--   USE_MKLDNN            : ON\r\n--   USE_NCCL              : False\r\n--   USE_NNPACK            : True\r\n--   USE_NUMPY             : ON\r\n--   USE_OBSERVERS         : ON\r\n--   USE_OPENCL            : OFF\r\n--   USE_OPENCV            : False\r\n--   USE_OPENMP            : False\r\n--   USE_TBB               : True\r\n--   USE_PROF              : OFF\r\n--   USE_QNNPACK           : True\r\n--   USE_REDIS             : OFF\r\n--   USE_ROCKSDB           : OFF\r\n--   USE_ZMQ               : OFF\r\n--   USE_DISTRIBUTED       : True\r\n--     USE_MPI             : OFF\r\n--     USE_GLOO            : ON\r\n--     USE_GLOO_IBVERBS    : OFF\r\n--   NAMEDTENSOR_ENABLED   : False\r\n--   Public Dependencies  : Threads::Threads;caffe2::mkl;caffe2::mkldnn\r\n--   Private Dependencies : qnnpack;nnpack;cpuinfo;fp16;gloo;aten_op_header_gen;foxi_loader;rt;gcc_s;gcc;dl\r\n-- Configuring incomplete, errors occurred!\r\nSee also ""/torch_build/pytorch/build/CMakeFiles/CMakeOutput.log"".\r\nSee also ""/torch_build/pytorch/build/CMakeFiles/CMakeError.log"".\r\nTraceback (most recent call last):\r\n  File ""setup.py"", line 752, in <module>\r\n    build_deps()\r\n  File ""setup.py"", line 320, in build_deps\r\n    build_dir=\'build\')\r\n  File ""/torch_build/pytorch/tools/build_pytorch_libs.py"", line 67, in build_caffe2\r\n    rerun_cmake)\r\n  File ""/torch_build/pytorch/tools/setup_helpers/cmake.py"", line 272, in generate\r\n    self.run(args, env=my_env)\r\n  File ""/torch_build/pytorch/tools/setup_helpers/cmake.py"", line 100, in run\r\n    check_call(command, cwd=self._build_dir, env=env)\r\n  File ""/usr/lib/python2.7/subprocess.py"", line 540, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command \'[\'cmake\', \'-GNinja\', \'-DBLAS=MKL\', \'-DBUILDING_WITH_TORCH_LIBS=ON\', \'-DBUILD_BINARY=True\', \'-DBUILD_CAFFE2_OPS=True\', \'-DBUILD_PYTHON=True\', \'-DBUILD_SHARED_LIBS=ON\', \'-DBUILD_TEST=True\', \'-DCAFFE2_STATIC_LINK_CUDA=False\', \'-DCMAKE_BUILD_TYPE=Release\', \'-DCMAKE_CXX_FLAGS= \', \'-DCMAKE_C_FLAGS= \', \'-DCMAKE_EXE_LINKER_FLAGS=\', \'-DCMAKE_INSTALL_PREFIX=/torch_build/pytorch/torch\', \'-DCMAKE_PREFIX_PATH=/usr/lib/python2.7/dist-packages\', \'-DCMAKE_SHARED_LINKER_FLAGS=\', \'-DINSTALL_TEST=True\', \'-DNAMEDTENSOR_ENABLED=False\', \'-DNCCL_EXTERNAL=False\', \'-DNUMPY_INCLUDE_DIR=/usr/lib/python2.7/dist-packages/numpy/core/include\', \'-DONNX_ML=True\', \'-DONNX_NAMESPACE=onnx_torch\', \'-DPYTHON_EXECUTABLE=/usr/bin/python\', \'-DPYTHON_INCLUDE_DIR=/usr/include/python2.7\', \'-DPYTHON_LIBRARY=/usr/lib/libpython2.7.so.1.0\', \'-DTHD_SO_VERSION=1\', \'-DTORCH_BUILD_VERSION=1.2.0a0+1ffa9d3\', \'-DUSE_ASAN=False\', \'-DUSE_CUDA=False\', \'-DUSE_DISTRIBUTED=True\', \'-DUSE_FBGEMM=True\', \'-DUSE_FFMPEG=False\', \'-DUSE_LEVELDB=False\', \'-DUSE_LMDB=False\', \'-DUSE_MKLDNN=True\', \'-DUSE_NCCL=False\', \'-DUSE_NNPACK=True\', \'-DUSE_NUMPY=True\', \'-DUSE_OPENCV=False\', \'-DUSE_QNNPACK=True\', \'-DUSE_ROCM=False\', \'-DUSE_SYSTEM_EIGEN_INSTALL=OFF\', \'-DUSE_SYSTEM_NCCL=False\', \'-DUSE_TENSORRT=False\', \'-DUSE_OPENMP=False\', \'-DUSE_TBB=True\', \'-DINTEL_MKL_TBB=True\', \'-DMKLDNN_THREADING=TBB\', \'-DPARALLEL_BACKEND=NATIVE\', \'-DMKLDNN_ENABLE_CONCURRENT_EXEC=ON\', \'/torch_build/pytorch\']\' returned non-zero exit status 1\r\n', '\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n']",['python setup.py build'],0,0
150,pytorch,12210,closed,PackagesNotFoundError: unable to install torchvision in anaconda prompt on windows 10,"

returns the following:

",module: windows,"['Use `pip install torchvision` instead. It is not yet available in Anaconda Cloud.', '`-c soumith` should never be used. Where did you find instructions to use that?', ""@soumith probably in one of PyPI's instructions for older versions, [such as 0.1.8 here](https://pypi.org/project/torchvision/0.1.8/). There's also [a very prominent Anaconda Cloud link](https://anaconda.org/soumith/torchvision) (that was first in the search that got me here) which also gives out `-c soumith`."", 'conda install -c pytorch torchvision']",[],"['conda install torchvision -c soumith', 'PackagesNotFoundError: The following packages are not available from current channels: - torchvision']",0,0
151,pytorch,17699,closed,nn.Embedding broken for half,"## üêõ Bug

Using the following script will break with error 



<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:

1. Run the script above

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

No error
<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


 - PyTorch Version (e.g., 1.0): 1.0.1.post2
 - OS (e.g., Linux): Mac OSX 10.14.3
 - How you installed PyTorch (, , source): conda
 - Build command you used (if compiling from source):
 - Python version: 3.6
 - CUDA/cuDNN version:
 - GPU models and configuration:
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->
",,['There is very limited support for half datatype on CPU. The script above will work for both nn.Embedding and input on cuda device. '],"['\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nb = nn.Embedding(10, 20, 0)\r\nb.half()\r\nb(torch.randint(0, 10, size=(5, 10)))\r\n', '\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n']","['_th_index_select is not implemented for type torch.HalfTensor', 'conda', 'pip']",0,0
152,pytorch,6111,closed,Report bug script,"In our issue submission checklist, we collect a lot of information that can be gotten mechanically. We should have a simple way of collecting this info as a script and uploading it to GitHub.",,"['I can work on this. `torch.utils.bottleneck` already contains an environment analyzer:\r\n```\r\n--------------------------------------------------------------------------------\r\n  Environment Summary\r\n--------------------------------------------------------------------------------\r\nPyTorch 0.4.0a0+4c89349 DEBUG compiled w/ CUDA 8.0.44\r\nRunning with Python 2.7 and CUDA 8.0.44\r\n\r\n`pip list` truncated output:\r\ntorch (0.4.0a0+4c89349, /home/rzou/pytorch)\r\n```']",[],[],0,0
153,pytorch,598,closed,Problem with backward hook function,"Hi,

there is something strange in the  step (or maybe something I don't understand). If I define a Module that takes 3 inputs, the  has to be of size 3, right ? But this is not the case here (from the backward_hook point of view):



In that case, when I print grad_input throught the hook function, it is just composed of two elements... Could you tell me where am I wrong ? But  seem correctly computed

cc @ezyang @gchanan @zou3519 @SsnL @albanD @gqchen",high priority module: autograd module: docs triaged,"[""Ok, so the problem is that module hooks are actually registered on the last function that the module has created. In your case `x + y + z` is computed as `((x + y) + z)` so the hook is registered on that `(_ + z)` operation, and this is why you're getting only two grad inputs.\r\n\r\nWe'll definitely have to resolve this but it will need a large change in the autograd internals. However, right now @colesbury is rewriting them to make it possible to have multiple functions dispatched in parallel, and they would heavily conflict with his work. For now use only Variable hooks (or module hooks, but not on containers). Sorry!"", '@apaszke Is the refactor you referred to this one: https://github.com/pytorch/pytorch/pull/1016? Since that is merged, is this bug solvable now?', ""Yes. We can remove the on hold label, but I don't have any good solutions in mind"", '@apaszke Any progress or ideas on this issue, about how to solve. I may write a PR if required.', ""No progress on this. It's actually a pretty hard problem to solve properly. I think I'd be ok with disallowing registering hooks on modules that have submodules as a partial workaround."", '@apaszke  Any progress or ideas on this issue? I also encounter this problem we the output of forward function is the result of ```torch.chunk```. ', ""Not yet. It's really hard to solve, and not such a common pitfall. I think we might just disable the function for now."", 'So here is a copypaste from the duplicate I filed today.\r\n\r\nThe [pytorch documentation for `nn.Module.register_backward_hook`](http://pytorch.org/docs/master/nn.html#torch.nn.Module.register_backward_hook) says:\r\n\r\n> The grad_input and grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of grad_input in subsequent computation.\r\n\r\nThis is not quite accurate:\r\n- `grad_output` is indeed the gradient of the loss w.r.t. the layer output. So if you have a layer l and do, say, `y = l(x) ; loss = y.sum(); loss.backward()`, you get the gradient of `loss` w.r.t. `y`. So far so good.\r\n-  What is unexpected to most users is that `grad_input` are the inputs to the last operation in the layer. For linear layers, this is fairly complete, as the last op is `torch.addmm` multiplying the input with the weight and adding the bias. For other layers (e.g. do a Sequential, it‚Äôll be the last op of the last layer, the inputs not even remotely related to the sequential layer‚Äôs inputs). You can see what will be used by looking at `y.grad_fn`.\r\n\r\nThis may be confusing to users, e.g. https://discuss.pytorch.org/t/exact-meaning-of-grad-input-and-grad-output/14186\r\n\r\nSo improving the docs is one thing, but should we make the hook actually live up to the description?\r\n\r\nI see three strategies:\r\n- Documentation only.\r\n- The straightforward way of providing input gradients: collect the grad_ins with variable hooks and call the module hook when we have all of them. We loose the ability to return a different gradient.\r\n- The somewhat convoluted way: If the module has hooks, wrap the module forward in a autograd function - similar to checkpointing. The the variable hook for the output would do the right thing.\r\n', ""I think we should just remove this functionality for now (raise an error when `register_backward_hook` is called). It never worked correctly and is really hard to implement. I don't think the checkpointing way is good in this case, because it can cause certain backward hooks to trigger multiple times, and that's generally not what you want. It also doesn't work with `.grad`."", '@apaszke Remove this functionality from all modules or only from Container type modules? If entirely removed, how should we obtain, for example, gradient norm?', 'I encountered the same issue. Is there any alternative to replace the gradient of a *sequence* of operations? Similarly to [custom_gradient](https://www.tensorflow.org/api_docs/python/tf/custom_gradient) in Tensorflow 1.8?\r\n\r\nWhat I am trying to do is override the gradient of an entire layer at once. The use-case is to implement some attribution methods as in https://github.com/marcoancona/DeepExplain, by modifying the gradient of the output with respect to the input features.', 'Could this problem be properly noted in the doc before some solution?  Since it has existed so long and misleads many people including me.', ""> Could this problem be properly noted in the doc before some solution? Since it has existed so long and misleads many people including me.\r\n\r\nSure, let's do that for now."", 'The corresponding pull requested https://github.com/pytorch/pytorch/pull/1016 mentioned by @apaszke has been marked as merged.\r\n\r\n@ludc - is this issue resolved?', ""No, this has never been fixed, and I think it's a wontfix for the foreseeable future."", ""I must say I did like @albanD 's approach in #12573 . And think it might be worthwhile even if it has limitations (as long as hooks not being called is the only harm), one might warn if by the end of the forward the inputs have been inplace-modified..\r\n\r\nOf course, my ideal PyTorch has inplace hooks for #7313, which might help with the issues that killed #12573.\r\n\r\n"", '@t-vi This approach lead to other issues with inplace ops in the autograd.\r\nI will have time in the next couple of month to work on them properly and hopefully fix them all.\r\n\r\nA note has been added in the documentation though explaining that they are mostly broken at the moment.', 'added @albanD to assignee list based on the above conversation.', 'Can someone summarize why this turned out to be hard to implement?', ""Hi,\r\n\r\nAs many things touching the autograd engine, this is hard because of inplace operations!\r\nWe need a way to attach to all the differentiable inputs/outputs and get all the gradients for the input at once.\r\n\r\nThe current approach simply hooks onto the last Node associated with the first output. So for complex modules, it misses all intermediary results and is fairly broken.\r\n\r\nThe new approach I tried to introduce above was to add a specific Node into the graph that will be inserted at the input and output and will be able to call the callback with the saved data.\r\nThe current blocking (but that will be resolved soon ü§û ) problem is that the autograd engine does not really allow for these NoOp Nodes. It also does not allow to return views of the input if there is more than one output (because in the case where an inplace operation is performed on this output, the graph need to be rewritten and we can only do that if a Node has a single output).\r\n\r\nNot sure if it's really clear. Does that help?"", ""Yes, makes sense. Restricting to single-output modules seems fine since that's the most common use-case.\r\n\r\nPS, the motivation for module backward hooks is that the user code becomes simpler. Example -- implement https://arxiv.org/abs/1510.01799 . Currently this requires a module forward hook to save the activations, then backward hook to combine saved activations with backprops. Module backward hook would let you do this in a single function"", 'Hi all,\r\n\r\nis there any fix intended other than forbidding? I ask because there are people who implement ""non-pure-gradient"" backward attributions (e.g. LRP) via custom stuff in backward hooks in the context of explaining ML predictions. For all those people it would be helpful, while forbidding backward hooks would cause a bigger problem as far as I can see.\r\n\r\n\r\nI think I stumbled over something related when implementing a backward attribution and using it on GPU:\r\nnn.Conv2d with bias=True\r\n\r\nin short: on the CPU i see the grad inputs which i would expect (input feature map x, conv-weights, conv-bias)\r\non the GPU i see 2 input shapes to what looks like a bias addition. \r\n\r\ncode attached below. apologies for code style like from public restrooms. in this case it creates an inconsistency CPU-GPU\r\n\r\n#code starts here\r\n\r\nfrom __future__ import print_function, division\r\nimport torch\r\nimport torch.nn as nn\r\nimport numpy as np\r\n\r\ndef add_ahook(model):\r\n    for module in model.modules():\r\n        module.register_backward_hook(testbackwardhook)\r\n\r\ndef testbackwardhook(module, relevance_input, relevance_output):\r\n\r\n    print( \'types of rel inp, relout\', type(relevance_input), type(relevance_output), len(relevance_input), len(relevance_output))\r\n\r\n    print( \'out[0].shape\', relevance_output[0].shape)\r\n    for k in range(len(relevance_input)):\r\n      if hasattr(relevance_input[k],\'shape\'):\r\n        print( k, \'relevance_input[\'+str(k)+\'].shape\', relevance_input[k].shape)\r\n      else:\r\n        print( k, \'relevance_input[\'+str(k)+\']\', relevance_input[k])\r\n\r\ndef runstuff():\r\n  c=nn.Conv2d(in_channels=5, out_channels=9, kernel_size=3, stride=1,\r\n                     padding=1, groups=1, bias=True, dilation=1)\r\n\r\n  add_ahook(c)\r\n\r\n  x=torch.ones((1,5,7,7))\r\n  x.requires_grad=True  \r\n\r\n  device2=torch.device(\'cuda:0\')\r\n  #device2=torch.device(\'cpu\')\r\n\r\n  x=x.to(device2)\r\n  c=c.to(device2)\r\n\r\n  z=c(x)\r\n  print(\'forwardoutput\',z.shape)\r\n  torch.sum(z).backward(retain_graph=True)  \r\n\r\nif __name__==\'__main__\':\r\n  runstuff()\r\n\r\n\r\n\r\n']","['import torch\r\nfrom torch.autograd import Variable\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\ndef bh(m,go,gi):\r\n    print(""Grad Input"")\r\n    print(go)\r\n    print(""Grad Output"")\r\n    print(gi)\r\n\r\nclass M(nn.Module):\r\n    def __init__(self):\r\n        super(M,self).__init__()\r\n        self.register_backward_hook(bh)\r\n\r\n\r\n    def forward(self,x,y,z):\r\n        return (x+y+z)\r\n\r\nx=Variable(torch.randn(1,5),requires_grad=True)\r\ny=Variable(torch.randn(1,5),requires_grad=True)\r\nz=Variable(torch.randn(1,5),requires_grad=True)\r\n\r\ncriterion=nn.MSELoss()\r\nmod=M()\r\nout=mod(x,y,z)\r\nloss=criterion(out,Variable(torch.randn(1,5)))\r\nloss.backward()']","['backward', 'grad_input', 'x.grad, y.grad and z.grad']",0,0
154,pytorch,27703,closed,FAIL: test_torch (__main__.TestDocCoverage),"## üêõ Bug

after our v1.3.0 doc changes i am seeing a test failure

FAIL: test_torch (__main__.TestDocCoverage)


## To Reproduce

Steps to reproduce the behavior:

1. git checkout upstream master
2. python ./test/test_docs_coverage.py

AssertionError: Items in the second set but not the first:
'real'
'imag'
'conj'
'angle' : 
The lists of tensor methods documented in tensors.rst and in python are
different. Did you forget to add a new thing to tensors.rst, or whitelist
things you don't want to document?

======================================================================
FAIL: test_torch (__main__.TestDocCoverage)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""./test/test_docs_coverage.py"", line 73, in test_torch
    don't want to document?''')
AssertionError: Items in the first set but not the second:
'promote_types'
'result_type'
'can_cast'
Items in the second set but not the first:
'quantize_per_tensor'
'quantize_per_channel' : 
The lists of functions documented in torch.rst and in python are different.
Did you forget to add a new thing to torch.rst, or whitelist things you
don't want to document?

----------------------------------------------------------------------
Ran 2 tests in 0.003s

FAILED (failures=2)

## Expected behavior


git checkout pr-fix-doc-test-failure

gottbrath@ubuntu:~/pytorch-for-doc$ python ./test/test_docs_coverage.py 
..
----------------------------------------------------------------------
Ran 2 tests in 0.003s

OK




cc @ezyang",module: doc infra module: docs module: tests triaged,"['pr is https://github.com/pytorch/pytorch/pull/27684\r\n', 'ok .. that PR might not quite do the trick. our index.rtf PR got reverted.. let me go see if I can solve that and this at the same time. ', ""@gottbrath I'm flagging this as triaged since I assume you're onto this."", 'yep.. i am on it!', 'We reverted the diff', 'This does not seem like an issue (the docs test is behaving as intended), so I am closing this. Please feel free to reopen if I misinterpreted anything.']",[],[],0,0
155,pytorch,8369,closed,Something wrong with torch 0.4.0 in rnn?,"If you have a question or would like help and support, please ask at our
[forums](https://discuss.pytorch.org/).

If you are submitting a feature request, please preface the title with [feature request].
If you are submitting a bug report, please fill in the following details.

## Issue description
I got something wrong with torch 0.4.0 in rnn. it's like there are errors about the RAM.


## Code example

Please try to provide a minimal example to repro the bug.
Error messages and stack traces are also helpful.

/home/python3/bin/python3.6': double free or corruption (fasttop): 0x00007fe73000aa50 ***
======= Backtrace: =========
/lib64/libc.so.6(+0x7c619)[0x7fe86b938619]
/usr/local/nvidia/lib64/libcuda.so.1(+0x1dedcf)[0x7fe860e4ddcf]
/usr/local/nvidia/lib64/libcuda.so.1(+0xf6ebb)[0x7fe860d65ebb]
/usr/local/nvidia/lib64/libcuda.so.1(cuStreamCreate+0x5b)[0x7fe860e9672b]
/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(+0x3258786)[0x7fe801f1c786]
/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(+0x328dec4)[0x7fe801f51ec4]
/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(_ZN17RNNBackwardFilterIfffE4initEP12cudnnContextP14cudnnRNNStructi11PerfOptions+0x3b0)[0x7fe807792d10]
/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(cudnnRNNBackwardWeights+0xed1)[0x7fe807791f01]
/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(_ZN2at6native26_cudnn_rnn_backward_weightERKNS_6TensorENS_8ArrayRefIS1_EElS3_S3_S3_S3_lllbdbbNS4_IlEES3_S3_+0xa7e)[0x7fe8000a0e5e]
/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(_ZN2at6native19_cudnn_rnn_backwardERKNS_6TensorENS_8ArrayRefIS1_EElS3_S3_S3_S3_S3_S3_S3_lllbdbbNS4_IlEES3_S3_St5arrayIbLm4EE+0x22f)[0x7fe8000a388f]
/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(_ZNK2at4Type19_cudnn_rnn_backwardERKNS_6TensorENS_8ArrayRefIS1_EElS3_S3_S3_S3_S3_S3_S3_lllbdbbNS4_IlEES3_S3_St5arrayIbLm4EE+0x9f)[0x7fe80030061f]
/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZNK5torch8autograd12VariableType19_cudnn_rnn_backwardERKN2at6TensorENS2_8ArrayRefIS3_EElS5_S5_S5_S5_S5_S5_S5_lllbdbbNS6_IlEES5_S5_St5arrayIbLm4EE+0x43b)[0x7fe824eead1b]
/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZN5torch8autograd9generated16CudnnRnnBackward5applyERKSt6vectorINS0_8VariableESaIS4_EE+0x6c6)[0x7fe824fb7f56]
/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZN5torch8autograd6Engine17evaluate_functionERNS0_12FunctionTaskE+0x3e2)[0x7fe824e46b62]
/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZN5torch8autograd6Engine11thread_mainEPNS0_9GraphTaskE+0xe5)[0x7fe824e47b75]
/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZN5torch8autograd6Engine11thread_initEi+0x5e)[0x7fe824e4402e]
/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZN5torch8autograd6python12PythonEngine11thread_initEi+0x2a)[0x7fe824e71a8a]
/lib64/libstdc++.so.6(+0xb52b0)[0x7fe859a602b0]
/lib64/libpthread.so.0(+0x7e25)[0x7fe86c38fe25]
/lib64/libc.so.6(clone+0x6d)[0x7fe86b9b434d]
======= Memory map: ========

ollecting environment information...
PyTorch version: 0.4.0
Is debug build: No
CUDA used to build PyTorch: 8.0.61

OS: CentOS Linux 7 (Core)
GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)
CMake version: version 2.8.12.2

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 8.0.61
GPU models and configuration: 
GPU 0: Tesla P40
GPU 1: Tesla P40
GPU 2: Tesla P40
GPU 3: Tesla P40

Nvidia driver version: 390.30
cuDNN version: Probably one of the following:
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.6.0.21
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a

Versions of relevant libraries:
[pip3] numpy (1.14.2)
[pip3] torch (0.4.0)
[pip3] torchvision (0.2.1)
[conda] Could not collect
`

 by the way, The dataset size is about 300+G. 

Is the cuDNN error or something wrong with pytorch?
",awaiting response (this tag is deprecated) high priority,"['I just change the code by simple FC, no error show up', 'Could you post a full and runnable (and preferably minimal) script please?', '@SsnL Sorry. I can post the full script .but the script can not be run without data file . Do you need the pure script without data file?', 'A full script is better than no script :)\r\nAs for the data, @shuxiaobo, could you try putting in some fake data (like `torch.randn(*sizes)` ) and seeing if the memory corruption still happens?', ""Sorry , I changed my code to another server, the error didn't show up for several days. thanks"", 'I get the the same error when using RNN in training ', '@doudoubilly do you by any change have a way we can reproduce this?', ""@soumith I just train a small LSTM. For several epochs. Error below occur.\r\n\r\n*** Error in `python': double free or corruption (fasttop): 0x00007ff260038490 *** \r\n======= Backtrace: =========\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7ff37596d7e5]\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x8037a)[0x7ff37597637a]\r\n/lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7ff37597a53c]\r\n/usr/lib64/nvidia/libcuda.so.1(+0x1d1453)[0x7ff3705e6453]\r\n/usr/lib64/nvidia/libcuda.so.1(+0xeee02)[0x7ff370503e02]\r\n/usr/lib64/nvidia/libcuda.so.1(+0xeeeb0)[0x7ff370503eb0]\r\n/usr/lib64/nvidia/libcuda.so.1(cuStreamCreateWithPriority+0x61)[0x7ff37062b831]\r\n/home/abc/Envs/pt-py3/lib/python3.5/site-packages/torch/lib/libATen.so(+0x2fdb482)[0x7ff2d625d482]\r\n/home/abc/Envs/pt-py3/lib/python3.5/site-packages/torch/lib/libATen.so(+0x2fdb69e)[0x7ff2d625d69e]\r\n/home/abc/Envs/pt-py3/lib/python3.5/site-packages/torch/lib/libATen.so(+0x30095e7)[0x7ff2d628b5e7]\r\n/home/abc/Envs/pt-py3/lib/python3.5/site-packages/torch/lib/libATen.so(_ZN17RNNBackwardFilterIfffE4initEP12cudnnContextP14cudnnRNNStructi11PerfOptions+0x3b0)[0x7ff2dbce0e70]\r\n/home/abc/Envs/pt-py3/lib/python3.5/site-packages/torch/lib/libATen.so(cudnnRNNBackwardWeights+0xed1)[0x7ff2dbce0061]\r\n/home/abc/Envs/pt-py3/lib/python3.5/site-packages/torch/lib/libATen.so(_ZN2at6native26_cudnn_rnn_backward_weightERKNS_6TensorENS_8ArrayRefIS1_EElS3_S3_S3_S3_lllbdbbNS4_IlEES3_S3_+0xa6f)[0x7ff2d45842cf]\r\n/home/abc/Envs/pt-py3/lib/python3.5/site-packages/torch/lib/libATen.so(_ZN2at6native19_cudnn_rnn_backwardERKNS_6TensorENS_8ArrayRefIS1_EElS3_S3_S3_S3_S3_S3_S3_lllbdbbNS4_IlEES3_S3_St5arrayIbLm4EE+0x22f)[0x7ff2d4586c4f]\r\n/home/abc/Envs/pt-py3/lib/python3.5/site-packages/torch/lib/libATen.so(_ZNK2at4Type19_cudnn_rnn_backwardERKNS_6TensorENS_8ArrayRefIS1_EElS3_S3_S3_S3_S3_S3_S3_lllbdbbNS4_IlEES3_S3_St5arrayIbLm4EE+0x9f)[0x7ff2d47e3a0f]\r\n/home/abc/Envs/pt-py3/lib/python3.5/site-packages/torch/_C.cpython-35m-x86_64-linux-gnu.so(_ZNK5torch8autograd12VariableType19_cudnn_rnn_backwardERKN2at6TensorENS2_8ArrayRefIS3_EElS5_S5_S5_S5_S5_S5_S5_lllbdbbNS6_IlEES5_S5_St5arrayIbLm4EE+0x43b)[0x7ff3005e3cbb]\r\n/home/abc/Envs/pt-py3/lib/python3.5/site-packages/torch/_C.cpython-35m-x86_64-linux-gnu.so(_ZN5torch8autograd9generated16CudnnRnnBackward5applyERKSt6vectorINS0_8VariableESaIS4_EE+0x6c6)[0x7ff3006b0ef6]\r\n/home/abc/Envs/pt-py3/lib/python3.5/site-packages/torch/_C.cpython-35m-x86_64-linux-gnu.so(_ZN5torch8autograd6Engine17evaluate_functionERNS0_12FunctionTaskE+0x3e2)[0x7ff30053fb02]\r\n/home/abc/Envs/pt-py3/lib/python3.5/site-packages/torch/_C.cpython-35m-x86_64-linux-gnu.so(_ZN5torch8autograd6Engine11thread_mainEPNS0_9GraphTaskE+0xe5)[0x7ff300540b15]\r\n/home/abc/Envs/pt-py3/lib/python3.5/site-packages/torch/_C.cpython-35m-x86_64-linux-gnu.so(_ZN5torch8autograd6Engine11thread_initEi+0x5e)[0x7ff30053cfce]\r\n/home/abc/Envs/pt-py3/lib/python3.5/site-packages/torch/_C.cpython-35m-x86_64-linux-gnu.so(_ZN5torch8autograd6python12PythonEngine11thread_initEi+0x2a)[0x7ff30056aa2a]\r\n/usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb8c80)[0x7ff36f420c80]\r\n/lib/x86_64-linux-gnu/libpthread.so.0(+0x76ba)[0x7ff375cc76ba]\r\n/lib/x86_64-linux-gnu/libc.so.6(clone+0x6d)[0x7ff3759fd41d]\r\n"", '@doudoubilly \r\n`sudo apt-get install google-perftools\r\nexport LD_PRELOAD=""/usr/lib/libtcmalloc.so.4"" \r\nOR\r\nsudo apt-get install libtcmalloc-minimal4\r\nexport LD_PRELOAD=""/usr/lib/libtcmalloc_minimal.so.4""`\r\ntry it if needed']","['python\r\n        def __init__(self, emb_ranges, x3_dim, x12_dim = 20, cell_size = 128,\r\n                 num_layers = 1, lr = 0.0002):\r\n        super(SelfComputeModel, self).__init__()\r\n        self.num_layers = num_layers\r\n        self.cell_size = cell_size\r\n\r\n        self.embeddings = nn.ModuleList()\r\n        for i in emb_ranges:\r\n            self.embeddings.append(nn.Embedding(i, x12_dim))\r\n\r\n        self.feat_dim = x12_dim * len(emb_ranges) + x3_dim\r\n        self.x3_dim = x3_dim\r\n        self.input_fc = nn.Linear(self.feat_dim, cell_size)\r\n        self.rnn = nn.LSTM(cell_size, cell_size, num_layers = num_layers)\r\n        self.output_fc = nn.Linear(cell_size, 1)\r\n\r\n    def forward(self, x_12, x_3, mask, label, is_h):\r\n        x_12, x_3, mask, label = self.numpy_to_torch(x_12, x_3, mask, label)\r\n        # Embedding\r\n        x12_feat = []\r\n        for i in range(x_12.shape[2]):\r\n            x12_feat.append(self.embeddings[i](x_12[:, :, i]))\r\n        feat = torch.cat(x12_feat + [x_3], 2)\r\n\r\n        T, bs, _ = feat.size()\r\n\r\n        feat = feat.resize(T * bs, self.feat_dim)\r\n        feat = F.relu(self.input_fc(feat))\r\n\r\n        h0 = Variable(torch.zeros(self.num_layers, bs, self.cell_size)).cuda()\r\n        c0 = Variable(torch.zeros(self.num_layers, bs, self.cell_size)).cuda()\r\n        feat = feat.resize(T, bs, self.cell_size)\r\n        outputs, _ = self.rnn(feat, [h0, c0])\r\n\r\n        outputs = outputs.resize(T * bs, self.cell_size)\r\n        pred = F.relu(self.output_fc(outputs)).squeeze(1)\r\n        pred = pred.resize(T, bs)\r\n\r\n        pred = (pred * mask).sum(0).squeeze(0)\r\n\r\n        return pred, FloatTensor(label)\r\n\r\n    def numpy_to_torch(self, x_12, x_3, mask, is_h, label = None):\r\n        tensors = []\r\n        x_12 = x_12.transpose([1, 0, 2])\r\n        tensors.append(Variable(torch.LongTensor(x_12)).cuda())\r\n        x_3 = x_3.transpose([1, 0, 2])\r\n        tensors.append(Variable(torch.FloatTensor(x_3)).cuda())\r\n        mask = mask.transpose()\r\n        tensors.append(Variable(torch.FloatTensor(mask)).cuda())\r\n        if label is not None:\r\n            tensors.append(Variable(torch.FloatTensor(label)).cuda())\r\n        tensors.append(Variable(torch.FloatTensor(is_h)).cuda())\r\n        return tensors\r\n', '\r\n## System Info\r\nPlease copy and paste the output from our\r\n\r\n']","['', 'bash\r\n*** Error in ', '']",0,0
156,pytorch,29246,closed,Replacing convolutional blocks causes exporting to Onnx to fail,"## üêõ Bug

We are trying to export a model created in PyTorch to Onnx. The model is created with function:

and then, one convolutional block of that model is swapped with a different kind of block, like so:

When we try to export this updated model, like so:

an error occurs -  for some reason, PyTorch fails to export the model, that otherwise is fully functional.

If the blocks are not swapped, the network is exported successfully.

## To Reproduce

You can get the code from a temporary repository I've created for this issue:


Run  file, you will get the following error:




## Expected behavior

We expect to have our model exported, even if it has its convolutional blocks swapped with another type of block. Ideally, we would want to swap all the blocks of the model, not just the first one, as in the example given.

## Environment

 - PyTorch Version: 1.2.0
 - OS: Ubuntu 19.04
 - How you installed PyTorch: pip3
 - Python version: 3.7.3
 - CUDA/cuDNN version: No Cuda
 - GPU models and configuration: No


cc @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof",module: onnx triaged,[],"['\r\nmodel = WideResNet(40, 2, Conv, BasicBlock)\r\n', '\r\nmodel = update_block(block_number, model, block_type)\r\n', '\r\ntorch.onnx.export(model, torch_input, save_name)\r\n', '\r\nhttps://github.com/kasparas-ban/Exporting-models-to-Onnx.git\r\n', '\r\nTraceback (most recent call last):\r\n  File ""export_model.py"", line 42, in <module>\r\n    torch.onnx.export(model, torch_input, save_name)   # <- GIVES ERROR\r\n  File ""/home/kasparas/.local/lib/python3.7/site-packages/torch/onnx/__init__.py"", line 132, in export\r\n    strip_doc_string, dynamic_axes)\r\n  File ""/home/kasparas/.local/lib/python3.7/site-packages/torch/onnx/utils.py"", line 64, in export\r\n    example_outputs=example_outputs, strip_doc_string=strip_doc_string, dynamic_axes=dynamic_axes)\r\n  File ""/home/kasparas/.local/lib/python3.7/site-packages/torch/onnx/utils.py"", line 329, in _export\r\n    _retain_param_name, do_constant_folding)\r\n  File ""/home/kasparas/.local/lib/python3.7/site-packages/torch/onnx/utils.py"", line 213, in _model_to_graph\r\n    graph, torch_out = _trace_and_get_graph_from_model(model, args, training)\r\n  File ""/home/kasparas/.local/lib/python3.7/site-packages/torch/onnx/utils.py"", line 174, in _trace_and_get_graph_from_model\r\n    raise RuntimeError(""state_dict changed after running the tracer; ""\r\nRuntimeError: state_dict changed after running the tracer; something weird is happening in your model!\r\n']",['export_model.py'],0,0
157,pytorch,2126,closed,when i changed thetorchvision.utils source code long to int,how do i do this,,[],[],[],0,0
158,pytorch,18671,closed,[jit] crash in unpickler code,"See this test result: https://fburl.com/testinfra/6yuycydh

",oncall: jit,"[""to reproduce, check out D14581129 in fbcode and run:\r\n```\r\nbuck test @mode/dev caffe2/test:jit -- 'test_cumprod_zeros_dim2_neg0 \\(test_jit\\.TestJitGeneratedAutograd\\)'\r\n```\r\n""]","[""\r\ntest_cumprod_zeros_dim2_neg0 (test_jit.TestJitGeneratedAutograd) ... ../fbcode/third-party-buck/gcc-5-glibc-2.23/build/libgcc/include/c++/5.5.0/bits/stl_iterator.h:810:45: runtime error: pointer index expression with base 0x000000000000 overflowed to 0xfffffffffffffff0\r\n    #0 0x7fdca4075dd8 in __gnu_cxx::__normal_iterator<c10::IValue*, std::vector<c10::IValue, std::allocator<c10::IValue> > >::operator-(long) const ../fbcode/third-party-buck/gcc-5-glibc-2.23/build/libgcc/include/c++/5.5.0/bits/stl_iterator.h:810\r\n    #1 0x7fdca483d2e4 in std::vector<c10::IValue, std::allocator<c10::IValue> >::back() ../fbcode/third-party-buck/gcc-5-glibc-2.23/build/libgcc/include/c++/5.5.0/bits/stl_vector.h:882\r\n    #2 0x7fdca4e9027b in torch::jit::Unpickler::readInstruction() caffe2/torch/csrc/jit/pickler.cpp:342\r\n    #3 0x7fdca4e8db77 in torch::jit::Unpickler::run() caffe2/torch/csrc/jit/pickler.cpp:324\r\n    #4 0x7fdca4e8d0b2 in torch::jit::Unpickler::parse_ivalue_list() caffe2/torch/csrc/jit/pickler.cpp:294\r\n    #5 0x7fdca4af3128 in torch::jit::(anonymous namespace)::ScriptModuleDeserializer::loadAttributeTable() caffe2/torch/csrc/jit/import.cpp:157\r\n    #6 0x7fdca4aeee51 in torch::jit::(anonymous namespace)::ScriptModuleDeserializer::deserialize(std::function<std::shared_ptr<torch::jit::script::Module> (std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&)>, c10::optional<c10::Device>, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&) caffe2/torch/csrc/jit/import.cpp:135\r\n    #7 0x7fdca4aed3c0 in torch::jit::import_ir_module(std::function<std::shared_ptr<torch::jit::script::Module> (std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&)>, std::istream&, c10::optional<c10::Device>, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&) caffe2/torch/csrc/jit/import.cpp:293\r\n    #8 0x7fdca8bd94c7 in torch::jit::script::initJitScriptBindings(_object*)::$_35::operator()(std::function<std::shared_ptr<torch::jit::script::Module> (std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&)>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, pybind11::object, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&) const caffe2/torch/csrc/jit/script/init.cpp:1080\r\n    #9 0x7fdca8bd8d04 in void pybind11::detail::argument_loader<std::function<std::shared_ptr<torch::jit::script::Module> (std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&)>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, pybind11::object, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&>::call_impl<void, torch::jit::script::initJitScriptBindings(_object*)::$_35&, 0ul, 1ul, 2ul, 3ul, pybind11::detail::void_type>(torch::jit::script::initJitScriptBindings(_object*)::$_35&&&, std::integer_sequence<unsigned long, 0ul, 1ul, 2ul, 3ul>, pybind11::detail::void_type&&) third-party-buck/gcc-5-glibc-2.23/build/pybind11/889256a/include/pybind11/cast.h:1860\r\n    #10 0x7fdca8bd8acc in std::enable_if<std::is_void<void>::value, pybind11::detail::void_type>::type pybind11::detail::argument_loader<std::function<std::shared_ptr<torch::jit::script::Module> (std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&)>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, pybind11::object, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&>::call<void, pybind11::detail::void_type, torch::jit::script::initJitScriptBindings(_object*)::$_35&>(torch::jit::script::initJitScriptBindings(_object*)::$_35&&&) && third-party-buck/gcc-5-glibc-2.23/build/pybind11/889256a/include/pybind11/cast.h:1842\r\n    #11 0x7fdca8bd86dd in void pybind11::cpp_function::initialize<torch::jit::script::initJitScriptBindings(_object*)::$_35, void, std::function<std::shared_ptr<torch::jit::script::Module> (std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&)>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, pybind11::object, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&, pybind11::name, pybind11::scope, pybind11::sibling>(torch::jit::script::initJitScriptBindings(_object*)::$_35&&, void (*)(std::function<std::shared_ptr<torch::jit::script::Module> (std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&)>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, pybind11::object, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::operator()(pybind11::detail::function_call&) const third-party-buck/gcc-5-glibc-2.23/build/pybind11/889256a/include/pybind11/pybind11.h:154\r\n    #12 0x7fdca8bd82f8 in void pybind11::cpp_function::initialize<torch::jit::script::initJitScriptBindings(_object*)::$_35, void, std::function<std::shared_ptr<torch::jit::script::Module> (std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&)>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, pybind11::object, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&, pybind11::name, pybind11::scope, pybind11::sibling>(torch::jit::script::initJitScriptBindings(_object*)::$_35&&, void (*)(std::function<std::shared_ptr<torch::jit::script::Module> (std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&)>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, pybind11::object, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::__invoke(pybind11::detail::function_call&) third-party-buck/gcc-5-glibc-2.23/build/pybind11/889256a/include/pybind11/pybind11.h:131\r\n""]",[],0,0
159,pytorch,15567,closed,"Improve error messages for pack_padded_sequence(..., enforced_sorted=True)",Right now it errors out saying that sequences must be sorted. Users may not be aware that they can pass in  to the function to avoid the sorting requirement if they do not intend to do ONNX export; it would be great if the error message suggested that instead.,todo,[],[],['enforce_sorted=False'],0,0
160,pytorch,25515,closed,ONNX export failed on ATen operator upsample_bilinear2d,"## üêõ Bug

When I use torch.onnx.export , I met opset support problem:


## To Reproduce

Steps to reproduce the behavior:

1. pytorch inference file
2. change the output to onnx.export

## Environment

 - PyTorch Version (e.g., 1.0): 1.2
 - OS (e.g., Linux): Centos
 - How you installed PyTorch (, , source): Conda
 - Build command you used (if compiling from source): 
 - Python version: 3.7.3
 - CUDA/cuDNN version: 10/7.6
 - Any other relevant information: pytorch model is trained by pytorch 1.0.0
",module: onnx triaged,"['Maybe a duplicate of #22906', 'Same']","['\r\n/export/liuxiao/local/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py:562: UserWarning: ONNX export failed on ATen operator upsample_bilinear2d because torch.onnx.symbolic_opset9.upsample_bilinear2d does not exist\r\n  .format(op_name, opset_version, op_name))\r\nTraceback (most recent call last):\r\n  File ""convertPytorch2ONNX.py"", line 54, in <module>\r\n    torch_out = torch.onnx.export(senetModel, input_var, sys.argv[4], export_params=True, verbose=True, input_names=input_names, output_names=output_names)\r\n  File ""/export/liuxiao/local/anaconda3/lib/python3.7/site-packages/torch/onnx/__init__.py"", line 132, in export\r\n    strip_doc_string, dynamic_axes)\r\n  File ""/export/liuxiao/local/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py"", line 64, in export\r\n    example_outputs=example_outputs, strip_doc_string=strip_doc_string, dynamic_axes=dynamic_axes)\r\n  File ""/export/liuxiao/local/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py"", line 329, in _export\r\n    _retain_param_name, do_constant_folding)\r\n  File ""/export/liuxiao/local/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py"", line 225, in _model_to_graph\r\n    _disable_torch_constant_prop=_disable_torch_constant_prop)\r\n  File ""/export/liuxiao/local/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py"", line 127, in _optimize_graph\r\n    graph = torch._C._jit_pass_onnx(graph, operator_export_type)\r\n  File ""/export/liuxiao/local/anaconda3/lib/python3.7/site-packages/torch/onnx/__init__.py"", line 163, in _run_symbolic_function\r\n    return utils._run_symbolic_function(*args, **kwargs)\r\n  File ""/export/liuxiao/local/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py"", line 563, in _run_symbolic_function\r\n    op_fn = sym_registry.get_registered_op(op_name, \'\', opset_version)\r\n  File ""/export/liuxiao/local/anaconda3/lib/python3.7/site-packages/torch/onnx/symbolic_registry.py"", line 91, in get_registered_op\r\n    return _registry[(domain, version)][opname]\r\nKeyError: \'upsample_bilinear2d\'\r\n']","['conda', 'pip']",0,0
161,pytorch,16392,closed,squeeze operation edge case ,"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->

I found an inconsistent behavior with 

> tensor.squeeze()

, it could be that it was intentional and if so, I can't understand the reason for it.

## To Reproduce

1 ) 



2) 



In the second example, I hoped to get


I'd be glad to know whether it was a bug, and if not, why.

Cheers!
Shiran",,"['This is well-explained in the docs:\r\n\r\n> Returns a tensor with all the dimensions of input of size 1 removed.\r\n\r\n> For example, if input is of shape: (A√ó1√óB√óC√ó1√óD)(A \\times 1 \\times B \\times C \\times 1 \\times D)(A√ó1√óB√óC√ó1√óD) then the out tensor will be of shape: (A√óB√óC√óD)(A \\times B \\times C \\times D)(A√óB√óC√óD).\r\n\r\nYour first example has size 2 x 1 which results in a tensor with size 2. The second example has size 1 x 1 which results in a scalar (no dimensions). I believe this is expected behavior.']",[],"['print(torch.tensor([[4], [5]]).squeeze())', 'tensor([4,5])', 'print(torch.tensor([[4]]).squeeze())', 'tensor(4)', 'tensor([4])']",0,0
162,pytorch,3981,closed, Errors when computing second order gradients on gpu,"Here is a piece of test code:



If the following conditions meet, the second order gradient cannot be computed:

1. you want to compute second order grad (not first order grad)
2. you network contains convolution (fc layer functions normally)
3. Computation on gpu (on cpu, everything was fine)

And the error is:

Add    won't fix anything.",,"['@pkuwjf what is the value of `print(torch.__version__)`?', '@soumith 0.2.0_3', ""@pkuwjf Master version pytorch + latest cuda/cudnn work fine.\r\nAlso, if I remembering correctly, on 0.2.0_3 you can disable cudnn via `torch.backends.cudnn.enabled = False` for second order grad with convolution to work, but it'll be quite slow. "", 'Thanks to developers!\r\nIn the 0.3.0, this bug has been fixed.']","[""\r\nimport math\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.autograd import Variable\r\nfrom torch.nn.parameter import Parameter\r\n\r\nclass ConvNet(nn.Module):\r\n    def __init__(self):\r\n        super(ConvNet, self).__init__()\r\n        self.conv1_weight = Parameter(torch.randn(10,1,3,3))\r\n        self.conv1_bias = Parameter(torch.randn(10))\r\n\r\n    def forward(self, x):\r\n        out = x\r\n        out = F.conv2d(out, self.conv1_weight, bias=self.conv1_bias)\r\n        return out\r\n\r\nif __name__ == '__main__':\r\n    from torch.autograd import grad\r\n    model = ConvNet()\r\n    model.cuda()\r\n    x = Variable(torch.randn(1,1,28,28)).cuda()\r\n    print(x.size())\r\n    y = model(x)\r\n    print(y.size())\r\n    loss = torch.mean(y.pow(2))\r\n    g = grad(loss, model.parameters(), create_graph=True, retain_graph=True)[0]\r\n    gg = grad(g[0,0,0,0], model.parameters(), retain_graph=True)[0] ## bug\r\n"", '\r\nRuntimeError: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.\r\n']",['.contiguous()'],0,0
163,pytorch,23393,closed,RuntimeError: set_storage is not allowed on Tensor created from .data or .detach(),"## üêõ Bug Here is the issue while trying to execute the this bunch of code. 

<!-- A clear and concise description of what the bug is. -->

## My code is below..



   
   
    
 = 




 - PyTorch Version (1.0):
 - Linux
 - pip3 install torchvision:
 - Build command you used (if compil:
 - Python version 3.7


",module: error checking triaged,"['Hi Imad,\r\nHere is a quick fix:\r\n1. Replace weight.data.set_ with weight.set_.\r\n2. Add with th.no_grad(): block.\r\n\r\nRevised version: \r\n\r\n```python\r\nwith th.no_grad():\r\n    model.weight.set_(((alice_model.weight.data + bob_model.weight.data) / 2).get())\r\n```\r\n\r\nThe line after the colon (:) needs to be indented.  \r\n\r\nDetails: https://discuss.pytorch.org/t/api-change-for-tensor-data-set-in-torch-nightly/33310/6\r\nHope this helps.\r\nAgata\r\n', 'cc: @gchanan should we improve the error message for release', 'Thank you, Soumith, for your revision. Yes, having a clear error message would help with debugging.', '@yf225 can we improve the error message here?\r\n\r\nMaybe something like:\r\n```\r\nset_ is not allowed on a Tensor created from .data or .detach().\r\nIf your intent is to change the metadata of a Tensor without autograd tracking the metadata\r\nchange, remove the .data / detach() call and wrap the change in a `with torch.no_grad():` block.  For example, change:\r\n    x.data.set_(y)\r\nto:\r\n    with torch.no_grad():\r\n         x.set_(y)\r\n```', 'One of my questions here would be if this is a safe operation. If you share memory between Tensors tracked by autograd in this way, can you run into race conditions during backward etc.?', 'the issue resolved with \r\nwith th.no_grad():\r\n    model.weight.set_(((alice_model.weight.data + bob_model.weight.data) / 2).get())', 'Thank you. This works with syft 0.2.9.']","[""\r\nbob = sy.VirtualWorker(hook, id='bob')\r\nalice = sy.VirtualWorker(hook, id='alice')\r\nsecure_worker = sy.VirtualWorker(hook, id='secure_worker')\r\n"", '\r\nbob.add_workers([alice, secure_worker])\r\nalice.add_workers([bob, secure_worker])\r\nsecure_worker.add_workers([alice, bob])\r\n', '\r\ndata = th.tensor([[1,1],[0,0],[0,1],[1,0.]], requires_grad = True)\r\ntarget = th.tensor([[0],[1],[1],[0.]], requires_grad = True)\r\nbob_data = data[0:2].send(bob)\r\nbob_target = data[0:2].send(bob)\r\nalice_data = data[2:0].send(alice)\r\nalice_target = data[2:0].send(alice)\r\nmodel = nn.Linear(2,1)\r\nbob_model = model.copy().send(bob)\r\nalice_model = model.copy().send(alice)\r\nbob_opt = optim.SGD(params=bob_model.parameters(), lr=0.1)\r\nalice_opt = optim.SGD(params=alice_model.parameters(), lr=0.1)\r\n\r\nbob_opt.zero_grad()\r\nbob_pred = bob_model(bob_data)\r\nbob_loss = ((bob_pred - bob_target)**2).sum()\r\nbob_loss.backward()\r\nbob_opt.step()\r\nbob_loss = bob_loss.get().data\r\n    \r\nalice_opt.zero_grad()\r\nalice_pred = alice_model(alice_data)\r\nalice_loss = ((alice_pred - alice_target)**2).sum()\r\nalice_loss.backward()\r\nalice_opt.step()\r\nalice_loss = alice_loss.get().data\r\n    \r\nalice_model.move(secure_worker)\r\nbob_model.move(secure_worker)\r\nsecure_worker._objects\r\n\r\n> {49257609311: Parameter containing:\r\n tensor([[-0.5138, -0.4376]], requires_grad=True),\r\n 44327728419: Parameter containing:\r\n tensor([-0.2540], requires_grad=True),\r\n 3967825735: Parameter containing:\r\n tensor([[0.3148, 0.3909]], requires_grad=True),\r\n 97077675381: Parameter containing:\r\n tensor([0.1789], requires_grad=True)}\r\n----> model.weight.data.set_(((alice_model.weight.data + bob_model.weight.data) /2).get())\r\n\r\n\r\nRuntimeError Traceback (most recent call last)\r\n<ipython-input-26-841b837bc662> in <module>\r\n----> 1 model.weight.data.set_(((alice_model.weight.data + bob_model.weight.data) /2).get())\r\n\r\nRuntimeError: set_storage is not allowed on Tensor created from .data or .detach()\r\n']","['import', 'syft', 'as', 'sy', 'import', 'torch', 'as', 'th', 'from', 'torch', 'import', 'nn,', 'optim', 'hook', 'sy.TorchHook(th)', '\r\n\r\n---------------------------------------------------------------------------\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n', '']",0,0
164,pytorch,3657,closed,No link to the previous version doc in master doc,"http://pytorch.org/docs/master/
The link to switch doc is dead.

http://pytorch.org/docs/0.2.0/
Old versions are still available and the link is ok there.",,"['thanks for the heads-up, not sure what happened there, will look.', 'fixed via https://github.com/pytorch/builder/commit/70fae11cb7baec08210135fd7c5b27fb3b156887']",[],[],0,0
165,pytorch,4403,closed,Pytorch docsÔºöRNN weight_ih_l[k]  shape error,In pytorch docsÔºåtorch.nn.RNN parts show that variable **weight_ih_l[k]**  has shape of input_size x hidden_sizeÔºåhowever it should be hidden_size x input_size.,,[],[],[],0,0
166,pytorch,4673,closed,GPU memory consumption with cudnn.enabled = False,"
According to this code, difference in used GPU memory  before and after convolution is  4809 MB - 1345 MB = 3.5 GB . This is too much since *inp* is just 536MB and *embed0* is 134 MB. From my experiments this issue is fixed if i delete nn.BatchNorm2d block.

What can be the reason for this strange behavior? 

PS: I turned off CUDNN to avoid another bug with ConvTranspose3d  [#4344]

Pyton2.7, PyTorch 0.3 (pip),  CUDA 8, CUDNN 7
",,"[""Convolutions need some extra scratch space, and our non-cuDNN CUDA convolutions aren't very efficient, but we don't plan to invest a lot of work into them, since cuDNN is usually going to be waaaaaay better.\r\n\r\nAlso, note that we're using GitHub issues for bug reports only, and all questions should be posted on [our forums](discuss.pytorch.org)."", 'Thank you for reply! I will use forum for questions']","[""\r\nimport torch as th \r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\nimport subprocess\r\nimport torch.backends.cudnn as cudnn\r\ncudnn.enabled = False\r\n\r\ndef get_gpu_memory_map():\r\n    \r\n    result = subprocess.check_output(\r\n        [\r\n            'nvidia-smi', '--query-gpu=memory.used',\r\n            '--format=csv,nounits,noheader'\r\n        ])\r\n\r\n    gpu_memory = [int(x) for x in result.strip().split('\\n')]\r\n    gpu_memory_map = dict(zip(range(len(gpu_memory)), gpu_memory))\r\n    \r\n    return gpu_memory_map\r\n\r\ndef conv2D_block(in_channels, out_channels, kernel_size=3, stride=1):\r\n    padding = kernel_size//2\r\n    return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\r\n                         nn.LeakyReLU(negative_slope=0.1, inplace=True),\r\n                         nn.BatchNorm2d(out_channels))\r\n\r\ndownsample0 = conv2D_block(in_channels=64, out_channels=64, kernel_size=5, stride=2)\r\ndownsample0.float().cuda()\r\ndownsample0.eval()\r\n\r\ninp = Variable(th.rand(1, 64, 3000/2, 3000/2).float().cuda(), volatile=True)\r\nprint 'before', get_gpu_memory_map()\r\nembed0 = downsample0(inp);  th.cuda.empty_cache() \r\nprint 'after downsample0', get_gpu_memory_map()   \r\n""]",[],0,0
167,pytorch,16646,closed,loss.backward() cause malloc: *** error for object 0x7fb849370210: incorrect checksum for freed object,"I build a net: resnet18 + ctc loss, when run loss.backward(), there will be a error:python(17267,0x700007df0000) malloc: *** error for object 0x7fb849370210: incorrect checksum for freed object
i not find any method to resolve it?

note: my training date size is not equal, this mean :the widht of image is variable, and the height of imgage is 32,    the label length is variable

========= my net.py is ===================
# coding=utf-8
import torch
import torch.nn as nn
import torch.nn.functional as F


# Áõ¥Êé•ÂÆö‰πâÂèåÂ±ÇLSTM
class BiLSTM(nn.Module):
    def __init__(self, num_in, num_hidden, num_out):
        super(BiLSTM, self).__init__()
        self.LSTM = nn.LSTM(input_size=num_in, hidden_size=num_hidden, num_layers=2,
                            bidirectional=True, batch_first=False)
        self.embeding = nn.Linear(num_hidden * 2, num_out)

    def forward(self, input):
        n, c, h, w = input.size()
        input = input.permute(0, 3, 2, 1)
        input = input.reshape(n, w, h*c)
        input = input.permute(1, 0, 2)
        recurrent, _ = self.LSTM(input)

        out_put = self.embeding(recurrent)
        return out_put


class ResidualBlock(nn.Module):
    def __init__(self, inchannel, outchannel, stride=(1, 1)):
        super(ResidualBlock, self).__init__()
        self.left = nn.Sequential(
            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),
            nn.BatchNorm2d(outchannel),
            nn.ReLU(inplace=True),
            nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(outchannel)
        )
        self.shortcut = nn.Sequential()
        if stride != (1, 1) or inchannel != outchannel:
            self.shortcut = nn.Sequential(
                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(outchannel)
            )

    def forward(self, x):
        out = self.left(x)
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class ResNet(nn.Module):
    def __init__(self, ResidualBlock, num_classes=10, test=False):
        super(ResNet, self).__init__()
        self.inchannel = 64
        self.conv1 = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU()
        )
        self.layer1 = self.make_layer(ResidualBlock, 64, 2, stride=(1, 1))
        self.layer2 = self.make_layer(ResidualBlock, 128, 2, stride=(2, 2))
        self.layer3 = self.make_layer(ResidualBlock, 256, 2, stride=(2, 2))
        self.layer4 = self.make_layer(ResidualBlock, 512, 2, stride=(2, 1))

        # self.rnn = nn.Sequential(
        #     BidirectionalLSTM(512, 512, 512),
        #     BidirectionalLSTM(512, 512, num_classes),)
        self.rnn = BiLSTM(512, 512, num_classes)
        # self.fc = nn.Linear(512, num_classes)
        self.test = test

    def make_layer(self, block, channels, num_blocks, stride):
        strides = [stride] + [(1, 1)]*(num_blocks - 1)
        layers = []
        for stride in strides:
            layers.append(block(self.inchannel, channels, stride))
            self.inchannel = channels
        return nn.Sequential(*layers)

    def forward(self, x):
        out = self.conv1(x)
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = F.avg_pool2d(out, (4, 1))

        if self.test:
            return out

        # out = out.view(out.size(0), -1)
        # out = self.fc(out)
        out = self.rnn(out)
        return out


def resnet18():
    return ResNet(ResidualBlock)



========== my train code is ==========
    def train_batch(self, epoch):
        self.model.train()
        for batch_idx, (img, label, width, lens) in enumerate(self.train_loader):
            self.optimizer.zero_grad()
            preds = self.model(img)
            log_probs = preds.log_softmax(2).detach().requires_grad_()
            loss = self.criterion(log_probs, label, width, lens)
            print(loss)
            loss.backward()
            self.optimizer.step()  # Êõ¥Êñ∞ÂèÇÊï∞
            if (batch_idx+1) % self.log_interval == 0:
                lr = self.scheduler.get_lr()[0]
                src_str = self.label_convert.get_str_from_label(lables=label)
                preds_size = torch.IntTensor([preds.size(0)] * preds.size(1))
                _, preds = preds.max(2)
                preds = preds.squeeze(2)
                preds = preds.transpose(1, 0)
                sim_preds = self.label_convert.decode(preds, preds_size, raw=False)
                n_correct = 0
                for pred, target in zip(sim_preds, src_str):
                    if pred == target:
                        n_correct += 1
                distance_value = util.edit_distance(src_str, sim_preds)
                localtime = datetime.datetime.now().strftime(""%Y-%m-%d %H:%M:%S"")
                print('[%s]\tTrain Epoch: {%d/%d}\tLoss:%.6f, acc: %.2f, error: %.2f, lr = %.4f'
                      % (str(localtime), epoch, batch_idx, loss, n_correct, distance_value, lr))
            if (batch_idx+1) % self.save_interval == 0:
                new_save_name = 'resnet18_%d_%d.pth' % (epoch, batch_idx+1)
                torch.save(self.state, new_save_name)
                util.update_file(self.save_path, new_save_name)
        new_save_name = 'resnet18_%d.pth' % epoch
        torch.save(self.state, new_save_name)
        util.update_file(self.save_path, new_save_name)
",,"['my os is mocOS', '@bjmajic where are you calling CTCLoss?', '@zou3519 pytorch 1.0 has the ctcloss, and i call it in \r\npreds = self.model(img)\r\nlog_probs = preds.log_softmax(2).detach().requires_grad_()\r\nloss = self.criterion(log_probs, label, width, lens)\r\nprint(loss)\r\nloss.backward()', 'Can you output the `log_probs`, `label` (at least the shapes and devices for those), and the `width` and `lens`, please? Thank you!', '@t-vi thank you.   It was my faultÔºå not a bug.  it caused by dismatch of labels dim and features dim.  ', 'Even if it was a mismatch we should have an error message and never corrupt the memory', 'So we (@ashermancinelli) improved the checks for input/target lengths recently and it might be worth trying if that helps. We do not check that the labels fall into the valid range (neither, I believe, do the scatter ops).\r\n']",[],[],0,0
168,pytorch,21249,closed,The code is incomplete and the data set is missing," Hello, have you ever encountered the pytorch official website fine-tuning Maskrcnn, less code, and the data set mask in pedestrian detection is all blackÔºåIn references/detection/, we have a number of helper functions to simplify training and evaluating detection models. Here, we will use references/detection/engine.py, references/detection/utils.py and references/detection/transforms.py. Just copy them to your folder and use them here.there is less code",,"['be more descriptive and reopen issue in https://github.com/pytorch/vision', 'thanks\n\n\n\n\nOn 06/02/2019 00:26, Soumith Chintala wrote:\n\nClosed #21249.\n\n‚Äî\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.']",[],[],0,0
169,pytorch,1114,closed,Sequential to support better slicing,"

Would be nice in absence of [remove_module](https://github.com/pytorch/pytorch/issues/358), but probably low-priority.",,"[""we just wanted people to convert Sequential to List to do advanced indexing. \r\n\r\nImplementing all advanced indexing in Sequential seemed way to unnecessary considering how much code we'll have to add.""]","['python\r\nimport torch.nn as nn\r\n\r\na = nn.Sequential(nn.Linear(1, 1), nn.Linear(1, 1))\r\n\r\na[0] # works\r\nnn.Sequential(*a[:-1]) # blows\r\nnn.Sequential(*list(a)[:-1]) # works\r\n']",[],0,0
170,pytorch,19253,closed,torch.pow() in a script module produces an error,"## üêõ Bug

The backward of torch.pow() in a traced module produces an error when the input is on cuda.

> RuntimeError:
Expected tensor to have CUDA Backend, but got tensor with CPU Backend (while checking arguments for CUDA_tensor_apply4) (checkBackend at ../aten/src/ATen/TensorUtils.cpp:202)

The backward is defined in torch/csrc/jit/symbolic_script.cpp.
I think the reason is that the backend of the first argument of torch.where() is always CPU.



## To Reproduce

The following model is the one in
https://github.com/pytorch/examples/blob/master/mnist/main.py
but I just inserted torch.pow().



## Environment
PyTorch version: 1.1.0a0+7e73783
Is debug build: No
CUDA used to build PyTorch: 9.2.88

OS: CentOS Linux release 7.5.1804 (Core)
GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)
CMake version: version 3.12.2

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 9.2.88
GPU models and configuration:
GPU 0: Tesla P100-PCIE-16GB
GPU 1: Tesla P100-PCIE-16GB

Nvidia driver version: 396.26
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] numpy==1.15.4
[pip] numpydoc==0.8.0
[pip] torch==1.1.0a0+7e73783
[pip] torchvision==0.2.2.post3
[conda] blas                      1.0                         mkl
[conda] magma-cuda90              2.5.0                         1    pytorch
[conda] magma-cuda92              2.5.0                         1    pytorch
[conda] mkl                       2019.1                      144
[conda] mkl-include               2019.3                      199
[conda] mkl-service               1.1.2            py37he904b0f_5
[conda] mkl_fft                   1.0.6            py37hd81dba3_0
[conda] mkl_random                1.0.2            py37hd81dba3_0
[conda] torch                     1.1.0a0+7e73783           dev_0    <develop>
[conda] torchvision               0.2.2.post3              pypi_0    pypi

",oncall: jit,"[""Just a quick shout that I have a fix for this, but I'm also fixing pow_2 (with scalar base), which makes it a bit slower.""]","['\r\ndef pow_0(self,\r\n          exponent: float):\r\n    def backward(grad_output):\r\n        grad_self = torch.where(torch.tensor(exponent == 0.0), torch.zeros_like(self), grad_output * exponent * torch.pow(self, exponent - 1))\r\n        return grad_self, None\r\n    return torch.pow(self, exponent), backward\r\n', '\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\r\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\r\n        self.fc1 = nn.Linear(4*4*50, 500)\r\n        self.fc2 = nn.Linear(500, 10)\r\n\r\n    def forward(self, x):\r\n        x = F.relu(self.conv1(x))\r\n        x = F.max_pool2d(x, 2, 2)\r\n        x = F.relu(self.conv2(x))\r\n        x = F.max_pool2d(x, 2, 2)\r\n        x = x.view(64, 4*4*50)\r\n        x = self.fc1(x)\r\n        x = F.relu(x)\r\n        x = self.fc2(x)\r\n        x = torch.pow(x, 1.1)\r\n        x = F.log_softmax(x, dim=1)\r\n        return x\r\n\r\nm = Net().cuda()\r\nx = torch.randn(64, 1, 28, 28, requires_grad=True).cuda()\r\ntraced_net = torch.jit.trace(m, x)\r\ntraced_output = traced_net(x)\r\ntgt = torch.randn(traced_output.size()).cuda()\r\ntraced_output.backward(tgt)\r\n']",[],0,0
171,pytorch,13574,closed,dist.all_reduce([tensor]) throws confusing error message,"

It seems internally the input is wrapped in another list, but this error is confusing.",oncall: distributed,"[""@SsnL  Not sure if there is anything to fix here, since this is pybind's error message and torch.distributed wraps up that pybind function."", ""@teng-li No, the problem is that `all_reduce` python code always wraps the input into a list, and then calls the pybind function. So the pybind function sees a list of list of tensors and then throw such error. It is really confusing because users who don't know the implementation details will be very confused since they send in a list of tensors, and the error message says that its type doesn't match a list of tensors."", '@SsnL  I understand what you mean, an easier fix I can think about is to add a type check before every collective? Any better thoughts?', '@teng-li Maybe simply adding an `isinstance(tensor, torch.Tensor)` in `allreduce` python code?', '@SsnL can you review / stamp that PR?', '@teng-li Thanks for fixing! Sorry I was asleep then...']","['\r\nTypeError: allreduce(): incompatible function arguments. The following argument types are supported:\r\n1. (self: torch.distributed.ProcessGroup, arg0: List[at::Tensor], arg1: torch.distributed.AllreduceOptions) -> c10d::ProcessGroup::Work\r\n2. (self: torch.distributed.ProcessGroup, tensor: at::Tensor, op: torch.distributed.ReduceOp = ReduceOp.SUM) -> c10d::ProcessGroup::Work\r\n']",[],0,0
172,pytorch,13409,closed,"[jit] add suggestions in ""unknown builtin op"" error messages","As an example, the following:

will fail with ""unknown builtin op"", even though  (with the underscore) is bound.

We should suggest ops with similar names when we can't find a matching schema.

So the error message could look like:
aten::masked_fillaten:masked_fill_`?
@torch.jit.script
def foo(a):
    mask = torch.rand(1).byte()
    a.masked_fill(mask, 1)
    ~~~~~~~~ <--- HERE
    return a
",oncall: jit,[],"['\r\n@torch.jit.script\r\ndef foo(a):\r\n    mask = torch.rand(1).byte()\r\n    a.masked_fill(mask, 1)\r\n    return a\r\n']","['masked_fill_', '', '\r\nRuntimeError:\r\nunknown builtin op: ', ', did you mean: ']",0,0
173,pytorch,3293,closed,Tensor constructor passed numpy ndarray does not check type.,"

According to @colesbury, however, it does look like ndarray passed into Tensor is doing the correct codepath.

CC @aszlam",,"[""I'm not sure what can we do about it short term. There's no guarantee that we can reliably represent `int8` until we change `char` to `signed char`. It's fine to use such array in a constructor which will cast the memory"", ""The bug is not that we don't support int8, but that if we don't support it, Tensor constructor needs to reject it."", ""No, I can't see why would we need to make it this way. `torch.from_numpy` has no choice other than raising an error, because there is no corresponding type in torch, so it would violate the contract. However, when you use a tensor constructor, you explicitly agree on what the type should be, so casting the array to whatever is needed is a reasonable decision"", 'maybe I am misunderstanding ""you explicitly agree on what the type should be"" ?   with torch.Tensor(numpy_array) ?\n\n________________________________\nFrom: Adam Paszke <notifications@github.com>\nSent: Thursday, October 26, 2017 12:31:46 PM\nTo: pytorch/pytorch\nCc: Arthur Szlam; Mention\nSubject: Re: [pytorch/pytorch] Tensor constructor passed numpy ndarray does not check type. (#3293)\n\n\nNo, I can\'t see why would we need to make it this way. torch.from_numpy has no choice other than raising an error, because there is no corresponding type in torch, so it would violate the contract. However, when you use a tensor constructor, you explicitly agree on what the type should be, so casting the array to whatever is needed is a reasonable decision\n\n‚Äî\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<https://github.com/pytorch/pytorch/issues/3293#issuecomment-339723667>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AF1_-ejy3frN_4YU1DOWdX-dynmFJNq1ks5swLPygaJpZM4QGvjN>.\n', 'Yes. `torch.Tensor` is always an alias for one of the tensor types, not a ""try to infer the datatype"" thing. By default it\'s `torch.FloatTensor`, but you can change it with `torch.set_default_tensor_type`', 'NOTABUG.']","['\r\n>>> torch.Tensor(numpy.ndarray(shape=(2,2),dtype=""int8""))\r\n\r\n 0.6229  0.0000\r\n 0.0000  0.0000\r\n[torch.FloatTensor of size 2x2]\r\n\r\n>>> torch.from_numpy(numpy.ndarray(shape=(2,2),dtype=""int8""))\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\nRuntimeError: can\'t convert a given np.ndarray to a tensor - it has an invalid type. The only supported types are: double, float, int64, int32, and uint8.\r\n']",[],0,0
174,pytorch,27954,closed,[quantization] Dynamic LSTM doesn't serialize properly when traced,"Repro



Output



cc @suo @jerryzh168 @jianyuh @dzhulgakov @raghuramank100",oncall: jit oncall: quantization,"['cc @suo ', 'As a workaround, users can apply this post-processing to their model after calling `quantize_dynamic`:\r\n\r\n```\r\n            def visit_and_script(mod):\r\n                if isinstance(mod, torch.nn.quantized.dynamic.modules.LSTM):\r\n                    return torch.jit.script(mod)\r\n                for k, v in mod.named_children():\r\n                    setattr(mod, k, visit_and_script(v))\r\n                return mod\r\n\r\n            model = visit_and_script(model)\r\n```', ""This is an intersection of multiple issues:\r\n\r\n* Overload mechanism doesn't work for tracing\r\n* Tracing doesn't capture attribute lookups for list attributes"", '@jamesr66a \r\nIs it work now? \r\nI am also facing this problem in PyTorch 1.3.0.\r\n\r\nWhich PyTorch version do I use to solve this problem?', '> @jamesr66a\r\n> Is it work now?\r\n> I am also facing this problem in PyTorch 1.3.0.\r\n> \r\n> Which PyTorch version do I use to solve this problem?\r\n\r\nLooks like this is fixed after 1.3.0 is released, I think you should be able to find it in nightly version, or try 1.3.1(not sure if this is fixed there) or wait for 1.4.0', ""> @jamesr66a\r\n> Is it work now?\r\n> I am also facing this problem in PyTorch 1.3.0.\r\n> \r\n> Which PyTorch version do I use to solve this problem?\r\n\r\nIt's only occurred when we use the 8-bit quantization on LSTM with jit.trace.\r\nIt's fine the 8-bit quantization on LSTM with jit.script.""]","[""python\r\nimport torch\r\nimport copy\r\nimport io\r\nfrom torch.quantization import default_qconfig, quantize_dynamic\r\n\r\nclass LSTMDynamicModel(torch.nn.Module):\r\n    def __init__(self):\r\n        super(LSTMDynamicModel, self).__init__()\r\n        self.qconfig = default_qconfig\r\n        self.lstm = torch.nn.LSTM(2, 2).to(dtype=torch.float)\r\n\r\n    def forward(self, x):\r\n        x = self.lstm(x)\r\n        return x\r\n\r\nd_in, d_hid = 2, 2\r\nmodel = LSTMDynamicModel().eval()\r\ncell = model.lstm\r\n\r\n# Replace parameter values s.t. the range of values is exactly\r\n# 255, thus we will have 0 quantization error in the quantized\r\n# GEMM call. This i s for testing purposes.\r\n#\r\n# Note that the current implementation does not support\r\n# accumulation values outside of the range representable by a\r\n# 16 bit integer, instead resulting in a saturated value. We\r\n# must take care that in our test we do not end up with a dot\r\n# product that overflows the int16 range, e.g.\r\n# (255*127+255*127) = 64770. So, we hardcode the test values\r\n# here and ensure a mix of signedness.\r\nvals = [[100, -155],\r\n        [100, -155],\r\n        [-155, 100],\r\n        [-155, 100],\r\n        [100, -155],\r\n        [-155, 100],\r\n        [-155, 100],\r\n        [100, -155]]\r\nif isinstance(cell, torch.nn.LSTM):\r\n    num_chunks = 4\r\nvals = vals[:d_hid * num_chunks]\r\ncell.weight_ih_l0 = torch.nn.Parameter(\r\n    torch.tensor(vals, dtype=torch.float),\r\n    requires_grad=False)\r\ncell.weight_hh_l0 = torch.nn.Parameter(\r\n    torch.tensor(vals, dtype=torch.float),\r\n    requires_grad=False)\r\n\r\nref = copy.deepcopy(cell)\r\n\r\nmodel_int8 = quantize_dynamic(model=model, dtype=torch.qint8)\r\nmodel_fp16 = quantize_dynamic(model=model, dtype=torch.float16)\r\n\r\n# Smoke test extra reprs\r\ncell_int8 = model_int8.lstm\r\ncell_fp16 = model_fp16.lstm\r\n\r\nassert type(cell_int8) == torch.nn.quantized.dynamic.LSTM, \\\r\n    'torch.nn.LSTM should be converted to torch.nn.quantized.dynamic.LSTM after quantize_dynamic'\r\nassert type(cell_fp16) == torch.nn.quantized.dynamic.LSTM, \\\r\n    'torch.nn.LSTM should be converted to torch.nn.quantized.dynamic.LSTM after quantize_dynamic'\r\n\r\nniter = 10\r\nx = torch.tensor([[100, -155],\r\n                  [-155, 100],\r\n                  [100, -155]], dtype=torch.float).unsqueeze(0).repeat(niter, 1, 1)\r\n\r\nh0_vals = [[-155, 100],\r\n           [-155, 155],\r\n           [100, -155]]\r\n\r\nhx = torch.tensor(h0_vals, dtype=torch.float).unsqueeze(0)\r\ncx = torch.tensor(h0_vals, dtype=torch.float).unsqueeze(0)\r\n\r\nif isinstance(ref, torch.nn.LSTM):\r\n    hiddens = (hx, cx)\r\n\r\nref_out, ref_hid = ref(x, hiddens)\r\n\r\n# Compare int8 quantized to unquantized\r\noutput_int8, final_hiddens_int8 = cell_int8(x, hiddens)\r\n\r\ntorch.testing.assert_allclose(output_int8, ref_out)\r\n\r\nfor out_val, ref_val in zip(final_hiddens_int8, ref_hid):\r\n    torch.testing.assert_allclose(out_val, ref_val)\r\n\r\nclass ScriptWrapper(torch.nn.Module):\r\n    def __init__(self, cell):\r\n        super(ScriptWrapper, self).__init__()\r\n        self.cell = cell\r\n\r\n    def forward(self, x, hiddens):\r\n        # type: (torch.Tensor, Tuple[torch.Tensor, torch.Tensor]) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]\r\n        return self.cell(x, hiddens)\r\n\r\n# TODO: TorchScript overloads don't work without this wrapper\r\ncell_script = torch.jit.trace(ScriptWrapper(cell_int8), (x, (hx, cx)))\r\n# cell_script = torch.jit.script(ScriptWrapper(cell_int8))\r\nout_script, hid_script = cell_script(x, hiddens)\r\nfor out_val, ref_val in zip(out_script, ref_out):\r\n    torch.testing.assert_allclose(out_val, ref_val)\r\n\r\n# Test save/load\r\nb = io.BytesIO()\r\n# torch.jit.save(cell_script, b)\r\ntorch.jit.save(cell_script, 'foo.zip')\r\n\r\nloaded = torch.jit.load('foo.zip')\r\nloaded(x, (hx, cx))\r\n"", '\r\n../aten/src/ATen/native/QuantizedLinear.cpp:328: UserWarning: FOUND weight out of range \r\n../aten/src/ATen/native/QuantizedLinear.cpp:328: UserWarning: FOUND weight out of range \r\nTraceback (most recent call last):\r\n  File ""export_test.py"", line 110, in <module>\r\n    loaded(x, (hx, cx))\r\n  File ""/data/users/jamesreed/pytorch/torch/nn/modules/module.py"", line 541, in __call__\r\n    result = self.forward(*input, **kwargs)\r\nRuntimeError: Expected temporary cpp type wrapper of type PackedLinearWeight\r\nThe above operation failed in interpreter, with the following stack trace:\r\nat code/__torch__/torch/nn/modules/module.py:11:17\r\n    argument_2: Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tuple[Tensor, Tensor]]:\r\n    hx, hx0, = argument_2\r\n    _0, _1, _2 = torch.quantized_lstm(input, [hx, hx0], [CONSTANTS.c0, CONSTANTS.c1], True, 1, 0., True, False, False, dtype=12, use_dynamic=True)\r\n                 ~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n    return (_0, (_1, _2))\r\nCompiled from code /data/users/jamesreed/pytorch/torch/nn/quantized/dynamic/modules/rnn.py(332): forward_impl\r\n/data/users/jamesreed/pytorch/torch/nn/quantized/dynamic/modules/rnn.py(347): forward_tensor\r\n/data/users/jamesreed/pytorch/torch/nn/quantized/dynamic/modules/rnn.py(386): forward\r\n/data/users/jamesreed/pytorch/torch/nn/modules/module.py(525): _slow_forward\r\n/data/users/jamesreed/pytorch/torch/nn/modules/module.py(539): __call__\r\nexport_test.py(95): forward\r\n/data/users/jamesreed/pytorch/torch/nn/modules/module.py(525): _slow_forward\r\n/data/users/jamesreed/pytorch/torch/nn/modules/module.py(539): __call__\r\n/data/users/jamesreed/pytorch/torch/jit/__init__.py(1013): trace_module\r\n/data/users/jamesreed/pytorch/torch/jit/__init__.py(874): trace\r\nexport_test.py(98): <module>\r\n\r\n']",[],0,0
175,pytorch,5826,closed,"UserWarning: tensor1/other is not broadcastable to self, but they have the same number of elements. Falling back to deprecated pointwise behavior","- OS: Ubuntu 16.04.3 LTS (GNU/Linux 4.4.0-1038-aws x86_64)
- PyTorch version:  0.4.0a0+7f864bb (Source), and 0.3.1 (Pip).

- How you installed PyTorch (conda, pip, source): Both Pip and Source
- Python version: Python2 and Python3
- CUDA/cuDNN version: Cuda 9, cuDNN v7
- GPU models and configuration: Tesla K80        
- GCC version (if compiling from source): gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5)


When using the Pip version (Python2 & 3), I am getting these errors when using the  L-BFGS and Adam optimizers: 


Running optimization with ADAM:


 
Running optimization with L-BFGS:



This is my feval function: 



And this is how I run it: 




If I run the above code with the latest Github version, I get this error which happens in a completely different area: 




So I am not sure if the above error is still related to the Pip issues, if something got changed/broken, etc... ",awaiting response (this tag is deprecated),"['These might be related: https://github.com/pytorch/pytorch/issues/3268\r\nhttps://discuss.pytorch.org/t/userwarning-for-torch-load-state-dict/7424\r\nhttps://discuss.pytorch.org/t/userwarning-after-upgrading/4990\r\n', 'Hi @ProGamerGov , the warning message means the two tensors associated with the operator has the same number of elements but not broadcastable. See [here](https://github.com/pytorch/pytorch/blob/ee1b7b50b39a8261f2b2ba2a95340f51de92ddbc/docs/source/notes/broadcasting.rst#L94) for more details. \r\nLooks like you are running into some shape specific issues, if you could provide you minimal repro script with fake input (random Tensor with the same shape), we can help on that.']","[""\r\n/usr/local/lib/python2.7/dist-packages/torch/legacy/optim/adam.py:65: UserWarning: tensor1 is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\r\n  x.addcdiv_(-stepSize, state['m'], state['denom'])\r\n"", ""\r\n('<optim.lbfgs>', 'creating recyclable direction/step/history buffers')\r\n/usr/local/lib/python2.7/dist-packages/torch/legacy/optim/lbfgs.py:197: UserWarning: other is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\r\n  x.add_(t, d)\r\n"", '\r\nnum_calls = [0]\r\ndef feval(x):\r\n  num_calls[0] += 1\r\n  net.updateOutput(x.cuda())\r\n  grad = net.updateGradInput(x.cuda(), dy.cuda())\r\n  loss = 0\r\n  for mod in content_losses:\r\n    loss = loss + mod.loss\r\n  for mod in style_losses:\r\n    loss = loss + mod.loss\r\n  return loss, grad.view(grad.nelement())\r\n', '\r\noptim_state = None\r\nif params.optimizer == \'lbfgs\':\r\n  optim_state = {\r\n    ""maxIter"": params.num_iterations,\r\n    ""verbose"": True,\r\n    ""tolX"": -1,\r\n    ""tolFun"": -1,\r\n  }\r\n  if params.lbfgs_num_correction > 0:\r\n    optim_state.nCorrection = params.lbfgs_num_correction\r\nelif params.optimizer == \'adam\':\r\n    optim_state = {\r\n      ""learningRate"": params.learning_rate,\r\n    }\r\n\r\n# Run optimization.\r\nif params.optimizer == \'lbfgs\':\r\n  print(""Running optimization with L-BFGS"")\r\n  x, losses = optim.lbfgs(feval, img, optim_state)\r\nelif params.optimizer == \'adam\':\r\n  print(""Running optimization with ADAM"")\r\n  for t in xrange(params.num_iterations):\r\n    x, losses = optim.adam(feval, img, optim_state)\r\n', '\r\nTraceback (most recent call last):\r\n  File ""test2.py"", line 124, in <module>\r\n    net.updateOutput(content_image_caffe)\r\n  File ""/usr/local/lib/python2.7/dist-packages/torch/legacy/nn/Sequential.py"", line 36, in updateOutput\r\n    currentOutput = module.updateOutput(currentOutput)\r\n  File ""/usr/local/lib/python2.7/dist-packages/torch/legacy/nn/SpatialConvolution.py"", line 84, in updateOutput\r\n    self._viewWeight()\r\n  File ""/usr/local/lib/python2.7/dist-packages/torch/legacy/nn/SpatialConvolution.py"", line 75, in _viewWeight\r\n    self.gradWeight = self.gradWeight.view(self.nOutputPlane, self.nInputPlane * self.kH * self.kW)\r\nRuntimeError: invalid argument 2: size \'[64 x 27]\' is invalid for input with 0 elements at /home/ubuntu/pytorch/aten/src/TH/THStorage.c:41\r\n']",['torch.legacy.optim'],0,0
176,pytorch,9054,closed,UBSAN error: call to function THDRequest_free through pointer to incorrect function type 'void (*)(void *)',"Both  and  in  throws error with UBSAN (error line: https://github.com/pytorch/pytorch/blob/master/torch/csrc/PtrWrapper.cpp#L49):

",,"['@colesbury Should we just ask UBSAN to ignore it, or is there a better fix for this?', '@yf225 probably best just to fix the error. I think the easiest change is to make `THDRequest_free` take a `void*` pointer instead of `THDRequest*` and then cast to `THDRequest*` before calling delete.']","[""\r\ntest_irecv (__main__.TestDistBackend) ... torch/csrc/PtrWrapper.cpp:50:3: runtime error: call to function THDRequest_free through pointer to incorrect function type 'void (*)(void *)'\r\n/var/lib/jenkins/pytorch/torch/lib/THD/base/DataChannelRequest.cpp:4: note: THDRequest_free defined here\r\n    #0 0x7fef1eacff05 in THPWrapper_dealloc(THPWrapper*) /var/lib/jenkins/pytorch/torch/csrc/PtrWrapper.cpp:50:3\r\n    #1 0x5623c8179e96 in dict_dealloc (/opt/conda/bin/python3.6+0xeee96)\r\n    #2 0x5623c822a60d in subtype_dealloc (/opt/conda/bin/python3.6+0x19f60d)\r\n    #3 0x5623c8179b1e in list_dealloc (/opt/conda/bin/python3.6+0xeeb1e)\r\n    #4 0x5623c8179916 in frame_dealloc (/opt/conda/bin/python3.6+0xee916)\r\n    #5 0x5623c8222dcf in _PyEval_EvalCodeWithName (/opt/conda/bin/python3.6+0x197dcf)\r\n    #6 0x5623c8223940 in fast_function (/opt/conda/bin/python3.6+0x198940)\r\n    #7 0x5623c8229754 in call_function (/opt/conda/bin/python3.6+0x19e754)\r\n    #8 0x5623c824bcb9 in _PyEval_EvalFrameDefault (/opt/conda/bin/python3.6+0x1c0cb9)\r\n    #9 0x5623c8222c25 in _PyEval_EvalCodeWithName (/opt/conda/bin/python3.6+0x197c25)\r\n    #10 0x5623c8223940 in fast_function (/opt/conda/bin/python3.6+0x198940)\r\n    #11 0x5623c8229754 in call_function (/opt/conda/bin/python3.6+0x19e754)\r\n    #12 0x5623c824bcb9 in _PyEval_EvalFrameDefault (/opt/conda/bin/python3.6+0x1c0cb9)\r\n    #13 0x5623c8223d7a in _PyFunction_FastCallDict (/opt/conda/bin/python3.6+0x198d7a)\r\n    #14 0x5623c8199f5e in _PyObject_FastCallDict (/opt/conda/bin/python3.6+0x10ef5e)\r\n    #15 0x5623c819ea02 in _PyObject_Call_Prepend (/opt/conda/bin/python3.6+0x113a02)\r\n    #16 0x5623c819999d in PyObject_Call (/opt/conda/bin/python3.6+0x10e99d)\r\n    #17 0x5623c824d46f in _PyEval_EvalFrameDefault (/opt/conda/bin/python3.6+0x1c246f)\r\n    #18 0x5623c822370a in fast_function (/opt/conda/bin/python3.6+0x19870a)\r\n    #19 0x5623c8229754 in call_function (/opt/conda/bin/python3.6+0x19e754)\r\n    #20 0x5623c824bcb9 in _PyEval_EvalFrameDefault (/opt/conda/bin/python3.6+0x1c0cb9)\r\n    #21 0x5623c822370a in fast_function (/opt/conda/bin/python3.6+0x19870a)\r\n    #22 0x5623c8229754 in call_function (/opt/conda/bin/python3.6+0x19e754)\r\n    #23 0x5623c824bcb9 in _PyEval_EvalFrameDefault (/opt/conda/bin/python3.6+0x1c0cb9)\r\n    #24 0x5623c822370a in fast_function (/opt/conda/bin/python3.6+0x19870a)\r\n    #25 0x5623c8229754 in call_function (/opt/conda/bin/python3.6+0x19e754)\r\n    #26 0x5623c824bcb9 in _PyEval_EvalFrameDefault (/opt/conda/bin/python3.6+0x1c0cb9)\r\n    #27 0x5623c8223d7a in _PyFunction_FastCallDict (/opt/conda/bin/python3.6+0x198d7a)\r\n    #28 0x5623c8199f5e in _PyObject_FastCallDict (/opt/conda/bin/python3.6+0x10ef5e)\r\n    #29 0x5623c819ea02 in _PyObject_Call_Prepend (/opt/conda/bin/python3.6+0x113a02)\r\n    #30 0x5623c819999d in PyObject_Call (/opt/conda/bin/python3.6+0x10e99d)\r\n    #31 0x5623c81f602a in slot_tp_init (/opt/conda/bin/python3.6+0x16b02a)\r\n    #32 0x5623c82299b6 in type_call (/opt/conda/bin/python3.6+0x19e9b6)\r\n    #33 0x5623c8199d7a in _PyObject_FastCallDict (/opt/conda/bin/python3.6+0x10ed7a)\r\n    #34 0x5623c82297cd in call_function (/opt/conda/bin/python3.6+0x19e7cd)\r\n    #35 0x5623c824bcb9 in _PyEval_EvalFrameDefault (/opt/conda/bin/python3.6+0x1c0cb9)\r\n    #36 0x5623c822370a in fast_function (/opt/conda/bin/python3.6+0x19870a)\r\n    #37 0x5623c8229754 in call_function (/opt/conda/bin/python3.6+0x19e754)\r\n    #38 0x5623c824bcb9 in _PyEval_EvalFrameDefault (/opt/conda/bin/python3.6+0x1c0cb9)\r\n    #39 0x5623c822370a in fast_function (/opt/conda/bin/python3.6+0x19870a)\r\n    #40 0x5623c8229754 in call_function (/opt/conda/bin/python3.6+0x19e754)\r\n    #41 0x5623c824bcb9 in _PyEval_EvalFrameDefault (/opt/conda/bin/python3.6+0x1c0cb9)\r\n    #42 0x5623c822370a in fast_function (/opt/conda/bin/python3.6+0x19870a)\r\n    #43 0x5623c8229754 in call_function (/opt/conda/bin/python3.6+0x19e754)\r\n    #44 0x5623c824bcb9 in _PyEval_EvalFrameDefault (/opt/conda/bin/python3.6+0x1c0cb9)\r\n    #45 0x5623c822370a in fast_function (/opt/conda/bin/python3.6+0x19870a)\r\n    #46 0x5623c8229754 in call_function (/opt/conda/bin/python3.6+0x19e754)\r\n    #47 0x5623c824bcb9 in _PyEval_EvalFrameDefault (/opt/conda/bin/python3.6+0x1c0cb9)\r\n    #48 0x5623c822370a in fast_function (/opt/conda/bin/python3.6+0x19870a)\r\n    #49 0x5623c8229754 in call_function (/opt/conda/bin/python3.6+0x19e754)\r\n    #50 0x5623c824bcb9 in _PyEval_EvalFrameDefault (/opt/conda/bin/python3.6+0x1c0cb9)\r\n    #51 0x5623c8222a93 in _PyEval_EvalCodeWithName (/opt/conda/bin/python3.6+0x197a93)\r\n    #52 0x5623c822403a in _PyFunction_FastCallDict (/opt/conda/bin/python3.6+0x19903a)\r\n    #53 0x5623c8199f5e in _PyObject_FastCallDict (/opt/conda/bin/python3.6+0x10ef5e)\r\n    #54 0x5623c819ea02 in _PyObject_Call_Prepend (/opt/conda/bin/python3.6+0x113a02)\r\n    #55 0x5623c819999d in PyObject_Call (/opt/conda/bin/python3.6+0x10e99d)\r\n    #56 0x5623c824d46f in _PyEval_EvalFrameDefault (/opt/conda/bin/python3.6+0x1c246f)\r\n    #57 0x5623c8222a93 in _PyEval_EvalCodeWithName (/opt/conda/bin/python3.6+0x197a93)\r\n    #58 0x5623c8223e1a in _PyFunction_FastCallDict (/opt/conda/bin/python3.6+0x198e1a)\r\n    #59 0x5623c8199f5e in _PyObject_FastCallDict (/opt/conda/bin/python3.6+0x10ef5e)\r\n    #60 0x5623c819ea02 in _PyObject_Call_Prepend (/opt/conda/bin/python3.6+0x113a02)\r\n    #61 0x5623c819999d in PyObject_Call (/opt/conda/bin/python3.6+0x10e99d)\r\n    #62 0x5623c81f69b6 in slot_tp_call (/opt/conda/bin/python3.6+0x16b9b6)\r\n    #63 0x5623c8199d7a in _PyObject_FastCallDict (/opt/conda/bin/python3.6+0x10ed7a)\r\n    #64 0x5623c82297cd in call_function (/opt/conda/bin/python3.6+0x19e7cd)\r\n    #65 0x5623c824bcb9 in _PyEval_EvalFrameDefault (/opt/conda/bin/python3.6+0x1c0cb9)\r\n    #66 0x5623c8222a93 in _PyEval_EvalCodeWithName (/opt/conda/bin/python3.6+0x197a93)\r\n    #67 0x5623c822403a in _PyFunction_FastCallDict (/opt/conda/bin/python3.6+0x19903a)\r\n    #68 0x5623c8199f5e in _PyObject_FastCallDict (/opt/conda/bin/python3.6+0x10ef5e)\r\n    #69 0x5623c819ea02 in _PyObject_Call_Prepend (/opt/conda/bin/python3.6+0x113a02)\r\n    #70 0x5623c819999d in PyObject_Call (/opt/conda/bin/python3.6+0x10e99d)\r\n    #71 0x5623c824d46f in _PyEval_EvalFrameDefault (/opt/conda/bin/python3.6+0x1c246f)\r\n    #72 0x5623c8222a93 in _PyEval_EvalCodeWithName (/opt/conda/bin/python3.6+0x197a93)\r\n    #73 0x5623c8223e1a in _PyFunction_FastCallDict (/opt/conda/bin/python3.6+0x198e1a)\r\n    #74 0x5623c8199f5e in _PyObject_FastCallDict (/opt/conda/bin/python3.6+0x10ef5e)\r\n    #75 0x5623c819ea02 in _PyObject_Call_Prepend (/opt/conda/bin/python3.6+0x113a02)\r\n    #76 0x5623c819999d in PyObject_Call (/opt/conda/bin/python3.6+0x10e99d)\r\n    #77 0x5623c81f69b6 in slot_tp_call (/opt/conda/bin/python3.6+0x16b9b6)\r\n    #78 0x5623c8199d7a in _PyObject_FastCallDict (/opt/conda/bin/python3.6+0x10ed7a)\r\n    #79 0x5623c82297cd in call_function (/opt/conda/bin/python3.6+0x19e7cd)\r\n    #80 0x5623c824bcb9 in _PyEval_EvalFrameDefault (/opt/conda/bin/python3.6+0x1c0cb9)\r\n    #81 0x5623c8222a93 in _PyEval_EvalCodeWithName (/opt/conda/bin/python3.6+0x197a93)\r\n    #82 0x5623c822403a in _PyFunction_FastCallDict (/opt/conda/bin/python3.6+0x19903a)\r\n    #83 0x5623c8199f5e in _PyObject_FastCallDict (/opt/conda/bin/python3.6+0x10ef5e)\r\n    #84 0x5623c819ea02 in _PyObject_Call_Prepend (/opt/conda/bin/python3.6+0x113a02)\r\n    #85 0x5623c819999d in PyObject_Call (/opt/conda/bin/python3.6+0x10e99d)\r\n    #86 0x5623c824d46f in _PyEval_EvalFrameDefault (/opt/conda/bin/python3.6+0x1c246f)\r\n    #87 0x5623c8222a93 in _PyEval_EvalCodeWithName (/opt/conda/bin/python3.6+0x197a93)\r\n    #88 0x5623c8223e1a in _PyFunction_FastCallDict (/opt/conda/bin/python3.6+0x198e1a)\r\n    #89 0x5623c8199f5e in _PyObject_FastCallDict (/opt/conda/bin/python3.6+0x10ef5e)\r\n    #90 0x5623c819ea02 in _PyObject_Call_Prepend (/opt/conda/bin/python3.6+0x113a02)\r\n    #91 0x5623c819999d in PyObject_Call (/opt/conda/bin/python3.6+0x10e99d)\r\n    #92 0x5623c81f69b6 in slot_tp_call (/opt/conda/bin/python3.6+0x16b9b6)\r\n    #93 0x5623c8199d7a in _PyObject_FastCallDict (/opt/conda/bin/python3.6+0x10ed7a)\r\n    #94 0x5623c82297cd in call_function (/opt/conda/bin/python3.6+0x19e7cd)\r\n    #95 0x5623c824bcb9 in _PyEval_EvalFrameDefault (/opt/conda/bin/python3.6+0x1c0cb9)\r\n    #96 0x5623c822370a in fast_function (/opt/conda/bin/python3.6+0x19870a)\r\n    #97 0x5623c8229754 in call_function (/opt/conda/bin/python3.6+0x19e754)\r\n    #98 0x5623c824bcb9 in _PyEval_EvalFrameDefault (/opt/conda/bin/python3.6+0x1c0cb9)\r\n    #99 0x5623c822370a in fast_function (/opt/conda/bin/python3.6+0x19870a)\r\n    #100 0x5623c8229754 in call_function (/opt/conda/bin/python3.6+0x19e754)\r\n    #101 0x5623c824bcb9 in _PyEval_EvalFrameDefault (/opt/conda/bin/python3.6+0x1c0cb9)\r\n    #102 0x5623c8222f20 in _PyEval_EvalCodeWithName (/opt/conda/bin/python3.6+0x197f20)\r\n    #103 0x5623c8223e1a in _PyFunction_FastCallDict (/opt/conda/bin/python3.6+0x198e1a)\r\n    #104 0x5623c8199f5e in _PyObject_FastCallDict (/opt/conda/bin/python3.6+0x10ef5e)\r\n    #105 0x5623c819ea02 in _PyObject_Call_Prepend (/opt/conda/bin/python3.6+0x113a02)\r\n    #106 0x5623c819999d in PyObject_Call (/opt/conda/bin/python3.6+0x10e99d)\r\n    #107 0x5623c81f602a in slot_tp_init (/opt/conda/bin/python3.6+0x16b02a)\r\n    #108 0x5623c82299b6 in type_call (/opt/conda/bin/python3.6+0x19e9b6)\r\n    #109 0x5623c8199d7a in _PyObject_FastCallDict (/opt/conda/bin/python3.6+0x10ed7a)\r\n    #110 0x5623c82297cd in call_function (/opt/conda/bin/python3.6+0x19e7cd)\r\n    #111 0x5623c824bcb9 in _PyEval_EvalFrameDefault (/opt/conda/bin/python3.6+0x1c0cb9)\r\n    #112 0x5623c8224458 in PyEval_EvalCodeEx (/opt/conda/bin/python3.6+0x199458)\r\n    #113 0x5623c82251eb in PyEval_EvalCode (/opt/conda/bin/python3.6+0x19a1eb)\r\n    #114 0x5623c829f9a3 in run_mod (/opt/conda/bin/python3.6+0x2149a3)\r\n    #115 0x5623c829fda0 in PyRun_FileExFlags (/opt/conda/bin/python3.6+0x214da0)\r\n    #116 0x5623c829ffa3 in PyRun_SimpleFileExFlags (/opt/conda/bin/python3.6+0x214fa3)\r\n    #117 0x5623c82a3a9d in Py_Main (/opt/conda/bin/python3.6+0x218a9d)\r\n    #118 0x5623c816b4bd in main (/opt/conda/bin/python3.6+0xe04bd)\r\n    #119 0x7fef3a2b482f in __libc_start_main /build/glibc-Cl5G7W/glibc-2.23/csu/../csu/libc-start.c:291\r\n    #120 0x5623c8252772 in _start (/opt/conda/bin/python3.6+0x1c7772)\r\n""]","['test_irecv', 'test_isend', 'TestDistBackend']",0,0
177,pytorch,27037,closed,`padding_mode` support for `functional.unfold`,"## üöÄ Feature
Add  support for  operation.

## Motivation
 is a very useful function for window operations other than convolution. In its current implementation, it only supports the ""constant"" padding mode, and automatically fills the margins with zeros. I have a use case where I need to use the ""edge"" padding mode. 

## Pitch

I want to  to have the  argument exactly like the convolution operation. This would also make the API more coherent.

## Alternatives

I guess the alternative is to manually copy cell values to the padded area.
",,"['I just realized `nn.functional.pad` function exists and can be used to solve this problem, without building in `padding_mode` into `unfold`. In light of this, maybe this issue can be closed.']",[],"['padding_mode', 'functional.unfold', 'unfold', 'unfold', 'padding_mode']",0,0
178,pytorch,7721,closed,"Error compiling from source (OS X 10.12, CUDA 9.0, CUDNN 7.0.4)","Installed clean OS 10.12.6 on MacBook Pro late 2013 with GT 750M video card. Been trying to get this to work for weeks, tried every tip I can find. Following error message:

running install
running build_deps
+ WITH_CUDA=0
+ [[ --with-cuda == \-\-\w\i\t\h\-\c\u\d\a ]]
+ WITH_CUDA=1
+ shift
+ WITH_ROCM=0
+ [[ --with-nnpack == \-\-\w\i\t\h\-\r\o\c\m ]]
+ WITH_NNPACK=0
+ [[ --with-nnpack == \-\-\w\i\t\h\-\n\n\p\a\c\k ]]
+ WITH_NNPACK=1
+ shift
+ WITH_MKLDNN=0
+ [[ ATen == \-\-\w\i\t\h\-\m\k\l\d\n\n ]]
+ WITH_GLOO_IBVERBS=0
+ [[ ATen == \-\-\w\i\t\h\-\g\l\o\o\-\i\b\v\e\r\b\s ]]
+ WITH_DISTRIBUTED_MW=0
+ [[ ATen == \-\-\w\i\t\h\-\d\i\s\t\r\i\b\u\t\e\d\-\m\w ]]
+ CMAKE_INSTALL='make install'
+ USER_CFLAGS=
+ USER_LDFLAGS=
+ [[ -n '' ]]
+ [[ -n '' ]]
+ [[ -n '' ]]
++ dirname tools/build_pytorch_libs.sh
+ cd tools/..
+++ pwd
++ printf '%q\n' /Users/hsr/pytorch
+ PWD=/Users/hsr/pytorch
+ BASE_DIR=/Users/hsr/pytorch
+ TORCH_LIB_DIR=/Users/hsr/pytorch/torch/lib
+ INSTALL_DIR=/Users/hsr/pytorch/torch/lib/tmp_install
+ THIRD_PARTY_DIR=/Users/hsr/pytorch/third_party
+ CMAKE_VERSION=cmake
+ C_FLAGS=' -DTH_INDEX_BASE=0 -I""/Users/hsr/pytorch/torch/lib/tmp_install/include""   -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/TH"" -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THC""   -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THS"" -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THCS""   -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THNN"" -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THCUNN""'
+ C_FLAGS=' -DTH_INDEX_BASE=0 -I""/Users/hsr/pytorch/torch/lib/tmp_install/include""   -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/TH"" -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THC""   -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THS"" -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THCS""   -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THNN"" -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THCUNN"" -DOMPI_SKIP_MPICXX=1'
+ LDFLAGS='-L""/Users/hsr/pytorch/torch/lib/tmp_install/lib"" '
+ LD_POSTFIX=.so.1
+ LD_POSTFIX_UNVERSIONED=.so
++ uname
+ [[ Darwin == \D\a\r\w\i\n ]]
+ LDFLAGS='-L""/Users/hsr/pytorch/torch/lib/tmp_install/lib""  -Wl,-rpath,@loader_path'
+ LD_POSTFIX=.1.dylib
+ LD_POSTFIX_UNVERSIONED=.dylib
+ CPP_FLAGS=' -std=c++11 '
+ GLOO_FLAGS=
+ THD_FLAGS=
+ NCCL_ROOT_DIR=/Users/hsr/pytorch/torch/lib/tmp_install
+ [[ 1 -eq 1 ]]
+ GLOO_FLAGS='-DUSE_CUDA=1 -DNCCL_ROOT_DIR=/Users/hsr/pytorch/torch/lib/tmp_install'
+ [[ 0 -eq 1 ]]
+ [[ 0 -eq 1 ]]
+ CWRAP_FILES='/Users/hsr/pytorch/torch/lib/ATen/Declarations.cwrap;/Users/hsr/pytorch/torch/lib/THNN/generic/THNN.h;/Users/hsr/pytorch/torch/lib/THCUNN/generic/THCUNN.h;/Users/hsr/pytorch/torch/lib/ATen/nn.yaml'
+ CUDA_NVCC_FLAGS=' -DTH_INDEX_BASE=0 -I""/Users/hsr/pytorch/torch/lib/tmp_install/include""   -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/TH"" -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THC""   -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THS"" -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THCS""   -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THNN"" -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THCUNN"" -DOMPI_SKIP_MPICXX=1'
+ [[ '' -eq 1 ]]
+ '[' -z 8 ']'
+ BUILD_TYPE=Release
+ [[ -n '' ]]
+ [[ -n '' ]]
+ echo 'Building in Release mode'
Building in Release mode
+ mkdir -p torch/lib/tmp_install
+ for arg in '""$@""'
+ [[ ATen == \n\c\c\l ]]
+ [[ ATen == \g\l\o\o ]]
+ [[ ATen == \A\T\e\n ]]
+ pushd /Users/hsr/pytorch/aten
~/pytorch/aten ~/pytorch
+ build_aten
+ mkdir -p build
+ pushd build
~/pytorch/aten/build ~/pytorch/aten ~/pytorch
+ cmake .. -DCMAKE_BUILD_TYPE=Release -DNO_CUDA=0 -DNO_NNPACK=0 -DCUDNN_INCLUDE_DIR=/usr/local/cuda/include -DCUDNN_LIB_DIR=/usr/local/cuda/lib -DCUDNN_LIBRARY=/usr/local/cuda/lib/libcudnn.7.dylib -DNO_MKLDNN=1 -DMKLDNN_INCLUDE_DIR= -DMKLDNN_LIB_DIR= -DMKLDNN_LIBRARY= -DATEN_NO_CONTRIB=1 -DCMAKE_INSTALL_PREFIX=/Users/hsr/pytorch/torch/lib/tmp_install -DCMAKE_EXPORT_COMPILE_COMMANDS=1 -DCMAKE_C_FLAGS= -DCMAKE_CXX_FLAGS= -DCMAKE_EXE_LINKER_FLAGS= -DCMAKE_SHARED_LINKER_FLAGS= -DWITH_ROCM=0
-- Autodetected CUDA architecture(s): 3.0 
-- Found CUDA with FP16 support, compiling with torch.CudaHalfTensor
-- Removing -DNDEBUG from compile flags
-- MAGMA not found. Compiling without MAGMA support
-- Could not find hardware support for NEON on this machine.
-- No OMAP3 processor on this machine.
-- No OMAP4 processor on this machine.
-- SSE2 Found
-- SSE3 Found
-- AVX Found
-- AVX2 Found
-- Atomics: using GCC intrinsics
-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m]
--   Library mkl_intel_lp64: /Users/hsr/anaconda3/envs/kate/lib/libmkl_intel_lp64.dylib
--   Library mkl_intel_thread: /Users/hsr/anaconda3/envs/kate/lib/libmkl_intel_thread.dylib
--   Library mkl_core: /Users/hsr/anaconda3/envs/kate/lib/libmkl_core.dylib
--   Library iomp5: /Users/hsr/anaconda3/envs/kate/lib/libiomp5.dylib
--   Library pthread: /usr/lib/libpthread.dylib
--   Library m: /usr/lib/libm.dylib
-- MKL library found
-- Found a library with BLAS API (mkl).
-- Found a library with LAPACK API. (mkl)
-- Found cuDNN: v7.0.4  (include: /usr/local/cuda/include, library: /usr/local/cuda/lib/libcudnn.7.dylib)
disabling MKLDNN because NO_MKLDNN is set
CMake Deprecation Warning at src/ATen/CMakeLists.txt:25 (CMAKE_POLICY):
  The OLD behavior for policy CMP0026 will be removed from a future version
  of CMake.

  The cmake-policies(7) manual explains that the OLD behaviors of all
  policies are deprecated and that a policy should be set to OLD only under
  specific short-term circumstances.  Projects should be ported to the NEW
  behavior and not rely on setting a policy to OLD.


-- Using python found in /Users/hsr/anaconda3/envs/kate/bin/python
-- TBB: using libc++.
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
-- Configuring build for SLEEF-v3.2
   Target system: Darwin-16.7.0
   Target processor: x86_64
   Host system: Darwin-16.7.0
   Host processor: x86_64
   Detected C compiler: AppleClang @ /Library/Developer/CommandLineTools/usr/bin/clang
-- Using option  to compile libsleef
-- Building shared libs : OFF
-- MPFR : LIB_MPFR-NOTFOUND
-- GMP : LIBGMP-NOTFOUND
-- RUNNING_ON_TRAVIS : 0
-- COMPILER_SUPPORTS_OPENMP : 
disable contrib because ATEN_NO_CONTRIB is set
-- Configuring done
-- Generating done
-- Build files have been written to: /Users/hsr/pytorch/aten/build
+ make install -j8
[  0%] Built target mkdisp
[  0%] Built target common
[  2%] Built target mkalias
[  2%] Built target mkrename
[  6%] Built target cpuinfo
[  6%] Built target cuda_aten_files_are_generated
[  6%] Built target aten_files_are_generated
Scanning dependencies of target tbb_static
[  6%] Built target mkmasked_gnuabi
[  6%] Built target arraymap
[  6%] Built target mkrename_gnuabi
[  6%] Built target renamedsp256.h_generated
[  7%] Built target dispavx.c_generated
[  8%] Built target headers
[  9%] Built target renameSSE2.h_generated
[  9%] Built target renameAVX.h_generated
[  9%] Built target renameFMA4.h_generated
[  9%] Built target renameSSE4.h_generated
[  9%] Built target renameAVX2.h_generated
[ 10%] Built target dispsse.c_generated
[ 10%] Built target renameAVX2128.h_generated
[ 10%] Built target renamedsp128.h_generated
Scanning dependencies of target sleefavx2
Scanning dependencies of target sleefavx2128
[ 11%] Built target sleefsse2
[ 11%] Built target dispavx_obj
[ 11%] Built target sleefavx
[ 11%] Built target sleeffma4
[ 11%] Built target sleefsse4
[ 11%] Building C object sleef/src/libm/CMakeFiles/sleefavx2.dir/sleefsimdsp.c.o
[ 11%] Building C object sleef/src/libm/CMakeFiles/sleefavx2.dir/sleefsimddp.c.o
Scanning dependencies of target dispsse_obj
[ 11%] Building C object sleef/src/libm/CMakeFiles/sleefavx2128.dir/sleefsimdsp.c.o
[ 12%] Building C object sleef/src/libm/CMakeFiles/sleefavx2128.dir/sleefsimddp.c.o
[ 12%] Building C object sleef/src/libm/CMakeFiles/dispsse_obj.dir/dispsse.c.o
[ 12%] Building CXX object src/ATen/cpu/tbb/CMakeFiles/tbb_static.dir/Users/hsr/pytorch/third_party/tbb/src/tbb/arena.cpp.o
[ 12%] Building CXX object src/ATen/cpu/tbb/CMakeFiles/tbb_static.dir/Users/hsr/pytorch/third_party/tbb/src/tbb/concurrent_vector.cpp.o
[ 13%] Building CXX object src/ATen/cpu/tbb/CMakeFiles/tbb_static.dir/Users/hsr/pytorch/third_party/tbb/src/tbb/condition_variable.cpp.o
In file included from /Users/hsr/pytorch/third_party/tbb/src/tbb/arena.cpp:23:
In file included from /Users/hsr/pytorch/third_party/tbb/src/tbb/scheduler.h:26:
/Users/hsr/pytorch/third_party/tbb/src/tbb/mailbox.h:102:52: error: unknown type
      name 'isolation_tag'
    task_proxy* internal_pop( __TBB_ISOLATION_EXPR(isolation_tag isolation) ) {
                                                   ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/mailbox.h:108:27: error: use of
      undeclared identifier 'no_isolation'; did you mean 'isolation'?
        if ( isolation != no_isolation ) {
                          ^~~~~~~~~~~~
                          isolation
/Users/hsr/pytorch/third_party/tbb/src/tbb/mailbox.h:102:66: note: 'isolation'
      declared here
    task_proxy* internal_pop( __TBB_ISOLATION_EXPR(isolation_tag isolation) ) {
                                                                 ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/mailbox.h:109:36: error: no member
      named 'isolation' in 'tbb::internal::task_prefix'
            while ( curr->prefix().isolation != isolation ) {
                    ~~~~~~~~~~~~~~ ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/mailbox.h:206:44: error: unknown type
      name 'isolation_tag'
    task_proxy* pop( __TBB_ISOLATION_EXPR( isolation_tag isolation ) ) {
                                           ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/concurrent_vector.cpp:123:9: error: 
      use of undeclared identifier 'enforce_segment_allocated'
        enforce_segment_allocated(s.load<relaxed>()); //it's hard to rec...
        ^
In file included from /Users/hsr/pytorch/third_party/tbb/src/tbb/arena.cpp:23:
/Users/hsr/pytorch/third_party/tbb/src/tbb/scheduler.h:237:43: error: unknown
      type name 'isolation_tag'
    task* get_task( __TBB_ISOLATION_EXPR( isolation_tag isolation ) );
                                          ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/concurrent_vector.cpp:188:13: error: 
      use of undeclared identifier 'enforce_segment_allocated'
            enforce_segment_allocated(s.load<relaxed>());
            ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/scheduler.h:246:31: error: unknown
      type name 'isolation_tag'
    task* get_task( size_t T, isolation_tag isolation, bool& tasks_omitted );
                              ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/scheduler.h:257:51: error: unknown
      type name 'isolation_tag'
    task* get_mailbox_task( __TBB_ISOLATION_EXPR( isolation_tag isolation ) );
                                                  ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/scheduler.h:265:75: error: unknown
      type name 'isolation_tag'
  ...steal_task( __TBB_ISOLATION_ARG( arena_slot& victim_arena_slot, isolatio...
                                                                     ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/concurrent_vector.cpp:273:9: error: 
      use of undeclared identifier 'enforce_segment_allocated'
        enforce_segment_allocated(array0); // initial segment should be ...
        ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/scheduler.h:366:115: error: unknown
      type name 'isolation_tag'
  ...__TBB_atomic reference_count& completion_ref_count, isolation_tag isolat...
                                                         ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/scheduler.h:424:47: error: unknown
      type name 'isolation_tag'
    task* reload_tasks( __TBB_ISOLATION_EXPR( isolation_tag isolation ) );
                                              ^
[ 13%] Building CXX object src/ATen/cpu/tbb/CMakeFiles/tbb_static.dir/Users/hsr/pytorch/third_party/tbb/src/tbb/critical_section.cpp.o
/Users/hsr/pytorch/third_party/tbb/src/tbb/scheduler.h:426:127: error: unknown
      type name 'isolation_tag'
  ...__TBB_ISOLATION_ARG( intptr_t top_priority, isolation_tag isolation ) );
                                                 ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/concurrent_vector.cpp:391:9: error: 
      use of undeclared identifier 'enforce_segment_allocated'
        enforce_segment_allocated(my_segment[k].load<relaxed>()); //if v...
        ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/scheduler.h:430:52: error: unknown
      type name 'isolation_tag'
    task* winnow_task_pool ( __TBB_ISOLATION_EXPR( isolation_tag isolation ) );
                                                   ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/scheduler.h:434:88: error: unknown
      type name 'isolation_tag'
  ...size_t H0 , __TBB_ISOLATION_ARG( size_t T0, isolation_tag isolation ) );
                                                 ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/concurrent_vector.cpp:409:13: error: 
      use of undeclared identifier 'enforce_segment_allocated'
            enforce_segment_allocated(my_segment[k].load<relaxed>());
            ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/concurrent_vector.cpp:466:9: error: 
      use of undeclared identifier 'enforce_segment_allocated'
        enforce_segment_allocated(my_segment[i].load<relaxed>());
        ^
6 errors generated.
make[2]: *** [src/ATen/cpu/tbb/CMakeFiles/tbb_static.dir/Users/hsr/pytorch/third_party/tbb/src/tbb/concurrent_vector.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
In file included from /Users/hsr/pytorch/third_party/tbb/src/tbb/arena.cpp:28:
/Users/hsr/anaconda3/envs/kate/include/tbb/internal/_flow_graph_impl.h:25:2: error: 
      Do not #include this internal file directly; use public TBB headers
      instead.
#error Do not #include this internal file directly; use public TBB heade...
 ^
/Users/hsr/anaconda3/envs/kate/include/tbb/internal/_flow_graph_impl.h:102:31: error: 
      use of undeclared identifier 'continue_msg'
    class function_body_leaf< continue_msg, continue_msg, B> : public fu...
                              ^
/Users/hsr/anaconda3/envs/kate/include/tbb/internal/_flow_graph_impl.h:125:38: error: 
      use of undeclared identifier 'continue_msg'
    class function_body_leaf< Input, continue_msg, B> : public function_...
                                     ^
/Users/hsr/anaconda3/envs/kate/include/tbb/internal/_flow_graph_impl.h:148:31: error: 
      use of undeclared identifier 'continue_msg'
    class function_body_leaf< continue_msg, Output, B > : public functio...
                              ^
/Users/hsr/anaconda3/envs/kate/include/tbb/internal/_flow_graph_impl.h:208:40: error: 
      unknown class name 'task'; did you mean 'tbb::task'?
    class forward_task_bypass : public task {
                                       ^~~~
                                       tbb::task
/Users/hsr/anaconda3/envs/kate/include/tbb/task.h:542:7: note: 'tbb::task'
      declared here
class task: __TBB_TASK_BASE_ACCESS interface5::internal::task_base {
      ^
In file included from /Users/hsr/pytorch/third_party/tbb/src/tbb/arena.cpp:28:
/Users/hsr/anaconda3/envs/kate/include/tbb/internal/_flow_graph_impl.h:218:29: error: 
      use of undeclared identifier 'SUCCESSFULLY_ENQUEUED'
            if (new_task == SUCCESSFULLY_ENQUEUED) new_task = NULL;
                            ^
fatal error: too many errors emitted, stopping now [-ferror-limit=]
[ 13%] Built target sleefavx2128
[ 14%] Built target sleefavx2
20 errors generated.
make[2]: *** [src/ATen/cpu/tbb/CMakeFiles/tbb_static.dir/Users/hsr/pytorch/third_party/tbb/src/tbb/arena.cpp.o] Error 1
[ 14%] Built target dispsse_obj
Scanning dependencies of target sleef
[ 15%] Building C object sleef/src/libm/CMakeFiles/sleef.dir/sleefdp.c.o
[ 15%] Building C object sleef/src/libm/CMakeFiles/sleef.dir/sleefsp.c.o
[ 15%] Building C object sleef/src/libm/CMakeFiles/sleef.dir/sleefld.c.o
make[1]: *** [src/ATen/cpu/tbb/CMakeFiles/tbb_static.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
[ 15%] Linking C static library ../../lib/libsleef.a
[ 15%] Built target sleef
make: *** [all] Error 2

Any help appreciated, thanks for your time. ",module: build,"['cc @cpuhrsch ', ""I'm currently in the process [of updating tbb](https://github.com/pytorch/pytorch/pull/7734). This also includes a build on Mac, however on 10.13, which appears to [succeed](https://ci.pytorch.org/jenkins/job/caffe2-builds/job/conda2-macos10.13-trigger-build/1833/)."", 'Is something preventing you from upgrading to 10.13 @HughRunyan?', 'I tried 10.13 first, it was failing in a similar manner, though not at\nexactly the same spot. I saw a few places online that people had been\nsuccessful with 10.12, so I installed a clean partition of that to work on.\n\nOn Mon, May 21, 2018 at 11:09 AM, cpuhrsch <notifications@github.com> wrote:\n\n> Is something preventing you from upgrading to 10.13 @HughRunyan\n> <https://github.com/HughRunyan>?\n>\n> ‚Äî\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/7721#issuecomment-390735924>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/APJZbdpPFMJL5GU9nyCEOw73xaLx4bBlks5t0wL1gaJpZM4UGVB0>\n> .\n>\n', ""We'll be better able to help you with 10.13 since our contbuild system and development platforms are based on 10.13. Is there a particular reason you chose 10.12 over 10.13?"", ""Just that it was older and seemed more likely to be supported. I will\nreturn to 10.13.4 and try that. I'm actually currently focusing on getting\na Linux 16.04 similarly-configured partition to work, but it's also\nfailing. Everything seems fine until I try to open Spyder in Anaconda -- it\ncomes up just black instead of the usual white-background boxes. Not black\nbackground -- fully, black, no visible text. Smells like a driver issue of\nsome kind...non-pytorch conda environments can run Spyder just fine. I'm\ncurrently trying building from source instead of the binaries. I'll switch\nback to OSX if this doesn't pan out. I should know any minute now.\n\nOn Mon, May 21, 2018 at 11:23 AM, cpuhrsch <notifications@github.com> wrote:\n\n> We'll be better able to help you with 10.13 since our contbuild system and\n> development platforms are based on 10.13. Is there a particular reason you\n> chose 10.12 over 10.13?\n>\n> ‚Äî\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/7721#issuecomment-390739802>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/APJZbUhG0j25fWcIucDEILPegEV_eBmeks5t0wY-gaJpZM4UGVB0>\n> .\n>\n"", 'Cuda9.1 + cudnn 7 + 10.13.4 + egpu works for me, gotta make sure the gcc version and the command are correct though.', ""I was using command line tools and xcode 8.2. I tried the former command\nline tools with xcode 9.3.1 as well? But I believe you are correct that the\nproblem is somewhere in the gcc universe. The card isn't external -- I have\ngeforce 750M GT onboard. It's not fast by modern GPU standards, but should\nbe a lot faster than CPU. It has nvidia compute power (or whatever exactly\nthey call it) 3.0.\n\nOn Tue, May 22, 2018 at 6:20 AM, Hongbin Liu <notifications@github.com>\nwrote:\n\n> Cuda9.1 + cudnn 7 + 10.13.4 + egpu works for me, gotta make sure the gcc\n> version and the command are correct though.\n>\n> ‚Äî\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/7721#issuecomment-390987042>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/APJZbevy1C29vosMMra6KLZ670oCsv1pks5t1BC0gaJpZM4UGVB0>\n> .\n>\n"", ""OK! I fresh installed OSX 10.13, used current Xcode and command line tools\n8.2, cuda 9.1 and cudnn 7.0.5. The package built! But I'm still\ngetting torch.cuda.is_available() --> False. Any ideas?\n\nOn Tue, May 22, 2018 at 7:36 AM, Hugh Runyan <hurunyan@ucsd.edu> wrote:\n\n> I was using command line tools and xcode 8.2. I tried the former command\n> line tools with xcode 9.3.1 as well? But I believe you are correct that the\n> problem is somewhere in the gcc universe. The card isn't external -- I have\n> geforce 750M GT onboard. It's not fast by modern GPU standards, but should\n> be a lot faster than CPU. It has nvidia compute power (or whatever exactly\n> they call it) 3.0.\n>\n> On Tue, May 22, 2018 at 6:20 AM, Hongbin Liu <notifications@github.com>\n> wrote:\n>\n>> Cuda9.1 + cudnn 7 + 10.13.4 + egpu works for me, gotta make sure the gcc\n>> version and the command are correct though.\n>>\n>> ‚Äî\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/pytorch/pytorch/issues/7721#issuecomment-390987042>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/APJZbevy1C29vosMMra6KLZ670oCsv1pks5t1BC0gaJpZM4UGVB0>\n>> .\n>>\n>\n>\n"", '@HughRunyan - During installation do you see an indication that CUDA was detected? Like ""Autodetected CUDA[...]""?', 'At some point late it says:\n\nDetected CUDA at /usr/local/cuda\n\n\nIt mentions it here and there, but I think the main spot you\'re looking for\nis:\n\n\n-- Automatic GPU detection failed. Building for common architectures.\n\n-- Autodetected CUDA architecture(s): 3.0;3.5;5.0;5.2;6.0;6.1;7.0;7.0+PTX\n\n-- Found CUDA with FP16 support, compiling with torch.CudaHalfTensor\n\n\nIt manages to find cuDNN fine far as I can tell.\n\n\nOn Tue, May 22, 2018 at 1:57 PM, cpuhrsch <notifications@github.com> wrote:\n\n> @HughRunyan <https://github.com/HughRunyan> - During installation do you\n> see an indication that CUDA was detected? Like ""Autodetected CUDA[...]""?\n>\n> ‚Äî\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/7721#issuecomment-391137995>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/APJZbVD75TXX6_BhM5VtuBMI7P86ESnLks5t1HuogaJpZM4UGVB0>\n> .\n>\n', 'It has been a while. @HughRunyan is this error still relevant?', 'i dont think so\n\nOn Fri, Nov 20, 2020 at 8:52 AM mrshenli <notifications@github.com> wrote:\n\n> It has been a while. @HughRunyan\n> <https://urldefense.com/v3/__https://github.com/HughRunyan__;!!Mih3wA!TJrwxl537qWfH4_6GMr4_QB3AqUeX-KYbKsOPcurvh-QAr2WwLT6h27mnCsuLvFx$>\n> is this error still relevant?\n>\n> ‚Äî\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://urldefense.com/v3/__https://github.com/pytorch/pytorch/issues/7721*issuecomment-731282020__;Iw!!Mih3wA!TJrwxl537qWfH4_6GMr4_QB3AqUeX-KYbKsOPcurvh-QAr2WwLT6h27mnGIhZO94$>,\n> or unsubscribe\n> <https://urldefense.com/v3/__https://github.com/notifications/unsubscribe-auth/ADZFS3N334BTPUTX4XJLPQDSQ2NF5ANCNFSM4FAZKB2A__;!!Mih3wA!TJrwxl537qWfH4_6GMr4_QB3AqUeX-KYbKsOPcurvh-QAr2WwLT6h27mnNy25xE3$>\n> .\n>\n\n\n-- \nHugh Runyan (he/him/his)\nSandin Coral Ecology Lab/Kuester Cultural Heritage Engineering Initiative\nScripps Institution of Oceanography/UCSD\n']",[],['-Wall -Wno-unused -Wno-attributes -Wno-unused-result -ffp-contract=off -fno-math-errno -fno-trapping-math'],0,0
179,pytorch,22050,closed,RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR in 1.1.0 ,"I am currently using Windows 7, Pytorch version 1.1.0, 
It seems from what I have read this error should have been fixed
in 1.1.0?

Here is the environment:

Collecting environment information...
PyTorch version: 1.1.0
Is debug build: No
CUDA used to build PyTorch: 9.0

OS: Microsoft Windows 7 Professional
GCC version: (tdm64-1) 5.1.0

CMake version: version 3.7.0-rc1


Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration: GPU 0: GeForce GTX 1070
Nvidia driver version: 411.31
cuDNN version: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\bin\cudnn64_7.dll

Versions of relevant libraries:
[pip3] numpy==1.16.4

[pip3] torch==1.1.0

[pip3] torchsummary==1.5.1

[pip3] torchvision==0.3.0
[conda] Could not collect


Here is the traceback:





I have multiple versions of CUDA installed for compatibility
with Tensorflow, Keras and Pytorch.


Thank you. 



The script I am running is the  script using 


https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html




",module: convolution module: cuda module: cudnn module: nn triaged,"['@Moondra  Based on discussions in #16831 and #21819, this seems to be fixed in cudnn 7.6. Can you try upgrade your cudnn?', 'Okay, will do and report back. ', ""I have multiple versions of CUDA so will need to do it on all of them. I did so on CUDA 9 (update cudnn), but so far no luck. I am assuming I need to do it on CUDA 10 as that's what's being used at runtime. "", ""I've updated it to 7.6 but I am still running into the same problem.\r\nhttps://developer.nvidia.com/rdp/cudnn-download\r\nFor Cuda 10.\r\nExtracted it and placed it in the CUDA 10 folder.\r\n\r\nAny thing else I can try? "", 'Hi @ngimel @mruberry, it seems the error fixed in #16831 occurs again. This is also reported in #21819. Do you have any thoughts on what might be the cause here?', '@ngimel @csarofeen since I‚Äôm no longer at Nvidia.', ""Hi @Moondra,\r\n\r\nthanks for reporting this issue.\r\nIt seems you are using PyTorch binaries with CUDA 9 and also installed CUDA 10.0.130 locally.\r\n\r\nIf that's correct, placing the cudnn files in the local CUDA10 folder won't change the used cudnn version in the PyTorch binaries. You would need to build PyTorch from source to use your local CUDA and cudnn.\r\n\r\nYou could check the currently used cudnn version via `torch.backends.cudnn.version()`.\r\n\r\n\r\n"", 'Hi ptrblck,\r\n\r\nI tried placing the cudnn file in CUDA9 directory as well (as far I can remember!) as I have multiple CUDA folders for compatibility with Keras (tf backend).   \r\n\r\nAt this path:\r\n""C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA""\r\n\r\nI have:\r\n\r\nv8.0\r\nv9.0\r\nv10.0\r\n\r\nI will try running the command you provided. Thank you so much for your help.', ""Creating a new comment just incase edits don't notify:\r\n\r\nI ran the command:\r\n\r\n7005"", ""@Moondra If you are using the PyTorch binaries, CUDA and cudnn will be shipped with these bins, so that you cannot simply copy the cudnn files to switch the version.\r\nE.g. if you are running\r\n```\r\nconda install pytorch-nightly cudatoolkit=10.0 -c pytorch\r\n```\r\nyou'll see something like\r\n```\r\nThe following NEW packages will be INSTALLED:\r\n\r\n  cudatoolkit        pkgs/main/linux-64::cudatoolkit-10.0.130-0\r\n  ninja              pkgs/main/linux-64::ninja-1.9.0-py37hfd86e86_0\r\n  pytorch-nightly    pytorch/linux-64::pytorch-nightly-1.2.0.dev20190805-py3.7_cuda10.0.130_cudnn7.6.2_0\r\n```\r\nwhich shows you that cudnn7.6.2_0 will be installed.\r\nCould you try to run this command and check, if the error is still thrown?"", ""So sorry for the late reply. I am not using Conda, so I am assuming pip will suffice. \r\nThank you.\r\n\r\nEdit if I do: `pip install pytorch-nightly cudatoolkit=10.0 -c pytorch`\r\n\r\nI get back \r\n` Could not open requirements file: [Errno 2] No such file or directory: 'pytorch`\r\n\r\nI can remove `pytorch` and try but I will wait for you instructions. "", 'Try to use this command from the [website](https://pytorch.org/get-started/locally/):\r\n```\r\npip3 install torch torchvision\r\n```\r\n\r\nThis should install PyTorch 1.2, CUDA10.0 for Python3.6 on a Linux OS.\r\nJust select whatever config matches your current setup and use the shown install command.', '> \r\n> \r\n> Try to use this command from the [website](https://pytorch.org/get-started/locally/):\r\n> \r\n> ```\r\n> pip3 install torch torchvision\r\n> ```\r\n> \r\n> This should install PyTorch 1.2, CUDA10.0 for Python3.6 on a Linux OS.\r\n> Just select whatever config matches your current setup and use the shown install command.\r\n\r\nSorry for the late reply. I have been juggling many things. \r\n\r\nIt seems to be working (initial tests). I will just need to run different scripts just to make sure.\r\nThank you so much for your patience.', 'Everything is working perfectly now. Thank you so much for your help and patience. ']",[],"['Traceback (most recent call last):\r\n  File ""C:\\Users\\Moondra\\Desktop\\PYTORCH\\PYTORCH BASICS\\PYTORCH ENVIRONMENTS\\PYTORCH STUFF\\pytorch_test.py"", line 191, in <module>\r\n    num_epochs=3)\r\n  File ""C:\\Users\\Moondra\\Desktop\\PYTORCH\\PYTORCH BASICS\\PYTORCH ENVIRONMENTS\\PYTORCH STUFF\\pytorch_test.py"", line 52, in train_model\r\n    outputs = model(inputs)\r\n  File ""C:\\Users\\Moondra\\Desktop\\PYTORCH\\PYTORC~1\\PYTORC~2\\test1\\lib\\site-packages\\torch\\nn\\modules\\module.py"", line 493, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""C:\\Users\\Moondra\\Desktop\\PYTORCH\\PYTORC~1\\PYTORC~2\\test1\\lib\\site-packages\\torchvision\\models\\densenet.py"", line 119, in forward\r\n    features = self.features(x)\r\n  File ""C:\\Users\\Moondra\\Desktop\\PYTORCH\\PYTORC~1\\PYTORC~2\\test1\\lib\\site-packages\\torch\\nn\\modules\\module.py"", line 493, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""C:\\Users\\Moondra\\Desktop\\PYTORCH\\PYTORC~1\\PYTORC~2\\test1\\lib\\site-packages\\torch\\nn\\modules\\container.py"", line 92, in forward\r\n    input = module(input)\r\n  File ""C:\\Users\\Moondra\\Desktop\\PYTORCH\\PYTORC~1\\PYTORC~2\\test1\\lib\\site-packages\\torch\\nn\\modules\\module.py"", line 493, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""C:\\Users\\Moondra\\Desktop\\PYTORCH\\PYTORC~1\\PYTORC~2\\test1\\lib\\site-packages\\torch\\nn\\modules\\conv.py"", line 338, in forward\r\n    self.padding, self.dilation, self.groups)\r\nRuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR', 'transfer learning', 'Densenet']",0,0
180,pytorch,14373,closed,[c10d] Barrier to synchronize with previously kicked off work in ProcessGroupGloo,In #14294 there is a failing test for this behavior (and it is therefore skipped). The stack that that PR is a part of removes deprecated code and does a few simplifications. It is easier to change the behavior to synchronize more strongly after the stack of #14294 is merged.,oncall: distributed,['This is addressed in #14386.'],[],[],0,0
181,pytorch,16027,closed,More readable error message for index error of nn.Embedding  in CUDA,"## üöÄ Feature
More readable error message for index error of nn.Embedding  in CUDA

## Motivation

If nn.Embedding received a tensor containing larger values than , raise an error as follows:



However, if nn.Embedding exists in CUDA, raises error message as follows:



This error message is too hard to know why the error is occurred.
I spent a lot of time to find out the reason, so I created this feature request.",,"[""It's complicated; if we do specific error checks then the code becomes much slower because we'd have to launch a kernel just to do the error checking.""]","['\r\nRuntimeError: index out of range at ...\r\n', '\r\nRuntimeError: cuda runtime error (59) : device-side assert triggered ...\r\n']",['num_embeddings'],0,0
182,pytorch,4655,closed,Error with cuda9 docker build,"When building pytorch with docker with 


I get the following error and the build stops:


I cloned the latest master with 
Ubuntu 16.04
Docker version 17.09.1-ce, build 19e2cf6",,['Duplicate of #3542'],[],"['docker build -t pytorch_cuda9 -f tools/docker/Dockerfile9 .', ""Step 10/14 : RUN git submodule update --init\r\n ---> Running in 34e6a586e55a\r\nfatal: Not a git repository: /home/ubuntu/pytorch/.git/modules/torch/lib/gloo\r\nUnable to find current revision in submodule path 'torch/lib/gloo'\r\nThe command '/bin/sh -c git submodule update --init' returned a non-zero code: 1\r\n"", 'git clone --recursive https://github.com/pytorch/pytorch']",0,0
183,pytorch,23466,closed,import failure (macOS),"## üêõ Bug
I was using pytorch without any issue, but I just found out today that when importing torch, it threw me an error. Possibly regarding MKL library. I could not figure out why.

## To Reproduce

Steps to reproduce the behavior:

1. 
1.
1.
Below are the error message:


## Expected behavior

Clearly, it should not be happening.

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:



 - PyTorch Version (e.g., 1.0): 1.1.0 (although not shown above, this is the version I installed from conda)
 - OS (e.g., Linux):
 - How you installed PyTorch (, , source):
 - Build command you used (if compiling from source):
 - Python version:
 - CUDA/cuDNN version:
 - GPU models and configuration:
 - Any other relevant information:

## Additional context

I have no idea what happened but probably because I was using julia and trying to install ubuntu on my Mac. Maybe somehow I messed up with my environment?
",,"['After searching on google I found that it is possibly because I add the following into `~/.bash_profile`:\r\n`export LD_LIBRARY_PATH=/opt/arrayfire/lib:$LD_LIBRARY_PATH`.\r\n\r\nI added this line when trying to use ArrarFire for julia. After commenting it out, everything works fine now.\r\n\r\nThank you!', 'My workaround\r\n` $ conda activate [MYENV]`\r\n`$ pip uninstall torch  `\r\n`$ pip install torch   `\r\n']","['\r\nPython 3.7.3 | packaged by conda-forge | (default, Mar 27 2019, 15:43:19) \r\n[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import torch\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/anaconda3/lib/python3.7/site-packages/torch/__init__.py"", line 79, in <module>\r\n    from torch._C import *\r\nImportError: dlopen(/anaconda3/lib/python3.7/site-packages/torch/_C.cpython-37m-darwin.so, 9): Symbol not found: _mkl_serv_inspector_loaded\r\n  Referenced from: /anaconda3/lib/python3.7/site-packages/torch/lib/../../../../libmkl_intel_thread.dylib\r\n  Expected in: flat namespace\r\n in /anaconda3/lib/python3.7/site-packages/torch/lib/../../../../libmkl_intel_thread.dylib\r\n>>> \r\n', '\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n', '\r\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\n\r\nOS: Mac OSX 10.14.5\r\nGCC version: Could not collect\r\nCMake version: version 3.14.5\r\n\r\nPython version: 3.7\r\nIs CUDA available: N/A\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.4\r\n[conda] mkl                       2019.4                      233  \r\n[conda] pytorch                   1.1.0                   py3.7_0    pytorch\r\n[conda] torchvision               0.3.0             py37_cuNone_1    pytorch\r\n']","['import torch', 'conda', 'pip']",0,0
184,pytorch,7857,open,Feature Request: Logistic Distribution,"Could we have logistic distribution in PyTorch?

https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/contrib/distributions/python/ops/logistic.py

cc @fritzo @neerajprad @alicanb @nikitaved @brianjo @mruberry",feature module: distributions triaged,"['I think you can build this using the existing transforms:\r\n\r\nX ~ Exponential(1) => a + b * log(exp(X) - 1) ~ Logistic(a, b)\r\n\r\ncc: @fritzo ', ""We decided not to add this in #4833 since it's so easy to implement using `TransformedDistribution`:\r\n```py\r\nTransformedDistribution(Uniform(0, 1), [SigmoidTransform().inv, AffineTransform(a, b)])\r\n```\r\nMaybe it's worth adding this as an example in the `TransformedDistribution` docs?"", 'I feel like there should be a warning/reminder along with this example that the distribution will sometimes return -inf when Uniform returns 0.', ""> We decided not to add this in #4833 since it's so easy to implement using `TransformedDistribution`:\r\n> \r\n> ```python\r\n> TransformedDistribution(Uniform(0, 1), [SigmoidTransform().inv, AffineTransform(a, b)])\r\n> ```\r\n> \r\n> Maybe it's worth adding this as an example in the `TransformedDistribution` docs?\r\n\r\nSorry to comment on an old and closed FR but I can't work out how to get this working in pyro.\r\nI ran this snippet:\r\n ```python\r\ndef Logistic(a,b):\r\n    return TransformedDistribution(Uniform(0, 1), [SigmoidTransform().inv, AffineTransform(a, b)])\r\n```\r\nwhich is ok, but later in a model i get the error `TransformedDistribution' object is not callable`. \r\nDo I need to make a `Logistic` class which inherits from `TransformedDistribution`?\r\nOr is there some torch to pyro magic to do this?"", ""In case anyone else arrives from google, I was importing from torch, not pyro which caused the errors:\r\n\r\nBased on @fritzo's transformations\r\n```python\r\nfrom pyro.distributions import Uniform\r\nfrom pyro.distributions.torch import TransformedDistribution\r\nfrom pyro.distributions.transforms import SigmoidTransform, AffineTransform\r\ndef Logistic(a,b):\r\n    return TransformedDistribution(Uniform(0, 1), [SigmoidTransform().inv, AffineTransform(a,b)])\r\n```\r\nUsage inside a model: `x = pyro.sample('x', Logistic(torch.ones(1), torch.ones(1)))`"", ""That's right, Pyro distributions also inherit from Pyro's [TorchDistributionMixin](http://docs.pyro.ai/en/stable/distributions.html#pyro.distributions.torch_distribution.TorchDistributionMixin) class which adds Pyro-specific functionality such as a `.__call__()` method and `.score_parts()` method for ELBO computations. When you import `TransformedDistribution` from Pyro you get a class that derives from both the `TransformedDistribution` in torch and the mixin class."", ""> We decided not to add this in #4833 since it's so easy to implement using `TransformedDistribution`:\r\n> \r\n> ```python\r\n> TransformedDistribution(Uniform(0, 1), [SigmoidTransform().inv, AffineTransform(a, b)])\r\n> ```\r\n> \r\n> Maybe it's worth adding this as an example in the `TransformedDistribution` docs?\r\n\r\n\r\nsorry I don't get why you won't add something because it is easy. you might even just provide a constant like \r\n```\r\nLogisticDistribution = TransformedDistribution(Uniform(0, 1), [SigmoidTransform().inv, AffineTransform(a, b)])\r\n```\r\n\r\nthis way we as user can always be sure it is supported and works as expected. everything we handcrafted can contain a potential bug we have introduced into our model. "", '@KIC point taken, feel free to add a simple `class Logistic(TransformedDistribution)` together with declared `.support`, docs, and tests in test/distributions/test_distributions.py.', 'Looks like after all it is not so easy. Check this example:\r\n\r\n```python\r\nfrom torch.distributions import Uniform, SigmoidTransform, AffineTransform, TransformedDistribution, MixtureSameFamily\r\n\r\ndef Logistic(loc, scale):\r\n    return TransformedDistribution(Uniform(0, 1), [SigmoidTransform().inv, AffineTransform(loc, scale)])\r\n\r\ncat = t.rand(10, 1)\r\ndist = MixtureSameFamily(\r\n    Categorical(probs=t.cat([1-cat, cat], dim=1)),\r\n    Logistic(loc=t.randn(10, 2), scale=t.ones(10, 2))\r\n)\r\n\r\nprint(dist.sample())\r\n```\r\n\r\nthis errors out\r\n```\r\n~/.local/lib/python3.8/site-packages/torch/distributions/mixture_same_family.py in __init__(self, mixture_distribution, component_distribution, validate_args)\r\n     75         # Check that the number of mixture component matches\r\n     76         km = self._mixture_distribution.logits.shape[-1]\r\n---> 77         kc = self._component_distribution.batch_shape[-1]\r\n     78         if km is not None and kc is not None and km != kc:\r\n     79             raise ValueError(""`mixture_distribution component` ({0}) does not""\r\n\r\nIndexError: tuple index out of range\r\n```\r\n\r\nhowever, we can work around this issue like so:\r\n```python\r\ndef Logistic(loc, scale):\r\n    return TransformedDistribution(\r\n        Uniform(t.zeros(loc.shape), t.ones(loc.shape)),\r\n        [SigmoidTransform().inv, AffineTransform(loc, scale)]\r\n    )\r\n```\r\n\r\nI am not sure if this solution covers all cases we can have. But it is a good example of why it is good to offer even ""easy"" solutions within the library/framework. If you can confirm that this covers all cases I am more than happy to provide a PR.\r\n\r\n\r\n\r\n', '@KIC looks good to me, contribution welcome!  Actually I think it would be best to add a whole new subclass of TransformedDistribution, similar to [Pareto](https://github.com/pytorch/pytorch/blob/ef13341a8d1128a5fbd54716c89358cbd20c0a1d/torch/distributions/pareto.py#L30), that way it can provide metadata and properties like `.mean`:\r\n\r\n<details>\r\n\r\n```py\r\nclass Logistic(TransformedDistribution):\r\n    """"""\r\n    TODO add docs\r\n    """"""\r\n    arg_constraints = {""loc"": constraints.real, ""scale"": constraints.positive}\r\n    support = constraints.real\r\n    has_rsample = True\r\n\r\n    def __init__(self, loc, scale, validate_args=None):\r\n        self.loc, self.scale = broadcast_all(loc, scale)\r\n        base_dist = Uniform(self.loc.new_zeros(self.loc.shape),\r\n                            self.scale.new_zeros(self.scale.shape))\r\n        transforms = [SigmoidTransform().inv, AffineTransform(self.loc, self.scale)]\r\n        super().__init__(base_dist, transforms, validate_args=validate_args)\r\n\r\n    def expand(self, batch_shape):\r\n        # TODO copy from another transformed distribution, e.g. Pareto\r\n\r\n    @property\r\n    def mean(self):\r\n       return self.loc\r\n\r\n    @property\r\n    def variance(self):\r\n        return (math.pi ** 2 / 3) * self.scale ** 2\r\n```\r\n\r\n</details>']",[],[],0,0
185,pytorch,28902,closed,"C++ `torch::tensor` by default gives a double tensor, which is different from Python `torch.tensor` behavior","Example:
In C++:


In Python:


cc @yf225",module: cpp triaged,"['It is because C++ does implicit conversion from {1., 2., 3.} to double type.  \r\nIf we force input is float, it will give your float tensor.\r\n\r\nFor example:\r\n```c++\r\n    auto t = torch::tensor({float(1.), float(2.), float(3.)});\r\n    std::cout <<""data type is "" << t.dtype().name() <<std::endl;\r\n```\r\nit will print ""float"" data type.   ', '@maxwillzq Thanks for the investigation! This should already be fixed by https://github.com/pytorch/pytorch/pull/29632.']","['cpp\r\ntorch::tensor({1., 2., 3.}).dtype() -> double\r\n', 'python\r\n>>> torch.tensor([1., 2., 3.]).dtype\r\ntorch.float32\r\n']",[],0,0
186,pytorch,27793,closed,[quantization] QNNPACK Engine for Dynamic Quantization,"We should support QNNPACK for dynamic quantization.

- [ ] RNN classes
- [ ] RNNCell classes
- [ ] Linear

cc @jerryzh168 @jianyuh @dzhulgakov",oncall: quantization triaged,"['Can we list the use-cases where this is needed to help prioritize this work?', '@supriyar this is a fairly old issue relating to dynamic quant on qnnpack, feel free to close if the work is done', 'Yeah we do support dynamic quantization on mobile now using qnnpack backend. PR 32479 and 32757 added this.']",[],[],0,0
187,pytorch,24062,closed,[quant] use qconfig_dict in graph mode,"## üöÄ Feature

Extend the initial implementation to use qconfig_dict,
Need scoping support, cc @ZolotukhinM",quantization_release_1.3 triaged,"[""PR for adding Scopes: #21939 \r\n\r\nBut there is also an alternative now. We need the scope info to recover the info we're losing during inlining. Instead of recovering this info (which is done in the PR above), we can disable inlining. That's also planned and @suo is currently working on it. Without inlining we would be always  able to figure out the parent Function/Module for any node.\r\n"", 'this is done, but we want to support non-scriptable observer as well']",[],[],0,0
188,pytorch,27295,closed,Documentation of which torch.tensor operations work on QTensors,Create a table listing which methods work on the quantized tensors (only a subset of the tensor methods work). ,module: docs quantization_release_1.3 triaged,"[""in discussion today we decided that this should be broader and be a list of all the functions that quantization does or doesn't work with. "", 'the example that folks referenced in the meeting was \r\n\r\nhttps://pytorch.org/docs/master/onnx.html#supported-operators', 'any progress on this list of operations? @raghuramank100 ?', ""this is done. Thanks to all the folks who worked on this list @dzhulgakov @z-a-f @raghuramank100 (and possibly others, I don't recall). ""]",[],[],0,0
189,pytorch,25512,closed,Can't import torch after installing with Conda on Ubuntu,"## üêõ Bug

I installed CUDA and CUDNN via the following tutorial: (https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html), then,
I installed pytorch on Ubuntu 18.04 according to the pytorch website, selecting stable (1.2), Linux, Conda, Python3.7, CUDA10.0, using a virtualenv.  Then, when I go to import:

>>> import torch
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'torch'

I rebooted, made sure my PATH variable included anaconda:
 /home/dan/anaconda3/bin:/home/dan/anaconda3/envs/torch_1-2-0/bin:/home/dan/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda-10.1/bin:/usr/local/cuda-10.1/bin

I'm sure that it is pointing to the correct Python: alias python='/home/dan/anaconda3/bin/python'

Upon seeing some advice in a different ticket to ""conda upgrade cudatoolkit"", I did that, and didn't work.

I then decided maybe since my CUDA version is 10.1, I would alter the install command (conda install pytorch torchvision cudatoolkit=10.0 -c pytorch) to say cudatoolkit10.1.  That rolled pytorch back to 1.0, CUDA to 9.0, and generally didn't work, so I put everything back after that.

 - PyTorch Version (e.g., 1.0): 1.2
 - OS (e.g., Linux): Ubuntu 18.04
 - How you installed PyTorch (, , source): conda install pytorch torchvision cudatoolkit=10.0 -c pytorch (from the correct virtualenv)
 - Python version: 3.7.3, Conda version 4.6.11
 - CUDA/cuDNN version: 10.1 / 7.6.3
 - GPU models and configuration: NVIDIA 2080 TI RTX
 - Any other relevant information:  conda list:
# Name                    Version                   Build  Channel
cudatoolkit               10.1.168                      0  
pytorch                   1.2.0           cpu_py37h00be3c6_0  
torchvision               0.4.0           cuda100py37hecfc37a_0

Any help would be greatly appreciated.",,['User error.  Aliased python to anaconda3/bin/python instead of letting the virtual environment do its magic in deciding which interpreter to run.  Sorry!'],[],"['conda', 'pip']",0,0
190,pytorch,3395,closed,dlopen libnvrtc instead of dynamically linking against it,"Starting from commit https://github.com/pytorch/pytorch/commit/50e51eaa7fa5c252c2f4508cb5984734050177f1 we are linking against nvrtc and libcuda on CUDA builds. 
This has two consequences:
- pytorch CUDA builds can only be done on GPU-enabled machines, because libcuda.so comes with the nvidia driver
- the pytorch binaries with libcuda.so link dependency cant be run on CPU-only machines.

We need to fix this, and instead dlopen nvrtc, so that we can ship the next set of binaries to run on CPU / CUDA machines (just like our previous releases).",,"['cc: @ngimel ', 'We are building inside docker, which is technically not GPU-enabled. If toolkit is installed (which it can be without driver), libcuda is found in stubs and build proceeds normally. ', '@ngimel i cant ship the libcuda stub in binaries, can I?', 'Probably not, so the second problem still remains. ', ""according to https://github.com/pytorch/pytorch/pull/3455#issuecomment-341690170 it's still not working for CUDA9"", 'My setup details in #3455: Arch Linux, CUDA 9.0.176, cuDNN 7.0.3, gcc 6.4.1, built without docker or virtualenv, by running python setup.py install --user. Just in case you need it.', '@xdever i finally reproduced it on an ArchLinux docker, looking...', '@xdever fixed on master']",[],[],0,0
191,pytorch,25117,closed,Nvidia driver & cudatoolkit installed properly but check_driver fails,"I have successfully installed NVIDIA driver & cudatoolkit via conda. However, I am not able to use cuda in pytorch (even though it installed successfully). 

Previously, I was using Pytorch with CUDA 8.0, and wanted to upgrade. I removed / purge all CUDA through:


 
Then I updated my Nvidia drivers to 4.10 via PPA (Ubuntu 16.04):



Everything worked smoothly. The output of :


The output of 


Since I wanted conda to manage my CUDA version, I installed the cudatoolkit through conda env (python 3.6):



again, everything installs perfectly. When I run:

but using cuda fails. I get the following error message




I restarted, removed all irrelevant environment variables which may have caused issues (LD_LIBRARY_PATH), removed conda, reinstalled, tried cuda 9.2, but nothing works. I am not sure what the issue could be. Any ideas?

I searched a bit, and found [this](https://discuss.pytorch.org/t/found-no-nvidia-driver-on-your-system-but-its-there/35063) pytorch thread. Since I completely removed CUDA from my system this shouldn't be the problem, but I think somehow it may be related. 

**EDIT:**
It isn't surprising given my error, but following [this issue](https://github.com/pytorch/pytorch/issues/4546), I checked:
",,"['Probably this question would be better asked at [discuss.pytorch.org](https://discuss.pytorch.org/).\r\n\r\nIt is strange that `nvidia-smi` shows `CUDA Version: N/A`?', 'You could try to install nvidida-driver-418 (or 415) from ppa. If that does not work you could try install using the deb or runfile installers from the nvidia website.', 'closing in favor of forums', '> Probably this question would be better asked at [discuss.pytorch.org](https://discuss.pytorch.org/).\r\n> \r\n> It is strange that `nvidia-smi` shows `CUDA Version: N/A`?\r\n\r\nhttps://forums.developer.nvidia.com/t/nvidia-smi-doesnt-show-cuda-version-even-after-installation/68738 this is the reason but still confusing why ']","['\r\nsudo apt-get --purge remove cuda\r\nsudo apt-get autoremove\r\ndpkg --list |grep ""^rc"" | cut -d "" "" -f 3 | xargs sudo dpkg --purge\r\n', '\r\nsudo add-apt-repository ppa:graphics-drivers/ppa\r\nsudo apt-get update\r\nsudo apt-get install nvidia-410\r\n', '\r\nFri Aug 23 22:29:48 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 410.78       Driver Version: 410.78       CUDA Version: N/A      |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |\r\n| 25%   35C    P8    13W / 250W |    531MiB / 11177MiB |      1%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|===https://stackoverflow.com/questions/54264338/why-does-pytorch-not-find-my-nvdia-drivers-for-cuda-support==========================================================================|\r\n|    0      1445      G   /usr/lib/xorg/Xorg                           317MiB |\r\n|    0      2035      G   compiz                                       101MiB |\r\n|    0      3572      G   ...uest-channel-token=13099850080781834209   110MiB |\r\n+-----------------------------------------------------------------------------+\r\n', '\r\nNVRM version: NVIDIA UNIX x86_64 Kernel Module  410.78  Sat Nov 10 22:09:04 CST 2018\r\nGCC version:  gcc version 4.9.4 (Ubuntu 4.9.4-2ubuntu1~16.04)\r\n', '\r\nprint(torch.cuda.device_count()) # --> 0\r\nprint(torch.version.cuda) # --> 10.0.130\r\n', '\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/rana/anaconda3/envs/py36torch12cu10/lib/python3.6/site-packages/torch/cuda/__init__.py"", line 178, in _lazy_init\r\n    _check_driver()\r\n  File ""/home/rana/anaconda3/envs/py36torch12cu10/lib/python3.6/site-packages/torch/cuda/__init__.py"", line 99, in _check_driver\r\n    http://www.nvidia.com/Download/index.aspx"""""")\r\nAssertionError: \r\nFound no NVIDIA driver on your system. Please check that you\r\nhave an NVIDIA GPU and installed a driver from\r\nhttp://www.nvidia.com/Download/index.aspx\r\n']","['nvidia-smi', 'cat /proc/driver/nvidia/version', 'conda install pytorch torchvision cudatoolkit=10.0 -c pytorch', 'torch._C._cuda_getDriverVersion() # -> 0']",0,0
192,pytorch,27352,closed,torch quant docs all showing up in the index.html,"When I checkout the [ branch](https://github.com/gottbrath/pytorch/tree/quantization_1_3_doc/docs), run make html from /docs, and look at the index.html, all the old quantization docs show up even though the [index.rst ](https://github.com/gottbrath/pytorch/blob/quantization_1_3_doc/docs/source/index.rst)lists quantization.rst. 

Did some testing and research and it looks like it's because in quantization.rst there are multiple H1 headers. We would need to decrease all the headers except the first by one level. 

![image](https://user-images.githubusercontent.com/8042156/66173258-1b130600-e604-11e9-914c-7f3e2b874aa7.png)
",module: docs quantization_release_1.3 triaged,['Corrected in this PR: https://github.com/gottbrath/pytorch/pull/10'],[],['quantization_1_3_doc'],0,0
193,pytorch,15866,closed,Casting to and from at::Half on CPU is not supported yet,"Traceback (most recent call last):
  File ""/home/tcl/PycharmProjects/caffe2/mnist_caffe2.py"", line 278, in <module>
    workspace.CreateNet(train_model.net, overwrite=False)
  File ""/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py"", line 171, in CreateNet
    StringifyProto(net), overwrite,
  File ""/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py"", line 197, in CallWithExceptionIntercept
    return func(*args, **kwargs)
RuntimeError: [enforce fail at cast_op.cc:69] . Casting to and from at::Half on CPU is not supported yet
frame #0: c10::ThrowEnforceNotMet(char const*, int, char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, void const*) + 0x78 (0x7f234572e408 in /usr/local/lib/python2.7/dist-packages/caffe2/python/../../torch/lib/libc10.so)
frame #1: <unknown function> + 0x1636612 (0x7f2346f74612 in /usr/local/lib/python2.7/dist-packages/caffe2/python/../../torch/lib/libcaffe2.so)
frame #2: <unknown function> + 0x163e3c8 (0x7f2346f7c3c8 in /usr/local/lib/python2.7/dist-packages/caffe2/python/../../torch/lib/libcaffe2.so)
frame #3: <unknown function> + 0x163e55e (0x7f2346f7c55e in /usr/local/lib/python2.7/dist-packages/caffe2/python/../../torch/lib/libcaffe2.so)
frame #4: std::_Function_handler<std::unique_ptr<caffe2::OperatorBase, std::default_delete<caffe2::OperatorBase> > (caffe2::OperatorDef const&, caffe2::Workspace*), std::unique_ptr<caffe2::OperatorBase, std::default_delete<caffe2::OperatorBase> > (*)(caffe2::OperatorDef const&, caffe2::Workspace*)>::_M_invoke(std::_Any_data const&, caffe2::OperatorDef const&, caffe2::Workspace*&&) + 0x23 (0x7f2348971803 in /usr/local/lib/python2.7/dist-packages/caffe2/python/caffe2_pybind11_state.so)
frame #5: <unknown function> + 0x13efc1c (0x7f2346d2dc1c in /usr/local/lib/python2.7/dist-packages/caffe2/python/../../torch/lib/libcaffe2.so)
frame #6: <unknown function> + 0x13f2aaa (0x7f2346d30aaa in /usr/local/lib/python2.7/dist-packages/caffe2/python/../../torch/lib/libcaffe2.so)
frame #7: caffe2::CreateOperator(caffe2::OperatorDef const&, caffe2::Workspace*, int) + 0x3b9 (0x7f2346d30f29 in /usr/local/lib/python2.7/dist-packages/caffe2/python/../../torch/lib/libcaffe2.so)
frame #8: caffe2::SimpleNet::SimpleNet(std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*) + 0x295 (0x7f2346d27b75 in /usr/local/lib/python2.7/dist-packages/caffe2/python/../../torch/lib/libcaffe2.so)
frame #9: <unknown function> + 0x13ebf2e (0x7f2346d29f2e in /usr/local/lib/python2.7/dist-packages/caffe2/python/../../torch/lib/libcaffe2.so)
frame #10: <unknown function> + 0x13ce563 (0x7f2346d0c563 in /usr/local/lib/python2.7/dist-packages/caffe2/python/../../torch/lib/libcaffe2.so)
frame #11: caffe2::CreateNet(std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*) + 0x47d (0x7f2346cffc4d in /usr/local/lib/python2.7/dist-packages/caffe2/python/../../torch/lib/libcaffe2.so)
frame #12: caffe2::Workspace::CreateNet(std::shared_ptr<caffe2::NetDef const> const&, bool) + 0xfd (0x7f2346d6e1dd in /usr/local/lib/python2.7/dist-packages/caffe2/python/../../torch/lib/libcaffe2.so)
frame #13: caffe2::Workspace::CreateNet(caffe2::NetDef const&, bool) + 0x91 (0x7f2346d6f371 in /usr/local/lib/python2.7/dist-packages/caffe2/python/../../torch/lib/libcaffe2.so)
frame #14: <unknown function> + 0x4e616 (0x7f234896a616 in /usr/local/lib/python2.7/dist-packages/caffe2/python/caffe2_pybind11_state.so)
frame #15: <unknown function> + 0x4e8dc (0x7f234896a8dc in /usr/local/lib/python2.7/dist-packages/caffe2/python/caffe2_pybind11_state.so)
frame #16: <unknown function> + 0x86831 (0x7f23489a2831 in /usr/local/lib/python2.7/dist-packages/caffe2/python/caffe2_pybind11_state.so)
frame #17: PyEval_EvalFrameEx + 0x695b (0x55e76d2878db in /usr/bin/python2.7)
frame #18: PyEval_EvalCodeEx + 0x6da (0x55e76d27ed0a in /usr/bin/python2.7)
frame #19: PyEval_EvalFrameEx + 0x5cb8 (0x55e76d286c38 in /usr/bin/python2.7)
frame #20: PyEval_EvalCodeEx + 0x6da (0x55e76d27ed0a in /usr/bin/python2.7)
frame #21: PyEval_EvalFrameEx + 0x567e (0x55e76d2865fe in /usr/bin/python2.7)
frame #22: PyEval_EvalCodeEx + 0x6da (0x55e76d27ed0a in /usr/bin/python2.7)
frame #23: PyEval_EvalCode + 0x19 (0x55e76d27e629 in /usr/bin/python2.7)
frame #24: <unknown function> + 0x12461f (0x55e76d2af61f in /usr/bin/python2.7)
frame #25: PyRun_FileExFlags + 0x82 (0x55e76d2aa322 in /usr/bin/python2.7)
frame #26: PyRun_SimpleFileExFlags + 0x18d (0x55e76d2a967d in /usr/bin/python2.7)
frame #27: Py_Main + 0x68b (0x55e76d2581ab in /usr/bin/python2.7)
frame #28: __libc_start_main + 0xe7 (0x7f2367bf4b97 in /lib/x86_64-linux-gnu/libc.so.6)
frame #29: _start + 0x2a (0x55e76d257a2a in /usr/bin/python2.7)",,[],[],[],0,0
194,pytorch,8181,closed,"Stop using ""from pip import main""",See https://github.com/pytorch/pytorch/pull/7411 for backstory,,"[""I don't think we have ever used this in our own code. I would even doubt we have ever directly imported pip anywhere :-)"", ""Yes it seems to be the case. I wonder which code we're using has this problem.""]",[],[],0,0
195,pytorch,11907,closed,Please rename package pytorch to stay consistent with entire python world,"Hi,
 Like many, I installed pytorch, which says it is installing the package 'pytorch' and yet it is called 'torch' when you attempt to import it from python.

This cuts against the standard practices of the entire python world. Please remedy.

thanks!",,"['While I agree with the principle, pytorch is by no mean the only offender (scikit-learn come to mind) and that kind of changes would break a lot of code.\r\nThen again, maybe such breakage would be manageable within the context of 1.0 migration.', ""Yes. Scikit with it's annoying and unnecessary hyphens is also an issue. "", ""in `pypi`, which is the standard practice in the python world, we call it `torch` (`pip install torch`), but in `conda`, which is a multi-language repo, we call it `pytorch`.\r\n\r\nIt's unlikely that we will change this.""]",[],[],0,0
196,pytorch,17882,closed,ProcessGroupNCCL error/timeout handling,"As of NCCL 2.4 there are functions to detect I/O errors and abort running kernels. This is required to implement timeouts and force workers to raise an error or terminate when other workers fail.

See https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/api/comms.html and  and  and https://devblogs.nvidia.com/massively-scale-deep-learning-training-nccl-2-4/ for an example.",oncall: distributed triaged,"['@pietern Should we close this out? @mrshenli created a follow up for error handling within the server: https://github.com/pytorch/pytorch/issues/24906.', '@pritamdamania87 Yep, sounds good.']",[],"['ncclCommAbort', 'ncclCommGetAsyncError']",0,0
197,pytorch,1280,closed,Multi-GPU forward pass fails if first GPU id is not 0,"I can specify multiple GPU IDs and train a network using them just fine. However, when it comes time to run a validation dataset through the trained network, pytorch throws an error when using a list of GPU IDs unless the GPU ID list starts with id 0.

Here's a sample function that will cause an error:



If I train the network using  the function above executes with no problem.  However, if I train the network using  it will throw an error.",,"['try `CUDA_VISIBLE_DEVICES=1,2,3 python train.py`', 'When I try that I get an error:\r\n`RuntimeError: cuda runtime error (10) : invalid device ordinal at torch/csrc/cuda/Module.cpp:84`', 'you dont have 4 GPUs, looks like it. \r\n\r\n```\r\nCUDA_VISIBLE_DEVICES=1,2,3 python train.py\r\n```\r\nThis means it will try to use GPU2, 3, 4\r\nChange this appropriately depending on the number of GPUs you have.\r\nYou can check number of GPUs with the command `nvidia-smi`\r\n', 'Hmm I most certainly do have four GPUs, as the network trains on GPUs 1,2,3 just fine (per my original post).  The error has to do with something in the torch/pytorch pipeline assuming the wrong GPU when trying to load validation data (unless I misunderstand how to load the data onto GPU1, which the method above is supposed to demonstrate).\r\n\r\n`RuntimeError: all tensors must be on devices[0]`\r\n\r\nTo wit:\r\n```\r\n$ nvidia-smi\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1080    Off  | 0000:02:00.0     Off |                  N/A |\r\n| 24%   32C    P8    12W / 180W |    320MiB /  8113MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 1080    Off  | 0000:03:00.0     Off |                  N/A |\r\n| 24%   36C    P8    12W / 180W |    119MiB /  8113MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce GTX 1080    Off  | 0000:81:00.0     Off |                  N/A |\r\n| 24%   36C    P8    13W / 180W |    119MiB /  8113MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce GTX 1080    Off  | 0000:82:00.0     Off |                  N/A |\r\n| 24%   35C    P8    12W / 180W |    119MiB /  8113MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n```', ""If you use @kuangliu suggestion of using `CUDA_VISIBLE_DEVICES=1,2,3` you need to set `gpu_ids = [0,1,2]` because what the env variable is going to do is make sure that if you ask for device 0, you get 1, if you ask for 1, you get 2 etc...\r\n\r\nIn your case, if you say you training code works but not your validation, you may want to check what are the difference between them if you don't want to go with the `CUDA_VISIBLE_DEVICES` solution.\r\nIn particular, make sure that you set the `use_gpu` argument and everything else in the same way."", 'okay so conclusion is:\r\n`gpu_ids = [0,1,2]`.\r\nThen do:\r\n```\r\nCUDA_VISIBLE_DEVICES=""1,2,3"" python train.py\r\n```', ""Ah... wow that's so confusing!  Yes, I can confirm that this works.  Wonder if there's any way to make it more obvious."", 'If you want to get funky, you can actually do `CUDA_VISIBLE_DEVICES=3,1,2` and asking for 0 will get you device 3, asking for 1 will get you device 1 and asking for device 2 will get you device 2.\r\nThis may be a bit confusing at first but it is actually quite powerful !', ""Yeah I get that now that it's been highlighted, however, I bet anyone trying to learn the nuances of this will be confused for a while as well.  For my own enlightenment, what would be the reason where you might want to care whether your tensors live on GPU 3 vs GPU 1 if you're intending to use all of the GPUs anyway?"", 'The `CUDA_VISIBLE_DEVICES` is provided by nvidia and impact the nvidia driver directly (pytorch is not responsible for that at all). So even though in our case we usually only use it to mask out GPUs, some other application need this finer control.', ""So if you're simply indexing within the CUDA_VISIBLE_DEVICES list when specifying device in pytorch why not match the NVIDIA GPU id rather than the index within the CUDA_VISIBLE_DEVICES list?"", ""We don't really have access to that, the nvidia driver is doing this conversion.\r\nUndoing this conversion done in the nvidia driver in pytorch would be super confusing for anyone knowing what `CUDA_VISIBLE_DEVICES` is doing. And google search would fail you if you get any problem because the behaviour in pytorch would sudently become different from everything else."", 'Ok that makes sense, thanks for the clarification!', '@albanD I have noticed similar problems when specifying the device ids. Whatever combination is chosen, it always requires device with id 0.', ""@edgarriba I'm not sure to understand. If your observation is that whatever you put in `CUDA_VISIBLE_DEVICES`, indexing in pytorch always start at 0 and then grow one by one, that is not a problem, that is the expected behavior. [This](http://acceleware.com/blog/cudavisibledevices-masking-gpus) may clarify things."", ""what I'm saying is that when I specify the device ids (one or more) it needs to appear device 0, otherwise it always fails. Incrementing the `CUDA_VISIBLE_DEVICES` indexes doesn't solve the problem."", '@edgarriba hi, did you resolve this problem ?', 'What if I train a model in a machine with 4 gpus using gpu 2 and 3, and then want to evaluate on a new machine with only one gpu? Why the gpu placement during training matters for the testing? This is tricky and inflexible. ', ""For people coming from google, looking for a **fix/workaround**:\r\n\r\nIf you want to do a forward pass on devices=[1, 3], you need to manually move your model to device `1`. You can do that by calling `mymodel.cuda(1)'. \r\n\r\nAlso note, that pytorch *loves* to move parameters back to device 0. So if the error persists, double check that the relevant model parameters are still on device 0 at execution time. \r\n\r\n@soumith and @albanD: Can you reopen this? There are legit reasons why a user might want to not perform a computation on device 0. As an example I have a model which consists of a CNN as well as a very memory intensive RNN. I like to run the CNN on devices [0,1] and the RNN on devices [2,3]. Making this work was very clumsy. On the other hand I do not see much reason why you need to restrict broadcasting to a very specific source device.\r\n\r\n\r\n\r\n"", ""@MarvinTeichmann that's a bug, it should be possible to do this. Can you please create a new issue and post a small script we could use to reproduce the problem (e.g. use a simplified network with dummy data)?"", '@apaszke Could you just reopen this? I am currently lacking the time to write a detailed issue report. But even more importantly, this post is where people are currently landing when they google the error. It would be better to keep things together.\r\n\r\nAlso, the details provided by OP post are enough to reproduce the bug.\r\n\r\n', ""I can't run the script posted in the original issue, because it's not self contained. The issue is likely that your model is not on the first device, as it should be, and that's what the error says.""]","['\r\ndef visualize_model(model=None, num_images=5, use_gpu=False, gpu_ids=[0]):\r\n    print(""Using GPU: "" + str(gpu_ids[0]))\r\n    for i, data in enumerate(dset_loaders[\'val\']):\r\n        inputs, labels = data\r\n        if use_gpu:\r\n            inputs, labels = Variable(inputs.cuda(device=gpu_ids[0])), Variable(labels.cuda(device=gpu_ids[0]))\r\n        else:\r\n            inputs, labels = Variable(inputs), Variable(labels)\r\n\r\n        outputs = model(inputs)      # ERROR HERE!\r\n        _, preds = torch.max(outputs.data, 1)\r\n\r\n        plt.figure()\r\n        imshow(inputs.cpu().data[0])\r\n        plt.title(\'pred: {}\'.format(dset_classes[labels.data[0]]))\r\n        plt.show()\r\n\r\n        if i == num_images - 1:\r\n            break\r\n ']","['gpu_ids = [0,1,2]', 'gpu_ids = [1,2,3]']",0,0
198,pytorch,21922,closed,Segmentation fault using all_reduce with cuda:1 (MPI),"## üêõ Bug

Using  under cuda-aware MPI with a cuda device other than  causes segmentation fault. I managed to bypass this for a very specific use case by setting  and then using  within pytorch.
 
## To Reproduce



Steps to reproduce the behavior:

> mpirun -np 4 --oversubscribe -host 127.0.0.1 python test.py

Segmentation fault:

## Expected behavior

No crash

## Environment

PyTorch version: 1.1.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 16.04.6 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration: 
GPU 0: Tesla P100-PCIE-16GB
GPU 1: Tesla P100-PCIE-16GB

Nvidia driver version: 418.67
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.16.4
[pip3] numpydoc==0.8.0
[pip3] torch==1.1.0
[pip3] torchvision==0.3.0a0+c94a158
[conda] blas                      1.0                         mkl  
[conda] libmklml                  2018.0.3                      0  
[conda] magma-cuda100             2.5.0                         1    pytorch
[conda] mkl                       2019.4                      243  
[conda] mkl-dnn                   0.14                 h6bb024c_0  
[conda] mkl-include               2019.4                      243  
[conda] mkl-service               2.0.2            py36h7b6447c_0  
[conda] mkl_fft                   1.0.12           py36ha843d7b_0  
[conda] mkl_random                1.0.2            py36hd81dba3_0  
[conda] torch                     1.1.0                    pypi_0    pypi
[conda] torchvision               0.3.0a0+c94a158          pypi_0    pypi


MPI 3.0.0 - cuda aware",module: cuda oncall: distributed triaged,"['All we do on the PyTorch side is pass through these CUDA pointers to MPI.\r\n\r\nAre you sure that your MPI distribution can deal with non-0 CUDA devices?', 'I use OpenMPI 3.0.0, it seems to support multiple GPUs per device. See:\nhttps://www.open-mpi.org/faq/?category=runcuda (see code sample near ""*NUMA\nNode Issues"")*\n\nOn Wed, Jun 26, 2019 at 1:56 PM Pieter Noordhuis <notifications@github.com>\nwrote:\n\n> All we do on the PyTorch side is pass through these CUDA pointers to MPI.\n>\n> Are you sure that your MPI distribution can deal with non-0 CUDA devices?\n>\n> ‚Äî\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/21922?email_source=notifications&email_token=AAOXEVPTK7JNCJCJJYKQVF3P4NDM5A5CNFSM4HZDAGAKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODYTERWA#issuecomment-505825496>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAOXEVIYM5J6C54RHMHRUTDP4NDM5ANCNFSM4HZDAGAA>\n> .\n>\n', 'I can reproduce this issue with OpenMPI 3.1.1.', 'It seems to work if the CUDA device is set. Working on a PR.', 'Hi Pieter,\n\nWhat do you mean by ""if the CUDA device is set""?\n\nOn Tue, Jul 2, 2019 at 2:18 PM Pieter Noordhuis <notifications@github.com>\nwrote:\n\n> It seems to work if the CUDA device is set. Working on a PR.\n>\n> ‚Äî\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/21922?email_source=notifications&email_token=AAOXEVJWFTLAW37Y7BMTLVDP5M2PXA5CNFSM4HZDAGAKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZA5U2I#issuecomment-507632233>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAOXEVLWDJKKWLU5OIOCHHLP5M2PXANCNFSM4HZDAGAA>\n> .\n>\n', 'CUDA has a thread local that determines the ""current"" device. If the current device is 0, but the tensor you\'re operating on lives on device 1, the crash you reported happens. In #22446 I added some code to ProcessGroupMPI to automatically set the current device to the correct one before calling MPI.']","['python\r\nimport os\r\nimport socket\r\nimport torch\r\nimport torch.distributed as dist\r\n\r\n\r\ndef run(rank, size):\r\n    t = torch.rand(1).cuda()\r\n    gather_t = [torch.ones_like(t) for _ in range(size)]\r\n    dist.all_gather(gather_t, t)\r\n\r\ndef init_processes(rank, size, fn, backend=\'tcp\'):\r\n    """""" Initialize the distributed environment. """"""\r\n    dist.init_process_group(backend, rank=rank, world_size=size)\r\n    fn(rank, size)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    world_size = int(os.environ[\'OMPI_COMM_WORLD_SIZE\'])\r\n    world_rank = int(os.environ[\'OMPI_COMM_WORLD_RANK\'])\r\n\r\n    torch.cuda.set_device(1)\r\n    init_processes(world_rank, world_size, run, backend=\'mpi\')\r\n', '\r\n[prototype:02781] *** Process received signal ***\r\n[prototype:02781] Signal: Segmentation fault (11)\r\n[prototype:02781] Signal code: Invalid permissions (2)\r\n[prototype:02781] Failing at address: 0x7f4e64a00000\r\n[prototype:02781] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x11390)[0x7f4f088a8390]\r\n[prototype:02781] [ 1] /lib/x86_64-linux-gnu/libc.so.6(+0x14e045)[0x7f4f0861b045]\r\n[prototype:02781] [ 2] /opt/openmpi-3.0.0/lib/libopen-pal.so.40(+0x49eec)[0x7f4eb2f8ceec]\r\n[prototype:02781] [ 3] /opt/openmpi-3.0.0/lib/libmpi.so.40(ompi_datatype_sndrcv+0x53a)[0x7f4ee0b4713a]\r\n[prototype:02781] [ 4] /opt/openmpi-3.0.0/lib/libmpi.so.40(ompi_coll_base_allgather_intra_recursivedoubling+0x8f)[0x7f4ee0b898bf]\r\n[prototype:02781] [ 5] /opt/openmpi-3.0.0/lib/libmpi.so.40(MPI_Allgather+0x12e)[0x7f4ee0b47e3e]\r\n[prototype:02781] [ 6] /root/anaconda/lib/python3.6/site-packages/torch/lib/libtorch_python.so(+0x71d826)[0x7f4ef9818826]\r\n[prototype:02781] [ 7] /root/anaconda/lib/python3.6/site-packages/torch/lib/libtorch_python.so(_ZN4c10d15ProcessGroupMPI7runLoopEv+0x27c)[0x7f4ef9814c6c]\r\n[prototype:02781] [ 8] /root/anaconda/lib/libstdc++.so.6(+0xb8678)[0x7f4ee259e678]\r\n[prototype:02781] [ 9] /lib/x86_64-linux-gnu/libpthread.so.0(+0x76ba)[0x7f4f0889e6ba]\r\n[prototype:02781] [10] /lib/x86_64-linux-gnu/libc.so.6(clone+0x6d)[0x7f4f085d441d]\r\n[prototype:02781] *** End of error message ***\r\n']","['all_reduce', 'cuda:0', 'CUDA_VISIBLE_DEVICES=1', 'cuda:0', '']",0,0
199,pytorch,20101,closed,jit tracing error for nn.Sequential with nn.Conv2d in torch 1.1.0 ,"## üêõ Bug

 
when tracing nn.Sequential with nn.Conv2d in torch 1.1.0 

## To Reproduce

Steps to reproduce the behavior:



Raises the following error 


## Expected behavior

Expected to convert without issues

## Environment

PyTorch version: 1.1.0
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 18.04.2 LTS
GCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
CMake version: version 3.10.2

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.1.85
cuDNN version: /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.0.5

Versions of relevant libraries:
[pip3] numpy==1.16.2
[pip3] numpy-image-widget==2019.1.6
[pip3] torch==1.1.0
[pip3] torchfile==0.1.0
[pip3] torchvision==0.2.1
[conda] Could not collect
",high priority oncall: jit triaged,"[""@oldnaari \r\n\r\nCould you please try calling `trace` on your `model` rather than `model.forward` ?  It will still be tracing `model.forward`, but it will also capture all module's parameters.\r\n\r\n```python\r\nmodel = nn.Sequential(nn.Conv2d(1, 1, 3))\r\ntorch.jit.trace(model, torch.randn(1, 1, 3, 3))\r\n```\r\n\r\nWe are going to split `trace` into `trace` and `trace_module` and then we could consider making this particular error message more helpful"", '@Krovatkin But according to https://github.com/pytorch/pytorch/issues/19070, `torch.jit.trace(model.forward, torch.randn(1, 1, 3, 3))` would eventually also work (by the 2nd syntax sugar), right?', '@SsnL , #19070, I believe, addresses a slightly different issue. Namely, it allows one to trace multiple methods as a part of a single module (https://github.com/pytorch/pytorch/pull/19905)\r\n', '@Krovatkin Yes. I agree that this is orthogonal with `trace_module`. What I was asking is that, in the scenario of tracing a single module method, shouldn‚Äôt `trace(n.forward, x)` still work eventually, because it would be converted to `trace(n.forward.__self__, x)`?', ""@SsnL I honestly don't know how @zdevito wants us to handle this particular case.\r\nIf it's a truly `static` function (i.e. it doesn't use module's parameters) we wouldn't want to necessarily create a whole new module to host it, since we now have standalone functions. This is exactly how `trace` works now. It will create a standalone function even for module's methods:\r\n\r\n```python\r\n        name = getattr(func, '__name__', 'forward')\r\n        if name == '<lambda>':\r\n            name = '_lambda'  # make name a valid identifier\r\n        traced = torch._C._create_function_from_trace(name, func, example_inputs,\r\n                                                      var_lookup_fn,\r\n                                                      _force_outplace)\r\n```\r\n\r\n On the downside, if we pass a function that does use module's parameters we get `RuntimeError: Cannot insert a Tensor that requires grad as a constant. Consider making it a parameter or input, or detaching the gradient` \r\n\r\nIf we decide that we don't want to encourage users to write `static` functions like this, we could drop support for this case, then we could tweak `trace` to do what you are suggesting."", '@Krovatkin Yes I really hope @zdevito can help clarify. I am a bit lost on the relation between the proposal in #19070 and `trace_module` of #19905. It seems that #19905 covers majority of the functionalities proposed in #19070 but with a different API. \r\n\r\nIt feels to me that #19905 is trying to make the distinction that `trace` traces regular static functions and `trace_module` returns a traced module, but I am not 100% certain.\r\n\r\nAdditionally, I really hope that there will be some nice syntax sugar to make the `trace_module` API easier to use. Something that can work like a decorator would be nice to have.\r\n\r\nOut of curiosity, is there a large overhead of using a traced function vs. a traced module? In particular, e.g., if I separately trace two methods of the same module using `trace_module` and just use the two resulting traced modules (e.g., reassign them as methods), what could go wrong?', ""In a chat with @zdevito we concluded that the API would be the following:\r\n1. `trace_module` takes in a module and (optionally, defaulting to `['forward']`) list of methods to trace. Not tracing `forward` is forbidden.\r\n2. `trace` _always_ traces a single function (and returns a `torch.jit.Function`) without taking any parameters into the context. The only exception to that rule is that if you pass in a module, it will dispatch to `trace_module` for backward compatibility."", '@SsnL https://github.com/pytorch/pytorch/pull/20368 should make trace API consistent with what @apaszke and @zdevito decided. If you pass `my_module.my_not_forward_method` it will complain now.', 'Both repros now run correctly, mod dim tweak in original (trace model.forward) to match suggested (trace model). (unmodified original gives unrelated error on channel agreement). Closing', 'For anyone who had the exact error when using `trace` for a model with **a customized layer**, removing `to(my_device)` from `nn.Parameter(...).to(my_device)` would actually resolve the problem. Thought it might help.']","['python\r\nRuntimeError: Cannot insert a Tensor that requires grad as a constant. Consider making it a parameter or input, or detaching the gradient\r\n', 'python\r\nfrom torch import nn\r\nimport torch.jit\r\n\r\nmodel = nn.Sequential(nn.Conv2d(2, 2, 1, 1, 1))\r\n\r\ntorch.jit.trace(model.forward, torch.randn(1, 1, 2, 2))\r\n', 'python\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-3-5e9a2f5de8a5> in <module>\r\n      4 model = nn.Sequential(nn.Conv2d(2, 2, 1, 1, 1))\r\n      5 \r\n----> 6 torch.jit.trace(model.forward, torch.randn(1, 1, 2, 2))\r\n\r\n~/.virtualenvs/test/lib/python3.6/site-packages/torch/jit/__init__.py in trace(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, _force_outplace, _module_class)\r\n    693         traced = torch._C._create_function_from_trace(name, func, example_inputs,\r\n    694                                                       var_lookup_fn,\r\n--> 695                                                       _force_outplace)\r\n    696 \r\n    697     # Check the trace against new traces created from user-specified inputs\r\n\r\n~/.virtualenvs/test/lib/python3.6/site-packages/torch/nn/modules/container.py in forward(self, input)\r\n     90     def forward(self, input):\r\n     91         for module in self._modules.values():\r\n---> 92             input = module(input)\r\n     93         return input\r\n     94 \r\n\r\n~/.virtualenvs/test/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    489             hook(self, input)\r\n    490         if torch._C._get_tracing_state():\r\n--> 491             result = self._slow_forward(*input, **kwargs)\r\n    492         else:\r\n    493             result = self.forward(*input, **kwargs)\r\n\r\n~/.virtualenvs/test/lib/python3.6/site-packages/torch/nn/modules/module.py in _slow_forward(self, *input, **kwargs)\r\n    479         tracing_state._traced_module_stack.append(self)\r\n    480         try:\r\n--> 481             result = self.forward(*input, **kwargs)\r\n    482         finally:\r\n    483             tracing_state.pop_scope()\r\n\r\n~/.virtualenvs/test/lib/python3.6/site-packages/torch/nn/modules/conv.py in forward(self, input)\r\n    336                             _pair(0), self.dilation, self.groups)\r\n    337         return F.conv2d(input, self.weight, self.bias, self.stride,\r\n--> 338                         self.padding, self.dilation, self.groups)\r\n    339 \r\n    340 \r\n\r\nRuntimeError: Cannot insert a Tensor that requires grad as a constant. Consider making it a parameter or input, or detaching the gradient\r\nTensor:\r\n(1,1,.,.) = \r\n  0.3550\r\n\r\n(2,1,.,.) = \r\n 0.01 *\r\n  9.7722\r\n\r\n(1,2,.,.) = \r\n -0.5052\r\n\r\n(2,2,.,.) = \r\n  0.5900\r\n[ Variable[CPUType]{2,2,1,1} ]\r\n']",[],0,0
200,pytorch,9261,closed,memory leak for PyTorch -> dlpack -> CuPy,"Hi again,

for this one, I'm not sure whether it is a cupy or pytorch issue, either, but I don't think this should run out of memory:

 will be high. Again it could be a refcounting thing that things are not going out of scope properly. (In which case it would be more of a CuPy issue.)

Best regards

Thomas",todo,"[""Does it leak if you omit `cupy.fromDlpack`? If it doesn't leak, sounds like a cupy issue. PyTorch's `to_dlpack` function returns a capsule, and from there it's cupy's job to make sure they free it when they're done with it."", ""My impression is that CuPy implements a contract to be similar to the numpy/PyTorch sharing of data and PyTorch doesn't and instead interprets the contract you describe .\r\n\r\nI must admit that I haven't seen the specs to argue about it but intuitively, the CuPy implementation makes much more sense to me.\r\n\r\nI can still access a tensor I pass to to_dlpack. What happens with that when CuPy decides it doesn't need the capsule anymore?\r\n"", ""So I think that the problem here is that CuPy doesn't call the deleter.\r\nI'll submit a PR to CuPy."", 'CuPY PR: https://github.com/cupy/cupy/pull/1445']","[""\r\nimport cupy\r\nimport torch.utils.dlpack\r\nimport gc\r\nfor i in range(10000):\r\n  b = torch.randn(1000,1000, device='cuda')\r\n  a = cupy.fromDlpack(torch.utils.dlpack.to_dlpack(b))\r\n  gc.collect()\r\n""]",['torch.cuda.memory_allocated()'],0,0
201,pytorch,1626,closed, Modules/gcmodule.c:380: visit_decref: Assertion `((gc)->gc.gc_refs >> (1)) != 0' failed.,"Continued from: https://github.com/pytorch/pytorch/issues/1624 (running tests on a debug build of python3.6.1.

The following tests in TestNN failed with gc problems:

test_variable_sequence_cuda
test_cuda_rnn_fused
test_rnn_initial_hidden_state
test_RNN_cpu_vs_cudnn_no_dropout
test_RNN_cpu_vs_cudnn_with_dropout
test_RNN_dropout_state
test_RNN_change_dropout

Error is:
((gc)->gc.gc_refs >> (1)) != 0' failed.

#0  0x00007ffff712c1d7 in raise () at /lib64/libc.so.6
#1  0x00007ffff712d8c8 in abort () at /lib64/libc.so.6
#2  0x00007ffff7125146 in __assert_fail_base () at /lib64/libc.so.6
#3  0x00007ffff71251f2 in  () at /lib64/libc.so.6
#4  0x0000000000435f63 in visit_decref (op=0x7fffc006e4f8, data=<optimized out>) at Modules/gcmodule.c:380
#5  0x00007fffecd7c20d in THPVariable_traverse(THPVariable*, visitproc, void*) (self=self@entry=0x7fffc00781f0, visit=visit@entry=0x435efd <visit_decref>, arg=arg@entry=0x0)
    at torch/csrc/autograd/python_variable.cpp:92
#6  0x00000000004a86ce in subtype_traverse (self=0x7fffc00781f0, visit=0x435efd <visit_decref>, arg=0x0) at Objects/typeobject.c:1021
#7  0x00000000004356ee in subtract_refs (containers=containers@entry=0x885d20 <generations>) at Modules/gcmodule.c:399
#8  0x0000000000436809 in collect (generation=generation@entry=0, n_collected=n_collected@entry=0x7fffffff96b8, n_uncollectable=n_uncollectable@entry=0x7fffffff96b0, nofail=nofail@entry=0)
    at Modules/gcmodule.c:956
#9  0x0000000000436c47 in collect_with_callback (generation=0) at Modules/gcmodule.c:1128
#10 0x0000000000436d3f in collect_generations () at Modules/gcmodule.c:1151
#11 0x0000000000436eac in _PyObject_GC_Alloc (use_calloc=use_calloc@entry=0, basicsize=basicsize@entry=72) at Modules/gcmodule.c:1726
#12 0x0000000000437382 in _PyObject_GC_Malloc (basicsize=basicsize@entry=72) at Modules/gcmodule.c:1736
#13 0x00000000004a8443 in PyType_GenericAlloc (type=0x7fffed9eb400 <THPSizeType>, nitems=2) at Objects/typeobject.c:936
#14 0x00007fffecd3083f in THPSize_New(int, long*) (dim=2, sizes=0x4547f5c0) at torch/csrc/Size.cpp:16
#15 0x00007fffed1c2102 in THCPDoubleTensor_size(_object*, _object*, _object*) (self=<optimized out>, args=0x7ffff7fa0058, kwargs=0x0)
    at /data/users/gchanan/pytorch6/torch/csrc/generic/TensorMethods.cpp:650
#16 0x00000000004950a9 in _PyCFunction_FastCallDict (func_obj=func_obj@entry=0x7fffc28df328, args=args@entry=0x4547e598, nargs=nargs@entry=0, kwargs=kwargs@entry=0x0) at Objects/methodobject.c:231
#17 0x000000000049545a in _PyCFunction_FastCallKeywords (func=func@entry=0x7fffc28df328, stack=stack@entry=0x4547e598, nargs=nargs@entry=0, kwnames=kwnames@entry=0x0) at Objects/methodobject.c:295
#18 0x0000000000530f41 in call_function (pp_stack=pp_stack@entry=0x7fffffff9908, oparg=oparg@entry=0, kwnames=kwnames@entry=0x0) at Python/ceval.c:4798
#19 0x000000000053a5b6 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3284
#20 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x4547e3e8, throwflag=throwflag@entry=0) at Python/ceval.c:718
#21 0x0000000000530b4b in _PyEval_EvalCodeWithName (_co=0x7fffc2406e80, globals=<optimized out>, locals=locals@entry=0x0, args=args@entry=0x7fffc008c480, argcount=4, kwnames=kwnames@entry=0x0, kwargs=kwargs@entry=0x8, kwcount=kwcount@entry=0, kwstep=kwstep@entry=2, defs=defs@entry=0x0, defcount=defcount@entry=0, kwdefs=kwdefs@entry=0x0, closure=closure@entry=0x0, name=name@entry=0x0, qualname=qualname@entry=0x0) at Python/ceval.c:4128
#22 0x0000000000531168 in PyEval_EvalCodeEx (_co=<optimized out>, globals=<optimized out>, locals=locals@entry=0x0, args=args@entry=0x7fffc008c480, argcount=<optimized out>, kws=kws@entry=0x0, kwcount=kwcount@entry=0, defs=defs@entry=0x0, defcount=defcount@entry=0, kwdefs=0x0, closure=0x0) at Python/ceval.c:4149
#23 0x000000000047386f in function_call (func=0x7fffc23a6840, arg=0x7fffc008c458, kw=0x0) at Objects/funcobject.c:604
#24 0x0000000000444e8c in PyObject_Call (func=func@entry=0x7fffc23a6840, args=args@entry=0x7fffc008c458, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2246
#25 0x00000000005316b0 in PyEval_CallObjectWithKeywords (func=0x7fffc23a6840, args=0x7fffc008c458, kwargs=kwargs@entry=0x0) at Python/ceval.c:4718
#26 0x0000000000444c69 in PyObject_CallObject (o=<optimized out>, a=<optimized out>) at Objects/abstract.c:2172
#27 0x00007fffecd74eb3 in THPFunction_apply(_object*, _object*) (cls=0x11c9178, _inputs=0x7fffc1da3418) at torch/csrc/autograd/python_function.cpp:722
#28 0x00000000004950b8 in _PyCFunction_FastCallDict (func_obj=func_obj@entry=0x7fffc28d61c0, args=args@entry=0x4547bf70, nargs=nargs@entry=3, kwargs=kwargs@entry=0x0) at Objects/methodobject.c:234
#29 0x000000000049545a in _PyCFunction_FastCallKeywords (func=func@entry=0x7fffc28d61c0, stack=stack@entry=0x4547bf70, nargs=nargs@entry=3, kwnames=kwnames@entry=0x0) at Objects/methodobject.c:295
#30 0x0000000000530f41 in call_function (pp_stack=pp_stack@entry=0x7fffffff9e58, oparg=oparg@entry=3, kwnames=kwnames@entry=0x0) at Python/ceval.c:4798
#31 0x000000000053a5b6 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3284
#32 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x4547bdc8, throwflag=throwflag@entry=0) at Python/ceval.c:718
#33 0x00000000005301ec in _PyFunction_FastCall (co=co@entry=0x7fffc31e2640, args=0x7fffffffa038, args@entry=0x7fffffffa020, nargs=nargs@entry=3, globals=globals@entry=0x7fffc31e1e68)
    at Python/ceval.c:4880
---Type <return> to continue, or q <return> to quit---
#34 0x000000000053c110 in _PyFunction_FastCallDict (func=func@entry=0x7fffc23f61c8, args=args@entry=0x7fffffffa020, nargs=nargs@entry=3, kwargs=kwargs@entry=0x0) at Python/ceval.c:4982
#35 0x0000000000445042 in _PyObject_FastCallDict (func=func@entry=0x7fffc23f61c8, args=args@entry=0x7fffffffa020, nargs=nargs@entry=3, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2295
#36 0x0000000000445345 in _PyObject_Call_Prepend (func=0x7fffc23f61c8, obj=0x7fffc0085e28, args=0x7fffc22eb988, kwargs=0x0) at Objects/abstract.c:2358
#37 0x000000000045ee6b in method_call (method=<optimized out>, args=<optimized out>, kwargs=<optimized out>) at Objects/classobject.c:317
#38 0x0000000000444e8c in PyObject_Call (func=func@entry=0x7fffc23963d8, args=args@entry=0x7fffc22eb988, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2246
#39 0x00000000004af68e in call_method (o=<optimized out>, nameid=nameid@entry=0x8a5740 <PyId___setitem__>, format=format@entry=0x635a54 ""(OO)"") at Objects/typeobject.c:1453
#40 0x00000000004af814 in slot_mp_ass_subscript (self=<optimized out>, key=<optimized out>, value=<optimized out>) at Objects/typeobject.c:5944
#41 0x00000000004444f0 in PyObject_SetItem (o=o@entry=0x7fffc0085e28, key=key@entry=0x7fffc22eb838, value=value@entry=0x7fffc0089b80) at Objects/abstract.c:180
#42 0x00000000005345cc in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:1727
#43 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x4547ed58, throwflag=throwflag@entry=0) at Python/ceval.c:718
#44 0x0000000000530b4b in _PyEval_EvalCodeWithName (_co=0x7fffc1f6b700, globals=<optimized out>, locals=locals@entry=0x0, args=<optimized out>, argcount=1, kwnames=0x0, kwargs=0x307b1470, kwcount=0, kwstep=kwstep@entry=1, defs=0x7fffc1f69d18, defcount=defcount@entry=1, kwdefs=kwdefs@entry=0x0, closure=closure@entry=0x0, name=name@entry=0x7fffefef55e0, qualname=qualname@entry=0x7fffefef55e0)
    at Python/ceval.c:4128
#45 0x0000000000530da7 in fast_function (func=func@entry=0x7fffc1f8a110, stack=<optimized out>, nargs=nargs@entry=1, kwnames=kwnames@entry=0x0) at Python/ceval.c:4939
#46 0x0000000000530ffe in call_function (pp_stack=pp_stack@entry=0x7fffffffa4e8, oparg=oparg@entry=1, kwnames=kwnames@entry=0x0) at Python/ceval.c:4819
#47 0x000000000053a5b6 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3284
#48 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x307b1208, throwflag=throwflag@entry=0) at Python/ceval.c:718
#49 0x00000000005301ec in _PyFunction_FastCall (co=<optimized out>, args=0x7fffc231ff18, nargs=nargs@entry=2, globals=<optimized out>) at Python/ceval.c:4880
#50 0x0000000000530cf7 in fast_function (func=func@entry=0x7fffc1dc39b0, stack=<optimized out>, nargs=nargs@entry=2, kwnames=kwnames@entry=0x0) at Python/ceval.c:4915
#51 0x0000000000530ffe in call_function (pp_stack=pp_stack@entry=0x7fffffffa6e8, oparg=oparg@entry=1, kwnames=kwnames@entry=0x0) at Python/ceval.c:4819
#52 0x000000000053a5b6 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3284
#53 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x7fffc231fd78, throwflag=throwflag@entry=0) at Python/ceval.c:718
#54 0x00000000005301ec in _PyFunction_FastCall (co=<optimized out>, args=0x307b0898, nargs=nargs@entry=1, globals=<optimized out>) at Python/ceval.c:4880
#55 0x0000000000530cf7 in fast_function (func=func@entry=0x7fffc1dc3b20, stack=<optimized out>, nargs=nargs@entry=1, kwnames=kwnames@entry=0x0) at Python/ceval.c:4915
#56 0x0000000000530ffe in call_function (pp_stack=pp_stack@entry=0x7fffffffa8e8, oparg=oparg@entry=0, kwnames=kwnames@entry=0x0) at Python/ceval.c:4819
#57 0x000000000053a5b6 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3284
#58 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x307b0698, throwflag=throwflag@entry=0) at Python/ceval.c:718
#59 0x0000000000530b4b in _PyEval_EvalCodeWithName (_co=_co@entry=0x7fffede08340, globals=globals@entry=0x7fffede77670, locals=locals@entry=0x0, args=args@entry=0x7fffffffab20, argcount=argcount@entry=2, kwnames=kwnames@entry=0x7ffff7fa0080, kwargs=kwargs@entry=0x7ffff7fa0088, kwcount=kwcount@entry=0, kwstep=kwstep@entry=2, defs=0x7fffede0ff88, defcount=1, kwdefs=0x0, closure=0x0, name=0x7fffede06860, qualname=0x7fffede054a0) at Python/ceval.c:4128
#60 0x000000000053c29d in _PyFunction_FastCallDict (func=func@entry=0x7fffedd4f280, args=args@entry=0x7fffffffab20, nargs=nargs@entry=2, kwargs=kwargs@entry=0x7fffc00e09b8) at Python/ceval.c:5031
#61 0x0000000000445042 in _PyObject_FastCallDict (func=func@entry=0x7fffedd4f280, args=args@entry=0x7fffffffab20, nargs=nargs@entry=2, kwargs=kwargs@entry=0x7fffc00e09b8) at Objects/abstract.c:2295
#62 0x0000000000445345 in _PyObject_Call_Prepend (func=0x7fffedd4f280, obj=0x7fffc00d0e28, args=0x7fffc00e1ae8, kwargs=0x7fffc00e09b8) at Objects/abstract.c:2358
#63 0x000000000045ee6b in method_call (method=<optimized out>, args=<optimized out>, kwargs=<optimized out>) at Objects/classobject.c:317
#64 0x0000000000444e8c in PyObject_Call (func=0x7fffeff04838, args=0x7fffc00e1ae8, kwargs=0x7fffc00e09b8) at Objects/abstract.c:2246
#65 0x000000000052f6b9 in do_call_core (func=func@entry=0x7fffeff04838, callargs=callargs@entry=0x7fffc00e1ae8, kwdict=kwdict@entry=0x7fffc00e09b8) at Python/ceval.c:5067
#66 0x000000000053aad1 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3366
#67 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x7fffc0064250, throwflag=throwflag@entry=0) at Python/ceval.c:718
#68 0x0000000000530b4b in _PyEval_EvalCodeWithName (_co=_co@entry=0x7fffede084c0, globals=globals@entry=0x7fffede77670, locals=locals@entry=0x0, args=args@entry=0x7fffffffaec0, argcount=argcount@entry=2, kwnames=kwnames@entry=0x0, kwargs=kwargs@entry=0x8, kwcount=kwcount@entry=0, kwstep=kwstep@entry=2, defs=0x0, defcount=0, kwdefs=0x0, closure=0x0, name=0x7ffff7fa3270, qualname=0x7fffede04ce8)
    at Python/ceval.c:4128
---Type <return> to continue, or q <return> to quit---
#69 0x000000000053c29d in _PyFunction_FastCallDict (func=func@entry=0x7fffedd4f3f0, args=args@entry=0x7fffffffaec0, nargs=nargs@entry=2, kwargs=kwargs@entry=0x0) at Python/ceval.c:5031
#70 0x0000000000445042 in _PyObject_FastCallDict (func=func@entry=0x7fffedd4f3f0, args=args@entry=0x7fffffffaec0, nargs=nargs@entry=2, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2295
#71 0x0000000000445345 in _PyObject_Call_Prepend (func=0x7fffedd4f3f0, obj=0x7fffc00d0e28, args=0x7fffc00e1b50, kwargs=0x0) at Objects/abstract.c:2358
#72 0x000000000045ee6b in method_call (method=<optimized out>, args=<optimized out>, kwargs=<optimized out>) at Objects/classobject.c:317
#73 0x0000000000444e8c in PyObject_Call (func=func@entry=0x7fffede3c6e8, args=args@entry=0x7fffc00e1b50, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2246
#74 0x00000000004b1e1d in slot_tp_call (self=<optimized out>, args=0x7fffc00e1b50, kwds=0x0) at Objects/typeobject.c:6167
#75 0x00000000004450b1 in _PyObject_FastCallDict (func=func@entry=0x7fffc00d0e28, args=args@entry=0x307af060, nargs=nargs@entry=1, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2316
#76 0x00000000004456e5 in _PyObject_FastCallKeywords (func=func@entry=0x7fffc00d0e28, stack=0x307af060, nargs=nargs@entry=1, kwnames=kwnames@entry=0x0) at Objects/abstract.c:2480
#77 0x0000000000531011 in call_function (pp_stack=pp_stack@entry=0x7fffffffb0c8, oparg=oparg@entry=1, kwnames=kwnames@entry=0x0) at Python/ceval.c:4822
#78 0x000000000053a5b6 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3284
#79 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x307aee98, throwflag=throwflag@entry=0) at Python/ceval.c:718
#80 0x0000000000530b4b in _PyEval_EvalCodeWithName (_co=_co@entry=0x7fffede1e880, globals=globals@entry=0x7fffedeaad78, locals=locals@entry=0x0, args=args@entry=0x7fffffffb300, argcount=argcount@entry=2, kwnames=kwnames@entry=0x7ffff7fa0080, kwargs=kwargs@entry=0x7ffff7fa0088, kwcount=kwcount@entry=0, kwstep=kwstep@entry=2, defs=0x7fffedd4d5c8, defcount=1, kwdefs=0x0, closure=0x0, name=0x7fffede06860, qualname=0x7fffedd50970) at Python/ceval.c:4128
#81 0x000000000053c29d in _PyFunction_FastCallDict (func=func@entry=0x7fffedd54eb8, args=args@entry=0x7fffffffb300, nargs=nargs@entry=2, kwargs=kwargs@entry=0x7fffc00e0a30) at Python/ceval.c:5031
#82 0x0000000000445042 in _PyObject_FastCallDict (func=func@entry=0x7fffedd54eb8, args=args@entry=0x7fffffffb300, nargs=nargs@entry=2, kwargs=kwargs@entry=0x7fffc00e0a30) at Objects/abstract.c:2295
#83 0x0000000000445345 in _PyObject_Call_Prepend (func=0x7fffedd54eb8, obj=0x7fffc00e1400, args=0x7fffc00e1cf0, kwargs=0x7fffc00e0a30) at Objects/abstract.c:2358
#84 0x000000000045ee6b in method_call (method=<optimized out>, args=<optimized out>, kwargs=<optimized out>) at Objects/classobject.c:317
#85 0x0000000000444e8c in PyObject_Call (func=0x7ffff064cf38, args=0x7fffc00e1cf0, kwargs=0x7fffc00e0a30) at Objects/abstract.c:2246
#86 0x000000000052f6b9 in do_call_core (func=func@entry=0x7ffff064cf38, callargs=callargs@entry=0x7fffc00e1cf0, kwdict=kwdict@entry=0x7fffc00e0a30) at Python/ceval.c:5067
#87 0x000000000053aad1 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3366
#88 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x7fffc00d9e20, throwflag=throwflag@entry=0) at Python/ceval.c:718
#89 0x0000000000530b4b in _PyEval_EvalCodeWithName (_co=_co@entry=0x7fffede14a00, globals=globals@entry=0x7fffedeaad78, locals=locals@entry=0x0, args=args@entry=0x7fffffffb6a0, argcount=argcount@entry=2, kwnames=kwnames@entry=0x0, kwargs=kwargs@entry=0x8, kwcount=kwcount@entry=0, kwstep=kwstep@entry=2, defs=0x0, defcount=0, kwdefs=0x0, closure=0x0, name=0x7ffff7fa3270, qualname=0x7fffedd55838)
    at Python/ceval.c:4128
#90 0x000000000053c29d in _PyFunction_FastCallDict (func=func@entry=0x7fffedd54d48, args=args@entry=0x7fffffffb6a0, nargs=nargs@entry=2, kwargs=kwargs@entry=0x0) at Python/ceval.c:5031
#91 0x0000000000445042 in _PyObject_FastCallDict (func=func@entry=0x7fffedd54d48, args=args@entry=0x7fffffffb6a0, nargs=nargs@entry=2, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2295
#92 0x0000000000445345 in _PyObject_Call_Prepend (func=0x7fffedd54d48, obj=0x7fffc00e1400, args=0x7fffc00d0e90, kwargs=0x0) at Objects/abstract.c:2358
#93 0x000000000045ee6b in method_call (method=<optimized out>, args=<optimized out>, kwargs=<optimized out>) at Objects/classobject.c:317
#94 0x0000000000444e8c in PyObject_Call (func=func@entry=0x7fffedee0288, args=args@entry=0x7fffc00d0e90, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2246
#95 0x00000000004b1e1d in slot_tp_call (self=<optimized out>, args=0x7fffc00d0e90, kwds=0x0) at Objects/typeobject.c:6167
#96 0x00000000004450b1 in _PyObject_FastCallDict (func=func@entry=0x7fffc00e1400, args=args@entry=0x307ae830, nargs=nargs@entry=1, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2316
#97 0x00000000004456e5 in _PyObject_FastCallKeywords (func=func@entry=0x7fffc00e1400, stack=0x307ae830, nargs=nargs@entry=1, kwnames=kwnames@entry=0x0) at Objects/abstract.c:2480
#98 0x0000000000531011 in call_function (pp_stack=pp_stack@entry=0x7fffffffb8a8, oparg=oparg@entry=1, kwnames=kwnames@entry=0x0) at Python/ceval.c:4822
#99 0x000000000053a5b6 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3284
#100 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x307ae668, throwflag=throwflag@entry=0) at Python/ceval.c:718
#101 0x0000000000530b4b in _PyEval_EvalCodeWithName (_co=_co@entry=0x7fffede1e880, globals=globals@entry=0x7fffedeaad78, locals=locals@entry=0x0, args=args@entry=0x7fffffffbae0, argcount=argcount@entry=2, kwnames=kwnames@entry=0x7ffff7fa0080, kwargs=kwargs@entry=0x7ffff7fa0088, kwcount=kwcount@entry=0, kwstep=kwstep@entry=2, defs=0x7fffedd4d5c8, defcount=1, kwdefs=0x0, closure=0x0, name=0x7fffede06860, qualname=0x7fffedd50970) at Python/ceval.c:4128
#102 0x000000000053c29d in _PyFunction_FastCallDict (func=func@entry=0x7fffedd54eb8, args=args@entry=0x7fffffffbae0, nargs=nargs@entry=2, kwargs=kwargs@entry=0x7fffc00e0058) at Python/ceval.c:5031
#103 0x0000000000445042 in _PyObject_FastCallDict (func=func@entry=0x7fffedd54eb8, args=args@entry=0x7fffffffbae0, nargs=nargs@entry=2, kwargs=kwargs@entry=0x7fffc00e0058) at Objects/abstract.c:2295
---Type <return> to continue, or q <return> to quit---
#104 0x0000000000445345 in _PyObject_Call_Prepend (func=0x7fffedd54eb8, obj=0x7fffc00d0dc0, args=0x7fffc00d0ae8, kwargs=0x7fffc00e0058) at Objects/abstract.c:2358
#105 0x000000000045ee6b in method_call (method=<optimized out>, args=<optimized out>, kwargs=<optimized out>) at Objects/classobject.c:317
#106 0x0000000000444e8c in PyObject_Call (func=0x7fffede3c7c8, args=0x7fffc00d0ae8, kwargs=0x7fffc00e0058) at Objects/abstract.c:2246
#107 0x000000000052f6b9 in do_call_core (func=func@entry=0x7fffede3c7c8, callargs=callargs@entry=0x7fffc00d0ae8, kwdict=kwdict@entry=0x7fffc00e0058) at Python/ceval.c:5067
#108 0x000000000053aad1 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3366
#109 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x7fffc00d9c28, throwflag=throwflag@entry=0) at Python/ceval.c:718
#110 0x0000000000530b4b in _PyEval_EvalCodeWithName (_co=_co@entry=0x7fffede14a00, globals=globals@entry=0x7fffedeaad78, locals=locals@entry=0x0, args=args@entry=0x7fffffffbe80, argcount=argcount@entry=2, kwnames=kwnames@entry=0x0, kwargs=kwargs@entry=0x8, kwcount=kwcount@entry=0, kwstep=kwstep@entry=2, defs=0x0, defcount=0, kwdefs=0x0, closure=0x0, name=0x7ffff7fa3270, qualname=0x7fffedd55838)
    at Python/ceval.c:4128
#111 0x000000000053c29d in _PyFunction_FastCallDict (func=func@entry=0x7fffedd54d48, args=args@entry=0x7fffffffbe80, nargs=nargs@entry=2, kwargs=kwargs@entry=0x0) at Python/ceval.c:5031
#112 0x0000000000445042 in _PyObject_FastCallDict (func=func@entry=0x7fffedd54d48, args=args@entry=0x7fffffffbe80, nargs=nargs@entry=2, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2295
#113 0x0000000000445345 in _PyObject_Call_Prepend (func=0x7fffedd54d48, obj=0x7fffc00d0dc0, args=0x7fffc00d0bb8, kwargs=0x0) at Objects/abstract.c:2358
#114 0x000000000045ee6b in method_call (method=<optimized out>, args=<optimized out>, kwargs=<optimized out>) at Objects/classobject.c:317
#115 0x0000000000444e8c in PyObject_Call (func=func@entry=0x7ffff0655e58, args=args@entry=0x7fffc00d0bb8, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2246
#116 0x00000000004b1e1d in slot_tp_call (self=<optimized out>, args=0x7fffc00d0bb8, kwds=0x0) at Objects/typeobject.c:6167
#117 0x00000000004450b1 in _PyObject_FastCallDict (func=func@entry=0x7fffc00d0dc0, args=args@entry=0x307ad4b0, nargs=nargs@entry=1, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2316
#118 0x00000000004456e5 in _PyObject_FastCallKeywords (func=func@entry=0x7fffc00d0dc0, stack=0x307ad4b0, nargs=nargs@entry=1, kwnames=kwnames@entry=0x0) at Objects/abstract.c:2480
#119 0x0000000000531011 in call_function (pp_stack=pp_stack@entry=0x7fffffffc088, oparg=oparg@entry=1, kwnames=kwnames@entry=0x0) at Python/ceval.c:4822
#120 0x000000000053a5b6 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3284
#121 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x307ad298, throwflag=throwflag@entry=0) at Python/ceval.c:718
#122 0x00000000005301ec in _PyFunction_FastCall (co=<optimized out>, args=0x307acf50, nargs=nargs@entry=2, globals=<optimized out>) at Python/ceval.c:4880
#123 0x0000000000530cf7 in fast_function (func=func@entry=0x7fffedca1110, stack=<optimized out>, nargs=nargs@entry=2, kwnames=kwnames@entry=0x0) at Python/ceval.c:4915
#124 0x0000000000530ffe in call_function (pp_stack=pp_stack@entry=0x7fffffffc288, oparg=oparg@entry=1, kwnames=kwnames@entry=0x0) at Python/ceval.c:4819
#125 0x000000000053a5b6 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3284
#126 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x307acda8, throwflag=throwflag@entry=0) at Python/ceval.c:718
#127 0x00000000005301ec in _PyFunction_FastCall (co=<optimized out>, args=0x307a77b8, nargs=nargs@entry=1, globals=<optimized out>) at Python/ceval.c:4880
#128 0x0000000000530cf7 in fast_function (func=func@entry=0x7fffedca1a68, stack=<optimized out>, nargs=nargs@entry=1, kwnames=kwnames@entry=0x0) at Python/ceval.c:4915
#129 0x0000000000530ffe in call_function (pp_stack=pp_stack@entry=0x7fffffffc488, oparg=oparg@entry=0, kwnames=kwnames@entry=0x0) at Python/ceval.c:4819
#130 0x000000000053a5b6 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3284
#131 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x307a75b8, throwflag=throwflag@entry=0) at Python/ceval.c:718
#132 0x0000000000530b4b in _PyEval_EvalCodeWithName (_co=_co@entry=0x7fffedceb400, globals=globals@entry=0x7fffedd4c9b8, locals=locals@entry=0x0, args=args@entry=0x7fffffffc6c0, argcount=argcount@entry=1, kwnames=kwnames@entry=0x7fffc22e5e10, kwargs=kwargs@entry=0x7fffc22e5e18, kwcount=2,
    kwcount@entry=1, kwstep=kwstep@entry=2, defs=0x7fffedca12a8, defcount=11, kwdefs=0x7fffedcefd78, closure=0x0, name=0x7ffff7fa3430, qualname=0x7fffedcef478) at Python/ceval.c:4128
#133 0x000000000053c29d in _PyFunction_FastCallDict (func=func@entry=0x7fffedca1338, args=args@entry=0x7fffffffc6c0, nargs=nargs@entry=1, kwargs=kwargs@entry=0x7fffc00d5238) at Python/ceval.c:5031
#134 0x0000000000445042 in _PyObject_FastCallDict (func=func@entry=0x7fffedca1338, args=args@entry=0x7fffffffc6c0, nargs=nargs@entry=1, kwargs=kwargs@entry=0x7fffc00d5238) at Objects/abstract.c:2295
#135 0x0000000000445345 in _PyObject_Call_Prepend (func=0x7fffedca1338, obj=0x7fffc00d09b0, args=0x7ffff7fa0058, kwargs=0x7fffc00d5238) at Objects/abstract.c:2358
#136 0x000000000045ee6b in method_call (method=<optimized out>, args=<optimized out>, kwargs=<optimized out>) at Objects/classobject.c:317
#137 0x0000000000444e8c in PyObject_Call (func=func@entry=0x7ffff064ce58, args=args@entry=0x7ffff7fa0058, kwargs=kwargs@entry=0x7fffc00d5238) at Objects/abstract.c:2246
#138 0x00000000004b1a5c in slot_tp_init (self=<optimized out>, args=0x7ffff7fa0058, kwds=0x7fffc00d5238) at Objects/typeobject.c:6380
#139 0x00000000004ada4d in type_call (type=0xb5e1c8, args=0x7ffff7fa0058, kwds=0x7fffc00d5238) at Objects/typeobject.c:915
#140 0x00000000004450b1 in _PyObject_FastCallDict (func=func@entry=0xb5e1c8, args=args@entry=0x3079d738, nargs=nargs@entry=0, kwargs=kwargs@entry=0x7fffc00d5238) at Objects/abstract.c:2316
---Type <return> to continue, or q <return> to quit---
#141 0x00000000004456e5 in _PyObject_FastCallKeywords (func=func@entry=0xb5e1c8, stack=0x3079d738, nargs=nargs@entry=0, kwnames=kwnames@entry=0x7fffc1f43260) at Objects/abstract.c:2480
#142 0x0000000000531011 in call_function (pp_stack=pp_stack@entry=0x7fffffffc900, oparg=<optimized out>, kwnames=kwnames@entry=0x7fffc1f43260) at Python/ceval.c:4822
#143 0x000000000053a6d4 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3300
#144 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x3079d588, throwflag=throwflag@entry=0) at Python/ceval.c:718
#145 0x00000000005301ec in _PyFunction_FastCall (co=<optimized out>, args=0xa1a588, nargs=nargs@entry=0, globals=<optimized out>) at Python/ceval.c:4880
#146 0x0000000000530cf7 in fast_function (func=func@entry=0x7fffc1d9e3f0, stack=<optimized out>, nargs=nargs@entry=0, kwnames=kwnames@entry=0x0) at Python/ceval.c:4915
#147 0x0000000000530ffe in call_function (pp_stack=pp_stack@entry=0x7fffffffcaf8, oparg=oparg@entry=0, kwnames=kwnames@entry=0x0) at Python/ceval.c:4819
#148 0x000000000053a5b6 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3284
#149 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0xa1a3f8, throwflag=throwflag@entry=0) at Python/ceval.c:718
#150 0x0000000000530b4b in _PyEval_EvalCodeWithName (_co=_co@entry=0x7ffff0a6c4c0, globals=globals@entry=0x7ffff7eaac88, locals=locals@entry=0x7ffff7eaac88, args=args@entry=0x0, argcount=argcount@entry=0, kwnames=kwnames@entry=0x0, kwargs=kwargs@entry=0x8, kwcount=kwcount@entry=0, kwstep=kwstep@entry=2, defs=defs@entry=0x0, defcount=defcount@entry=0, kwdefs=kwdefs@entry=0x0, closure=closure@entry=0x0, name=name@entry=0x0, qualname=qualname@entry=0x0) at Python/ceval.c:4128
#151 0x0000000000531168 in PyEval_EvalCodeEx (_co=_co@entry=0x7ffff0a6c4c0, globals=globals@entry=0x7ffff7eaac88, locals=locals@entry=0x7ffff7eaac88, args=args@entry=0x0, argcount=argcount@entry=0, kws=kws@entry=0x0, kwcount=kwcount@entry=0, defs=defs@entry=0x0, defcount=defcount@entry=0, kwdefs=kwdefs@entry=0x0, closure=closure@entry=0x0) at Python/ceval.c:4149
#152 0x00000000005311b2 in PyEval_EvalCode (co=co@entry=0x7ffff0a6c4c0, globals=globals@entry=0x7ffff7eaac88, locals=locals@entry=0x7ffff7eaac88) at Python/ceval.c:695
#153 0x00000000004231c4 in run_mod (mod=mod@entry=0xab1fe0, filename=filename@entry=0x7ffff0b5f660, globals=globals@entry=0x7ffff7eaac88, locals=locals@entry=0x7ffff7eaac88, flags=flags@entry=0x7fffffffcea0, arena=arena@entry=0x7ffff0b94520) at Python/pythonrun.c:980
#154 0x0000000000425a44 in PyRun_FileExFlags (fp=fp@entry=0x9501a0, filename_str=filename_str@entry=0x7ffff7efaeb0 ""test/test_nn.py"", start=start@entry=257, globals=globals@entry=0x7ffff7eaac88, locals=locals@entry=0x7ffff7eaac88, closeit=closeit@entry=1, flags=flags@entry=0x7fffffffcea0) at Python/pythonrun.c:933
#155 0x0000000000425ddb in PyRun_SimpleFileExFlags (fp=fp@entry=0x9501a0, filename=<optimized out>,
    filename@entry=0x7ffff7efaeb0 ""test/test_nn.py"", closeit=closeit@entry=1, flags=flags@entry=0x7fffffffcea0) at Python/pythonrun.c:396
#156 0x0000000000425f37 in PyRun_AnyFileExFlags (fp=fp@entry=0x9501a0, filename=0x7ffff7efaeb0 ""test/test_nn.py"", closeit=closeit@entry=1, flags=flags@entry=0x7fffffffcea0) at Python/pythonrun.c:80
#157 0x00000000004349d9 in run_file (fp=fp@entry=0x9501a0, filename=filename@entry=0x917300 L""test/test_nn.py"", p_cf=p_cf@entry=0x7fffffffcea0) at Modules/main.c:338
#158 0x0000000000435583 in Py_Main (argc=argc@entry=3, argv=argv@entry=0x916010) at Modules/main.c:809
#159 0x000000000041d21a in main (argc=3, argv=0x7fffffffd018) at ./Programs/python.c:69
`

Other tests from test/run_test.sh pass, although I didn't run the MAGMA tests because I haven't installed it outside of a conda environment.",high priority,"['The problem is in THPVariable_traverse:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/e1d257bc6d472ee297df1719bf344bae359dbeaa/torch/csrc/autograd/python_variable.cpp#L92\r\n\r\nIt\'s important that Py_VISIT is called once per ""owned"" reference. For example, one of the traverseprocs can decrement refcounts:\r\n\r\nhttps://github.com/python/cpython/blob/6f0eb93183519024cb360162bdd81b9faec97ba6/Modules/gcmodule.c#L378\r\n\r\nThe problem is that multiple Variables may have the same grad_fn. The Py_VISIT on the THPFunction is called more times than the refcount. The ownership graph is:\r\n\r\n```\r\nTHPVariable -> Variable -+-> PyFunction (refcnt 2) -> THPFunction (refcnt 1)\r\n                         |\r\nTHPVariable -> Variable -+\r\n```', ""Can we just ignore `grad_fn`? It's not very useful, because it won't traverse the autograd graph anyway."", ""Yeah, I can't think of any better option."", 'It seems that if we ignore `grad_fn` it appears that there are uncollectable cycles (TestAutograd.test_hooks_cycle fails). \r\n\r\nI suspect this bug was exposed by https://github.com/pytorch/pytorch/commit/72e819099468ee69a819620c72c0130f8f93e59d. (Previously, the Python refcnt of `grad_fn` likely matched the number of references to it because we were creating new `shared_ptr`s for each reference.)\r\n']","['\r\n\r\nRunning it through gdb on ""test_variable_sequence_cuda"" yields the following stack trace:\r\n']","['', '\r\n Modules/gcmodule.c:380: visit_decref: Assertion ', '']",0,0
202,pytorch,17560,closed,Please test and add the AdaBound optimizer into the next stable release,"Now we have another powerful general purpose optimizer called AdaBound that trains as fast as Adam and as good as SGD, see [https://github.com/Luolc/AdaBound](https://github.com/Luolc/AdaBound).

It appears to work better than Adam in several tasks including NLP and CV, with slightly higher accuracy and smoothier learning curve.

![image](https://user-images.githubusercontent.com/10172392/53534724-08e75b80-3b3b-11e9-87b7-debe21b3908e.png)

I think this would be a great and useful addition, thanks!",triage review,"['New implementations such as these can go into pytorch/contrib initially.', 'I also agree with the request of adding AdaBound optimizer to contrib. The provided implementation by the author unfortunately does not have support for PyTorch 1.0', ""I'd agree, send a PR to https://github.com/pytorch/contrib with unit tests, I'm happy to merge""]",[],[],0,0
203,pytorch,6932,closed,"Exception NameError: ""global name 'FileNotFoundError' is not defined""","

This error is coming while implementing pytorch faster-rcnn repository. Any solutions please??",,"['can you use the issue template, we need more information to help you.', '@soumith Using Pytorch 0.3.0 solves the problem.', '@R1234A are you using python 2?\r\n\r\nAFAIK FileNotFoundError only exists in python 3, so this should be fixed\r\n\r\nAlso, @R1234A this error is hiding the real bug in the code, which seems to be the following:\r\n> RuntimeError: reciprocal is not implemented for type torch.cuda.LongTensor\r\n\r\nDid you mean to take the reciprocal of a LongTensor?', '@SsnL does the shutdown workers FileNotFoundError happen on python 2.7 as well?', ""@zou3519 I haven't tested on py2. But I believe it should be an `IOError` with `ENOENT` errno."", '@zou3519  Yup I am using Python 2. To overcome this error of FileNotFoundError I have downgraded Pytorch from 0.4.0 version to 0.3.0. Use Pytorch 0.3.0 with Python 2, it will work fine.', ""@R1234A alternatively, upgrading to Python 3 will fix the issue (and Python 3 is nicer than Python 2). We'll fix this though."", 'For a quick fix you can make dataloader.py python2 compatible by defining FileNotFoundError somewhere\r\n```\r\ntry:\r\n    FileNotFoundError\r\nexcept NameError:\r\n    FileNotFoundError = IOError\r\n```', 'This error still exist in pytorch 0.4', '@isacarnekvist Yes it is fixed after 0.4', '@SsnL I am still getting it...', '@isalirezag did you build pytorch from source? It was fixed after 0.4 is released, so it will be in the next pytorch release.', '@soumith I updated it via `conda install pytorch torchvision cuda90 -c pytorch`\r\n\r\n', 'https://github.com/pytorch/pytorch#from-source', 'Can anyone help me with my problem?\r\n\r\n\r\n', ""> @soumith Using Pytorch 0.3.0 solves the problem.\r\n\r\n@R1234A\r\n\r\nI was also working with faster r-cnn and got the error. Downgrading to 0.3.0 solved FileNotFoundError but will produce another error: \r\n`self.ratio_list_batch[left_idx:(right_idx+1)] = torch.tensor(target_ratio.astype(np.float64)) # trainset ratio list ,each batch is same number\r\nTypeError: 'module' object is not callable`\r\n\r\nWhere this error is related to `torch.tensor` in the above error message. Upgrading can solve this error but will produce the previous one.\r\n\r\nHow do you cope with the second error?""]",[],"['Traceback (most recent call last):\r\n  File ""trainval_net.py"", line 322, in <module>\r\n    rois_label = fasterRCNN(im_data, im_info, gt_boxes, num_boxes)\r\n  File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 491, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""/home/faster-rcnn.pytorch/lib/model/faster_rcnn/faster_rcnn.py"", line 50, in forward\r\n    rois, rpn_loss_cls, rpn_loss_bbox = self.RCNN_rpn(base_feat, im_info, gt_boxes, num_boxes)\r\n  File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 491, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""/home/faster-rcnn.pytorch/lib/model/rpn/rpn.py"", line 87, in forward\r\n    rpn_data = self.RPN_anchor_target((rpn_cls_score.data, gt_boxes, im_info, num_boxes))\r\n  File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 491, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""/home/faster-rcnn.pytorch/lib/model/rpn/anchor_target_layer.py"", line 157, in forward\r\n    positive_weights = 1.0 / num_examples\r\n  File ""/usr/local/lib/python2.7/dist-packages/torch/tensor.py"", line 320, in __rdiv__\r\n    return self.reciprocal() * other\r\nRuntimeError: reciprocal is not implemented for type torch.cuda.LongTensor\r\nException NameError: ""global name \'FileNotFoundError\' is not defined"" in <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fb842e6e3d0>> ignored']",0,0
204,pytorch,23366,closed,[PyTorch][Feature Request]Lookahead Optimizer,"## üöÄ Lookahead Optimizer
https://arxiv.org/pdf/1907.08610.pdf

In this paper they show new Optimizer can save time, improve accuracy than SGD and work both on CV/NLP.
I think it may worth to try.
",feature module: optimizer,"['I think so.', ""It somehow looks like Stochastic Weight Averaging, could be plugged into every existed optimizer. Maybe it's more appropriate to write some wrapper class like scheduler."", ""We don't think this method should be in pytorch core, as opposed to your own personal repository or something like https://github.com/pytorch/contrib , at least not yet in time.\r\n\r\nOur reservation is that we want to include methods that the community uses as a standard, or else the code maintenance problem balloons up for us.\r\nWe do show discretion based on what the paper shows as evidence, for example BatchNorm was included within weeks of it's publish date (in Torch).\r\nIn terms of rejected methods, we've rejected newly minted papers such as Swish ( #3260 , #3182 ), Yellowfin ( https://github.com/pytorch/pytorch/issues/1960 ) and many others, and rightly so, these haven't become standardized in the community (like LSTM / Transformer / BatchNorm).\r\n\r\nIf you have a differing opinion, let us know why, and we can re-think.\r\n\r\ntl;dr: The paper doesn't show evidence that makes it a method that has obvious long-term success. If the paper does have long-term success in the field we will include it""]",[],[],0,0
205,pytorch,24826,open,Transformer Lack of Embedding Layer and Positional Encodings,"The Transformer implementation docs (https://pytorch.org/docs/stable/nn.html?highlight=transformer#torch.nn.Transformer) state that they implement the original paper but fail to acknowledge that they don‚Äôt implement the following: 
* Layer Norm as the default normalization option. 
* Positional Encodings
* Embeddings before the encoding and decoding step 

It‚Äôs fine that these are all not implemented directly in the module but making it more clear that they aren‚Äôt and were in the original paper would be helpful. 

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @brianjo @mruberry @albanD @zhangguanheng66 @jlin27",feature high priority module: docs module: nn oncall: transformer/mha triaged,"['Tagging this to reflect that we could implement the functionality described or just document the existing implementation. Both would seem to be somewhat valid.', '>     * Layer Norm as the default normalization option.\r\n\r\nCould you elaborate what you are missing regarding `LayerNorm` as default normalization? \r\n[TransformerEncoderLayer](\r\nhttps://github.com/pytorch/pytorch/blob/c6fe864db3e17830bf12957a64e6fd579ddeffad/torch/nn/modules/transformer.py#L223) and [TransformerDecoderLayer](https://github.com/pytorch/pytorch/blob/c6fe864db3e17830bf12957a64e6fd579ddeffad/torch/nn/modules/transformer.py#L302) both use `LayerNorm` (hard built-in) and the `nn.Transformer` creates a `TransformerEncoder` with additional final `LayerNorm` by default.\r\n\r\nI think the Transformer abstraction that was implemented is really good as it is. Maybe [fairseq](https://github.com/pytorch/fairseq) could be mentioned for people looking for full NLP models which offer embedding to decoding.', 'Re LayerNorm: Huh, sorry that I missed that. I guess I got confused by the fact that there is an option for further normalization in the ""TransformerEncoder"" and Decoder. \r\n\r\n![image](https://user-images.githubusercontent.com/12433427/63453782-6dfb7a00-c417-11e9-927d-514777fdab61.png)\r\n\r\nI don\'t think anything more needs to be implemented just better documentation on what people are and aren\'t getting.\r\n\r\nEmbeddings aren\'t a big deal but were used in the original paper and I think it should be flagged that they aren\'t here. \r\n\r\nThe same is true for the positional encodings but these are much more important for the model\'s performance and difficult to implement. (this was noted here https://github.com/pytorch/pytorch/issues/10459#issuecomment-413116713)\r\n\r\n\r\n', ""Recently [found this snippet in here](https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec#d554) that implements PositionalEncoding that can be easily added at the beggining of your `forward(x)` and before calling the transformer encoder forward.\r\n\r\nI updated to work for more recent versions:\r\n```Python\r\nclass PositionalEncoder(torch.nn.Module):\r\n    def __init__(self, d_model, max_seq_len=160):\r\n        super().__init__()\r\n        self.d_model = d_model\r\n        pe = torch.zeros(max_seq_len, d_model)\r\n        for pos in range(max_seq_len):\r\n            for i in range(0, d_model, 2):\r\n                pe[pos, i] = \\\r\n                    math.sin(pos / (10000 ** ((2 * i) / d_model)))\r\n                pe[pos, i + 1] = \\\r\n                    math.cos(pos / (10000 ** ((2 * (i + 1)) / d_model)))\r\n        pe = pe.unsqueeze(0)\r\n        self.register_buffer('pe', pe)\r\n\r\n    def forward(self, x):\r\n        with torch.no_grad():\r\n            x = x * math.sqrt(self.d_model)\r\n            seq_len = x.size(1)\r\n            pe = self.pe[:, :seq_len]\r\n            x = x + pe\r\n            return x\r\n```\r\n\r\nEdit: Surely that _nested for_ can be optimized with tensor logic because for big sequences or big  `d_model ` parameter it can take some time to initialize the module."", ""> Recently [found this snippet in here](https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec#d554) that implements PositionalEncoding that can be easily added at the beggining of your `forward(x)` and before calling the transformer encoder forward.\r\n> \r\n> I updated to work for more recent versions:\r\n> \r\n> ```python\r\n> class PositionalEncoder(torch.nn.Module):\r\n>     def __init__(self, d_model, max_seq_len=160):\r\n>         super().__init__()\r\n>         self.d_model = d_model\r\n>         pe = torch.zeros(max_seq_len, d_model)\r\n>         for pos in range(max_seq_len):\r\n>             for i in range(0, d_model, 2):\r\n>                 pe[pos, i] = \\\r\n>                     math.sin(pos / (10000 ** ((2 * i) / d_model)))\r\n>                 pe[pos, i + 1] = \\\r\n>                     math.cos(pos / (10000 ** ((2 * (i + 1)) / d_model)))\r\n>         pe = pe.unsqueeze(0)\r\n>         self.register_buffer('pe', pe)\r\n> \r\n>     def forward(self, x):\r\n>         with torch.no_grad():\r\n>             x = x * math.sqrt(self.d_model)\r\n>             seq_len = x.size(1)\r\n>             pe = self.pe[:, :seq_len]\r\n>             x = x + pe\r\n>             return x\r\n> ```\r\n> \r\n> Edit: Surely that _nested for_ can be optimized with tensor logic because for big sequences or big `d_model ` parameter it can take some time to initialize the module.\r\n\r\nMay I ask why use no_grad in the forward function? Wouldn't it prevent the update of word embeddings?"", 'This tutorial uses the TransformerEncoder class to implement the original paper, and looks like it has a cleaner positional encoding. \r\n\r\nhttps://pytorch.org/tutorials/beginner/transformer_tutorial.html', ""+1 on it being odd that nn.Transformer doesn't implement positional encoding. The lack of word embeddings makes sense, but it seems like the vast majority of transformers would require positional encodings.\r\n\r\nAlso, example usage for nn.Transformer training and evaluation would improve the docs a lot. It seems like nn.Transformer has been abandoned in favor of the encoder, decoder, etc. classes that comprise it, as suggested by the tutorial that @kryptec referenced. Seems like it would be better to have a tutorial for nn.Transformer, and details on how to override pieces of functionality, like how positional encoding is performed.\r\n\r\n[Edit: I could make a PR for nn.Transformer docs]"", 'Yeah, right. Also it could be useful to provide an implementation of new alternative positional encodings, for example, relative positional encodings.', ""I agree positional encoding should really be implemented and part of the transformer - I'm less concerned that the embedding is separate.\r\n\r\nIn particular, the input shape of the PyTorch transformer is different from other implementations (src is SNE rather than NSE) meaning you have to be very careful using common positional encoding implementations.\r\n\r\nI also find it a little odd/confusing that src is SNE yet src_key_padding_mask is NS"", 'Bumping to high pri due to activity & because positional encodings are generally essential for using the Transformer layers effectively', 'can we just push the positional encoding from [pytorch transformer tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html) to the nn module? and I find it to be in terms with the paper', ""I'm also in agreement with having positional encoding included by default in the nn.Transformer class."", 'vote for this feature', 'I too agree that positional encodings should be a part of the transformer class. However, instead of hard coding them, it is perhaps better to pass a flag in the constructor about whether to use positional encodings or not (can default to true). There definitely can be (and are) cases when positional encodings are not necessary, so such flexibility would be important', 'I agree with @ankit61 - many transformer models use position embeddings in place of the original sinusoidal encoding, and some may use no encoding. So the actual embedding layer should be configurable ']",[],[],0,0
206,pytorch,26218,open,Default adam epsilon to 1e-7 when on fp16,"## üöÄ Feature
If you use 1e-8 as default and you use 16 bit, it will round to zero, and thus you get no stability gains.


cc @vincentqb",enhancement module: half module: numerical-stability module: optimizer triaged,"['@mcarilli @csarofeen, who work on mixed precision.', ""I'm a little wary of this.  When @puririshi98 was running some experiments training networks with fp16 parameter values + stochastic rounding, at one point we ran into numerical instabilities and one of the things we tried was increasing the value of epsilon.  The numerical instabilities turned out to be caused by something completely unrelated to epsilon, but I recall that increasing epsilon in some cases impaired the converged accuracy, even for pure fp32 runs.  I can dig up the exact networks and results if needed.\r\n\r\nIn general the optimizer shouldn't update fp16 parameters directly, because small updates later in training won't be captured.  This is also true for bfloat16 (even more so in fact, because bfloat has fewer mantissa bits), and is totally independent from the value of epsilon.  Given that the optimizer should act on fp32 parameters, and that (in our anecdotal experience) increasing epsilon can impair accuracy, to be safe I lean towards keeping the value of epsilon where it is, even if 1e-8 flushes to zero in fp16."", 'i could see that. Using 1e-7 has made a huge different for my training on fp16 - especially with NCE loss. The 1e-8 WILL become 0 on fp16 (you can run a quick test by converting that value into .half() to verify). \r\n\r\ni‚Äôm worried that a lot of people using fp16 abandon projects because of stability issues likely caused by a default no one messes with. \r\n\r\nso, if 1e-8 = 0 when using half, it‚Äôs basically not serving any purpose as a default', 'this seems like a reasonable default for fp16, especially if someone does the updates in fp16 (rather naively).\r\n\r\n@williamFalcon will accept a PR to fix this.', '@williamFalcon  Why 1e-7 in particular? For some loss methods, even this is not sufficient: https://github.com/keras-team/keras/issues/11860. Maybe 1e-4 instead?\r\n\r\nAlso will this be the default for both ""pure"" fp16 training and the training which uses mixed precision techniques (like master copy in fp32)? \r\n\r\nIt might impact accuracy for the latter case and since the general recommendation is to do training with mixed precision (as opposed to pure half) due to reasons mentioned by @mcarilli above,  this might be silently running into accuracy loss as it might not be apparent to users that eps. value could be contributing to the loss.  This might be harder to debug than seeing inf/nan loss during training.', ""For parameters that are fp16, i guess it makes sense to increase epsilon to a value that's > 0 in fp16. But imo it doesn't make sense to train with pure fp16 parameters at all, at least not without stochastic rounding or some other fancy tricks that Pytorch optimizers don't currently implement.\r\n\r\nMy main recommendation is that when acting on fp32 params, we don't increase epsilon above its current value of 1e-8."", 'If the change is for fp16 params only, it should be fine. The only question is what should be the exact value , 1e-7, 1e-4, something else?', 'i chose 1e-7 because it is the smallest value which won‚Äôt become zero. I‚Äôm hesitant to use a larger value because it might bias gradients too much. My intuition here is to use the minimum value that won‚Äôt become zero. ', ""Is this still an issue?  Because Adam's eps is still 1e-8.\r\n\r\nOr does Lightning not actually perform backprop/updates in 16-bit?"", 'CC: @xwang233 @ptrblck ', '@williamFalcon If I understand the issue correctly, you are using pure FP16 training by calling `model.half()` in PyTorch Lightning and suggest to increase the `eps` values for `Adam` for these FP16 parameters or are you seeing the same issue using `amp`?']",[],[],0,0
207,pytorch,13929,closed,Maturing JIT Optimization Framework,"## üöÄ Feature
Taking a current look at the JIT optimization framework there are a couple of feature missing. 

We do not have a structure that captures post pass analysis of the optimization pass. Analysis are fundamental in optimization as they allow us to understand the necessary passes better. For example if PyTorch had a simple analysis per pass that calculated the number of nodes in the computational graph changed we could introduce features such as fixed point optimization. We could mature even further to Immutable passes (analysis passes) which outline in what order to apply passes to maximize performance. Currently we have a very trivial implementation of optimization which simply runs [passes in linear order](https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/graph_executor.cpp#L458).

Another feature are pass managers. PyTorch implicitly does this by defining families of optimization passes (e.g. PeepholeOptimizeImpl implements a family of peephole optimization). An explicit OO implementation of pass managers will substantially clean up the code.

Simplifying the implementation of passes will increase agility. There's a lot of code reuse happening with respect to iterating the graph and checking for a pattern to apply a transform. If we could implement type's of passes (e.g. PredicatePass/PatternMatchPass, LoopPass), we could simplify code and reduce code reuse. 

## Motivation
I'm hoping by introducing these concepts to PyTorch's JIT we could mature the optimization framework and increase the performance of PyTorch in general.

## Pitch

This same approach and features I've implemented in the [ONNX optimization framework](https://github.com/onnx/onnx/tree/master/onnx/optimizer). If we agree that PyTorch needs these features I can start working on this.",oncall: jit,"[""For the JIT, we should be focused on using it to make programs run faster, and do tasks that specifically improve performance or add more language features. I don't think we should be abstract because it might (possibly, in the future) be helpful to those end goals.  Abstracting first frequently ends up with a lot of useless abstractions. LLVM added pass managers because they had to manage the fact that some passes ran at different levels (basic block, function, module), and they needed to weave the smaller-scoped passes together into a single pass for efficiency and because it makes better code. Currently we only have module-level passes anyway, so this stuff would only make it harder for developers to understand what is happening. \r\n\r\nIt is very possible in the future we will end up with perf optimizations or language features that necessitate infrastructure like this, but for now I think it is best to wait. "", ""@zdevito This makes sense for pass managers. But for things like fixed point optimization where we require some form of pass analysis to implement efficiently how would we go about it with the current framework? Without any form of pass analysis it's impossible to do non-trivial ordering of passes. \r\n\r\nImplementing types of passes (e.g. PredicateBasedPass) helps simplify the code by removing code reuse and providing a clean implementation for the optimization pass (a good example is the ONNX codebase).\r\n\r\nThank you for your detailed reply!""]",[],[],0,0
208,pytorch,2470,closed,var(dim) and std(dim) are broken for FloatTensor,"It seems like  and  are broken on version 0.2.0.1. Please note that you need to pass in the  argument to reproduce this bug. The parameterless  and  are working correctly.

For example:
",,"['Reproduced with `0.2.0_1`, and DoubleTensor is ok.', ""I'd say this is due to precision errors during the computations, which are performed in single precision in the `var(0)` case, and in double precision in the `var()` case.\r\nTo inspect that, just perform the computations one by one on the terminal for example:\r\n```python\r\n# corresponds to the biased estimator\r\ndef var_all(t):\r\n    s = t.sum() / t.numel()\r\n    s2 = (t * t).sum() / t.numel()\r\n    print('E[t]^2:', s * s)\r\n    print('E[t^2]:', s2)\r\n    return max((s2 - s * s), 0)\r\n\r\ntensor = torch.DoubleTensor([2281.5, 2281.25])\r\nprint(var_all(tensor.float()))\r\nprint('-------------')\r\nprint(var_all(tensor))\r\n```\r\nPrints\r\n```\r\nE[t]^2: 5204671.890625\r\nE[t^2]: 5204671.75\r\n0\r\n-------------\r\nE[t]^2: 5204671.890625\r\nE[t^2]: 5204671.90625\r\n0.015625\r\n```\r\n\r\nBut, looking at the code, it seems that the accumulation [is done in `double` precision](https://github.com/pytorch/pytorch/blob/master/torch/lib/TH/generic/THTensorMath.c#L2996-L2997), so I'm not sure why this is happening. I'm probably missing something obvious here"", '@fmassa The final result of sum is still stored as float. The result is still rounded to the closest 32 bit float approximation. ', 'One alternative would be to use more numerically stable versions of variance: \r\nhttps://en.wikipedia.org/wiki/Algorithms_for_calculating_variance', 'iirc, accreal type for float is float, so the computation is done in float. At any rate, the method used is extremely unstable, Welford method would give much better results https://docs.google.com/document/d/1V3VEm9zQ17jsbWT9dxK_iS5qRZZenU6tdApj-VsbGk0/edit\r\n', ""@ngimel I thought that `accreal` in TH was `double` for `float`, and [it seems to be the case](https://github.com/pytorch/pytorch/blob/master/torch/lib/TH/THGenerateFloatType.h#L6) (even though in CUDA it's `float`). Anyway, the computation is indeed a bit unstable and might be better to improve it."", 'Ah, ok, I mixed up with cuda. A bit unsettling that cuda and cpu results are expected to be different. ', 'varall uses two-pass algorithm to calculate variance (calculate mean, subtract, calculate variance), as opposed to var(dim), which uses an unstable one-pass: https://github.com/pytorch/pytorch/blob/master/torch/lib/TH/generic/THTensorMath.c#L3146-L3153, that would explain the difference between var() and var(dim). Welford (one-pass) algorithm should be better than even two-pass. ', 'Actually, std/var implementations are inconsistent both in TH and THC, for tensors and for variables.\r\nFor tensors:\r\n1) in both TH and THC .var() is doing a two-pass variance calculation.\r\n2) in both TH and THC .var(dim) is doing one-pass unstable variance calculation, however, in TH it accumulates in double, in THC it accumulates in float for float inputs. Even with double accumulation, it is really unstable, as original poster has shown. For half type in cuda, it will be truncated to half at each step, so it is basically unusable. In cuda, special kernels are used for that.\r\n\r\nFor variables all the std calculations are done using two-pass algorithm, which is better than one-pass naive, but still worse than one-pass Welford. It is implemented via tensor ops (calculate mean, subtract mean, do pointwise multiply, reduce). For cuda, it also results in bad performance, because 5-7 kernels and a coupe of synchronous d2h transfers are launched for each variance computation. The following script compares tensor/var performance on cuda, with 3x-4x slower perf for variables.\r\n```.py\r\nimport torch\r\nimport time\r\nfrom torch.autograd import Variable\r\n\r\n\r\ndef timeit(fn, nrep, args=None):\r\n    torch.cuda.synchronize()\r\n    start = time.time()\r\n    for i in range(nrep):\r\n       if args is not None:\r\n           fn(args)\r\n       else:\r\n           fn()\r\n    torch.cuda.synchronize()\r\n    print((time.time()-start)*1000./float(nrep), ""ms"")\r\n\r\na=torch.randn(512,2048).cuda()\r\nnrep = 100\r\ntimeit(a.var, nrep)\r\ntimeit(a.var, nrep, 0)\r\ntimeit(a.var, nrep, 1)\r\na=Variable(a)\r\ntimeit(a.var, nrep)\r\ntimeit(a.var, nrep, 0)\r\ntimeit(a.var, nrep, 1)\r\n```\r\nAlso, almost everywhere mean and std are computed together, so mean computation is essentially repeated when std is computed. So the questions here are\r\n1) does it make sense to have mean_std api to compute mean and variance/std together?\r\n2) does it make sense to have special implementation for variables the way there are implementations now for tensors (though we cannot reuse current tensor implementations because they are broken)? It would also require special kernels for backward. It should help weight norm/layer norm layers. ', ""I've also noticed some weird behavior of std(dim) on FloatTensor, maybe it's related to this thread. Running the following code:\r\n```python\r\nfor i in range(16):\r\n    x = 10 ** i\r\n    t = torch.FloatTensor([x, x])\r\n    std = t.std()\r\n    std_0 = t.std(0)[0]\r\n    print('t.std(): {}'.format(std))\r\n    print('t.std(0): {}'.format(std_0))\r\n```\r\nThe output of t.std(0) will 'sometime' be nonzero.""]","['python\r\nimport torch\r\ntensor = torch.FloatTensor([2281.5, 2281.25])\r\ntensor.var(0) # => 0\r\ntensor.var()  # => 0.03125\r\n']","['var(dim)', 'std(dim)', 'dim', 'var()', 'std()']",0,0
209,pytorch,15676,closed,Tests for Vec256 classes,"## üöÄ Feature
We should have tests in C++ for the Vec256 classes.

## Motivation
Currently, the Vec256 code is only tested indirectly through operators that use it. This has led to insufficient test coverage and a number of recent bugs in Vec256 and the operators that use it. We should add sufficient test infrastructure so that we can easily add test cases for new functions on Vec256. It's important that the tests cover the different data types (float, double, int32, int64, etc.) and instruction sets (generic, avx, avx2).

This will require changes to CMake since the current test cases don't enable the AVX/AVX2 instruction sets.

cc @VitalyFedyunin",feature module: cpu module: tests module: vectorization triaged,"['I need this, too. I want to add power VSX support. Adding These direct tests definitely would be helpful for correctness check. \r\nAnd I believe that it is not a repetitive task. ', 'Totally agree that we need it. Just curious have you seen any implementations mistakes which could be detected by such tests?', '@quickwritereader I think it would be a very useful activity for unit tests to come into existence as part of the POWER work, if you can put that into your work scope.', 'I will start to add test cases and modify Cmake to handle it.  \r\n', ""I already added many test cases. I try to PR it on coming weeks. Right now I am trying to add check for float math functions as its' precision is implementation-dependent and tied to argument domains. \r\nThe test will use random initialization and special values  and rely on the correctness of the aligned load/store of vec256. As union like type puning is discouraged.\r\nActually, for 4bytes as float and less types we could do exhausted tests within valid domain range, too. But I think running randomized  tests several times is ok.\r\nThanks. just wanted to inform\r\n\r\n""]",[],[],0,0
210,pytorch,6968,closed,[feature request] Implement torch.stack() / _cat() for also for torch.HalfTensor,"## Issue Description
It seems that stacking is currently (PyTorch 0.4) is not supported for tensors of type ; see code example. 

I don't know if this is intentional for some reason (hence I didn't label this issue as a bug), but I couldn't find any issue or documentation pointing to that. Since some methods, like , depend on  (in this case implicitly through the  method), it would be great if either stacking for HalfTensors could get implemented, or the documentation could be updated to contain some kind of hint.

## Code Example


On PyTorch 0.3.1, one gets basically the same error when one tries to do the same thing:
",todo,"['we dont support any operation other than `copy` and serialization on torch.HalfTensor (the CPU version). The GPU HalfTensor supports all operations', '@soumith while we don‚Äôt want to have any math operations for half on CPU I feel like the argument given by @timothygebhard for having cat is very reasonable. Cat can be implemented efficiently because it doesn‚Äôt care how the data looks like, it‚Äôs only copying raw bytes around.', ""If you do this, it would probably be a good idea to port THTensor_(cat) to ATen while you're at it."", 'I got a very similar error: RuntimeError: _cat_out is not implemented for type torch.HalfTensor. To my mind, we could add support for this method for data preparation/loading on CPU so that a single tensor type (namely torch.half) is used on CPU and GPU.', 'I ran into this issue with a `DataLoader` that works on a `numpy.fp16` dataset. \r\nWriting a custom `collate_fn` for the dataloader solves the immediate problem, but it would be nice if it works out of the box.', ""I'm confused, why did we close this issue? There was an MR for this that looked like it worked, but was subsequently closed, together with this issue/"", ""as replied on the PR, it's merged into master, it's available in the nightlies, and will be part of 1.1""]","[""\r\nPython 3.6.5 (default, Mar 30 2018, 06:41:49)\r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 6.3.1 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: import torch\r\n\r\nIn [2]: torch.__version__\r\nOut[2]: '0.4.0'\r\n\r\nIn [3]: tensor_1_float = torch.tensor([1, 2, 3], dtype=torch.float)\r\n\r\nIn [4]: tensor_2_float = torch.tensor([4, 5, 6], dtype=torch.float)\r\n\r\nIn [5]: torch.stack([tensor_1_float, tensor_2_float])\r\nOut[5]:\r\ntensor([[ 1.,  2.,  3.],\r\n        [ 4.,  5.,  6.]])\r\n\r\nIn [6]: tensor_1_half = torch.tensor([1, 2, 3], dtype=torch.half)\r\n\r\nIn [7]: tensor_2_half = torch.tensor([4, 5, 6], dtype=torch.half)\r\n\r\nIn [8]: torch.stack([tensor_1_half, tensor_2_half])\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-8-10f49aa85ea5> in <module>()\r\n----> 1 torch.stack([tensor_1_half, tensor_2_half])\r\n\r\nRuntimeError: _cat is not implemented for type torch.HalfTensor\r\n"", ""\r\nTypeError: Type torch.HalfTensor doesn't implement stateless method cat\r\n""]","['HalfTensor', 'torch.utils.data.DataLoader', 'torch.stack()', 'default_collate()']",0,0
211,pytorch,6164,closed,feature requests and future roadmap,"PyTorch Feature Requests and Future RoadMap
--------------------------------
Hello everyone and thank your for this great project. I would like to point to some things that I find important for the future growth of pytorch. 

1. Easy name conventions throughout the library. I believe that some things should be renamed so that it resonates and are easier to remember. This will also lower the entry bar for new people exposed to pytorch.

Example:


The  or  doesn't mean anything to me and as soon as I use them my brain has already forgotten about it. A more sane naming good be  that seems to be a universal convention pretty much since other major libraries are using it.

I would also love to see something like ,  , , , , etc.

2. Model dissection. 
More often we find ourselves in a scenario where we would like to get the output of a complex model at different levels.

Example:


3. Dataset preprocessing and transformation pipeline switch between cpu/gpu
Many times we find ourselves the need to do the data preprocessing on the gpu especially in the scenario where there is an abundance. But the choice at the moment is inexistent. There should be a flag for the user to easily switch between cpu and gpu upon request in order to chose where the data preprocessing should take place.

4. Addition of negative indexing in Tensors.

5. Integration and applicability with already pre-existing tools.
Some tools that already pre-exist and are nice to have an out of the box support and integration with them. For instance tensorboard is great but at the moment we need to rely on third party developed plugins for this to work. Instead it would be better if pytorch had the ability to dump logs that can be consumed by tensorboard directly. 

5. Serving models is completely inexistent at the moment of writing.

For anyone else please feel free to add anything else you might think is important for future directions.
Basically I just watched the whole tf dev summit and the one advantage that pytorch might had against tf was the dynamic models but now that's gonna with eager mode. And since folks at tf have faster release cycles they can fail and iterate faster on concepts and ideas. A lot of people where nagging about the language choices which where horrific but since then they have iterated over a lot and brought it up to a sane level .

These things make me wonder about the future directions and where pytorch is heading?


Thank you!
",,"[""1. `torch.save` and `torch.load` may work as `model.save` and `model.load`.\r\n2. I don't think `pytorch` uses `Layer`\r\n3. `torch.set_default_tensor_type('torch.cuda.DoubleTensor')` should probably solve your problem.\r\n4. `tensor[-4]` is already supported.\r\n5.  refer to https://github.com/lanpa/tensorboard-pytorch\r\n6.  agree with you.\r\n\r\nCorrect me if I misunderstand."", ""@Stonesjtu Thank you for the input.\r\n\r\n> torch.set_default_tensor_type('torch.cuda.DoubleTensor') should probably solve your problem.\r\n\r\nDoes that mean that whenever I construct the Dataset api all the preprocessing is gonna happen on the gpu? My understanding from reading around the forums was that something like that didn't exist unless I'm mistaken?\r\n"", ""Technically that mean when you runs `b = torch.Tensor(5,5)`, `b` will be on GPU by default. I don't quite understand the need for pre-processing data on GPU actually."", ""@Stonesjtu \r\n\r\n> I don't quite understand the need for pre-processing data on GPU actually.\r\n\r\nSay you have many gpus, instead of doing the pre-processing on cpu you could leverage the gpus that are sitting there doing nothing.  That would speed up things a lot and during training the actual gpus that are responsible for the computation won't have to wait the cpu to finish to load data into gpu memory.  Things can happen in parallel especially if your pre-processing is computationally expensive."", 'You can already to data preprocessing on GPU in dataloader workers with spawn method.', '@SsnL can you provide some pointers or minimal working example?', '`nn.Module` represents a tree of modules. It has nothing to do with neural network layers.', '@goldsborough Thanks Peter, but I was thinking how hard it is to rename things. Just to have a consistency across libraries. One can transfer previous knowledge acquired by using some other library directly to pytorch without overhead. That would make the transition  easier for newcomers and increase the visibility of the library.', '@kirk86 if all libraries were to be exactly the same, then we would effectively have a single library üòÑ I agree with @goldsborough, and this is the reason why `Module`s will never be renamed to `Layers`. They are just different things, and renaming would only add confusion.', ""@apaszke Thanks for the reply. I wasn't of course implying to have a single library üòÜ . I understand that there are differences but sane easy to remember naming conventions that register immediately to your long term memory are quite helpful to get to speed up with the library. But I do get it, I understand the reason. Thanks!""]","[""\r\nimport torch\r\nimport torchvision\r\nimport torchvision.models as models\r\nmodel = models.densenet201()\r\n\r\nto iterate over the layers I'll have to do the following\r\n\r\nfor idx, m in enumerate(model.named_modules):\r\n"", ""\r\nlet's say we have the model = models.densenet201()\r\nto get the output at different levels we would have to do something like this\r\nmodel1 = nn.Sequential(*list(model.children())[:4])\r\nmodel2 = nn.Sequential(*list(model.children())[:7])\r\n\r\nWhich feels more of a hack to me TBH.\r\n\r\nI would much  rather prefer to do the following:\r\n\r\nmodel1 = model.layers.layer_name or model.layers[4].output\r\n""]","['named_modules', 'named_children', 'model.layers', 'model.layers.add(x)', 'model.layers.pop(x)', 'model.layers.get(x)', 'model.save()', 'model.load()', 'tf.layers']",0,0
212,pytorch,25172,closed,test_det_logdet_slogdet_batched (in test_cuda.py) fails on ppc64le,"## üêõ Bug



https://github.com/pytorch/pytorch/blob/v1.2.0/test/test_cuda.py#L2224 fails for me on ppc64le.

On following the test through to https://github.com/pytorch/pytorch/blob/v1.2.0/test/test_torch.py#L6558 and then testing all the different possible combinations, the failures are seen for all combinations of  and  in both  and  (the other tests in there are fine).

I went through and commented out the assert statements and printed out the tensors that were being compared for each of the combinations:



## To Reproduce

Steps to reproduce the behavior:

1. Build from source on ppc64le
1. Run the tests

## Expected behavior

The test should pass

## Environment

PyTorch version: 1.2.0
Is debug build: No
CUDA used to build PyTorch: 10.1.105

OS: Red Hat Enterprise Linux
GCC version: (GCC) 8.2.0
CMake version: version 3.13.3

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.1.105
GPU models and configuration: 
GPU 0: Tesla V100-SXM2-16GB
GPU 1: Tesla V100-SXM2-16GB

Nvidia driver version: 418.67
cuDNN version: 7.4.2.24
magma version: 7.5.1

Versions of relevant libraries:
[pip3] numpy==1.16.2
[pip3] torch==1.2.0
[conda] Could not collect

PyTorch built from source
Build command: PYTORCH_BUILD_VERSION=1.2.0 PYTORCH_BUILD_NUMBER=1 VERBOSE=1 LDFLAGS=""$LDFLAGS -ldl"" CFLAGS="""" CXXFLAGS="""" USE_FFMPEG=ON USE_GLOO_IBVERBS=1 USE_GFLAGS=ON USE_GLOG=ON CUDNN_LIB_DIR=$EBROOTCUDNN/lib64 CUDNN_INCLUDE_DIR=$EBROOTCUDNN/include TORCH_CUDA_ARCH_LIST=""3.5 6.0 7.0""  /rds/bear-apps/devel/2019a/branfosj-eb-pytorch/EL7/EL7-power9/software/Python/3.7.2-GCCcore-8.2.0/bin/python setup.py build

",module: POWER module: flaky-tests module: tests triaged,"['cc @vishwakftw @avmgithub ', 'cc @hartb @dncliss', ""This may not be a problem in pytorch itself. Can you try with this change to use the open-coded reference, rather than calculating the reference with numpy.linalg?\r\n\r\n```\r\ndiff --git a/test/test_torch.py b/test/test_torch.py\r\nindex 0bde722d41..4a086ebb90 100644\r\n--- a/test/test_torch.py\r\n+++ b/test/test_torch.py\r\n@@ -6333,7 +6333,7 @@ class _TestTorchMixin(object):\r\n     @staticmethod\r\n     def _test_det_logdet_slogdet(self, device):\r\n         def reference_slogdet(M):\r\n-            if TEST_NUMPY:\r\n+            if False:\r\n                 sdet, logabsdet = np.linalg.slogdet(M.detach().cpu().numpy())\r\n                 return M.new_tensor(sdet), M.new_tensor(logabsdet)\r\n             else:\r\n```\r\n\r\nThat's been working for us, and we've seen similar with a couple other tests using numpy.linalg to calculate a reference. We're currently suspecting the OpenBLAS underlying numpy, but still investigating."", '@hartb The problem I am seeing is in `_test_det_logdet_slogdet_batched` not `_test_det_logdet_slogdet`.\r\n\r\nI have extracted out the one test and set `TEST_NUMPY = False` and `TEST_SCIPY = False` in `common_utils.py`. Then I print the tensors rather than use the `assertEqual` (as that function has a `numpy.bool_` check in it that fails once `TEST_NUMPY = False` has been set). Then I can run the test without an `import numpy` and that is how I generated the output in my first message.', ""Ah, yes--I assumed without checking that both tests would use common reference code.\r\n\r\nI'm also seeing the failure in the `batched` case, but seems like only in the `device='cuda'` case. Checking to see if I can come up with a small testcase to dig into"", '@hartb could you check if the matrices are being generated correctly? We use LAPACK (CPU) for generating them, so it could be possible that they are not being generated correctly if there is an existing issue with OpenBLAS integration.', ""I think it may not be the generation of the matrices. Here's a test with fixed input that gives differing results on x86 vs Power:\r\n\r\n```\r\n$ cat test.py\r\nimport torch\r\n\r\ntorch.set_printoptions(precision=12)\r\n\r\nt = torch.tensor([[[ 0.294988564756,  0.449998498770, -0.784137990498],\r\n         [-0.021970818344, -0.033515995034,  0.058402783714],\r\n         [-0.834089623986, -1.272385181926,  2.217175307076]],\r\n\r\n        [[ 0.169896779706,  0.546266650404, -0.310455111930],\r\n         [-0.212337271404, -0.682724947503,  0.388007303462],\r\n         [ 0.003353761784,  0.010783301598, -0.006128382727]],\r\n\r\n        [[-0.680596452631, -0.883266052975, -0.975217012922],\r\n         [ 0.300852740259,  0.390441371517,  0.431087628433],\r\n         [ 0.822925802886,  1.067978569380,  1.179158722096]]],\r\n        device='cuda')\r\n\r\na1 = torch.logdet(t)\r\n\r\nt = t.cpu()\r\n\r\na2 = torch.logdet(t)\r\n\r\nprint(a1)\r\nprint(a2)\r\n```\r\n\r\nOn x86:\r\n\r\n```\r\n$ python test.py\r\ntensor([-36.210922241211, -40.601089477539, -34.499458312988], device='cuda:0')\r\ntensor([-36.210922241211, -40.601089477539, -34.499458312988])\r\n```\r\n\r\nOn Power:\r\n\r\n```\r\n$ python test.py\r\ntensor([-36.210922241211, -40.601089477539, -34.499458312988], device='cuda:0')\r\ntensor([-36.918212890625, -39.672676086426, -34.159103393555])\r\n```\r\n\r\nSo the odd man out here is the calculation of `logdet()` on CPU on Power.\r\n"", 'And similar results if I compare output of `torch.lu()` on that input: x86 CPU, x86 GPU, and Power GPU all match; Power CPU is outlier.\r\n\r\nSo looks like that may well go back to the LAPACK code.', 'I take back the previous; looks like the `lu()` was just choosing different pivots on Power CPU; get similar results on x86 an Power after taking that into account. Still looking.', ""Maybe an interesting result with a modified version of the test:\r\n\r\n```\r\n$ cat test.py\r\nimport torch\r\n\r\ntorch.set_printoptions(precision=12)\r\n\r\nT = torch.tensor([[[ 0.294988564756,  0.449998498770, -0.784137990498],\r\n         [-0.021970818344, -0.033515995034,  0.058402783714],\r\n         [-0.834089623986, -1.272385181926,  2.217175307076]],\r\n\r\n        [[ 0.169896779706,  0.546266650404, -0.310455111930],\r\n         [-0.212337271404, -0.682724947503,  0.388007303462],\r\n         [ 0.003353761784,  0.010783301598, -0.006128382727]],\r\n\r\n        [[-0.680596452631, -0.883266052975, -0.975217012922],\r\n         [ 0.300852740259,  0.390441371517,  0.431087628433],\r\n         [ 0.822925802886,  1.067978569380,  1.179158722096]]])\r\n\r\nfor dev in ['cpu', 'cuda']:\r\n    T = T.to(dev)\r\n    print(f'Batch  {dev:4} {torch.logdet(T)}')\r\n    print(f'Manual {dev:4} {torch.stack([torch.logdet(t) for t in T])}')\r\n```\r\n\r\nOn x86, we get common results across CPU vs GPU, and across batched vs non-batched:\r\n\r\n```\r\n$ python test.py\r\nBatch  cpu  tensor([-36.210922241211, -40.601089477539, -34.499458312988])\r\nManual cpu  tensor([-36.210922241211, -40.601089477539, -34.499458312988])\r\nBatch  cuda tensor([-36.210922241211, -40.601089477539, -34.499458312988], device='cuda:0')\r\nManual cuda tensor([-36.210922241211, -40.601089477539, -34.499458312988], device='cuda:0')\r\n```\r\n\r\nOn Power, we get a result that matches x86 in the batched-GPU case, and different-but-common results in all the other cases:\r\n\r\n```\r\nBatch  cpu  tensor([-36.918212890625, -39.672676086426, -34.159103393555])\r\nManual cpu  tensor([-36.918212890625, -39.672676086426, -34.159103393555])\r\nBatch  cuda tensor([-36.210922241211, -40.601089477539, -34.499458312988], device='cuda:0')\r\nManual cuda tensor([-36.918212890625, -39.672676086426, -34.159103393555], device='cuda:0')\r\n```\r\n\r\n@vishwakftw does this suggest anything about the cause to you? I see that the CPU implementation of `apply_lu()` gets into common code for both the bached and non-batched cases, while the GPU implementation differs between batched and non-. That's in harmony with batched GPU being different from the other results on Power, but I'm not sure what in those various code paths might contribute to the actual difference in results.\r\n\r\nLooks like one difference in those paths is how / where the pivot calculation is done, and I did see a difference in OpenBLAS/LAPACK `sgetrf_()`'s pivot choice between x86 and Power.\r\n\r\nI'm more a systems than linear algebra person, so not sure how concerned I should be about that."", ""The determinants of those three are: -1.0408340855860843e-17, -2.168404344971009e-19, and 0. So two are very close to being singular and one is. This means that there'll be particularly susceptible to accumulation of numerical errors during the decomposition (apply_lu) process and differences in pivot choices is a strong possibility for the differences we are seeing in results."", ""And I guess even small differences in those very small determinants would map to much larger than tolerated differences in logdet() and slogdet().\r\n\r\nI'm also seeing seeing some diffences in the cases where the calculated determinant is very small negative vs expected 0, which then maps to `nan` vs `-inf` in the logdet() results, which the test also won't tolerate.\r\n\r\nLooking to see if I can find a failure with larger determinants."", ""For me the failure occurs only in the `sing` case of the test, where `random_square_matrix_of_rank()` is called to construct a matrix with rank < size // 2. Those input matrices have the vanishingly-small determinants and the test failure is then either:\r\n\r\n- small difference in _very small_ det() => gross difference in logdet() & slogdet()\r\n- _very small_ det() calculated as 0 on one platform vs very small negative on the other, leading to `-inf` vs `nan` miscompare\r\n\r\nFor the other test flavors, where the determinant isn't vanishingly small, the two platforms agree within the expected tolerance.\r\n\r\n@vishwakftw Looks like you added these tests and that input construction routine. Does this make sense to you? Are those very small determinants expected given the construction method? And if so, should the `'sing'` tests be more tolerant of logdet() / slogdet() differences?"", 'I can understand the point of failure - thank you @hartb for the detailed analysis! I will look into resolving the issue as soon as possible, and this might include rewriting the random_square_matrix_of_rank function.', ""Thank you @vishwakftw; we'll watch for a change to land.  And thanks really to @branfosj for pointing out that the determinants were very small. That was the key.""]","['\r\n    @unittest.skipIf(not TEST_MAGMA, ""no MAGMA library detected"")\r\n    def test_det_logdet_slogdet_batched(self):\r\n        _TestTorchMixin._test_det_logdet_slogdet_batched(self, \'cuda\')\r\n', ""\r\n3\r\n(3,)\r\n    run_test(matsize, batchdims, mat_chars=['sing'])\r\n        self.assertEqual(actual_value, expected_value, allow_inf=True)\r\ntensor([-2.5175e-35, -1.5868e-38, -9.0737e-36], device='cuda:0')\r\ntensor([-4.0940e-35,  0.0000e+00,  0.0000e+00], device='cuda:0')\r\n\r\n        self.assertEqual(actual_value, expected_value, allow_inf=True)\r\ntensor([nan, nan, nan], device='cuda:0')\r\ntensor([nan, -inf, -inf], device='cuda:0')\r\n\r\n        self.assertEqual(sign_value, actual_value[0], allow_inf=True)\r\ntensor([-1.,  0.,  0.], device='cuda:0')\r\ntensor([-1., -1., -1.], device='cuda:0')\r\n\r\n        self.assertEqual(expected_value, actual_value[1], allow_inf=True)\r\ntensor([-79.1810,     -inf,     -inf], device='cuda:0')\r\ntensor([-79.6672, -87.0365, -80.6877], device='cuda:0')\r\n\r\n    run_test(matsize, batchdims, mat_chars=['sing', 'non_sing'])\r\n        self.assertEqual(actual_value, expected_value, allow_inf=True)\r\ntensor([ 3.8190e-34, -3.9619e-01, -3.5083e-33], device='cuda:0')\r\ntensor([ 6.4256e-34, -3.9619e-01, -1.0143e-32], device='cuda:0')\r\n\r\n        self.assertEqual(actual_value, expected_value, allow_inf=True)\r\ntensor([-76.9479,      nan,      nan], device='cuda:0')\r\ntensor([-76.4276,      nan,      nan], device='cuda:0')\r\n\r\n        self.assertEqual(sign_value, actual_value[0], allow_inf=True)\r\ntensor([ 1., -1., -1.], device='cuda:0')\r\ntensor([ 1., -1., -1.], device='cuda:0')\r\n\r\n        self.assertEqual(expected_value, actual_value[1], allow_inf=True)\r\ntensor([-76.4276,  -0.9259, -73.6685], device='cuda:0')\r\ntensor([-76.9479,  -0.9259, -74.7302], device='cuda:0')\r\n\r\n\r\n3 \r\n(5, 3)\r\n    run_test(matsize, batchdims, mat_chars=['sing'])\r\n        self.assertEqual(actual_value, expected_value, allow_inf=True)\r\ntensor([[ 1.7500e-33, -3.9282e-36,  4.5183e-36],\r\n        [ 9.6257e-34,  8.1698e-35, -1.6681e-40],\r\n        [ 1.2369e-34,  1.6281e-35, -1.5470e-33],\r\n        [ 2.4720e-33, -2.6346e-33, -6.8148e-35],\r\n        [ 3.7382e-35, -1.8944e-33,  5.6313e-35]], device='cuda:0')\r\ntensor([[ 0.0000e+00,  0.0000e+00, -0.0000e+00],\r\n        [ 1.0046e-33,  6.8543e-35, -0.0000e+00],\r\n        [-0.0000e+00, -0.0000e+00,  0.0000e+00],\r\n        [ 2.3483e-33,  0.0000e+00,  0.0000e+00],\r\n        [ 0.0000e+00, -2.2742e-33,  0.0000e+00]], device='cuda:0')\r\n  \r\n        self.assertEqual(actual_value, expected_value, allow_inf=True)\r\ntensor([[-75.4257,      nan, -81.3849],\r\n        [-76.0235, -78.4900,      nan],\r\n        [-78.0753, -80.1031,      nan],\r\n        [-75.0803,      nan,      nan],\r\n        [-79.2719,      nan, -78.8621]], device='cuda:0')\r\ntensor([[    -inf,     -inf,     -inf],\r\n        [-75.9808, -78.6656,     -inf],\r\n        [    -inf,     -inf,     -inf],\r\n        [-75.1316,     -inf,     -inf],\r\n        [    -inf,      nan,     -inf]], device='cuda:0')\r\n  \r\n        self.assertEqual(sign_value, actual_value[0], allow_inf=True)\r\ntensor([[ 0.,  0., -0.],\r\n        [ 1.,  1., -0.],\r\n        [-0., -0.,  0.],\r\n        [ 1.,  0.,  0.],\r\n        [ 0., -1.,  0.]], device='cuda:0')\r\ntensor([[ 1., -1.,  1.],\r\n        [ 1.,  1., -1.],\r\n        [ 1.,  1., -1.],\r\n        [ 1., -1., -1.],\r\n        [ 1., -1.,  1.]], device='cuda:0')\r\n  \r\n        self.assertEqual(expected_value, actual_value[1], allow_inf=True)\r\ntensor([[    -inf,     -inf,     -inf],\r\n        [-75.9808, -78.6656,     -inf],\r\n        [    -inf,     -inf,     -inf],\r\n        [-75.1316,     -inf,     -inf],\r\n        [    -inf, -75.1637,     -inf]], device='cuda:0')\r\ntensor([[-75.4257, -81.5249, -81.3849],\r\n        [-76.0235, -78.4900, -91.5917],\r\n        [-78.0753, -80.1031, -75.5490],\r\n        [-75.0803, -75.0166, -78.6714],\r\n        [-79.2719, -75.3464, -78.8621]], device='cuda:0')\r\n\r\n    run_test(matsize, batchdims, mat_chars=['sing', 'non_sing'])\r\n        self.assertEqual(actual_value, expected_value, allow_inf=True)\r\ntensor([[-7.2073e-35, -9.6858e-01,  2.1245e-34],\r\n        [-2.1929e-02, -3.3690e-33,  8.4981e-02],\r\n        [ 2.3438e-36,  8.2995e-01, -4.4813e-33],\r\n        [ 2.1107e+00,  3.5198e-34, -1.5583e+00],\r\n        [ 1.3380e-36,  2.1991e+00, -2.2806e-34]], device='cuda:0')\r\ntensor([[-0.0000e+00, -9.6858e-01,  0.0000e+00],\r\n        [-2.1929e-02, -2.5940e-33,  8.4981e-02],\r\n        [-0.0000e+00,  8.2995e-01, -0.0000e+00],\r\n        [ 2.1107e+00,  3.0731e-34, -1.5583e+00],\r\n        [ 2.6372e-36,  2.1991e+00, -0.0000e+00]], device='cuda:0')\r\n\r\n        self.assertEqual(actual_value, expected_value, allow_inf=True)\r\ntensor([[     nan,      nan, -77.5343],\r\n        [     nan,      nan,  -2.4653],\r\n        [-82.0413,  -0.1864,      nan],\r\n        [  0.7470, -77.0295,      nan],\r\n        [-82.6019,   0.7880,      nan]], device='cuda:0')\r\ntensor([[    -inf,      nan,     -inf],\r\n        [     nan,      nan,  -2.4653],\r\n        [    -inf,  -0.1864,     -inf],\r\n        [  0.7470, -77.1652,      nan],\r\n        [-81.9234,   0.7880,     -inf]], device='cuda:0')\r\n\r\n        self.assertEqual(sign_value, actual_value[0], allow_inf=True)\r\ntensor([[-0., -1.,  0.],\r\n        [-1., -1.,  1.],\r\n        [-0.,  1., -0.],\r\n        [ 1.,  1., -1.],\r\n        [ 1.,  1., -0.]], device='cuda:0')\r\ntensor([[-1., -1.,  1.],\r\n        [-1., -1.,  1.],\r\n        [ 1.,  1., -1.],\r\n        [ 1.,  1., -1.],\r\n        [ 1.,  1., -1.]], device='cuda:0')\r\n\r\n        self.assertEqual(expected_value, actual_value[1], allow_inf=True)\r\ntensor([[       -inf, -3.1922e-02,        -inf],\r\n        [-3.8199e+00, -7.5032e+01, -2.4653e+00],\r\n        [       -inf, -1.8638e-01,        -inf],\r\n        [ 7.4703e-01, -7.7165e+01,  4.4361e-01],\r\n        [-8.1923e+01,  7.8803e-01,        -inf]], device='cuda:0')\r\ntensor([[-7.8615e+01, -3.1922e-02, -7.7534e+01],\r\n        [-3.8199e+00, -7.4771e+01, -2.4653e+00],\r\n        [-8.2041e+01, -1.8638e-01, -7.4485e+01],\r\n        [ 7.4703e-01, -7.7029e+01,  4.4361e-01],\r\n        [-8.2602e+01,  7.8803e-01, -7.7463e+01]], device='cuda:0')\r\n\r\n\r\n5\r\n(3,)\r\n    run_test(matsize, batchdims, mat_chars=['sing'])\r\n        self.assertEqual(actual_value, expected_value, allow_inf=True)\r\ntensor([ 1.0570e-49, -1.6987e-49,  9.0382e-51], device='cuda:0')\r\ntensor([ 0.0000e+00, -4.2903e-49,  3.6495e-50], device='cuda:0')\r\n\r\n        self.assertEqual(actual_value, expected_value, allow_inf=True)\r\ntensor([-112.7712,       nan, -115.2304], device='cuda:0')\r\ntensor([     -inf,       nan, -113.8347], device='cuda:0')\r\n\r\n        self.assertEqual(sign_value, actual_value[0], allow_inf=True)\r\ntensor([ 0., -1.,  1.], device='cuda:0')\r\ntensor([ 1., -1.,  1.], device='cuda:0')\r\n  \r\n        self.assertEqual(expected_value, actual_value[1], allow_inf=True)\r\ntensor([     -inf, -111.3703, -113.8347], device='cuda:0')\r\ntensor([-112.7712, -112.2968, -115.2304], device='cuda:0')\r\n  \r\n    run_test(matsize, batchdims, mat_chars=['sing', 'non_sing'])\r\n        self.assertEqual(actual_value, expected_value, allow_inf=True)\r\ntensor([-7.0089e-49, -2.0986e+00,  6.3015e-51], device='cuda:0')\r\ntensor([-6.6740e-49, -2.0986e+00,  1.0697e-49], device='cuda:0')\r\n  \r\n        self.assertEqual(actual_value, expected_value, allow_inf=True)\r\ntensor([      nan,       nan, -115.5911], device='cuda:0')\r\ntensor([      nan,       nan, -112.7593], device='cuda:0')\r\n  \r\n        self.assertEqual(sign_value, actual_value[0], allow_inf=True)\r\ntensor([-1., -1.,  1.], device='cuda:0')\r\ntensor([-1., -1.,  1.], device='cuda:0')\r\n  \r\n        self.assertEqual(expected_value, actual_value[1], allow_inf=True)\r\ntensor([-110.9285,    0.7413, -112.7593], device='cuda:0')\r\ntensor([-110.8795,    0.7413, -115.5911], device='cuda:0')\r\n\r\n\r\n5 \r\n(5, 3)\r\n    run_test(matsize, batchdims, mat_chars=['sing'])\r\n        self.assertEqual(actual_value, expected_value, allow_inf=True)\r\ntensor([[-5.4639e-49,  1.0809e-48, -1.4355e-48],\r\n        [ 4.0162e-50, -1.6792e-49, -1.4588e-49],\r\n        [ 4.5016e-49, -6.4845e-49,  4.9028e-49],\r\n        [ 8.8669e-49, -1.1956e-50, -9.0934e-50],\r\n        [ 1.0180e-50, -2.3492e-49,  2.9704e-50]], device='cuda:0')\r\ntensor([[-0.0000e+00,  1.0751e-48,  5.9039e-49],\r\n        [ 0.0000e+00, -4.9508e-50,  0.0000e+00],\r\n        [ 5.8240e-49, -1.3868e-48,  0.0000e+00],\r\n        [ 8.6981e-49, -1.3410e-49,  0.0000e+00],\r\n        [-6.8240e-51, -1.2646e-48, -3.6321e-49]], device='cuda:0')\r\n  \r\n        self.assertEqual(actual_value, expected_value, allow_inf=True)\r\ntensor([[      nan, -110.4463,       nan],\r\n        [-113.7389,       nan,       nan],\r\n        [-111.3222,       nan, -111.2369],\r\n        [-110.6443,       nan,       nan],\r\n        [-115.1115,       nan, -114.0406]], device='cuda:0')\r\ntensor([[     -inf, -110.4516, -111.0511],\r\n        [     -inf,       nan,      -inf],\r\n        [-111.0647,       nan,      -inf],\r\n        [-110.6636,       nan,      -inf],\r\n        [      nan,       nan,       nan]], device='cuda:0')\r\n  \r\n        self.assertEqual(sign_value, actual_value[0], allow_inf=True)\r\ntensor([[-0.,  1.,  1.],\r\n        [ 0., -1.,  0.],\r\n        [ 1., -1.,  0.],\r\n        [ 1., -1.,  0.],\r\n        [-1., -1., -1.]], device='cuda:0')\r\ntensor([[-1.,  1., -1.],\r\n        [ 1., -1., -1.],\r\n        [ 1., -1.,  1.],\r\n        [ 1., -1., -1.],\r\n        [ 1., -1.,  1.]], device='cuda:0')\r\n  \r\n        self.assertEqual(expected_value, actual_value[1], allow_inf=True)\r\ntensor([[     -inf, -110.4516, -111.0511],\r\n        [     -inf, -113.5297,      -inf],\r\n        [-111.0647, -110.1971,      -inf],\r\n        [-110.6636, -112.5332,      -inf],\r\n        [-115.5114, -110.2894, -111.5369]], device='cuda:0')\r\ntensor([[-111.1285, -110.4463, -110.1625],\r\n        [-113.7389, -112.3083, -112.4491],\r\n        [-111.3222, -110.9573, -111.2369],\r\n        [-110.6443, -114.9506, -112.9217],\r\n        [-115.1115, -111.9726, -114.0406]], device='cuda:0')\r\n  \r\n    run_test(matsize, batchdims, mat_chars=['sing', 'non_sing'])\r\n        self.assertEqual(actual_value, expected_value, allow_inf=True)\r\ntensor([[ 2.0255e-49, -4.7865e-01,  5.8494e-50],\r\n        [-3.8510e+00,  2.4657e-49,  4.6330e-01],\r\n        [-9.1485e-51,  2.4230e-02, -5.0224e-49],\r\n        [ 7.1769e-01, -1.7340e-49, -4.0999e+00],\r\n        [-1.5233e-48, -5.1991e-01,  1.5723e-49]], device='cuda:0')\r\ntensor([[ 4.0698e-49, -4.7865e-01,  0.0000e+00],\r\n        [-3.8510e+00, -4.9829e-49,  4.6330e-01],\r\n        [ 0.0000e+00,  2.4230e-02,  0.0000e+00],\r\n        [ 7.1769e-01,  0.0000e+00, -4.0999e+00],\r\n        [ 0.0000e+00, -5.1991e-01, -2.2120e-49]], device='cuda:0')\r\n  \r\n        self.assertEqual(actual_value, expected_value, allow_inf=True)\r\ntensor([[-112.1209,       nan, -113.3629],\r\n        [      nan, -111.9242,   -0.7694],\r\n        [      nan,   -3.7202,       nan],\r\n        [  -0.3317,       nan,       nan],\r\n        [      nan,       nan, -112.3741]], device='cuda:0')\r\ntensor([[-111.4231,       nan,      -inf],\r\n        [      nan,       nan,   -0.7694],\r\n        [     -inf,   -3.7202,      -inf],\r\n        [  -0.3317,      -inf,       nan],\r\n        [     -inf,       nan,       nan]], device='cuda:0')\r\n  \r\n        self.assertEqual(sign_value, actual_value[0], allow_inf=True)\r\ntensor([[ 1., -1.,  0.],\r\n        [-1., -1.,  1.],\r\n        [ 0.,  1.,  0.],\r\n        [ 1.,  0., -1.],\r\n        [ 0., -1., -1.]], device='cuda:0')\r\ntensor([[ 1., -1.,  1.],\r\n        [-1.,  1.,  1.],\r\n        [-1.,  1., -1.],\r\n        [ 1., -1., -1.],\r\n        [-1., -1.,  1.]], device='cuda:0')\r\n  \r\n        self.assertEqual(expected_value, actual_value[1], allow_inf=True)\r\ntensor([[-111.4231,   -0.7368,      -inf],\r\n        [   1.3483, -111.2207,   -0.7694],\r\n        [     -inf,   -3.7202,      -inf],\r\n        [  -0.3317,      -inf,    1.4110],\r\n        [     -inf,   -0.6541, -112.0328]], device='cuda:0')\r\ntensor([[-112.1209,   -0.7368, -113.3629],\r\n        [   1.3483, -111.9242,   -0.7694],\r\n        [-115.2183,   -3.7202, -111.2128],\r\n        [  -0.3317, -112.2762,    1.4110],\r\n        [-110.1032,   -0.6541, -112.3741]], device='cuda:0')\r\n""]","['matsize', 'batchdims', ""run_test(matsize, batchdims, mat_chars=['sing'])"", ""run_test(matsize, batchdims, mat_chars=['sing', 'non_sing'])""]",0,0
213,pytorch,20855,closed,pytorch0.4 -> pytorch1.0.1 RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation,"The code run well in pytorch  0.4 . But in pytorch1.0.1, I got the error  as following. How to locate the question and resolve it? Thanks very much.
 File ""G:/20190215-backup/bsandnielianToImage/optimizer_ExpAnglesplit_faceseg_splitBs_faceExpRecong.py"", line 215, in optimizer
    loss.backward()
  File ""E:\Program Files\Python35\lib\site-packages\torch\tensor.py"", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File ""E:\Program Files\Python35\lib\site-packages\torch\autograd\__init__.py"", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation",,['I resolve the issue by replacing\r\na += b\r\nto\r\na = a + b\r\n'],[],[],0,0
214,pytorch,6258,closed,"[feature request] use mkl_vml.h for exp, log vectorization on CPU","@cpuhrsch #6192 disables imprecise vectorized function from avx_mathfunc.h
[vml](https://software.intel.com/en-us/mkl-developer-reference-c-vm-mathematical-functions) is a vector math library from MKL, both performance and precision is promised. As long as  and  is installed in conda, vml can be used in ATen.
@MlWoo was preparing the code previously, the job got interrupted since he is in hospital lately :(
Below is the performance comparison of avx_mathfunc and vml in exp. Though the performance improvement is not so much, the precision is guaranteed.
if this looks ok to you, we will continue the job, after @MlWoo finish his therapy anyway.
",,"['@mingfeima that sounds like a nice addition! Please only remember that it would have to remain an optional, albeit recommended, path.', 'Hello @mingfeima, I hope @MlWoo is doing well and will feel good again soon! \r\n\r\nThis looks great! Will [SVML](https://software.intel.com/en-us/node/524289) deprecated/replaced by this? Will VML be open-sourced soon?', ""I would prefer integrating an open-source library rather than additional proprietary libraries as long as performance is similar. https://github.com/shibatch/sleef in particular looks promising. (I believe it's under review for inclusion into LLVM)\r\n\r\nVML, if I remember correctly, used to always execute the slow path on AMD CPUs [[1]](http://www.agner.org/optimize/blog/read.php?i=49). @mingfeima is this still the case?"", ""@apaszke well noticed, if we do it, it will be an optional, albeit recommended, path\r\n\r\n@cpuhrsch SVML provides similar functionality, but it is not under MKL, it is a icc submodule. Once you have mkl installed via conda, you have VML. Only in case you have icc installed, you have SVML. So based on current pytorch dependency `conda install numpy pyyaml mkl mkl-include setuptools cmake cffi typing`, VML is a better option than SVML.\r\n\r\n@colesbury if the open source library [sleef](https://github.com/shibatch/sleef) provides similar performance and also precision is guaranteed, it's going to be the best. Have you started work on that?"", ""@mingfeima I'm looking into sleef next after what I'm currently working on.""]",['\r\navx_mathfunc.h\r\nexp:            size: 10^2      elapsed avg: 0.00018437 variance: 0.00002326    type: torch.FloatTensor\r\nexp:            size: 10^3      elapsed avg: 0.00051801 variance: 0.00012947    type: torch.FloatTensor\r\nexp:            size: 10^4      elapsed avg: 0.00399129 variance: 0.00030216    type: torch.FloatTensor\r\nexp:            size: 10^5      elapsed avg: 0.05558920 variance: 0.00430511    type: torch.FloatTensor\r\n\r\nvml\r\nexp:            size: 10^2      elapsed avg: 0.00004063 variance: 0.00001652    type: torch.FloatTensor\r\nexp:            size: 10^3      elapsed avg: 0.00048499 variance: 0.00003081    type: torch.FloatTensor\r\nexp:            size: 10^4      elapsed avg: 0.00325508 variance: 0.00007704    type: torch.FloatTensor\r\nexp:            size: 10^5      elapsed avg: 0.04254997 variance: 0.00301427    type: torch.FloatTensor\r\n\r\n'],"['mkl', 'mkl-include']",0,0
215,pytorch,630,open,Add Peephole connections for LSTMs?,From this [paper](http://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf). Peephole connections seem to help in learning precise timings of events.,feature triaged,"['Yeah, that could be added as part of the LSTM cell.', '@apaszke Is there any progress on this?', 'Nope. But I can guide you if you want to add it', '@apaszke Sure, I will dig the codes in future a few days.', ""@apaszke One question, why are there two biases, namely `b_ih` and `b_hh` in pytorch's implementation? Is there any sensible reason for this redundant setting?"", ""Because that's the format that cuDNN uses. In hindsight I think we shouldn't have done it this way, but it's not a high priority for us to change that."", ""@apaszke The CPU version can run without problem now, but for CUDA version I'm stuck at  /torch/nn/_functions/thnn/rnnFusedPointwise.py, line 58, which states `self.backend.LSTMFused_updateOutput`. I can't find where this class/method is defined."", 'https://github.com/pytorch/pytorch/blob/master/torch/lib/THCUNN/generic/FusedRNNKernel.cu', 'And for the CuDNN version,  where is `cudnnCreateRNNDescriptor` defined as referred from /torch/backends/cudnn/__init__.py, line217?\r\n', 'You can‚Äôt add peepholes to the cuDNN version. And that function, along with the others that start with cudnn, is a fixed part of a closed-source nvidia library.', ""OK, I get it, maybe we can explicitly name this version as `*_cudnn` as in Lasagne. \r\nCome back to the CUDA version, after a quick through of the functions `LSTMFused_updateOutput` and `LSTM_forw_ind_wrap`, I couldn't figure out where the actual LSTM calculation is done. Please correct me if I'm wrong, I know nothing about CUDA programming."", ""You don't need to modify the C source. Just implement it in autograd and expand the checks to make sure that we never use the C implementations when peephole is enabled."", ""@apaszke That's exactly what I'm doing for the time being. So basically the job is done here?"", 'A CUDA version of peepholes may be necessary. For training speed, Autograd version is ~4.x times slower than the CuDNN version (no peepholes).', 'According to discussions from  [link1](https://discuss.pytorch.org/t/how-to-speed-up-for-loop-in-customized-rnn/1012/11) and [link2](https://www.reddit.com/r/MachineLearning/comments/66rriz/d_rnns_are_much_faster_in_pytorch_than_tensorflow/), it seems to be a recognized problem for slow autograd implementation', 'probably nicer to have a separate PeepholeLstmCell function+mode rather than adding a peephole option to the RNN interface since not all RNNs have the concept of a peephole and it clutters the API', 'I would like to take this opportunity to once again bring up the discussion of more flexible RNN APIs. While it is easy to write a custom recurrent cell in pytorch to use in a single layer unidirectional net, if one wants to use non-standard cell implementations (or modify existing implementations) with stacking and bidirectional, it becomes quite cumbersome and clutters the API, as this PR and skip_input PRs demonstrate. @csarofeen had a proposal for more flexible APIs in #711, and while ultimately we may decide to take some other route, it is time to start thinking about it, or we will come to major releases that would freeze the APIs without working it out. ', ""interesting, i actually wrote a more general version of his/her proposal in (lua)torch with multidimensional RNNs in mind. it turned out to be too cumbersome and i do not believe it is the right approach going forward...after writing that i've pretty much only used either cudnn directly or manually wrote forward + backwards. would be happy to discuss my experience""]",[],[],0,0
216,pytorch,4540,closed,Cuda out of memory but gpu memory is utilized about a half,"I'm running slightly modified code of EDSR baseline network from here: https://github.com/thstkdgus35/EDSR-PyTorch
The training on first epoch goes well, but on the test stage when loading a model it raises an error:

But what is strange is that gpu memory at that moment is utilised about a half. Here's nvidia-smi output (at the moment of peak memory usage):

In test function I also use volatile=True option. What could be the problem?
OS: Win10
GPU: 1060 6G


  
  ",,"['Which version of PyTorch are you using? Btw, cuda memory usage can change very quickly, so you may not have captured the peak moment. It is also possible that the program tries to allocate a tensor larger than sum of all remaining memory, causing OOM.\r\n  ', ""Thank you for your answer! I use pytorch 0.3 win64 version from here: https://anaconda.org/peterjc123/pytorch. Also, about model size, it isn't large, because model has only 16 res blocks (which don't have batchnorm) and when I save trained model after first epoch before test on disk, it weights only 6MB. Also, I believe, that test input image is about hd size, so can this model allocate so large tensors? How can I trace this process?"", ""Weights alone may not amount to much (and there may be extra memory baggage besides the numerical values of the weights alone, for example). Keep in mind that there is a whole computational graph and whatever. Did you try running it on CPU to get a feel for RAM usage? It's almost certainly not 1-to-1, but it may be an indicator.\r\n  \r\nEDIT: Ah, I see you have `volatile` set to `True`. I'd still run it through a CPU, though."", ""Thanks for your suggestion! I'll try it soon.\r\n  "", '@SebyakinAndrei Hi, I am going through a similar problem of running out of memory. Do you find any solution yet? Thanks!', '@SebyakinAndrei @jiecaoyu are any of you using `create_graph=True` in a call to `.backward()`?', '@benvcutilli I think not. Also, I am using ```volatile=True``` to prevent saving unnecessary intermediate results during evaluation.\r\n\r\nActually, for my case, experiments runs well for batches with batch size = 256. But it will break for the last batch in each epoch which has a size lower than 256, for example 143.', ""@jiecaoyu I recognized that the model, that I used is very raw and not very memory-efficient, so I basically moved to similar model, based on tensorflow, that successfully runs on my hardware. \r\nAbout the original problem, I think that, as said SsnL earlier, CUDA memory changes very quickly and monitoring tools can't handle it."", 'So, I think that I need to close this issue. Thanks to all for your suggestions and advices!', 'do you have any solutions? @jiecaoyu \r\nmy gpu memory would increase a lot as the batch_size increases. Also, it breaks for the last batch', '@emergencyd I think you can simply skip the last batch. I still dont know what is the reason behind this.', ""When I use pytorch for training there are time that due to errors or because i changed my mind in between training I choose to nterupt th ejupyter kernel. The GPU doesn't flush the memory thinking the data is still usefull and this creates a problem when I do changes in the code and try to run it for the training again. \r\nIts problematic because the GPU memory reamins loaded utill the kernel is restarted and you'll have to run through the notebook again. \r\nPytorch should instead flush the memory when it detect that the training has crashed or is interrupted. ""]","['\r\nTHCudaCheck FAIL file=d:\\pytorch\\pytorch\\torch\\lib\\thc\\generic/THCStorage.cu line=58 error=2 : out of memory\r\nTraceback (most recent call last):\r\n  File ""main.py"", line 17, in <module>\r\n    t.test()\r\n  File ""D:\\Projects\\SR\\EDSR-PyTorch-master\\code\\trainer.py"", line 100, in test\r\n    output = _test_forward(input)\r\n  File ""D:\\Projects\\SR\\EDSR-PyTorch-master\\code\\trainer.py"", line 91, in _test_forward\r\n    return self.model(x)\r\n  File ""C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py"", line 325, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""D:\\Projects\\SR\\EDSR-PyTorch-master\\code\\model\\EDSR.py"", line 50, in forward\r\n    x = self.tail(res)\r\n  File ""C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py"", line 325, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py"", line 67, in forward\r\n    input = module(input)\r\n  File ""C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py"", line 325, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py"", line 67, in forward\r\n    input = module(input)\r\n  File ""C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py"", line 325, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py"", line 277, in forward\r\n    self.padding, self.dilation, self.groups)\r\n  File ""C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py"", line 90, in conv2d\r\n    return f(input, weight, bias)\r\nRuntimeError: cuda runtime error (2) : out of memory at d:\\pytorch\\pytorch\\torch\\lib\\thc\\generic/THCStorage.cu:58\r\n', '\r\nC:\\Program Files\\NVIDIA Corporation\\NVSMI>nvidia-smi -i 0 -q -d MEMORY\r\n\r\n==============NVSMI LOG==============\r\n\r\nTimestamp                           : Mon Jan 08 23:49:46 2018\r\nDriver Version                      : 388.13\r\n\r\nAttached GPUs                       : 1\r\nGPU 00000000:02:00.0\r\n    FB Memory Usage\r\n        Total                       : 6144 MiB\r\n        Used                        : 3657 MiB\r\n        Free                        : 2487 MiB\r\n    BAR1 Memory Usage\r\n        Total                       : 256 MiB\r\n        Used                        : 229 MiB\r\n        Free                        : 27 MiB\r\n']",[],0,0
217,pytorch,15436,closed,Type Conversions,"## üêõ Bug

I'm quite new to pytorch so I'm not exactly sure if this is a bug or just an error on my end.

I'm attempting to build a simple LogisticRegression Module on the  dataset in sklearn but for some reason, I keep getting this.

> RuntimeError: Expected object of scalar type Double but got scalar type Float for argument #2 'mat2'

I've tried to convert the both my X and y variables to  but it still seems to think it's float for some reason.

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:








<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

The error generated is:

> ---------------------------------------------------------------------------
> RuntimeError                              Traceback (most recent call last)
> <ipython-input-122-c1edd1a76c76> in <module>
>      10         optimizer.zero_grad()
>      11 
> ---> 12         outputs = model(train)
>      13         loss = criterion(outputs, test)
>      14 
> 
> ~/miniconda3/envs/mlbook/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
>     487             result = self._slow_forward(*input, **kwargs)
>     488         else:
> --> 489             result = self.forward(*input, **kwargs)
>     490         for hook in self._forward_hooks.values():
>     491             hook_result = hook(self, input, result)
> 
> <ipython-input-117-fca446c51ad3> in forward(self, x)
>       5 
>       6     def forward(self, x):
> ----> 7         out = self.layer(x)
>       8         return out
> 
> ~/miniconda3/envs/mlbook/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
>     487             result = self._slow_forward(*input, **kwargs)
>     488         else:
> --> 489             result = self.forward(*input, **kwargs)
>     490         for hook in self._forward_hooks.values():
>     491             hook_result = hook(self, input, result)
> 
> ~/miniconda3/envs/mlbook/lib/python3.6/site-packages/torch/nn/modules/linear.py in forward(self, input)
>      65     @weak_script_method
>      66     def forward(self, input):
> ---> 67         return F.linear(input, self.weight, self.bias)
>      68 
>      69     def extra_repr(self):
> 
> ~/miniconda3/envs/mlbook/lib/python3.6/site-packages/torch/nn/functional.py in linear(input, weight, bias)
>    1352         ret = torch.addmm(torch.jit._unwrap_optional(bias), input, weight.t())
>    1353     else:
> -> 1354         output = input.matmul(weight.t())
>    1355         if bias is not None:
>    1356             output += torch.jit._unwrap_optional(bias)
> 
> RuntimeError: Expected object of scalar type Double but got scalar type Float for argument #2 'mat2'

## Expected behavior
Train and test the data on the network
<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:

PyTorch version: 1.0.0
Is debug build: No
CUDA used to build PyTorch: None

OS: Mac OSX 10.14.3
GCC version: Could not collect
CMake version: version 3.13.2

Python version: 3.6
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip] numpy (1.14.0)
[pip] numpydoc (0.7.0)
[pip] torch (1.0.0)
[pip] torchvision (0.2.1)
[conda] blas                      1.0                         mkl
[conda] mkl                       2018.0.1             hfbd8650_4
[conda] mkl-service               1.1.2            py36h7ea6df4_4
[conda] pytorch                   1.0.0                   py3.6_1    pytorch
[conda] torchvision               0.2.1                      py_2    pytorch

## Additional context

<!-- Add any other context about the problem here. -->
When I attempt to check the datatype at each point, I get  and 
 respectively.",,"[""Is there any particular reason for you to cast all tensors to double by calling `.double()`?\r\n\r\nThe error stated that there was a type mismatch between the input (which is a `DoubleTensor`) and the network's weights (which is a `FloatTensor`).\r\n\r\nJust calling `.float()` instead to cast your input to `FloatTensor `will solve the problem."", '@ChunML answers the above question.\r\n\r\nIn the future if you have doubts or questions, reach out at https://discuss.pytorch.org']","['\r\nX, y = make_classification(n_samples=60000,n_features=10, n_redundant=0, n_informative=2)\r\n', '\r\nX_train,y_train, X_test, y_test = train_test_split(X, y, stratify=y, random_state=42)\r\nX_train = Variable(torch.from_numpy(X_train)).double()\r\ny_train = Variable(torch.from_numpy(y_train)).double()\r\nX_test = Variable(torch.from_numpy(X_test)).double()\r\ny_test = Variable(torch.from_numpy(y_test)).double()\r\n', ""\r\ninput_dim = 40000\r\noutput_dim = 2\r\ncriterion = nn.CrossEntropyLoss()\r\nlearning_rate = 0.001\r\nbatch_size = 100\r\nn_iters = 3000\r\nnum_epochs = int(n_iters / (len(X_train) / batch_size))\r\n\r\nmodel = SingleLayeredNetwork(input_dim, output_dim)\r\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\r\n\r\nclass SingleLayeredNetwork(nn.Module):\r\n    def __init__(self, input_dimension, output_dimension):\r\n        super(SingleLayeredNetwork, self).__init__()\r\n        self.layer = nn.Linear(input_dimension, output_dimension)\r\n\r\n    def forward(self, x):\r\n        out = self.layer(x)\r\n        return out\r\n\r\n\r\niter = 0\r\nfor epoch in range(num_epochs):\r\n    for train, test in zip(X_train, y_train):\r\n        print(train.dtype)\r\n        print(torch.typename(train))\r\n        optimizer.zero_grad()\r\n        \r\n        outputs = model(train)\r\n        loss = criterion(outputs, test)\r\n        \r\n        loss.backward()\r\n\r\n        # Updating parameters\r\n        optimizer.step()\r\n\r\n        iter += 1\r\n\r\n        if iter % 500 == 0:\r\n            # Calculate Accuracy         \r\n            correct = 0\r\n            total = 0\r\n            # Iterate through test dataset\r\n            for tra, tes in zip(X_test, y_test):\r\n                # Load images to a Torch Variable\r\n                tra = tra\r\n\r\n                # Forward pass only to get logits/output\r\n                outputs = model(train)\r\n\r\n                # Get predictions from the maximum value\r\n                # 100 x 1\r\n                _, predicted = torch.max(train, 1)\r\n\r\n                # Total number of labels\r\n                total += labels.size(0)\r\n\r\n                # Total correct predictions\r\n                correct += (predicted == test).sum()\r\n\r\n            accuracy = 100 * correct.item() / total\r\n\r\n            # Print Loss\r\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\r\n\r\n\r\n"", '\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n']","['make_classification', 'double', 'torch.float64', 'torch.DoubleTensor']",0,0
218,pytorch,20029,closed,RuntimeError:CUDA Error:out of memory,"## üêõ Bug
 With a CUDA 10.1 and RTX GPU. 
When I try even the most simpliest code like
nums = torch.randn(2,2).cuda(), it show the above error
And torch.cuda.is_available() is True.
Any idea?",,"[""please use our bug report template, we need so much more info that we request in the template to understand what's up"", 'what?']",[],[],0,0
219,pytorch,23070,closed,adam.py KeyError: 'betas',"## üêõ Bug

File ""STS_main.py"", line 196, in <module>
    model, optimizer, loss = train(model, train_iter, optimizer, criterion)
  File ""STS_main.py"", line 72, in train
    optimizer.step()
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/optim/adam.py"", line 85, in step
    beta1, beta2 = group['betas']
KeyError: 'betas'
",module: optimizer triaged,"['Could you please provide environment information or copy-paste the output from [the environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)?', '@vishwakftw \r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~16.04~ppa1) 7.4.0\r\nCMake version: version 3.13.3\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.168\r\nGPU models and configuration: GPU 0: Tesla K80\r\nNvidia driver version: 418.67\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.4\r\n[pip3] numpydoc==0.8.0\r\n[pip3] pytorch-pretrained-bert==0.6.2\r\n[pip3] pytorch-transformers==0.7.0\r\n[pip3] torch==1.1.0\r\n[pip3] torchvision==0.3.0\r\n[pip3] torchwordemb==0.0.9\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2018.0.2                      1  \r\n[conda] mkl-service               1.1.2            py36h17a0993_4  \r\n[conda] mkl_fft                   1.0.1            py36h3010b51_0  \r\n[conda] mkl_random                1.0.1            py36h629b387_0  \r\n[conda] pytorch-pretrained-bert   0.6.2                     <pip>\r\n[conda] pytorch-transformers      0.7.0                     <pip>\r\n[conda] torch                     1.1.0                     <pip>\r\n[conda] torchvision               0.3.0                     <pip>\r\n[conda] torchwordemb              0.0.9                     <pip>\r\n', 'Could you provide a reproduction script?', 'What  is reproduction script?\r\n@vishwakftw ', '> What is reproduction script?\r\n\r\nA code example to help produce the same error you are getting.', '`optimizer = optim.SGD(model.parameters(), lr = Param.lr)`\r\n`for epoch in range(1, 100):`\r\n       `train(model, optimizer)`\r\nerror occurs in the 2nd epoch\r\nif th optimizer is initialised inside the train() module, then there is no such error. \r\n', 'I am unable to reproduce the error. Can you provide a small but complete and self-contained code that reproduces the error?', 'From the initial bug shown, it seems as if in the ""group"" dictionary, there is no key ""betas"" that you are trying to find the value of. Also, every key in a dictionary has one value (1:1), so can you explain why you are equating both ""beta1"" and ""beta2"" to the value that you are trying to get from the key ""betas.""', ""@Dhanachandra you have modifed adam.py yourself, or you loaded an incorrect checkpoint into your constructed optimizer. Either ways, it's your code's bug."", '@sethd1 `betas` is a tuple of 2 elements, so `beta1, beta2 = betas` will unpack `betas` ']",[],[],0,0
220,pytorch,4894,closed,Inconsistent size error when using pin_memory() after torch.unbind(),"- OS: Ubuntu 14.04
- PyTorch version: 0.3.0.post4
- How you installed PyTorch: conda
- Python version: 3.6.3
- CUDA version: 8

Script to reproduce the error:

",,"[""Thank you for the bug report!\r\n\r\nThis doesn't error out on `Variable`s:\r\n```\r\nimport torch\r\nfrom torch.autograd import Variable\r\na = Variable(torch.zeros(3, 4))\r\na1, a2, a3 = torch.unbind(a, 0)\r\na1 = a1.pin_memory()\r\n```\r\nThe RuntimeError with tensors will be fixed when Tensor and Variable are merged."", ""As for pytorch 0.4 (nightly build 2018.04.20, installed via conda) this problem does not exist anymore. \r\n```python\r\n>>> import torch\r\n>>> a = torch.zeros(3, 4)\r\n>>> a1, a2, a3 = torch.unbind(a, 0)\r\n>>> a1 = a1.pin_memory()\r\n>>> torch.__version__\r\n'2018.04.20'\r\n```\r\nClosing the issue."", '@sytrus-in-github good to hear!']","['python\r\nimport torch\r\na = torch.zeros(3, 4)\r\na1, a2, a3 = torch.unbind(a, 0)\r\na1 = a1.pin_memory()\r\n', '\r\nTraceback (most recent call last):\r\n\r\n  File ""<ipython-input-67-b93d604f6d7e>"", line 1, in <module>\r\n    a1 = a1.pin_memory()\r\n\r\n  File ""<path-to>/anaconda3/lib/python3.6/site-packages/torch/tensor.py"", line 82, in pin_memory\r\n    return type(self)().set_(storage.pin_memory()).view_as(self)\r\n\r\n  File ""<path-to>/anaconda3/lib/python3.6/site-packages/torch/tensor.py"", line 198, in view_as\r\n    return self.view(tensor.size())\r\n\r\nRuntimeError: invalid argument 2: size \'[4]\' is invalid for input with 12 elements at /opt/conda/conda-bld/pytorch_1512386481460/work/torch/lib/TH/THStorage.c:41\r\n']",[],0,0
221,pytorch,31528,open,cuCtxGetDevice error and seg fault with DDP and OpenMPI,"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->
When using DistributedDataParallel with OpenMPI+UCX for certain tested models I'm getting this error in the DDP constructor (I believe it's during the model broadcast):



I observed this error using a resnet50 model but no error if I instead used a very small single-layer CNN.

## To Reproduce

The script is here:
https://github.com/sparticlesteve/nersc-pytorch-build/blob/911fc67b6667d3c6e3be972169e30e34c1a33af5/test_ddp.py

I submit via slurm a single-node job with 8 MPI ranks for 8 V100 gpus, something like:



My full log with stack trace is here:
https://gist.github.com/sparticlesteve/7307694f89329c277e16e452b524fefa

## Environment

PyTorch version: 1.3.1
Is debug build: No
OpenMPI: 4.0.1 with UCX 1.6
CUDA used to build PyTorch: 10.1.168
OS: openSUSE Leap 15.0
GCC version: (GCC) 7.3.0 20180125 (Cray Inc.)
CMake version: version 3.14.0
Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.1.168
GPU models and configuration:
GPU 0: Tesla V100-SXM2-16GB
GPU 1: Tesla V100-SXM2-16GB
GPU 2: Tesla V100-SXM2-16GB
GPU 3: Tesla V100-SXM2-16GB
GPU 4: Tesla V100-SXM2-16GB
GPU 5: Tesla V100-SXM2-16GB
GPU 6: Tesla V100-SXM2-16GB
GPU 7: Tesla V100-SXM2-16GB

Nvidia driver version: 440.33.01
cuDNN version: Could not collect

cc @ngimel @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528",module: cuda oncall: distributed triaged,"['@pietern, do you have an idea?', ""Thanks for posting the issue, @sparticlesteve.\r\n\r\nIf this happens during model broadcast of a larger model, let's see if we can find out what the size threshold is for this to happen. Instead of running the model like you do in the example script, could you create a little routine that performs a broadcast of a progressively larger CUDA tensor? If that same scripts prints the size for which the broadcast succeeds, we can isolate when this happens.\r\n\r\nMy hypothesis is that MPI with CUDA support performs device-to-host copies for small messages and execute the collective on the host, followed by a host-to-device copy. For larger messages (at some threshold), it tries to use CUDA IPC, but fails because not every process has a CUDA context for every other device (hence the failure in `cuCtxGetDevice`).\r\n\r\nFollowing on this, another thing you could try, is for every process to iterate over the list of local GPUs and create a tiny CUDA tensor on each of them. This will trigger initialization of a CUDA context for every device in every process. This will come at a significant memory cost (because CUDA contexts can take up quite a bit of memory individually), but it would isolate the issue further."", ""Hi @pietern,\r\n\r\nThanks for your reply and suggestions.\r\n\r\nI implemented a simple broadcast test:\r\nhttps://gist.github.com/sparticlesteve/71c9765307189bf8dc776803438f737a\r\n\r\nWith array size `2**16` (256kB), it runs successfully, while with size `2**17` (512kB) it produces the `cuCtxGetDevice` error.\r\n\r\nInterestingly, if I comment out the broadcast and just test the allreduce, I can increase the array size pretty much up to the GPU memory limit without any errors. Surely if there's a problem with contexts it would also affect the allreduce, right?"", 'I tried your other suggestion and allocated a small tensor on each gpu from each rank before trying the broadcast:\r\nhttps://gist.github.com/sparticlesteve/53751369bbff2b51c1ef474f3254c609\r\nIt showed the same behavior, however.', ""Thanks for giving it a try, and I'm glad to see that the tensor size is what trips the error.\r\n\r\nI double checked the implementation of broadcast and allreduce in the MPI process group and can confirm they both correctly use a device guard, so the thread that calls the MPI functions will have the correct CUDA device set (see `c10::DeviceGuard`):\r\n\r\nhttps://github.com/pytorch/pytorch/blob/9116f02bebf3a5260feef5732d36c54ecb3b4033/torch/lib/c10d/ProcessGroupMPI.cpp#L320-L362\r\n\r\nPer your second comment, you have created a context on each GPU and the error still happens. I think this is the point where an OpenMPI+UCX+CUDA expert needs to take a look. I'm not familiar enough with that stack to have an intuition what might be causing this issue. It looks like we're doing all the right things on the PyTorch end."", 'Hi @pietern,\r\n\r\nIf you have time, could you take a look at the discussions on my UCX issue https://github.com/openucx/ucx/issues/4707?\r\nThe folks there had some comments/questions on the device contexts, threading, and TLS. They thought this looks like a similar issue that affected rapids.\r\n\r\nThanks!']",['\r\n0: [1576565271.426514] [cgpu06:45376:0]    cuda_ipc_md.c:62   UCX  ERROR cuCtxGetDevice(&cu_device) is failed. ret:invalid device context\r\n0: [1576565271.426547] [cgpu06:45376:0]       ucp_rkey.c:250  UCX  ERROR Failed to unpack remote key from remote md[4]: Input/output error\r\n0: [cgpu06:45376:0:45523] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x20)\r\n'],['srun --ntasks-per-node 8 -u -l python test_ddp.py --backend mpi'],0,0
222,pytorch,18701,closed,[MSVC] Caffe2-Debug Static Size Too Big LNK1248,"## üêõ Bug

pytorch-Debug-build\lib\Debug\caffe2-Debug.lib: fatal error LNK1248: image size (10007FE9E) exceeds maximum allowable size (FFFFFFFF)
## To Reproduce

Steps to reproduce the behavior:

1. Compile latest master on Windows as static

## Expected behavior

Links

## Environment

 - PyTorch Version (e.g., 1.0): master
 - OS (e.g., Linux): Windows
 - How you installed PyTorch (, , source): source
 - Build command you used (if compiling from source): cmake -DUSE_CUDA=ON -DBUILD_SHARED_LIBS=OFF
 - Python version: 3.7
 - CUDA/cuDNN version: 10.1
 - GPU models and configuration: GTX 1060
 - Any other relevant information:

## Additional context

Did not have this issue in 1.0.1 but perhaps size has just gone over the threshold.

It appears MSVC has a 4GB code/data file size limit. I'm sure there's a workaround.
Although, Microsoft says to split up the library: https://developercommunity.visualstudio.com/content/problem/332991/static-lib-4gb-file-size-limit-lnk1248-image-size.html",module: static linking module: windows triaged,"[""I think the cause may be that we are using /Z7 in the linking flags. According to the MSVC [document](https://docs.microsoft.com/en-us/cpp/build/reference/z7-zi-zi-debug-information-format?view=vs-2017), `/Z7` will put all the debugging stuff into the object files, thus making large object/libraries. The alternatives are /Zi and /ZI. However, we need sccache to rebuild things quickly in CI, so we cannot use them. But if you don't use sccache, then you can try whether it works."", 'OK, I will try /Zi\r\n\r\nAlso of note is that it now takes 6 hours to compile on an i7-7700 whereas previously (v1.0.1) it was less than half that time.\r\n\r\nWas there any major changes with regards to MSVC flags in this time?', 'Are you using Ninja? Have you specified `TORCH_NVCC_ARCH_LIST`? In our CI, the build time for 16 VCPU cores is 20-30 minutes with ninja and sccache.', ""Well if I used sccache it would be fast for sure. What was the first run speed?\r\n\r\nNot using Ninja. Using the regular VS generator with 8 jobs. I only have 4 cores. I haven't seen any benefit in Ninja recently and I need to make a VS project.\r\n\r\nI haven't specified TORCH_NVCC_ARCH_LIST. What is the default? Does it build every arch? That could be it if the default behaviour changed.\r\n\r\nI'll try it on a 2xXeon Gold 6128 instead."", ""> I haven't specified TORCH_NVCC_ARCH_LIST. What is the default? Does it build every arch? That could be it if the default behaviour changed.\r\n\r\nIf you don't specify it, there is a chance that it is not automatically detected. And then, it will build for all arches which will be time-consuming. \r\n\r\n> I haven't seen any benefit in Ninja recently \r\n\r\nPreviously, VS doesn't support parallel custom build jobs, so building cuda sources will take a long time. However, looks like it is implemented in a newer version of VS, but I don't know exactly whether it is well supported by CMake."", ""Yeah, I've enabled parallel builds since it was supported. Since then, I haven't used Ninja (I get similar build time).\r\n\r\nOn Linux, I get a build time of 20 minutes (with 4 cores again). Is it possible the card detection only works on Linux? I'm using a Titan V there.\r\nDoes it print out which arches it is building for somewhere? I didn't see it in the summary. That would be a nice addition, like what OpenCV does.\r\nAlso, if I wanted to build for all arches since 30, is there an 'All' or do I have to manually specify them?"", 'Yes, it is printed out in `Summary`. And `All` is an acceptable value for TORCH_NVCC_ARCH_LIST.', ""I'm having trouble finding it in the `Summary`.\r\n\r\nFrom OpenCV:\r\n>     --   NVIDIA CUDA:                   YES (ver 10.1, CUBLAS)\r\n>     --     NVIDIA GPU arch:             30 35 37 50 52 60 61 70 75\r\n>     --     NVIDIA PTX archs:\r\n\r\nFrom Torch:\r\n>   USE_CUDA              :  ON\r\n>     CUDA static link    : OFF\r\n>     USE_CUDNN           : ON\r\n>     CUDA version        : 10.1\r\n>     cuDNN version       : 7.5.0"", 'Torch: \r\n> -- Found CUDNN: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2/include  \r\n-- Found cuDNN: v7.2.1  (include: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2/include, library: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2/lib/x64/cudnn.lib)\r\n-- Added CUDA NVCC flags for: -gencode;arch=compute_50,code=sm_50', ""Right, that gets printed somewhere above the Summary. No wonder I've never seen it.\r\nSo I just checked on my Windows laptop and it's only generating for arch 61.\r\n\r\n>     -- Caffe2: CUDA detected: 10.1\r\n>     -- Caffe2: CUDA nvcc is: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/bin/nvcc.exe\r\n>     -- Caffe2: CUDA toolkit directory: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\r\n>     -- Caffe2: Header version is: 10.1\r\n>     -- Found cuDNN: v7.5.0  (include: C:/work/ifaceengine/pre_built/cuda/include, library: C:/work/ifaceengine/pre_built/cuda/lib/x64/cudnn.lib)\r\n>     -- Autodetected CUDA architecture(s): 6.1\r\n>     -- Added CUDA NVCC flags for: -gencode;arch=compute_61,code=sm_61\r\n> \r\n... about 200 lines in here\r\n>     --\r\n>     -- ******** Summary ********\r\n>     -- General:\r\n>     --   CMake version         : 3.14.0\r\n>     --   CMake command         : C:/Program Files/CMake/bin/cmake.exe\r\n>     --   System                : Windows\r\n>     --   C++ compiler          : C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx64/x64/cl.exe\r\n>     --   C++ compiler id       : MSVC\r\n>     --   C++ compiler version  : 19.16.27027.1\r\n>     --   BLAS                  : MKL\r\n>     --   CXX flags             : /arch:AVX -openmp /MP /bigobj\r\n>     --   Build type            : Release\r\n>     --   Compile definitions   : _CRT_SECURE_NO_DEPRECATE=1;USE_MSC_ATOMICS=1\r\n>     --   CMAKE_PREFIX_PATH     : C:/.hunter/_Base/eae0747/3588e2d/03e751e/Install\r\n>     --   CMAKE_INSTALL_PREFIX  : C:/.hunter/_Base/eae0747/3588e2d/03e751e/Build/pytorch/Install\r\n>     --\r\n>     --   TORCH_VERSION         : 1.0.0\r\n>     --   CAFFE2_VERSION        : 1.0.0\r\n>     --   BUILD_ATEN_MOBILE     : OFF\r\n>     --   BUILD_ATEN_ONLY       : OFF\r\n>     --   BUILD_BINARY          : OFF\r\n>     --   BUILD_CUSTOM_PROTOBUF : ON\r\n>     --     Protobuf compiler   :\r\n>     --     Protobuf includes   :\r\n>     --     Protobuf libraries  :\r\n>     --   BUILD_DOCS            : OFF\r\n>     --   BUILD_PYTHON          : OFF\r\n>     --   BUILD_CAFFE2_OPS      : ON\r\n>     --   BUILD_SHARED_LIBS     : OFF\r\n>     --   BUILD_TEST            : OFF\r\n>     --   USE_ASAN              : OFF\r\n>     --   USE_CUDA              : ON\r\n>     --     CUDA static link    : OFF\r\n>     --     USE_CUDNN           : ON\r\n>     --     CUDA version        : 10.1\r\n>     --     cuDNN version       : 7.5.0\r\n>     --     CUDA root directory : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\r\n>     --     CUDA library        : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64/cuda.lib\r\n>     --     cudart library      : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64/cudart.lib\r\n>     --     cublas library      : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64/cublas.lib\r\n>     --     cufft library       : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64/cufft.lib\r\n>     --     curand library      : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64/curand.lib\r\n>     --     cuDNN library       : C:/pre_built/cuda/lib/x64/cudnn.lib\r\n>     --     nvrtc               : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64/nvrtc.lib\r\n>     --     CUDA include path   : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\r\n>     --     NVCC executable     : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/bin/nvcc.exe\r\n>     --     CUDA host compiler  : C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx64/x64/cl.exe\r\n>     --     USE_TENSORRT        : OFF\r\n>     --   USE_ROCM              : OFF\r\n>     --   USE_EIGEN_FOR_BLAS    : ON\r\n>     --   USE_FBGEMM            : OFF\r\n>     --   USE_FFMPEG            : OFF\r\n>     --   USE_GFLAGS            : OFF\r\n>     --   USE_GLOG              : OFF\r\n>     --   USE_LEVELDB           : OFF\r\n>     --   USE_LITE_PROTO        : OFF\r\n>     --   USE_LMDB              : OFF\r\n>     --   USE_METAL             : OFF\r\n>     --   USE_MKL               : OFF\r\n>     --   USE_MKLDNN            : OFF\r\n>     --   USE_NCCL              : OFF\r\n>     --   USE_NNPACK            : OFF\r\n>     --   USE_NUMPY             : ON\r\n>     --   USE_OBSERVERS         : OFF\r\n>     --   USE_OPENCL            : OFF\r\n>     --   USE_OPENCV            : OFF\r\n>     --   USE_OPENMP            : ON\r\n>     --   USE_PROF              : OFF\r\n>     --   USE_QNNPACK           : OFF\r\n>     --   USE_REDIS             : OFF\r\n>     --   USE_ROCKSDB           : OFF\r\n>     --   USE_ZMQ               : OFF\r\n>     --   USE_DISTRIBUTED       : OFF\r\n>     --   Public Dependencies  : Threads::Threads;pybind11::pybind11;onnx::onnx\r\n>     --   Private Dependencies : cpuinfo::cpuinfo;FP16::fp16;cub::cub;aten_op_header_gen;onnx::onnxifi_loader\r\n>     -- Configuring done\r\n>     -- Generating done\r\n\r\nToo bad, that doesn't explain this issue (static size being so big) or the slow compiles."", '@peterjc123, /Zi did work\r\nThe PDB was 250MB\r\n\r\nRelease:\r\n7.5M c10.lib\r\n2.1M c10_cuda.lib\r\n1.1G caffe2.lib\r\n1.7G  caffe2_gpu.lib\r\n5.5M caffe2_protos.lib\r\n384M torch.lib\r\n\r\nDebug:\r\n8.4M c10_cuda-Debug.lib\r\n28M c10-Debug.lib\r\n2.1G caffe2_gpu-Debug.lib\r\n8.3M caffe2_protos-Debug.lib\r\n3.4G caffe2-Debug.lib\r\n705M torch-Debug.lib\r\n\r\nDo you think it would be possible to only use Z7 for CI?', 'I guess I can add an option for the user to choose /Zi over /Z7.']",[],"['conda', 'pip']",0,0
223,pytorch,1357,closed,ConvNd C implementation doesn't check tensor types,"the error only happens in very specific conditions. (change the view of input tensor and use weight converted from numpy array.)
 
The output of functional conv2d is far off in scale (e12 or so.) ",,"['this happens because `np.random.rand` generates double precision tensor by default, whereas torch generates single precision. We need to add guards for tensor types in THNN and THCUNN.', ""It's a problem with `checkTypes` function, that only [checks if the arguments are all on CUDA or CPU, but ignores the requested type](https://github.com/pytorch/pytorch/blob/master/torch/csrc/nn/THNN_generic.inc.h#L32-L54). Should be a simple fix."", 'Whoops! Yeah it just ignores the ""thpp::Type type"" parameter', 'what type is expected here? ', 'a is a FloatTensor but weights is a DoubleTensor.\r\n\r\nChange:\r\n`weights = torch.from_numpy(weights.reshape(1,1,5,5))`\r\nto:\r\n`weights = torch.from_numpy(weights.reshape(1,1,5,5)).float()`\r\n\r\n(or change a to torch.randn(...).double())', 'cool thanks! ']","['\r\nimport torch\r\nimport torch.nn as nn\r\nimport numpy as np\r\nfrom torch.autograd import Variable\r\nfilter_size = 5\r\na = torch.randn(3,3,64,64) \r\nweights = np.random.rand(25).reshape(5,5)\r\nweights = torch.from_numpy(weights.reshape(1,1,5,5)) \r\nout1 = nn.functional.conv2d(Variable(a.view(9,1,64,64)), Variable(weights)) \r\npsi = nn.Conv2d(1,1, filter_size, bias=True) \r\npsi.weight.data.copy_(weights) \r\npsi.bias.data.fill_(0)\r\nout2 = psi(Variable(a.view(9,1,64,64))) \r\nprint(out1.mean())\r\nprint(out2.mean()) \r\n']",[],0,0
224,pytorch,5938,closed,RuntimeError: value cannot be converted to type uint8_t without overflow: 10000,"I am trying to run the [this code](https://github.com/yunjey/pytorch-tutorial/blob/4b67434961a64ba5e19e63d44f0b9979d2a9aa11/tutorials/01-basics/feedforward_neural_network/main-gpu.py).

The training progress is perfectÔºåbut the accuracy did not print outÔºåand some error came out like thisÔºö

And the code isÔºö


I have tried it before with python3.5 on win10Ôºåit could ran perfectlyÔºåbut somehow couldn't use the cudaÔºåso i turn to the ubuntuÔºåis it the python version problemÔºü

- OS:ubuntu16.04
- PyTorch version:0.4.0a0+7cbbc0b
- How you installed PyTorch (conda, pip, source):source
- Python version:2.7
- CUDA/cuDNN version:9.0
- GCC version (if compiling from source):gcc (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609

I know that this question shouldn't be asked here cause it is not the pytorch's issus, but i can't access the stackoverflow, it is slow on Chinese internet .So it will be very appericated if someone could help me with this.Thx



",,"['yeah the tutorials needs to be updated. for now, you can try this:\r\n```\r\ncorrect += (predicted == labels).sum().item()\r\n```', '@SsnL it worksÔºÅthanks a lot', '> yeah the tutorials needs to be updated. for now, you can try this:\r\n> \r\n> ```\r\n> correct += (predicted == labels).sum().item()\r\n> ```\r\n\r\ncan you tell me why?\r\ncause i got the same error, but my code is different from his', ""`(predicted == labels).sum()` is a uint8 tensor. `correct / total` can't be represented as an uint8. so you need to get the python number.""]","['\r\nEpoch [5/5], Step [200/600], Loss: 0.0404\r\nEpoch [5/5], Step [300/600], Loss: 0.0511\r\nEpoch [5/5], Step [400/600], Loss: 0.0042\r\nEpoch [5/5], Step [500/600], Loss: 0.0464\r\nEpoch [5/5], Step [600/600], Loss: 0.1073\r\nTraceback (most recent call last):\r\n  File ""fnn.py"", line 84, in <module>\r\n    print(\'Accuracy of the network on the 10000 test images: %d %%\' % (100 * correct / total))\r\nRuntimeError: value cannot be converted to type uint8_t without overflow: 10000\r\n', ""\r\n# Test the Model\r\ncorrect = 0\r\ntotal = 0\r\nfor images, labels in test_loader:\r\n    images = Variable(images.view(-1, 28*28))\r\n    outputs = net(images)\r\n    _, predicted = torch.max(outputs.data, 1)\r\n    total += labels.size(0)\r\n    correct += (predicted == labels).sum()\r\n\r\nprint('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\r\n\r\n""]",[],0,0
225,pytorch,14864,open,[discussion] Recommend a different file extension for models (.PTH is a special extension for Python),"*.pth files are used by Python to list additional package search paths: https://docs.python.org/3/library/site.html

The pth files will be loaded as text files by Python interpreter. At some point when I had some PyTorch model pth file placed along with the sources, it caused a hang of Python at startup (it was trying to parse the big binary file as a list of paths).

Maybe just *.pt?",triaged,"[""As far as I can tell, `.pt` is used in many bits anyway, e.g. https://pytorch.org/tutorials/advanced/cpp_export.html , even if I have seen .pth (or even .pth.tar, when it wasn't a tar) in the wild.\r\nBut yes, I agree that standardizing on something not colliding with basic Python functionality is a good thing."", 'I think  the downloadable torchvision models have pth extension', ""e.g. Intel's distiller uses the strange `.pth.tar` as well: https://nervanasystems.github.io/distiller/model_zoo/index.html"", 'I think this is especially pertinent with a new announcement of Torch Hub:\r\n\r\nhttps://pytorch.org/docs/master/hub.html and https://github.com/pytorch/vision/blob/master/hubconf.py both mention *.pth files\r\n\r\n@soumith ', 'sure, we can change our models to `.pt`, I have no reservations.\r\nDo you might sending PRs, or pointing out where all you noticed the `.pth` recommendations so that we can change them?', ""@soumith Sure! I'll find all occurences and paste the pointers here :)"", 'An incomplete yet list (so far searched on github pytorch, torchvision, examples):\r\n1. https://github.com/pytorch/pytorch/blob/6fccca4278143146d03de2403e701c411c71c807/test/onnx/test_pytorch_onnx_caffe2.py#L96-L108\r\n\r\n2. https://github.com/pytorch/pytorch/blob/c47f680086802267aa93ea2dfa96b4a1a7a54dd1/torch/utils/model_zoo.py#L28-L52\r\n\r\n3. https://github.com/pytorch/pytorch/blob/5734e9677564743fc4000cfb955fb42046689be9/docs/source/hub.rst\r\n\r\n4. https://github.com/pytorch/vision/blob/8f943d4e0c380cb0a5587b6e0e032932576fabea/torchvision/models/vgg.py#L12-L19\r\n\r\n5. https://github.com/pytorch/vision/blob/71182bc1ea27652f9952f6d60d8b27e408fc940e/torchvision/models/resnet.py#L10-L14\r\n\r\n6. https://github.com/pytorch/vision/blob/c7e9bd3006b0144fd1a94724f08122f673fe3587/hubconf.py#L48-L62\r\n\r\n7. https://github.com/pytorch/vision/blob/d5637696eba298f96a5fda44c6462f97ad1f987c/torchvision/models/densenet.py#L12-L15\r\n\r\n8. https://github.com/pytorch/vision/blob/dc0238b82f0df5c44ec9878cb41011d1852a7afd/torchvision/models/squeezenet.py#L11-L12\r\n\r\n9. https://github.com/pytorch/vision/blob/1fb0ccf71620d113cb72696b2eb8317b3e252cbb/torchvision/models/alexnet.py#L9\r\n\r\n10. https://github.com/pytorch/vision/blob/85369e3a315697be7e167f303d44f6b69d46c8ee/torchvision/models/inception.py#L12\r\n\r\n11. https://github.com/pytorch/examples/blob/29c2ed8ca6dc36fc78a3e74a5908615619987863/dcgan/README.md\r\n\r\n12. https://github.com/pytorch/examples/blob/29c2ed8ca6dc36fc78a3e74a5908615619987863/super_resolution/README.md\r\n\r\n13. https://github.com/pytorch/examples/blob/2fc0211d30b808f049ab7e7f4990858cf2ac471f/fast_neural_style/neural_style/neural_style.py#L107-L217\r\n\r\n14. https://github.com/pytorch/examples/blob/64f829ce495dad43392451c7431ae26eeee39bad/dcgan/main.py#L260-L261\r\n\r\n15. https://github.com/pytorch/examples/blob/29c2ed8ca6dc36fc78a3e74a5908615619987863/super_resolution/main.py#L75\r\n\r\n16. https://github.com/pytorch/examples/blob/29c2ed8ca6dc36fc78a3e74a5908615619987863/fast_neural_style/README.md\r\n\r\n17. https://github.com/pytorch/examples/blob/15e27719d75e35358555a27215665c797999740f/imagenet/main.py#L349-L352', ""If `*.pt` is reserved for zipballs from saved JIT'ted models, it may be needed to recommend a different extension for raw saved tensors (preferably not `*.pth` or fake `*.tar`)"", 'Hi,\r\nAny updates on this?  It\'s a rather trivial issue, but it would be nice to have a ""standard"" and meaningful file extension for the PyTorch checkpoint files.\r\nThanks!', ""we can go with `*.ptc`. We haven't had time to actually do the task though."", '@soumith `*.ptc` for both pickle format (from torch.save and state_dict) and zip format from JIT?', 'maybe `.pt` for pickle format and `.ptc` (pytorch compiled) for JIT', 'One alternative more verbose option: `*.torch.pkl`, `*.torch.zip`, `*.torch.h5`', ""i think that's too long"", '@soumith Another option: `*.pt` and `*.ptz` (hints that it is a collection of multiple things, like `npz`).', ""Hi, any update on this?\r\nMy current library still uses `.pth` to save models and `.pt` to save tensors. Let me know the standard if it's finally determined, so that I could apply it on my library.\r\n\r\nI don't recommend `.ptz` if we don't have a `torch.savez` function and the same style as numpy `np.savez(file_path, key1=value1,key2=value2)`. Do we plan to have it?"", ':+1: for chained extensions not concealing the underlying format. Just `.zip` and `.tar` conveys not enough info about what is inside them (it can be `pickle`, for me `pickle` === ""I cannot accept that"").']",[],[],0,0
226,pytorch,2677,closed,"Require_grad = True, but printed as ""None""","My code is

x = Variable(torch.rand(8, 1, 5, 5), requires_grad=True)
conv = nn.Conv2d(1, 1, 3)
y = conv(x)
final = torch.sum(y)
print('y.grad', y.grad)
final.backward()
print('y.grad', y.grad)
print('y.grad', y.requires_grad)

However, the output is 
y.grad None
y.grad None
y.grad True

If y.requires_grad==True, shouldn't the second y.grad output some gradients instead of None? For x, the gradient showed normally after the backward()",,"['Hi @jianwolf, \r\n\r\nThe intermediate gradients of non leaf variables are not stored to save memory, if you would like to inspect or access them there is more details in this [pytorch forum post](https://discuss.pytorch.org/t/why-cant-i-see-grad-of-an-intermediate-variable/94). ']",[],[],0,0
227,pytorch,4387,closed,cannot allocate memory,"the problem arise when run after 1epoch, but works well in first train epoch as well as the  followed 1st validate epoch. batch_size is 8 and work_number is 2„ÄÇwhy this happened ?
File ""/home/luhongchao/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataload
er.py"", line 301, in __iter__
    return DataLoaderIter(self)
  File ""/home/luhongchao/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataload
er.py"", line 158, in __init__
    w.start()
  File ""/home/luhongchao/anaconda2/lib/python2.7/multiprocessing/process.py"", line 130, 
in start
    self._popen = Popen(self)
  File ""/home/luhongchao/anaconda2/lib/python2.7/multiprocessing/forking.py"", line 121, 
in __init__
    self.pid = os.fork()
OSError: [Errno 12] Cannot allocate memory",,"[""It sounds like you don't have enough RAM in your machine."", '@luhc15 @zou3519 Do you have enough swap space? I got this error, then I created swap([reference this](https://stackoverflow.com/questions/1367373/python-subprocess-popen-oserror-errno-12-cannot-allocate-memory)) and fixed it. ', 'Recommend this. Setting swap space equal to RAM space and swappiness=0 worked for me.\r\nhttps://www.digitalocean.com/community/tutorials/how-to-add-swap-space-on-ubuntu-16-04']",[],[],0,0
228,pytorch,12887,closed,Segmentation fault when summing uint8 tensor,"Hi. I'm using PyTorch 0.5.0a0+09896d1

This code:


is returning me a ""Segmentation fault"". If instead I do , it works. Also, the problem does not occur with .

EDIT: a more efficient way to reproduce the bug:
",needs reproduction,"['Well, just crashed cinnamon when trying to run this on my machine, but for starters, do you mean `uint8` or `int64` ? Your title and your code contradict each others. Have 1.0rc0 installed, but in any case, keep in mind that `a` is absolutely massive (about 20gb of memory).\r\n\r\nHowever, running `(a.numpy() == a.numpy()).sum()` worked fairly well on my machine, so I guess this has to do with lazyness: the numpy `sum()` probably sum as the comparison goes, while the torch version first try to complete the comparison, store the result and then sum it, which can obviously wreak havoc when dealing with such tensors. The segfault could be a code for a memory error.', 'I would guess it has less to do with the operation and more to do with the size of the tensor. \r\n\r\nOn my machine `a.size()` causes an overflow error stating `OverflowError: Python int too large to convert to C long`, but `a.numpy().shape` works correctly. I still get the overflow error when using different tensor types. Using `ByteTensor` to define `a` only uses ~3GB of memory compared to ~22GB, but still throws the same error. And if I set `size=1000*1000*1000`, then `a.size()` starts working again for both `ByteTensor` and `LongTensor`.', ""@HKervadec even though `a` is a `int64` tensor, `a == a` becomes a `uint8` tensor, so my title is correct. That being said, there is indeed no need to use a `LongTensor` for `a` as @alexcaveny pointed out. I updated original post with a more memory efficient way to reproduce the bug.\r\n\r\nTo give a bit of context, I typically store a dataset of concatenated sentences in a single vector `a`, then I compute the number of unknown words by doing: `(a == UNK_IDX).sum()`. I currently do: `(a.numpy() == UNK_IDX).sum()` to avoid the issue, but the error shouldn't be there in PyTorch."", ""I can't reproduce this on master."", ""@zou3519 it may not crash on the machine you're running on (it doesn't on mine either) but the sum on a `tensor` eats almost an order of magnitude more memory than the sum on a NumPy array:\r\n\r\n``` python\r\nimport torch\r\na = torch.empty(3 * 1000**3, dtype=torch.uint8)\r\n(a == 0).sum()\r\n```\r\n\r\neats 25 GB of memory while\r\n\r\n``` python\r\nimport torch\r\na = torch.empty(3 * 1000**3, dtype=torch.uint8)\r\n(a.numpy() == 0).sum()\r\n```\r\n\r\ntakes only 3 GB."", ""This issue and the increase memory usage seem to be resolved on recent builds of PyTorch. If you're still seeing either issue with PyTorch 1.7 or later please open a new issue.""]","['python\r\nsize = 3*1000*1000*1000\r\na = torch.LongTensor(size).random_(2)\r\n(a == a).sum()\r\n', 'python\r\nsize = 3*1000*1000*1000\r\na = torch.ByteTensor(size).random_(2)\r\n(a == 0).sum()\r\n']","['(a.numpy() == a.numpy()).sum()', 'size = 1000*1000*1000']",0,0
229,pytorch,6437,open,[caffe2] Double precision for operators?,"A lot of the operators are not defined for double precision. I am planning on experimenting with double precision calculations and will add them as necessary. Would there be interest in me submitting a PR request with them? Is there a reason they are not currently enabled? The only drawback is larger library file size and slower compilation time but I suppose not by much.

Thank you,
Svet",caffe2,[],[],[],0,0
230,pytorch,2351,closed,The Performance of v0.2.0 is 20X slower than v0.1.12,"Hi, i use pytroch to train  all-cnn model on cifar-10 by [Entropy-SGD](https://github.com/ucla-vision/entropy-sgd/tree/master/python). 

The log of v0.1.12_2 is 
Train: [ 1] 0.9442 33.32% [13.62s]
Train: [ 2] 0.8139 28.38% [14.39s]
Train: [ 3] 0.7006 24.22% [13.76s]
Train: [ 4] 0.6254 21.68% [13.71s]
Train: [ 5] 0.5720 19.96% [13.63s]

and the log of v0.2.0_1 is
Train: [ 1] 0.2238 7.84% [294.03s]
Train: [ 2] 0.1349 4.66% [291.71s]
Train: [ 3] 0.0950 3.26% [291.67s]
Train: [ 4] 0.0715 2.41% [290.50s]
Train: [ 5] 0.0558 1.89% [295.84s]

All code is same but the only difference is the version of pytroch. Why the performance of the two has the gap?

P.S. Can you re-generate the pip resource for v.0.1.12 ?",,"['The v0.2.0_1 seems to be converging faster. Are you using the same batch size?', '@pavanky Yes, all the params are the same. The convergence with epoch number seems faster by v0.2.0_1, but with time is much slower. The whole traing process is intolerable. I will ask the author of [Entropy-SGD](https://github.com/ucla-vision/entropy-sgd/tree/master/python) for more details. Thank you very much!', 'I maintain [Entropy-SGD](https://github.com/ucla-vision/entropy-sgd). I cannot recreate your numbers @ivorytwoer . With pytorch v0.2, GTX 1080 and default arguments for allcnn (just run `python train.py -m allcnn`), I get\r\n```\r\nTrain: [ 0] 0.6012 20.99% [355.72s]\r\nTest: [ 0] 0.3643 12.5480%\r\n```\r\nwhich is close to the numbers with v0.1.12. (i) Can you try to remove v0.1.12 and reinstall it again, and (ii) your numbers look off, after the 1st epoch, the training and validation errors with Entropy-SGD for allcnn on CIFAR-10 should be about 20% and 12%, respectively.', ""@pratikac \r\nThank you for your reply.\r\n\r\nRunning `python train.py -m allcnn --gamma=1e-4 --scoping=1e-3 --noise=1e-4 -B 200 -L 20` with pytorch v0.2.0 on a Titan X (Pascal), I get results like:\r\n```\r\nTrain: [ 0] 0.6048 21.14% [298.86s]\r\nTest: [ 0] 0.3562 12.1964%\r\n\r\nTrain: [ 1] 0.2279 7.80% [294.37s]\r\nTrain: [ 2] 0.1307 4.56% [293.68s]\r\nTrain: [ 3] 0.0896 3.09% [298.54s]\r\nTrain: [ 4] 0.0684 2.33% [293.65s]\r\nTrain: [ 5] 0.0558 1.88% [297.58s]\r\nTest: [ 5] 0.3631 9.5353%\r\n\r\nTrain: [ 6] 0.0445 1.46% [294.67s]\r\nTrain: [ 7] 0.0395 1.34% [298.33s]\r\nTrain: [ 8] 0.0348 1.12% [295.30s]\r\nTrain: [ 9] 0.0310 1.07% [293.53s]\r\nTrain: [10] 0.0289 0.96% [291.89s]\r\nTest: [10] 0.4058 9.6966%\r\n```\r\nHowever, before I upgraded pytorch, result of running the same command was like:\r\n```\r\nTrain: [ 0] 1.3807 48.86% [14.19s]\r\nTest: [ 0] 0.9708 33.8474%\r\n\r\nTrain: [ 1] 0.9442 33.32% [13.62s]\r\nTrain: [ 2] 0.8139 28.38% [14.39s]\r\nTrain: [ 3] 0.7006 24.22% [13.76s]\r\nTrain: [ 4] 0.6254 21.68% [13.71s]\r\nTrain: [ 5] 0.5720 19.96% [13.63s]\r\nTest: [ 5] 0.5057 17.4680%\r\n\r\nTrain: [ 6] 0.5326 18.45% [13.66s]\r\nTrain: [ 7] 0.4979 17.38% [13.45s]\r\nTrain: [ 8] 0.4722 16.39% [13.73s]\r\nTrain: [ 9] 0.4371 15.04% [13.58s]\r\nTrain: [10] 0.4100 14.21% [13.96s]\r\nTest: [10] 0.4053 13.7587%\r\n```\r\nThe execution time of v0.2.0 is much slower than v0.1.12.\r\n\r\nI cann't find .whl file from [pytorch.org](http://pytorch.org/) for v0.1.12, so I will reinstall v0.1.12 from source and then try to run this command again."", 'I suspect there is something else in your code unrelated to pytorch versions because 14 secs to perform L=20 epochs on allcnn with cifar-10 does not look right :) On the other hand, your numbers correspond to SGD, i.e., setting L = 0.', ""@pratikac Yeah! I think that I forgot to add the option `-L 20` and then got this result. Thank you for  locating the problem! Also thank @pavanky for your help :)\r\nSorry for disturbing you both and I'm really sorry about my carelessness.""]",[],[],0,0
231,pytorch,12013,open,[Feature Request] Make nn layers accept empty batch size,"Now that we have support for tensors with zero in its size, I believe it would be very handy to have support for accepting batches of size 0 in  functions.

A (non-exhaustive) list of functions that would be good supporting:
- [x] 
- [x] 
- [x] 
- [x] 

Handling the losses is a bit trickier, because it generally involves computing a , which results in  due to 0 / 0 division. I'd expect having a 0 loss for empty batches to make sense, but that's debatable so might be worth postponing this decision.

cc @ezyang @gchanan @zou3519 @albanD @mruberry",function request high priority module: nn triaged,"['@fmassa what is the use of 0 batch size? What should we support it?', ""@bhushan23 some models (see Detectron Mask R-CNN for example) have a data-dependent generation of batches inside of their model. So even if we pass a fixed number of images, there might be no image which contains (or at least the model doesn't detect) a certain subset of categories, which corresponds to forwarding to a model a batch size of zero.\r\n\r\nAnother use-cases if when tracing models using the jit. If we support empty batch sizes, the runtime cost for the tracing is reduced, as we don't need to compute convolutions/ poolings anymore, almost if we had shape inference functions for those operations.\r\n\r\nMore generally, with support for zero-sized tensors, a number of restrictions that we used to have can be removed, and just makes writing code that works in some edge cases a no-brainer"", ""thanks @fmassa \r\nHow important is this? I will take this up but won't be able to work until next week."", ""@bhushan23 this is not an urgent feature, so no worries. I'm assigning it to you then."", ""I'm using empty tensors (as in, some dimension being zero) to define clearer interfaces: sometimes some parts of my network's computation shouldn't be performed; instead of adding boolean flags to (de)activate those parts, I give them empty tensors as inputs.\r\n\r\nA good PyTorch support for this would mean a better developer experience for me: now I need explicit checks to handle these cases, which add new codepaths (that could have bugs, need to be tested, and add to the maintenance burden); if I could delegate that to PyTorch, my code would be sleeker."", '@lerks I totally agree with you, and I do believe having native support for those cases would bring value.\r\n\r\nMy approach in this case for now was to have special-cased modules which handle tensors with 0 in its dimension, see [here for an example](https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/layers/misc.py).', 'This would also allow use of `DistributedDataParallel` with empty batches. The workaround @fmassa posted where these layers are wrapped and return a different autograd function if the batch is empty ([this one](https://github.com/facebookresearch/maskrcnn-benchmark/blob/9b53d15c12cca33764ff218f61e95bb52bc7d0f3/maskrcnn_benchmark/layers/misc.py#L17-L26)) does not work with `DistributedDataParallel` because it depends on a fixed set of autograd function hooks. By using a dynamically generated function the existing hooks for these layers will never be called and the worker with the empty batch will hang on the backwards pass waiting for those hooks to be called.', 'The [workaround](https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/layers/misc.py) by @fmassa cannot apply to `nn.SyncBatchNorm`, because all the workers have to enter the layer in order to finish the allgather call. Otherwise the layer will deadlock.', 'We should bump the priority of this feature. It will also simplify the task of having torch script-ready models for object detection', 'For quantization support, we are planning on supporting torchvision. Having this support as part of regular torch.nn removes the need to special case torchvision.ops. Can we bump this up in priority?', 'Added needs discussion because bumped the priority to high', 'I will be looking into conv and conv transpose ', '@bhushan23 any updates on this issue? did you managed to make some progress on it?', '@fmassa no. I will work on it over the weekend.', 'Hey @bhushan23 did you managed to make progress on this issue?\r\n\r\nIt would be good to have it implemented soon, as it is important to a few people like @raghuramank100 and @ppwwyyxx ', 'Hi @bhushan23 , this is a feature that we need fairly soon, so please let us know if you are able to work on it/need it reviewed etc.', '@fmassa I am little confused regarding handling the backward pass and looking into it\r\nforward pass will simply be returning a tensor of expected output shape here https://github.com/pytorch/pytorch/blob/13292ec3c7f47228b93dbef1ca750b66b4b8e0af/aten/src/ATen/native/Convolution.cpp#L512\r\n@raghuramank100 will let you know if I want to pass this on to someone by end of day\r\n@gchanan do you have any ideas on handling backward pass? If we handle this at ATen, then will need to change all the backends to respectively handle 0d shape. Another approach is to handle at python api level (one place)\r\nIssue will later is exposing tensor specific handling to higher level whereas ATen approach is much more concrete and non-exposing\r\n', ""@fmassa @gchanan @raghuramank100 @soumith I don't have enough bandwidth to finish this sooner. I would like someone else to take over.\r\n\r\nI tried a small change with [mkldnn](https://github.com/bhushan23/pytorch/commit/ebff27f61ac00036d79ad2dbd062d98773714439) \r\nBasically, If any one of dimension is 0 then, pass it as it is (we don't need to compute the shape): computing shape is incorrect representation and such a tensors should be ignored during both forward and backward pass."", 'It seems pooling also has problem with empty-batch size\r\n\r\n```python\r\n/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc in __call__(self, *input, **kwargs)\r\n    545             result = self._slow_forward(*input, **kwargs)\r\n    546         else:\r\n--> 547             result = self.forward(*input, **kwargs)\r\n    548         for hook in self._forward_hooks.values():\r\n    549             hook_result = hook(self, input, result)\r\n\r\n/usr/local/lib/python2.7/dist-packages/torch/nn/modules/pooling.pyc in forward(self, input)\r\n    139         return F.max_pool2d(input, self.kernel_size, self.stride,\r\n    140                             self.padding, self.dilation, self.ceil_mode,\r\n--> 141                             self.return_indices)\r\n    142\r\n    143\r\n\r\n/usr/local/lib/python2.7/dist-packages/torch/_jit_internal.pyc in fn(*args, **kwargs)\r\n    132             return if_true(*args, **kwargs)\r\n    133         else:\r\n--> 134             return if_false(*args, **kwargs)\r\n    135\r\n    136     if if_true.__doc__ is None and if_false.__doc__ is not None:\r\n\r\n/usr/local/lib/python2.7/dist-packages/torch/nn/functional.pyc in _max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\r\n    485         stride = torch.jit.annotate(List[int], [])\r\n    486     return torch.max_pool2d(\r\n--> 487         input, kernel_size, stride, padding, dilation, ceil_mode)\r\n    488\r\n    489 max_pool2d = boolean_dispatch(\r\n\r\nRuntimeError: non-empty 3D or 4D input tensor expected but got ndim: 4\r\n\r\n```\r\n\r\nThis is the input tensor `tensor([], size=(0, 64, 112, 112))`', 'I have an input and model that I get `Floating point exception` and segfault when a tensor is empty. Ideally empty tensors should just be short-circuit to empty outputs, but at least there should be no segfault.', 'I just assigned this issue to myself, but if someone else is working on it, please let me know.', 'Actually, after looking more into this, I think this issue might be a bit too involved for me at this point since I only started working on PyTorch a week ago.', ""I'm thinking of working on this issue. Starting with `ConvTranspose1d`, do you think this code would be a useful way of reproducing the issue? Would be a good starting point for a test.\r\n\r\n``` python\r\nimport torch\r\n\r\ny = torch.ones(1,1,1)\r\nd1 = torch.nn.ConvTranspose1d(1, 1, 3)\r\nd1.weight.data = torch.ones(1,1,3)\r\nb = torch.tensor(2)\r\nx = d1(b)\r\n\r\nprint(x)\r\n```\r\n\r\nAlso, @fmassa says that the `conv_{1,2,3}d` NN's currently accept 0-dim tensors but I'm unable to find a PR that says so and the current pytorch on conda does not seem to allow it either. Can you please point me to where this has been implemented?"", 'Changing `ConvTranspose1d` to work with 0 dimensional tensors.\r\n\r\n# Problem\r\nThe ConvTranspose1d throws errors when you give it 0-dimensional tensors as input.\r\nThis should be made possible because there are cases where you have parts of an image\r\nwithout any feature and you want to be able to pass them through the layer without\r\nwriting any edge case handling code.\r\n\r\nThe solution should involve working with tensors that have 0-dim in any dimension.\r\n\r\nA sample failing program is as follows:\r\n``` python\r\nimport torch \r\n \r\ny = torch.ones(1,1,1) \r\nd1 = torch.nn.ConvTranspose1d(1, 1, 3) \r\nd1.weight.data = torch.ones(1,1,3) \r\nb = torch.ones([]) \r\nq = d1(b)\r\n```\r\n\r\n# Literature Survey\r\n\r\n## Previous Work\r\n\r\n[This](https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/layers/misc.py) link has some details of a workaround for the problem by writing wrapper autograd\r\nfunctions around `ConvTraspose1d`. It works by intercepting the `forward` and `backward` functions.\r\nThe `forward` function simply returns a new empty tensor of a different calculated shape.\r\nI have not been able to make the `backward` work out of the box in that file since the\r\ntensor object returned by the `forward` does not have a `grad\\_fn` and therefore leads\r\nto error:\r\n```\r\nRuntimeError: element 0 of tensors does not require grad and does not have a grad\\_fn\r\n```\r\n\r\nA very simple C++-level effort to make the forward pass for the MKLDNN backend work\r\nhas been proposed [here](https://github.com/bhushan23/pytorch/commit/ebff27f61ac00036d79ad2dbd062d98773714439) but it does not have a lot of code worth talking about.\r\n\r\nA comment [here](https://github.com/pytorch/pytorch/issues/12013#issuecomment-529614705) states some\r\nsuggestions for implementing backward pass.\r\n\r\n# Goals\r\n\r\nCan be roughly split into goals:\r\n* Change the forward differentiation to handle 0-dim tensor input.\r\n* Change the backward differentiation to handle empty gradient produced from 0-dim forward pass.\r\n\r\n', 'Past work: https://github.com/pytorch/pytorch/pull/26214', ""I'm working on the `batch_norm` now and have a doubt regarding the batch norm layer accepting 0 dim input size.\r\n\r\nIt seems that `BatchNorm1d` already works for 0-dim input. Here's the sample code:\r\n```\r\nimport torch\r\n\r\nbn = torch.nn.BatchNorm1d(5)\r\nx = torch.ones(0,5)\r\nbn(x)\r\n#=> tensor([], size=(0, 5), grad_fn=<NativeBatchNormBackward>)\r\n```\r\n\r\nCan please clarify with a minimal example how the batch norm issue is supposed to work?"", '@v0dro: that looks correct.  You probably want to also check 3-dimensional inputs for BatchNorm1d, and the correct input sizes for BatchNorm2d, BatchNorm3d.', 'Another issue with BatchNorm is how to update its moving_mean and moving_variance in case of 0-batchsize inputs. There is no convention but I would argue that the correct way is to leave them not updated.', ""Regarding the running mean and var:\r\n\r\n``` python\r\nimport torch\r\n\r\nbn = torch.nn.BatchNorm1d(5)\r\nx = torch.ones(0,5,9)\r\nbn(x)\r\n# tensor([], size=(0, 5, 9), grad_fn=<NativeBatchNormBackward>)\r\n\r\nbn.running_mean   \r\n# tensor([nan, nan, nan, nan, nan])\r\n\r\nbn.running_var\r\n# tensor([0.9000, 0.9000, 0.9000, 0.9000, 0.9000])\r\n```\r\nConsidering that the 0-dim is not actually supposed to 'do' anything I don't think the running variables should be updated. Thoughts?"", 'About 2D and 3D batch norm:\r\n``` python\r\nimport torch\r\n\r\nbn = torch.nn.BatchNorm2d(5) \r\nprint(bn.running_mean) \r\nprint(bn.running_var) \r\nx = torch.ones(0,5,2,2) \r\nbn(x) \r\nprint(bn.running_var) \r\nprint(bn.running_mean)\r\n\r\n# tensor([0., 0., 0., 0., 0.])\r\n# tensor([1., 1., 1., 1., 1.])\r\n# tensor([0.9000, 0.9000, 0.9000, 0.9000, 0.9000])\r\n# tensor([nan, nan, nan, nan, nan])\r\n```\r\n\r\nFor the 3D case:\r\n``` python\r\nimport torch\r\n\r\nbn = torch.nn.BatchNorm3d(5) \r\nprint(bn.running_mean) \r\nprint(bn.running_var) \r\nx = torch.ones(0,5,2,2,2) \r\nbn(x) \r\nprint(bn.running_var) \r\nprint(bn.running_mean)\r\n\r\n# tensor([0., 0., 0., 0., 0.])\r\n# tensor([1., 1., 1., 1., 1.])\r\n# tensor([0.9000, 0.9000, 0.9000, 0.9000, 0.9000])\r\n# tensor([nan, nan, nan, nan, nan])\r\n```', 'Backward also needs to be tested:\r\n```python\r\nimport torch\r\nbn = torch.nn.BatchNorm2d(5)\r\nx = torch.ones(0,5,2,2)\r\nx.requires_grad = True\r\ny = bn(x)\r\ny.sum().backward()\r\nprint(y)  # correct\r\nprint(bn.running_var)  # wrong\r\nprint(bn.running_mean)  # wrong\r\nprint(x.grad)  # correct\r\nprint(bn.weight.grad)  # wrong\r\nprint(bn.bias.grad)  # correct\r\n\r\n#tensor([], size=(0, 5, 2, 2), grad_fn=<NativeBatchNormBackward>)\r\n#tensor([0.9000, 0.9000, 0.9000, 0.9000, 0.9000]) expect 1\r\n#tensor([nan, nan, nan, nan, nan]) expect 0\r\n#tensor([], size=(0, 5, 2, 2))\r\n#tensor([nan, nan, nan, nan, nan]) expect 0\r\n#tensor([0., 0., 0., 0., 0.])\r\n```', 'Works. So in summary, for the batch norm the fix is centered around the running variables.', 'The behaviour seems to have changed in the current master.  Here\'s my program:\r\n``` python\r\nimport torch\r\n\r\nprint(""-----BatchNorm2d-----"")\r\nbn = torch.nn.BatchNorm2d(5)\r\nx = torch.ones(0,5,2,2, requires_grad=True)\r\ny = bn(x)\r\nprint(""\\tFORWARD PASS:"")\r\nprint(""\\t\\ty: "", y)\r\nprint(""\\t\\trunning mean: "", bn.running_mean)\r\nprint(""\\t\\trunning var: "", bn.running_var)\r\ny.sum().backward()\r\nprint(""\\tBACKWARD PASS:"")\r\nprint(""\\t\\tweight grad: "", bn.weight.grad)\r\nprint(""\\t\\tbias grad: "", bn.bias.grad)\r\n```\r\n\r\nAnd this is the output:\r\n```\r\n-----BatchNorm2d-----\r\n\tFORWARD PASS:\r\n\t\ty:  tensor([], size=(0, 5, 2, 2), requires_grad=True)\r\n\t\trunning mean:  tensor([0., 0., 0., 0., 0.])\r\n\t\trunning var:  tensor([1., 1., 1., 1., 1.])\r\n\tBACKWARD PASS:\r\n\t\tweight grad:  None\r\n\t\tbias grad:  None\r\n```\r\n\r\n@ezyang @rgommers can you please comment whether this behaviour is alright? The issue is already solved if yes.', 'The expected behavior for backward was given above in https://github.com/pytorch/pytorch/issues/12013#issuecomment-569876108', '@ppwwyyxx ok but why would you want the weights to be intialiazed to 0 at all if no computation will be taking place? \r\n\r\nMeanwhile here is the 0-dim batch size acceptance PR for interpolate: https://github.com/pytorch/pytorch/pull/32400', 'All flavors of convolution and batch norm now correctly handle empty batches and return zero gradients for parameters. ', 'Should close this one subject to merging of #32384 ', 'Closing since all relevant PRs have been merged.', '@v0dro thanks. Are empty batch sizes also supported for pooling now? this was the error before:\r\n\r\n>    489 max_pool2d = boolean_dispatch(\r\n> RuntimeError: non-empty 3D or 4D input tensor expected but got ndim: 4', ""I found that `Linear` still does not work for empty batch on master.\r\n```\r\n    import torch\r\n\r\n    x = torch.randn(0, 3).cuda()\r\n    w = torch.randn(3, 3).cuda()\r\n    b = torch.randn(3).cuda()\r\n    w.requires_grad = True\r\n    b.requires_grad = True\r\n    z = torch.addmm(b, x, w.t())\r\n    loss = z.sum()\r\n    # for some strange reason, it works with this loss:\r\n    # loss = F.cross_entropy(z, torch.zeros(z.shape[0], dtype=torch.long,\r\n    #    device=z.device), reduction='sum')\r\n\r\n    loss.backward()\r\n```\r\ngives an error of \r\n```\r\nRuntimeError: at::cuda::blas::gemm<float> argument ldb must be positive and less than 2147483647 but got 0 (gemm at caffe2/aten/src/ATen/cuda/CUDABlas.cpp:163)                                    \r\n```"", '> A (non-exhaustive) list of functions ...\r\n\r\nThe _non-exhaustive_ seems important here - the listed functions in the PR description were all done, but no triaging of other functions happened.\r\n\r\nPerhaps needs a generic test for all functions in the `nn.functional` namespace?', ""I'll prepare a list of NN modules that still don't support 0-size batches."", '@rgommers how about we close this one and move the discussion to #38115 ?', ""> @rgommers how about we close this one and move the discussion to #38115 ?\r\n\r\nAs discussed offline, let's keep this open, close gh-38115, use `<details>` to clean up the large comments so the conversation becomes readable again, and check off all the other layers in the checklist above that we found do support empty batch dim.\r\n\r\nNext step here is to fix the currently segfaulting operators (`InstanceNorm*d`). And then there's ~45 other operators left.""]",[],"['nn.functional', 'conv{1-2-3}d', 'conv_transpose{1-2-3}d', 'batch_norm', 'interpolate', '.mean()', 'NaN']",0,0
232,pytorch,30798,closed,"How can I implement ""nn.unFold"" on 5D tensor?","‚ùì How can I implement ""nn.unFold"" on 5D tensor?

I am implementing an operation on 3D image. I found I need ""nn.unFold"" function in my process. But until now, pytorch does not have official implementation in latest release version. 

I want to implement it in official release code form by myself. But I am a little confused by the relationship  between implementation part of c++ and cuda code. 

Could some one give me some suggestions that which files I should refer to corresponding to implementation of ""nn.unFold"" in 4D version...",enhancement triaged,"[""You might be able to get some inspiration from the implementation we had in the old torch7 library [here](https://github.com/torch/nn/blob/872682558c48ee661ebff693aa5a41fcdefa7873/lib/THNN/generic/VolumetricConvolutionMM.c#L99-L263).\r\nI'm not sure how efficient it will be though."", 'Hi, I got the same problem. I try to use ""Tenror.unfold()"" to implement the function, however, it consumes a lot of memory.\r\nk = kernel_size\r\ns = step\r\nd = dilation\r\nf= x.unfold(2, k, s).unfold(3, k, s).unfold(4, k, s) \r\nf= f[:, :, :, :, :, ::d, ::d, ::d]', 'This question is more appropriate to our forums, here: https://discuss.pytorch.org/. ']",[],[],0,0
233,pytorch,1128,closed,Pad a list of tensors,"Taking in a list of tensors, can we have a function to pad them? This is a useful feature to process sequences of different lengths and process mini batches. 

I think the documentation of pad_packed_sequence and torch.nn.utils.rnn.pack_padded_sequence are a little confusing http://pytorch.org/docs/nn.html?highlight=pad_packed#torch.nn.utils.rnn.pad_packed_sequence maybe adding some examples could help the explanation. ",medium priority (this tag is deprecated),"['I will second this request. This will be particularly useful for NLP researchers.\r\n\r\nIn addition, what is the most efficient way to pad a 1D tensor in Pytorch, apart from manually concatenating a zero tensor to it? The built-in `torch.nn.functional.pad()` function does not seem to work for 1D tensors.', '@yuhaozhang F.pad() works for 1D tensors if you add an extra dimension', '@szagoruyko Thanks. Maybe I am wrong, but the [source code](http://pytorch.org/docs/_modules/torch/nn/functional.html#pad) seems a bit confusing. The documentation of the function says ""Currently only 2D and 3D padding supported"", while the implementation suggests that ""Only 4D and 5D padding is supported for now"".', '@yuhaozhang To accomplish two dimensional padding, you need an input tensor with 4 axes and similarly for three dimensional padding, you need an input tensor with 6 axes. That is my interpretation of the source code, I may be mistaken.', '@szagoruyko For example:\r\n\r\n```\r\nimport torch.nn.functional as F\r\n\r\ninput = Variable(torch.rand(1, 1, 10)) # 1D (N, C, L)\r\ninput_2d = input.unsqueeze(2) # add a fake height\r\nF.pad(input_2d, (2, 0, 0, 0)).view(1, 1, -1) # left padding (and remove height)\r\nF.pad(input_2d, (0, 2, 0, 0)).view(1, 1, -1) # right padding (and remove height)\r\n```\r\n\r\nBut agreed, it would be nice if the `F.pad` function also supported 1D inputs and a `(left, right)` padding tuple.', 'Have we had an example in Sep 2017?', 'as of last week, `F.pad` now supports all dimensional Tensors, 1D, 2D, 3D, etc.', ""@apaszke I would love to work on this feature if this hasn't started yet. Do you have a preferred approach?\r\nI was thinking if we have a function `fill_with_zeros` or something that takes list of different length lists, seq_length and dtype and returns a tensor of mentioned type with mentioned length made either by truncation or by padding with zero\r\n\r\n```\r\ninputs = [[1, 2], [1, 2, 3], [1, 2, 3, 4]]\r\npadded_tensor = fill_with_zeros(inputs, seq_len=3, dtype='float')\r\nprint(padded_tensor)\r\n# 3x3 tensor\r\n[\r\n    [1, 2, 0],\r\n    [1, 2, 3],\r\n    [1, 2, 3]\r\n]\r\n```"", ""@hhsecond Yes, that would be great! I think it should be in `torch.nn.utils.rnn` and be named `pad_sequence`. It should get three arguments: a list of sequences (Tensors) sorted by length in decreasing order, a list of their lengths, and `batch_first` boolean. It's similar to `pack_padded_sequence`, except that the first argument would be a list of Variables instead of a single Variable.\r\n\r\nIt would also be useful to add `pack_sequence` which would be like `pack_padded_sequence(pad_sequence(input, lengths), lengths)`, but more efficient (no need to have the padded intermediate)."", ""@apaszke Sounds perfect, one quick question though. Wouldn't it be good if the function accepts a list of python lists (and desired sequence length as argument) where each list can be of varying length where if it only accepts tensors, the user has to convert each sentence to a tensor (assuming each sentence is of different length) separately before passing that to the `pad_sequence`. something like\r\n`pad_sequence(inputs, sequence_length, lengths)`\r\n\r\nThe `pack_sequence` sounds helpful, will do that as well."", 'It can accept a list of lists, but it definitely should also accept a list of Variables (this is more important)', 'I guess [this](https://github.com/pytorch/pytorch/issues/3584) is a good idea to implement inside `pad_sequence` or `pack_sequence`, any thoughts?', '@apaszke Do have a decision on sorting as mentioned in the issue 3584? Do I need to handle that in the `pad_sequence` function? And I was having second thoughts about the acceptable dimensions? What all should be accepted.\r\n- batch `x` seq_len `x` feature_len (batch will be 1 in most cases)\r\n- seq_len `x` batch `x` feature_len (batch will be 1 in most cases)\r\n- seq_len `x` feature_len\r\n- more than 3 dimensions\r\n\r\nIs this feature going into 0.3 release?', ""Yes, handling that in `pad_sequence` would be good. I'm not sure what's the issue with the dimensions that should be accepted. I think the whole point is to accept Variables of arbitrary trailing dimensions, and concatenate them along a new one in such a way that they are padded with 0s if too small."", 'If you implement it fairly quickly, we might still backport it to 0.3. `master` is now 0.4', ""@apaszke I'll try to make it into 0.3. \r\nThe problem with dimensions is, in the case of 2 or more dimensional Variables, user could pass\r\n- batch `x` seq_len `x` more_dim (batch dim would be 1 mostly and we'll **concatenating** on zeroth dim)\r\n- seq_len `x` feature_len `x`  more_dim (where user ignored batch dim, we'll be **stacking** on zeroth or 1st dimension depends on batch_first)\r\nBut how could we understand which dimension has variable length sequence. I can analyse the input and fetch this info or I can ask user to pass the dimension that needs padding?"", ""There's no way to tell if the batch dim is included. Since in most cases you'll want to concatenate the sentence to actually form a batch, I think defaulting to stacking makes sense"", '@apaszke I have made the PR #3875 with `pad_sequence` and `pack_sequence` but without internal sorting. As discussed in #3584, we will need to keep the track of original order. Perhaps we could return the order to the user and user has to pass that when he calls `pad_pack_sequence` to get the sequence in that order back. \r\n\r\nAnother possible RNN utility: Like `pack_padded_sequence` and `pad_packed_sequence`, I was thinking about `unpack_sequence` that inverse the `pack_sequence` I have written now, this also could accept the actual order and returns the unpacked sequence in given order, any thoughts?', ""I'm fine with `unpack_sequence`. Ordering is problematic, so let's stay away from it for now"", '@hhsecond Maybe the answer is obvious but I cannot figure out why we could not give the desirable length for padding instead of sorting the tensor in decreasing length order. Maybe I have a bias from NLP ...', '@Diego999  @hhsecond  I also have the same question. Giving the desirable length of padding would make the task much simpler.', '@Deepayan137 Happy to not be the only one. By there way, my question is also related to pack_padded_sequence ', '@Diego999  @hhsecond I was also wondering why do we need to first pad the seq to equal length manually and then pass it to pack_padded func to generate this PackedSequence object.\r\nhttps://github.com/pytorch/pytorch/releases/tag/v0.1.10\r\nAny Idea on this?', '@Deepayan137 Now there is this new function in master http://pytorch.org/docs/master/nn.html#torch.nn.utils.rnn.pad_sequence which will do the job for you. But as before, the list should be ordered by decreasing length', '``` bash\r\n>>> from torch.nn.utils.rnn import pad_sequence\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\nImportError: cannot import name \'pad_sequence\'\r\n``` \r\nI have the latest version of pytorch\r\n``` bash\r\nprint(torch.__version__)\r\n0.3.1.post2\r\n```\r\nAny suggestion? What version are you using?\r\n', '@Deepayan137 the function is available only on master. You have to build from source', 'Okay thanks a lot Diegio999 :+1: ', '@Deepayan137  to answer the original question: https://discuss.pytorch.org/t/why-lengths-should-be-given-in-sorted-order-in-pack-padded-sequence/3540 ', 'Is it possible to pack a padded 4d tensor for multi-turn dialog scenario?\r\n\r\n(B x T x L x D)\r\n\r\nAssuming B, T, and D are fixed, but the sequence length L varies, is it possible to use pack_padded_sequence under current implementation?']",[],[],0,0
234,pytorch,28404,closed,[Quantization] (Error): No function is registered for schema aten,"## ‚ùì Questions and Help
    I tried to quantizate my own shuffle model through 'STATIC QUANTIZATION WITH EAGER MODE IN PYTORCH'.
    But when it comes to forward propagation, the time to assess model losses, I failed:

    

cc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100",oncall: quantization triaged,"['me too', 'Currently, the support for modules is experimental and includes only Conv2d, Linear, ReLU, MaxPool2d. Details for which is given here: https://github.com/pytorch/pytorch/wiki/torch_quantization_design_proposal\r\nThat means, you need to either convert your native modules to supported modules or fuse onto these modules.', 'Yeah, please fuse batchnorm, see tutorial: https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html', ""I'm going to close this for now, feel free to reopen if the issue persists"", '> me too\r\n\r\nyou can  fuse the batchnorm or merge it with conv manually', '@jiacheng1gujiaxin  merge it manually ? how ? ', '@jerryzh168  I am still stuck on this issue, could you please help ? \r\nI have already put in the QuantStub and DeQuantStub blocks, but I am stuck on the layer fusion part. \r\n\r\nI am trying to use the model here -\r\nhttps://github.com/clovaai/CRAFT-pytorch/blob/master/craft.py', '@jerryzh168 Hello, does the fusion, support batch norm before convolution? Thanks.', 'How to merge batch_norm layer with conv layer?']","['\r\nRuntimeError: No function is registered for schema aten::native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor) on tensor type QuantizedCPUTensorId; available functions are CPUTensorId, CUDATensorId, MkldnnCPUTensorId, VariableTensorId\r\nThe above operation failed in interpreter, with the following stack trace:\r\nat code/__torch__/shuff_slim/___torch_mangle_297.py:316:13\r\n']",[],0,0
235,pytorch,5331,closed,[ppc64le] FAIL: test_set_flush_denormal (__main__.TestTorch),"hi,  I just saw this error in the last few days.  both for GPU and CPU only environment.
Is the test precision assertion correct ?


",,"[""On PPC64 it looks like denormal flags aren't being set correctly. This PR https://github.com/pytorch/pytorch/pull/5294 from 2 days ago added denormals support, but likely assumed only AMD64 / x86"", 'currently the set_flush_denormal call will always return False if on ppc64le.\r\nThe unittest has a skipIf  as below.   I believe the intention is to skip this\r\ntest if not running on x86.  But since set_flush_denormal returns a False on\r\nppc64le , then it actually runs the test and not run on x86\r\n\r\n    @unittest.skipIf(torch.set_flush_denormal(False),\r\n                     ""flush_denormal not supported"")\r\n\r\nI believe is you just add a not as below in the test_torch.py \r\n\r\n    @unittest.skipIf(not torch.set_flush_denormal(False),\r\n                     ""flush_denormal not supported"")\r\n\r\nthe test should then run on x86 but skipped for ppc64le or other platforms.', 'Doing calls that have side effects in skipIf is a bad idea. We should write a context manager that always resets the previous state + skips the test if the call returned False', '@apaszke what can we do in the meantime to skip over the denormal test, which is invalid,  on ppc64 ? ', 'Skip this test everywhere. These checks will affect all tests and we don‚Äôt want that', 'closed via https://github.com/pytorch/pytorch/pull/5387']","['\r\nFAIL: test_set_flush_denormal (__main__.TestTorch)\r\n\r\nTraceback (most recent call last):\r\n  File ""test_torch.py"", line 5519, in test_set_flush_denormal\r\n    self.assertEqual(float_tensor[1], 0.0, prec=0.0)  # tiny_float to zero\r\n  File ""/home/freddie/builder/jenkins/pytorch/pytorch/test/common.py"", line 256, in assertEqual\r\n    super(TestCase, self).assertLessEqual(abs(x - y), prec, message)\r\nAssertionError: 1.0005271035279194e-42 not less than or equal to 0.0 :\r\n']",[],0,0
236,pytorch,23756,open,[feature request] Core API for invertible and flow-like ops,"Inplace BatchNorm seems to be developed by Mapillary here: https://github.com/mapillary/inplace_abn

This would be a very nice addition to core PyTorch (for memory savings).

cc @vincentqb @fritzo @neerajprad @alicanb @vishwakftw @SsnL",feature low priority module: distributions module: nn triaged,"[""@fmassa says: This requires us to have a fused batchnorm and nonlinearity afterwards, because you can't have fused batchnorm with arbitrary nonlinearities (what they use right now are like, leaky relu--things with 1-to-1 mapping between x and y, so you can have an inverse). This is not actually currently present in PyTorch, so we should keep this outside of core for now.\r\n\r\n@gchanan: We have fused batchnorm with quantized tensors, though! It's in `_intrinsics`, or something like that. (@ezyang: And lazy tensor could let you use the fused kernel too.)"", ""Yes, there exists [ConvBnRelu2d](https://github.com/pytorch/pytorch/blob/master/torch/nn/_intrinsic/qat/modules/conv_fused.py#L176) and [fuse_conv_bn_eval(conv, bn)](https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/fusion.py#L7). If there's a separate manual inplace fuser for conv_bn_leakyrelu for train time, it could help increase batch sizes for a lot of common models"", 'One interesting way could be an interface/attributes for supporting reversible ops. If all ops in a subgraph are reversible, then theoretically the subgraph could be run with inplace ops.\r\n\r\nI wonder if reversibility could be deduced from the graph analysis.\r\n\r\nA transitionary way could be having nn.Modules support an additional `inverse()` method and a `InplaceSequential` container (or some with `torch.inplace()` modifier)', 'In fact, the `inverse()` API (together with `torch.inplace()`) could be very natural additions for implementing invertible normalizing flows: https://invertibleworkshop.github.io/ @ezyang @fmassa \r\n\r\nA recent implementation of invertible flows with `inverse()` API: https://github.com/rtqichen/residual-flows/blob/master/lib/resflow.py#L222', ""Inverse would be cool, though I'm not sure if we can justify the complexity (I'm not sure if this belongs in autograd or not...). Unrelated but similar: forward mode autodiff would already require us to write a second set of formulas for the transpositions of our derivatives #10223. Inverse is more similar to reverse-mode AD since it requires a program transformation to swap the order of operations."", 'Some graph analysis to enable inplace if all ops are inversible within the block probably can be framed as an autograd feature :) Though I understand your point.\r\n\r\nSaving memory and not storing intermediate results is tempting :)', ""@ezyang In fact this API already exists in torch.distributions.transforms.\r\n\r\nI was always a bit puzzled why the distribution transforms contain a parallel set of operations (like ExpTransform, LogTransform) instead of adapting torch.exp and torch.log. Now it's clear one of the main reasons is missing inverse() and log_abs_det_jacobian() interface for core operations.\r\n\r\nIf inverse / log_abs_det_jacobian interface was supported in core, many of those doubles could go away (and maybe domain/codomain constraints could have been attached to already existing ops)."", 'That sounds like a much more compelling reason :)', ""On the batchnorm story side this could be:\r\n1. implement inverse() for BatchNorm and LeakyReLU (an idea about extensions: a generic way to add inverse for existing functions, maybe also a way of automatic computing a function inverse in the compiler if it's a composition of invertible functions?)\r\n2. implement inplace argument for BatchNorm (if inplace is used, it uses inverse to reconstruct the input)\r\n\r\nWith these primitives, at the very least the user can write the ConvBNLeakyRelu inplace composition themselves\r\n\r\nFor distributions/flows, also log_abs_det_jacobian extension of existing ops is needed (and maybe a generic way of endowing existing ops with more functionality like co/domain constraints or more information)"", ""@ezyang @fmassa  A minimal change to existing batchnorm would be supporting `out=` argument. Probably calling it with `out=input` would already work. Should I file a separate issue about this?\r\n\r\nIf I understand well, most of Mapillary's [C++ code](https://github.com/mapillary/inplace_abn/blob/master/src/inplace_abn_kernels.cuh#L154) is to do batch_norm inplace. And the inversion, input/recomputation code is trivial. E.g. it could probably be hacked so that saved BatchNorm input is updated before the gradient computation happens.\r\n\r\nPyTorch C++ BatchNorm backward call already seems to support passed explicit input argument. So if forward call supports passing explicit output argument, it would enable implementing inplace batchnorm with a very little amoun of C++ code, probably none at all, if the wrappers are exposed to Python"", '> A minimal change to existing batchnorm would be supporting out= argument. Probably calling it with out=input would already work. Should I file a separate issue about this?\r\n\r\nYes please!', 'a recent CMU/FAIR flow codebase: https://github.com/XuezheMax/flowseq', ""cc @stefanwebb who owns Pyro's module for transforms/flows."", '[(Sountsov, Radul, Vasudevan 2020)](https://arxiv.org/pdf/2001.05035.pdf) hints at a forthcoming paper about similar work in JAX:\r\n> The diffeomorphism must be an invertible function with tractable Jacobian log-determinant.\r\nFunMC relies on its backend to compute the inverse and the log-determinant of the Jacobian. One\r\npractical way to do this is to code the diffeomorphism using the tfp.bijectors library [Dillon et al.\r\n2017] (FunMC knows the inversion and Jacobian computation API defined thereby). The automatic\r\ninversion mechanism being developed for JAX [Vikram et al. 2020] looks like a promising future\r\nalternative.\r\n\r\n> ""Vikram, S., Radul, A., and Hoffman, M. D. Probabilistic Programming Transformations for JAX. 2020. In preparation.""\r\n', 'This can be done out-of-tree (implement your own inplace batch-norm and its inverse).\r\n\r\nThis could move into core if flow-like ops become more ubiquitous.', '(If you are having trouble figuring out how to implement this out of tree, let us know; we are open to features which make this easier to develop out of tree.)', '@gchanan I will return to inplace batchnorm work when I have time (we have a plan laid out with @ezyang in the last PR) - I already have an out-of-tree inplace version and with further few modifications in-tree (mostly function binding modifications), it will become almost trivial.\r\n\r\nProper support for invertible (and especially invertible and inplace) ops requires tracking of inverse_fn for every variable and a special graph traversal (that works in tandem with existing autograd) and also some tweaks for tensor version tracking - that will be hard to implement in general case out-of-tree. We discussed this recently with @albanD.\r\n\r\nRegular flows with sequential containers are of course already implemented ten times in repos like pyro.flows, and a lot of normalizing flows works roll their own APIs with minor differences and duplicate efforts.', ""@gchanan maybe it was confusing, I had repurposed the existing inplace batchnorm discussion for the generic flow / invertible computations. Maybe another issue with more concrete discussion of what's required for a general case support (e.g. how exactly tracking computing inverse should interact with existing reverse mode AD traversal) should be created."", 'Quick notes after some offline discussions with @ezyang \r\n\r\n## Looking only at linear maps\r\n\r\n### 1-1 mapping\r\nFor a simple linear mapping associated with a matrix `A`, the existence of the inverse is directly linked to the existence of the pseudo inverse of A.\r\nThe pseudo inverse rules can be used to inverse a simple composition of such functions: `y = ABx` =>  `x = B^-1 A^-1 y`.\r\nIf we align the use of A and B, we get `x = (yT A^-1T B^-1T)T`. If we compute this by doing left-association. This corresponds exactly to the kind of backward pass that our autograd engine computes after replacing the jacobian of A by the transpose of its inverse and the grad_output by the output.\r\n\r\n### many-1 mapping\r\nFor a bilinear map with input `x = (x1, x2)` and matrix `A = (A1, A2)` (with A1 and A2 being the blocks corresponding to x1 and x2 respectively).\r\nIn this case, inverting `A` will give the right result.\r\nNote that we cannot split this inversion into two pieces for A1 and A2 independently.\r\n\r\n### 1-many mapping\r\nFor variables that are ""re-used"" like:\r\n```\r\ny = Ax\r\nz = Bx\r\n```\r\nWe can see this as an equivalent map that takes `x` as inputs and `(y, z)` (stacked) as output.\r\nIn that case, we want to invert the matrix `(A, B)` (stacked).\r\nAs can be seen from this system, a single `x` exists and should be reconstructed by this system.\r\n\r\nThis has interesting impact for the ""autograd-like"" view we have above. Indeed, such engine will see here two elementary operations and a Tensor that is re-used.\r\nIn autograd, when such branching happens, the gradients from the different branch should be accumulated. In this setting, assuming all branch are properly invertible, all branch will compute the same result.\r\nSo a ""strict"" mode would ensure that the results are the same from all branch while a ""fast"" mode would only compute the result from one branch (the shortest one?).\r\nThe leads to the observation that a ""general backward engine"" should allow for:\r\n- Custom ""accumulation"" mechanism to handle branching\r\n- Custom work-discovery algorithm \r\n\r\n\r\n## Beyond linear map\r\n\r\nIn a very hand-wavy manner, I will claim that the same thing is true for general function that have an inverse function.', '@albanD @ezyang \r\nAbout inverse formuals: for many cases useful in practice these formulas can be found in normalizing flows repos and packages.\r\n\r\nAbout branching: I think user should have control (if wanted) about which branch is used for computing the inverse (for your example, I think it should be the most well-conditioned branch), like we have control by using `.detach()` or `torch.utils.checkpoint`. Maybe:\r\n```\r\ny = Ax.use_for_inverse() #some other name would be needed for sure\r\nz = Bx\r\n``` \r\n\r\n\r\nAbout interaction: I think inverse computations (especially when intersecting with autograd and inplace) may require some tricky ""reachability"" / ""computation ordering"" (not all subgraphs may require inverse computed for them, sometimes inverse may be required if inplace is used to compute backward). Maybe you call this ""work discovery""?', '> About branching\r\n\r\nInteresting, we did not consider numerical stability :)\r\nIndeed, it would be good to allow the user to choose the branch.\r\n\r\n> Maybe you call this ""work discovery""?\r\n\r\nI was not thinking about interaction with the autograd here but more what would need to change in the current autograd engine to support executing ""inverse graph"".', 'One good thing would be allowing to externally attach an inverse op to existing ops, so that I could say in my code that torch.log should be used as inverse of torch.exp (imagine a case when it does not have inverse in core library).\r\n\r\nAn intermediate variant:\r\n```python\r\ndef myexp(x):\r\n  y = x.exp()\r\n  def inv(y):\r\n    return y.log()\r\n  return y, inv\r\n```\r\n\r\nLike seems to be done in some internal TorchScript AD formulas for grad formulas.\r\n\r\nOf course something like:\r\n```python\r\ntorch.define_inverse(torch.exp, torch.log)\r\n```\r\n\r\nis cleaner', ""I think the reparametrization discussion is also very relevant because both inverse and log_abs_det_jacobian are needed for flows and maybe they need to cache some computations\r\n\r\n(about the recently submitted reparametrization PR: i don't like that it again does monkey patching  new parameters - doesn't seem very easily composable and leads to https://github.com/pytorch/pytorch/issues/33618)"", ""Given that #33344 is merged (with `right_inverse` duck typing method support, right? No InvertibleModule was introduced, right? @Lezcano), I'm proposing to discuss:\r\n- adding `right_inverse`/`inverse` method for existing modules: LeakyRelu, BatchNorm https://github.com/pytorch/pytorch/issues/26288\r\n- invertible sequential container (to not having to deal with https://github.com/pytorch/pytorch/issues/23756#issuecomment-589783564 at the beginning)\r\n- inplace forward option for invertible containers \r\n- adding inverses for functions? torch.exp, then probably ExpTransform could be deprecated (also if nn.Distributions could becone nn.Modules if caching problems are fixed @fritzo) \r\n- inplace gradient computation for sequential container: https://github.com/pytorch/pytorch/issues/46168\r\n\r\nEither standard Sequential container could be adapted or a new InvertibleSequential could be introduced. The problem for hooks could be solved by requiring the user to explicitly call something like:\r\n`container.disable_hooks()`\r\n\r\nA recent model making use of reversible nets is Performer: https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html"", ""A couple notes on these points.\r\n- The current implementation of #33344 uses an invertible sequential container to store the parametrisations.  At the moment, this container is exposed to the public, but it is not designed to be used from the outside. It would seem natural to think how far we are willing to go with this design. If we are going to start adding `right_inverse`s to the modules that may have them, `nn.Sequential` would be a very good candidate to have this.\r\n- At the moment, the parametrisation functionality is designed to be used on `nn.Module`s not on regular functions. It is not so clear how to do was done in that PR for plain functions.\r\n- The design of caching on parametrisations assumed that they would sit on top of a `Parameter` or a buffer. It would be possible to somehow modify this design to allow doing this in the setting of flows, where what's parametrised is the input of a module (not a parameter), but this would need some thinking, as it's not as straightforward as it may seem.\r\n\r\nAbout inplace forward / gradients, I do not know enough to say anything reasonable for or against it, but it looks tricky a priori."", 'Now remembering more, the problem with inplace gradients as also that currently gradient computation could return views. But a restricted container that always uses out contiguous (or contiguous+permuted) variant of gradient computaiton would still be useful', 'Also inplace / out-variants of `right_inverse` (or maybe other `inverse`) interface variants would be useful for memory-economic implementation of reversbile nets ', 'my experience with frankenstein inplace batchnorm was quite good in terms of memory reduction (~30%), but at a cost of a few percent slowdown (since it was not fused i guess)', '> invertible sequential container (to not having to deal with #23756 (comment) at the beginning)\r\n\r\nThis might be fairly tricky to do.\r\nIn particular, while our autograd is pretty good at doing basic checkpointing, it might be trickier to do this trick where you recompute the values from the end.\r\nWe would need to think about that.\r\n\r\n> adding inverses for functions? \r\n\r\nThis might be a semantic issue we bump into about the ""level"" at which you implement the feature: either torch.nn or native functions. \r\n\r\n> inplace gradient computation for sequential container: #46168\r\n\r\nAs you mentioned above, this can lead to other issues. Depending on how the sequence container works, we can reconsider that later. But I wouldn\'t bet on it from the start.', '> This might be a semantic issue we bump into about the ""level"" at which you implement the feature\r\n\r\n@zou3519 @Chillee  this touches on the question of user-defined graph transformations, maybe such dispatch to forward / inverse and their gradients etc could be implemented at very high-level close-to-frontend side, and then this decouples a bit - maybe that\'s closer to jax :)', ""Related: found that hidden JAX tutorial about computing function inverse automatically: https://jax.readthedocs.io/en/latest/notebooks/Writing_custom_interpreters_in_Jax.html#your-first-interpreter-invert (but it doesn't concern itself with inplace or interaction of inverse with gradient computation in context of reversible nets)"", 'As a follow up, here is a POC for implementing this in pytorch: https://colab.research.google.com/drive/1hxs1_PMJR7CpPm9bTQGoU3P0iFOY6NlO#scrollTo=q_EOxqQGgsGl\r\nNote that this is sub-optimal as we do the forward twice while in theory you would need to do it once. But at least we get the memory benefit.\r\n\r\nGiven this, I feel like having a third party repo that provides inverses for nn.Module (and does monkey patching on them when the user asks?) and InvertibleSequential from the POC would be a good first step to unlock research on that subject.\r\nI think that if such repo gets some traction, we can consider moving the low level details for it to core (with a similar approach as we have for https://github.com/facebookresearch/higher).\r\n\r\nWhat do you think?', '@albanD Also maybe we could reach out to authors of other generative invertible frameworks should be talked to: @fritzo, Zico Kolter (http://implicit-layers-tutorial.org/), torchdiffeq, Performer etc...\r\n\r\nInplace invertible BatchNorm + LeakyRelu also works really well and saves 20-30% of memory (https://github.com/mapillary/inplace_abn or my issue that reuses methods from SyncBatchNorm) and is a good practical showcase (along with i-RevNets and Performer) + I wonder if they could further be fused by compiler', ""> Given this, I feel like having a third party repo that provides inverses for nn.Module\r\n\r\nAlso, more modules should support inplace. And InvertibleSequential may need to patch out certain versions (decrement them back) updates so that version tracker doesn't complain. E.g. if one wants to do both BatchNorm and LeakyRelu inplace (to be rolled back during inverse+backward), one should sidestep version tracker"", ""> And InvertibleSequential may need to patch out certain versions (decrement them back) updates so that version tracker doesn't complain.\r\n\r\nThere is no need because we don't save anything in the graph. So nothing will complain that the version changed ;) "", 'But multiple output tensors would still be allocated, right? (unless Python and allocator are being perfect at buffer reuse)', 'Also, maybe @lucidrains would have some comments, since they implemented reversible Reformer in pytorch: https://github.com/lucidrains/reformer-pytorch', ""> But multiple output tensors would still be allocated, right?\r\n\r\nThat's a different concern :D \r\nThis code won't automagically make every op an inplace op indeed. It will only ensure that at any time, you never keep alive more than one input/output pair.\r\nMaking everything inplace would indeed be another step forward, but that would require more significant change of the user's code."", ""I know, but some operations may also support `inplace` argument :) LeakyRelu already does. BatchNorm could support it too, but I haven't maintained my motivation to finish that work ^^"", 'I guess a more operationalizable question is: would it work if you use `LeakyRelu(inplace = True)` inside your InvertibleContainer?\r\n\r\nIf yes, would it work when InvertibleSequential is part of a larger model that may depend on versions to be restored?\r\n\r\nAlso, things like `InvertibleSequential(batchnorm(inplace) + learkyrelu(inplace) + leakydropout (inplace))` could be all fused together in theory by a smart compiler into one big `InvertibleFusedGroup(inplace)`', '@albanD https://mobile.twitter.com/PierreAblin/status/1391771747851046913 reversible backbones with constant memory footprint get more traction :) cc @michaelsdr', ""Though in the code does some smart stuff to prevent precision loss and also has a tricky backward pass: https://github.com/michaelsdr/momentumnet/blob/02988380114b8b17f32ba2240a8cb61a3f049ff3/momentumnet/momentum_net.py#L77 - maybe not very efficient\r\n\r\nAlso, I don't get how the codebase handles varying widths in the net (maybe does reversible only within the same-channels blocks?)""]",[],[],0,0
237,pytorch,145,closed,embeddings layer with IntTensor / cuda.IntTensor inputs,"Would it be possible to feed the embedding layer with IntTensors rather than LongTensors only?
",feature todo triaged,"[""is this for perf issues? what's your motivation? We can prob easily support this.\n"", ""This is really not urgent, and would just be for convenience. Also I think this is weird to only accept LongTensors for nn.Embedding as people usually don't have a lot of embeddings (nothing that would require a Long anyway). But for people working on big MT datasets this would save half the space once the dataset is loaded in memory.\n"", ""Can't you keep your dataset as an int tensor, and convert batches to long before the forward? This shouldn't be very expensive.\n"", ""Yes, I'm doing this. This is fine.\n"", ""Ok, I'll keep the issue open, but with a very low priority (it's similar to #28).\n"", 'Hi @soumith  @apaszke, \r\nI have a use case where my training time is extremely large (About 5-6 days on a single gpu) and any increase in batch size would be extremely useful. I was checking through the docs for Pytorch 0.4.0 and this feature (Being able to access Embeddings with IntTensors still does not exist).\r\nIs there a reason why it is not supported at the moment?\r\n\r\nThanks', '@apaszke @soumith In my case, the lack of `torch.int32` support causes compatibility issues when exporting to ONNX and running in TensorRT 7. As mentioned in Nvidia\'s [docs](https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/python_api/infer/FoundationalTypes/DataType.html#datatype), int64 is not supported in TensorRT. \r\n\r\nTherefore, when I run the model there are many warnings """"One or more weights outside the range of INT32 was clamped"". The outputs are also not as accurate as other inference providers, indicating that the cast from int64 to int32 caused actual degradation in the model accuracy. I only have 150 embedding vectors, so int64 is way overkill', ""Guys, please, fix that! üî•\r\n\r\nYou did great job here. But if anyone need to use TensorRT, they can't use your embedding layer."", 'It seems that for new versions of TensorRT, the engine automatically converts all weights and inputs to int32 and TensorRT works well for int32 input in that case. However, using int64 is still an overkill if you inference your PyTorch model directly or use ONNX.', 'This was added in https://github.com/pytorch/pytorch/pull/46758 and is now working just fine on master for `dtype=torch.int`.']",[],[],0,0
238,pytorch,5302,closed,Slow first .cuda() call,"I upgraded to v0.3.1 using conda and have since experienced really slow performance the first time I call .cuda(). I tried solutions listed [here](https://discuss.pytorch.org/t/model-cuda-takes-long-time/102/20) and [here](https://discuss.pytorch.org/t/model-cuda-takes-long-time/102/20) but no luck. I then tried installing the latest version from source but still experienced the slowdown. Any help would be hugely appreciated!

Info
- OS: CentOS 7.4.1708
- PyTorch version: tried 0.3.1 (conda) and 0.4.0a0+6279367 (source)
- How you installed PyTorch (conda, pip, source): conda, source
- Python version: 3.6.3
- CUDA/cuDNN version: 8.0.61, 7.0.2
- GPU models and configuration: 1080ti
- GCC version (if compiling from source): 7.2.0",,"[""The cuda context is lazily initialized, so it's expected that the first cuda() call is slower."", 'With the update, the delay is on the order of minutes to cuda a model,\nwhereas previously it was just a few seconds, so it seems like something is\nwrong\n\nOn Feb 19, 2018 23:18, ""Tongzhou Wang"" <notifications@github.com> wrote:\n\n> The cuda context is lazily initialized, so it\'s expected that the first\n> cuda() call is slower.\n>\n> ‚Äî\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/5302#issuecomment-366862767>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AFQ7G9DGR3A8_t9prAMdtQ81pni71jjRks5tWkepgaJpZM4SLDVr>\n> .\n>\n', ""[build_log.log](https://github.com/pytorch/pytorch/files/1741280/build_log.log)\r\n[nvidia-smi.log](https://github.com/pytorch/pytorch/files/1741281/nvidia-smi.log)\r\n\r\nLog files from calling '> python setup.py build develop' and '> nvidia-smi'"", 'Ended up working itself out...probably a hardware issue', ""CUDA JIT compiles all the kernels in the shared library when it's initialized. This can take a few minutes the first time. It caches the results (usually in `~/.nv/ComputeCache/`) so subsequent sessions should be much quicker."", ""@colesbury our binary and source builds dont make CUDA JIT-compile anything. We ship SASS for all supported GPU architectures.\r\n\r\n@W4ngatang I'm so glad it worked itself out, we couldn't think of what was going wrong"", 'If you encounter the issue again, run the CUDA initialization under strace:\r\n\r\n```\r\nstrace python -c ""import torch; torch.cuda.FloatTensor(1)""\r\n```\r\n\r\nor to log to a file\r\n\r\n```\r\nstrace python -c ""import torch; torch.cuda.FloatTensor(1)"" 2>&1 | tee strace.log\r\n```', 'I have the same issue. \r\nIt seems as if pytorch is looping the whole memory (takes up to 3 minutes on my machine E5-2620). \r\nI attached the strace of:\r\n`strace python -c ""import torch; torch.cuda.FloatTensor(1)"" 2>&1 | tee strace.log`\r\nAnyway, the issue seems to be fixed with 0.3.0.post4, but might be helpful in case of a regression.\r\n\r\nAffected Version:\r\nPython: (3, 6, 4)\r\nCuDNN: 7005\r\nCUDA: 8.0.61\r\nTorch: 0.3.0\r\nNumpy: 1.14.0\r\n\r\nWorking Version:\r\nPython: (3, 6, 3)    <-- No effect\r\nCuDNN: 7003       <-- No effect\r\nCUDA: 9.0.176      <-- No effect\r\nTorch: 0.3.0.post4 <-- This fixes the issue\r\nNumpy: 1.13.3\r\n\r\nImportant:\r\nThe version from the official conda stream (pkgs/main) is affected at the moment. Take the one from pytorch directly, which is working:\r\n`conda install -c pytorch pytorch`\r\n\r\n[strace.zip](https://github.com/pytorch/pytorch/files/1745793/strace.zip)\r\n']",[],[],0,0
239,pytorch,9356,closed,RuntimeError: cuda runtime error (8) : invalid device function at /opt/conda/conda-bld/pytorch_1518243271935/work/torch/lib/THCUNN/generic/Threshold.cu:34,"[forums](https://discuss.pytorch.org/).

If you are submitting a bug report, please fill in the following details.
I am using linux machine with ubuntu in it . 
Ubuntu Version 16.04 lts
NVIDIA  930mx
Nvidia driver version :  384.184

## Issue description
RuntimeError: cuda runtime error (8) : invalid device function at /opt/conda/conda-bld/pytorch_1518243271935/work/torch/lib/THCUNN/generic/Threshold.cu:34
Provide a short description.

## Code example
 This code is is for the segnet , for detection of Vessel in retina.

Please try to provide a minimal example to repro the bug.
Error messages and stack traces are also helpful.

## System Info
Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


- PyTorch or Caffe2:
- How you installed PyTorch (conda, pip, source): conda
- Build command you used (if compiling from source):
- OS:  Ubuntu 16.04
- PyTorch version: 0.3.1u
- Python version: 3.6
- CUDA/cuDNN version:7.5
![issue_pytorch](https://user-images.githubusercontent.com/24300927/42598377-a3d0db30-8579-11e8-9db3-886d6a9e498c.png)

- GPU models and configuration:
- GCC version (if compiling from source):
- CMake version:
- Versions of any other relevant libraries:
",,"['Can you try 0.4?', 'with 930mx, pytorch 0.3.1 wont work. This is because we didn;t ship for CUDA capability 5.0 support in 0.3.1 release.\r\n0.4 release ships with 5.0 support, and hence will work without giving this error.']","[""\r\niterations = 100\r\ntrainLoss = []\r\ntestLoss = []\r\nstart = time.time()\r\n\r\nfor epoch in range(iterations):\r\n    epochStart = time.time()\r\n    runningLoss = 0   \r\n    net.train(True) # For training\r\n    for data in trainLoader:\r\n        inputs,labels = data\r\n        # Wrap them in Variable\r\n        if use_gpu:\r\n            inputs, labels = Variable(inputs.cuda()), \\\r\n                Variable(labels.long().cuda())\r\n        else:\r\n            inputs, labels = Variable(inputs), Variable(labels.long())      \r\n \r\n        \r\n        # Feed-forward input data through the network\r\n        outputs = net(inputs)\r\n        # Compute loss/error\r\n        loss = criterion(F.log_softmax(outputs), labels)      \r\n        # Initialize gradients to zero\r\n        optimizer.zero_grad()                  \r\n        # Backpropagate loss and compute gradients\r\n        loss.backward()\r\n        # Update the network parameters\r\n        optimizer.step()\r\n        # Accumulate loss per batch\r\n        runningLoss += loss.data[0]          \r\n    avgTrainLoss = runningLoss/600.0    \r\n    trainLoss.append(avgTrainLoss)\r\n  \r\n    \r\n    # Evaluating performance on test set for each epoch\r\n    net.train(False) # For testing\r\n    test_runningLoss = 0    \r\n    for data in testLoader:\r\n        inputs,labels = data\r\n        # Wrap them in Variable\r\n        if use_gpu:\r\n            inputs, labels = Variable(inputs.cuda()), \\\r\n                Variable(labels.long().cuda())\r\n        else:\r\n            inputs, labels = Variable(inputs), Variable(labels.long())         \r\n        outputs = net(inputs)       \r\n         # Compute loss/error\r\n        loss = criterion(F.log_softmax(outputs), labels)      \r\n        # Accumulate loss per batch\r\n        test_runningLoss += loss.data[0] \r\n        \r\n    avgTestLoss = test_runningLoss/20.0    \r\n    testLoss.append(avgTestLoss)\r\n        \r\n    # Plotting Loss vs Epochs\r\n    fig1 = plt.figure(1)        \r\n    plt.plot(range(epoch+1),trainLoss,'r--',label='train')        \r\n    plt.plot(range(epoch+1),testLoss,'g--',label='test')        \r\n    if epoch==0:\r\n        plt.legend(loc='upper left')\r\n        plt.xlabel('Epochs')\r\n        plt.ylabel('Loss')   \r\n      \r\n    \r\n    epochEnd = time.time()-epochStart\r\n    print('At Iteration: {:.0f} /{:.0f}  ;  Training Loss: {:.6f}; Time consumed: {:.0f}m {:.0f}s '\\\r\n          .format(epoch + 1,iterations,avgTrainLoss,epochEnd//60,epochEnd%60))\r\n    print('At Iteration: {:.0f} /{:.0f}  ;  Testing Loss: {:.6f} ; Time consumed: {:.0f}m {:.0f}s '\\\r\n          .format(epoch + 1,iterations,avgTestLoss,epochEnd//60,epochEnd%60))\r\nend = time.time()-start\r\nprint('Training completed in {:.0f}m {:.0f}s'.format(end//60,end%60))\r\n"", '\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n']",[],0,0
240,pytorch,28781,closed, run run_test.py has error,"## ‚ùì Questions and Help
hi 
I am new at pytorch 

i try to install pytorch with ROCm and step by step following https://rocm-documentation.readthedocs.io/en/latest/Deep_learning/Deep-learning.html#building-pytorch-for-rocm 

output of ""PYTORCH_TEST_WITH_ROCM=1 python test/run_test.py --verbose"" should all correct ,but i got some error like this


test_lerp_tensor_weights,
test_unused_output_device_cuda were failed 

could anyone help me figure out please

my hardware :
os ubuntu 16.04
motherboard ROG CROSSHAIR VI HERO
cpu 2700x
gpu rx480 * 2


",module: tests triaged,"['Closing this issue due to age. Sorry to hear you were having a tough time getting multiple ROCm cards to work with PyTorch. You may need to use only a single ROCm card at a time. With CUDA cards you can do this by exporting CUDA_VISIBLE_DEVICES -- maybe that works for ROCm, too? ']","['\r\nTest executor: [\'/usr/bin/python\']\r\nExcluding cpp_extensions on ROCm\r\nExcluding distributed on ROCm\r\nExcluding multiprocessing on ROCm\r\nExcluding multiprocessing_spawn on ROCm\r\nSelected tests: autograd, c10d, c10d_spawn, cuda, cuda_primary_ctx, dataloader, distributions, docs_coverage, expecttest, fake_quant, indexing, jit, logging, mkldnn, nccl, nn, numba_integration, optim, qat, quantization, quantized, quantized_tensor, quantized_nn_mods, quantizer, sparse, torch, type_info, type_hints, utils, namedtuple_return_api, jit_fuser, tensorboard, namedtensor, type_promotion, jit_disabled, function_schema\r\nRunning test_autograd ... [2019-10-28 03:37:35.924929]\r\ntest___getitem__ (__main__.TestAutograd) ... ok\r\ntest___getitem___adv_index (__main__.TestAutograd) ... ok\r\ntest___getitem___adv_index_beg (__main__.TestAutograd) ... ok\r\ntest___getitem___adv_index_comb (__main__.TestAutograd) ... ok\r\ntest___getitem___adv_index_dup (__main__.TestAutograd) ... ok\r\ntest___getitem___adv_index_end (__main__.TestAutograd) ... ok\r\ntest___getitem___adv_index_mid (__main__.TestAutograd) ... ok\r\ntest___getitem___adv_index_sub (__main__.TestAutograd) ... ok\r\ntest___getitem___adv_index_sub_2 (__main__.TestAutograd) ... ok\r\ntest___getitem___adv_index_sub_3 (__main__.TestAutograd) ... ok\r\ntest___getitem___adv_index_var (__main__.TestAutograd) ... ok\r\ntest___getitem___slice (__main__.TestAutograd) ... ok\r\ntest___getitem___slice_index (__main__.TestAutograd) ... ok\r\ntest___radd___constant (__main__.TestAutograd) ... ok\r\ntest___radd___scalar_constant (__main__.TestAutograd) ... ok\r\ntest___rdiv___constant (__main__.TestAutograd) ... ok\r\ntest___rdiv___scalar_constant (__main__.TestAutograd) ... ok\r\ntest___rmul___constant (__main__.TestAutograd) ... ok\r\ntest___rmul___scalar_constant (__main__.TestAutograd) ... ok\r\ntest___rpow___constant (__main__.TestAutograd) ... ok\r\ntest___rpow___scalar_constant (__main__.TestAutograd) ... ok\r\ntest___rsub___constant (__main__.TestAutograd) ... ok\r\ntest___rsub___scalar_constant (__main__.TestAutograd) ... ok\r\ntest_abs (__main__.TestAutograd) ... ok\r\ntest_abs_scalar (__main__.TestAutograd) ... ok\r\ntest_accumulate_grad (__main__.TestAutograd) ... ok\r\ntest_accumulate_grad_tensor_reference (__main__.TestAutograd) ... ok\r\ntest_acos (__main__.TestAutograd) ... ok\r\ntest_add (__main__.TestAutograd) ... ok\r\ntest_add_broadcast_all (__main__.TestAutograd) ... ok\r\ntest_add_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_add_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_add_constant (__main__.TestAutograd) ... ok\r\ntest_add_scalar (__main__.TestAutograd) ... ok\r\ntest_add_scalar_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_add_scalar_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_add_scalar_constant (__main__.TestAutograd) ... ok\r\ntest_addbmm (__main__.TestAutograd) ... ok\r\ntest_addbmm_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_addbmm_broadcast_lhs_coef (__main__.TestAutograd) ... ok\r\ntest_addbmm_coef (__main__.TestAutograd) ... ok\r\ntest_addbmm_scalar_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_addbmm_scalar_broadcast_lhs_coef (__main__.TestAutograd) ... ok\r\ntest_addcdiv (__main__.TestAutograd) ... ok\r\ntest_addcdiv_broadcast_all (__main__.TestAutograd) ... ok\r\ntest_addcdiv_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_addcdiv_scalar (__main__.TestAutograd) ... ok\r\ntest_addcdiv_scalar_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_addcdiv_scalar_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_addcdiv_scalar_scale (__main__.TestAutograd) ... ok\r\ntest_addcdiv_scalar_scale_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_addcdiv_scalar_scale_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_addcdiv_scale (__main__.TestAutograd) ... ok\r\ntest_addcdiv_scale_broadcast_all (__main__.TestAutograd) ... ok\r\ntest_addcdiv_scale_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_addcmul (__main__.TestAutograd) ... ok\r\ntest_addcmul_broadcast_all (__main__.TestAutograd) ... ok\r\ntest_addcmul_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_addcmul_scalar (__main__.TestAutograd) ... ok\r\ntest_addcmul_scalar_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_addcmul_scalar_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_addcmul_scalar_scale (__main__.TestAutograd) ... ok\r\ntest_addcmul_scalar_scale_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_addcmul_scalar_scale_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_addcmul_scale (__main__.TestAutograd) ... ok\r\ntest_addcmul_scale_broadcast_all (__main__.TestAutograd) ... ok\r\ntest_addcmul_scale_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_addmm (__main__.TestAutograd) ... ok\r\ntest_addmm_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_addmm_broadcast_lhs_coef (__main__.TestAutograd) ... ok\r\ntest_addmm_coef (__main__.TestAutograd) ... ok\r\ntest_addmm_scalar_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_addmm_scalar_broadcast_lhs_coef (__main__.TestAutograd) ... ok\r\ntest_addmv (__main__.TestAutograd) ... ok\r\ntest_addmv_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_addmv_broadcast_lhs_coef (__main__.TestAutograd) ... ok\r\ntest_addmv_coef (__main__.TestAutograd) ... ok\r\ntest_addmv_scalar_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_addmv_scalar_broadcast_lhs_coef (__main__.TestAutograd) ... ok\r\ntest_addr (__main__.TestAutograd) ... ok\r\ntest_addr_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_addr_broadcast_lhs_coef (__main__.TestAutograd) ... ok\r\ntest_addr_coef (__main__.TestAutograd) ... ok\r\ntest_anomaly_detect_nan (__main__.TestAutograd) ... ok\r\ntest_as_strided (__main__.TestAutograd) ... ok\r\ntest_asin (__main__.TestAutograd) ... ok\r\ntest_atan (__main__.TestAutograd) ... ok\r\ntest_atan2 (__main__.TestAutograd) ... ok\r\ntest_atan2_broadcast_all (__main__.TestAutograd) ... ok\r\ntest_atan2_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_atan2_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_atan2_scalar (__main__.TestAutograd) ... ok\r\ntest_atan_scalar (__main__.TestAutograd) ... ok\r\ntest_attribute_deletion (__main__.TestAutograd) ... ok\r\ntest_backward (__main__.TestAutograd) ... ok\r\ntest_backward_badcalls (__main__.TestAutograd) ... ok\r\ntest_backward_copy (__main__.TestAutograd) ... ok\r\ntest_backward_no_grad (__main__.TestAutograd) ... ok\r\ntest_backward_twice_retained_graph_with_saved_values (__main__.TestAutograd) ... ok\r\ntest_backward_twice_retained_graph_without_saved_values (__main__.TestAutograd) ... ok\r\ntest_backward_twice_with_saved_values (__main__.TestAutograd) ... ok\r\ntest_backward_twice_without_saved_values (__main__.TestAutograd) ... ok\r\ntest_baddbmm (__main__.TestAutograd) ... ok\r\ntest_baddbmm_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_baddbmm_broadcast_lhs_coef (__main__.TestAutograd) ... ok\r\ntest_baddbmm_coef (__main__.TestAutograd) ... ok\r\ntest_baddbmm_scalar_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_baddbmm_scalar_broadcast_lhs_coef (__main__.TestAutograd) ... ok\r\ntest_bmm (__main__.TestAutograd) ... ok\r\ntest_broadcast_tensors (__main__.TestAutograd) ... ok\r\ntest_call_legacy_twice (__main__.TestAutograd) ... /root/pytorch/torch/csrc/autograd/python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n  ""(Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)"");\r\nok\r\ntest_cat (__main__.TestAutograd) ... ok\r\ntest_cat_empty (__main__.TestAutograd) ... ok\r\ntest_cat_empty_legacy (__main__.TestAutograd) ... ok\r\ntest_cat_negdim_1 (__main__.TestAutograd) ... ok\r\ntest_cat_negdim_2 (__main__.TestAutograd) ... ok\r\ntest_ceil (__main__.TestAutograd) ... ok\r\ntest_ceil_scalar (__main__.TestAutograd) ... ok\r\ntest_chain_matmul (__main__.TestAutograd) ... ok\r\ntest_checkpointing (__main__.TestAutograd) ... skipped \'test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test\'\r\ntest_cholesky (__main__.TestAutograd) ... ok\r\ntest_cholesky_inverse (__main__.TestAutograd) ... ok\r\ntest_cholesky_solve (__main__.TestAutograd) ... ok\r\ntest_chunk (__main__.TestAutograd) ... ok\r\ntest_chunk_dim (__main__.TestAutograd) ... ok\r\ntest_chunk_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_clamp (__main__.TestAutograd) ... ok\r\ntest_clamp_max (__main__.TestAutograd) ... ok\r\ntest_clamp_max_scalar (__main__.TestAutograd) ... ok\r\ntest_clamp_max_scalar_kwarg (__main__.TestAutograd) ... ok\r\ntest_clamp_min (__main__.TestAutograd) ... ok\r\ntest_clamp_min_scalar (__main__.TestAutograd) ... ok\r\ntest_clamp_scalar (__main__.TestAutograd) ... ok\r\ntest_clone (__main__.TestAutograd) ... ok\r\ntest_clone_scalar (__main__.TestAutograd) ... ok\r\ntest_contiguous (__main__.TestAutograd) ... ok\r\ntest_contiguous_not_contiguous (__main__.TestAutograd) ... ok\r\ntest_cos (__main__.TestAutograd) ... ok\r\ntest_cos_scalar (__main__.TestAutograd) ... ok\r\ntest_cosh (__main__.TestAutograd) ... ok\r\ntest_cosh_scalar (__main__.TestAutograd) ... ok\r\ntest_cross (__main__.TestAutograd) ... ok\r\ntest_cross_dim (__main__.TestAutograd) ... ok\r\ntest_cumprod (__main__.TestAutograd) ... ok\r\ntest_cumprod_dim1 (__main__.TestAutograd) ... ok\r\ntest_cumprod_dim1_neg0 (__main__.TestAutograd) ... ok\r\ntest_cumprod_scalar (__main__.TestAutograd) ... ok\r\ntest_cumprod_scalar_zeros (__main__.TestAutograd) ... ok\r\ntest_cumprod_zeros_dim0 (__main__.TestAutograd) ... ok\r\ntest_cumprod_zeros_dim0_cast (__main__.TestAutograd) ... ok\r\ntest_cumprod_zeros_dim0_cast_neg0 (__main__.TestAutograd) ... ok\r\ntest_cumprod_zeros_dim0_neg0 (__main__.TestAutograd) ... ok\r\ntest_cumprod_zeros_dim1 (__main__.TestAutograd) ... ok\r\ntest_cumprod_zeros_dim1_neg0 (__main__.TestAutograd) ... ok\r\ntest_cumprod_zeros_dim2 (__main__.TestAutograd) ... ok\r\ntest_cumprod_zeros_dim2_neg0 (__main__.TestAutograd) ... ok\r\ntest_cumsum_dim0 (__main__.TestAutograd) ... ok\r\ntest_cumsum_dim0_neg0 (__main__.TestAutograd) ... ok\r\ntest_cumsum_dim0_scalar (__main__.TestAutograd) ... ok\r\ntest_cumsum_dim0_scalar_neg0 (__main__.TestAutograd) ... ok\r\ntest_cumsum_dim1 (__main__.TestAutograd) ... ok\r\ntest_cumsum_dim1_cast (__main__.TestAutograd) ... ok\r\ntest_cumsum_dim1_cast_neg0 (__main__.TestAutograd) ... ok\r\ntest_cumsum_dim1_neg0 (__main__.TestAutograd) ... ok\r\ntest_custom_autograd_no_early_free (__main__.TestAutograd) ... ok\r\ntest_custom_autograd_repeated_grad_grad (__main__.TestAutograd) ... ok\r\ntest_deep_reentrant (__main__.TestAutograd) ... ok\r\ntest_dep_nograd (__main__.TestAutograd) ... ok\r\ntest_dependent_backward (__main__.TestAutograd) ... ok\r\ntest_det (__main__.TestAutograd) ... ok\r\ntest_det_1x1 (__main__.TestAutograd) ... ok\r\ntest_det_batched (__main__.TestAutograd) ... ok\r\ntest_det_batched_1x1 (__main__.TestAutograd) ... ok\r\ntest_det_batched_distinct_singular_values (__main__.TestAutograd) ... ok\r\ntest_det_batched_symmetric (__main__.TestAutograd) ... ok\r\ntest_det_batched_symmetric_pd (__main__.TestAutograd) ... ok\r\ntest_det_batched_symmetric_psd (__main__.TestAutograd) ... ok\r\ntest_det_dim2_null (__main__.TestAutograd) ... ok\r\ntest_det_distinct_singular_values (__main__.TestAutograd) ... ok\r\ntest_det_rank1 (__main__.TestAutograd) ... ok\r\ntest_det_rank2 (__main__.TestAutograd) ... ok\r\ntest_det_symmetric (__main__.TestAutograd) ... ok\r\ntest_det_symmetric_pd (__main__.TestAutograd) ... ok\r\ntest_det_symmetric_psd (__main__.TestAutograd) ... ok\r\ntest_detach (__main__.TestAutograd) ... ok\r\ntest_detach_base (__main__.TestAutograd)\r\ndetaching base does not detach view ... ok\r\ntest_diag_1d (__main__.TestAutograd) ... ok\r\ntest_diag_2d (__main__.TestAutograd) ... ok\r\ntest_diag_2d_1 (__main__.TestAutograd) ... ok\r\ntest_diag_2d_2 (__main__.TestAutograd) ... ok\r\ntest_diag_2d_tall (__main__.TestAutograd) ... ok\r\ntest_diag_2d_tall_neg (__main__.TestAutograd) ... ok\r\ntest_diag_2d_tall_pos (__main__.TestAutograd) ... ok\r\ntest_diag_2d_wide (__main__.TestAutograd) ... ok\r\ntest_diag_2d_wide_neg (__main__.TestAutograd) ... ok\r\ntest_diag_2d_wide_pos (__main__.TestAutograd) ... ok\r\ntest_diag_embed (__main__.TestAutograd) ... ok\r\ntest_diagonal_2d (__main__.TestAutograd) ... ok\r\ntest_diagonal_2d_1 (__main__.TestAutograd) ... ok\r\ntest_diagonal_2d_2 (__main__.TestAutograd) ... ok\r\ntest_diagonal_2d_tall (__main__.TestAutograd) ... ok\r\ntest_diagonal_2d_tall_neg (__main__.TestAutograd) ... ok\r\ntest_diagonal_2d_tall_pos (__main__.TestAutograd) ... ok\r\ntest_diagonal_2d_wide (__main__.TestAutograd) ... ok\r\ntest_diagonal_2d_wide_neg (__main__.TestAutograd) ... ok\r\ntest_diagonal_2d_wide_pos (__main__.TestAutograd) ... ok\r\ntest_diagonal_3d_1 (__main__.TestAutograd) ... ok\r\ntest_diagonal_3d_2 (__main__.TestAutograd) ... ok\r\ntest_diagonal_3d_3 (__main__.TestAutograd) ... ok\r\ntest_diagonal_derivative_requires_grad (__main__.TestAutograd) ... ok\r\ntest_dir (__main__.TestAutograd) ... ok\r\ntest_dist (__main__.TestAutograd) ... ok\r\ntest_dist_4 (__main__.TestAutograd) ... ok\r\ntest_dist_4_broadcast_all (__main__.TestAutograd) ... ok\r\ntest_dist_4_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_dist_4_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_dist_broadcast_all (__main__.TestAutograd) ... ok\r\ntest_dist_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_dist_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_dist_scalar (__main__.TestAutograd) ... ok\r\ntest_dist_scalar_4 (__main__.TestAutograd) ... ok\r\ntest_dist_scalar_4_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_dist_scalar_4_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_dist_scalar_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_dist_scalar_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_div (__main__.TestAutograd) ... ok\r\ntest_div_broadcast_all (__main__.TestAutograd) ... ok\r\ntest_div_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_div_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_div_constant (__main__.TestAutograd) ... ok\r\ntest_div_scalar (__main__.TestAutograd) ... ok\r\ntest_div_scalar_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_div_scalar_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_div_scalar_constant (__main__.TestAutograd) ... ok\r\ntest_dot (__main__.TestAutograd) ... ok\r\ntest_duplicate_backward_root (__main__.TestAutograd) ... ok\r\ntest_eq_ (__main__.TestAutograd) ... ok\r\ntest_eq__broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_eq__pyscalar (__main__.TestAutograd) ... ok\r\ntest_eq__pyscalar_scalar (__main__.TestAutograd) ... ok\r\ntest_eq__scalar (__main__.TestAutograd) ... ok\r\ntest_eq__scalar_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_erf (__main__.TestAutograd) ... ok\r\ntest_erf_scalar (__main__.TestAutograd) ... ok\r\ntest_erfc (__main__.TestAutograd) ... ok\r\ntest_erfc_scalar (__main__.TestAutograd) ... ok\r\ntest_erfinv (__main__.TestAutograd) ... ok\r\ntest_erfinv_scalar (__main__.TestAutograd) ... ok\r\ntest_exp (__main__.TestAutograd) ... ok\r\ntest_exp_scalar (__main__.TestAutograd) ... ok\r\ntest_expand (__main__.TestAutograd) ... ok\r\ntest_expand_1_element (__main__.TestAutograd) ... ok\r\ntest_expand_as (__main__.TestAutograd) ... ok\r\ntest_expand_new_dim (__main__.TestAutograd) ... ok\r\ntest_expand_new_dim_front_old_front_1 (__main__.TestAutograd) ... ok\r\ntest_expand_scalar_to_dims (__main__.TestAutograd) ... ok\r\ntest_expand_scalar_to_scalar (__main__.TestAutograd) ... ok\r\ntest_expand_size (__main__.TestAutograd) ... ok\r\ntest_expm1 (__main__.TestAutograd) ... ok\r\ntest_expm1_scalar (__main__.TestAutograd) ... ok\r\ntest_fft_ifft_rfft_irfft (__main__.TestAutograd) ... skipped \'PyTorch is built without MKL support\'\r\ntest_fill (__main__.TestAutograd) ... ok\r\ntest_fill__number (__main__.TestAutograd) ... ok\r\ntest_fill__number_scalar (__main__.TestAutograd) ... ok\r\ntest_fill__variable (__main__.TestAutograd) ... ok\r\ntest_flip_d0 (__main__.TestAutograd) ... ok\r\ntest_flip_d012 (__main__.TestAutograd) ... ok\r\ntest_flip_d02 (__main__.TestAutograd) ... ok\r\ntest_flip_d20 (__main__.TestAutograd) ... ok\r\ntest_flip_neg_d (__main__.TestAutograd) ... ok\r\ntest_floor (__main__.TestAutograd) ... ok\r\ntest_floor_scalar (__main__.TestAutograd) ... ok\r\ntest_fmod (__main__.TestAutograd) ... ok\r\ntest_fmod_scalar (__main__.TestAutograd) ... ok\r\ntest_fmod_scalar_tensor (__main__.TestAutograd) ... ok\r\ntest_fmod_scalar_tensor_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_fmod_scalar_tensor_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_fmod_tensor (__main__.TestAutograd) ... ok\r\ntest_fmod_tensor_broadcast_all (__main__.TestAutograd) ... ok\r\ntest_fmod_tensor_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_fmod_tensor_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_frac (__main__.TestAutograd) ... ok\r\ntest_frac_scalar (__main__.TestAutograd) ... ok\r\ntest_free_deep_graph (__main__.TestAutograd) ... ok\r\ntest_free_deep_graph_complicated (__main__.TestAutograd) ... ok\r\ntest_free_deep_graph_pyfunction (__main__.TestAutograd) ... ok\r\ntest_function (__main__.TestAutograd) ... ok\r\ntest_function_returns_input (__main__.TestAutograd) ... ok\r\ntest_gather_dim0 (__main__.TestAutograd) ... ok\r\ntest_gather_dim0_neg0 (__main__.TestAutograd) ... ok\r\ntest_gather_dim1 (__main__.TestAutograd) ... ok\r\ntest_gather_dim1_neg0 (__main__.TestAutograd) ... ok\r\ntest_gather_scalar_both (__main__.TestAutograd) ... ok\r\ntest_gather_scalar_both_neg0 (__main__.TestAutograd) ... ok\r\ntest_gather_scalar_index (__main__.TestAutograd) ... ok\r\ntest_gather_scalar_index_neg0 (__main__.TestAutograd) ... ok\r\ntest_gather_scalar_input (__main__.TestAutograd) ... ok\r\ntest_gather_scalar_input_neg0 (__main__.TestAutograd) ... ok\r\ntest_gc_in_destructor (__main__.TestAutograd) ... /root/pytorch/torch/csrc/autograd/python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n  ""(Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)"");\r\n/root/pytorch/torch/csrc/autograd/python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n  ""(Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)"");\r\n/root/pytorch/torch/csrc/autograd/python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n  ""(Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)"");\r\n/root/pytorch/torch/csrc/autograd/python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n  ""(Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)"");\r\n/root/pytorch/torch/csrc/autograd/python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n  ""(Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)"");\r\n/root/pytorch/torch/csrc/autograd/python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n  ""(Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)"");\r\n/root/pytorch/torch/csrc/autograd/python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n  ""(Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)"");\r\n/root/pytorch/torch/csrc/autograd/python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n  ""(Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)"");\r\n/root/pytorch/torch/csrc/autograd/python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n  ""(Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)"");\r\n/root/pytorch/torch/csrc/autograd/python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n  ""(Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)"");\r\nok\r\ntest_ge_ (__main__.TestAutograd) ... ok\r\ntest_ge__broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_ge__pyscalar (__main__.TestAutograd) ... ok\r\ntest_ge__pyscalar_scalar (__main__.TestAutograd) ... ok\r\ntest_ge__scalar (__main__.TestAutograd) ... ok\r\ntest_ge__scalar_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_ger (__main__.TestAutograd) ... ok\r\ntest_grad (__main__.TestAutograd) ... ok\r\ntest_grad_badcalls (__main__.TestAutograd) ... ok\r\ntest_grad_fn_badcalls (__main__.TestAutograd) ... ok\r\ntest_grad_nonleaf (__main__.TestAutograd) ... ok\r\ntest_grad_nonleaf_many_outputs (__main__.TestAutograd) ... ok\r\ntest_grad_nonleaf_register_hook (__main__.TestAutograd) ... ok\r\ntest_grad_unreachable (__main__.TestAutograd) ... ok\r\ntest_gradcheck_fail_when_no_differentiable_outputs_and_num_grad_not_zero (__main__.TestAutograd) ... ok\r\ntest_gradcheck_nondeterministic (__main__.TestAutograd) ... ok\r\ntest_gradcheck_single_input (__main__.TestAutograd) ... ok\r\ntest_gradcheck_sparse_input (__main__.TestAutograd) ... ok\r\ntest_gt_ (__main__.TestAutograd) ... ok\r\ntest_gt__broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_gt__pyscalar (__main__.TestAutograd) ... ok\r\ntest_gt__pyscalar_scalar (__main__.TestAutograd) ... ok\r\ntest_gt__scalar (__main__.TestAutograd) ... ok\r\ntest_gt__scalar_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_hessian_vector (__main__.TestAutograd) ... ok\r\ntest_hook_none (__main__.TestAutograd) ... ok\r\ntest_hooks (__main__.TestAutograd) ... ok\r\ntest_hooks_cpp (__main__.TestAutograd) ... ok\r\ntest_index_add_dim (__main__.TestAutograd) ... ok\r\ntest_index_add_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_index_add_scalar_all_dim (__main__.TestAutograd) ... ok\r\ntest_index_add_scalar_all_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_index_add_scalar_input_dim (__main__.TestAutograd) ... ok\r\ntest_index_add_scalar_input_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_index_backward_does_not_save_tensor (__main__.TestAutograd) ... ok\r\ntest_index_copy_dim (__main__.TestAutograd) ... ok\r\ntest_index_copy_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_index_copy_scalar_all_dim (__main__.TestAutograd) ... ok\r\ntest_index_copy_scalar_all_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_index_copy_scalar_input_dim (__main__.TestAutograd) ... ok\r\ntest_index_copy_scalar_input_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_index_fill_dim (__main__.TestAutograd) ... ok\r\ntest_index_fill_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_index_fill_scalar_both_dim (__main__.TestAutograd) ... ok\r\ntest_index_fill_scalar_both_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_index_fill_scalar_index_dim (__main__.TestAutograd) ... ok\r\ntest_index_fill_scalar_index_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_index_fill_scalar_input_dim (__main__.TestAutograd) ... ok\r\ntest_index_fill_scalar_input_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_index_fill_variable_dim (__main__.TestAutograd) ... ok\r\ntest_index_fill_variable_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_index_select_dim (__main__.TestAutograd) ... ok\r\ntest_index_select_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_index_select_scalar_dim (__main__.TestAutograd) ... ok\r\ntest_index_select_scalar_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_index_select_scalar_mixed_dim (__main__.TestAutograd) ... ok\r\ntest_index_select_scalar_mixed_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_indexing (__main__.TestAutograd) ... ok\r\ntest_indexing_duplicates (__main__.TestAutograd) ... ok\r\ntest_inplace (__main__.TestAutograd) ... ok\r\ntest_inplace_view_backprop_base (__main__.TestAutograd) ... ok\r\ntest_inplace_view_backprop_view (__main__.TestAutograd) ... ok\r\ntest_inplace_view_backprop_view_of_view (__main__.TestAutograd) ... ok\r\ntest_inplace_view_backward (__main__.TestAutograd) ... ok\r\ntest_inplace_view_gradcheck (__main__.TestAutograd) ... ok\r\ntest_inplace_view_makes_base_require_grad (__main__.TestAutograd) ... ok\r\ntest_inplace_view_modify_base (__main__.TestAutograd) ... ok\r\ntest_inplace_view_non_contig (__main__.TestAutograd) ... ok\r\ntest_inplace_view_of_view (__main__.TestAutograd) ... ok\r\ntest_inplace_view_python (__main__.TestAutograd) ... ok\r\ntest_inplace_view_saved_output (__main__.TestAutograd) ... ok\r\ntest_inplace_view_weak_grad_fn (__main__.TestAutograd) ... ok\r\ntest_invalid_gradients (__main__.TestAutograd) ... ok\r\ntest_inverse (__main__.TestAutograd) ... ok\r\ntest_inverse_batched (__main__.TestAutograd) ... ok\r\ntest_isolated_node (__main__.TestAutograd) ... ok\r\ntest_kthvalue (__main__.TestAutograd) ... ok\r\ntest_kthvalue_dim (__main__.TestAutograd) ... ok\r\ntest_kthvalue_dim_1d (__main__.TestAutograd) ... ok\r\ntest_kthvalue_dim_1d_neg0 (__main__.TestAutograd) ... ok\r\ntest_kthvalue_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_kthvalue_keepdim_dim (__main__.TestAutograd) ... ok\r\ntest_kthvalue_keepdim_dim_1d (__main__.TestAutograd) ... ok\r\ntest_kthvalue_keepdim_dim_1d_neg0 (__main__.TestAutograd) ... ok\r\ntest_kthvalue_keepdim_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_kthvalue_scalar (__main__.TestAutograd) ... ok\r\ntest_kthvalue_scalar_dim (__main__.TestAutograd) ... ok\r\ntest_kthvalue_scalar_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_kthvalue_scalar_keepdim_dim (__main__.TestAutograd) ... ok\r\ntest_kthvalue_scalar_keepdim_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_le_ (__main__.TestAutograd) ... ok\r\ntest_le__broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_le__pyscalar (__main__.TestAutograd) ... ok\r\ntest_le__pyscalar_scalar (__main__.TestAutograd) ... ok\r\ntest_le__scalar (__main__.TestAutograd) ... ok\r\ntest_le__scalar_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_leaf_assignment (__main__.TestAutograd) ... ok\r\ntest_legacy_function_deprecation_warning (__main__.TestAutograd) ... ok\r\ntest_legacy_function_none_grad (__main__.TestAutograd) ... /root/pytorch/torch/csrc/autograd/python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\r\n  ""(Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)"");\r\nok\r\ntest_lerp_broadcast_all (__main__.TestAutograd) ... ok\r\ntest_lerp_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_lerp_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_lerp_scalar (__main__.TestAutograd) ... ok\r\ntest_lerp_scalar_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_lerp_scalar_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_lerp_scalar_no_broadcast (__main__.TestAutograd) ... ok\r\ntest_lerp_tensor_weights (__main__.TestAutograd) ... ERROR\r\ntest_log (__main__.TestAutograd) ... ok\r\ntest_log10 (__main__.TestAutograd) ... ok\r\ntest_log10_scalar (__main__.TestAutograd) ... ok\r\ntest_log1p (__main__.TestAutograd) ... ok\r\ntest_log1p_scalar (__main__.TestAutograd) ... ok\r\ntest_log2 (__main__.TestAutograd) ... ok\r\ntest_log2_scalar (__main__.TestAutograd) ... ok\r\ntest_log_scalar (__main__.TestAutograd) ... ok\r\ntest_log_softmax_kwarg_dtype_would_break_jit_loader (__main__.TestAutograd) ... ok\r\ntest_logdet (__main__.TestAutograd) ... ok\r\ntest_logdet_1x1 (__main__.TestAutograd) ... ok\r\ntest_logdet_batched (__main__.TestAutograd) ... ok\r\ntest_logdet_batched_1x1 (__main__.TestAutograd) ... ok\r\ntest_logdet_batched_distinct_singular_values (__main__.TestAutograd) ... ok\r\ntest_logdet_batched_symmetric (__main__.TestAutograd) ... ok\r\ntest_logdet_batched_symmetric_pd (__main__.TestAutograd) ... ok\r\ntest_logdet_distinct_singular_values (__main__.TestAutograd) ... ok\r\ntest_logdet_symmetric (__main__.TestAutograd) ... ok\r\ntest_logdet_symmetric_pd (__main__.TestAutograd) ... ok\r\ntest_logsumexp (__main__.TestAutograd) ... ok\r\ntest_logsumexp_scalar (__main__.TestAutograd) ... ok\r\ntest_lt_ (__main__.TestAutograd) ... ok\r\ntest_lt__broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_lt__pyscalar (__main__.TestAutograd) ... ok\r\ntest_lt__pyscalar_scalar (__main__.TestAutograd) ... ok\r\ntest_lt__scalar (__main__.TestAutograd) ... ok\r\ntest_lt__scalar_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_mark_non_differentiable (__main__.TestAutograd) ... ok\r\ntest_mark_non_differentiable_mixed (__main__.TestAutograd) ... ok\r\ntest_mark_non_differentiable_none (__main__.TestAutograd) ... ok\r\ntest_masked_fill (__main__.TestAutograd) ... ok\r\ntest_masked_fill_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_masked_fill_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_masked_fill_scalar (__main__.TestAutograd) ... ok\r\ntest_masked_fill_scalar_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_masked_fill_scalar_variable (__main__.TestAutograd) ... ok\r\ntest_masked_fill_tensor (__main__.TestAutograd) ... ok\r\ntest_masked_scatter (__main__.TestAutograd) ... ok\r\ntest_masked_scatter_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_masked_scatter_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_masked_scatter_scalar (__main__.TestAutograd) ... ok\r\ntest_masked_scatter_scalar_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_masked_select (__main__.TestAutograd) ... ok\r\ntest_masked_select_broadcast_all (__main__.TestAutograd) ... ok\r\ntest_masked_select_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_masked_select_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_masked_select_scalar (__main__.TestAutograd) ... ok\r\ntest_masked_select_scalar_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_masked_select_scalar_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_matmul (__main__.TestAutograd) ... ok\r\ntest_matmul_1d_2d (__main__.TestAutograd) ... ok\r\ntest_matmul_1d_3d (__main__.TestAutograd) ... ok\r\ntest_matmul_1d_4d (__main__.TestAutograd) ... ok\r\ntest_matmul_2d_1d (__main__.TestAutograd) ... ok\r\ntest_matmul_2d_2d (__main__.TestAutograd) ... ok\r\ntest_matmul_2d_3d (__main__.TestAutograd) ... ok\r\ntest_matmul_3d_1d (__main__.TestAutograd) ... ok\r\ntest_matmul_3d_2d (__main__.TestAutograd) ... ok\r\ntest_matmul_4d_1d (__main__.TestAutograd) ... ok\r\ntest_matmul_4d_4d (__main__.TestAutograd) ... ok\r\ntest_matrix_power_n=-1 (__main__.TestAutograd) ... ok\r\ntest_matrix_power_n=-2 (__main__.TestAutograd) ... ok\r\ntest_matrix_power_n=-3 (__main__.TestAutograd) ... ok\r\ntest_matrix_power_n=0 (__main__.TestAutograd) ... ok\r\ntest_matrix_power_n=1 (__main__.TestAutograd) ... ok\r\ntest_matrix_power_n=2 (__main__.TestAutograd) ... ok\r\ntest_matrix_power_n=3 (__main__.TestAutograd) ... ok\r\ntest_max (__main__.TestAutograd) ... ok\r\ntest_max_dim (__main__.TestAutograd) ... ok\r\ntest_max_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_max_elementwise (__main__.TestAutograd) ... ok\r\ntest_max_elementwise_broadcast_all (__main__.TestAutograd) ... ok\r\ntest_max_elementwise_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_max_elementwise_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_max_keepdim_dim (__main__.TestAutograd) ... ok\r\ntest_max_keepdim_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_max_scalar (__main__.TestAutograd) ... ok\r\ntest_max_scalar_dim (__main__.TestAutograd) ... ok\r\ntest_max_scalar_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_max_scalar_elementwise (__main__.TestAutograd) ... ok\r\ntest_max_scalar_elementwise_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_max_scalar_elementwise_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_max_scalar_keepdim_dim (__main__.TestAutograd) ... ok\r\ntest_max_scalar_keepdim_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_mean (__main__.TestAutograd) ... ok\r\ntest_mean_dim (__main__.TestAutograd) ... ok\r\ntest_mean_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_mean_dtype (__main__.TestAutograd) ... ok\r\ntest_mean_keepdim_dim (__main__.TestAutograd) ... ok\r\ntest_mean_keepdim_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_mean_scalar (__main__.TestAutograd) ... ok\r\ntest_mean_scalar_dim (__main__.TestAutograd) ... ok\r\ntest_mean_scalar_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_mean_scalar_keepdim_dim (__main__.TestAutograd) ... ok\r\ntest_mean_scalar_keepdim_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_median (__main__.TestAutograd) ... ok\r\ntest_median_dim (__main__.TestAutograd) ... ok\r\ntest_median_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_median_keepdim_dim (__main__.TestAutograd) ... ok\r\ntest_median_keepdim_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_median_scalar (__main__.TestAutograd) ... ok\r\ntest_median_scalar_dim (__main__.TestAutograd) ... ok\r\ntest_median_scalar_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_median_scalar_keepdim_dim (__main__.TestAutograd) ... ok\r\ntest_median_scalar_keepdim_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_min (__main__.TestAutograd) ... ok\r\ntest_min_dim (__main__.TestAutograd) ... ok\r\ntest_min_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_min_elementwise (__main__.TestAutograd) ... ok\r\ntest_min_elementwise_broadcast_all (__main__.TestAutograd) ... ok\r\ntest_min_elementwise_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_min_elementwise_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_min_keepdim_dim (__main__.TestAutograd) ... ok\r\ntest_min_keepdim_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_min_scalar (__main__.TestAutograd) ... ok\r\ntest_min_scalar_dim (__main__.TestAutograd) ... ok\r\ntest_min_scalar_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_min_scalar_elementwise (__main__.TestAutograd) ... ok\r\ntest_min_scalar_elementwise_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_min_scalar_elementwise_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_min_scalar_keepdim_dim (__main__.TestAutograd) ... ok\r\ntest_min_scalar_keepdim_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_mm (__main__.TestAutograd) ... ok\r\ntest_mode (__main__.TestAutograd) ... ok\r\ntest_mode_dim (__main__.TestAutograd) ... ok\r\ntest_mode_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_mode_keepdim_dim (__main__.TestAutograd) ... ok\r\ntest_mode_keepdim_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_mode_scalar (__main__.TestAutograd) ... ok\r\ntest_mode_scalar_dim (__main__.TestAutograd) ... ok\r\ntest_mode_scalar_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_mode_scalar_keepdim_dim (__main__.TestAutograd) ... ok\r\ntest_mode_scalar_keepdim_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_mul (__main__.TestAutograd) ... ok\r\ntest_mul_broadcast_all (__main__.TestAutograd) ... ok\r\ntest_mul_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_mul_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_mul_constant (__main__.TestAutograd) ... ok\r\ntest_mul_out (__main__.TestAutograd) ... ok\r\ntest_mul_out_result_requires_grad (__main__.TestAutograd) ... ok\r\ntest_mul_scalar (__main__.TestAutograd) ... ok\r\ntest_mul_scalar_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_mul_scalar_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_mul_scalar_constant (__main__.TestAutograd) ... ok\r\ntest_multi_backward (__main__.TestAutograd) ... ok\r\ntest_multi_backward_no_grad (__main__.TestAutograd) ... ok\r\ntest_mv (__main__.TestAutograd) ... ok\r\ntest_mvlgamma_p=1 (__main__.TestAutograd) ... ok\r\ntest_mvlgamma_p=2 (__main__.TestAutograd) ... ok\r\ntest_mvlgamma_p=3 (__main__.TestAutograd) ... ok\r\ntest_mvlgamma_p=5 (__main__.TestAutograd) ... ok\r\ntest_narrow_dim (__main__.TestAutograd) ... ok\r\ntest_narrow_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_narrow_empty_dim (__main__.TestAutograd) ... ok\r\ntest_narrow_empty_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_naughty_anomaly_access (__main__.TestAutograd) ... expected failure\r\ntest_naughty_autograd_function_stashing_ctx (__main__.TestAutograd) ... ok\r\ntest_naughty_legacy_function_backward_before_forward (__main__.TestAutograd) ... ok\r\ntest_naughty_legacy_function_early_access (__main__.TestAutograd) ... ok\r\ntest_naughty_legacy_variable_grad_fn (__main__.TestAutograd) ... ok\r\ntest_ne_ (__main__.TestAutograd) ... ok\r\ntest_ne__broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_ne__pyscalar (__main__.TestAutograd) ... ok\r\ntest_ne__pyscalar_scalar (__main__.TestAutograd) ... ok\r\ntest_ne__scalar (__main__.TestAutograd) ... ok\r\ntest_ne__scalar_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_next_functions (__main__.TestAutograd) ... ok\r\ntest_no_grad (__main__.TestAutograd) ... ok\r\ntest_no_grad_assignment (__main__.TestAutograd) ... ok\r\ntest_no_grad_copy (__main__.TestAutograd) ... ok\r\ntest_no_grad_input (__main__.TestAutograd) ... ok\r\ntest_no_grad_modifies_version (__main__.TestAutograd) ... ok\r\ntest_no_grad_python_function (__main__.TestAutograd)\r\nPython Functions should respect grad mode. ... ok\r\ntest_no_requires_grad_inplace (__main__.TestAutograd) ... ok\r\ntest_no_unnecessary_save (__main__.TestAutograd) ... ok\r\ntest_norm_-inf (__main__.TestAutograd) ... ok\r\ntest_norm_0 (__main__.TestAutograd) ... ok\r\ntest_norm_0_2_dim (__main__.TestAutograd) ... ok\r\ntest_norm_0_2_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_norm_0_5 (__main__.TestAutograd) ... ok\r\ntest_norm_1 (__main__.TestAutograd) ... ok\r\ntest_norm_1_2_dim (__main__.TestAutograd) ... ok\r\ntest_norm_1_2_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_norm_1_5_default (__main__.TestAutograd) ... ok\r\ntest_norm_1_5_dim (__main__.TestAutograd) ... ok\r\ntest_norm_1_5_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_norm_2 (__main__.TestAutograd) ... ok\r\ntest_norm_2_2_dim (__main__.TestAutograd) ... ok\r\ntest_norm_2_2_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_norm_2_dim (__main__.TestAutograd) ... ok\r\ntest_norm_2_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_norm_2_dim_scalar (__main__.TestAutograd) ... ok\r\ntest_norm_2_dim_scalar_neg0 (__main__.TestAutograd) ... ok\r\ntest_norm_3 (__main__.TestAutograd) ... ok\r\ntest_norm_3_2_dim (__main__.TestAutograd) ... ok\r\ntest_norm_3_2_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_norm_3_dim (__main__.TestAutograd) ... ok\r\ntest_norm_3_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_norm_3_dim_scalar (__main__.TestAutograd) ... ok\r\ntest_norm_3_dim_scalar_neg0 (__main__.TestAutograd) ... ok\r\ntest_norm_default (__main__.TestAutograd) ... ok\r\ntest_norm_fro (__main__.TestAutograd) ... ok\r\ntest_norm_fro_default (__main__.TestAutograd) ... ok\r\ntest_norm_inf (__main__.TestAutograd) ... ok\r\ntest_norm_inf_2_dim (__main__.TestAutograd) ... ok\r\ntest_norm_keepdim_1_5_dim (__main__.TestAutograd) ... ok\r\ntest_norm_keepdim_1_5_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_norm_keepdim_2_dim (__main__.TestAutograd) ... ok\r\ntest_norm_keepdim_2_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_norm_keepdim_2_dim_scalar (__main__.TestAutograd) ... ok\r\ntest_norm_keepdim_2_dim_scalar_neg0 (__main__.TestAutograd) ... ok\r\ntest_norm_keepdim_3_dim (__main__.TestAutograd) ... ok\r\ntest_norm_keepdim_3_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_norm_keepdim_3_dim_scalar (__main__.TestAutograd) ... ok\r\ntest_norm_keepdim_3_dim_scalar_neg0 (__main__.TestAutograd) ... ok\r\ntest_norm_neg_0_5 (__main__.TestAutograd) ... ok\r\ntest_norm_neg_1 (__main__.TestAutograd) ... ok\r\ntest_norm_neg_1_2_dim (__main__.TestAutograd) ... ok\r\ntest_norm_neg_1_2_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_norm_neg_1_5 (__main__.TestAutograd) ... ok\r\ntest_norm_neg_2 (__main__.TestAutograd) ... ok\r\ntest_norm_neg_2_2_dim (__main__.TestAutograd) ... ok\r\ntest_norm_neg_2_2_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_norm_nuc (__main__.TestAutograd) ... ok\r\ntest_norm_nuc_batched (__main__.TestAutograd) ... ok\r\ntest_norm_subgradient (__main__.TestAutograd) ... ok\r\ntest_numpy_requires_grad (__main__.TestAutograd) ... ok\r\ntest_once_differentiable (__main__.TestAutograd) ... ok\r\ntest_permute (__main__.TestAutograd) ... ok\r\ntest_permute_neg_dim (__main__.TestAutograd) ... ok\r\ntest_permute_scalar (__main__.TestAutograd) ... ok\r\ntest_pickle (__main__.TestAutograd) ... ok\r\ntest_pinverse (__main__.TestAutograd) ... ok\r\ntest_pow (__main__.TestAutograd) ... ok\r\ntest_pow_broadcast_all (__main__.TestAutograd) ... ok\r\ntest_pow_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_pow_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_pow_constant (__main__.TestAutograd) ... ok\r\ntest_pow_scalar (__main__.TestAutograd) ... ok\r\ntest_pow_scalar_base (__main__.TestAutograd) ... ok\r\ntest_pow_scalar_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_pow_scalar_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_pow_scalar_constant (__main__.TestAutograd) ... ok\r\ntest_pow_zero_tensor_gradient (__main__.TestAutograd) ... ok\r\ntest_prod (__main__.TestAutograd) ... ok\r\ntest_prod_dim (__main__.TestAutograd) ... ok\r\ntest_prod_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_prod_keepdim_dim (__main__.TestAutograd) ... ok\r\ntest_prod_keepdim_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_prod_keepdim_zeros_dims0 (__main__.TestAutograd) ... ok\r\ntest_prod_keepdim_zeros_dims0_neg0 (__main__.TestAutograd) ... ok\r\ntest_prod_keepdim_zeros_dims1 (__main__.TestAutograd) ... ok\r\ntest_prod_keepdim_zeros_dims1_neg0 (__main__.TestAutograd) ... ok\r\ntest_prod_keepdim_zeros_dims2 (__main__.TestAutograd) ... ok\r\ntest_prod_keepdim_zeros_dims2_neg0 (__main__.TestAutograd) ... ok\r\ntest_prod_scalar (__main__.TestAutograd) ... ok\r\ntest_prod_scalar_dim (__main__.TestAutograd) ... ok\r\ntest_prod_scalar_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_prod_scalar_dim_zero (__main__.TestAutograd) ... ok\r\ntest_prod_scalar_dim_zero_neg0 (__main__.TestAutograd) ... ok\r\ntest_prod_scalar_keepdim_dim (__main__.TestAutograd) ... ok\r\ntest_prod_scalar_keepdim_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_prod_scalar_keepdim_dim_zero (__main__.TestAutograd) ... ok\r\ntest_prod_scalar_keepdim_dim_zero_neg0 (__main__.TestAutograd) ... ok\r\ntest_prod_scalar_zero (__main__.TestAutograd) ... ok\r\ntest_prod_single_zero (__main__.TestAutograd) ... ok\r\ntest_prod_zerodims0 (__main__.TestAutograd) ... ok\r\ntest_prod_zerodims1 (__main__.TestAutograd) ... ok\r\ntest_prod_zerodims2 (__main__.TestAutograd) ... ok\r\ntest_prod_zeros_dims0 (__main__.TestAutograd) ... ok\r\ntest_prod_zeros_dims0_neg0 (__main__.TestAutograd) ... ok\r\ntest_prod_zeros_dims1 (__main__.TestAutograd) ... ok\r\ntest_prod_zeros_dims1_neg0 (__main__.TestAutograd) ... ok\r\ntest_prod_zeros_dims2 (__main__.TestAutograd) ... ok\r\ntest_prod_zeros_dims2_neg0 (__main__.TestAutograd) ... ok\r\ntest_profiler (__main__.TestAutograd) ... ok\r\ntest_profiler_aggregation_fake (__main__.TestAutograd) ... ok\r\ntest_profiler_aggregation_lstm (__main__.TestAutograd) ... ok\r\ntest_profiler_shapes (__main__.TestAutograd) ... ok\r\ntest_put (__main__.TestAutograd) ... ok\r\ntest_put_accumulate (__main__.TestAutograd) ... ok\r\ntest_qr_square_batched (__main__.TestAutograd) ... ok\r\ntest_qr_square_many_batched (__main__.TestAutograd) ... ok\r\ntest_qr_square_single (__main__.TestAutograd) ... ok\r\ntest_qr_tall_batched (__main__.TestAutograd) ... ok\r\ntest_qr_tall_many_batched (__main__.TestAutograd) ... ok\r\ntest_qr_tall_single (__main__.TestAutograd) ... ok\r\ntest_reciprocal (__main__.TestAutograd) ... ok\r\ntest_reciprocal_scalar (__main__.TestAutograd) ... ok\r\ntest_record_function (__main__.TestAutograd) ... FAIL\r\ntest_reduce_dtype (__main__.TestAutograd) ... ok\r\ntest_reentrant (__main__.TestAutograd) ... ok\r\ntest_reentrant_priority (__main__.TestAutograd) ... ok\r\ntest_remainder (__main__.TestAutograd) ... ok\r\ntest_remainder_scalar (__main__.TestAutograd) ... ok\r\ntest_remainder_scalar_tensor (__main__.TestAutograd) ... ok\r\ntest_remainder_scalar_tensor_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_remainder_tensor (__main__.TestAutograd) ... ok\r\ntest_remainder_tensor_broadcast_all (__main__.TestAutograd) ... ok\r\ntest_remainder_tensor_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_renorm_dim (__main__.TestAutograd) ... ok\r\ntest_renorm_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_renorm_norm_1 (__main__.TestAutograd) ... ok\r\ntest_renorm_norm_inf (__main__.TestAutograd) ... ok\r\ntest_repeat (__main__.TestAutograd) ... ok\r\ntest_repeat_scalar (__main__.TestAutograd) ... ok\r\ntest_repeat_single_number (__main__.TestAutograd) ... ok\r\ntest_repeat_unsqueeze (__main__.TestAutograd) ... ok\r\ntest_requires_grad (__main__.TestAutograd) ... ok\r\ntest_requires_grad_ (__main__.TestAutograd) ... ok\r\ntest_requires_grad_inplace (__main__.TestAutograd) ... ok\r\ntest_reshape (__main__.TestAutograd) ... ok\r\ntest_reshape_1d (__main__.TestAutograd) ... ok\r\ntest_reshape_as (__main__.TestAutograd) ... ok\r\ntest_reshape_as_scalar (__main__.TestAutograd) ... ok\r\ntest_reshape_as_scalar_to_dims (__main__.TestAutograd) ... ok\r\ntest_reshape_scalar_to_1d (__main__.TestAutograd) ... ok\r\ntest_reshape_scalar_to_scalar (__main__.TestAutograd) ... ok\r\ntest_reshape_size (__main__.TestAutograd) ... ok\r\ntest_resize (__main__.TestAutograd) ... ok\r\ntest_resize__fewer_dims (__main__.TestAutograd) ... ok\r\ntest_resize__scalar (__main__.TestAutograd) ... ok\r\ntest_resize__scalar_to_dims (__main__.TestAutograd) ... ok\r\ntest_resize_as_ (__main__.TestAutograd) ... ok\r\ntest_resize_as__scalar (__main__.TestAutograd) ... ok\r\ntest_resize_as__scalar_to_dims (__main__.TestAutograd) ... ok\r\ntest_retain_grad (__main__.TestAutograd) ... ok\r\ntest_retain_grad_cycle (__main__.TestAutograd) ... ok\r\ntest_return_duplicate (__main__.TestAutograd) ... ok\r\ntest_return_duplicate_inplace (__main__.TestAutograd) ... ok\r\ntest_return_leaf (__main__.TestAutograd) ... ok\r\ntest_return_leaf_inplace (__main__.TestAutograd) ... ok\r\ntest_roll_d0 (__main__.TestAutograd) ... ok\r\ntest_roll_d02 (__main__.TestAutograd) ... ok\r\ntest_roll_d12 (__main__.TestAutograd) ... ok\r\ntest_roll_d20 (__main__.TestAutograd) ... ok\r\ntest_roll_flattened (__main__.TestAutograd) ... ok\r\ntest_roll_loop_shift (__main__.TestAutograd) ... ok\r\ntest_roll_neg_shift (__main__.TestAutograd) ... ok\r\ntest_roll_three_dims (__main__.TestAutograd) ... ok\r\ntest_rot90_default (__main__.TestAutograd) ... ok\r\ntest_rot90_k1_d01 (__main__.TestAutograd) ... ok\r\ntest_rot90_k1_d12 (__main__.TestAutograd) ... ok\r\ntest_rot90_k1_neg_d (__main__.TestAutograd) ... ok\r\ntest_round (__main__.TestAutograd) ... ok\r\ntest_round_scalar (__main__.TestAutograd) ... ok\r\ntest_rsqrt (__main__.TestAutograd) ... ok\r\ntest_rsqrt_scalar (__main__.TestAutograd) ... ok\r\ntest_save_none_for_backward (__main__.TestAutograd) ... ok\r\ntest_save_output_nr (__main__.TestAutograd) ... ok\r\ntest_saved_variables_deprecated (__main__.TestAutograd) ... ok\r\ntest_scatter_add_dim0 (__main__.TestAutograd) ... ok\r\ntest_scatter_add_dim0_neg0 (__main__.TestAutograd) ... ok\r\ntest_scatter_add_dim1 (__main__.TestAutograd) ... ok\r\ntest_scatter_add_dim1_neg0 (__main__.TestAutograd) ... ok\r\ntest_scatter_add_scalar_all_dim0 (__main__.TestAutograd) ... ok\r\ntest_scatter_add_scalar_all_dim0_neg0 (__main__.TestAutograd) ... ok\r\ntest_scatter_dim0 (__main__.TestAutograd) ... ok\r\ntest_scatter_dim0_neg0 (__main__.TestAutograd) ... ok\r\ntest_scatter_dim1 (__main__.TestAutograd) ... ok\r\ntest_scatter_dim1_neg0 (__main__.TestAutograd) ... ok\r\ntest_scatter_scalar_all_dim0 (__main__.TestAutograd) ... ok\r\ntest_scatter_scalar_all_dim0_neg0 (__main__.TestAutograd) ... ok\r\ntest_scatter_scalartensor_all_dim0 (__main__.TestAutograd) ... ok\r\ntest_scatter_scalartensor_all_dim0_neg0 (__main__.TestAutograd) ... ok\r\ntest_select_1d (__main__.TestAutograd) ... ok\r\ntest_select_dim (__main__.TestAutograd) ... ok\r\ntest_select_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_select_sum (__main__.TestAutograd) ... ok\r\ntest_select_wrap_dim (__main__.TestAutograd) ... ok\r\ntest_select_wrap_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_set_data_preserve_pyobj (__main__.TestAutograd) ... ok\r\ntest_set_data_tensorimpl_type (__main__.TestAutograd) ... ok\r\ntest_set_grad_enabled (__main__.TestAutograd) ... ok\r\ntest_setitem (__main__.TestAutograd) ... ok\r\ntest_setitem_mask (__main__.TestAutograd) ... ok\r\ntest_shape (__main__.TestAutograd) ... ok\r\ntest_sharded_grad (__main__.TestAutograd) ... ok\r\ntest_sigmoid (__main__.TestAutograd) ... ok\r\ntest_sigmoid_scalar (__main__.TestAutograd) ... ok\r\ntest_sign (__main__.TestAutograd) ... ok\r\ntest_sign_scalar (__main__.TestAutograd) ... ok\r\ntest_sin (__main__.TestAutograd) ... ok\r\ntest_sin_scalar (__main__.TestAutograd) ... ok\r\ntest_sinh (__main__.TestAutograd) ... ok\r\ntest_sinh_scalar (__main__.TestAutograd) ... ok\r\ntest_slogdet_1x1_neg_det (__main__.TestAutograd) ... ok\r\ntest_slogdet_1x1_pos_det (__main__.TestAutograd) ... ok\r\ntest_slogdet_batched_1x1_neg_det (__main__.TestAutograd) ... ok\r\ntest_slogdet_batched_distinct_singular_values (__main__.TestAutograd) ... ok\r\ntest_slogdet_batched_pos_det (__main__.TestAutograd) ... ok\r\ntest_slogdet_batched_symmetric (__main__.TestAutograd) ... ok\r\ntest_slogdet_batched_symmetric_pd (__main__.TestAutograd) ... ok\r\ntest_slogdet_distinct_singular_values (__main__.TestAutograd) ... ok\r\ntest_slogdet_neg_det (__main__.TestAutograd) ... ok\r\ntest_slogdet_pos_det (__main__.TestAutograd) ... ok\r\ntest_slogdet_sign (__main__.TestAutograd) ... ok\r\ntest_slogdet_symmetric (__main__.TestAutograd) ... ok\r\ntest_slogdet_symmetric_pd (__main__.TestAutograd) ... ok\r\ntest_solve (__main__.TestAutograd) ... ok\r\ntest_solve_batched (__main__.TestAutograd) ... ok\r\ntest_solve_batched_broadcast_A (__main__.TestAutograd) ... ok\r\ntest_solve_batched_broadcast_b (__main__.TestAutograd) ... ok\r\ntest_solve_batched_dims (__main__.TestAutograd) ... ok\r\ntest_sort (__main__.TestAutograd) ... ok\r\ntest_sort_dim (__main__.TestAutograd) ... ok\r\ntest_sort_dim_desc (__main__.TestAutograd) ... ok\r\ntest_sort_dim_desc_scalar (__main__.TestAutograd) ... ok\r\ntest_sort_dim_scalar (__main__.TestAutograd) ... ok\r\ntest_sort_scalar (__main__.TestAutograd) ... ok\r\ntest_sparse_backward (__main__.TestAutograd) ... ok\r\ntest_sparse_gather_both_scalar (__main__.TestAutograd) ... ok\r\ntest_sparse_gather_dim0 (__main__.TestAutograd) ... ok\r\ntest_sparse_gather_dim1 (__main__.TestAutograd) ... ok\r\ntest_sparse_gather_dim_neg (__main__.TestAutograd) ... ok\r\ntest_sparse_gather_ind_scalar (__main__.TestAutograd) ... ok\r\ntest_sparse_gather_x_scalar (__main__.TestAutograd) ... ok\r\ntest_sparse_mm_backward (__main__.TestAutograd) ... ok\r\ntest_split (__main__.TestAutograd) ... ok\r\ntest_split_dim (__main__.TestAutograd) ... ok\r\ntest_split_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_split_size_list (__main__.TestAutograd) ... ok\r\ntest_split_size_list_dim (__main__.TestAutograd) ... ok\r\ntest_split_size_list_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_split_with_sizes (__main__.TestAutograd) ... ok\r\ntest_split_with_sizes_dim (__main__.TestAutograd) ... ok\r\ntest_split_with_sizes_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_split_with_sizes_size_0 (__main__.TestAutograd) ... ok\r\ntest_sqrt (__main__.TestAutograd) ... ok\r\ntest_sqrt_scalar (__main__.TestAutograd) ... ok\r\ntest_squeeze (__main__.TestAutograd) ... ok\r\ntest_squeeze_1_dim (__main__.TestAutograd) ... ok\r\ntest_squeeze_1_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_squeeze_input_sizes_are_ones (__main__.TestAutograd) ... ok\r\ntest_squeeze_not_1_dim (__main__.TestAutograd) ... ok\r\ntest_squeeze_not_1_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_squeeze_scalar (__main__.TestAutograd) ... ok\r\ntest_squeeze_scalar_neg0 (__main__.TestAutograd) ... ok\r\ntest_stack (__main__.TestAutograd) ... ok\r\ntest_std (__main__.TestAutograd) ... ok\r\ntest_std_dim (__main__.TestAutograd) ... ok\r\ntest_std_dim_1d (__main__.TestAutograd) ... ok\r\ntest_std_dim_1d_neg0 (__main__.TestAutograd) ... ok\r\ntest_std_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_std_keepdim_dim (__main__.TestAutograd) ... ok\r\ntest_std_keepdim_dim_1d (__main__.TestAutograd) ... ok\r\ntest_std_keepdim_dim_1d_neg0 (__main__.TestAutograd) ... ok\r\ntest_std_keepdim_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_std_mean (__main__.TestAutograd) ... ok\r\ntest_std_mean_dim (__main__.TestAutograd) ... ok\r\ntest_std_mean_dim_1d (__main__.TestAutograd) ... ok\r\ntest_std_mean_keepdim_dim (__main__.TestAutograd) ... ok\r\ntest_std_mean_keepdim_dim_1d (__main__.TestAutograd) ... ok\r\ntest_sub (__main__.TestAutograd) ... ok\r\ntest_sub_broadcast_all (__main__.TestAutograd) ... ok\r\ntest_sub_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_sub_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_sub_constant (__main__.TestAutograd) ... ok\r\ntest_sub_scalar_broadcast_lhs (__main__.TestAutograd) ... ok\r\ntest_sub_scalar_broadcast_rhs (__main__.TestAutograd) ... ok\r\ntest_sub_scalar_constant (__main__.TestAutograd) ... ok\r\ntest_sum (__main__.TestAutograd) ... ok\r\ntest_sum_dim (__main__.TestAutograd) ... ok\r\ntest_sum_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_sum_keepdim_dim (__main__.TestAutograd) ... ok\r\ntest_sum_keepdim_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_sum_multi_dim (__main__.TestAutograd) ... ok\r\ntest_sum_multi_dim_keepdim (__main__.TestAutograd) ... ok\r\ntest_sum_scalar (__main__.TestAutograd) ... ok\r\ntest_sum_scalar_dim (__main__.TestAutograd) ... ok\r\ntest_sum_scalar_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_sum_scalar_keepdim_dim (__main__.TestAutograd) ... ok\r\ntest_sum_scalar_keepdim_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_sum_to_with_empty_dim_grad (__main__.TestAutograd) ... ok\r\ntest_svd (__main__.TestAutograd) ... ok\r\ntest_svd_batched (__main__.TestAutograd) ... ok\r\ntest_svd_large (__main__.TestAutograd) ... ok\r\ntest_svd_no_singularvectors (__main__.TestAutograd) ... ok\r\ntest_svd_tall (__main__.TestAutograd) ... ok\r\ntest_svd_tall_all (__main__.TestAutograd) ... ok\r\ntest_svd_tall_all_batched (__main__.TestAutograd) ... ok\r\ntest_svd_tall_batched (__main__.TestAutograd) ... ok\r\ntest_svd_wide (__main__.TestAutograd) ... ok\r\ntest_svd_wide_all (__main__.TestAutograd) ... ok\r\ntest_svd_wide_all_batched (__main__.TestAutograd) ... ok\r\ntest_svd_wide_batched (__main__.TestAutograd) ... ok\r\ntest_symeig (__main__.TestAutograd) ... ok\r\ntest_symeig_no_eigenvectors (__main__.TestAutograd) ... ok\r\ntest_t (__main__.TestAutograd) ... ok\r\ntest_take (__main__.TestAutograd) ... ok\r\ntest_take_scalar_both (__main__.TestAutograd) ... ok\r\ntest_take_scalar_data (__main__.TestAutograd) ... ok\r\ntest_take_scalar_index (__main__.TestAutograd) ... ok\r\ntest_tan (__main__.TestAutograd) ... ok\r\ntest_tanh (__main__.TestAutograd) ... ok\r\ntest_tanh_scalar (__main__.TestAutograd) ... ok\r\ntest_thread_shutdown (__main__.TestAutograd) ... ok\r\ntest_to_sparse (__main__.TestAutograd) ... ok\r\ntest_too_many_grads (__main__.TestAutograd) ... ok\r\ntest_topk (__main__.TestAutograd) ... ok\r\ntest_topk_dim (__main__.TestAutograd) ... ok\r\ntest_topk_dim_desc (__main__.TestAutograd) ... ok\r\ntest_topk_dim_desc_neg0 (__main__.TestAutograd) ... ok\r\ntest_topk_dim_desc_scalar (__main__.TestAutograd) ... ok\r\ntest_topk_dim_desc_scalar_neg0 (__main__.TestAutograd) ... ok\r\ntest_topk_dim_desc_sort (__main__.TestAutograd) ... ok\r\ntest_topk_dim_desc_sort_neg0 (__main__.TestAutograd) ... ok\r\ntest_topk_dim_desc_sort_scalar (__main__.TestAutograd) ... ok\r\ntest_topk_dim_desc_sort_scalar_neg0 (__main__.TestAutograd) ... ok\r\ntest_topk_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_topk_dim_scalar (__main__.TestAutograd) ... ok\r\ntest_topk_dim_scalar_neg0 (__main__.TestAutograd) ... ok\r\ntest_topk_scalar (__main__.TestAutograd) ... ok\r\ntest_trace (__main__.TestAutograd) ... ok\r\ntest_transpose_1d (__main__.TestAutograd) ... ok\r\ntest_transpose_2d (__main__.TestAutograd) ... ok\r\ntest_transpose_3d (__main__.TestAutograd) ... ok\r\ntest_transpose_dim (__main__.TestAutograd) ... ok\r\ntest_transpose_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_transpose_dim_neg0_neg1 (__main__.TestAutograd) ... ok\r\ntest_transpose_dim_neg1 (__main__.TestAutograd) ... ok\r\ntest_transpose_scalar (__main__.TestAutograd) ... ok\r\ntest_trapz (__main__.TestAutograd) ... ok\r\ntest_triangular_solve (__main__.TestAutograd) ... ok\r\ntest_tril (__main__.TestAutograd) ... ok\r\ntest_tril_batched (__main__.TestAutograd) ... ok\r\ntest_tril_batched_idx (__main__.TestAutograd) ... ok\r\ntest_tril_idx (__main__.TestAutograd) ... ok\r\ntest_tril_more_batched (__main__.TestAutograd) ... ok\r\ntest_triu (__main__.TestAutograd) ... ok\r\ntest_triu_batched (__main__.TestAutograd) ... ok\r\ntest_triu_batched_idx (__main__.TestAutograd) ... ok\r\ntest_triu_idx (__main__.TestAutograd) ... ok\r\ntest_triu_more_batched (__main__.TestAutograd) ... ok\r\ntest_trunc (__main__.TestAutograd) ... ok\r\ntest_trunc_scalar (__main__.TestAutograd) ... ok\r\ntest_type_conversions (__main__.TestAutograd) ... ok\r\ntest_unbind (__main__.TestAutograd) ... ok\r\ntest_unfold (__main__.TestAutograd) ... ok\r\ntest_unfold_lastdim (__main__.TestAutograd) ... ok\r\ntest_unfold_lastdim_neg0 (__main__.TestAutograd) ... ok\r\ntest_unfold_neg0 (__main__.TestAutograd) ... ok\r\ntest_unfold_scalar (__main__.TestAutograd) ... ok\r\ntest_unfold_scalar_neg0 (__main__.TestAutograd) ... ok\r\ntest_unsqueeze_first (__main__.TestAutograd) ... ok\r\ntest_unsqueeze_first_neg0 (__main__.TestAutograd) ... ok\r\ntest_unsqueeze_last (__main__.TestAutograd) ... ok\r\ntest_unsqueeze_last_neg0 (__main__.TestAutograd) ... ok\r\ntest_unsqueeze_middle (__main__.TestAutograd) ... ok\r\ntest_unsqueeze_middle_neg0 (__main__.TestAutograd) ... ok\r\ntest_unsqueeze_scalar (__main__.TestAutograd) ... ok\r\ntest_unsqueeze_scalar_neg0 (__main__.TestAutograd) ... ok\r\ntest_unused_output (__main__.TestAutograd) ... ok\r\ntest_var (__main__.TestAutograd) ... ok\r\ntest_var_dim (__main__.TestAutograd) ... ok\r\ntest_var_dim_1d (__main__.TestAutograd) ... ok\r\ntest_var_dim_1d_neg0 (__main__.TestAutograd) ... ok\r\ntest_var_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_var_keepdim_dim (__main__.TestAutograd) ... ok\r\ntest_var_keepdim_dim_1d (__main__.TestAutograd) ... ok\r\ntest_var_keepdim_dim_1d_neg0 (__main__.TestAutograd) ... ok\r\ntest_var_keepdim_dim_neg0 (__main__.TestAutograd) ... ok\r\ntest_var_mean (__main__.TestAutograd) ... ok\r\ntest_var_mean_differentiable (__main__.TestAutograd) ... ok\r\ntest_var_mean_dim (__main__.TestAutograd) ... ok\r\ntest_var_mean_dim_1d (__main__.TestAutograd) ... ok\r\ntest_var_mean_keepdim_dim (__main__.TestAutograd) ... ok\r\ntest_var_mean_keepdim_dim_1d (__main__.TestAutograd) ... ok\r\ntest_variable_traverse (__main__.TestAutograd) ... ok\r\ntest_version_counter (__main__.TestAutograd) ... ok\r\ntest_view (__main__.TestAutograd) ... ok\r\ntest_view_1d (__main__.TestAutograd) ... ok\r\ntest_view_as (__main__.TestAutograd) ... ok\r\ntest_view_as_scalar (__main__.TestAutograd) ... ok\r\ntest_view_as_scalar_to_dims (__main__.TestAutograd) ... ok\r\ntest_view_scalar_to_1d (__main__.TestAutograd) ... ok\r\ntest_view_scalar_to_scalar (__main__.TestAutograd) ... ok\r\ntest_view_size (__main__.TestAutograd) ... ok\r\ntest_volatile_deprecated (__main__.TestAutograd) ... ok\r\ntest_where (__main__.TestAutograd) ... ok\r\ntest_where_broadcast_all (__main__.TestAutograd) ... ok\r\ntest_where_scalar (__main__.TestAutograd) ... ok\r\ntest_where_scalar_broadcast_mask (__main__.TestAutograd) ... ok\r\ntest_where_scalar_broadcast_non_mask (__main__.TestAutograd) ... ok\r\ntest_zero_ (__main__.TestAutograd) ... ok\r\ntest_zero__scalar (__main__.TestAutograd) ... ok\r\ntest_GRU_grad_and_gradgrad_cpu (__main__.TestAutogradDeviceTypeCPU) ... ok\r\ntest_LSTM_grad_and_gradgrad_cpu (__main__.TestAutogradDeviceTypeCPU) ... ok\r\ntest_advanced_indexing_backwards_large_cpu (__main__.TestAutogradDeviceTypeCPU) ... skipped \'Only runs on cuda\'\r\ntest_backward_device_cpu (__main__.TestAutogradDeviceTypeCPU) ... skipped \'fewer than 2 devices detected\'\r\ntest_cdist_cpu (__main__.TestAutogradDeviceTypeCPU) ... ok\r\ntest_cross_device_reentrant_autograd_cpu (__main__.TestAutogradDeviceTypeCPU) ... skipped \'Only runs on cuda\'\r\ntest_ctc_loss_cpu (__main__.TestAutogradDeviceTypeCPU) ... ok\r\ntest_ctc_loss_cudnn_cpu (__main__.TestAutogradDeviceTypeCPU) ... skipped \'Only runs on cuda\'\r\ntest_free_unneeded_tensor_cpu (__main__.TestAutogradDeviceTypeCPU) ... skipped \'Only runs on cuda\'\r\ntest_grad_assignment_cpu (__main__.TestAutogradDeviceTypeCPU) ... ok\r\ntest_inputbuffer_add_multidevice_cpu (__main__.TestAutogradDeviceTypeCPU) ... skipped \'fewer than 2 devices detected\'\r\ntest_lstmcell_backward_only_one_output_grad_cpu (__main__.TestAutogradDeviceTypeCPU) ... skipped \'Only runs on cuda\'\r\ntest_pdist_large_cpu (__main__.TestAutogradDeviceTypeCPU) ... ok\r\ntest_pin_memory_cpu (__main__.TestAutogradDeviceTypeCPU) ... skipped \'Only runs on cuda\'\r\ntest_profiler_emit_nvtx_cpu (__main__.TestAutogradDeviceTypeCPU) ... skipped \'Only runs on cuda\'\r\ntest_pyscalar_conversions_cpu (__main__.TestAutogradDeviceTypeCPU) ... ok\r\ntest_requires_grad_factory_cpu_float32 (__main__.TestAutogradDeviceTypeCPU) ... ok\r\ntest_requires_grad_factory_cpu_float64 (__main__.TestAutogradDeviceTypeCPU) ... ok\r\ntest_rnn_backward_to_input_but_not_parameters_cpu (__main__.TestAutogradDeviceTypeCPU) ... skipped \'Only runs on cuda\'\r\ntest_set_requires_grad_only_for_floats_cpu_float32 (__main__.TestAutogradDeviceTypeCPU) ... ok\r\ntest_set_requires_grad_only_for_floats_cpu_float64 (__main__.TestAutogradDeviceTypeCPU) ... ok\r\ntest_set_requires_grad_only_for_floats_cpu_int16 (__main__.TestAutogradDeviceTypeCPU) ... ok\r\ntest_set_requires_grad_only_for_floats_cpu_int32 (__main__.TestAutogradDeviceTypeCPU) ... ok\r\ntest_set_requires_grad_only_for_floats_cpu_int64 (__main__.TestAutogradDeviceTypeCPU) ... ok\r\ntest_set_requires_grad_only_for_floats_cpu_int8 (__main__.TestAutogradDeviceTypeCPU) ... ok\r\ntest_sparse_ctor_getter_backward_cpu (__main__.TestAutogradDeviceTypeCPU) ... ok\r\ntest_sparse_mask_autograd_cpu (__main__.TestAutogradDeviceTypeCPU) ... ok\r\ntest_unused_output_device_cpu (__main__.TestAutogradDeviceTypeCPU) ... skipped \'fewer than 2 devices detected\'\r\ntest_where_functional_cpu (__main__.TestAutogradDeviceTypeCPU) ... ok\r\ntest_GRU_grad_and_gradgrad_cuda (__main__.TestAutogradDeviceTypeCUDA) ... /root/.local/lib/python2.7/site-packages/torch/backends/cudnn/__init__.py:107: UserWarning: PyTorch was compiled without cuDNN support. To use cuDNN, rebuild PyTorch making sure the library is visible to the build system.\r\n  ""PyTorch was compiled without cuDNN support. To use cuDNN, rebuild ""\r\nok\r\ntest_LSTM_grad_and_gradgrad_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok\r\ntest_advanced_indexing_backwards_large_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok\r\ntest_backward_device_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok\r\ntest_cdist_cuda (__main__.TestAutogradDeviceTypeCUDA) ... skipped ""test doesn\'t currently work on the ROCm stack""\r\ntest_cross_device_reentrant_autograd_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok\r\ntest_ctc_loss_cuda (__main__.TestAutogradDeviceTypeCUDA) ... skipped ""test doesn\'t currently work on the ROCm stack""\r\ntest_ctc_loss_cudnn_cuda (__main__.TestAutogradDeviceTypeCUDA) ... skipped ""test doesn\'t currently work on the ROCm stack""\r\ntest_free_unneeded_tensor_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok\r\ntest_grad_assignment_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok\r\ntest_inputbuffer_add_multidevice_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok\r\ntest_lstmcell_backward_only_one_output_grad_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok\r\ntest_pdist_large_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok\r\ntest_pin_memory_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok\r\ntest_profiler_emit_nvtx_cuda (__main__.TestAutogradDeviceTypeCUDA) ... skipped ""test doesn\'t currently work on the ROCm stack""\r\ntest_pyscalar_conversions_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok\r\ntest_requires_grad_factory_cuda_float32 (__main__.TestAutogradDeviceTypeCUDA) ... ok\r\ntest_requires_grad_factory_cuda_float64 (__main__.TestAutogradDeviceTypeCUDA) ... ok\r\ntest_rnn_backward_to_input_but_not_parameters_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok\r\ntest_set_requires_grad_only_for_floats_cuda_float16 (__main__.TestAutogradDeviceTypeCUDA) ... ok\r\ntest_set_requires_grad_only_for_floats_cuda_float32 (__main__.TestAutogradDeviceTypeCUDA) ... ok\r\ntest_set_requires_grad_only_for_floats_cuda_float64 (__main__.TestAutogradDeviceTypeCUDA) ... ok\r\ntest_set_requires_grad_only_for_floats_cuda_int16 (__main__.TestAutogradDeviceTypeCUDA) ... ok\r\ntest_set_requires_grad_only_for_floats_cuda_int32 (__main__.TestAutogradDeviceTypeCUDA) ... ok\r\ntest_set_requires_grad_only_for_floats_cuda_int64 (__main__.TestAutogradDeviceTypeCUDA) ... ok\r\ntest_set_requires_grad_only_for_floats_cuda_int8 (__main__.TestAutogradDeviceTypeCUDA) ... ok\r\ntest_sparse_ctor_getter_backward_cuda (__main__.TestAutogradDeviceTypeCUDA) ... skipped ""test doesn\'t currently work on the ROCm stack""\r\ntest_sparse_mask_autograd_cuda (__main__.TestAutogradDeviceTypeCUDA) ... ok\r\ntest_unused_output_device_cuda (__main__.TestAutogradDeviceTypeCUDA) ... Memory access fault by GPU node-2 (Agent handle: 0x35ea260) on address (nil). Reason: Page not present or supervisor privilege.\r\nTraceback (most recent call last):\r\n  File ""test/run_test.py"", line 455, in <module>\r\n    main()\r\n  File ""test/run_test.py"", line 447, in main\r\n    raise RuntimeError(message)\r\nRuntimeError: test_autograd failed! Received signal: SIGIOT\r\n\r\n']",[],0,0
241,pytorch,4302,closed,'torch.HalfTensor' object has no attribute 'mean'," results into
 > 'torch.HalfTensor' object has no attribute 'mean'

 is 0.4.0a0+5f7c550",,"[""we don't have math for CPU Half type (it would be very slow), convert it to cuda or CPU Float.""]",[],"['torch.HalfTensor([1,2,3]).mean()', 'torch.__version__']",0,0
242,pytorch,5790,open,Add hookable weights,"It would be nice to have hookable weights, where per-layer parameters can be operated upon and determined dynamically during each forward pass. The following is a short list of techniques that could benefit from this functionality:

- Quantization[[1]](https://arxiv.org/abs/1609.07061)
- Pruning[[2]](https://arxiv.org/abs/1510.00149)[[3]](https://arxiv.org/abs/1707.06168)[[4]](https://arxiv.org/abs/1802.00124)
- Stochastic gradient estimation[[5]](https://arxiv.org/abs/1711.00123)[[6]](https://arxiv.org/abs/1703.07370)
- DropConnect[[7]](https://cs.nyu.edu/~wanli/dropc/)
- Regularization[[8]](https://openreview.net/forum?id=H1Y8hhg0b)
- Manifold Tangent Classifier[[9]](https://papers.nips.cc/paper/4409-the-manifold-tangent-classifier.pdf)

I've begun work on [such a framework](https://github.com/castorini/candle), but the code is research-oriented and not production ready. I think the community would benefit from having official support for hookable weights.

I think it would also be interesting to have a more extensible backprop framework, in the sense of allowing for computation of different user-defined quantities across the computation graph. For example, the Hessian diagonals can be computed efficiently[[10]](http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf) using a backprop-like approach.

Thanks for reading.


cc @albanD @mruberry @jbschlosser @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a @vkuzo",enhancement module: nn triaged,"['Could you provide an example of what you mean by hookable weights?', 'Yep. The idea is to pass some weights _w_ through a user-specified function _g(w)_ for each forward pass, before the layer operates on the input. _g(w)_ is then used for the weights instead of _w_ for that layer. _g_ would of course be the identity function in the normal case. Here are a few practical examples:\r\n\r\n1. Pruning\r\nWe would like to zero out weights according to some criterion. Yes--we can zero the weights with the current implementation, but it\'s more elegant to have something like the following:\r\n\r\n```python\r\nlinear = nn.Linear(1024, 1024)\r\nconv = nn.Conv2d(3, 64, 3)\r\n...\r\ndef magnitude_prune(weight, max_magnitude=..., pct=...):\r\n    ...\r\n    return prune_output\r\nlinear.hook_weight(magnitude_prune, max_magnitude=0.01, pct=0.1)\r\nconv.hook_weight(magnitude_prune, max_magnitude=0.1, pct=0.1)\r\n```\r\n\r\nThis could represent pruning 0.1% of the weights (at every forward pass) that are less than 0.1. Beyond this simple example, there are other methods that necessarily involve hooking weights. Consider [this paper](https://openreview.net/forum?id=H1Y8hhg0b):\r\n\r\n```\r\nfunction g(w):\r\n    z <- z ~ p(z | phi)\r\n    w_new <- z * w\r\n    return w_new\r\n\r\nlayer.hook_weight(g)\r\n```\r\n\r\nwhere `z ~ p(z | phi)` is the (continuous) hard concrete distribution, `w` are the original weights, and `phi` is a learnable parameter. During training, `dL / dphi` is computed using backprop and used to update `phi`. Since `phi` is a parameter, this suggests that `g` can also take on the form of an `nn.Module`.\r\n\r\n2. Quantization\r\nWe would like to quantize some weights _w_. Consider the case of [1-bit quantization](https://arxiv.org/abs/1609.07061):\r\n\r\n```python\r\nclass BinarizeFunction(torch.autograd.Function):\r\n    @staticmethod\r\n    def forward(ctx, x):\r\n        """"""Outputs +/- 1 depending on magnitude""""""\r\n        return x.clamp(0, 1).ceil() * 2 - 1\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_output):\r\n        return grad_output # straight-through estimator\r\n\r\nbinarize_function = BinarizeFunction.apply\r\n\r\ndef binarize(w):\r\n    w.clamp_(0, 1)\r\n    return binarize_function(w)\r\n\r\nlayer.hook_weight(binarize)\r\n```\r\n\r\nThis isn\'t completely correct; it\'s just an illustrative example. For more concrete examples, please see[[1]](https://github.com/castorini/candle/blob/master/candle/proxy.py)[[2]](https://github.com/castorini/candle/blob/master/candle/prune.py)[[3]](https://github.com/castorini/candle/blob/master/candle/regularize.py).', 'What about the approach of the weight_norm implementation? I think it solves similar problem. Certainly it is more verbose than the simple hooking functions in your example, though.', ""If `hook_weight` runs at every forward, why can't you just use `register_forward_hook` and access `module.weight` inside? It shouldn't be any more complicated than the usage of `hook_weight`"", 'I thought in those cases we would encourage the use of the functional interface for writing the computation graph? I think assigning to a module weight in the `forward_hook` might not be permitted actually.\r\n\r\nWhat about something like\r\n```python\r\nclass MyModel(nn.Module):\r\n    def __init__(self):\r\n        self.conv1 = nn.Conv2d(1, 1, 1)\r\n        self.conv2 = nn.Conv2d(1, 1, 1)\r\n        self.conv3 = nn.Conv2d(1, 1, 1)\r\n\r\n    def _my_crazy_op(self, x, layer):\r\n        weight = layer.weight\r\n        bias = layer.bias\r\n        weight = a_crazy_func(weight)\r\n        bias = another_crazy_func(bias)\r\n        return F.conv2d(x, weight, bias, stride, ...)\r\n\r\n    def forward(self, x):\r\n        x = self.conv1(x)\r\n        x = self._my_crazy_op(x, self.conv2)\r\n        x = self._my_crazy_op(x, self.conv3)\r\n        return x\r\n```', ""@apaszke We could register a `forward_pre_hook` and modify the weights there, but it'd be better to distinguish between the actual module weights and what's provided by our custom ops, which might have parameters themselves. I guess we could then revert to the original weights in a `forward_hook`, but that's not as concise as having explicit weight hooks/providers.\r\n\r\n@fmassa Yes, that would work, but then we have to worry about bias, stride, etc. It's definitely a viable solution; it just adds some boilerplate. Actually, that amounts to writing a weight hooking layer:\r\n```python\r\nclass HookedConv2d(nn.Module):\r\n    def __init__(self, weight_provider, registry=None):\r\n        super().__init__()\r\n        self.weight_provider = weight_provider\r\n\r\n    def hook_weight(self, weight_hook, **kwargs):\r\n        self.weight_provider = weight_hook(self.weight_provider, **kwargs)\r\n\r\n    def forward(self, x):\r\n        weights = self.weight_provider()\r\n        return F.conv2d(x, *weights, **...)\r\n```\r\n"", ""Hey, just bumping this issue.. I really think a lot of people could benefit from this. For example, the popular [AWD-LSTM ](https://github.com/salesforce/awd-lstm-lm) uses a janky hook (out of necessity)  for LSTM weights and literally breaks with every minor version release, since there's no official support for hookable weights.\r\n\r\n@apaszke @soumith "", ""hey, I thought about this a bit.\r\n\r\nI dont think hookable weights are the right solution, because we already do provide stuff that's very close.\r\nFIrst, we provide [model.named_children](https://pytorch.org/docs/stable/nn.html?highlight=named_children#torch.nn.Module.named_children), `named_modules` and `named_parameters` via which you can comfortably do something like:\r\n\r\n```python\r\nprune_model_(model)\r\nout = model(inp)\r\n...\r\n```\r\n\r\nwhere `prune_model` loops over the layers, and for each instance (via the variable name or via `isinstance` do something reasonable).\r\n\r\nAs mentioned above, you give a scenario that your weight pruning logic might have Parameters itself -- well in that case, you are forced to either write separate layers, or register those Parameters to either the existing `model` or to a separate prune learning model -- any of the above solutions should work right?\r\n\r\nIf you're not convinced, I'd like to discuss this further and get this thread to a closure.\r\n\r\nOn  the topic of pruning, @mickypaganini has opened https://github.com/pytorch/pytorch/issues/20402 which I'm more inclined to accept under `torch.nn.utils`, so I'd like to get this topic further along in discussion as well.\r\n\r\nAgain, sorry for the delay."", 'Thanks for your input.\r\n> where prune_model loops over the layers, and for each instance (via the variable name or via isinstance do something reasonable).\r\n\r\n> @mickypaganini has opened #20402 which I\'m more inclined to accept under torch.nn.utils\r\n\r\nThose solutions are reasonable for parameterless pruning.\r\n\r\n> you give a scenario that your weight pruning logic might have Parameters itself -- well in that case, you are forced to either write separate layers, or register those Parameters to either the existing model or to a separate prune learning model\r\n\r\nFor most problems, it\'s true that @apaszke\'s and @fmassa\'s solutions work, e.g., either implementing the logic in a forward hook or a module with `functional` calls. The main problem is elegance and modularity. The issue with @apaszke\'s suggestion is that the weights need to be explicitly set. Using forward hooks don\'t play well with multiple weight hooks either, which forces the user to write more boilerplate. For example, if I have both a quantization and a pruning component, I would need to write some code to marry the two to the same group of weights. It\'s not hard, it just requires boilerplate.\r\n\r\nOn the other hand, weight hooks conveniently handle these two problems: first, we don\'t need to explicitly set the weights; second, we can chain together multiple hooks. Both of these have clear benefits for writing more modular code:\r\n\r\n```python\r\nmodule = MyModule()\r\nmodule.register_weight_hook(\'weight\', QuantizeHook(bits=8))\r\nmodule.register_weight_hook(\'weight\', LearnablePruningHook())\r\nmodule.register_weight_hook(\'weight\', DropConnectHook())\r\nmodule.register_weight_hook(\'scale\', SomeOtherHook())\r\n```\r\n\r\nIn the forward hook solution, we have to manually set the weights, as well as ""coordinate"" the right set of weights between the multiple hooks. Neither manually setting nor coordinating the weights is a terrible solution; it just requires more boilerplate. In the example above, the user doesn\'t need to worry about setting the weights, coordinating weights, or vice versa. All `QuantizeHook` has to contain is the quantization logic, and that\'s all the user would have to write for modular code. I guess this is analogous to Python\'s `@property`; by decoupling the weight getter from the attribute, we enable more complex, modularized operations over the weights.\r\n\r\nAs for @fmassa\'s suggestion, a similar argument can be made. This time, we also have to worry about the (hyper)parameters of the target functional, which would be annoying to restructure for, say, pretrained, prewritten models. Furthermore, that suggestion doesn\'t work with LSTMs and GRUs.\r\n\r\nAgain, I think the alternative solutions are fine. I understand this thread isn\'t top priority for PyTorch -- I just think hookable weights would improve over the existing solutions, mostly with respect to readability and modularity. It also follows the style of the existing forward hooking mechanisms.', ""the above sample does look tempting.\r\nOne problem that affects hooks -- but especially affects this style of code (because it's doing considerable non-trivial work) is that hooks aren't serializable -- storing so much state about the mechanism of the model in something like this seems laden with side-effects (esp. across serialization-deserialization).\r\nBut I think you are building a case for hooks (I am still not convinced though ;-) ).\r\nI have seen something similar, but gotten in a different way, with weight_norm: https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/weight_norm.py , but not everyone has liked it."", '> One problem that affects hooks -- but especially affects this style of code (because it\'s doing considerable non-trivial work) is that hooks aren\'t serializable\r\n\r\nHmm, I guess `model.(named_)parameters()` would additionally have to return the parameters of the weight hooks. I think the same holds for forward hooks as well. I also see your point about deserialization. If I\'m unmistaken, the user must provide the correct ""namespace"" with the appropriate classes at deserialization, which already happens in some of the code I see in practice.\r\n\r\nI suppose if users follow the recommended PyTorch serialization-deserialization route, it should be fine:\r\n\r\n> When saving a model for inference, it is only necessary to save the trained model‚Äôs learned parameters. Saving the model‚Äôs state_dict with the torch.save() function will give you the most flexibility for restoring the model later, which is why it is the recommended method for saving models.\r\n\r\n```python\r\n# Serialization\r\nmodule = MyModule(some_hyperparameter=10)\r\nmodule.register_forward_hook(mean_activation_prune) # Improper, just for exposition..\r\nmodule.register_weight_hook(\'weight_name\', QuantizeHook(bits=8))\r\nsd = module.state_dict()\r\n\'hooks.weight.weight_name.quantize\' in sd # True\r\n\'hooks.forward.mean_activation_prune\' in sd # True\r\ntorch.save(sd, \'state_dict.pt\')\r\n\r\n# Deserialization\r\nsd = torch.load(\'state_dict.pt\')\r\nmodule = MyModule(some_hyperparameter=10)\r\nmodule.register_forward_hook(mean_activation_prune)\r\nmodule.register_weight_hook(\'weight_name\', QuantizeHook(bits=8))\r\nmodule.load_state_dict(sd)\r\n```', 'Any updates? Another problem may be the gradient of the weight. In quantization aware training, we may want to modify the conv weight manually and make the `scale` and `zero point` appeared in the backward graph to make them updated in end-to-end training process.\r\n\r\nHere is a sample:\r\n```\r\n\r\nclass MyModule(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = nn.Conv2d(1, 2, 3, bias=False)\r\n        self.scale = torch.nn.Parameter(torch.tensor([0.5]))\r\n\r\n    def forward(self, x):\r\n        new_weight = self.layer.weight\r\n        self.scale = some_operation(self.scale)\r\n        new_weight = new_weight * self.sclae\r\n        self.layer.weight = new_weight \r\n        x = self.layer(x)\r\n        return x\r\n```\r\n\r\nSince the final new_weight is of type `torch.Tensor`, we can not directly assign it to  `self.layer.weight` . If we construct a new `torch.nn.Parameter(new_weight)` and assign it to `self.layer.weight`, then the scale will not appear in the backward graph.', 'Clearing the assignees. This sounds a lot like https://github.com/pytorch/pytorch/issues/49171', 'The original question was mostly about reparametrization I think that was added in https://github.com/pytorch/pytorch/issues/28937\r\n\r\nDoes that satisfy your use case?']",[],[],0,0
243,pytorch,25481,open,[feature request] symmetric matrix square root,"This is needed when incorporating curvature information into optimization

There's PyTorch implementation of symmetric matrix square root op [here](https://github.com/msubhransu/matrix-sqrt) but they use PyTorch for backward pass only, and use scipy for forward pass

I've hacked something together by mirroring 




cc @vincentqb @vishwakftw @jianyuh @nikitaved @pearu @mruberry @heitorschueroff @SsnL",function request module: linear algebra triaged,"['I actually need a batched version of this, cc @vishwakftw for ideas', 'One naive idea would be to compute the symmetric eigendecomposition and compute the square root of the eigenvalues, and then reconstruct the matrix.\r\n\r\nedit: I didn‚Äôt see the code sorry. I‚Äôll look into this.', 'There is also a Newton step-based matrix squared root: https://github.com/pytorch/pytorch/issues/9983#issuecomment-431806692', ""@vadimkantorov these are the same guys that did PyTorch op for matrix square root. So I wonder why they chose to use scipy for the forward pass. They mention that Newton step convergence radius is limited. I've seen this problem when playing with Newon method for matrix inversion (Schultz iteration), the initial guess had to be pretty close to the answer to get reasonable performance."", 'It seems that https://github.com/msubhransu/matrix-sqrt/blob/master/matrix_sqrt.py does not use any scipy... (I couldn\'t find any ""scipy"" substring). I must be missing something.', 'You are right, I was thinking of this implementation, which is from different authors https://github.com/steveli/pytorch-sqrtm', '@yaroslavvb Ah, I see. Though they seem to use scipy both for forward and backward (using a Sylvester equation solver in scipy) :)', 'I have few points to make here since I worked on [the same problem before](https://github.com/ModarTensai/network_moments/blob/f66bfb44d5407024e1ef43ef19deb1da31133c27/network_moments/torch/utils/ops.py#L168-L207):\r\n\r\n- `b.t()` should be replaced by `b.transpose(-2, -1)` to support batching\r\n- `b.diag()` should be replaced by `b.diag_embed()` to support batching\r\n- `A @ b.diag_embed()` should be replaced by `A * b.unsqeeuze(-2)` for efficiency and batching\r\n- `_, s, v = matrix.svd()` is more stable and efficient, for [some reason](https://github.com/pytorch/pytorch/issues/16072#issuecomment-454769041), than `s, v = matrix.symeig(eigenvectors=True)` to pass `torch.autograd.gradcheck`\r\n- [scipy.linalg.pinvh (v1.3.1)](https://github.com/scipy/scipy/blob/v1.3.1/scipy/linalg/basic.py#L1402-L1410) multiplies the condition value by `max(a.shape)` instead of `1E3` or `1E6`\r\n- `cond` can be computed without taking the absolute value of the eigenvalues since we are taking their square root and assuming positive definite matrices\r\n- `eps` should not be hard coded and [can be computed](https://github.com/ModarTensai/network_moments/blob/f66bfb44d5407024e1ef43ef19deb1da31133c27/network_moments/torch/utils/utils.py#L8-L19) for any given type or by using @vadimkantorov suggestion `torch.finfo`\r\n- Putting it all together:\r\n```python\r\ndef symsqrt(matrix):\r\n    """"""Compute the square root of a positive definite matrix.""""""\r\n    # perform the decomposition\r\n    # s, v = matrix.symeig(eigenvectors=True)\r\n    _, s, v = matrix.svd()  # passes torch.autograd.gradcheck()\r\n    # truncate small components\r\n    above_cutoff = s > s.max() * s.size(-1) * torch.finfo(s.dtype).eps\r\n    s = s[..., above_cutoff]\r\n    v = v[..., above_cutoff]\r\n    # compose the square root matrix\r\n    return (v * s.sqrt().unsqueeze(-2)) @ v.transpose(-2, -1)\r\n```\r\n- **Bonus**: `scipy.linalg.solve_sylvester(a, a, b)` [can be computed](https://github.com/ModarTensai/network_moments/blob/f66bfb44d5407024e1ef43ef19deb1da31133c27/network_moments/torch/utils/utils.py#L39-L67) as follows:\r\n```python\r\ndef special_sylvester(a, b):\r\n    """"""Solves the eqation `A @ X + X @ A = B` for a positive definite `A`.""""""\r\n    # https://math.stackexchange.com/a/820313\r\n    s, v = a.symeig(eigenvectors=True)\r\n    d = s.unsqueeze(-1)\r\n    d = d + d.transpose(-2, -1)\r\n    vt = v.transpose(-2, -1)\r\n    c = vt @ b @ v\r\n    return v @ (c / d) @ vt\r\n```', '@ModarTensai about eps, is [`torch.finfo(s.dtype).eps`](https://pytorch.org/docs/stable/type_info.html?highlight=finfo#torch.torch.finfo) useful here? another way would be allow user passing their own eps if they wish', ""You are absolutely right. Last time I checked, they didn't port `finfo` from numpy yet. I will update it in the original comment."", '@ModarTensai nice tips!\r\n- I got the 1e3 and 1e6 factors from scipy.linalg.pinv2 . However, in master, [they were removed](https://github.com/scipy/scipy/pull/10067) and I couldn\'t track down where they came from\r\n- solve_sylvester can be extended to no solution/multiple solutions case by [discarding eigenvalues](https://gist.github.com/yaroslavvb/1a258310b266ac7ade35b025c9e6fae3) below the cut-off. Empirically this seem to give the [same answer](https://www.wolframcloud.com/obj/yaroslavvb/newton/lyapunov.nb) as LeastSquares on Kronecker-expanded equations\r\n- for some applications `symsqrt(A)` can be replaced by a ""non-symmetric matrix square root"" `A=BB\'` which can be faster (ie, 10-100x faster when `A`=Hessian of cross-entropy loss)', '@ModarTensai The `symsqrt` function does not work for batched inputs, eg. of shape [32, 100, 100]. The error encountered is:\r\n```python\r\n<ipython-input-2-730fdb238ed4> in symsqrt(matrix)\r\n      7     above_cutoff = s > s.max() * s.size(-1) * torch.finfo(s.dtype).eps\r\n      8     s = s[..., above_cutoff]\r\n----> 9     v = v[..., above_cutoff]\r\n     10     # compose the square root matrix\r\n     11     return (v * s.sqrt().unsqueeze(-2)) @ v.transpose(-2, -1)\r\n\r\nIndexError: The shape of the mask [32, 100] at index 0does not match the shape of the indexed tensor [32, 100, 100] at index 1\r\n```', '@rharish101, thank you for pointing this out. We cannot do that anymore. Here is a quick fix for you:\r\n```python\r\ndef symsqrt(matrix):\r\n    """"""Compute the square root of a positive definite matrix.""""""\r\n    _, s, v = matrix.svd()\r\n    zero = torch.zeros((), device=s.device, dtype=s.dtype)\r\n    threshold = s.max(-1).values * s.size(-1) * torch.finfo(s.dtype).eps\r\n    s = s.where(s > threshold.unsqueeze(-1), zero)  # zero out small components\r\n    return (v * s.sqrt().unsqueeze(-2)) @ v.transpose(-2, -1)\r\n```\r\nA more performent code should truncate the common columns in the batch and zero out the rest.', '( @ModarTensai could also probably save `unsqueeze()` on `threshold` if `keepdim=True` is used in `s.max(dim = -1, keepdim = True).values` )', '@vadimkantorov, that is true but the line became too long for my taste :)\r\n@rharish101, here is the same code again but with truncation as well.\r\n```python\r\ndef symsqrt(matrix):\r\n    """"""Compute the square root of a positive definite matrix.""""""\r\n    _, s, v = matrix.svd()\r\n    good = s > s.max(-1, True).values * s.size(-1) * torch.finfo(s.dtype).eps\r\n    components = good.sum(-1)\r\n    common = components.max()\r\n    unbalanced = common != components.min()\r\n    if common < s.size(-1):\r\n        s = s[..., :common]\r\n        v = v[..., :common]\r\n        if unbalanced:\r\n            good = good[..., :common]\r\n    if unbalanced:\r\n        s = s.where(good, torch.zeros((), device=s.device, dtype=s.dtype))\r\n    return (v * s.sqrt().unsqueeze(-2)) @ v.transpose(-2, -1)\r\n```\r\nYou can test it out like this:\r\n```python\r\nx = torch.randn(5, 10, 10).double()\r\nx = x @ x.transpose(-1, -2)\r\ny = symsqrt(x)\r\nprint(torch.allclose(x, y @ y.transpose(-1, -2)))\r\nx.requires_grad = True\r\ntorch.autograd.gradcheck(symsqrt, [x])\r\ntorch.autograd.gradgradcheck(symsqrt, [x])\r\n```', 'Thank you all for your prompt replies!', 'Hi everyone, \r\nI don\'t know exactly why but torch.svd() generates errors when the gradient is required while torch.symeig() is fine with the gradient but computations are run on CPUs.\r\n\r\nFor these reasons I implement a custom torch.autograd.Function following this implementation: [https://github.com/msubhransu/matrix-sqrt](url)\r\nI didn\'t know exactly how to handle the silent parameters inside the function so I added the device as an input. Now, all the computations are run on the GPU.\r\n\r\nEdit: well, I figured it out, the device is stored in input.device\r\n\r\n```python\r\nclass MatrixSquareRoot(Function):\r\n    """"""Square root of a positive definite matrix.\r\n    NOTE: matrix square root is not differentiable for matrices with\r\n          zero eigenvalues.\r\n    """"""    \r\n    @staticmethod\r\n    def forward(ctx, input):\r\n        dim = input.shape[0]\r\n        norm = torch.norm(input.double())\r\n        Y = input.double()/norm\r\n        I = torch.eye(dim,dim,device=input.device).double()\r\n        Z = torch.eye(dim,dim,device=input.device).double()\r\n        for i in range(20):\r\n            T = 0.5*(3.0*I - Z.mm(Y))\r\n            Y = Y.mm(T)\r\n            Z = T.mm(Z)\r\n        sqrtm = Y*torch.sqrt(norm)\r\n        ctx.mark_dirty(Y,I,Z)\r\n        ctx.save_for_backward(sqrtm)\r\n        return sqrtm\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_output):\r\n        grad_input = None\r\n        sqrtm, = ctx.saved_tensors\r\n        dim = sqrtm.shape[0]\r\n        norm = torch.norm(sqrtm)\r\n        A = sqrtm/norm\r\n        I = torch.eye(dim, dim, device=sqrtm.device).double()\r\n        Q = grad_output.double()/norm\r\n        for i in range(20):\r\n            Q = 0.5*(Q.mm(3.0*I-A.mm(A))-A.t().mm(A.t().mm(Q)-Q.mm(A)))\r\n            A = 0.5*A.mm(3.0*I-A.mm(A))\r\n        grad_input = 0.5*Q\r\n        return grad_input    \r\nsqrtm = MatrixSquareRoot.apply\r\n```\r\n', ""@JonathanVacher torch.symeig should run on GPU if the input is on GPU. What are the SVD failures? There's a rare SVD failure on singular matrices due to a limitation of gesdd algorithm, fails both in magma and scipy implementations -- https://github.com/pytorch/pytorch/issues/25978#issue-492018796  . Using Newton iteration seems to require double precision to provide similar level of accuracy, but this makes it 10x slower than decomposition-based approaches\r\n\r\nI've benchmarked the three solutions in the colab here on 4000 matrix and T4 GPU, modifying your code to use same precision as the input -- [Pytorch symmetric square root](https://colab.research.google.com/drive/1wSO1MFh_ZCfOnejFnW1vkD71jaJy2Olu#scrollTo=3nL81G7LRWCm)\r\n\r\n```\r\nsymeig: 1.4 seconds\r\nsvd: 4.4 seconds\r\nNewton (30 iterations, double precision): 33 seconds\r\n```\r\n\r\nSwitching to single precision makes it run at similar speed as svd/symeig versions but at lower accuracy. So it seems Newton-based implementation is inferior to decomposition-based. However, it could be useful for stochastic setting -- when the target is a noisy sample, a single Newton step may give desired precision, in which case this approach would be an order of magnitude faster"", ""Ok, so if it's work for you I may have a problem.\r\n1/ with my implementation, my algorithm is running with 1 CPU core and 1 GPU\r\n2/ with symeig my algorithm is running but 8 CPU cores are used in addition to 1 GPU\r\n3/ with svd it crashes at the 1st iteration when computing a svd.\r\nI tried to heavily regularize my the covariances so I can conclude that it's not because of matrix singularity (and anyway it works with symeig).\r\n```\r\nIntel MKL ERROR: Parameter 4 was incorrect on entry to SLASCL.\r\nIntel MKL ERROR: Parameter 4 was incorrect on entry to SLASCL.\r\nTraceback (most recent call last):\r\n[...]\r\n _, s, v = matrix.svd()  # passes torch.autograd.gradcheck()\r\nRuntimeError: svd_cuda: the updating process of SBDSDC did not converge (error: 14)\r\n```\r\nHere are my env infos:\r\n```\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.105\r\nGPU models and configuration: \r\nGPU 0: TITAN V\r\nGPU 1: TITAN V\r\nGPU 2: TITAN V\r\nGPU 3: TITAN V\r\n\r\nNvidia driver version: 430.50\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.0\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.1\r\n[pip3] torch==1.4.0\r\n[pip3] torchfile==0.1.0\r\n[pip3] torchvision==0.4.2\r\n[conda] Could not collect\r\n```\r\nAny idea ? My Cmake version ?"", ""@JonathanVacher  You could rule out difference in environment by adding your code to my colab notebook and running it there, or copy-pasting code from that notebook and running it in your environment.\r\n\r\nWall-clock time is more useful measure than CPU utilization, running `taskset 0x1 python test_svd.py` drops my CPU usage from 32 cores to 1 core without affecting wall-clock time much.\r\n\r\nYour SVD error says `Intel MKL` so I'm guessing you are feeding CPU tensors both into your symeig and svd experiments. Google says this error is caused by `nan` or `inf` values in your input tensor, so try `assert np.isfinite(tensor.cpu().numpy())`. As a side-note, I've seen crashes in MKL which were fixed (at the expense of speed) by setting `OMP_NUM_THREADS=1` environment variable\r\n\r\n"", ""@yaroslavvb Magma uses MKL for some intermediate computation under the hood: I've had some MKL errors earlier with GPU eig() computations before: https://github.com/pytorch/pytorch/issues/9384 (when it's incorrect parameter error, I assume this is a bug in either Magma or MKL)"", ""Ok I don't understand at all how is it possible to observe such a difference in timing ! \r\nEdit: I updated the image, I forgot to synchronize with the correct GPU. It's still faster for me with 20 Newton iteration.\r\n\r\nAll implementations work now. I will need to check the rest of my code. I think everything is on the GPU memory though... Will see.\r\n\r\n![matrix-sqrt](https://user-images.githubusercontent.com/6490783/74290560-38698680-4d00-11ea-9829-34e678d46074.png)\r\n"", ""@JonathanVacher just reran this code on V100 and see same timing as above. It seems switching from T4 to V100 doesn't affect svd/symeig speed, meanwhile matmul-based Newton method becomes 4x faster in single precision (I expected 3x faster) and 30x faster in double precision. So for V100 cards, Newton method seems better in all respects.\r\n\r\nIt seems `torch.symeig` and `torch.svd` are having hard time saturating V100 compute units. I'm seeing power draw of about 100 watts when running those routines in the benchmark above, whereas Newton-method uses close to 300 Watts. Another advantage is that it works with half-precision. Getting about 2% relative error with 20 iterations in half precision on 8k-by-8k matrices, about 6x faster than symeig approach"", 'Hi All, \r\n\r\nI was wondering if there has been any updates on this? It has been about 8 months since the last comment above.\r\n\r\nI have a related question to this which I have posted in Stackoverflow here: https://stackoverflow.com/questions/64462253/pytorch-square-root-of-a-positive-semi-definite-matrix. I was wondering if anyone could assist me with this question. Would really appreciate it!', 'Many implementations are available in this issue and you can use any of them. There is no direct implementation in pytorch as the issue is still open.\r\n', 'I am just about to start working on general matrix square root, [FR](https://github.com/pytorch/pytorch/issues/9983#issuecomment-626629006). My plan is to use the IN iteration (Higman, Functions of Matrices, page 142, formula 6.20) with scaling and some compensation scheme for better precision. The good thing it does handle rank deficient cases as the iteration does start with a matrix `X + I`.\r\nIt is also possible to use the basic Newton method,  which has a quadratic (although local) convergence for inputs with small eigenvalues, however, I do not immediately see how to extend it to arbitrary matrices. It is possible to employ a proper rescaling only for inputs of full rank (and after the reslace we can loose the rank). The rank deficient case is not that obvious to me yet...\r\n\r\nLooks like the methods based on the Schur decomposition are best for CPU, so for symmetric matrices it is probably better to just resort to `symeig`.', '@ModarTensai , thank you for the sylvester equation code! It will be of use for the analytic backward!', '@nikitaved, you are most welcome. Please, let me know if you need anything else. Currently, I am busy with CVPR but I will gladly join this effort, if needed, after the deadline.', 'Hi @JonathanVacher, many thanks for your reply.\r\n\r\nHowever, I am looking for an implementation which works for positive **semi-definite** matrices. All the implementations that are available in this issue are only applicable for positive **definite** matrices?', ""> Hi @JonathanVacher, many thanks for your reply.\r\n> \r\n> However, I am looking for an implementation which works for positive **semi-definite** matrices. All the implementations that are available in this issue are only applicable for positive **definite** matrices?\r\n\r\nShould work for singular matrices, that's what `above_cutoff` logic is for"", '@leockl, if it is the backward you are concerned about (reciprocals of pairwise sums of eigenvalues), note that the matrix square root is a function of a matrix, which allows to get the gradient by running the function of a matrix on a 4x larger input.\r\nCheck out:\r\n```\r\nMathias, Roy.\r\nA Chain Rule for Matrix Functions and Applications.\r\nSIAM J. Matrix Anal. Appl. 17 (1996): 610-620.\r\n\r\n```\r\nIt is exactly how the backward for the matrix exponential is implemented in PyTorch.\r\n\r\nMaybe you could try [this](https://github.com/pytorch/pytorch/issues/25481#issuecomment-576493693) approach? Only forward methods, no need for explicit backward unless you can do it more efficiently (so that you could use `symeig` in forward, for example, `symeig.backward` has some limitations).', 'On the note regarding `symeig.backward` stability. It is only stable for inputs of full rank (minus 1) with distinct eigenvalues with some gap, as one of the intermediate steps of the algorithm is computing the reciprocals of pairwise differences between the eigenvalues.']","['\r\n def symsqrt(a, cond=None, return_rank=False):\r\n    """"""Computes the symmetric square root of a positive definite matrix""""""\r\n\r\n    s, u = torch.symeig(a, eigenvectors=True)\r\n    cond_dict = {torch.float32: 1e3 * 1.1920929e-07, torch.float64: 1E6 * 2.220446049250313e-16}\r\n\r\n    if cond in [None, -1]:\r\n        cond = cond_dict[a.dtype]\r\n\r\n    above_cutoff = (abs(s) > cond * torch.max(abs(s)))\r\n\r\n    psigma_diag = torch.sqrt(s[above_cutoff])\r\n    u = u[:, above_cutoff]\r\n\r\n    B = u @ torch.diag(psigma_diag) @ u.t()\r\n    if return_rank:\r\n        return B, len(psigma_diag)\r\n    else:\r\n        return B\r\n\r\ndef symsqrt_test():\r\n    def randomly_rotate(X):\r\n        """"""Randomly rotate d,n data matrix X""""""\r\n        d, n = X.shape\r\n        z = torch.randn((d, d), dtype=X.dtype)\r\n        q, r = torch.qr(z)\r\n        d = torch.diag(r)\r\n        ph = d / abs(d)\r\n        rot_mat = q * ph\r\n        return rot_mat @ X\r\n\r\n    n = 20\r\n    d = 10\r\n    X = torch.randn((d, n))\r\n\r\n    # embed in a larger space\r\n    X = torch.cat([X, torch.zeros_like(X)])\r\n    X = randomly_rotate(X)\r\n    cov = X @ X.t()\r\n    sqrt, rank = symsqrt(cov, return_rank=True)\r\n    assert rank == d\r\n    assert torch.allclose(sqrt @ sqrt, cov, atol=1e-5)\r\n']",['scipy.linalg.pinvh'],0,0
244,pytorch,28444,open,Problem installing from source on CentOS 6.5,"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:

I ran the following code:

1. conda create detectron2 python=3.7
2. conda activate detectron2
3. conda install numpy ninja pyyaml mkl mkl-include setuptools cmake cffi typing
4. git clone --recursive https://github.com/pytorch/pytorch
5. cd pytorch
6. export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-""$(dirname $(which conda))/../""}
7. python setup.py install

This gave the following stdout (sorry for the wall of text!):
-Wall -Wno-unused -Wno-attributes -Wno-unused-result -Wno-psabi -ffp-contract=off -fno-math-errno -fno-trapping-math/nethome/ebj26/apps/pytorch/third_party/nccl/nccl/src'
Compiling  misc/nvmlwrap.cc                    > /nethome/ebj26/apps/pytorch/build/nccl/obj/misc/nvmlwrap.o
Compiling  misc/rings.cc                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/misc/rings.o
Compiling  init.cc                             > /nethome/ebj26/apps/pytorch/build/nccl/obj/init.o
Compiling  misc/group.cc                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/misc/group.o
Grabbing   include/nccl_net.h                  > /nethome/ebj26/apps/pytorch/build/nccl/include/nccl_net.h
Compiling  misc/ibvwrap.cc                     > /nethome/ebj26/apps/pytorch/build/nccl/obj/misc/ibvwrap.o
Compiling  collectives/reduce_scatter.cc       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/reduce_scatter.o
Compiling  channel.cc                          > /nethome/ebj26/apps/pytorch/build/nccl/obj/channel.o
Compiling  collectives/all_gather.cc           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/all_gather.o
Compiling  transport/p2p.cc                    > /nethome/ebj26/apps/pytorch/build/nccl/obj/transport/p2p.o
Compiling  enqueue.cc                          > /nethome/ebj26/apps/pytorch/build/nccl/obj/enqueue.o
Compiling  bootstrap.cc                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/bootstrap.o
Compiling  misc/argcheck.cc                    > /nethome/ebj26/apps/pytorch/build/nccl/obj/misc/argcheck.o
Compiling  transport/shm.cc                    > /nethome/ebj26/apps/pytorch/build/nccl/obj/transport/shm.o
Compiling  transport/net.cc                    > /nethome/ebj26/apps/pytorch/build/nccl/obj/transport/net.o
Compiling  misc/topo.cc                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/misc/topo.o
Compiling  transport/net_ib.cc                 > /nethome/ebj26/apps/pytorch/build/nccl/obj/transport/net_ib.o
Compiling  collectives/broadcast.cc            > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/broadcast.o
Compiling  transport.cc                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/transport.o
Compiling  transport/net_socket.cc             > /nethome/ebj26/apps/pytorch/build/nccl/obj/transport/net_socket.o
Compiling  misc/utils.cc                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/misc/utils.o
Compiling  misc/trees.cc                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/misc/trees.o
Compiling  collectives/all_reduce.cc           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/all_reduce.o
Compiling  collectives/reduce.cc               > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/reduce.o
Generating nccl.h.in                           > /nethome/ebj26/apps/pytorch/build/nccl/include/nccl.h
Generating nccl.pc.in                          > /nethome/ebj26/apps/pytorch/build/nccl/lib/pkgconfig/nccl.pc
make[2]: Entering directory /nethome/ebj26/apps/pytorch/third_party/nccl/nccl/src/collectives/device'
make[2]: Entering directory /nethome/ebj26/apps/pytorch/third_party/nccl/nccl/src/collectives/device'
Linking    libnccl.so.2.4.8                    > /nethome/ebj26/apps/pytorch/build/nccl/lib/libnccl.so.2.4.8
make: *** [src.build] Segmentation fault (core dumped)
[200/3081] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/descriptor.cc.o
ninja: build stopped: subcommand failed.
Traceback (most recent call last):
  File ""setup.py"", line 759, in <module>
    build_deps()
  File ""setup.py"", line 311, in build_deps
    cmake=cmake)
  File ""/nethome/ebj26/apps/pytorch/tools/build_pytorch_libs.py"", line 59, in build_caffe2
    cmake.build(my_env)
  File ""/nethome/ebj26/apps/pytorch/tools/setup_helpers/cmake.py"", line 334, in build
    self.run(build_args, my_env)
  File ""/nethome/ebj26/apps/pytorch/tools/setup_helpers/cmake.py"", line 142, in run
    check_call(command, cwd=self.build_dir, env=env)
  File ""/nethome/ebj26/apps/anaconda3/envs/detectron2/lib/python3.7/subprocess.py"", line 347, in check_call
    raise CalledProcessError(retcode, cmd)
subprocess.CalledProcessError: Command '['cmake', '--build', '.', '--target', 'install', '--config', 'Release', '--', '-j', '16']' returned non-zero exit status 1.

PyTorch version: N/A
Is debug build: N/A
CUDA used to build PyTorch: N/A

OS: CentOS release 6.5 (Final)
GCC version: (GCC) 5.5.0
CMake version: version 3.14.0

Python version: 3.7
Is CUDA available: N/A
CUDA runtime version: Could not collect
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] numpy==1.17.2
[conda] blas                      1.0                         mkl  
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] mkl-service               2.3.0            py37he904b0f_0  
[conda] mkl_fft                   1.0.14           py37ha843d7b_0  
[conda] mkl_random                1.1.0            py37hd6b4f25_0
`

## Additional context

Any help would be greatly appreciated!
",module: build module: nccl triaged,['Looks like the the compiler segfaulted during linking time on nccl.\r\nMaybe you run out of memory during compilation?'],"['\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nClean install of pytorch from source\r\n\r\n## Environment\r\n\r\nOutput from collect_env.py:\r\n']","['', '\r\nThe use of NO_MKLDNN is deprecated and will be removed on Feb 20, 2020.Please use USE_MKLDNN instead.\r\nBuilding wheel torch-1.4.0a0+3fce612\r\n-- Building version 1.4.0a0+3fce612\r\ncmake -GNinja -DBUILD_PYTHON=True -DBUILD_TEST=True -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/nethome/ebj26/apps/pytorch/torch -DCMAKE_PREFIX_PATH=/nethome/ebj26/apps/anaconda3/envs/detectron2 -DNUMPY_INCLUDE_DIR=/nethome/ebj26/apps/anaconda3/envs/detectron2/lib/python3.7/site-packages/numpy/core/include -DPYTHON_EXECUTABLE=/nethome/ebj26/apps/anaconda3/envs/detectron2/bin/python -DPYTHON_INCLUDE_DIR=/nethome/ebj26/apps/anaconda3/envs/detectron2/include/python3.7m -DPYTHON_LIBRARY=/nethome/ebj26/apps/anaconda3/envs/detectron2/lib/libpython3.7m.so.1.0 -DTORCH_BUILD_VERSION=1.4.0a0+3fce612 -DUSE_CUDA=True -DUSE_MKLDNN=0 -DUSE_NUMPY=True /nethome/ebj26/apps/pytorch\r\n-- The CXX compiler identification is GNU 5.5.0\r\n-- The C compiler identification is GNU 5.5.0\r\n-- Check for working CXX compiler: /share/opt/gcc/5.5.0/bin/c++\r\n-- Check for working CXX compiler: /share/opt/gcc/5.5.0/bin/c++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Check for working C compiler: /share/opt/gcc/5.5.0/bin/gcc\r\n-- Check for working C compiler: /share/opt/gcc/5.5.0/bin/gcc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Not forcing any particular BLAS to be found\r\n-- Performing Test COMPILER_WORKS\r\n-- Performing Test COMPILER_WORKS - Success\r\n-- Performing Test SUPPORT_GLIBCXX_USE_C99\r\n-- Performing Test SUPPORT_GLIBCXX_USE_C99 - Success\r\n-- Performing Test CAFFE2_EXCEPTION_PTR_SUPPORTED\r\n-- Performing Test CAFFE2_EXCEPTION_PTR_SUPPORTED - Success\r\n-- std::exception_ptr is supported.\r\n-- Performing Test CAFFE2_IS_NUMA_AVAILABLE\r\n-- Performing Test CAFFE2_IS_NUMA_AVAILABLE - Failed\r\n-- NUMA is not available\r\n-- Performing Test CAFFE2_NEED_TO_TURN_OFF_DEPRECATION_WARNING\r\n-- Performing Test CAFFE2_NEED_TO_TURN_OFF_DEPRECATION_WARNING - Failed\r\n-- Turning off deprecation warning due to glog.\r\n-- Performing Test CAFFE2_COMPILER_SUPPORTS_AVX2_EXTENSIONS\r\n-- Performing Test CAFFE2_COMPILER_SUPPORTS_AVX2_EXTENSIONS - Failed\r\n-- Performing Test CAFFE2_COMPILER_SUPPORTS_AVX512_EXTENSIONS\r\n-- Performing Test CAFFE2_COMPILER_SUPPORTS_AVX512_EXTENSIONS - Failed\r\n-- Performing Test COMPILER_SUPPORTS_HIDDEN_VISIBILITY\r\n-- Performing Test COMPILER_SUPPORTS_HIDDEN_VISIBILITY - Success\r\n-- Performing Test COMPILER_SUPPORTS_HIDDEN_INLINE_VISIBILITY\r\n-- Performing Test COMPILER_SUPPORTS_HIDDEN_INLINE_VISIBILITY - Success\r\n-- Performing Test COMPILER_SUPPORTS_RDYNAMIC\r\n-- Performing Test COMPILER_SUPPORTS_RDYNAMIC - Success\r\n-- Building using own protobuf under third_party per request.\r\n-- Use custom protobuf build.\r\n-- Looking for pthread.h\r\n-- Looking for pthread.h - found\r\n-- Looking for pthread_create\r\n-- Looking for pthread_create - not found\r\n-- Looking for pthread_create in pthreads\r\n-- Looking for pthread_create in pthreads - not found\r\n-- Looking for pthread_create in pthread\r\n-- Looking for pthread_create in pthread - found\r\n-- Found Threads: TRUE  \r\n-- Caffe2 protobuf include directory: $<BUILD_INTERFACE:/nethome/ebj26/apps/pytorch/third_party/protobuf/src>$<INSTALL_INTERFACE:include>\r\n-- Trying to find preferred BLAS backend of choice: MKL\r\n-- MKL_THREADING = OMP\r\n-- Looking for sys/types.h\r\n-- Looking for sys/types.h - found\r\n-- Looking for stdint.h\r\n-- Looking for stdint.h - found\r\n-- Looking for stddef.h\r\n-- Looking for stddef.h - found\r\n-- Check size of void*\r\n-- Check size of void* - done\r\n-- Looking for cblas_sgemm\r\n-- Looking for cblas_sgemm - found\r\n-- MKL libraries: /nethome/ebj26/apps/anaconda3/envs/detectron2/lib/libmkl_intel_lp64.so;/nethome/ebj26/apps/anaconda3/envs/detectron2/lib/libmkl_gnu_thread.so;/nethome/ebj26/apps/anaconda3/envs/detectron2/lib/libmkl_core.so;-fopenmp;/usr/lib64/libpthread.so;/usr/lib64/libm.so;/usr/lib64/libdl.so\r\n-- MKL include directory: /nethome/ebj26/apps/anaconda3/envs/detectron2/include\r\n-- MKL OpenMP type: GNU\r\n-- MKL OpenMP library: -fopenmp\r\n-- The ASM compiler identification is GNU\r\n-- Found assembler: /share/opt/gcc/5.5.0/bin/gcc\r\n-- Check if compiler accepts -pthread\r\n-- Check if compiler accepts -pthread - yes\r\n-- Brace yourself, we are building NNPACK\r\n-- Performing Test NNPACK_ARCH_IS_X86_32\r\n-- Performing Test NNPACK_ARCH_IS_X86_32 - Failed\r\n-- Found PythonInterp: /nethome/ebj26/apps/anaconda3/envs/detectron2/bin/python (found version ""3.7.4"") \r\n-- NNPACK backend is x86-64\r\n-- Failed to find LLVM FileCheck\r\n-- Found Git: /usr/bin/git (found version ""1.7.1"") \r\n-- git Version: v1.4.0-505be96a\r\n-- Version: 1.4.0\r\n-- Performing Test HAVE_CXX_FLAG_STD_CXX11\r\n-- Performing Test HAVE_CXX_FLAG_STD_CXX11 - Success\r\n-- Performing Test HAVE_CXX_FLAG_WALL\r\n-- Performing Test HAVE_CXX_FLAG_WALL - Success\r\n-- Performing Test HAVE_CXX_FLAG_WEXTRA\r\n-- Performing Test HAVE_CXX_FLAG_WEXTRA - Success\r\n-- Performing Test HAVE_CXX_FLAG_WSHADOW\r\n-- Performing Test HAVE_CXX_FLAG_WSHADOW - Success\r\n-- Performing Test HAVE_CXX_FLAG_WERROR\r\n-- Performing Test HAVE_CXX_FLAG_WERROR - Success\r\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC\r\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC - Success\r\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS\r\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS - Success\r\n-- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32\r\n-- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32 - Failed\r\n-- Performing Test HAVE_CXX_FLAG_WFLOAT_EQUAL\r\n-- Performing Test HAVE_CXX_FLAG_WFLOAT_EQUAL - Success\r\n-- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING\r\n-- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING - Success\r\n-- Performing Test HAVE_CXX_FLAG_WNO_DEPRECATED_DECLARATIONS\r\n-- Performing Test HAVE_CXX_FLAG_WNO_DEPRECATED_DECLARATIONS - Success\r\n-- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING\r\n-- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING - Success\r\n-- Performing Test HAVE_CXX_FLAG_WD654\r\n-- Performing Test HAVE_CXX_FLAG_WD654 - Failed\r\n-- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY\r\n-- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY - Failed\r\n-- Performing Test HAVE_CXX_FLAG_COVERAGE\r\n-- Performing Test HAVE_CXX_FLAG_COVERAGE - Success\r\n-- Performing Test HAVE_STD_REGEX\r\n-- Performing Test HAVE_STD_REGEX\r\n-- Performing Test HAVE_STD_REGEX -- success\r\n-- Performing Test HAVE_GNU_POSIX_REGEX\r\n-- Performing Test HAVE_GNU_POSIX_REGEX\r\n-- Performing Test HAVE_GNU_POSIX_REGEX -- failed to compile\r\n-- Performing Test HAVE_POSIX_REGEX\r\n-- Performing Test HAVE_POSIX_REGEX\r\n-- Performing Test HAVE_POSIX_REGEX -- success\r\n-- Performing Test HAVE_STEADY_CLOCK\r\n-- Performing Test HAVE_STEADY_CLOCK\r\n-- Performing Test HAVE_STEADY_CLOCK -- success\r\nCMake Warning at cmake/Dependencies.cmake:520 (message):\r\n  A compiler with AVX512 support is required for FBGEMM.  Not compiling with\r\n  FBGEMM.  Turn this warning off by USE_FBGEMM=OFF.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:373 (include)\r\n\r\n\r\n-- Could NOT find Numa (missing: Numa_INCLUDE_DIR Numa_LIBRARIES) \r\nCMake Warning at cmake/Dependencies.cmake:598 (message):\r\n  Not compiling with NUMA.  Suppress this warning with -DUSE_NUMA=OFF\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:373 (include)\r\n\r\n\r\n-- Using third party subdirectory Eigen.\r\nPython 3.7.4\r\n-- Found PythonInterp: /nethome/ebj26/apps/anaconda3/envs/detectron2/bin/python (found suitable version ""3.7.4"", minimum required is ""2.7"") \r\n-- Found PythonLibs: /nethome/ebj26/apps/anaconda3/envs/detectron2/lib/libpython3.7m.so.1.0 (found suitable version ""3.7.4"", minimum required is ""2.7"") \r\n-- Could NOT find pybind11 (missing: pybind11_DIR)\r\n-- Could NOT find pybind11 (missing: pybind11_INCLUDE_DIR) \r\n-- Using third_party/pybind11.\r\n-- Could NOT find MPI_C (missing: MPI_C_LIB_NAMES MPI_C_HEADER_DIR MPI_C_WORKS) \r\n-- Could NOT find MPI_CXX (missing: MPI_CXX_LIB_NAMES MPI_CXX_HEADER_DIR MPI_CXX_WORKS) \r\n-- Could NOT find MPI (missing: MPI_C_FOUND MPI_CXX_FOUND) \r\nCMake Warning at cmake/Dependencies.cmake:831 (message):\r\n  Not compiling with MPI.  Suppress this warning with -DUSE_MPI=OFF\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:373 (include)\r\n\r\n\r\n-- Adding OpenMP CXX_FLAGS: -fopenmp\r\n-- Will link against OpenMP libraries: /share/opt/gcc/5.5.0/lib64/libgomp.so;/usr/lib64/libpthread.so\r\nCMake Warning (dev) at cmake/public/cuda.cmake:29 (find_package):\r\n  Policy CMP0074 is not set: find_package uses <PackageName>_ROOT variables.\r\n  Run ""cmake --help-policy CMP0074"" for policy details.  Use the cmake_policy\r\n  command to set the policy and suppress this warning.\r\n\r\n  Environment variable CUDA_ROOT is set to:\r\n\r\n    /share/apps/cuda/9.1.85\r\n\r\n  For compatibility, CMake is ignoring the variable.\r\nCall Stack (most recent call first):\r\n  cmake/Dependencies.cmake:896 (include)\r\n  CMakeLists.txt:373 (include)\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\n-- Found CUDA: /share/apps/cuda/9.1.85 (found version ""9.1"") \r\n-- Caffe2: CUDA detected: 9.1\r\n-- Caffe2: CUDA nvcc is: /share/apps/cuda/9.1.85/bin/nvcc\r\n-- Caffe2: CUDA toolkit directory: /share/apps/cuda/9.1.85\r\n-- Caffe2: Header version is: 9.1\r\n-- Found CUDNN: /share/apps/cuda/9.1.85/lib64/libcudnn.so  \r\n-- Found cuDNN: v7.1.2  (include: /share/apps/cuda/9.1.85/include, library: /share/apps/cuda/9.1.85/lib64/libcudnn.so)\r\nCMake Warning at cmake/public/utils.cmake:172 (message):\r\n  In the future we will require one to explicitly pass TORCH_CUDA_ARCH_LIST\r\n  to cmake instead of implicitly setting it as an env variable.  This will\r\n  become a FATAL_ERROR in future version of pytorch.\r\nCall Stack (most recent call first):\r\n  cmake/public/cuda.cmake:381 (torch_cuda_get_nvcc_gencode_flag)\r\n  cmake/Dependencies.cmake:896 (include)\r\n  CMakeLists.txt:373 (include)\r\n\r\n\r\n-- Added CUDA NVCC flags for: -gencode;arch=compute_70,code=sm_70\r\nCMake Warning at cmake/public/utils.cmake:172 (message):\r\n  In the future we will require one to explicitly pass TORCH_CUDA_ARCH_LIST\r\n  to cmake instead of implicitly setting it as an env variable.  This will\r\n  become a FATAL_ERROR in future version of pytorch.\r\nCall Stack (most recent call first):\r\n  cmake/External/nccl.cmake:13 (torch_cuda_get_nvcc_gencode_flag)\r\n  cmake/Dependencies.cmake:1012 (include)\r\n  CMakeLists.txt:373 (include)\r\n\r\n\r\n-- Could NOT find CUB (missing: CUB_INCLUDE_DIR) \r\nCMake Warning (dev) at third_party/gloo/CMakeLists.txt:21 (option):\r\n  Policy CMP0077 is not set: option() honors normal variables.  Run ""cmake\r\n  --help-policy CMP0077"" for policy details.  Use the cmake_policy command to\r\n  set the policy and suppress this warning.\r\n\r\n  For compatibility with older versions of CMake, option is clearing the\r\n  normal variable \'BUILD_BENCHMARK\'.\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\nCMake Warning (dev) at third_party/gloo/cmake/Cuda.cmake:133 (find_package):\r\n  Policy CMP0074 is not set: find_package uses <PackageName>_ROOT variables.\r\n  Run ""cmake --help-policy CMP0074"" for policy details.  Use the cmake_policy\r\n  command to set the policy and suppress this warning.\r\n\r\n  Environment variable CUDA_ROOT is set to:\r\n\r\n    /share/apps/cuda/9.1.85\r\n\r\n  For compatibility, CMake is ignoring the variable.\r\nCall Stack (most recent call first):\r\n  third_party/gloo/cmake/Dependencies.cmake:78 (include)\r\n  third_party/gloo/CMakeLists.txt:56 (include)\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\n-- Found CUDA: /share/apps/cuda/9.1.85 (found suitable version ""9.1"", minimum required is ""7.0"") \r\n-- CUDA detected: 9.1\r\n-- Could NOT find NCCL (missing: NCCL_INCLUDE_DIR NCCL_LIBRARY) \r\nCMake Warning at third_party/gloo/cmake/Dependencies.cmake:96 (message):\r\n  Not compiling with NCCL support.  Suppress this warning with\r\n  -DUSE_NCCL=OFF.\r\nCall Stack (most recent call first):\r\n  third_party/gloo/CMakeLists.txt:56 (include)\r\n\r\n\r\nCMake Warning at cmake/Dependencies.cmake:1097 (message):\r\n  Metal is only used in ios builds.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:373 (include)\r\n\r\n\r\nGenerated: /nethome/ebj26/apps/pytorch/build/third_party/onnx/onnx/onnx_onnx_torch-ml.proto\r\nGenerated: /nethome/ebj26/apps/pytorch/build/third_party/onnx/onnx/onnx-operators_onnx_torch-ml.proto\r\n-- \r\n-- ******** Summary ********\r\n--   CMake version         : 3.14.0\r\n--   CMake command         : /nethome/ebj26/apps/anaconda3/envs/detectron2/bin/cmake\r\n--   System                : Linux\r\n--   C++ compiler          : /share/opt/gcc/5.5.0/bin/c++\r\n--   C++ compiler version  : 5.5.0\r\n--   CXX flags             :  -Wno-deprecated -fvisibility-inlines-hidden -fopenmp -Wnon-virtual-dtor\r\n--   Build type            : Release\r\n--   Compile definitions   : TH_BLAS_MKL;ONNX_ML=1\r\n--   CMAKE_PREFIX_PATH     : /nethome/ebj26/apps/anaconda3/envs/detectron2;/share/apps/cuda/9.1.85\r\n--   CMAKE_INSTALL_PREFIX  : /nethome/ebj26/apps/pytorch/torch\r\n--   CMAKE_MODULE_PATH     : /nethome/ebj26/apps/pytorch/cmake/Modules;/nethome/ebj26/apps/pytorch/cmake/public/../Modules_CUDA_fix\r\n-- \r\n--   ONNX version          : 1.6.0\r\n--   ONNX NAMESPACE        : onnx_torch\r\n--   ONNX_BUILD_TESTS      : OFF\r\n--   ONNX_BUILD_BENCHMARKS : OFF\r\n--   ONNX_USE_LITE_PROTO   : OFF\r\n--   ONNXIFI_DUMMY_BACKEND : OFF\r\n--   ONNXIFI_ENABLE_EXT    : OFF\r\n-- \r\n--   Protobuf compiler     : \r\n--   Protobuf includes     : \r\n--   Protobuf libraries    : \r\n--   BUILD_ONNX_PYTHON     : OFF\r\n-- \r\n-- ******** Summary ********\r\n--   CMake version         : 3.14.0\r\n--   CMake command         : /nethome/ebj26/apps/anaconda3/envs/detectron2/bin/cmake\r\n--   System                : Linux\r\n--   C++ compiler          : /share/opt/gcc/5.5.0/bin/c++\r\n--   C++ compiler version  : 5.5.0\r\n--   CXX flags             :  -Wno-deprecated -fvisibility-inlines-hidden -fopenmp -Wnon-virtual-dtor\r\n--   Build type            : Release\r\n--   Compile definitions   : TH_BLAS_MKL;ONNX_ML=1\r\n--   CMAKE_PREFIX_PATH     : /nethome/ebj26/apps/anaconda3/envs/detectron2;/share/apps/cuda/9.1.85\r\n--   CMAKE_INSTALL_PREFIX  : /nethome/ebj26/apps/pytorch/torch\r\n--   CMAKE_MODULE_PATH     : /nethome/ebj26/apps/pytorch/cmake/Modules;/nethome/ebj26/apps/pytorch/cmake/public/../Modules_CUDA_fix\r\n-- \r\n--   ONNX version          : 1.4.1\r\n--   ONNX NAMESPACE        : onnx_torch\r\n--   ONNX_BUILD_TESTS      : OFF\r\n--   ONNX_BUILD_BENCHMARKS : OFF\r\n--   ONNX_USE_LITE_PROTO   : OFF\r\n--   ONNXIFI_DUMMY_BACKEND : OFF\r\n-- \r\n--   Protobuf compiler     : \r\n--   Protobuf includes     : \r\n--   Protobuf libraries    : \r\n--   BUILD_ONNX_PYTHON     : OFF\r\n-- Found CUDA with FP16 support, compiling with torch.cuda.HalfTensor\r\n-- Removing -DNDEBUG from compile flags\r\n-- MAGMA not found. Compiling without MAGMA support\r\n-- Could not find hardware support for NEON on this machine.\r\n-- No OMAP3 processor on this machine.\r\n-- No OMAP4 processor on this machine.\r\n-- Looking for cpuid.h\r\n-- Looking for cpuid.h - found\r\n-- Performing Test HAVE_GCC_GET_CPUID\r\n-- Performing Test HAVE_GCC_GET_CPUID - Success\r\n-- Performing Test NO_GCC_EBX_FPIC_BUG\r\n-- Performing Test NO_GCC_EBX_FPIC_BUG - Success\r\n-- Performing Test C_HAS_AVX_1\r\n-- Performing Test C_HAS_AVX_1 - Failed\r\n-- Performing Test C_HAS_AVX_2\r\n-- Performing Test C_HAS_AVX_2 - Success\r\n-- Performing Test C_HAS_AVX2_1\r\n-- Performing Test C_HAS_AVX2_1 - Failed\r\n-- Performing Test C_HAS_AVX2_2\r\n-- Performing Test C_HAS_AVX2_2 - Failed\r\n-- Performing Test C_HAS_AVX2_3\r\n-- Performing Test C_HAS_AVX2_3 - Failed\r\n-- Performing Test CXX_HAS_AVX_1\r\n-- Performing Test CXX_HAS_AVX_1 - Failed\r\n-- Performing Test CXX_HAS_AVX_2\r\n-- Performing Test CXX_HAS_AVX_2 - Success\r\n-- Performing Test CXX_HAS_AVX2_1\r\n-- Performing Test CXX_HAS_AVX2_1 - Failed\r\n-- Performing Test CXX_HAS_AVX2_2\r\n-- Performing Test CXX_HAS_AVX2_2 - Failed\r\n-- Performing Test CXX_HAS_AVX2_3\r\n-- Performing Test CXX_HAS_AVX2_3 - Failed\r\n-- AVX compiler support found\r\n-- Performing Test BLAS_F2C_DOUBLE_WORKS\r\n-- Performing Test BLAS_F2C_DOUBLE_WORKS - Failed\r\n-- Performing Test BLAS_F2C_FLOAT_WORKS\r\n-- Performing Test BLAS_F2C_FLOAT_WORKS - Success\r\n-- Performing Test BLAS_USE_CBLAS_DOT\r\n-- Performing Test BLAS_USE_CBLAS_DOT - Success\r\n-- Found a library with BLAS API (mkl).\r\n-- Found a library with LAPACK API (mkl).\r\ndisabling ROCM because NOT USE_ROCM is set\r\n-- MIOpen not found. Compiling without MIOpen support\r\ndisabling MKLDNN because USE_MKLDNN is not set\r\n-- Looking for clock_gettime in rt\r\n-- Looking for clock_gettime in rt - found\r\n-- Looking for mmap\r\n-- Looking for mmap - found\r\n-- Looking for shm_open\r\n-- Looking for shm_open - found\r\n-- Looking for shm_unlink\r\n-- Looking for shm_unlink - found\r\n-- Looking for malloc_usable_size\r\n-- Looking for malloc_usable_size - found\r\n-- Performing Test C_HAS_THREAD\r\n-- Performing Test C_HAS_THREAD - Success\r\n-- GCC 5.5.0: Adding gcc and gcc_s libs to link line\r\n-- don\'t use NUMA\r\n-- Found OpenSSL: /nethome/ebj26/apps/anaconda3/envs/detectron2/lib/libcrypto.so (found version ""1.1.1d"")  \r\n-- Check size of long double\r\n-- Check size of long double - done\r\n-- Performing Test COMPILER_SUPPORTS_LONG_DOUBLE\r\n-- Performing Test COMPILER_SUPPORTS_LONG_DOUBLE - Success\r\n-- Performing Test COMPILER_SUPPORTS_FLOAT128\r\n-- Performing Test COMPILER_SUPPORTS_FLOAT128 - Success\r\n-- Performing Test COMPILER_SUPPORTS_SSE2\r\n-- Performing Test COMPILER_SUPPORTS_SSE2 - Success\r\n-- Performing Test COMPILER_SUPPORTS_SSE4\r\n-- Performing Test COMPILER_SUPPORTS_SSE4 - Success\r\n-- Performing Test COMPILER_SUPPORTS_AVX\r\n-- Performing Test COMPILER_SUPPORTS_AVX - Success\r\n-- Performing Test COMPILER_SUPPORTS_FMA4\r\n-- Performing Test COMPILER_SUPPORTS_FMA4 - Success\r\n-- Performing Test COMPILER_SUPPORTS_AVX2\r\n-- Performing Test COMPILER_SUPPORTS_AVX2 - Failed\r\n-- Performing Test COMPILER_SUPPORTS_AVX512F\r\n-- Performing Test COMPILER_SUPPORTS_AVX512F - Failed\r\n-- Found OpenMP_C: -fopenmp (found version ""4.0"") \r\n-- Found OpenMP_CXX: -fopenmp (found version ""4.0"") \r\n-- Found OpenMP: TRUE (found version ""4.0"")  \r\n-- Performing Test COMPILER_SUPPORTS_OPENMP\r\n-- Performing Test COMPILER_SUPPORTS_OPENMP - Success\r\n-- Performing Test COMPILER_SUPPORTS_WEAK_ALIASES\r\n-- Performing Test COMPILER_SUPPORTS_WEAK_ALIASES - Success\r\n-- Performing Test COMPILER_SUPPORTS_BUILTIN_MATH\r\n-- Performing Test COMPILER_SUPPORTS_BUILTIN_MATH - Success\r\n-- Performing Test COMPILER_SUPPORTS_SYS_GETRANDOM\r\n-- Performing Test COMPILER_SUPPORTS_SYS_GETRANDOM - Failed\r\n-- Configuring build for SLEEF-v3.4.0\r\n   Target system: Linux-2.6.32-431.el6.x86_64\r\n   Target processor: x86_64\r\n   Host system: Linux-2.6.32-431.el6.x86_64\r\n   Host processor: x86_64\r\n   Detected C compiler: GNU @ /share/opt/gcc/5.5.0/bin/gcc\r\n-- Using option ', "" to compile libsleef\r\n-- Building shared libs : OFF\r\n-- MPFR : LIB_MPFR-NOTFOUND\r\n-- GMP : /usr/lib64/libgmp.so\r\n-- RT : /usr/lib64/librt.so\r\n-- FFTW3 : LIBFFTW3-NOTFOUND\r\n-- OPENSSL : 1.1.1d\r\n-- SDE : SDE_COMMAND-NOTFOUND\r\n-- RUNNING_ON_TRAVIS : 0\r\n-- COMPILER_SUPPORTS_OPENMP : 1\r\nAT_INSTALL_INCLUDE_DIR include/ATen/core\r\ncore header install: /nethome/ebj26/apps/pytorch/build/aten/src/ATen/core/TensorBody.h\r\ncore header install: /nethome/ebj26/apps/pytorch/build/aten/src/ATen/core/TensorMethods.h\r\n-- Include NCCL operators\r\n-- Excluding ideep operators as we are not using ideep\r\n-- Excluding image processing operators due to no opencv\r\n-- Excluding video processing operators due to no opencv\r\n-- MPI operators skipped due to no MPI support\r\n-- Include Observer library\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cudnn/AffineGridGenerator.cpp\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cudnn/BatchNorm.cpp\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cudnn/Conv.cpp\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cudnn/GridSampler.cpp\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cudnn/LossCTC.cpp\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cudnn/RNN.cpp\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/miopen/BatchNorm_miopen.cpp\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/miopen/RNN_miopen.cpp\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cpp\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/cuda/CUDABlas.cpp\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/cuda/CUDAContext.cpp\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/cuda/PinnedMemoryAllocator.cpp\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/CUDAUnaryOps.cpp\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp\r\n--   /nethome/ebj26/apps/pytorch/build/aten/src/ATen/CUDAType.cpp\r\n--   /nethome/ebj26/apps/pytorch/build/aten/src/ATen/CUDAType.h\r\n--   /nethome/ebj26/apps/pytorch/build/aten/src/ATen/LegacyTHFunctionsCUDA.cpp\r\n--   /nethome/ebj26/apps/pytorch/build/aten/src/ATen/LegacyTHFunctionsCUDA.h\r\n--   /nethome/ebj26/apps/pytorch/build/aten/src/ATen/SparseCUDAType.cpp\r\n--   /nethome/ebj26/apps/pytorch/build/aten/src/ATen/SparseCUDAType.h\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/THCCachingHostAllocator.cpp\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/THCGeneral.cpp\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/THCStorageCopy.cpp\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/THCTensor.cpp\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/THCReduceApplyUtils.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/THCBlas.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/THCSleep.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/THCStorage.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/THCStorageCopy.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/THCTensor.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/THCTensorCopy.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/THCTensorMath.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/THCTensorMathBlas.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/THCTensorMathMagma.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/THCTensorMathPairwise.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/THCTensorMathReduce.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/THCTensorMathScan.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/THCTensorIndex.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/THCTensorRandom.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/THCTensorScatterGather.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/THCTensorTopK.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/THCTensorSort.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/THCSortUtils.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/THCTensorMode.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorSortByte.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathCompareTByte.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathPointwiseByte.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathCompareByte.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathReduceByte.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMaskedByte.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorSortChar.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathCompareTChar.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathPointwiseChar.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathCompareChar.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathReduceChar.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMaskedChar.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorSortShort.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathCompareTShort.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathPointwiseShort.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathCompareShort.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathReduceShort.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMaskedShort.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorSortInt.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathCompareTInt.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathPointwiseInt.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathCompareInt.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathReduceInt.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMaskedInt.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorSortLong.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathCompareTLong.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathPointwiseLong.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathCompareLong.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathReduceLong.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMaskedLong.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorSortHalf.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathCompareTHalf.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathPointwiseHalf.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathCompareHalf.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathReduceHalf.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMaskedHalf.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorSortFloat.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathCompareTFloat.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathPointwiseFloat.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathCompareFloat.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathReduceFloat.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMaskedFloat.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorSortDouble.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathCompareTDouble.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathPointwiseDouble.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathCompareDouble.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathReduceDouble.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMaskedDouble.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathCompareTBool.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathCompareBool.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathReduceBool.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMaskedBool.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THC/generated/THCTensorMathPointwiseBool.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THCUNN/AbsCriterion.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THCUNN/BCECriterion.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THCUNN/ELU.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THCUNN/GatedLinearUnit.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THCUNN/HardTanh.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THCUNN/LeakyReLU.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THCUNN/LogSigmoid.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THCUNN/MSECriterion.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THCUNN/MultiLabelMarginCriterion.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THCUNN/MultiMarginCriterion.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THCUNN/RReLU.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THCUNN/Sigmoid.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THCUNN/SmoothL1Criterion.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THCUNN/SoftMarginCriterion.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THCUNN/SoftPlus.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THCUNN/SoftShrink.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THCUNN/SpatialClassNLLCriterion.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THCUNN/SpatialConvolutionMM.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THCUNN/SpatialDepthwiseConvolution.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/THCUNN/Tanh.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/cuda/detail/IndexUtils.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/Activation.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/AveragePool2d.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/AveragePool3d.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/BatchLinearAlgebra.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/BinaryOpsKernel.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/CUDAScalar.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/Col2Im.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/Copy.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/CrossKernel.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/DilatedMaxPool2d.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/DilatedMaxPool3d.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/DistanceKernel.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/Distributions.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/Dropout.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/Embedding.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/EmbeddingBag.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/FillKernel.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/FractionalMaxPool2d.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/FractionalMaxPool3d.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/GridSampler.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/Im2Col.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/Indexing.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/Lerp.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/LinearAlgebra.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/Loss.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/LossCTC.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/MaxUnpooling.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/MultinomialKernel.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/NaiveConvolutionTranspose3d.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/NaiveDilatedConvolution.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/Normalization.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/PointwiseOpsKernel.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/PowKernel.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/RNN.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/RangeFactories.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/Reduce.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/ReduceOpsKernel.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/ReflectionPad.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/Repeat.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/ReplicationPadding.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/Resize.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/SoftMax.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/SortingKthValue.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/SparseMM.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/SpectralOps.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/SummaryOps.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/TensorCompare.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/TensorFactories.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/TensorTransformations.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/UnaryOpsKernel.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/Unique.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/UpSampleBicubic2d.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/UpSampleBilinear2d.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/UpSampleLinear1d.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/UpSampleNearest1d.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/UpSampleNearest2d.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/UpSampleNearest3d.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/WeightNorm.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/cuda/layer_norm_kernel.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/sparse/cuda/SparseCUDABlas.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/quantized/cuda/fake_quantize_core.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/quantized/cuda/fake_quantize_per_channel_affine.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/native/quantized/cuda/fake_quantize_per_tensor_affine.cu\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/cudnn/Descriptors.cpp\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/cudnn/Handle.cpp\r\n--   /nethome/ebj26/apps/pytorch/aten/src/ATen/cudnn/Types.cpp\r\n--   /nethome/ebj26/apps/pytorch/caffe2/core/common_cudnn.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/core/blob_serialization_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/core/common_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/core/event_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/core/context_gpu.cu\r\n--   utils/math/broadcast.cu\r\n--   utils/math/elementwise.cu\r\n--   utils/math/reduce.cu\r\n--   utils/math/transpose.cu\r\n--   utils/math_gpu.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/contrib/aten/aten_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/contrib/nccl/cuda_nccl_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/contrib/nccl/cuda_nccl_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/contrib/gloo/allreduce_ops_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/contrib/gloo/broadcast_ops_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/contrib/gloo/common_world_ops_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/db/create_db_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/distributed/file_store_handler_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/channelwise_conv3d_op_cudnn.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/conv_op_cache_cudnn.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/conv_op_cudnn.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/conv_transpose_op_cudnn.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/depthwise_3x3_conv_op_cudnn.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/dropout_op_cudnn.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/elu_op_cudnn.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/local_response_normalization_op_cudnn.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/order_switch_ops_cudnn.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/pool_op_cudnn.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/sigmoid_op_cudnn.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/softmax_op_cudnn.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/spatial_batch_norm_op_cudnn.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/tanh_op_cudnn.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/transpose_op_cudnn.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/communicator_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/concat_split_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/conv_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/conv_op_shared_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/conv_transpose_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/counter_ops_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/do_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/elementwise_add_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/elementwise_sub_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/exp_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/expand_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/expand_squeeze_dims_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/free_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/fully_connected_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/if_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/im2col_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/load_save_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/locally_connected_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/log_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/matmul_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/negate_gradient_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/negative_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/order_switch_ops_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/prepend_dim_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/reshape_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/scale_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/shape_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/sqr_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/sqrt_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/stop_gradient_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/tensor_protos_db_input_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/while_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/zero_gradient_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/abs_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/accumulate_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/accuracy_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/acos_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/affine_channel_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/arg_ops.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/asin_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/assert_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/atan_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/batch_gather_ops.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/batch_matmul_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/batch_moments_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/boolean_mask_ops.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/boolean_unmask_ops.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/bucketize_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/cast_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/cbrt_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/ceil_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/channel_backprop_stats_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/channel_shuffle_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/channel_stats_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/clip_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/copy_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/cos_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/cosh_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/cosine_embedding_criterion_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/cross_entropy_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/cube_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/data_couple_gpu.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/deform_conv_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/distance_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/dropout_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/elementwise_div_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/elementwise_linear_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/elementwise_mul_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/elementwise_ops.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/elu_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/enforce_finite_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/ensure_cpu_output_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/erf_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/filler_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/find_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/floor_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/gather_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/gelu_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/generate_proposals_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/generate_proposals_op_util_nms_gpu.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/given_tensor_byte_string_to_uint8_fill_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/given_tensor_fill_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/glu_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/group_norm_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/gru_unit_op_gpu.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/half_float_ops.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/hard_sigmoid_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/instance_norm_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/integral_image_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/layer_norm_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/leaky_relu_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/lengths_pad_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/lengths_tile_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/local_response_normalization_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/logit_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/loss_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/lp_pool_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/lstm_unit_op_gpu.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/margin_ranking_criterion_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/max_pool_with_index.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/mean_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/mem_query_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/minmax_ops.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/moments_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/multi_class_accuracy_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/normalize_ops.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/one_hot_ops.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/pack_segments.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/pad_op_gpu.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/perplexity_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/piecewise_linear_transform_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/pool_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/pow_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/prelu_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/reciprocal_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/reduce_front_back_max_ops.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/reduce_front_back_sum_mean_ops.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/reduce_ops.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/reduction_ops.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/relu_n_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/relu_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/replace_nan_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/resize_3d_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/resize_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/reverse_packed_segs_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/rmac_regions_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/roi_align_gradient_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/roi_align_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/roi_align_rotated_gradient_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/roi_align_rotated_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/roi_pool_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/rsqrt_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/scale_blobs_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/segment_reduction_op_gpu.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/selu_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/sequence_ops.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/sigmoid_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/sin_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/sinh_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/slice_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/softmax_ops.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/softplus_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/softsign_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/space_batch_op_gpu.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/sparse_normalize_op_gpu.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/sparse_to_dense_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/spatial_batch_norm_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/stump_func_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/summarize_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/swish_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/tan_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/tanh_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/thresholded_relu_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/tile_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/top_k.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/transpose_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/unique_ops.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/upsample_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/utility_ops.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/weighted_sample_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/rnn/recurrent_op_cudnn.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/rnn/recurrent_network_blob_fetcher_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/rnn/recurrent_network_executor_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/operators/rnn/recurrent_network_op_gpu.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/queue/queue_ops_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/sgd/iter_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/sgd/learning_rate_op_gpu.cc\r\n--   /nethome/ebj26/apps/pytorch/caffe2/sgd/adadelta_op_gpu.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/sgd/adagrad_op_gpu.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/sgd/adam_op_gpu.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/sgd/fp16_momentum_sgd_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/sgd/fp32_momentum_sgd_op.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/sgd/lars_op_gpu.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/sgd/momentum_sgd_op_gpu.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/sgd/rmsprop_op_gpu.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/sgd/yellowfin_op_gpu.cu\r\n--   /nethome/ebj26/apps/pytorch/caffe2/../torch/csrc/jit/fuser/cuda/fused_kernel.cpp\r\n--   /nethome/ebj26/apps/pytorch/caffe2/../torch/csrc/autograd/profiler_cuda.cpp\r\n--   /nethome/ebj26/apps/pytorch/caffe2/../torch/csrc/autograd/functions/comm.cpp\r\n--   /nethome/ebj26/apps/pytorch/caffe2/../torch/csrc/cuda/comm.cpp\r\n-- /share/opt/gcc/5.5.0/bin/c++ /nethome/ebj26/apps/pytorch/caffe2/../torch/abi-check.cpp -o /nethome/ebj26/apps/pytorch/build/abi-check\r\n-- Determined _GLIBCXX_USE_CXX11_ABI=1\r\n-- pytorch is compiling with OpenMP. \r\nOpenMP CXX_FLAGS: -fopenmp. \r\nOpenMP libraries: /share/opt/gcc/5.5.0/lib64/libgomp.so;/usr/lib64/libpthread.so.\r\n-- Caffe2 is compiling with OpenMP. \r\nOpenMP CXX_FLAGS: -fopenmp. \r\nOpenMP libraries: /share/opt/gcc/5.5.0/lib64/libgomp.so;/usr/lib64/libpthread.so.\r\n-- Using ATen parallel backend: OMP\r\n-- Using lib/python3.7/site-packages as python relative installation path\r\nCMake Warning at CMakeLists.txt:576 (message):\r\n  Generated cmake files are only fully tested if one builds with system glog,\r\n  gflags, and protobuf.  Other settings may generate files that are not well\r\n  tested.\r\n\r\n\r\n-- \r\n-- ******** Summary ********\r\n-- General:\r\n--   CMake version         : 3.14.0\r\n--   CMake command         : /nethome/ebj26/apps/anaconda3/envs/detectron2/bin/cmake\r\n--   System                : Linux\r\n--   C++ compiler          : /share/opt/gcc/5.5.0/bin/c++\r\n--   C++ compiler id       : GNU\r\n--   C++ compiler version  : 5.5.0\r\n--   BLAS                  : MKL\r\n--   CXX flags             :  -Wno-deprecated -fvisibility-inlines-hidden -fopenmp -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math\r\n--   Build type            : Release\r\n--   Compile definitions   : TH_BLAS_MKL;ONNX_ML=1;ONNX_NAMESPACE=onnx_torch;HAVE_MMAP=1;_FILE_OFFSET_BITS=64;HAVE_SHM_OPEN=1;HAVE_SHM_UNLINK=1;HAVE_MALLOC_USABLE_SIZE=1\r\n--   CMAKE_PREFIX_PATH     : /nethome/ebj26/apps/anaconda3/envs/detectron2;/share/apps/cuda/9.1.85\r\n--   CMAKE_INSTALL_PREFIX  : /nethome/ebj26/apps/pytorch/torch\r\n-- \r\n--   TORCH_VERSION         : 1.4.0\r\n--   CAFFE2_VERSION        : 1.4.0\r\n--   BUILD_CAFFE2_MOBILE   : ON\r\n--   USE_STATIC_DISPATCH   : OFF\r\n--   BUILD_BINARY          : OFF\r\n--   BUILD_CUSTOM_PROTOBUF : ON\r\n--     Link local protobuf : ON\r\n--   BUILD_DOCS            : OFF\r\n--   BUILD_PYTHON          : True\r\n--     Python version      : 3.7.4\r\n--     Python executable   : /nethome/ebj26/apps/anaconda3/envs/detectron2/bin/python\r\n--     Pythonlibs version  : 3.7.4\r\n--     Python library      : /nethome/ebj26/apps/anaconda3/envs/detectron2/lib/libpython3.7m.so.1.0\r\n--     Python includes     : /nethome/ebj26/apps/anaconda3/envs/detectron2/include/python3.7m\r\n--     Python site-packages: lib/python3.7/site-packages\r\n--   BUILD_CAFFE2_OPS      : ON\r\n--   BUILD_SHARED_LIBS     : ON\r\n--   BUILD_TEST            : True\r\n--   INTERN_BUILD_MOBILE   : \r\n--   USE_ASAN              : OFF\r\n--   USE_CUDA              : True\r\n--     CUDA static link    : OFF\r\n--     USE_CUDNN           : ON\r\n--     CUDA version        : 9.1\r\n--     cuDNN version       : 7.1.2\r\n--     CUDA root directory : /share/apps/cuda/9.1.85\r\n--     CUDA library        : /share/apps/cuda/9.1.85/lib64/stubs/libcuda.so\r\n--     cudart library      : /share/apps/cuda/9.1.85/lib64/libcudart.so\r\n--     cublas library      : /share/apps/cuda/9.1.85/lib64/libcublas.so;/share/apps/cuda/9.1.85/lib64/libcublas_device.a\r\n--     cufft library       : /share/apps/cuda/9.1.85/lib64/libcufft.so\r\n--     curand library      : /share/apps/cuda/9.1.85/lib64/libcurand.so\r\n--     cuDNN library       : /share/apps/cuda/9.1.85/lib64/libcudnn.so\r\n--     nvrtc               : /share/apps/cuda/9.1.85/lib64/libnvrtc.so\r\n--     CUDA include path   : /share/apps/cuda/9.1.85/include\r\n--     NVCC executable     : /share/apps/cuda/9.1.85/bin/nvcc\r\n--     CUDA host compiler  : /share/opt/gcc/5.5.0/bin/gcc\r\n--     USE_TENSORRT        : OFF\r\n--   USE_ROCM              : OFF\r\n--   USE_EIGEN_FOR_BLAS    : \r\n--   USE_FBGEMM            : OFF\r\n--   USE_FFMPEG            : OFF\r\n--   USE_GFLAGS            : OFF\r\n--   USE_GLOG              : OFF\r\n--   USE_LEVELDB           : OFF\r\n--   USE_LITE_PROTO        : OFF\r\n--   USE_LMDB              : OFF\r\n--   USE_METAL             : OFF\r\n--   USE_MKL               : ON\r\n--   USE_MKLDNN            : OFF\r\n--   USE_NCCL              : ON\r\n--     USE_SYSTEM_NCCL     : OFF\r\n--   USE_NNPACK            : ON\r\n--   USE_NUMPY             : ON\r\n--   USE_OBSERVERS         : ON\r\n--   USE_OPENCL            : OFF\r\n--   USE_OPENCV            : OFF\r\n--   USE_OPENMP            : ON\r\n--   USE_TBB               : OFF\r\n--   USE_PROF              : OFF\r\n--   USE_QNNPACK           : ON\r\n--   USE_REDIS             : OFF\r\n--   USE_ROCKSDB           : OFF\r\n--   USE_ZMQ               : OFF\r\n--   USE_DISTRIBUTED       : ON\r\n--     USE_MPI             : OFF\r\n--     USE_GLOO            : ON\r\n--   BUILD_NAMEDTENSOR   : OFF\r\n--   Public Dependencies  : Threads::Threads;caffe2::mkl\r\n--   Private Dependencies : qnnpack;pytorch_qnnpack;nnpack;cpuinfo;fp16;gloo;aten_op_header_gen;foxi_loader;rt;gcc_s;gcc;dl\r\n-- Configuring done\r\nCMake Warning at cmake/Modules_CUDA_fix/upstream/FindCUDA.cmake:1844 (add_library):\r\n  Cannot generate a safe linker search path for target torch because files in\r\n  some directories may conflict with libraries in implicit directories:\r\n\r\n    link library [libgomp.so] in /share/opt/gcc/5.5.0/lib64 may be hidden by files in:\r\n      /nethome/ebj26/apps/anaconda3/envs/detectron2/lib\r\n\r\n  Some of these libraries may not be found correctly.\r\nCall Stack (most recent call first):\r\n  caffe2/CMakeLists.txt:627 (cuda_add_library)\r\n\r\n\r\nCMake Warning at cmake/Modules_CUDA_fix/upstream/FindCUDA.cmake:1844 (add_library):\r\n  Cannot generate a safe runtime search path for target torch because files\r\n  in some directories may conflict with libraries in implicit directories:\r\n\r\n    runtime library [libgomp.so.1] in /share/opt/gcc/5.5.0/lib64 may be hidden by files in:\r\n      /nethome/ebj26/apps/anaconda3/envs/detectron2/lib\r\n\r\n  Some of these libraries may not be found correctly.\r\nCall Stack (most recent call first):\r\n  caffe2/CMakeLists.txt:627 (cuda_add_library)\r\n\r\n\r\nCMake Warning at cmake/Modules_CUDA_fix/upstream/FindCUDA.cmake:1844 (add_library):\r\n  Cannot generate a safe linker search path for target\r\n  caffe2_detectron_ops_gpu because files in some directories may conflict\r\n  with libraries in implicit directories:\r\n\r\n    link library [libgomp.so] in /share/opt/gcc/5.5.0/lib64 may be hidden by files in:\r\n      /nethome/ebj26/apps/anaconda3/envs/detectron2/lib\r\n\r\n  Some of these libraries may not be found correctly.\r\nCall Stack (most recent call first):\r\n  modules/detectron/CMakeLists.txt:13 (CUDA_ADD_LIBRARY)\r\n\r\n\r\nCMake Warning at cmake/Modules_CUDA_fix/upstream/FindCUDA.cmake:1844 (add_library):\r\n  Cannot generate a safe runtime search path for target\r\n  caffe2_detectron_ops_gpu because files in some directories may conflict\r\n  with libraries in implicit directories:\r\n\r\n    runtime library [libgomp.so.1] in /share/opt/gcc/5.5.0/lib64 may be hidden by files in:\r\n      /nethome/ebj26/apps/anaconda3/envs/detectron2/lib\r\n\r\n  Some of these libraries may not be found correctly.\r\nCall Stack (most recent call first):\r\n  modules/detectron/CMakeLists.txt:13 (CUDA_ADD_LIBRARY)\r\n\r\n\r\n-- Generating done\r\n-- Build files have been written to: /nethome/ebj26/apps/pytorch/build\r\ncmake --build . --target install --config Release -- -j 16\r\n[185/3081] Performing build step for 'nccl_external'\r\nFAILED: nccl_external-prefix/src/nccl_external-stamp/nccl_external-build nccl/lib/libnccl_static.a \r\ncd /nethome/ebj26/apps/pytorch/third_party/nccl/nccl && env CCACHE_DISABLE=1 SCCACHE_DISABLE=1 make CXX=/share/opt/gcc/5.5.0/bin/c++ CUDA_HOME=/share/apps/cuda/9.1.85 NVCC=/share/apps/cuda/9.1.85/bin/nvcc NVCC_GENCODE=-gencode=arch=compute_70,code=sm_70 BUILDDIR=/nethome/ebj26/apps/pytorch/build/nccl VERBOSE=0 -j && /nethome/ebj26/apps/anaconda3/envs/detectron2/bin/cmake -E touch /nethome/ebj26/apps/pytorch/build/nccl_external-prefix/src/nccl_external-stamp/nccl_external-build\r\nmake -C src build BUILDDIR=/nethome/ebj26/apps/pytorch/build/nccl\r\nmake[1]: Entering directory "", ""/nethome/ebj26/apps/pytorch/third_party/nccl/nccl/src/collectives/device'\r\nGenerating rules                               > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/Makefile.rules\r\nmake[2]: Leaving directory "", ""/nethome/ebj26/apps/pytorch/third_party/nccl/nccl/src/collectives/device'\r\nCompiling  functions.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/functions.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_min_u64.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_prod_u32.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_sum_f16.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_sum_f64.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_max_f32.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_min_f64.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_min_i32.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_sum_i8.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_prod_f32.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_prod_i8.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_prod_f64.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_min_i8.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_prod_i64.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_sum_f32.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_sum_i32.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_sum_u8.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_max_u64.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_sum_u64.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_min_u32.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_min_u8.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_min_f32.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_min_f16.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_prod_i32.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_prod_u64.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_prod_f16.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_prod_u8.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_max_i8.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_max_f64.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_sum_u32.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_sum_i64.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_max_f16.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_max_u8.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_max_i64.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_max_u32.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_max_i32.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_prod_i32.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_sum_i32.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_min_i64.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_min_u8.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_prod_u8.o\r\nCompiling  reduce_scatter.cu                   > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_scatter_max_i64.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_min_u32.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_max_u8.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_min_i64.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_sum_i8.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_min_u64.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_max_f64.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_min_f32.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_prod_i8.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_prod_i32.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_prod_u32.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_max_u64.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_max_i8.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_sum_f16.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_max_i32.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_max_i64.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_sum_i64.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_max_i8.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_sum_f64.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_min_f64.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_prod_u32.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_min_i32.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_max_i32.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_max_u32.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_min_f32.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_prod_i64.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_min_u32.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_sum_u32.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_min_i32.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_min_u64.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_sum_u8.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_sum_i32.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_max_f16.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_sum_f64.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_max_u32.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_min_u8.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_max_f32.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_prod_f64.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_min_f16.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_min_i64.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_sum_u64.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_sum_i32.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_sum_i64.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_sum_i8.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_min_u32.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_max_f32.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_prod_f64.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_sum_i8.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_prod_u64.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_prod_i8.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_max_u8.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_sum_i32.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_max_i32.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_max_f32.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_max_f16.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_prod_i32.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_max_i8.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_min_f64.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_sum_f16.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_prod_f32.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_sum_u64.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_min_i64.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_sum_f64.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_min_u32.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_sum_i8.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_max_u64.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_sum_f32.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_prod_u8.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_min_f16.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_max_f64.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_max_u64.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_prod_u8.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_sum_u32.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_prod_i64.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_max_i32.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_min_u64.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_sum_u32.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_prod_f32.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_prod_u64.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_min_f16.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_min_i32.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_max_f64.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_sum_f32.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_max_f32.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_sum_f32.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_prod_f16.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_min_f64.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_max_i64.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_prod_f32.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_prod_f16.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_max_u8.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_min_i8.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_prod_f64.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_prod_u32.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_min_u8.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_sum_f16.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_min_f32.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_max_u8.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_sum_f16.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_prod_i32.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_min_u64.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_prod_f16.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_sum_u8.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_sum_f64.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_prod_f16.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_prod_u64.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_min_i32.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_sum_i64.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_sum_u64.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_min_i8.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_prod_i8.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_prod_u8.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_max_u32.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_min_i8.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_max_u32.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_min_u8.o\r\nCompiling  all_reduce.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_reduce_prod_i8.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_sum_u8.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_min_f16.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_prod_i64.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_prod_u64.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_max_f64.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_sum_i64.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_max_i8.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_max_f16.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_sum_u64.o\r\nCompiling  broadcast.cu                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/broadcast_sum_f32.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_sum_u8.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_sum_u32.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_min_i64.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_min_f64.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_max_i64.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_prod_f64.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_max_f16.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_max_u64.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_min_i8.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_min_f32.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_prod_i64.o\r\nCompiling  reduce.cu                           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/reduce_prod_f32.o\r\nCompiling  all_gather.cu                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/all_gather_prod_u32.o\r\nArchiving  objects                             > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/device/colldevice.a\r\nmake[2]: Leaving directory "", '']",0,0
245,pytorch,20402,closed,Tensor and nn.Module Pruning,"## üöÄ Tensor and nn.Module Pruning
Tensor method and/or  util to sparsify tensors and/or model, according to various pruning techniques in the literature. 

## Motivation
State-of-the-art deep learning techniques rely on over-parametrized models that are hard to deploy. On the contrary, biological neural networks are known to use efficient sparse connectivity. It's important to identify best techniques to compress models by reducing the number of parameters in them, in order to reduce memory, battery, and hardware consumption without sacrificing accuracy, deploy lightweight models on device, and guarantee privacy with private on-device computation. On the research front, pruning is used to investigate the differences in learning dynamics of over-parametrized and under-parametrized networks, to study the role of lucky sparse subnetworks and initializations (""lottery tickets"" [[1]](https://arxiv.org/abs/1803.03635)), as a destructive neural architecture search technique, and others.
Goal of this feature: harmonizing pruning practices by providing a standard interface in PyTorch.
Target audience: researchers, engineering and product teams.

## Pitch
Minimalist API, with deeper flexibility for power-users. 

At the tensor level, this could look as follows:

A not-in-place  method will return a  of the same type and size as the one it acts on.
In-place pruning supported via .

At the model level, this will require a bit of thinking but should follow similar API patterns. This is important because not all pruning methods make sense on all parameters in a model (pruning conv kernels != pruning biases != pruning RNNs != pruning in the presence of batch norm, etc.). 
First, we should have a sensible, well-documented default behavior for the average-user's API, where a call to  defaults to pruning PyTorch ""prepackaged"" modules (such as linear, conv, and recurrent layers) in some sensible, expected way. 
Most power users though would probably want to prune custom layers, or prune different layer types or layers at different depths using different pruning methods or pruning method parameters. This could be specified via a dictionary, which maps parameter names (contained in ) to a pruning method and its parameters: 


Similar to the tensor operations, model-level pruning could return a copy of the model, or act on the model in place.

Pruning methods can be used during or post- training; this implementation will be training-loop-agnostic: the user will have to take care of writing their own training loop to decide when to prune and what to do with the pruned object (re-initialize and retrain, finetune, etc.).

## Alternatives
Depending on where this will live within the codebase,  could also look like:  or . I personally would prefer the first option because the kwargs are parameters of the pruning method itself, while  is the tensor it acts on (some pruning methods will also have to take in some data  or  when they're applied), but the last option is more in line with how, say,  is implemented. Perhaps, for Module-level application, following the [example of the  implementation](https://pytorch.org/docs/stable/_modules/torch/nn/utils/weight_norm.html) using hooks will make this more PyTorch-y, but I don't know if we want to sacrifice the ability to act directly on a tensor that is not part of a Module. Would that go into ? Open to suggestions here.


cc @albanD @mruberry @jbschlosser",feature module: nn module: pruning triaged,"['cc @soumith. This sounds generally reasonable, but I am a little wary about signing up to figure out what ""reasonable default pruning"" behavior is for every module. Maybe this could live out of tree for the short term, and we can assess as people get more experience with what is 100%, obviously useful functionality, and what is a bit more opinion based and changes as research evolves?', 'Some other comments that I had in private conversation with @mickypaganini:\r\n\r\n* There\'s probably some API bikeshedding to do on the exact signature of `prune` (and whether or not it lives as a method or function). All of these possibilities are easy to implement, we just have to decide what to do. One non-obvious sticking point is that writing binding code for operators which take ""enums"" (like the string L1/L1Structured) is a bit more involved, and so we should also consider just having separate functions/methods for each pruning mechanism (unless there are other design reasons why having it as an enum is a good idea; for example, there are other functions that need to use the same enum, or if users will want to parametrize over pruning strategy uniformly over many calls to the prune method)\r\n*  If you do need to implement some actual CPU/CUDA kernels, it is probably easier to contribute them straight to PyTorch repository\r\n* For a specialized topic like this, it is very helpful to have someone who steps up to do continual maintenance and bugfixes for it\r\n* PyTorch prefers to put ""obviously good ideas"" in the library, and leave people the space to experiment\r\n\r\nMore motivation for merging directly to PyTorch, from Michela:\r\n\r\n> The only issue with [an external library for pruning] is that it\'ll become the (N+1)th solution for pruning models. I don\'t think anybody needs that. What we need is one centralized canonical way of doing it\r\n\r\nSome other reference material:\r\n* https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-pruning-api-42cac9157a6a', ""Thanks for summarizing. \r\nMain road block now: deciding where tensor pruning will live. \r\n\r\n1. If we go the `nn.utils` route: will the signature of `nn.utils.mypruningfunction` be `nn.utils.mypruningfunction(module, name, **kwargs)` as it's done in the `weight_norm` example I linked? That specific `.apply` method uses hooks, so it assumes that the tensor I want to prune is inside some module which will undergo some forward pass. It won't support independent tensors being pruned outside of this world.\r\n2. To counter this, should we have a separate tensor method? One should be able to prune any given tensor even if they are not part of a module.\r\n\r\nIt feels weird to have the thing that controls pruning application in the forward and backward pass also handle the actual pruning logic implementation. Where should I have that logic live?\r\n\r\n* I'm happy to make each pruning technique a separate function/method and avoid string identifiers. \r\n* I'm also happy to maintain these functionalities long term.\r\n* We can table default module pruning for each specific `nn.Module` kind for a later discussion.\r\n"", ""FYI, there's also temporal (lifetime) sparsity for activations:  \r\nhttps://arxiv.org/abs/1409.2752\r\nhttps://arxiv.org/abs/1903.11257"", 'Also, typically activation pruning is more aggressive during training (might even be disabled during test in some cases). ', ""I started prototyping this idea using simple pruning methods: random and L1-based unstructured pruning, and random and Ln-based structured pruning. These are simple methods that are either random or only depend on the magnitude of the weights. You can [check it out on my fork](https://github.com/mickypaganini/pytorch/blob/pruning/torch/nn/utils/prune.py).\r\n[Thanks @michaelklachko for pointers to other pruning method. We have a long list of candidate methods to implement in future iterations, including activation-based methods, weight evolution methods, etc.]\r\n\r\nPruning is currently located under `torch/nn/utils`, as suggested. The implementation design follows the example of [`weight_norm.py`](https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/weight_norm.py), using forward pre hooks to reparametrize a parameter in terms of the computed mask and the original parameter's value.\r\n\r\nOne would interact with pruning in a similar way as with weight norm, i.e. through functions like [`ln_structured_pruning(module, name, amount, n, axis)`](https://github.com/mickypaganini/pytorch/blob/pruning/torch/nn/utils/prune.py#L721), that applies pruning, and [`remove_pruning(module, name)`](https://github.com/mickypaganini/pytorch/blob/pruning/torch/nn/utils/prune.py#L752) that removes the hook and the reparametrization, and permanently substitutes `module[name]` with the pruned version of the parameter.\r\n\r\nPruning methods can be composed using a `PruningContainer`. This enables iterative pruning.\r\n\r\nI'm still writing tests to ensure correctness. \r\nLooking forward to your feedback!\r\n"", 'Note that pruning operation is similar to quantization. So perhaps the two can be implemented in a similar manner, or even combined. For example, binarizing ReLU activations, and ternarizing weights can be considered a form of pruning. ', ""Here's [one example](https://github.com/eladhoffer/convNet.pytorch/blob/master/models/modules/quantize.py) of how quantization op can be integrated into a layer. In my opinion, it makes sense to implement weight pruning/quantization a wrapper for a layer (e.g. like weightnorm), however for activations it's probably better to have it as a separate layer type (e.g. like batchnorm). "", ""@mickypaganini thanks a ton for the proposal, and your WIP branch.\r\n\r\nI reviewed the overall (new) design using hooks, and focused more around nn.Module rather than tensors themselves. I think this design is less invasive and makes a lot more sense -- because pruning is primarily targeted for weights anyways. It looks great.\r\n\r\nI have some bikeshedding points. I think you'd want to place the code in `nn.utils.prune` and then remove `pruning` from all of the functions that you implemented.\r\nFor example:\r\n\r\n```python\r\n# Current\r\nimport torch.nn.utils as utils\r\n\r\nutils.ln_structured_pruning(...)\r\nutils.remove_pruning(...)\r\n\r\n# New\r\nimport torch.nn.utils.prune as prune\r\n\r\nprune.ln_structured(...)\r\nprune.remove(....)\r\n```\r\n\r\nFor each doc snippet, you probably also want to add `Example:` apart from `Args:` and `Output:`\r\nI'm excited to see this unlock a bunch of pruning research.\r\n\r\n@michaelklachko I disagree that quantization should be combined with pruning because:\r\n- Pruning is experimental in it's research life-cycle, not many think about optimized code for a pruned model for all of the pruning methods that folks are currently doing research on.\r\n- Quantization on the other hand is extremely mature from the scientific computing perspective and in the neural network land -- there are optimized code and algorithms implemented for scale and shift quantization.\r\n- So, treating quantization as a subset of pruning in the interface and code-paths will either limit the interface we expand pruning to, because we want it to also run fast OR will limit the quantization to be of limited performance.\r\nSo, I think I prefer quantization to be a separate, fully-fleshed out interface, as is being implemented in https://github.com/pytorch/pytorch/issues/18318 instead of being conflated with more exotic pruning techniques.\r\n"", ""@mickypaganini one thing to keep in mind though is that using forward hooks has it's limitations.\r\nFor example, if you want to prune based on post-activation information, it's not very straight-forward to do so.\r\n\r\nExample:\r\n\r\n```python\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.nn.utils.prune as P\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.conv1 = P. random_structured(nn.Conv2d(1, 20, 5, 1), 'weight', 0.5, 1)\r\n\r\n    def forward(self, x):\r\n        y = F.relu(self.conv1(x))\r\n        z = F.max_pool2d(y, 2, 2)\r\n        return F.log_softmax(z, dim=1)\r\n```\r\n\r\nHere, you cannot prune based on the values of `y` using the current interface, you can prune only based on the values of `x`, `conv1.weight` and `conv1.bias` and at best `conv1`'s output. `y` is the value of the output of `conv1` after a `relu`, which isn't materialized after all pre and post hooks of `self.conv1` are already executed.\r\nIs this okay, or do you see any pruning methods that cannot be done because of this limitation in the interface?"", ""@soumith \r\n>     * Quantization on the other hand is extremely mature\r\n\r\nOh I wish this were true! :)  #18318 refers to only post-training quantization, and only deals with 8 bit precision. That's a good start, but it's trivial compared to all the active research on binary and ternary networks. As you go below 4 bits you start seeing accuracy degradation, inversely proportional to the model size. New papers get posted every week on ways to reduce that degradation. Just in the last six months I've seen ~10 papers claiming new state of the art :) Note that latest Nvidia cards offer native support for 1 bit ops. There are far more methods to perform quantization than to perform pruning :)\r\n\r\nAs I'm typing this, I'm training a convnet for a mixed signal chip we built, where the weights are analog, but activations have to be stored in digital memory, and therefore must be quantized. Note that in modern convnets activations consume a lot more memory than weights, so compressing them is a lot more effective from the point of view of memory constrained devices. So I'm currently exploring ways to binarize activations. How is this relevant to this ticket? As a separate effort, I've also experimented with several activation pruning methods to reduce power consumption, and I noticed that if I apply a temporal sparsity constraint on activations, it becomes easier to find the optimal clipping threshold for binarization. Basically, it's easier to binarize sparse activations, so the pruning can be considered as a first stage in the quantization process.\r\n\r\nHaving said that, I actually agree that pruning should be kept separately from quantization, not because one is more mature than other (both are still very experimental), but because they are sufficiently distinct concepts, and it makes sense to apply them as separate steps (which especially helps during debugging). My point was more towards treating them as similar type of operations. In fact, if someone is interested in doing quantization, they should be also looking at pruning methods, and vice versa. How you do one affects the effectiveness of doing the other. The ultimate goal is to compress the model. \r\n\r\nTLDR:\r\n1. Please don't ignore activations (rather than focusing only on weights), because their size is far more important for memory constrained devices. \r\n2. Quantization and Pruning don't have to be combined, but should be treated as similar operations, with similar interfaces (when applied to weights/activations, post/during training). They both are ways to perform model compression.\r\n\r\n\r\n\r\n"", ""@soumith thanks for the positive feedback!\r\n\r\nOK on `prune.ln_structured` instead of `utils.ln_structured_pruning`. I agree.\r\n\r\nOn your second point, perhaps a naive solution:\r\n```python\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.nn.utils.prune as P\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\r\n\r\n    def forward(self, x, return_hidden=False):\r\n        y = F.relu(self.conv1(x))\r\n        out = F.log_softmax(F.max_pool2d(y, 2, 2),  dim=1)\r\n        if return_hidden:\r\n            return y, out\r\n        else:\r\n            return out\r\n```\r\nThen, similar to other pruning methods provided so far, we'd have to implement an activation-based pruning method like `activation_based_pruning(module, name, X, amount)` with `module=net.conv1`, `name='weight'`, `X=<some input data>`, that would run the forward function with `return_hidden=True` to get the activations `y`, and compute a structured mask on the layer. Unlike the methods currently implemented in my work-in-progress, this family of pruning functions would require `X` as an argument.\r\n\r\nAn alternative would be to have `activation_based_pruning(module, name, activations, amount)`, where `activations` is the activated output from that hidden layer, presumably computed in some previous forward pass. \r\n\r\nOr we could even join the two and compute the activations from data `X`, unless precomputed `activations` is passed in. If both are passed, maybe warn but give precedence to the precompute `activations`? Not sure about this one...\r\n\r\nOption 1 might be the least error prone, but the least flexible. \r\nIn any case, these all allow you to prune the layer based on its activations on some representative data sample. Once one of these pruning functions is called, the mask is computed and stored in the forward pre hooks, so it's applied every time forward will be called.\r\n\r\nIf instead you were referring to some sort of real-time pruning with scope limited to each individual forward call, then that would require rewriting the `forward` and computing the mask in some functional way, instead of using forward hooks. That only makes sense if one wants to temporarily prune the activations before passing them on to the next layer, instead of pruning units in the layer itself."", 'By the way, even the last case of ephemeral activation pruning can already be supported by the pruning module as is. It will require interacting with the `<...>PruningMethod` objects directly. Example:\r\n```python\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.nn.utils.prune as P\r\n\r\nclass Net(nn.Module):\r\n     def __init__(self):\r\n         super(Net, self).__init__()\r\n         self.conv1 = nn.Conv2d(1, 20, 5, 1)\r\n         self.pruning1 = P.LnStructuredPruningMethod(amount=0.5, n=2, axis=-1)\r\n     def forward(self, x):\r\n         y = F.relu(self.conv1(x))\r\n         mask = self.pruning1.compute_mask(y, default_mask=torch.ones_like(y))\r\n         masked_y = y * mask.to(dtype=y.dtype)\r\n         out = F.log_softmax(F.max_pool2d(masked_y, 2, 2),  dim=1)\r\n         return out\r\n```\r\nThis uses no hooks.', ""> @soumith\r\n> \r\n> > ```\r\n> > * Quantization on the other hand is extremely mature\r\n> > ```\r\n> \r\n> Oh I wish this were true! :) #18318 refers to only post-training quantization, and only deals with 8 bit precision. That's a good start, but it's trivial compared to all the active research on binary and ternary networks. As you go below 4 bits you start seeing accuracy degradation, inversely proportional to the model size. New papers get posted every week on ways to reduce that degradation. Just in the last six months I've seen ~10 papers claiming new state of the art :) Note that latest Nvidia cards offer native support for 1 bit ops. There are far more methods to perform quantization than to perform pruning :)\r\n> \r\n> As I'm typing this, I'm training a convnet for a mixed signal chip we built, where the weights are analog, but activations have to be stored in digital memory, and therefore must be quantized. Note that in modern convnets activations consume a lot more memory than weights, so compressing them is a lot more effective from the point of view of memory constrained devices. So I'm currently exploring ways to binarize activations. How is this relevant to this ticket? As a separate effort, I've also experimented with several activation pruning methods to reduce power consumption, and I noticed that if I apply a temporal sparsity constraint on activations, it becomes easier to find the optimal clipping threshold for binarization. Basically, it's easier to binarize sparse activations, so the pruning can be considered as a first stage in the quantization process.\r\n> \r\n> Having said that, I actually agree that pruning should be kept separately from quantization, not because one is more mature than other (both are still very experimental), but because they are sufficiently distinct concepts, and it makes sense to apply them as separate steps (which especially helps during debugging). My point was more towards treating them as similar type of operations. In fact, if someone is interested in doing quantization, they should be also looking at pruning methods, and vice versa. How you do one affects the effectiveness of doing the other. The ultimate goal is to compress the model.\r\n> \r\n> TLDR:\r\n> \r\n> 1. Please don't ignore activations (rather than focusing only on weights), because their size is far more important for memory constrained devices.\r\n> 2. Quantization and Pruning don't have to be combined, but should be treated as similar operations, with similar interfaces (when applied to weights/activations, post/during training). They both are ways to perform model compression.\r\n\r\nI agree,  the goal is to unify interfaces for quantization and pruning at the level of APIs. Great point about activations.\r\n\r\nThe key difference is that there is a lot more hardware support for 8 bit (and now lower bitwidth) quantization, allowing for faster kernel implementations that run on a wide range of devices. Sparsity support is still limited to hardware accelerators and there seems to be a big distinction between sparsifying weights only (can be packed, treated like a constant for inference) and sparsifying activations (dynamic, requires specialized hw).\r\n\r\n"", ""@raghuramank100 \r\n> there seems to be a big distinction between sparsifying weights only (can be packed, treated like a constant for inference) and sparsifying activations (dynamic, requires specialized hw).\r\n\r\nJust to clarify, there are three potential efficiency benefits from pruning (or quantization): reducing model size on disk, reducing memory consumption (during training or inference), and saving computation. Out of these, memory consumption is arguably the biggest concern. Specifically, memory consumption during training is arguably the biggest concern for researchers (the Pytorch core userbase). That's why methods like gradient checkpointing are popular when working with large models. Note that during training there's no difference between packing/unpacking weights and packing/unpacking activations. \r\n\r\n@mickypaganini \r\nSometimes we want to gradually increase the amount of pruning done during training (or perhaps change some other pruning parameters). For example, in\r\n`self.pruning1 = P.LnStructuredPruningMethod(amount=0.5, n=2, axis=-1)`\r\nwe might want to change `amount` according to some schedule or even make it a trainable parameter.\r\nHow would you deal with such a scenario? "", '@michaelklachko: a quick solution would be to move the instantiation from the `__init__` to the `forward`, so that you can pass in new pruning parameters at each forward call. ', ""What do you mean? I'd probably do it like this, but I'm not sure if this is considered a good practice:\r\n\r\n```\r\nclass Net(nn.Module):\r\n     def __init__(self):\r\n         super(Net, self).__init__()\r\n         self.conv1 = nn.Conv2d(1, 20, 5, 1)\r\n         self.pruning1 = P.LnStructuredPruningMethod(n=2, axis=-1)\r\n         #or if trainable: self.amount = torch.nn.Parameter(...)\r\n     def forward(self, x, amount=0.5):\r\n         y = F.relu(self.conv1(x))\r\n         mask = self.pruning1.compute_mask(y, default_mask=torch.ones_like(y), amount=amount)  # or amount=self.amount\r\n         masked_y = y * mask.to(dtype=y.dtype)\r\n         out = F.log_softmax(F.max_pool2d(masked_y, 2, 2),  dim=1)\r\n         return out\r\n```\r\n"", '@mickypaganini , This is a very useful feature. Just as discussed, some issues we may need to consider. \r\n- In this case, local pruning can be very  nicely handle, but how about global pruning, which is also very common \r\n- for pruning existed models, for example VGG, resNet those are already implemented in torchvision. Under this framework, will it have to re-implement all models?', ""for pruning existing models, if it is via local pruning, then I dont think you need to reimplement them or modify their code, because you can simply loop over the modules and set the pruning hooks on them.\r\n\r\nIf it's activation pruning or global pruning, yes you have to change the code of those models, and we shouldn't strive to find a solution where you dont touch the source code of those models.\r\nLet's operate in a working state of -- copy-and-modify is perfectly acceptable."", 'I have a basic question about pruning. After applying a fine grained pruning methodology that masks out weights randomly, how will speed ups be achieved for these sparse weights in Pytorch ? Are there plans to release a SparseConv2d, SparseLinear etc ?', ""pruning is at a stage where we aren't looking at speedups and performance. it's still researchy."", 'Alright thank you. I was wondering then what is the use of Sparse Tensor support ? How can one leverage them for pruning ? I ask this as it seems like they are a natural fit for representing pruned tensors.\r\n\r\n', 'Pytorch library for sparse NN training: https://github.com/TimDettmers/sparse_learning', 'Thank you for this, I looked at the code and the author is masking out the weights with a boolean mask matrix. That is fine, however I wondering if pytorch will support pruning by using SparseTensors, that is, not computing the unnecessary multiplication with zeros. ', ""Hello,\r\nThis is a great feature and I think it's great to have a _normalised_ api for pruning, quantisation, etc. \r\n\r\n> If it's activation pruning or global pruning, yes you have to change the code of those models, and we shouldn't strive to find a solution where you dont touch the source code of those models.\r\n\r\nTo handle global structure pruning it should be able to change the dimension of the some tensors and to propagate the change to the rest of the graph. Do you think this should be a feature of this work or it's out of scope ? "", '@mickypaganini should this be closed as already implemented?  Or is there more to do?', 'Yes, this can be closed now. This was implemented and merged. Upcoming changes and improvements to the pruning module are being tracked in other issues.']","[""python\r\nt = torch.randn(3, 2, 4)\r\n# e.g.: randomly mask 6 entries\r\npruned_t = t.prune(method='random', amount=6)\r\n# e.g.: prune bottom 80% of entries by absolute value\r\npruned_t = t.prune(method='L1', amount=0.8)\r\n# e.g.: prune 50% of channels along the last dimension by L1 norm\r\npruned_t = t.prune(method='L1Structured', amount=0.5)\r\n# e.g.: prune 2 channels along the 0th dimension by L2 norm\r\npruned_t = t.prune(method='L2Structured', amount=2, axis=0)\r\n# e.g.: prune 1 channel along the last dimension by L0 norm\r\npruned_t = t.prune(method='LnStructured', n=0, amount=1)\r\n"", ""python\r\n{\r\n    'features.0.weight' : L1PruningMethod(amount=0.8),\r\n    ...\r\n}\r\n""]","['nn', '.prune', 'torch.Tensor', 't.prune_(...)', ""net.prune(method='L1', amount=0.8)"", 'net.state_dict().keys()', 't.prune(method=method, **kwargs)', 'torch.nn.utils.pruning_function(t, **kwargs)', 'torch.nn.utils.pruning_function(module=module, name=param_name, **kwargs)', 't', 'X', '(X, y)', 'weight_norm', 'weight_norm', 'torch.nn.functional']",0,0
246,pytorch,21004,closed,PTX JIT compilation failed when running FasterRCNN,"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->
Trying to run FasterRCNN newly released in torchvision 0.3 runs into PTX JIT compilation failed error. This fails on my Google Cloud instance using GPU, but does not fail using CPU

## To Reproduce

Steps to reproduce the behavior:

Simple code that fails


Error output:


<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

To run

## Environment


## Additional context

<!-- Add any other context about the problem here. -->
",oncall: jit,"['For me this issue occured with CUDA 10.0 and installing PyTorch from pip. Installing PyTorch 1.1 and Torchvision according to the instructions on https://pytorch.org/ (for Linux, CUDA 10.0, Python 3.6 and pip):\r\n```\r\npip3 install https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl\r\npip3 install https://download.pytorch.org/whl/cu100/torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl\r\n```\r\nsolved it for me.', 'This is a cuda initialization error in disguise, not really JIT compilation error, so making sure driver + pytorch cuda version + torch vision cuda version match usually solves it. It would be great to have a more helpful error message, but so far we are unable to repro. ', 'Thank you for taking a look! It seems like I cannot install the wheels on my current google cloud instance. I can try to create a new image with new installations and will report back if that solves it', 'I was able to run it successfully after install using correct conda commands. Thank you for the help!', '> For me this issue occured with CUDA 10.0 and installing PyTorch from pip. Installing PyTorch 1.1 and Torchvision according to the instructions on https://pytorch.org/ (for Linux, CUDA 10.0, Python 3.6 and pip):\r\n> \r\n> ```\r\n> pip3 install https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl\r\n> pip3 install https://download.pytorch.org/whl/cu100/torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl\r\n> ```\r\n> \r\n> solved it for me.\r\n\r\n\r\n`pip uninstall torch torchvision` and reinstall as you said solve my problem. Maybe I used the wrong source before.', 'I switched to CUDA 9.0 and it worked', 'seems like some incompatibility between cupy and pytorch at least on cuda 10.1']","['\r\nimport torch \r\nimport torchvision\r\nimport torch.nn as nn\r\n    \r\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\r\n\r\nmodel =torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True).to(device)\r\n    \r\nimages = list(torch.randn((1,3,512,512)).to(device))\r\ntargets = [{\r\n        ""boxes"": torch.randn((1,4)),\r\n        ""labels"": torch.randn((1)),\r\n        ""image_id"": torch.randn((1)),\r\n        ""area"": torch.randn((1)),\r\n        ""iscrowd"": torch.randn((1))\r\n    }]\r\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]\r\n\r\nloss_dict = model(images, targets)\r\n', '\r\nFile ""debug2.py"", line 19, in <module>\r\n    loss_dict = model(images, targets)\r\n  File ""/opt/anaconda3/envs/nuscenes/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 493, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""/opt/anaconda3/envs/nuscenes/lib/python3.7/site-packages/torchvision/models/detection/generalized_rcnn.py"", line 48, in forward\r\n    features = self.backbone(images.tensors)\r\n  File ""/opt/anaconda3/envs/nuscenes/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 493, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""/opt/anaconda3/envs/nuscenes/lib/python3.7/site-packages/torch/nn/modules/container.py"", line 92, in forward\r\n    input = module(input)\r\n  File ""/opt/anaconda3/envs/nuscenes/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 493, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""/opt/anaconda3/envs/nuscenes/lib/python3.7/site-packages/torchvision/models/_utils.py"", line 58, in forward\r\n    x = module(x)\r\n  File ""/opt/anaconda3/envs/nuscenes/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 493, in __call__\r\n    result = self.forward(*input, **kwargs)\r\nRuntimeError: /pytorch/torch/csrc/jit/fuser/cuda/fused_kernel.cpp:202: a PTX JIT compilation failed\r\n', '\r\nCollecting environment information...\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Debian GNU/Linux 9.8 (stretch)\r\nGCC version: (Debian 6.3.0-18+deb9u1) 6.3.0 20170516\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: Tesla P100-PCIE-16GB\r\nNvidia driver version: 410.72\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] intel-numpy==1.15.1\r\n[pip3] numpy==1.16.3\r\n[pip3] torch==1.1.0\r\n[pip3] torchvision==0.3.0\r\n[conda] torch                     1.1.0                    pypi_0    pypi\r\n[conda] torchvision               0.3.0                    pypi_0    pypi\r\n']",[],0,0
247,pytorch,14365,closed,Assertion fails when using DataParallel with two nn.Embedding ,"## üêõ Bug
I'm using the nightly build: 1.0.0.dev20181123. This issue is very similar to #13569 .  When I instantiate two nn.Embedding, with DataParallel and with max_norm=1.0, I get the following assert


If I remove the self.lut_dummy, the issue disappears. 

## To Reproduce


## Output
1.0.0.dev20181123
/home/software/LM_stash/amitoj/pytorch1.0/local/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py:25: UserWarning:
    There is an imbalance between your GPUs. You may want to exclude GPU 0 which
    has less than 75% of the memory or cores of GPU 1. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))
ok1
ok1
ok2
Traceback (most recent call last):
  File ""error_train.py"", line 25, in <module>
    main()
  File ""error_train.py"", line 22, in main
    output = model(src)
  File ""/home/software/LM_stash/amitoj/pytorch1.0/local/lib/python2.7/site-packages/torch/nn/modules/module.py"", line 479, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/software/LM_stash/amitoj/pytorch1.0/local/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py"", line 143, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File ""/home/software/LM_stash/amitoj/pytorch1.0/local/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py"", line 153, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File ""/home/software/LM_stash/amitoj/pytorch1.0/local/lib/python2.7/site-packages/torch/nn/parallel/parallel_apply.py"", line 83, in parallel_apply
    raise output
RuntimeError: output_nr_ == 0 ASSERT FAILED at /pytorch/torch/csrc/autograd/variable.cpp:196, please report a bug to PyTorch.

## Additional context
Observations
1. If I comment out the self.lut_dummy, the issue disappears. Though in my real model, I am using the .
2. If max_norm is removed, the issue disappears again.
3. If I set os.environ[""CUDA_VISIBLE_DEVICES""] to ""0"" or ""1"", again, the code works just fine.


cc @ezyang @gchanan @SsnL @albanD",high priority module: autograd triaged,"['I can reproduce this on master', 'Some more observations:\r\n\r\nThe immediate cause of the error is that `self.lut_a = torch.nn.Embedding(22, 256, max_norm=1.0).to(""cuda"")` is being defined **after** some specific types of layers (if it\'s defined after `nn.Linear` or `nn.Embedding`, this error will show; if it\'s defined after `nn.ReLU` or `nn.MaxPool2d`, this error doesn\'t show).', ""This is fixed on master by https://github.com/pytorch/pytorch/commit/6d63e9dbfffba9f925ac3af5232390a76aa54dce. https://github.com/pytorch/pytorch/pull/14549 adds a test to make sure it doesn't happen in the future."", ""Some more observations:\r\n\r\n`Broadcast` is called to replicate the module parameters.\r\nhttps://github.com/pytorch/pytorch/blob/b15242f70cce10c0a0f8c36b690ac2481c51814f/torch/nn/parallel/_functions.py#L9-L32\r\n\r\n`Broadcast` returns tensors that are views (in the broadcast_coalesced logic the code does some narrowing and viewing). However, autograd assumes that variables that are views must come from autograd functions with exactly one output. It's not entirely wrong but because (hypothesis, I'm going to test this now...) `Broadcast` is a big autograd function that hides its implementation, autograd triggers the assert seen in this issue."", ""I have a minimal repro that does not involve DataParallel:\r\n```\r\nimport torch\r\nfrom torch.autograd import Function\r\n\r\nclass Flatten3(Function):\r\n    @staticmethod\r\n    def forward(ctx, x, y):\r\n        ctx.save_for_backward(x, y)\r\n        return x.view(-1), y.view(-1)\r\n\r\n    @staticmethod\r\n    def backward(ctx, dx, dy):\r\n        x, y = ctx.saved_tensors\r\n        return dx.view(x.shape), dy.view(y.shape)\r\n\r\ninputs = [torch.randn(2, 2, requires_grad=True) for _ in range(2)]\r\n\r\n# a, b are all DifferentiableViewImpl with output_nr = {0, 1} respectively\r\na, b = Flatten3.apply(*inputs)\r\n\r\n# Modify the counter in no_grad mode.\r\n# b is still a DifferentiableViewImpl with output_nr = 1 (no_grad prevents\r\n# a new autograd function from being created)\r\nwith torch.no_grad():\r\n    b.zero_()\r\n\r\n# Throws assert because:\r\n# 1) b has been modified in place\r\n# 2) b is a view\r\n# 3) b's output_nr is not 0.\r\nz = b + a\r\n```\r\n\r\na few follow up questions for myself:\r\n- should torch.no_grad bump the version counter? \r\n- should we support DifferentiableViewImpl where the output_nr is not 0?\r\n- should we just error out in this case (instead of assert) and tell the user we don't support modifying views in place when the views aren't the first output of their function?\r\n"", '@zou3519 , Are there any news regarding this issue? thanks..', 'Hi there, I encounter with the same issue.\r\n\r\nAlthough I can call a norm function written by my own after calling backward at the end of every training step, I think it would be more efficient if this could be solved at the level of source codeüòÜ.\r\n\r\nThanks for your concern.', 'I encounter the same issue too. When the nn.Embedding initialized with ""max_norm=1"", then it occurs.', ""To answer @zou3519 questions above:\r\n\r\n> should torch.no_grad bump the version counter?\r\n\r\nYes, otherwise the computed gradients could be wrong and that should never be possible.\r\n\r\n> should we support DifferentiableViewImpl where the output_nr is not 0?\r\n\r\nNo, the way all the inplace stuff is inplemented only supports single output Functions. It would be a large change to support Functions with more outputs\r\n\r\n> should we just error out in this case (instead of assert) and tell the user we don't support modifying views in place when the views aren't the first output of their function?\r\n\r\nI would agree that the few asserts at this place should be proper errors. The user can actually make this happen."", 'Closing this as duplicate of https://github.com/pytorch/pytorch/issues/26546']","['\r\nRuntimeError: output_nr_ == 0 ASSERT FAILED at /pytorch/torch/csrc/autograd/variable.cpp:196, please report a bug to PyTorch.\r\n', '\r\nimport torch\r\nimport os\r\n\r\nprint torch.__version__\r\nclass Moda(torch.nn.Module):\r\n    def __init__(self):\r\n        super(Moda, self).__init__()\r\n        self.lut_dummy = torch.nn.Embedding(1, 1, max_norm=1.0).to(""cuda"")\r\n        self.lut_a = torch.nn.Embedding(22, 256, max_norm=1.0).to(""cuda"")\r\n\r\n    def forward(self, src):\r\n        print ""ok1""\r\n        ebd = self.lut_a(src)\r\n        print ""ok2""\r\n        return ebd\r\n\r\ndef main():\r\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = ""0,1""\r\n    model = Moda()\r\n    model = torch.nn.DataParallel(model)\r\n    src = torch.randint(4,(2,)).to(""cuda"")\r\n    output = model(src)\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n']",['lut_dummy'],0,0
248,pytorch,13850,closed,num_workers > 0 leading to OOM in non-IPython environment,"My code works perfectly in an IPython Jupyter Notebook environment. I'm training a GAN while asynchronous loading data with ImageFolder and DataLoader.  behaves as expected.

When I export my code to .py and execute it from VSCode inside my conda environment (where I started the notebook from) my memory quickly goes up until it reaches . (RAM goes to 100% aswell).

I am using this syntax because I'm on windows: 


Using  and  works but then my epoch time increases about 22x. Everything over 1 leads to the aforementioned OOMs.

Theres shouldn't be any issue because I'm just copying my Ipython code to VSCode and then run the .py file from there, why is this happening?


**System information**

PyTorch version: 0.4.1
Is debug build: No
CUDA used to build PyTorch: 9.2

OS: Microsoft Windows 10 Home
GCC version: Could not collect
CMake version: Could not collect

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 9.2.148
GPU models and configuration: GPU 0: GeForce RTX 2070
Nvidia driver version: 416.81
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] Could not collect
[conda] cuda92                    1.0                           0    pytorch
[conda] pytorch                   0.4.1           py37_cuda92_cudnn7he774522_1  [cuda92]  pytorch
[conda] torchvision               0.2.1                     <pip>",,"['Is there any code I can try here?', ""> Is there any code I can try here?\r\n\r\nI managed to fix it by putting my **whole** training code into main().\r\nBefore that I was only doing the training loop in main while keeping other functions outside. This led to memory buildup as soon as more worker processed were spawned that executed the code that comes before main() repeatedly.\r\n\r\nI'm not a Windows/Python guy so this wasn't obvious behaviour for me. I took the code snippet from the Windows FAQs from the Pytorch site: https://pytorch.org/docs/stable/notes/windows.html\r\n\r\nIs this normal procedure to put the whole code into main() when using num_workers multiprocessing?"", ""What do you mean whole code? The import section and the function definitions don't have to be wrapped into the if protection. But you have to put these operations into the functions or the if protection."", 'Yes, sorry I actually meant what you said. I put all the code that was outside main() in either main() or another function. I left the import section.\r\n\r\nThanks for the quick help on this issue.']","[""\r\nimport torch\r\n\r\ndef main()\r\n    for i, data in enumerate(dataloader):\r\n        # do something here\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n""]","['Num_workers', 'OOM: RuntimeError: CUDA error: out of memory', 'num_workers=0', 'num_workers=1']",0,0
249,pytorch,16437,open,cuDNN error when using half precision convolution,"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->

When using half precision convolution on PyTorch nightly (20190125), with torch.backends.cudnn.deterministic = True, CUDNN_STATUS_BAD_PARAM runtime error occurs

I found that when using latest PyTorch 1.0, or using fp32, or use torch.backends.cudnn.deterministic = False then this script runs without problem.

## To Reproduce

Steps to reproduce the behavior:





<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

Run without error
<!-- A clear and concise description of what you expected to happen. -->

## Environment

`

PyTorch version: 1.0.0.dev20190125
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 16.04.5 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration: GPU 0: Tesla V100-PCIE-32GB
Nvidia driver version: 410.79
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.1
/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a

Versions of relevant libraries:
[pip] Could not collect
[conda] Could not collect",,"['Could you please run it with `CUDA_LAUNCH_BLOCKING=1` and report the error message?', 'Error messages when ran with it,\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File ""run.py"", line 9, in <module>\r\n    print(conv(x).shape)\r\n  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 492, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py"", line 320, in forward\r\n    self.padding, self.dilation, self.groups)\r\nRuntimeError: cuDNN error: CUDNN_STATUS_BAD_PARAM\r\n```', 'cc: @syed-ahmed might be your recent cudnn algorithm selection diff?', ""Thanks for the cc! I'll try to reproduce and followup on this issue."", ""Hi, I'm verifying that this was caused by my PR, most probably for not calling cudnnSetConvolutionMathType before cudnnFind/cudnnGet. In the meanwhile, I have verified that with this patch in my PR https://github.com/pytorch/pytorch/pull/15881/commits/436aea6b362255bb203d76f3f1f2e26eec6f3fb5 and with the reproducer script from this issue, the cuDNN error goes away. "", ""@vishwakftw mentioned running with `CUDA_LAUNCH_BLOCKING=1`, how do you do that? Or... where could I find resources to learn how to work with CUDA's environment variables?""]","[""python\r\nimport torch\r\nfrom torch import nn\r\n\r\ntorch.backends.cudnn.deterministic = True\r\n\r\nx = torch.randn(1, 1, 16, 16).to('cuda').half()\r\nconv = nn.Conv2d(1, 1, 3, bias=False).to('cuda').half()\r\n\r\nprint(conv(x).shape)\r\n"", 'python\r\nTraceback (most recent call last):\r\n  File ""main.py"", line 9, in <module>\r\n    print(conv(x).shape)\r\n  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 492, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py"", line 320, in forward\r\n    self.padding, self.dilation, self.groups)\r\nRuntimeError: cuDNN error: CUDNN_STATUS_BAD_PARAM\r\n']",[''],0,0
250,pytorch,9172,closed,Kernel breaks  when doing backward pass with .permute with the -1 option,"## Issue description

Permuting the axis of a tensor using the  option breaks the kernel (tested in Jupyter) when doing the backward pass. The kernel just restart, there is no error provided.

## Code example

This code:

automatically breaks the Kernel of a Jupyter Notebook (run in JupyterLab).

Note that the following code:


worked well.

## System Info
PyTorch version: 0.3.1
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 16.04.3 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.5
Is CUDA available: Yes
CUDA runtime version: 9.0.176
GPU models and configuration: GPU 0: Tesla K80
Nvidia driver version: 384.111
cuDNN version: Probably one of the following:
/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21
/usr/local/cuda-8.0/lib64/libcudnn_static.a
/usr/local/cuda-9.0/lib64/libcudnn.so.7.0.5
/usr/local/cuda-9.0/lib64/libcudnn_static.a
/usr/local/lib/python3.5/dist-packages/torch/lib/libcudnn-7b07b0f1.so.7.0.5

Versions of relevant libraries:
[pip3] msgpack-numpy (0.4.1)
[pip3] numpy (1.14.0)
[pip3] torch (0.3.1)
[pip3] torchvision (0.2.0)
[conda] Could not collect",,"['What about in command line? The code works on master build with command line python.', ""I get:\r\n\r\n`*** Error in `python3.5': free(): invalid next size (fast): 0x00007f7e10000b40 ***`\r\n\r\npossibly due to a previous version of pytorch. Will try with current master (0.4.0) and revert.\r\n\r\nThanks!"", 'Not happening in 0.4.0. \r\n\r\nHappens only in 0.3.1.\r\n\r\nClosing the issue. Thanks.']","['\r\nimport torch\r\nfrom torch.autograd import Variable\r\na = Variable(torch.rand(2,3,2), requires_grad = True)\r\na = a.permute(1,0,-1)\r\na = a.sum()\r\na.backward()\r\n', '\r\nimport torch\r\nfrom torch.autograd import Variable\r\na = Variable(torch.rand(2,3,2), requires_grad = True)\r\na = a.permute(1,0,2)\r\na = a.sum()\r\na.backward()\r\n']",['-1'],0,0
251,pytorch,31698,closed,can't iter a dataSet imported by hdf5,"## ‚ùì Questions and Help

### Please note that this issue tracker is not a help form and this issue will be closed.

We have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:

- [Discussion Forum](https://discuss.pytorch.org/)

",,"['What is your question here?', 'Please ask for support debugging your code in https://discuss.pytorch.org/ . If through that discussion you discover that there is a bug in PyTorch causing your problem, please feel free to reopen this issue or create a new one.']","[""\r\nimport torch\r\nimport numpy as np\r\nimport h5py\r\nimport torch.utils.data as data\r\nimport torch.nn as nn\r\n\r\n\r\nclass H5Dataset(data.Dataset):\r\n\r\n    def __init__(self, file_path):\r\n        super(H5Dataset, self).__init__()\r\n        h5_file = h5py.File(file_path)\r\n        self.set_x = np.array(h5_file.get('train_set_x'))\r\n        self.set_y = np.array(h5_file.get('train_set_y'))\r\n\r\n    def __getitem__(self):\r\n        return (torch.from_numpy(self.set_x), torch.from_numpy(self.set_y))\r\n\r\n    def __len__(self):\r\n        return self.set_x.shape[0]\r\n\r\nclass H5DatasetTest(H5Dataset):\r\n\r\n    def __init__(self, file_path):\r\n        super(H5Dataset, self).__init__()\r\n        h5_file = h5py.File(file_path)\r\n        self.set_x = np.array(h5_file.get('test_set_x'))\r\n        self.set_y = np.array(h5_file.get('test_set_y'))\r\n        self.classes = np.array(h5_file.get('list_classes'))\r\n\r\n    def __getitem__(self):\r\n        return (torch.from_numpy(self.set_x), torch.from_numpy(self.set_y), self.classes)\r\n\r\ntrainSet = H5Dataset('datasets/train_catvnoncat.h5')\r\ntestSet = H5DatasetTest('datasets/test_catvnoncat.h5')\r\n\r\nmodel = nn.Sequential(\r\n        nn.Linear(64*64*3,4096),\r\n        nn.Sigmoid(),\r\n)\r\n\r\noptimizer = torch.optim.SGD(model.parameters(),lr=0.001)\r\n\r\nloss_func = nn.CrossEntropyLoss()\r\n\r\ntrain_loader = data.DataLoader(dataset=trainSet, batch_size=64, shuffle=True)\r\n\r\nfor i in range(100):\r\n    for i, (image,lebel) in enumerate(train_loader):\r\n        image = image.view(100, 64 * 64)\r\n        optimizer.zero_grad()\r\n""]",[],0,0
252,pytorch,473,open,Autograd IndexCopy is broken,"This happens when the index tensor contains duplicate elements. If this is not allowed index_copy_ should raise an exception when this happens and we should fix the test. Otherwise we need to fix the autograd op.

Here's an example:





cc @ezyang @SsnL @albanD",module: advanced indexing module: autograd triaged,"['Why would you `index_copy` with repeated indices? Sounds for me like UB is a reasonable. We should check that in C', 'I ran into this in https://github.com/pytorch/pytorch/pull/1884 and fixed the test for now.  I\'m not sure how to interpret  @apaszke\'s last comment: did you mean UB behavior is ""unreasonable"" rather than ""a reasonable""?  The last sentence reads that way.', ""I'd be ok with saying that it's UB in such case. There's no natural choice, and it can decrease performance."", '@apaszke Is this issue still worth working on? If adding the additional check can decrease performance, can we add another argument specifying that indexes contain duplicate elements?']","['python\r\nimport torch\r\nfrom torch.autograd import *\r\nx = Variable(torch.zeros(5, 5), requires_grad=True)\r\nprint(x)\r\ny = Variable(torch.range(1, 25).view(5, 5), requires_grad=True)\r\nprint(y)\r\nidx = Variable(torch.LongTensor([0, 0, 0, 0, 0]))\r\nz = x.index_copy(0, idx, y)\r\nprint(z)  # Note only the last row of y is copied to the first row of z. No other rows of y are used.\r\nz.backward(torch.ones(5, 5))\r\nprint(y.grad)  # Incorrectly all ones. Only the last row of y.grad should be non-zero.\r\n']",[],0,0
253,pytorch,25775,closed,mean/sum(dtype) arg matching gives bad error message with positional dtype arg,"## üêõ Bug

 is broken yet  works. The error message is confusing as well.

![Screenshot 2019-09-06 11 11 08](https://user-images.githubusercontent.com/5674597/64439027-1efc4880-d097-11e9-8c00-eb420962d78e.png)
",module: error checking module: reductions triaged,"['I think that we intentionally don\'t support positional dtype, similar to our factory functions:\r\n\r\n```\r\ntor>>> torch.empty(2, torch.float)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\nTypeError: empty(): argument \'size\' must be tuple of ints, but found element of type torch.dtype at pos 2\r\n```\r\n\r\nThough it would certainly be good to improve the error message.', 'Closing this issue as dtype is intentionally keyword only and the error message has been improved to:\r\n\r\n```\r\nTypeError: mean() received an invalid combination of arguments - got (torch.dtype), but expected one of:\r\n * (*, torch.dtype dtype)\r\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\r\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\r\n```']",[],"['t.sum(torch.float)', 't.sum(dtype=dtype)']",0,0
254,pytorch,31317,open,[feature request] [onnx] Export torch.stft as Conv1d till ONNX supports stft op,"As far as I understand, for the time being STFT [isn't natively supported by ONNX](https://github.com/onnx/onnx/blob/master/docs/Operators.md)
 
STFT can be exported as a Conv1d op with precomputed (windowed) Fourier basis: https://github.com/NVIDIA/mellotron/blob/master/stft.py

This is useful for tracking/exporting speech recongition models.

I've done a pure PyTorch version in https://github.com/vadimkantorov/convasr/blob/master/models.py#L315-L334 and it seems to work

cc @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof",enhancement module: onnx triaged,"[""My current workaround:\r\n\r\n```python\r\nimport math\r\nimport torch\r\n\r\nclass STFT(torch.nn.Module):\r\n    def __init__(self, win_length, hop_length, window_periodic = True, stft_mode = None):\r\n        super().__init__()\r\n        self.win_length = win_length\r\n        self.hop_length = hop_length\r\n        self.nfft = 2**math.ceil(math.log2(self.win_length))\r\n\tself.freq_cutoff = self.nfft // 2 + 1\r\n\tself.register_buffer('window', getattr(torch, window)(self.win_length, periodic = window_periodic).float())\r\n        if stft_mode == 'conv':\r\n\t\tfourier_basis = torch.rfft(torch.eye(self.nfft), signal_ndim = 1, onesided = False)\r\n\t\tforward_basis = fourier_basis[:self.freq_cutoff].permute(2, 0, 1).reshape(-1, 1, fourier_basis.shape[1])\r\n\t\tforward_basis = forward_basis * torch.as_tensor(\r\n\t\t\t\tlibrosa.util.pad_center(self.window, self.nfft), dtype = forward_basis.dtype\r\n\t\t\t)\r\n\t\tself.stft = torch.nn.Conv1d(\r\n\t\t\tforward_basis.shape[1],\r\n\t\t\tforward_basis.shape[0],\r\n\t\t\tforward_basis.shape[2],\r\n\t\t\tbias = False,\r\n\t\t\tstride = self.hop_length\r\n\t\t).requires_grad_(False)\r\n\t\tself.stft.weight.copy_(forward_basis)\r\n\telse:\r\n\t\tself.stft = None\r\n\r\n    def forward(self, signal):\r\n        pad = self.freq_cutoff - 1\r\n        padded_signal = torch.nn.functional.pad(signal.unsqueeze(1), (pad, pad), mode = 'reflect').squeeze(1))\r\n        real, imag = self.stft(padded_signal.unsqueeze(dim = 1)).split(self.freq_cutoff, dim = 1) if self.stft is not None else padded_signal.stft(self.nfft, hop_length = self.hop_length, win_length = self.win_length, window = self.window, center = False).unbind(dim = -1)\r\n        return real, imag\r\n\t\t\r\n```"", 'Great @vadimkantorov.  Please consider submitting a PR for adding ONNX export support.']",[],[],0,0
255,pytorch,22277,open,Handle all IntArrayRef expansions in ATen,See #22032 and #20866 for the rationale and #22073 for an example.,enhancement module: nn triaged,"['Hi @skrah It seems that some PRs are closed to fix the problem with max_pool and ArrayRef but the tests for C++ models in torchvision are still failing. Here is a [link](https://travis-ci.org/pytorch/vision/jobs/552837096). \r\n\r\nThe problem here is a maxpool inside a Functional:\r\n```\r\ntorch::nn::Functional(torch::max_pool2d, 3, 2, 1, 1, false)\r\n```\r\nThe error:\r\n```\r\n RuntimeError: max_pool2d_with_indices: internal error: all IntArrayRef sizes must be 2 (max_pool2d_with_indices_out_cpu_template at /opt/conda/conda-bld/pytorch-nightly_1561957697764/work/aten/src/ATen/native/DilatedMaxPool2d.cpp:137)\r\n```\r\nI can fix these for now with:\r\n```\r\ntorch::nn::Functional(torch::max_pool2d, 3,\r\n                              torch::ExpandingArray<2>({2, 2}), 1, 1, false)\r\n```\r\nBut this is not a good fix.', ""I think I assumed that `stride` is either `None` (empty list) or a full list:\r\n\r\n```\r\nclass _MaxPoolNd(Module):\r\n\r\n    def __init__(self, kernel_size, stride=None, padding=0, dilation=1,\r\n                 return_indices=False, ceil_mode=False):\r\n```\r\n\r\nBut according to the docstring it can also be a single integer, which makes sense.  So we'll have to expand that, too.""]",[],[],0,0
256,pytorch,22049,open,Cannot update part of the parameters in DistributedDataParallel.,"## üêõ Bug

When I use multiple GPU while the loss is calculated by only part of the parameters. I get the following errors. Use only one GPU works well.




## To Reproduce

Steps to reproduce the behavior:

Define a network in which the loss only depends on part of the parameters. We get:


find_unused_parameters=Truetorch.nn.parallel.DistributedDataParallelforwardforwardforward

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

PyTorch version: 1.2.0.dev20190620
CUDA used to build PyTorch: 9.0.176
OS: CentOS Linux release 7.5.1804 (Core)
GCC version: (crosstool-NG 1.23.0.449-a04d0) 7.3.0
CMake version: version 2.8.12.2

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration:
GPU 0: GeForce GTX 1080
GPU 1: GeForce GTX 1080
GPU 2: GeForce GTX 1080
GPU 3: GeForce GTX 1080

Nvidia driver version: 396.26
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] msgpack-numpy==0.4.3.2
[pip3] numpy==1.15.4
[pip3] pytorch-pretrained-bert==0.4.0
[pip3] torch==1.0.1.post2
[pip3] torchfile==0.1.0
[pip3] torchtext==0.4.0
[pip3] torchvision-nightly==0.2.1
[conda] pytorch-pretrained-bert   0.6.2                    pypi_0    pypi
[conda] torch-nightly             1.2.0.dev20190620          pypi_0    pypi
[conda] torchfile                 0.1.0                    pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi",oncall: distributed triaged,"['Did you try the instructions in the error message?', '@pietern  The instruction there just give a way to find which variable is not included. However, Since I want to train the first part of the net first. I intend not the update the parameters for the second net.', ""How do you freeze the parameters you don't want to train?\r\n\r\nIf you set `param.requires_grad = False` before wrapping the model with `torch.nn.parallel.DistributedDataParallel` it should work. If you set this after wrapping then DDP will still expect gradients for those parameters and give the error you posted. "", '@pietern Hi, Thanks for your answer. I want to train two network alternately, so, It is set dynamically after DDP. I think DDP should have some functions to dynamically freeze some part of the network, I feel this is a commonly used function.', 'Just to make sure I understand correctly:\r\n* You have 2 models that are each individually wrapped with DDP\r\n* You want to train them alternately\r\n* You do so by setting all model parameters to `requires_grad = False`\r\n\r\nFor the last one, do you freeze ALL parameters or only a subset? I believe that you can freeze the whole model today, and it should work out of the box. Only if you freeze a subset of the model will you get the error message you posted.', 'I have 2 models but I  wrapped them in one DDP. This may be the problem. Is it possible to wrap all models in one DDP and dynamically freeze the parameters? I think this may make thing much easier?', 'It is not possible today to partially freeze a DDP wrapped model. Either you freeze the whole thing (and no model parameter receives gradients), or none at all. If you want to alternate between two models, it is best to wrap them separately and freeze them entirely, separately, as well.', ""I was recently encountering the same problem. I guess PyTorch or the backend library is implemented in such manner due to the synchronization issue. My walk-around is to set the gradients of all 'freezed' parameters to zeros, right after calling `<any_loss>.backward()`. This solution is not necessary to be perfect for those non-stateless optimizers (e.g. SGD with momentum, Adam etc.), but the reduction error will no longer be triggered. Hope my solution will be helpful."", ""@xf3227 Did you try the fix that the error message suggests (`find_unused_parameters=True`)?\r\n\r\nIf you freeze a subset of parameters, there is currently no way for DDP to know if the same set is frozen across all processes. Therefore, the parameters that don't receive gradients will be made to contribute zeroes, and the reduction is executed as expected. If the parameters are frozen on all processes, the reduced gradient should be all zeroes on all processes (assuming you have called `zero_grads` yourself before starting the next iteration)."", ""@pietern Thank you for pointing that out. I guess I somehow misstated my idea. What I was trying doing was not to `detach` anything on the fly. After calling `<any_loss>.backward()` and before `<any_optimizer>.step()`, we have a chance to manually modify the gradients. Based on @fuzihaofzh‚Äòs description, he wanted to train two models alternatively, then he could just replace the gradients of one model by zeros. `<any_stateless_optimizer>.step()` will run as usual, but no change is going to be made since the gradients are zeros.\r\n\r\nBased on my knowledge, if I'm right, gradient reduction (or synchronization) happens during `<any_loss>.backward()`, anything coming after is processed independently on each GPU, so I believe it's safe to manipulate the gradients.\r\n\r\nPlease let me know if I misunderstand any point. I will really appreciate that. "", ""It's safe, but it's better to not synchronize at all if you don't have to."", 'I met the same issue.\r\nBut i solved it.\r\nThe reason is that in my model class, I define a fpn module with 5 level output feature maps in the init function, \r\nbut in forward function I only use 4 of them.\r\nWhen I use all of them, the problem was solved.\r\n**This is my supposed conclusion: you should use all output of each module in forward function.**']",[],"['', '\r\nRuntimeError: Expected to have finished reduction in the prior iteration before starting a\r\n new one. This error indicates that your module has parameters that were not used in produ\r\ncing loss. You can enable unused parameter detection by (1) passing the keyword argument ', ' to ', '; (2) making su\r\nre all ', "" function outputs participate in calculating loss. If you already have done\r\nthe above two steps, then the distributed data parallel module wasn't able to locate the\r\n output tensors in the return value of your module's "", ' function. Please include th\r\ne loss function and the structure of the return value of ', ' of your module when rep\r\norting this issue (e.g. list, dict, iterable). (prepare_for_backward at /pytorch/torch/csrc/distributed/c10d/reducer.cpp:429)\r\n', '']",0,0
257,pytorch,22013,open,Mysterious Tensor Indexing Problem,"## üêõ Bug

Indexing into a tensor with a 2d list of indices seems to fail sometimes, with a critical point when the number of indices is less than 32.

## To Reproduce



 fails with:


## Expected behavior

I expected this indexing to work the same for any number of indices.  This is problematic in my actual code as the size of the data varies.

## Environment

PyTorch version: 1.1.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 16.04.6 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CMake version: Could not collect

Python version: 3.6
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip3] numpy==1.16.4
[pip3] pytorch-pretrained-bert==0.6.2
[pip3] torch==1.1.0
[conda] Could not collect

## Additional context

<!-- Add any other context about the problem here. -->


cc @mruberry @rgommers @heitorschueroff",module: advanced indexing module: error checking module: numpy module: ux triaged,"['Does `M[torch.tensor(idxes)]` do what you want?', ""Aha!  Indeed it does.  Thank you!  That's an excellent workaround.\r\n\r\nI'm tempted to say this is still a bug.\r\n\r\nIndexing by native python lists seems like it should either be equivalent to indexing by a tensor or completely disallowed.  Or, at the very least, the error message should mention something about indexing by native python lists being unsupported."", ""In particular it does seem that we should have the same behavior regardless of whether the list has more or fewer than 32 elements.\r\n\r\nBut I don't know the exact PyTorch indexing rules -- will discuss with the rest of the team and update this issue accordingly."", ""Lists with 31 or fewer elements are interpreted as tuples in some cases. This is to match NumPy's behavior, which in turn is for backwards compatibility reasons. NumPy recently added a warning for this case. We should probably do the same.\r\n\r\nhttps://github.com/pytorch/pytorch/blob/38c9bb8261054ab6745793521ec765fae921da5b/torch/csrc/autograd/python_variable_indexing.cpp#L226-L264\r\n\r\nNumPy warns:\r\n\r\n```\r\nFutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\r\n```"", 'cc @VitalyFedyunin ', 'We should definitely improve the error message here.']","['python\r\nimport torch\r\n\r\ndef index_test(n):\r\n    M = torch.tensor([0.]*n) # Trivial example for illustrative purposes\r\n    idxes = [(a,) for a in range(n)]\r\n    return M[idxes]\r\n\r\nindex_test(31) # Note that this fails\r\nindex_test(32) # But this works\r\n', '\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-169-222ec1878948> in <module>\r\n      6     return M[idxes]\r\n      7 \r\n----> 8 index_test(31)\r\n\r\n<ipython-input-169-222ec1878948> in index_test(n)\r\n      4     M = torch.tensor([0.]*n) # Trivial example for illustrative purposes\r\n      5     idxes = [(a,) for a in range(n)]\r\n----> 6     return M[idxes]\r\n      7 \r\n      8 index_test(31)\r\n\r\nIndexError: too many indices for tensor of dimension 1\r\n']",['n=31'],0,0
258,pytorch,28306,open,Torch ONNX export broken for RandomUniform and RandomUniformLike,"## üêõ Bug

 reports issues in exporting models with  and .

## To Reproduce

Steps to reproduce the behavior:

Enable Case 2 or Case 4 and run the example below:



Errors reported by the torch.onnx exporter:

Case 2:


Cases 3 & 4:


## Expected behavior

ONNX RandomUniform export must by supported by torch and onnx model must be generated for all cases in the example script.

## Environment


 - PyTorch Version: 3.6
 - OS (e.g., Linux): Ubuntu 18.04.3 LTS
 - How you installed PyTorch (, , source): pip
 - Build command you used (if compiling from source): NA
 - Python version: 3.6
 - CUDA/cuDNN version: cuda 10.1 / cudnn 7.6.3
 - GPU models and configuration: 
GPU 0: GeForce GTX 1070 Ti
GPU 1: TITAN V
 - Any other relevant information:
[pip] torch==1.3.0
[pip] torchvision==0.4.1

## Additional context

NA


cc @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof",module: onnx triaged,"[""Thanks, I'm looking into this."", 'This PR will take care of case 2: https://github.com/pytorch/pytorch/pull/28470\r\nBut for cases 3 and 4, shape attribute of onnx random ops should be a constant, so current spec does not allow for exporting torch.rand(x.size())', '@neginraoof since the change was backed out, can you please resubmit? Thanks.', 'This fix is merged: https://github.com/pytorch/pytorch/pull/29354', ""Thanks - didn't notice the second MR. Will confirm and close the issue.""]","[""python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nprint(torch.__version__)\r\n\r\nclass test(torch.nn.Module):\r\n\r\n    def __init__(self, vocab_size=10, rnn_dims=512):\r\n        super().__init__()\r\n\r\n    def forward(self, x):\r\n        # CASE 1) RandomNormalLike - Works\r\n        # mask = torch.randn_like(x).to(torch.float32)\r\n\r\n        # CASE 2) RandomUniformLike - Broken\r\n        # mask = torch.rand_like(x).to(torch.float32)\r\n\r\n        # CASE 3) RandomNormal - Broken\r\n        # mask = torch.randn(x.size()).to(torch.float32)\r\n\r\n        # CASE 4) RandomUniform - Broken\r\n        mask = torch.rand(x.size()).to(torch.float32)\r\n\r\n        return mask\r\n\r\n\r\n# PyTorch model\r\nmodel = test()\r\n\r\ninput = torch.ones((1,256)).to(torch.float32).cuda()\r\noutput = model(input)\r\ntorch.onnx.export(model,\r\n                  input,\r\n                  'test_rand.onnx',\r\n                  example_outputs=output)\r\n"", '\r\nUserWarning: ONNX export failed on ATen operator rand_like because torch.onnx.symbolic_opset9.rand_like does not exist\r\n', '\r\nTypeError: i_(): incompatible function arguments. The following argument types are supported: 1. (self: torch._C.Node, arg0: str, arg1: int) -> torch._C.Node\r\n']","['torch.onnx.export', 'torch.rand', 'torch.rand_like', 'conda', 'pip']",0,0
259,pytorch,14726,open,[caffe2] Corresponding C++ API for prepare_prediction_net,"## üöÄ Feature
Corresponding C++ API for prepare_prediction_net

## Motivation

We have a python API that is able to load predictor models in MetaNetDef format. https://github.com/pytorch/pytorch/blob/edb88b5f3af03718b443d015f195faa1832ce95b/caffe2/python/predictor/predictor_exporter.py#L127 However, the corresponding C++ API is missing.

## Pitch

In the Python world, we are able to export and load models in MetaNetDef format, however, when we want to productionize the model and load it in C++, the API to do so is missing.

## Alternatives

Right now, the only alternative is to export the init and predict nets seperately as protobufs and load them in C++.",caffe2,[],[],[],0,0
260,pytorch,19126,open,weight_norm doesn't support eta and returns nan for zero weights,"## üêõ Bug

backprop on weights generated with torch._weight_norm that are zero filled yields nan gradients. I don't see a way to add an eta to the norm to prevent this.

## To Reproduce

Steps to reproduce the behavior:



I'm encountering nan's during backprop during training of a network with weight normalization. From this seemingly related thread it sounds like the advice is to add an eta to the norm, but in this case the norm is generated in pytorch's c++ implementation and I don't see an obvious way to do this.

## Expected behavior

I expect there to be a way to generate non-nan gradients for weight-norm weights that are zero filled

## Environment

 - PyTorch Version (e.g., 1.0): 1.0.0
 - OS (e.g., Linux): Mac and Centos
 - How you installed PyTorch (, , source): pip
 - Build command you used (if compiling from source):
 - Python version: 3.6
 - CUDA/cuDNN version: None
 - GPU models and configuration: None
 - Any other relevant information:

## Additional context



cc @albanD @mruberry",module: NaNs and Infs module: nn module: norms and normalization triaged,"[""you are using non public API and the gradient comes from the fact that you didn't add eps/eta in denominator yourself. use this instead: https://pytorch.org/docs/master/nn.html#torch.nn.utils.weight_norm"", 'thanks @SsnL, I can reproduce something similar with the public weight_norm api. How do I add an eps/eta in the denominator myself?\r\n\r\n>>> from torch.nn.utils import weight_norm\r\n>>> import torch.nn as nn\r\n>>> import torch\r\n>>> l = nn.Conv1d(3, 1, 1)\r\n>>> l.weight.data.normal_(0, 1e-100)\r\ntensor([[[0.],\r\n         [0.],\r\n         [0.]]])\r\n>>> wl = weight_norm(l)\r\n>>> o = wl(torch.zeros(1, 3, 1))\r\n>>> o.backward()\r\n>>> [p.grad for p in wl.parameters()]\r\n[tensor([1.]), tensor([[[nan]]]), tensor([[[nan],\r\n         [nan],\r\n         [nan]]])]', ""I'm having this problem as well. Any update?"", 'Following up', ""I think I encounter this too. Looking at the implementation https://pytorch.org/docs/master/_modules/torch/nn/utils/weight_norm.html#weight_norm\r\n\r\nI think modifying the `compute_weight` function as below should do the trick.\r\n\r\n```\r\ndef compute_weight(self, module):\r\n\r\n        g = getattr(module, self.name + '_g')\r\n        v = getattr(module, self.name + '_v')\r\n        w = v*(g/(torch.norm_except_dim(v, 2, dim)+1e-8)).expand_as(v)\r\n\r\n        return w\r\n```\r\nInspired by https://gist.github.com/rtqichen/b22a9c6bfc4f36e605a7b3ac1ab4122f\r\n\r\n"", 'is there any update on this issue? the suggested solution by @umgupta degrades my validation loss even when I set 1e-8 to zero. I guess norm_except_dim and _weight_norm have different implementation in compute_weight.', ""No update, sorry @rvaghefi. While we expect to work on reparameterization in the future it hasn't come up yet. ""]","['\r\nw = torch.zeros(1, 3)\r\nv = w / w.norm()\r\ng = w.norm()\r\nv.requires_grad = True\r\ng.requires_grad = True\r\ntorch._weight_norm(v, g).matmul(torch.randn(3,1)).backward()\r\nv.grad\r\n> tensor([[nan, nan, nan]])\r\ng.grad\r\n> tensor(nan)\r\n']","['conda', 'pip']",0,0
261,pytorch,14685,open,[Caffe2] Exception when creating gradient for [Cast] SquaredL2Distance as output layer of CNN network,"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->

I am getting the following error when I use the SquaredL2Distance operator as the output layer of my CNN network **(1st attempt)**:



28train_net

Then I tried to fix the error converting  to float (even though it is already float32, see below) using the Cast operator **(2nd attempt)**

However, I got the following error:



Furthermore, I created the LMDB training dataset which stores the image data as uint8 and the label as a multivalue of float64:
**key: 00000001
image_data: shape: (210, 280, 3) type: uint8
indicators:    shape: (14,)              type: float64**

**How can I fix the error?**

## To Reproduce

Steps to reproduce the behavior:

1. **Project consists in two files: a trainer and a creator. Run the trainer which calls the train function of the creator.**

Trainer.py:


CNNCreator_dpnet_dpnet.py:



<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

Execute SquaredL2Distance as the output layer of the CNN network

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


 - PyTorch Version (e.g., 1.0): **Caffer2 tag v0.4.0**
 - OS (e.g., Linux): **Ubuntu 16.04**
 - How you installed PyTorch (conda, pip, source): **Build from source (tag v0.4.0)**
 - Build command you used (if compiling from source):
 - Python version: **Python 2.7**
 - CUDA/cuDNN version: **8.0/7.0.5**
 - GPU models and configuration: **GTX 1050**
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->
",caffe2,"['Hi @BIGBALLON,\r\n\r\ndo you have maybe an idea on how to solve this issue?']","[""\r\ndist = model.net.SquaredL2Distance([label, fc9_], 'dist')\r\npredictions = dist.AveragedLoss([], ['predictions'])\r\n"", ""\r\nlabel_float = model.Cast(label, None, to=core.DataType.FLOAT)\r\ndist = model.net.SquaredL2Distance([label_float, fc9_], 'dist')\r\npredictions = dist.AveragedLoss([], ['predictions'])\r\n"", '\r\ncarlos@carlos-ubuntu:~/Documents/git/Caffe2_scripts/caffe2_torcs_predictor$ python CNNTrainer_dpnet_dpnet.py \r\nGPU mode selected\r\nTraceback (most recent call last):\r\n  File ""CNNTrainer_dpnet_dpnet.py"", line 24, in <module>\r\n    stepsize=8000\r\n  File ""/home/carlos/Documents/git/Caffe2_scripts/caffe2_torcs_predictor/CNNCreator_dpnet_dpnet.py"", line 153, in train\r\n    self.add_training_operators(train_model, predictions, label, device_opts, opt_type, base_learning_rate, policy, stepsize, epsilon, beta1, beta2, gamma, momentum)\r\n  File ""/home/carlos/Documents/git/Caffe2_scripts/caffe2_torcs_predictor/CNNCreator_dpnet_dpnet.py"", line 103, in add_training_operators\r\n    model.AddGradientOperators([loss])\r\n  File ""/home/carlos/Documents/git/pytorch/build/caffe2/python/model_helper.py"", line 335, in AddGradientOperators\r\n    self.grad_map = self.net.AddGradientOperators(*args, **kwargs)\r\n  File ""/home/carlos/Documents/git/pytorch/build/caffe2/python/core.py"", line 1840, in AddGradientOperators\r\n    self._net.op[skip:], ys)\r\n  File ""/home/carlos/Documents/git/pytorch/build/caffe2/python/core.py"", line 1107, in GetBackwardPass\r\n    return ir.GetBackwardPass(ys)\r\n  File ""/home/carlos/Documents/git/pytorch/build/caffe2/python/core.py"", line 982, in GetBackwardPass\r\n    forward_op_idx, all_input_to_grad)\r\n  File ""/home/carlos/Documents/git/pytorch/build/caffe2/python/core.py"", line 932, in _GenerateGradientsForForwardOp\r\n    forward_op, g_output)\r\n  File ""/home/carlos/Documents/git/pytorch/build/caffe2/python/core.py"", line 1080, in GetGradientForOp\r\n    format(op.type, e, str(op))\r\nException: Exception when creating gradient for [Cast]:[enforce fail at cast_op.cc:139] argsHelper.HasSingleArgumentOfType<string>(""from_type"") || argsHelper.HasSingleArgumentOfType<int>(""from_type""). Argument \'from_type\' of type int or string is required to get the gradient of CastOp .\r\nOp: \r\ninput: ""label""\r\noutput: ""train_net/Cast""\r\nname: """"\r\ntype: ""Cast""\r\narg {\r\n  name: ""to""\r\n  i: 1\r\n}\r\ndevice_option {\r\n  device_type: 1\r\n  cuda_gpu_id: 0\r\n}\r\n', '\r\nimport logging\r\nimport CNNCreator_dpnet_dpnet\r\n\r\nif __name__ == ""__main__"":\r\n    logging.basicConfig(level=logging.DEBUG)\r\n    logger = logging.getLogger()\r\n    handler = logging.FileHandler(""train.log"", ""w"", encoding=None, delay=""true"")\r\n    logger.addHandler(handler)\r\n\r\n    dpnet_dpnet = CNNCreator_dpnet_dpnet.CNNCreator_dpnet_dpnet()\r\n    dpnet_dpnet.train(\r\n        num_epoch=100,\r\n        batch_size=32,\r\n        context=\'gpu\',\r\n        opt_type=\'sgd\',\r\n        base_learning_rate=0.01,\r\n        policy=\'step\',\r\n        stepsize=8000\r\n    )\r\n\r\n', '\r\nfrom caffe2.python import workspace, core, model_helper, brew, optimizer\r\nfrom caffe2.python.predictor import mobile_exporter\r\nfrom caffe2.proto import caffe2_pb2\r\nimport numpy as np\r\nimport logging\r\nimport os\r\nimport sys\r\nimport lmdb\r\nimport leveldb\r\n\r\nclass CNNCreator_dpnet_dpnet:\r\n\r\n    module = None\r\n    _current_dir_ = os.path.join(\'./\')\r\n    _data_dir_    = os.path.join(_current_dir_, \'data\', \'dpnet_dpnet\')\r\n    _model_dir_   = os.path.join(_current_dir_, \'model\', \'dpnet_dpnet\')\r\n\r\n    INIT_NET    = os.path.join(_model_dir_, \'init_net.pb\')\r\n    PREDICT_NET = os.path.join(_model_dir_, \'predict_net.pb\')\r\n\r\n    def add_input(self, model, batch_size, db, db_type, device_opts):\r\n        with core.DeviceScope(device_opts):\r\n            # load the data\r\n            data_uint8, label = brew.db_input(\r\n                model,\r\n                blobs_out=[""data_uint8"", ""label""],\r\n                batch_size=batch_size,\r\n                db=db,\r\n                db_type=db_type,\r\n            )\r\n            # cast the data to float\r\n            data = model.Cast(data_uint8, ""data"", to=core.DataType.FLOAT)\r\n\r\n            # scale data from [0,255] down to [0,1]\r\n            data = model.Scale(data, data, scale=float(1./256))\r\n\r\n            # don\'t need the gradient for the backward pass\r\n            data = model.StopGradient(data, data)\r\n\r\n            return data, label\r\n\r\n    def create_model(self, model, data, label, device_opts):\r\n    \twith core.DeviceScope(device_opts):\r\n\r\n    \t\tdata = data\r\n    \t\t# data, output shape: {[3,210,280]}\r\n      \t\tconv1_ = brew.conv(model, data, \'conv1_\', dim_in=3, dim_out=96, kernel=11, stride=4, pad_t=5, pad_b=4, pad_l=4, pad_r=3) #legacy_pad=1)\r\n    \t\t# conv1_, output shape: {[96,53,70]}\r\n    \t\trelu1_ = brew.relu(model, conv1_, conv1_)\r\n    \t\tpool1_ = brew.max_pool(model, relu1_, \'pool1_\', kernel=3, stride=2, pad_t=1, pad_b=1, pad_l=1, pad_r=0) #legacy_pad=1)\r\n    \t\t# pool1_, output shape: {[96,27,35]}\r\n      \t\tconv2_ = brew.conv(model, pool1_, \'conv2_\', dim_in=96, dim_out=256, kernel=5, stride=4, pad_t=1, pad_b=1, pad_l=1, pad_r=1) #legacy_pad=1)\r\n    \t\t# conv2_, output shape: {[256,7,9]}\r\n    \t\trelu2_ = brew.relu(model, conv2_, conv2_)\r\n    \t\tpool2_ = brew.max_pool(model, relu2_, \'pool2_\', kernel=3, stride=2, pad_t=1, pad_b=1, pad_l=1, pad_r=1) #legacy_pad=1)\r\n    \t\t# pool2_, output shape: {[256,4,5]}\r\n      \t\tconv3_ = brew.conv(model, pool2_, \'conv3_\', dim_in=256, dim_out=384, kernel=3, stride=1, pad_t=1, pad_b=1, pad_l=1, pad_r=1) #legacy_pad=1)\r\n    \t\t# conv3_, output shape: {[384,4,5]}\r\n    \t\trelu3_ = brew.relu(model, conv3_, conv3_)\r\n      \t\tconv4_ = brew.conv(model, relu3_, \'conv4_\', dim_in=384, dim_out=384, kernel=3, stride=1, pad_t=1, pad_b=1, pad_l=1, pad_r=1) #legacy_pad=1)\r\n    \t\t# conv4_, output shape: {[384,4,5]}\r\n    \t\trelu4_ = brew.relu(model, conv4_, conv4_)\r\n      \t\tconv5_ = brew.conv(model, relu4_, \'conv5_\', dim_in=384, dim_out=256, kernel=3, stride=1, pad_t=1, pad_b=1, pad_l=1, pad_r=1) #legacy_pad=1)\r\n    \t\t# conv5_, output shape: {[256,4,5]}\r\n    \t\trelu5_ = brew.relu(model, conv5_, conv5_)\r\n    \t\tpool5_ = brew.max_pool(model, relu5_, \'pool5_\', kernel=3, stride=2, pad_t=1, pad_b=0, pad_l=1, pad_r=1) #legacy_pad=1)\r\n    \t\t# pool5_, output shape: {[256,2,3]}\r\n    \t\tfc5_ = brew.fc(model, pool5_, \'fc5_\', dim_in=256 * 2 * 3, dim_out=4096)\r\n    \t\t# fc5_, output shape: {[4096,1,1]}\r\n    \t\trelu6_ = brew.relu(model, fc5_, fc5_)\r\n    \t\tdropout6_ = brew.dropout(model, relu6_, \'dropout6_\', ratio=0.5, is_test=False)\r\n    \t\tfc6_ = brew.fc(model, dropout6_, \'fc6_\', dim_in=4096, dim_out=4096)\r\n    \t\t# fc6_, output shape: {[4096,1,1]}\r\n    \t\trelu7_ = brew.relu(model, fc6_, fc6_)\r\n    \t\tdropout7_ = brew.dropout(model, relu7_, \'dropout7_\', ratio=0.5, is_test=False)\r\n    \t\tfc7_ = brew.fc(model, dropout7_, \'fc7_\', dim_in=4096, dim_out=256)\r\n    \t\t# fc7_, output shape: {[256,1,1]}\r\n    \t\trelu8_ = brew.relu(model, fc7_, fc7_)\r\n    \t\tdropout8_ = brew.dropout(model, relu8_, \'dropout8_\', ratio=0.5, is_test=False)\r\n    \t\trelu9_ = brew.relu(model, dropout8_, dropout8_)\r\n    \t\tfc9_ = brew.fc(model, relu9_, \'fc9_\', dim_in=256, dim_out=14)\r\n    \t\t# fc9_, output shape: {[14,1,1]}\r\n\r\n            # FIRST ATTEMPT. Error got:  Tensor type mismatch, caller expects elements to be float while tensor contains double Error from operator\r\n            dist = model.net.SquaredL2Distance([label, fc9_], \'dist\')\r\n    \t\tpredictions = dist.AveragedLoss([], [\'predictions\'])\r\n\r\n            \'\'\'\r\n            # SECOND ATTEMPT: Error got:\r\n    \t\tlabel_float = model.Cast(label, None, to=core.DataType.FLOAT)\r\n    \t\tdist = model.net.SquaredL2Distance([label_float, fc9_], \'dist\')\r\n    \t\tpredictions = dist.AveragedLoss([], [\'predictions\'])\r\n            \'\'\'\r\n\r\n    \t\treturn predictions\r\n\r\n    # this adds the loss and optimizer\r\n    def add_training_operators(self, model, output, label, device_opts, opt_type, base_learning_rate, policy, stepsize, epsilon, beta1, beta2, gamma, momentum) :\r\n    \twith core.DeviceScope(device_opts):\r\n    \t\txent = model.LabelCrossEntropy([output, label], \'xent\')\r\n    \t\tloss = model.AveragedLoss(xent, ""loss"")\r\n\r\n    \t\tmodel.AddGradientOperators([loss])\r\n\r\n    \t\tif opt_type == \'adam\':\r\n    \t\t    if policy == \'step\':\r\n    \t\t        opt = optimizer.build_adam(model, base_learning_rate=base_learning_rate, policy=policy, stepsize=stepsize, beta1=beta1, beta2=beta2, epsilon=epsilon)\r\n    \t\t    elif policy == \'fixed\' or policy == \'inv\':\r\n    \t\t        opt = optimizer.build_adam(model, base_learning_rate=base_learning_rate, policy=policy, beta1=beta1, beta2=beta2, epsilon=epsilon)\r\n    \t\t    print(""adam optimizer selected"")\r\n    \t\telif opt_type == \'sgd\':\r\n    \t\t    if policy == \'step\':\r\n    \t\t        opt = optimizer.build_sgd(model, base_learning_rate=base_learning_rate, policy=policy, stepsize=stepsize, gamma=gamma, momentum=momentum)\r\n    \t\t    elif policy == \'fixed\' or policy == \'inv\':\r\n    \t\t        opt = optimizer.build_sgd(model, base_learning_rate=base_learning_rate, policy=policy, gamma=gamma, momentum=momentum)\r\n    \t\t    print(""sgd optimizer selected"")\r\n    \t\telif opt_type == \'rmsprop\':\r\n    \t\t    if policy == \'step\':\r\n    \t\t        opt = optimizer.build_rms_prop(model, base_learning_rate=base_learning_rate, policy=policy, stepsize=stepsize, decay=gamma, momentum=momentum, epsilon=epsilon)\r\n    \t\t    elif policy == \'fixed\' or policy == \'inv\':\r\n    \t\t        opt = optimizer.build_rms_prop(model, base_learning_rate=base_learning_rate, policy=policy, decay=gamma, momentum=momentum, epsilon=epsilon)\r\n    \t\t    print(""rmsprop optimizer selected"")\r\n    \t\telif opt_type == \'adagrad\':\r\n    \t\t    if policy == \'step\':\r\n    \t\t        opt = optimizer.build_adagrad(model, base_learning_rate=base_learning_rate, policy=policy, stepsize=stepsize, decay=gamma, epsilon=epsilon)\r\n    \t\t    elif policy == \'fixed\' or policy == \'inv\':\r\n    \t\t        opt = optimizer.build_adagrad(model, base_learning_rate=base_learning_rate, policy=policy, decay=gamma, epsilon=epsilon)\r\n    \t\t    print(""adagrad optimizer selected"")\r\n\r\n    def add_accuracy(self, model, output, label, device_opts, eval_metric):\r\n        with core.DeviceScope(device_opts):\r\n            if eval_metric == \'accuracy\':\r\n                accuracy = brew.accuracy(model, [output, label], ""accuracy"")\r\n            elif eval_metric == \'top_k_accuracy\':\r\n                accuracy = brew.accuracy(model, [output, label], ""accuracy"", top_k=3)\r\n            return accuracy\r\n\r\n    def train(self, num_epoch=1000, batch_size=64, context=\'gpu\', eval_metric=\'accuracy\', opt_type=\'adam\', base_learning_rate=0.001, weight_decay=0.001, policy=\'fixed\', stepsize=1, epsilon=1E-8, beta1=0.9, beta2=0.999, gamma=0.999, momentum=0.9) :\r\n        if context == \'cpu\':\r\n            device_opts = core.DeviceOption(caffe2_pb2.CPU, 0)\r\n            print(""CPU mode selected"")\r\n        elif context == \'gpu\':\r\n            device_opts = core.DeviceOption(caffe2_pb2.CUDA, 0)\r\n            print(""GPU mode selected"")\r\n\r\n    \tworkspace.ResetWorkspace(self._model_dir_)\r\n\r\n    \targ_scope = {""order"": ""NHWC""}\r\n    \t# == Training model ==\r\n    \ttrain_model= model_helper.ModelHelper(name=""train_net"", arg_scope=arg_scope)\r\n    \tdata, label = self.add_input(train_model, batch_size=batch_size, db=os.path.join(self._data_dir_, \'torcs-train-nchw-lmdb\'), db_type=\'lmdb\', device_opts=device_opts)\r\n    \tpredictions = self.create_model(train_model, data, label, device_opts=device_opts)\r\n    \tself.add_training_operators(train_model, predictions, label, device_opts, opt_type, base_learning_rate, policy, stepsize, epsilon, beta1, beta2, gamma, momentum)\r\n    \tself.add_accuracy(train_model, predictions, label, device_opts, eval_metric)\r\n    \twith core.DeviceScope(device_opts):\r\n    \t\tbrew.add_weight_decay(train_model, weight_decay)\r\n\r\n    \t# Initialize and create the training network\r\n    \tworkspace.RunNetOnce(train_model.param_init_net)\r\n    \tworkspace.CreateNet(train_model.net, overwrite=True)\r\n\r\n    \t# Main Training Loop\r\n    \tprint(""== Starting Training for "" + str(num_epoch) + "" epochs =="")\r\n    \tfor i in range(num_epoch):\r\n            workspace.RunNet(train_model.net)\r\n\r\n            if i % 50 == 0:\r\n            \tprint \'Iter \' + str(i) + \': \' + \'Loss \' + str(workspace.FetchBlob(""loss"")) + \' - \' + \'Accuracy \' + str(workspace.FetchBlob(\'accuracy\'))\r\n    \tprint(""Training done"")\r\n\r\n    \t# == Deployment model. ==\r\n    \t# We simply need the main AddModel part.\r\n    \tdeploy_model = model_helper.ModelHelper(name=""deploy_net"", arg_scope=arg_scope, init_params=False)\r\n    \tself.create_model(deploy_model, ""data"", label, device_opts)\r\n\r\n    \tprint(""Saving deploy model"")\r\n    \tself.save_net(self.INIT_NET, self.PREDICT_NET, deploy_model)\r\n\r\n    def save_net(self, init_net_path, predict_net_path, model):\r\n\r\n    \tinit_net, predict_net = mobile_exporter.Export(\r\n    \t\tworkspace,\r\n    \t\tmodel.net,\r\n    \t\tmodel.params\r\n    \t)\r\n\r\n        try:\r\n            os.makedirs(self._model_dir_)\r\n        except OSError:\r\n            if not os.path.isdir(self._model_dir_):\r\n                raise\r\n\r\n    \tprint(""Save the model to init_net.pb and predict_net.pb"")\r\n    \twith open(predict_net_path, \'wb\') as f:\r\n    \t\tf.write(model.net._net.SerializeToString())\r\n    \twith open(init_net_path, \'wb\') as f:\r\n    \t\tf.write(init_net.SerializeToString())\r\n\r\n    \tprint(""Save the model to init_net.pbtxt and predict_net.pbtxt"")\r\n\r\n    \twith open(init_net_path.replace(\'.pb\',\'.pbtxt\'), \'w\') as f:\r\n    \t\tf.write(str(init_net))\r\n    \twith open(predict_net_path.replace(\'.pb\',\'.pbtxt\'), \'w\') as f:\r\n    \t\tf.write(str(predict_net))\r\n    \tprint(""== Saved init_net and predict_net =="")\r\n\r\n    def load_net(self, init_net_path, predict_net_path, device_opts):\r\n        if not os.path.isfile(init_net_path):\r\n            logging.error(""Network loading failure. File \'"" + os.path.abspath(init_net_path) + ""\' does not exist."")\r\n            sys.exit(1)\r\n        elif not os.path.isfile(predict_net_path):\r\n            logging.error(""Network loading failure. File \'"" + os.path.abspath(predict_net_path) + ""\' does not exist."")\r\n            sys.exit(1)\r\n\r\n        init_def = caffe2_pb2.NetDef()\r\n    \twith open(init_net_path, \'rb\') as f:\r\n    \t\tinit_def.ParseFromString(f.read())\r\n    \t\tinit_def.device_option.CopyFrom(device_opts)\r\n    \t\tworkspace.RunNetOnce(init_def.SerializeToString())\r\n\r\n    \tnet_def = caffe2_pb2.NetDef()\r\n    \twith open(predict_net_path, \'rb\') as f:\r\n    \t\tnet_def.ParseFromString(f.read())\r\n    \t\tnet_def.device_option.CopyFrom(device_opts)\r\n    \t\tworkspace.CreateNet(net_def.SerializeToString(), overwrite=True)\r\n    \tprint(""== Loaded init_net and predict_net =="")\r\n', '\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n']","['', '\r\ncarlos@carlos-ubuntu:~/Documents/git/Caffe2_scripts/caffe2_torcs_predictor$ python CNNTrainer_dpnet_dpnet.py \r\nGPU mode selected\r\nsgd optimizer selected\r\nWARNING: Logging before InitGoogleLogging() is written to STDERR\r\nW1202 21:31:09.273607 27931 operator.cc:89] Operator Conv does not support the requested feature. Msg: The current padding scheme leads to unequal padding on the left and right, which is not supported by cudnn.. Proto is: input: ""data"" input: ""conv1__w"" input: ""conv1__b"" output: ""conv1_"" name: """" type: ""Conv"" arg { name: ""kernel"" i: 11 } arg { name: ""pad_l"" i: 4 } arg { name: ""pad_b"" i: 4 } arg { name: ""exhaustive_search"" i: 0 } arg { name: ""stride"" i: 4 } arg { name: ""pad_r"" i: 3 } arg { name: ""order"" s: ""NHWC"" } arg { name: ""pad_t"" i: 5 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: ""CUDNN""\r\nW1202 21:31:09.273897 27931 operator.cc:89] Operator Conv does not support the requested feature. Msg: The current padding scheme leads to unequal padding on the left and right, which is not supported by cudnn.. Proto is: input: ""data"" input: ""conv1__w"" input: ""conv1__b"" output: ""conv1_"" name: """" type: ""Conv"" arg { name: ""kernel"" i: 11 } arg { name: ""pad_l"" i: 4 } arg { name: ""pad_b"" i: 4 } arg { name: ""exhaustive_search"" i: 0 } arg { name: ""stride"" i: 4 } arg { name: ""pad_r"" i: 3 } arg { name: ""order"" s: ""NHWC"" } arg { name: ""pad_t"" i: 5 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: ""CUDNN""\r\nI1202 21:31:09.273910 27931 operator.cc:167] Engine CUDNN is not available for operator Conv.\r\nW1202 21:31:09.275804 27931 operator.cc:89] Operator MaxPool does not support the requested feature. Msg: The current padding scheme leads to unequal padding on the left and right, which is not supported by cudnn.. Proto is: input: ""conv1_"" output: ""pool1_"" name: """" type: ""MaxPool"" arg { name: ""kernel"" i: 3 } arg { name: ""pad_l"" i: 1 } arg { name: ""pad_b"" i: 1 } arg { name: ""cudnn_exhaustive_search"" i: 0 } arg { name: ""stride"" i: 2 } arg { name: ""pad_r"" i: 0 } arg { name: ""order"" s: ""NHWC"" } arg { name: ""pad_t"" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: ""CUDNN""\r\nW1202 21:31:09.276072 27931 operator.cc:89] Operator MaxPool does not support the requested feature. Msg: The current padding scheme leads to unequal padding on the left and right, which is not supported by cudnn.. Proto is: input: ""conv1_"" output: ""pool1_"" name: """" type: ""MaxPool"" arg { name: ""kernel"" i: 3 } arg { name: ""pad_l"" i: 1 } arg { name: ""pad_b"" i: 1 } arg { name: ""cudnn_exhaustive_search"" i: 0 } arg { name: ""stride"" i: 2 } arg { name: ""pad_r"" i: 0 } arg { name: ""order"" s: ""NHWC"" } arg { name: ""pad_t"" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: ""CUDNN""\r\nI1202 21:31:09.276083 27931 operator.cc:167] Engine CUDNN is not available for operator MaxPool.\r\nW1202 21:31:09.276993 27931 operator.cc:89] Operator MaxPool does not support the requested feature. Msg: The current padding scheme leads to unequal padding on the left and right, which is not supported by cudnn.. Proto is: input: ""conv5_"" output: ""pool5_"" name: """" type: ""MaxPool"" arg { name: ""kernel"" i: 3 } arg { name: ""pad_l"" i: 1 } arg { name: ""pad_b"" i: 0 } arg { name: ""cudnn_exhaustive_search"" i: 0 } arg { name: ""stride"" i: 2 } arg { name: ""pad_r"" i: 1 } arg { name: ""order"" s: ""NHWC"" } arg { name: ""pad_t"" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: ""CUDNN""\r\nW1202 21:31:09.277192 27931 operator.cc:89] Operator MaxPool does not support the requested feature. Msg: The current padding scheme leads to unequal padding on the left and right, which is not supported by cudnn.. Proto is: input: ""conv5_"" output: ""pool5_"" name: """" type: ""MaxPool"" arg { name: ""kernel"" i: 3 } arg { name: ""pad_l"" i: 1 } arg { name: ""pad_b"" i: 0 } arg { name: ""cudnn_exhaustive_search"" i: 0 } arg { name: ""stride"" i: 2 } arg { name: ""pad_r"" i: 1 } arg { name: ""order"" s: ""NHWC"" } arg { name: ""pad_t"" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: ""CUDNN""\r\nI1202 21:31:09.277199 27931 operator.cc:167] Engine CUDNN is not available for operator MaxPool.\r\nW1202 21:31:13.799096 27931 operator.cc:89] Operator ConvGradient does not support the requested feature. Msg: The current padding scheme leads to unequal padding on the left and right, which is not supported by cudnn.. Proto is: input: ""data"" input: ""conv1__w"" input: ""conv1__grad"" output: ""conv1__w_grad"" output: ""conv1__b_grad"" output: ""data_grad"" name: """" type: ""ConvGradient"" arg { name: ""kernel"" i: 11 } arg { name: ""pad_l"" i: 4 } arg { name: ""pad_b"" i: 4 } arg { name: ""exhaustive_search"" i: 0 } arg { name: ""stride"" i: 4 } arg { name: ""pad_r"" i: 3 } arg { name: ""order"" s: ""NHWC"" } arg { name: ""pad_t"" i: 5 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: ""CUDNN"" is_gradient_op: true\r\nW1202 21:31:13.799773 27931 operator.cc:89] Operator ConvGradient does not support the requested feature. Msg: The current padding scheme leads to unequal padding on the left and right, which is not supported by cudnn.. Proto is: input: ""data"" input: ""conv1__w"" input: ""conv1__grad"" output: ""conv1__w_grad"" output: ""conv1__b_grad"" output: ""data_grad"" name: """" type: ""ConvGradient"" arg { name: ""kernel"" i: 11 } arg { name: ""pad_l"" i: 4 } arg { name: ""pad_b"" i: 4 } arg { name: ""exhaustive_search"" i: 0 } arg { name: ""stride"" i: 4 } arg { name: ""pad_r"" i: 3 } arg { name: ""order"" s: ""NHWC"" } arg { name: ""pad_t"" i: 5 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: ""CUDNN"" is_gradient_op: true\r\nI1202 21:31:13.799808 27931 operator.cc:167] Engine CUDNN is not available for operator ConvGradient.\r\n== Starting Training for 100 epochs ==\r\nWARNING:caffe2.python.workspace:Original python traceback for operator ', ' in network ', ' in exception above (most recent call last):\r\nWARNING:caffe2.python.workspace:  File ""CNNTrainer_dpnet_dpnet.py"", line 24, in <module>\r\nWARNING:caffe2.python.workspace:  File ""/home/carlos/Documents/git/Caffe2_scripts/caffe2_torcs_predictor/CNNCreator_dpnet_dpnet.py"", line 146, in train\r\nWARNING:caffe2.python.workspace:  File ""/home/carlos/Documents/git/Caffe2_scripts/caffe2_torcs_predictor/CNNCreator_dpnet_dpnet.py"", line 86, in create_model\r\nTraceback (most recent call last):\r\n  File ""CNNTrainer_dpnet_dpnet.py"", line 24, in <module>\r\n    stepsize=8000\r\n  File ""/home/carlos/Documents/git/Caffe2_scripts/caffe2_torcs_predictor/CNNCreator_dpnet_dpnet.py"", line 159, in train\r\n    workspace.RunNet(train_model.net)\r\n  File ""/home/carlos/Documents/git/pytorch/build/caffe2/python/workspace.py"", line 217, in RunNet\r\n    StringifyNetName(name), num_iter, allow_fail,\r\n  File ""/home/carlos/Documents/git/pytorch/build/caffe2/python/workspace.py"", line 178, in CallWithExceptionIntercept\r\n    return func(*args, **kwargs)\r\nRuntimeError: [enforce fail at tensor.h:495] IsType<T>(). Tensor type mismatch, caller expects elements to be float while tensor contains double Error from operator: \r\ninput: ""label"" input: ""fc9_"" output: ""dist"" name: """" type: ""SquaredL2Distance"" device_option { device_type: 1 cuda_gpu_id: 0 }\r\n** while accessing input: label\r\n', '', 'label']",0,0
262,pytorch,21683,open,Import ONNX model to Pytorch,"## üöÄ Feature

Importing ONNX models into Pytorch.
## Motivation

Almost all other frameworks already support this. Importing ONNX models into Pytorch makes Pytorch much more flexible.

## Pitch

In , a function should be created to take the ONNX model and outputs a Pytorch model. ",feature module: onnx triaged,"['Hi everyone,\r\nAs i know caffe2 fully supports ONNX import/export and caffe 2 is a part of PyTorch recently. However, I can not import onnx model file to Pytorch and run inference properly. How can I resolve this issue?', ""Agreed. I was kind a shocked when I have learnt PyTorch doesn't have that."", '@soumith, any plans to roll out this functionality anytime soon? ', '`+` to motivation\r\nI need to convert an onnx model to torchscript (.pt) so I can run it on pytorch mobile. \r\nHow can I do this now? Any workarounds?', '+1, not supporting ONNX import feels like half of the ONNX implementation is missing', 'Is there any ongoing PR for this issue?', 'As a heavy torch user,  this feature sounds nice.', 'This feature is relevant, it should be implemented. ', 'Looking forward to have it in torch', 'This feature is very relevant, my motivation:\r\n\r\n- Load ONNX models into pytorch to compare models against same test cases', '@gordinmitya Have you found a solution to your problem?  I am trying to do something similar.', '@solarflarefx no ):\r\npytorch mobile suddenly stoped working without any changes from my side, so I decide to just skip it until mobile version become stable', 'onnxruntime may helps', 'Anyone figured a simple way to do this ? (without onnxruntime)', '@Franzis39 how does onnxruntime help?', ""I think it will be a huge challenge to restore the runtime graph in PyTorch. But if we focus on the model parameters, likes that we only load the parameters stored in an onnx file rather than load the full model, maybe more easy.\r\n\r\nMaybe this snippet will help:\r\n```python\r\nimport onnx\r\nfrom onnx import numpy_helper\r\nimport torch\r\nfrom torchvision import models\r\n\r\nmodel = models.resnet18()\r\ntorch.onnx.export(model, torch.randn(1, 3, 224, 224), 'resnet18.onnx')\r\n\r\nonnx_model = onnx.load('resnet18.onnx')\r\n\r\ngraph = onnx_model.graph\r\ninitalizers = dict()\r\nfor init in graph.initializer:\r\n    initalizers[init.name] = numpy_helper.to_array(init)\r\n\r\nfor name, p in model.named_parameters():\r\n    p.data = (torch.from_numpy(initalizers[name])).data\r\n```\r\n"", 'Found a not-perfect but working solution.\r\nhttps://gist.github.com/qinjian623/6aa777037534c1c1dccbb66f832e93b8', '+1', 'Now 151 likes but no sign of step forward. I dont know how the pytorch project works but is a minimum of reply from the project maintainers to be expected ? \r\n', ""A temporary not perfect time consuming idea I'm going to try is to convert onnx -> keras (onnx2keras) then keras -> pytorch (MMdnn)."", 'It is absolutely ridiculous that this issue is still open honestly. I exported a very time consuming model to Onnx for use with RT and now need to make some modifications to it in Pytorch and I realize that for some reason this is a missing capability. I fully expected that **if I can export to onnx format from pytorch that I should also be able to import it** - rarely have I ever encountered a framework that lets you export to a format without also being able to read that format and it is frankly bad practice. ', '@gchanan any updates on if/when this will be implemented in the future?', '+1 to this request, any ETA?', 'This looks like a pretty new and solid effort to achieve onnx to pytorch conversion: https://github.com/ToriML/onnx2pytorch', 'Any updates? almost 200 votes already.', 'Hi folks, I create a repo for generating pytorch code from onnx.\r\nhttps://github.com/fumihwh/onnx-pytorch\r\nDifferent from `ToriML/onnx2pytorch`, my repo GENERATE pytorch code, not Module instance in stack.']",[],['torch.onnx'],0,0
263,pytorch,31300,open,torch runtime error when manual link libmkldnn.so,"## üêõ Bug

I manual link libmkldnn.so, because I want to use some functions inside mkldnn, but pytorch get runtime error when I manual link libmkldnn.so

## To Reproduce

Steps to reproduce the behavior:


get runtime error


## Expected behavior

No runtime error

## Environment
PyTorch version: 1.3.0
Is debug build: No
CUDA used to build PyTorch: 10.1.243

OS: Ubuntu 16.04.3 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.7
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip3] numpy==1.17.3
[pip3] torch==1.3.0
[pip3] torchvision==0.4.1
[conda] Could not collect


cc @gujinghui @PenghuiCheng @XiaobingSuper @jianyuh",module: build module: mkldnn triaged,"['One possible solution: Pass -Bsymbolic or -Bsymbolic-functions to the linker when compiling pytorch\r\n\r\n<https://stackoverflow.com/questions/6538501/linking-two-shared-libraries-with-some-of-the-same-symbols>\r\n', 'Note that `-Bsymbolic` can have some unexpected side-effects [1]. On linux, you may be able to solve the problem without any build system changes by loading your own `mkldnn` library in a separate namespace using `dlmopen` [2] instead. `dlmopen` is not explicitly provided by ctypes, but can be called directly, and then used to construct a CDLL:\r\n\r\n```\r\nimport ctypes, os\r\n\r\nselfimg = ctypes.CDLL(\'\')\r\nselfimg.dlmopen.restype = ctypes.c_void_p\r\nselfimg.dlerror.restype = ctypes.c_char_p\r\n\r\nlib_path = b""/home/ubuntu/bld/TileDB-r/dist/lib/libtiledb.so""\r\ndlopen_flags = os.RTLD_NOW | os.RTLD_GLOBAL # | os.RTLD_DEEPBIND\r\n\r\ntry:\r\n    _h = selfimg.dlmopen(-1, lib_path, dlopen_flags)\r\n    my_dll = ctypes.CDLL(lib_path, handle=_h)\r\nexcept Exception as exc:\r\n    print(exc)\r\n    print(""Possibly got dlerror: \'{}\'"".format(selfimg.dlerror()))\r\n```\r\n\r\nthen use as usual:\r\n```\r\n>>> my_dll\r\n<CDLL \'my_dll\', handle 5611d5f23810 at 0x7ff5f3e928d0>\r\n\r\n\r\n>>> my_dll.tiledb_ctx_alloc\r\n<_FuncPtr object at 0x7ff5f08d8368>\r\n```\r\n\r\n[1] https://software.intel.com/en-us/articles/performance-tools-for-software-developers-bsymbolic-can-cause-dangerous-side-effects\r\n[2] https://manpages.debian.org/testing/manpages-dev/dlmopen.3.en.html']","['\r\nimport ctypes, os\r\n\r\nmkl_lib_name=""MY_MKL_PATH/lib/libmkldnn.so""\r\ndlopen_flags = os.RTLD_NOW | os.RTLD_GLOBAL # | os.RTLD_DEEPBIND\r\nctypes.CDLL(mkl_lib_name, dlopen_flags)\r\n\r\nnchw = [2, 3, 100, 100]\r\noihw = [4, 3, 5, 5]\r\n\r\nimport torch\r\nm = torch.nn.Conv2d(3, 4, 5, 1, 2)\r\nm(torch.rand(*nchw))\r\n', '\r\nTraceback (most recent call last):\r\n  File ""a.py"", line 13, in <module>\r\n    m(torch.rand(*nchw))\r\n  File ""/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py"", line 541, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py"", line 345, in forward\r\n    return self.conv2d_forward(input, self.weight)\r\n  File ""/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py"", line 342, in conv2d_forward\r\n    self.padding, self.dilation, self.groups)\r\nRuntimeError: std::exception\r\n']",[],0,0
264,pytorch,28249,open,Add scopes to autograd profiler,"## üöÄ Feature

Add scopes within a torchscript model so you can get profile information per scope for both the forward and backward pass.

## Motivation

For distributed execution (model parallelism) we want to be able to measure the exact CPU time spent on different parts of a torchscript module so we can shard the model in a reasonable manner to different machines. We have high level ""components"" that represent shardable units of execution and want to use the autograd profiler to profile per component.

## Pitch

We want to add two custom methods into torch so we can describe the scope. Making them in C++ allows us to use them in torchscript as well in python.

TorchScript pseudocode


Python Sugar


These methods would add scope information to RecordFunction and thus provide it to the autograd Profiler. https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/record_function.h

The current RECORD_FUNCTION implementation appears to track the lineage from backwards functions to the forward pass via sequence_nr so it should be easy to walk the RecordFunction tree and extract scope information. https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/function.h#L104-L116 https://github.com/pytorch/pytorch/blob/master/tools/autograd/gen_variable_type.py#L238

One notable thing is that sequence_nr is currently thread_local. From what I've heard, the autograd/backward pass can be multithreaded and thus we will likely need to extend sequence_nr to include a thread ID as well as the current thread_local sequence_nr.

## Alternatives

There's ways to do this when running in the python environment w/ register_backward_hook + a custom nn.Module however there's no equivalent way to do this with torchscript since it doesn't support register_backward_hook. The autograd backward pass can also run multithreaded from my understanding so that would break that approach anyways.",triaged,"['cc @dzhulgakov @ngimel @aazzolini @bddppq ', 'cc @ilia-cher ', '> the autograd/backward pass can be multithreaded\r\n\r\nActually even if it\'s not multi-threaded, since autograd has its own thread-pool, the backward ops will be invoked by a thread different from the one that executed the forward op (which is also the one that created and assigned its thread local sequence_nr to the backward op). I think we can either 1. encode the thread id into the thread local sequence_nr (like using first two bytes to store the thread id at initialization time) or 2. add an additional field (say ""creation_thread_id"") to the `torch::autograd::Node` class. Both should have very minimal perf impact (but we should run benchmark to proof it).', '@bddppq Why do we want to change that logic? Why would that help here?', 'I believe we already have very similar functionality (albeit it might not be available from TorchScript). Let me find a link to some previous discussion..', 'Some relevant discussions in https://github.com/pytorch/pytorch/issues/19422', 'After https://github.com/pytorch/pytorch/pull/23428 we could probably make `with torch.autograd.profiler.record_function(""scope_name"")` traceable/scriptable, as a way to add support for this into TorchScript', ""@albanD since the sequence IDs are thread local you need some way to reliably tie the backward pass (in a different thread) to the forward pass scopes. From my understanding, if you don't do that you'll end up with sequence ID conflicts from the events.\r\n\r\n@ilia-cher good to know, I'll take a look at doing that"", ""@ilia-cher @d4l3k Oh I didn't know the existence of `torch.autograd._push_range` (and `torch.autograd._pop_range`). But looks like they are directly operating at the `profiler::Event` level, I think we should probably change them to create (and destroy) a RecordFunction (and call its `.before` and dtor to trigger the profiler's push and pop range). The benefit is then it will work for other registered Callback. Re. jit support, I think the easiest way to implement is to register the RecordFunction creation and destroy functions as operators."", '@albanD Yeah what @d4l3k said (basically addressing this https://github.com/pytorch/pytorch/blob/ce16d68/torch/autograd/profiler.py#L423-L432)']","['py\r\ndef forward(self, x):\r\n  handle = torch._profiler_scope_enter(""foo"")\r\n  x = torch.relu(x)\r\n  ...\r\n  torch._profiler_scope_exit(handle)\r\n  return x\r\n', 'py\r\ndef forward(self, x):\r\n  with profiler.scope(""foo""):\r\n    x = torch.relu(x)\r\n    ...\r\n    return x\r\n']",[],0,0
265,pytorch,28884,open,Why attn_mask is not 3D tensor in nn.MultiheadAttention?,"## üöÄ Feature
I think shoud add 3D attn_mask.
## Motivation

For example, if in a translation task,  I want to let every word in the target sentence focus different words in source sentence, "" attn_mask"" should  in shape (tgt_len, scr_len) , ok! That's fine, that's exactly what it is now. But in different sentencesÔºåthe target word in same position may focus different source words, so the ""broadcast"" is unsuitable in this case and "" attn_mask"" should  in shape (batch_size, tgt_len, scr_len) . The same question is also in nn.Transfomers.
So I think it's necessary to fix.

cc @albanD @mruberry @jbschlosser @zhangguanheng66",enhancement module: nn oncall: transformer/mha triaged,[],[],[],0,0
266,pytorch,11532,open,[JIT][tracer] Slicing shape is specialized to tensor rank,"Example:






these  calls we emit (e.g. ) are specialized to the rank of the tensor we called  on",oncall: jit,[],"['\r\nimport torch\r\n\r\ndef fill_row_zero(x):\r\n    x = torch.cat((torch.rand(1, *x.shape[1:]), x[1:]), dim=0)\r\n    return x\r\n\r\ntraced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\r\nprint(traced.graph)\r\ntraced(torch.rand(3, 4, 5))\r\n', '\r\ngraph(%0 : Float(3, 4)) {\r\n  %4 : int = prim::Constant[value=1]()\r\n  %5 : int = aten::size(%0, %4)\r\n  %6 : Long() = prim::NumToTensor(%5)\r\n  %7 : int = prim::TensorToNum(%6)\r\n  %8 : int = prim::Constant[value=1]()\r\n  %9 : int[] = prim::ListConstruct(%8, %7)\r\n  %10 : int = prim::Constant[value=6]()\r\n  %11 : int = prim::Constant[value=0]()\r\n  %12 : int[] = prim::Constant[value=[0, -1]]()\r\n  %13 : Float(1, 4) = aten::rand(%9, %10, %11, %12)\r\n  %14 : int = prim::Constant[value=0]()\r\n  %15 : int = prim::Constant[value=1]()\r\n  %16 : int = prim::Constant[value=9223372036854775807]()\r\n  %17 : int = prim::Constant[value=1]()\r\n  %18 : Float(2, 4) = aten::slice(%0, %14, %15, %16, %17)\r\n  %19 : Dynamic[] = prim::ListConstruct(%13, %18)\r\n  %20 : int = prim::Constant[value=0]()\r\n  %21 : Float(3, 4) = aten::cat(%19, %20)\r\n  return (%21);\r\n}\r\n']","['size()', '  %5 : int = aten::size(%0, %4)', '.shape']",0,0
267,pytorch,20591,open,@ignore annotation for user defined type,"## üöÄ Feature
In user defined type, it would be nice to have a @ignore annotation similar to https://github.com/pytorch/pytorch/pull/16055

This allows the user to write python only methods that are only used in training / debugging (like )

cc @suo",jit-backlog oncall: jit triaged,"['@driazati can you take a look at this since you write the @ignore annotation?', 'This came up in torchvision, combined with the lack of https://github.com/pytorch/pytorch/issues/25462 made things difficult ']",[],['def  #visualize'],0,0
268,pytorch,7214,open,Do not put system paths in RPATH,"This is what I'm seeing today:


Paths  and  were taken from the build environment, but they do not belong in RPATH since those paths are not shipped with the application. Those paths may contain versions of DT_NEEDED libraries that you do not want to use with pytorch, however RPATH has precedence over everything else, so those libraries will get picked, and there is no way around that.

For example, let's say libcublas provided in   is not the version I want since it's too old, I have a newest version in , but I have no way to point to it since  will be ignored.

Caused by: https://github.com/pytorch/pytorch/pull/3255 (for )



cc @malfet @seemethere @walterddr",module: build triaged,"['So, if I understand correctly, the tension is:\r\n\r\n1. Having the PyTorch dynlib work out of the box on the build machine when cuDNN is installed in a non-standard path without having to add `LD_LIBRARY_PATH`, vs.\r\n2. Having the PyTorch dynlib respect default search order when it is built as a distributable and shipped somewhere else\r\n\r\nSo it seems, at least, there should be a way to prevent the RPATH from being set. But maybe, due to (1), we should still put in the RPATH by default.', '> So it seems, at least, there should be a way to prevent the RPATH from being set.\r\n\r\nThat would be fine, yes.\r\nRegarding whether it should be the default or not, depend how common the use case 1 is, in your experience. I think the whole situation has improved now:\r\n- cuDNN has deb packages and is now installed at the standard path, respecting multi-arch.\r\n- Package `cuda-cudart-9-0` will install `/etc/ld.so.conf.d/cuda-9-0.conf` which will add `/usr/local/cuda/lib64` to `ld.so.cache`.\r\n- Driver libraries (like `libcuda.so`) are now installed in a standard path in Ubuntu 18.04, instead of `/usr/lib/nvidia-390`. In ubuntu 16.04 it was not the case, but a file was installed in `/etc/ld.so.conf.d`.\r\n\r\nSo, notwithstanding driver install issues, you can get the latest CUDA,, cuDNN and NVIDIA driver with just `apt-get` commands, and everything will be at standard paths.', '@nikitaved Is this still an issue? ']",['\r\n$ readelf -d /opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/_nvrtc.cpython-36m-x86_64-linux-gnu.so | grep RPATH\r\n 0x000000000000000f (RPATH)              Library rpath: [/opt/conda/envs/pytorch-py3.6/lib:/usr/lib/x86_64-linux-gnu/:/usr/local/cuda/lib64:$ORIGIN/lib]\r\n'],"['/usr/lib/x86_64-linux-gnu', '/usr/local/cuda/lib64', '/usr/local/cuda/lib64', '~/cuda/lib64', 'LD_LIBRARY_PATH', '/usr/lib/x86_64-linux-gnu']",0,0
269,pytorch,12855,open,Model with Caffe2 runs much slower than it with pytorch in GPU mode !!!!,"Firstly, I run my model(CRNN model) on Pytorch GPU. With code:

**Result:  cost time: 0.002339872884750366**

Secondly, I convert my pytorch model to onnx and then convert onnx to pb in caffe2. With code:


and 


Thirdly,I run model in Caffe2 on GPU.  With code:

**Run time per RunNet: 0.005980247449874878**

**But, the speed is slower than that in Pytorch !  How weird!** 

Any one konws what happens!
",caffe2,"['Any chance you can upload your protobuf?', '@ezyang Sorry, Could you tell me how to open the protobuf file created by Caffe2.', 'Basically, can you upload `ocr_api.onnx` and `init_net.pb`?', '@ezyang  Thanks very much! I have uploaded into my Google Drive. \r\n\r\nhttps://drive.google.com/drive/folders/1XkRHQJ_dUVAVCFKUL-5ikcMaCLRY7vTd?usp=sharing\r\n', 'I met the same problem, how can I speed up the caffe2 model?']","[""\r\ndirectory = os.path.dirname(os.path.realpath(__file__))\r\nmodel_file = os.path.join(directory, './pth/netCRNN_epoch17_idx78105_step2733675.pth')\r\nkeys = new_key.KEY_4498\r\nmodel = Resnet18(False, num_classes=len(keys) + 2)\r\n\r\nstate_dict = torch.load(model_file)\r\nmodel.load_state_dict(state_dict)\r\nmodel.cuda()\r\nmodel.train(False)\r\n\r\nbegin = time.time()\r\ntol_time = 0\r\nfor i in range(5000):\r\n    x = torch.randn(1, 3, 32, 512, requires_grad=False).cuda()\r\n    begin = time.time()\r\n    out = model(x)\r\n    end = time.time()\r\n    tol_time += (end - begin)\r\n\r\nprint('cost time:', tol_time / 5000)\r\n"", '\r\ndirectory = os.path.dirname(os.path.realpath(__file__))\r\nmodel_file = os.path.join(directory, \'./pth/netCRNN_epoch17_idx78105_step2733675.pth\')\r\nkeys = new_key.KEY_4498\r\nmodel = Resnet18(False, num_classes=len(keys) + 2)\r\n\r\nstate_dict = torch.load(model_file)\r\nmodel.load_state_dict(state_dict)\r\nmodel.cuda()\r\nmodel.train(False)\r\n\r\nx = torch.randn(1, 3, 32, 512, requires_grad=False).cuda()\r\n\r\n# Export the model\r\ntorch_out = torch.onnx._export(model,  # model being run\r\n                               x,  # model input (or a tuple for multiple inputs)\r\n                               ""ocr_api.onnx"",  # where to save the model (can be a file or file-like object)\r\n                               export_params=True)\r\n', '\r\nconvert-onnx-to-caffe2 ocr_api.onnx --output predict_net.pb --init-net-output init_net.pb\r\n', ""\r\nworkspace.ResetWorkspace()\r\ndevice_opts = core.DeviceOption(caffe2_pb2.CUDA, 0)\r\n\r\ninput_data = np.random.rand(1, 3, 32, 128).astype(np.float32)  # NCHW\r\n\r\ninit_def = caffe2_pb2.NetDef()\r\nwith open('init_net.pb', 'rb') as f:\r\n    init_def.ParseFromString(f.read())\r\n    init_def.device_option.CopyFrom(device_opts)\r\n    workspace.RunNetOnce(init_def)\r\n    # workspace.RunNetOnce(init_def.SerializeToString())\r\n\r\nnet_def = caffe2_pb2.NetDef()\r\nwith open('predict_net.pb', 'rb') as f:\r\n    net_def.ParseFromString(f.read())\r\n    net_def.device_option.CopyFrom(device_opts)\r\n\r\n    input_name = net_def.external_input[0]\r\n    workspace.FeedBlob(input_name, input_data,device_opts)\r\n    workspace.CreateNet(net_def)\r\n    # workspace.CreateNet(net_def.SerializeToString())\r\n\r\nname = net_def.name\r\noutput_name = net_def.external_output[-1]\r\ninput_name = net_def.external_input[0]\r\nprint(name, input_name, output_name)\r\n\r\nnum_iters = 5000\r\n\r\ntol_time = 0\r\nfor i in range(num_iters):\r\n    input_data = np.random.rand(1, 3, 32, 512).astype(np.float32)  # NCHW\r\n    start = time.time()\r\n    workspace.FeedBlob(input_name, input_data,device_opts)\r\n    workspace.RunNet(name, 1)\r\n    results = workspace.FetchBlob(output_name)\r\n    end = time.time()\r\n    tol_time += (end - start)\r\n\r\nprint('Run time per RunNet: {}'.format(tol_time / num_iters))\r\n""]",[],0,0
270,pytorch,28882,open,Support RRef[T].__call__(*args) which invokes T.__call__(*args) on owner,"This is a necessary syntax sugar to for the following use case:



We can implement  by sending a message to the owner,
triggering the owner to run  locally, and returning immediately another RRef of the output.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528",module: rpc triaged,"['While this sugar is very tempting for its terseness,\r\n1) it is hard to enforce type-wise since it would require RRef[T] to adopt T\'s interface.\r\n2) its semantic is confusing in case T has methods such as ""owner"", ""to_here"" or such\r\n3) it hides the fact that we\'re calling a remote function\r\n4) it\'s unclear if the semantic should be ""rpc"", ""rpc_sync"" or ""rpc_async"". I believe there\'s no sensible default here.\r\n\r\nOne possible middle ground, while not as attractive syntax wise, would be:\r\n\r\nmodel.rpc_sync(MyModel.forward, input)\r\nmodel.rpc_async(MyModel.forward, input)\r\nmodel.remote(MyModel.forward, input)\r\n\r\nor\r\n\r\nmodel.rpc_sync(MyModel.forward)(input)', ""1. Would it be sufficient to pass back an exception message if `T` does not have a `__call__()` API?\r\n2. I was only planning to add support for the `__call__()` API, whose behavior would be:\r\n    * The user sends a message to RRef owner. \r\n    * When owner receives the message, it retrieves the `OwnerRRef` , and then gets the `py::object` of the `OwnerRRef`. \r\n    * If `__call__()` is not available on that `py::object`, return an exception message. Otherwise, run `__call__()` and send back the return value.\r\n3. Good point. \r\n4. I'd rather drop the `MyModel.forward` in the argument list, i.e., `mode.remote(input)`. This would avoid the problem in bullet 3. If the application would like to provide a different function then `__call__`, they could use the raw API. "", '(4) is about the fact that it\'s not sensible to pick ""remote"", ""rpc_sync"", or ""rpc_async"" for the user -- the user should pick one explicitly.\r\n\r\nIf you want to expose it only for __call__, then I\'d suggest you implement it as such:\r\n\r\nrref.call_sync(inputs)\r\nrref.call_async(inputs)\r\nrref.remote_call(inputs)\r\n\r\nMost of the time though you want to call more than one method in a remote object, so having a bit more general version of the above would help too.', ""I think we shouldn't do this for (3) alone.\r\n\r\nIf folks want to create a little wrapper that does this for them (i.e. a RRef wrapper class that makes the RPC call for you), then they should go right ahead. But for pure PyTorch RPC code, we must always know where the RPC calls are made, so we can reason about the execution of a program. As soon as you hide this, you can no longer look at a program and reason about its execution."", ""> I think we shouldn't do this for (3) alone.\r\n\r\nYes, discussed offline, @aazzolini proposed to do this explicitly in a `RemoteModule` class, and I agree that this will make it clearer.\r\n\r\nHowever, above won't solve the following concern:\r\n\r\n>  But for pure PyTorch RPC code, we must always know where the RPC calls are made, so we can reason about the execution of a program.\r\n\r\nEven without changes in `RemoteModule` or `RRef.__call__`, our distributed autograd and distributed optimizer are sending an unknown amount of rpc calls in constructor, `backward()`, and `step()`. The amount of RPCs could be none, to all owners or any subset, depending on the input data. There is a trade off here between API simplicity and RPC clarity. "", ""It is true that ultimately some layers will hide the RPC calls. What makes it particularly confusing for this specific use case is that the same syntax could either call a local or remote module, and could return a tensor, a future (or an RRef depending on the choice of flavor) -- so local vs remote calls are actually different APIs (with incompatible typing too) disguised as the same API.\r\nIt's also confusing for other reasons such as : why can I do module.forward() but not remote_module.forward() , etc ""]","['python\r\nclass MyModel(nn.Module):\r\n   pass\r\n\r\nmodel = rpc.remote(""worker0"", MyModel)\r\n# The following line should result in an RPC call to worker0 to\r\n# run MyModel constructor\r\noutputs_rref = model(inputs)\r\n']","['RRef[T].__call__(*args)', 'T.__call__(*args)']",0,0
271,pytorch,30413,open,nn.functional should maintain API parity with nn where possible,"## üöÄ Feature
 should provide a functional alternative to every stateful component in  that supports it.

Some, like  were deprecated in  because they're now the same as those in .

## Motivation

People usually import modules and submodules that are used often under short intuitive names.

One such name I've often encountered is , in the same vein as importing .

The current plan to move some math functions permanently away from  somewhat awkwardly splits the API.

1. Every operation that has a  has its name in  (to my knowledge).
2. Almost every operation (s) in  that can be exposed cleanly in a functional manner has a name in . But some don't.

## Pitch

Move all 'mathy' functions with class counterparts from  to  and expose aliases to the basic/fundamental ones in .

Here we get into ugly territory; I don't know how exactly I'd argue for some methods currently under , but ,  and co definitely belong in .

## Alternatives

Flip the implementation/re-exporting tale with the impls in  and the re-exports in .

## Additional context

E.g.
https://github.com/pytorch/pytorch/issues/6245 : Deprecate torch.nn.functional.tanh?
",enhancement module: nn triaged,[],[],"['nn.functional', 'nn', 'Tanh() -> tanh', 'nn.functional', 'torch', 'nn.functional as F', 'nn', 'nn.functional', 'Module', 'nn', 'Module', 'nn', 'nn.functional', 'torch', 'nn.functional', 'torch', 'torch', 'tanh', 'sigmoid', 'functional', 'torch', 'nn.functional']",0,0
272,pytorch,21459,open,RuntimeError: cublas runtime error ,"RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:411
My GPU is 2080ti,CUDA 10.0
How can I slove the problem?Thank you.",module: cublas triaged,"[""Without further information, it's hard to say what the problem is. Here are some related issues https://github.com/pytorch/pytorch/issues/7548 https://github.com/pytorch/pytorch/issues/17334\r\n\r\nPlease copy and paste the output of https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py in here, as it can provide useful information to help us debug further."", 'Thank you for your reply. @fmassa \r\nThe env output is :\r\nCollecting environment information...\r\nPyTorch version: 0.4.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: GeForce RTX 2080\r\nNvidia driver version: 410.48\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.5.0\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.4\r\n[pip3] torch==0.4.1\r\n[pip3] torch-sparse==0.4.0\r\n[pip3] torchvision==0.2.1\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.1                      144  \r\n[conda] mkl-service               1.1.2            py36he904b0f_5  \r\n[conda] mkl_fft                   1.0.10           py36ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py36hd81dba3_0  \r\n[conda] pytorch                   1.0.1           cuda100py36he554f03_0  \r\n[conda] torchvision               0.2.1                    py36_0\r\n\r\n\r\nWhen I run my own script, the error is:\r\nTraceback (most recent call last):\r\n  File ""tools/demo.py"", line 188, in <module>\r\n    demo()\r\n  File ""tools/demo.py"", line 174, in demo\r\n    corner_pred = eval_net(seg_pred, vertex_pred).cpu().detach().numpy()[0]\r\n  File ""/home/lab/.local/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 477, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""/home/lab/.local/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py"", line 121, in forward\r\n    return self.module(*inputs[0], **kwargs[0])\r\n  File ""/home/lab/.local/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 477, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""tools/demo.py"", line 54, in forward\r\n    return ransac_voting_layer_v3(mask, vertex_pred, 512, inlier_thresh=0.99)\r\n  File ""/home/lab/pvnet/lib/ransac_voting_gpu_layer/ransac_voting_gpu.py"", line 592, in ransac_voting_layer_v3\r\n    ATA=torch.matmul(normal.permute(0,2,1),normal)              # [vn,2,2]\r\nRuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:411\r\n\r\n\r\n', 'Seems to be a double post of #21587 ']",[],[],0,0
273,pytorch,9853,open,TestSequenceOps.test_gather_padding failing,"reference(*inputs)threshold

Sample: https://ci.pytorch.org/jenkins/job/caffe2-builds/job/py2-cuda9.0-cudnn7-aten-ubuntu16.04-test/4954/consoleText",caffe2,['This issue only occurs when Hypothesis is upgraded past 3.59'],[],"['', '\r\n=================================== FAILURES ===================================\r\n_____________________ TestSequenceOps.test_gather_padding ______________________\r\n\r\nself = <caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>\r\n\r\n    @given(start_pad_width=st.integers(min_value=0, max_value=2),\r\n>          end_pad_width=st.integers(min_value=0, max_value=2),\r\n           args=_gen_test_add_padding(with_pad_data=True),\r\n           **hu.gcs)\r\n    def test_gather_padding(self, start_pad_width, end_pad_width, args, gc, dc):\r\n\r\nlib/python2.7/dist-packages/caffe2/python/operator_test/sequence_ops_test.py:189: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n../lib/python2.7/dist-packages/hypothesis/core.py:604: in execute\r\n    result = self.test_runner(data, run)\r\n../lib/python2.7/dist-packages/hypothesis/executors.py:58: in default_new_style_executor\r\n    return function(data)\r\n../lib/python2.7/dist-packages/hypothesis/core.py:595: in run\r\n    return test(*args, **kwargs)\r\nlib/python2.7/dist-packages/caffe2/python/operator_test/sequence_ops_test.py:189: in test_gather_padding\r\n    end_pad_width=st.integers(min_value=0, max_value=2),\r\n../lib/python2.7/dist-packages/hypothesis/core.py:542: in test\r\n    result = self.test(*args, **kwargs)\r\nlib/python2.7/dist-packages/caffe2/python/operator_test/sequence_ops_test.py:207: in test_gather_padding\r\n    reference=partial(_gather_padding_ref, start_pad_width, end_pad_width))\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nself = <caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>\r\ndevice_option = device_type: 1\r\n\r\nop = input: ""data""\r\ninput: ""lengths""\r\noutput: ""start_padding""\r\noutput: ""end_padding""\r\nn...\r\narg {\r\n  name: ""end_padding_width""\r\n  i: 0\r\n}\r\ndevice_option {\r\n  device_type: 1\r\n}\r\n\r\ninputs = [array([], dtype=float64), array([], dtype=int32)]\r\nreference = <functools.partial object at 0x7fba1f33bb50>\r\ninput_device_options = None, threshold = 0.0001, output_to_grad = None\r\ngrad_reference = None, atol = 0.0001, outputs_to_check = [0, 1]\r\n\r\n    def assertReferenceChecks(\r\n        self,\r\n        device_option,\r\n        op,\r\n        inputs,\r\n        reference,\r\n        input_device_options=None,\r\n        threshold=1e-4,\r\n        output_to_grad=None,\r\n        grad_reference=None,\r\n        atol=None,\r\n        outputs_to_check=None,\r\n    ):\r\n        """"""\r\n            This runs the reference Python function implementation\r\n            (effectively calling ', ', and compares that\r\n            to the output of output, with an absolute/relative tolerance\r\n            given by the ', ' parameter.\r\n    \r\n            Useful for checking the implementation matches the Python\r\n            (typically NumPy) implementation of the same functionality.\r\n    \r\n            Usage example:\r\n    \r\n                @given(X=hu.tensor(), inplace=st.booleans(), **hu.gcs)\r\n                def test_softsign(self, X, inplace, gc, dc):\r\n                    op = core.CreateOperator(\r\n                        ""Softsign"", [""X""], [""X"" if inplace else ""Y""])\r\n    \r\n                    def softsign(X):\r\n                        return (X / (1 + np.abs(X)),)\r\n    \r\n                    self.assertReferenceChecks(gc, op, [X], softsign)\r\n            """"""\r\n        op = copy.deepcopy(op)\r\n        op.device_option.CopyFrom(device_option)\r\n    \r\n        with temp_workspace():\r\n            if (len(op.input) > len(inputs)):\r\n                raise ValueError(\r\n                    \'must supply an input for each input on the op: %s vs %s\' %\r\n                    (op.input, inputs))\r\n            _input_device_options = input_device_options or \\\r\n                core.InferOpBlobDevicesAsDict(op)[0]\r\n            for (n, b) in zip(op.input, inputs):\r\n                workspace.FeedBlob(\r\n                    n,\r\n                    b,\r\n                    device_option=_input_device_options.get(n, device_option)\r\n                )\r\n            net = core.Net(""opnet"")\r\n            net.Proto().op.extend([op])\r\n            test_shape_inference = False\r\n            try:\r\n                (shapes, types) = workspace.InferShapesAndTypes([net])\r\n                test_shape_inference = True\r\n            except RuntimeError as e:\r\n                # Temporarily catch runtime errors when inferring shape\r\n                # and type info\r\n                logging.warning(str(e))\r\n                if os.getenv(\'CAFFE2_ASSERT_SHAPEINFERENCE\') == \'1\':\r\n                    raise e\r\n            workspace.RunNetOnce(net)\r\n            reference_outputs = reference(*inputs)\r\n            if not (isinstance(reference_outputs, tuple) or\r\n                    isinstance(reference_outputs, list)):\r\n                raise RuntimeError(\r\n                    ""You are providing a wrong reference implementation. A ""\r\n                    ""proper one should return a tuple/list of numpy arrays."")\r\n            if not outputs_to_check:\r\n                self.assertEqual(len(reference_outputs), len(op.output))\r\n                outputs_to_check = list(range(len(op.output)))\r\n            outs = []\r\n            for (output_index, ref) in zip(outputs_to_check, reference_outputs):\r\n                output_blob_name = op.output[output_index]\r\n                output = workspace.FetchBlob(output_blob_name)\r\n                if output.dtype.kind in (\'S\', \'O\'):\r\n                    np.testing.assert_array_equal(output, ref)\r\n                else:\r\n                    if atol is None:\r\n                        atol = threshold\r\n                    np.testing.assert_allclose(\r\n                        output, ref, atol=atol, rtol=threshold,\r\n                        err_msg=(\r\n                            \'Output {0} is not matching the reference\'.format(\r\n>                               output_blob_name,\r\n                            )),\r\n                    )\r\nE                   AssertionError: \r\nE                   Not equal to tolerance rtol=0.0001, atol=0.0001\r\nE                   Output start_padding is not matching the reference\r\nE                   (mismatch 100.0%)\r\nE                    x: array(-4618794431967920128)\r\nE                    y: array(0.)\r\n\r\nlib/python2.7/dist-packages/caffe2/python/hypothesis_test_util.py:575: AssertionError\r\n----------------------------- Captured stderr call -----------------------------\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nWARNING:caffe2.python.workspace:CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\n------------------------------ Captured log call -------------------------------\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\nworkspace.py               331 WARNING  CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: data type: float64\r\n---------------------------------- Hypothesis ----------------------------------\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=0, args=(array([], dtype=int32), array([], dtype=float32), 0.0, 0.0), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=2, args=(array([ 8, 10,  9,  0], dtype=int32),\r\n array([ 0.3465074 , -0.5783911 ,  0.3465074 ,  0.8693086 ,  0.3465074 ,\r\n        -0.52675825,  0.3465074 ,  0.3465074 ,  0.3465074 ,  0.9321062 ,\r\n        -0.9681313 ,  0.3465074 ,  0.85339016, -0.995281  ,  0.3465074 ,\r\n         0.3465074 ,  0.3465074 , -0.6266759 ,  0.3465074 ,  0.25781822,\r\n         0.3465074 ,  0.38632435, -0.44261232,  0.3465074 ,  0.86272967,\r\n         0.3465074 ,  0.3465074 ], dtype=float32),\r\n -0.7661220172644854,\r\n 0.32672292989354795), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=2, args=(array([10,  9, 10,  1,  7], dtype=int32),\r\n array([ 0.62059796,  0.62059796,  0.62059796, -0.1115087 ,  0.62059796,\r\n         0.62059796,  0.62059796,  0.62059796,  0.62059796,  0.03708775,\r\n         0.62059796,  0.62059796, -0.6321088 ,  0.62059796,  0.62059796,\r\n         0.8428636 , -0.97314036, -0.664075  ,  0.62059796,  0.62059796,\r\n         0.62059796,  0.62059796,  0.62059796,  0.62059796,  0.62059796,\r\n         0.5944725 ,  0.62059796,  0.62059796,  0.62059796,  0.62059796,\r\n         0.62059796, -0.2803797 ,  0.62059796,  0.5317449 ,  0.62059796,\r\n         0.62059796,  0.62059796], dtype=float32),\r\n -0.7638539007421339,\r\n -0.7086946573830012), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=2, end_pad_width=2, args=(array([0, 8, 7, 4, 0], dtype=int32), array([[[0.7863818],\r\n         [0.7863818],\r\n         [0.7863818]],\r\n \r\n        [[0.7863818],\r\n         [0.7863818],\r\n         [0.7863818]],\r\n \r\n        [[0.7863818],\r\n         [0.7863818],\r\n         [0.7863818]],\r\n \r\n        [[0.7863818],\r\n         [0.7863818],\r\n         [0.7863818]],\r\n \r\n        [[0.7863818],\r\n         [0.7863818],\r\n         [0.7863818]],\r\n \r\n        [[0.7863818],\r\n         [0.7863818],\r\n         [0.7863818]],\r\n \r\n        [[0.7863818],\r\n         [0.7863818],\r\n         [0.7863818]],\r\n \r\n        [[0.7863818],\r\n         [0.7863818],\r\n         [0.7863818]],\r\n \r\n        [[0.7863818],\r\n         [0.7863818],\r\n         [0.7863818]],\r\n \r\n        [[0.7863818],\r\n         [0.7863818],\r\n         [0.7863818]],\r\n \r\n        [[0.7863818],\r\n         [0.7863818],\r\n         [0.7863818]],\r\n \r\n        [[0.7863818],\r\n         [0.7863818],\r\n         [0.7863818]],\r\n \r\n        [[0.7863818],\r\n         [0.7863818],\r\n         [0.7863818]],\r\n \r\n        [[0.7863818],\r\n         [0.7863818],\r\n         [0.7863818]],\r\n \r\n        [[0.7863818],\r\n         [0.7863818],\r\n         [0.7863818]],\r\n \r\n        [[0.7863818],\r\n         [0.7863818],\r\n         [0.7863818]],\r\n \r\n        [[0.7863818],\r\n         [0.7863818],\r\n         [0.7863818]],\r\n \r\n        [[0.7863818],\r\n         [0.7863818],\r\n         [0.7863818]],\r\n \r\n        [[0.7863818],\r\n         [0.7863818],\r\n         [0.7863818]]], dtype=float32), array([[0.41727993],\r\n        [0.41727993],\r\n        [0.41727993]], dtype=float32), array([[-0.0011362 ],\r\n        [-0.29718485],\r\n        [-0.0011362 ]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [ 0.61785346, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[ 0.02449792, -0.725019  ],\r\n        [-0.6544955 , -0.92966264],\r\n        [-0.8234622 , -0.8234622 ]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 689, in evaluate_test_data\r\n    result = self.execute(data, collect=True)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 604, in execute\r\n    result = self.test_runner(data, run)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/executors.py"", line 58, in default_new_style_executor\r\n    return function(data)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 600, in run\r\n    return test(*args, **kwargs)\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/sequence_ops_test.py"", line 189, in test_gather_padding\r\n    end_pad_width=st.integers(min_value=0, max_value=2),\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 542, in test\r\n    result = self.test(*args, **kwargs)\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/sequence_ops_test.py"", line 207, in test_gather_padding\r\n    reference=partial(_gather_padding_ref, start_pad_width, end_pad_width))\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/hypothesis_test_util.py"", line 575, in assertReferenceChecks\r\n    output_blob_name,\r\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/_private/utils.py"", line 1443, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/_private/utils.py"", line 780, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=0.0001, atol=0.0001\r\nOutput end_padding is not matching the reference\r\n(mismatch 100.0%)\r\n x: array(38654705669)\r\n y: array([[0., 0.],\r\n       [0., 0.],\r\n       [0., 0.]])\r\n\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [ 0.61785346, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[ 0.02449792, -0.725019  ],\r\n        [-0.6544955 , -0.92966264],\r\n        [-0.8234622 , -0.8234622 ]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 689, in evaluate_test_data\r\n    result = self.execute(data, collect=True)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 604, in execute\r\n    result = self.test_runner(data, run)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/executors.py"", line 58, in default_new_style_executor\r\n    return function(data)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 600, in run\r\n    return test(*args, **kwargs)\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/sequence_ops_test.py"", line 189, in test_gather_padding\r\n    end_pad_width=st.integers(min_value=0, max_value=2),\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 542, in test\r\n    result = self.test(*args, **kwargs)\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/sequence_ops_test.py"", line 207, in test_gather_padding\r\n    reference=partial(_gather_padding_ref, start_pad_width, end_pad_width))\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/hypothesis_test_util.py"", line 575, in assertReferenceChecks\r\n    output_blob_name,\r\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/_private/utils.py"", line 1443, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/_private/utils.py"", line 780, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=0.0001, atol=0.0001\r\nOutput end_padding is not matching the reference\r\n(mismatch 100.0%)\r\n x: array(38654705669)\r\n y: array([[0., 0.],\r\n       [0., 0.],\r\n       [0., 0.]])\r\n\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[ 0.61785346,  0.61785346],\r\n         [-0.62575763,  0.61785346],\r\n         [ 0.61785346,  0.61785346]],\r\n \r\n        [[ 0.61785346,  0.61785346],\r\n         [ 0.61785346,  0.61785346],\r\n         [ 0.61785346,  0.61785346]],\r\n \r\n        [[ 0.61785346,  0.61785346],\r\n         [ 0.61785346,  0.61785346],\r\n         [ 0.61785346,  0.61785346]]], dtype=float32), array([[-0.71080756, -0.71080756],\r\n        [-0.71080756, -0.71080756],\r\n        [ 0.2294148 , -0.71080756]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([ 2, 10], dtype=int32),\r\n array([-0.4377519 ,  0.31514454, -0.3215343 , -0.6535671 ,  0.31514454,\r\n         0.06280098,  0.31514454,  0.31514454,  0.31514454,  0.31514454,\r\n         0.35126773,  0.31514454], dtype=float32),\r\n -0.8295155601085189,\r\n 0.43787003682785286), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([ 2, 10], dtype=int32),\r\n array([-0.4377519 ,  0.31514454, -0.3215343 , -0.6535671 ,  0.31514454,\r\n         0.06280098,  0.31514454,  0.31514454,  0.31514454,  0.31514454,\r\n         0.35126773,  0.31514454], dtype=float32),\r\n -0.8295155601085189,\r\n 0.43787003682785286), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([ 2, 10], dtype=int32),\r\n array([-0.4377519 ,  0.31514454, -0.3215343 , -0.6535671 ,  0.31514454,\r\n         0.06280098,  0.31514454,  0.31514454,  0.31514454,  0.31514454,\r\n         0.35126773,  0.31514454], dtype=float32),\r\n -0.8295155601085189,\r\n 0.43787003682785286), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=2, end_pad_width=0, args=(array([2, 1], dtype=int32),\r\n array([-0.12546732, -0.12546732, -0.12546732], dtype=float32),\r\n -0.6535670833836938,\r\n 0.6457115366779941), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([], dtype=int32),\r\n array([], shape=(0, 3, 2), dtype=float32),\r\n array([[-0.71080756, -0.71080756],\r\n        [-0.17237663, -0.71080756],\r\n        [-0.12546732,  0.6818886 ]], dtype=float32),\r\n array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=2, end_pad_width=2, args=(array([], dtype=int32),\r\n array([], dtype=float32),\r\n -0.12524509770606265,\r\n 0.11963186579509325), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([], dtype=int32),\r\n array([], shape=(0, 3, 2), dtype=float32),\r\n array([[-0.71080756, -0.71080756],\r\n        [-0.17237663, -0.71080756],\r\n        [-0.12546732,  0.6818886 ]], dtype=float32),\r\n array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=2, end_pad_width=2, args=(array([], dtype=int32),\r\n array([], dtype=float32),\r\n -0.12524509770606265,\r\n 0.11963186579509325), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([1], dtype=int32),\r\n array([[-0.12546732,  0.61785346]], dtype=float32),\r\n array([ 0.2294148 , -0.71080756], dtype=float32),\r\n array([-0.50764036, -0.50764036], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([1], dtype=int32),\r\n array([[-0.12546732,  0.61785346]], dtype=float32),\r\n array([ 0.2294148 , -0.71080756], dtype=float32),\r\n array([-0.50764036, -0.50764036], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([1], dtype=int32),\r\n array([[-0.12546732,  0.61785346]], dtype=float32),\r\n array([ 0.2294148 , -0.71080756], dtype=float32),\r\n array([-0.50764036, -0.50764036], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([1], dtype=int32),\r\n array([[-0.12546732,  0.61785346]], dtype=float32),\r\n array([ 0.2294148 , -0.71080756], dtype=float32),\r\n array([-0.50764036, -0.50764036], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([1], dtype=int32),\r\n array([[-0.12546732,  0.61785346]], dtype=float32),\r\n array([ 0.2294148 , -0.71080756], dtype=float32),\r\n array([-0.50764036, -0.50764036], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([1], dtype=int32),\r\n array([[-0.12546732,  0.61785346]], dtype=float32),\r\n array([ 0.2294148 , -0.71080756], dtype=float32),\r\n array([-0.50764036, -0.50764036], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([ 3,  2, 10], dtype=int32),\r\n array([-1.3770291e-04, -1.3770291e-04,  6.1789516e-02, -6.5356708e-01,\r\n        -1.3770291e-04, -1.3770291e-04, -7.5024533e-01, -1.3770291e-04,\r\n        -1.3770291e-04, -1.3770291e-04,  3.5126773e-01, -1.3770291e-04,\r\n        -1.3770291e-04,  8.8494122e-02, -1.8391909e-01], dtype=float32),\r\n 0.7273016037656251,\r\n 0.18778884372890753), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=2, end_pad_width=0, args=(array([3, 2, 1], dtype=int32),\r\n array([-0.12546732, -0.12546732, -0.12546732, -0.12546732, -0.12546732,\r\n        -0.12546732], dtype=float32),\r\n -0.6535670833836938,\r\n 0.6457115366779941), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([ 3,  2, 10], dtype=int32),\r\n array([-1.3770291e-04, -1.3770291e-04,  6.1789516e-02, -6.5356708e-01,\r\n        -1.3770291e-04, -1.3770291e-04, -7.5024533e-01, -1.3770291e-04,\r\n        -1.3770291e-04, -1.3770291e-04,  3.5126773e-01, -1.3770291e-04,\r\n        -1.3770291e-04,  8.8494122e-02, -1.8391909e-01], dtype=float32),\r\n 0.7273016037656251,\r\n 0.18778884372890753), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=2, end_pad_width=0, args=(array([3, 2, 1], dtype=int32),\r\n array([-0.12546732, -0.12546732, -0.12546732, -0.12546732, -0.12546732,\r\n        -0.12546732], dtype=float32),\r\n -0.6535670833836938,\r\n 0.6457115366779941), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=2, end_pad_width=0, args=(array([3], dtype=int32),\r\n array([-0.12546732, -0.12546732, -0.12546732], dtype=float32),\r\n -0.6535670833836938,\r\n 0.6457115366779941), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=0, args=(array([3], dtype=int32), array([[-0.71080756, -0.71080756],\r\n        [-0.17237663, -0.71080756],\r\n        [-0.12546732,  0.6818886 ]], dtype=float32), array([-0.50764036, -0.50764036], dtype=float32), array([-0.6544955, -0.725019 ], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=1, args=(array([3], dtype=int32),\r\n array([0.24296041, 0.6318636 , 0.24296041], dtype=float32),\r\n 0.6178534807058208,\r\n -0.25014082969376666), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=0, args=(array([3], dtype=int32), array([[-0.71080756, -0.71080756],\r\n        [-0.17237663, -0.71080756],\r\n        [-0.12546732,  0.6818886 ]], dtype=float32), array([-0.50764036, -0.50764036], dtype=float32), array([-0.6544955, -0.725019 ], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=2, args=(array([3], dtype=int32), array([[[-0.12546732],\r\n         [-0.12546732]],\r\n \r\n        [[-0.12546732],\r\n         [-0.12546732]],\r\n \r\n        [[-0.12546732],\r\n         [-0.12546732]]], dtype=float32), array([[-0.17237663],\r\n        [-0.9245386 ]], dtype=float32), array([[0.81811625],\r\n        [0.08328725]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=0, args=(array([3], dtype=int32), array([[-0.71080756, -0.71080756],\r\n        [-0.17237663, -0.71080756],\r\n        [-0.12546732,  0.6818886 ]], dtype=float32), array([-0.50764036, -0.50764036], dtype=float32), array([-0.6544955, -0.725019 ], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=2, args=(array([3], dtype=int32), array([[[-0.12546732],\r\n         [-0.12546732]],\r\n \r\n        [[-0.12546732],\r\n         [-0.12546732]],\r\n \r\n        [[-0.12546732],\r\n         [-0.12546732]]], dtype=float32), array([[-0.17237663],\r\n        [-0.9245386 ]], dtype=float32), array([[0.81811625],\r\n        [0.08328725]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=0, args=(array([3], dtype=int32), array([[-0.71080756, -0.71080756],\r\n        [-0.17237663, -0.71080756],\r\n        [-0.12546732,  0.6818886 ]], dtype=float32), array([-0.50764036, -0.50764036], dtype=float32), array([-0.6544955, -0.725019 ], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=2, args=(array([3], dtype=int32), array([[[-0.12546732],\r\n         [-0.12546732]],\r\n \r\n        [[-0.12546732],\r\n         [-0.12546732]],\r\n \r\n        [[-0.12546732],\r\n         [-0.12546732]]], dtype=float32), array([[-0.17237663],\r\n        [-0.9245386 ]], dtype=float32), array([[0.81811625],\r\n        [0.08328725]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[-0.17049105,  0.2294148 , -0.62575763],\r\n        [ 0.2294148 ,  0.2294148 ,  0.2294148 ],\r\n        [ 0.2294148 ,  0.2294148 ,  0.2294148 ]], dtype=float32), array([-0.71080756, -0.71080756, -0.71080756], dtype=float32), array([-0.50764036, -0.50764036, -0.50764036], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=2, end_pad_width=2, args=(array([3], dtype=int32), array([[[-0.12546732],\r\n         [-0.12546732],\r\n         [-0.12546732]],\r\n \r\n        [[-0.12546732],\r\n         [-0.12546732],\r\n         [-0.12546732]],\r\n \r\n        [[-0.12546732],\r\n         [-0.12546732],\r\n         [-0.12546732]]], dtype=float32), array([[-0.9245386 ],\r\n        [-0.9245386 ],\r\n        [-0.17237663]], dtype=float32), array([[ 0.81811625],\r\n        [ 0.08328725],\r\n        [-0.75024533]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[0.11963186, 0.11963186, 0.11963186],\r\n         [0.11963186, 0.11963186, 0.11963186],\r\n         [0.11963186, 0.11963186, 0.11963186]],\r\n \r\n        [[0.11963186, 0.11963186, 0.11963186],\r\n         [0.11963186, 0.11963186, 0.11963186],\r\n         [0.11963186, 0.11963186, 0.11963186]],\r\n \r\n        [[0.11963186, 0.11963186, 0.11963186],\r\n         [0.11963186, 0.11963186, 0.11963186],\r\n         [0.11963186, 0.11963186, 0.11963186]]], dtype=float32), array([[-0.4270123 , -0.9667386 , -0.4270123 ],\r\n        [ 0.15222359,  0.2294148 ,  0.12841824],\r\n        [-0.4270123 , -0.4270123 , -0.7181849 ]], dtype=float32), array([[-0.8295156, -0.8295156, -0.8295156],\r\n        [-0.8295156, -0.8295156, -0.8295156],\r\n        [-0.8295156, -0.8295156, -0.8295156]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=2, end_pad_width=2, args=(array([3], dtype=int32), array([[[-0.12546732],\r\n         [-0.12546732],\r\n         [-0.12546732]],\r\n \r\n        [[-0.12546732],\r\n         [-0.12546732],\r\n         [-0.12546732]],\r\n \r\n        [[-0.12546732],\r\n         [-0.12546732],\r\n         [-0.12546732]]], dtype=float32), array([[-0.9245386 ],\r\n        [-0.9245386 ],\r\n        [-0.17237663]], dtype=float32), array([[ 0.81811625],\r\n        [ 0.08328725],\r\n        [-0.75024533]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[0.11963186, 0.11963186, 0.11963186],\r\n         [0.11963186, 0.11963186, 0.11963186],\r\n         [0.11963186, 0.11963186, 0.11963186]],\r\n \r\n        [[0.11963186, 0.11963186, 0.11963186],\r\n         [0.11963186, 0.11963186, 0.11963186],\r\n         [0.11963186, 0.11963186, 0.11963186]],\r\n \r\n        [[0.11963186, 0.11963186, 0.11963186],\r\n         [0.11963186, 0.11963186, 0.11963186],\r\n         [0.11963186, 0.11963186, 0.11963186]]], dtype=float32), array([[-0.4270123 , -0.9667386 , -0.4270123 ],\r\n        [ 0.15222359,  0.2294148 ,  0.12841824],\r\n        [-0.4270123 , -0.4270123 , -0.7181849 ]], dtype=float32), array([[-0.8295156, -0.8295156, -0.8295156],\r\n        [-0.8295156, -0.8295156, -0.8295156],\r\n        [-0.8295156, -0.8295156, -0.8295156]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=2, end_pad_width=2, args=(array([3], dtype=int32), array([[[-0.12546732],\r\n         [-0.12546732],\r\n         [-0.12546732]],\r\n \r\n        [[-0.12546732],\r\n         [-0.12546732],\r\n         [-0.12546732]],\r\n \r\n        [[-0.12546732],\r\n         [-0.12546732],\r\n         [-0.12546732]]], dtype=float32), array([[-0.9245386 ],\r\n        [-0.9245386 ],\r\n        [-0.17237663]], dtype=float32), array([[ 0.81811625],\r\n        [ 0.08328725],\r\n        [-0.75024533]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[0.11963186, 0.11963186, 0.11963186],\r\n         [0.11963186, 0.11963186, 0.11963186],\r\n         [0.11963186, 0.11963186, 0.11963186]],\r\n \r\n        [[0.11963186, 0.11963186, 0.11963186],\r\n         [0.11963186, 0.11963186, 0.11963186],\r\n         [0.11963186, 0.11963186, 0.11963186]],\r\n \r\n        [[0.11963186, 0.11963186, 0.11963186],\r\n         [0.11963186, 0.11963186, 0.11963186],\r\n         [0.11963186, 0.11963186, 0.11963186]]], dtype=float32), array([[-0.4270123 , -0.9667386 , -0.4270123 ],\r\n        [ 0.15222359,  0.2294148 ,  0.12841824],\r\n        [-0.4270123 , -0.4270123 , -0.7181849 ]], dtype=float32), array([[-0.8295156, -0.8295156, -0.8295156],\r\n        [-0.8295156, -0.8295156, -0.8295156],\r\n        [-0.8295156, -0.8295156, -0.8295156]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=2, end_pad_width=2, args=(array([3], dtype=int32), array([[[-0.12546732],\r\n         [-0.12546732],\r\n         [-0.12546732]],\r\n \r\n        [[-0.12546732],\r\n         [-0.12546732],\r\n         [-0.12546732]],\r\n \r\n        [[-0.12546732],\r\n         [-0.12546732],\r\n         [-0.12546732]]], dtype=float32), array([[-0.9245386 ],\r\n        [-0.9245386 ],\r\n        [-0.17237663]], dtype=float32), array([[ 0.81811625],\r\n        [ 0.08328725],\r\n        [-0.75024533]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.12546732, -0.12546732],\r\n         [-0.12546732, -0.12546732],\r\n         [-0.12546732, -0.12546732]],\r\n \r\n        [[-0.12546732, -0.12546732],\r\n         [-0.12546732, -0.12546732],\r\n         [-0.12546732, -0.12546732]],\r\n \r\n        [[-0.12546732, -0.12546732],\r\n         [-0.12546732, -0.12546732],\r\n         [-0.12546732, -0.12546732]]], dtype=float32), array([[-0.71080756, -0.71080756],\r\n        [-0.17237663, -0.71080756],\r\n        [-0.71080756,  0.6818886 ]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.12546732, -0.12546732],\r\n         [-0.12546732, -0.12546732],\r\n         [-0.12546732, -0.12546732]],\r\n \r\n        [[-0.12546732, -0.12546732],\r\n         [-0.12546732, -0.12546732],\r\n         [-0.12546732, -0.12546732]],\r\n \r\n        [[-0.12546732, -0.12546732],\r\n         [-0.12546732, -0.12546732],\r\n         [-0.12546732, -0.12546732]]], dtype=float32), array([[-0.71080756, -0.71080756],\r\n        [-0.17237663, -0.71080756],\r\n        [-0.71080756,  0.6818886 ]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.12546732, -0.12546732],\r\n         [-0.12546732, -0.12546732],\r\n         [-0.12546732, -0.12546732]],\r\n \r\n        [[-0.12546732, -0.12546732],\r\n         [-0.12546732, -0.12546732],\r\n         [-0.12546732, -0.12546732]],\r\n \r\n        [[-0.12546732, -0.12546732],\r\n         [-0.12546732, -0.12546732],\r\n         [-0.12546732, -0.12546732]]], dtype=float32), array([[-0.71080756, -0.71080756],\r\n        [-0.17237663, -0.71080756],\r\n        [-0.71080756,  0.6818886 ]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [ 0.61785346, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[ 0.02449792, -0.725019  ],\r\n        [-0.6544955 , -0.92966264],\r\n        [-0.8234622 , -0.8234622 ]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 689, in evaluate_test_data\r\n    result = self.execute(data, collect=True)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 604, in execute\r\n    result = self.test_runner(data, run)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/executors.py"", line 58, in default_new_style_executor\r\n    return function(data)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 600, in run\r\n    return test(*args, **kwargs)\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/sequence_ops_test.py"", line 189, in test_gather_padding\r\n    end_pad_width=st.integers(min_value=0, max_value=2),\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 542, in test\r\n    result = self.test(*args, **kwargs)\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/sequence_ops_test.py"", line 207, in test_gather_padding\r\n    reference=partial(_gather_padding_ref, start_pad_width, end_pad_width))\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/hypothesis_test_util.py"", line 575, in assertReferenceChecks\r\n    output_blob_name,\r\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/_private/utils.py"", line 1443, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/_private/utils.py"", line 780, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=0.0001, atol=0.0001\r\nOutput start_padding is not matching the reference\r\n(mismatch 100.0%)\r\n x: array(-4618794431967920128)\r\n y: array([[0., 0.],\r\n       [0., 0.],\r\n       [0., 0.]])\r\n\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [ 0.61785346, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[ 0.02449792, -0.725019  ],\r\n        [-0.6544955 , -0.92966264],\r\n        [-0.8234622 , -0.8234622 ]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 689, in evaluate_test_data\r\n    result = self.execute(data, collect=True)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 604, in execute\r\n    result = self.test_runner(data, run)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/executors.py"", line 58, in default_new_style_executor\r\n    return function(data)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 600, in run\r\n    return test(*args, **kwargs)\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/sequence_ops_test.py"", line 189, in test_gather_padding\r\n    end_pad_width=st.integers(min_value=0, max_value=2),\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 542, in test\r\n    result = self.test(*args, **kwargs)\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/sequence_ops_test.py"", line 207, in test_gather_padding\r\n    reference=partial(_gather_padding_ref, start_pad_width, end_pad_width))\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/hypothesis_test_util.py"", line 575, in assertReferenceChecks\r\n    output_blob_name,\r\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/_private/utils.py"", line 1443, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/_private/utils.py"", line 780, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=0.0001, atol=0.0001\r\nOutput start_padding is not matching the reference\r\n(mismatch 100.0%)\r\n x: array(-4618794431967920128)\r\n y: array([[0., 0.],\r\n       [0., 0.],\r\n       [0., 0.]])\r\n\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[ 0.61785346,  0.61785346],\r\n         [-0.62575763,  0.61785346],\r\n         [ 0.61785346,  0.61785346]],\r\n \r\n        [[ 0.61785346,  0.61785346],\r\n         [ 0.61785346,  0.61785346],\r\n         [ 0.61785346,  0.61785346]],\r\n \r\n        [[ 0.61785346,  0.61785346],\r\n         [ 0.61785346,  0.61785346],\r\n         [ 0.61785346,  0.61785346]]], dtype=float32), array([[-0.71080756, -0.71080756],\r\n        [-0.71080756, -0.71080756],\r\n        [ 0.2294148 , -0.71080756]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[ 0.02449792, -0.725019  ],\r\n        [-0.6544955 , -0.92966264],\r\n        [-0.8234622 , -0.8234622 ]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 689, in evaluate_test_data\r\n    result = self.execute(data, collect=True)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 604, in execute\r\n    result = self.test_runner(data, run)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/executors.py"", line 58, in default_new_style_executor\r\n    return function(data)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 600, in run\r\n    return test(*args, **kwargs)\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/sequence_ops_test.py"", line 189, in test_gather_padding\r\n    end_pad_width=st.integers(min_value=0, max_value=2),\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 542, in test\r\n    result = self.test(*args, **kwargs)\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/sequence_ops_test.py"", line 207, in test_gather_padding\r\n    reference=partial(_gather_padding_ref, start_pad_width, end_pad_width))\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/hypothesis_test_util.py"", line 575, in assertReferenceChecks\r\n    output_blob_name,\r\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/_private/utils.py"", line 1443, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/_private/utils.py"", line 780, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=0.0001, atol=0.0001\r\nOutput start_padding is not matching the reference\r\n(mismatch 100.0%)\r\n x: array(-4618794431967920128)\r\n y: array([[0., 0.],\r\n       [0., 0.],\r\n       [0., 0.]])\r\n\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[ 0.2294148 ,  0.2294148 ],\r\n         [-0.62575763,  0.2294148 ],\r\n         [ 0.2294148 ,  0.2294148 ]],\r\n \r\n        [[ 0.2294148 ,  0.2294148 ],\r\n         [ 0.2294148 ,  0.2294148 ],\r\n         [ 0.2294148 ,  0.2294148 ]],\r\n \r\n        [[ 0.2294148 ,  0.2294148 ],\r\n         [ 0.2294148 ,  0.2294148 ],\r\n         [ 0.2294148 ,  0.2294148 ]]], dtype=float32), array([[-0.71080756, -0.71080756],\r\n        [-0.71080756, -0.71080756],\r\n        [-0.71080756, -0.71080756]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[ 0.27146652, -0.9667386 ],\r\n         [-0.62575763,  0.27146652],\r\n         [ 0.27146652,  0.27146652]],\r\n \r\n        [[ 0.27146652,  0.27146652],\r\n         [-0.7181849 ,  0.27146652],\r\n         [ 0.01402934,  0.27146652]],\r\n \r\n        [[ 0.27146652, -0.54356337],\r\n         [ 0.27146652,  0.27146652],\r\n         [ 0.27146652,  0.27146652]]], dtype=float32), array([[-0.5017104, -0.5017104],\r\n        [-0.5017104, -0.5017104],\r\n        [-0.5017104, -0.5017104]], dtype=float32), array([[-0.1372088, -0.1372088],\r\n        [-0.1372088, -0.1372088],\r\n        [-0.1372088,  0.3028629]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[ 0.27146652,  0.9559304 ],\r\n         [-0.62575763,  0.27146652],\r\n         [-0.07308909,  0.27146652]],\r\n \r\n        [[ 0.27146652,  0.27146652],\r\n         [ 0.27146652,  0.27146652],\r\n         [ 0.01402934,  0.27146652]],\r\n \r\n        [[ 0.27146652, -0.54356337],\r\n         [ 0.27146652,  0.27146652],\r\n         [ 0.27146652,  0.27146652]]], dtype=float32), array([[-0.5017104, -0.5017104],\r\n        [-0.5017104, -0.5017104],\r\n        [-0.5017104, -0.5017104]], dtype=float32), array([[-0.1372088, -0.1372088],\r\n        [-0.1372088, -0.1372088],\r\n        [-0.1372088,  0.3028629]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[ 0.27146652,  0.9559304 ],\r\n         [-0.62575763,  0.27146652],\r\n         [-0.07308909,  0.27146652]],\r\n \r\n        [[ 0.27146652,  0.27146652],\r\n         [ 0.27146652,  0.27146652],\r\n         [ 0.01402934,  0.27146652]],\r\n \r\n        [[ 0.27146652, -0.54356337],\r\n         [ 0.27146652,  0.27146652],\r\n         [ 0.27146652,  0.27146652]]], dtype=float32), array([[-0.5017104, -0.5017104],\r\n        [-0.5017104, -0.5017104],\r\n        [-0.5017104, -0.5017104]], dtype=float32), array([[-0.1372088, -0.1372088],\r\n        [-0.1372088, -0.1372088],\r\n        [-0.1372088,  0.3028629]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[ 0.27146652,  0.9559304 ],\r\n         [-0.62575763,  0.27146652],\r\n         [ 0.07308909,  0.27146652]],\r\n \r\n        [[ 0.27146652,  0.27146652],\r\n         [ 0.27146652,  0.27146652],\r\n         [ 0.01402934,  0.27146652]],\r\n \r\n        [[ 0.27146652, -0.54356337],\r\n         [ 0.27146652,  0.27146652],\r\n         [ 0.27146652,  0.27146652]]], dtype=float32), array([[-0.5017104, -0.5017104],\r\n        [-0.5017104, -0.5017104],\r\n        [-0.5017104, -0.5017104]], dtype=float32), array([[-0.1372088, -0.1372088],\r\n        [-0.1372088, -0.1372088],\r\n        [-0.1372088,  0.3028629]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[ 0.27146652,  0.9559304 ],\r\n         [-0.62575763,  0.27146652],\r\n         [ 0.3127855 ,  0.27146652]],\r\n \r\n        [[ 0.27146652,  0.27146652],\r\n         [ 0.27146652,  0.27146652],\r\n         [ 0.01402934,  0.27146652]],\r\n \r\n        [[ 0.27146652, -0.54356337],\r\n         [ 0.27146652,  0.27146652],\r\n         [ 0.27146652,  0.27146652]]], dtype=float32), array([[-0.5017104, -0.5017104],\r\n        [-0.5017104, -0.5017104],\r\n        [-0.5017104, -0.5017104]], dtype=float32), array([[-0.1372088, -0.1372088],\r\n        [-0.1372088, -0.1372088],\r\n        [-0.1372088,  0.3028629]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[ 0.27146652,  0.9559304 ],\r\n         [-0.62575763,  0.27146652],\r\n         [ 0.07308909,  0.27146652]],\r\n \r\n        [[ 0.27146652,  0.27146652],\r\n         [ 0.27146652,  0.27146652],\r\n         [ 0.01402934,  0.27146652]],\r\n \r\n        [[ 0.27146652, -0.54356337],\r\n         [ 0.27146652,  0.27146652],\r\n         [ 0.27146652,  0.27146652]]], dtype=float32), array([[-0.5017104, -0.5017104],\r\n        [-0.5017104, -0.5017104],\r\n        [-0.5017104, -0.5017104]], dtype=float32), array([[-0.1372088, -0.1372088],\r\n        [-0.1372088, -0.1372088],\r\n        [-0.1372088,  0.3028629]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[ 0.27146652,  0.9559304 ],\r\n         [-0.62575763,  0.27146652],\r\n         [ 0.3127855 ,  0.27146652]],\r\n \r\n        [[ 0.27146652,  0.27146652],\r\n         [ 0.27146652,  0.27146652],\r\n         [ 0.01402934,  0.27146652]],\r\n \r\n        [[ 0.27146652, -0.54356337],\r\n         [ 0.27146652,  0.27146652],\r\n         [ 0.27146652,  0.27146652]]], dtype=float32), array([[-0.5017104, -0.5017104],\r\n        [-0.5017104, -0.5017104],\r\n        [-0.5017104, -0.5017104]], dtype=float32), array([[-0.1372088, -0.1372088],\r\n        [-0.1372088, -0.1372088],\r\n        [-0.1372088,  0.3028629]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[ 0.27146652,  0.9559304 ],\r\n         [-0.62575763,  0.27146652],\r\n         [ 0.07308909,  0.27146652]],\r\n \r\n        [[ 0.27146652,  0.27146652],\r\n         [ 0.27146652,  0.27146652],\r\n         [ 0.01402934,  0.27146652]],\r\n \r\n        [[ 0.27146652, -0.54356337],\r\n         [ 0.27146652,  0.27146652],\r\n         [ 0.27146652,  0.27146652]]], dtype=float32), array([[-0.5017104, -0.5017104],\r\n        [-0.5017104, -0.5017104],\r\n        [-0.5017104, -0.5017104]], dtype=float32), array([[-0.1372088, -0.1372088],\r\n        [-0.1372088, -0.1372088],\r\n        [-0.1372088,  0.3028629]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[ 0.27146652,  0.9559304 ],\r\n         [-0.62575763,  0.27146652],\r\n         [ 0.3127855 ,  0.27146652]],\r\n \r\n        [[ 0.27146652,  0.27146652],\r\n         [ 0.27146652,  0.27146652],\r\n         [ 0.01402934,  0.27146652]],\r\n \r\n        [[ 0.27146652, -0.54356337],\r\n         [ 0.27146652,  0.27146652],\r\n         [ 0.27146652,  0.27146652]]], dtype=float32), array([[-0.5017104, -0.5017104],\r\n        [-0.5017104, -0.5017104],\r\n        [-0.5017104, -0.5017104]], dtype=float32), array([[-0.1372088, -0.1372088],\r\n        [-0.1372088, -0.1372088],\r\n        [-0.1372088,  0.3028629]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.06448297, -0.06448297],\r\n         [-0.62575763, -0.06448297],\r\n         [ 0.2294148 , -0.06448297]],\r\n \r\n        [[-0.06448297, -0.06448297],\r\n         [-0.06448297, -0.06448297],\r\n         [-0.06448297, -0.06448297]],\r\n \r\n        [[-0.06448297, -0.06448297],\r\n         [-0.06448297, -0.06448297],\r\n         [-0.06448297, -0.06448297]]], dtype=float32), array([[0.06280098, 0.06280098],\r\n        [0.06280098, 0.06280098],\r\n        [0.06280098, 0.06280098]], dtype=float32), array([[ 0.7273016 ,  0.7273016 ],\r\n        [-0.81298965,  0.7273016 ],\r\n        [ 0.7273016 ,  0.31250054]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.06448297, -0.06448297],\r\n         [-0.62575763, -0.06448297],\r\n         [ 0.2294148 , -0.06448297]],\r\n \r\n        [[-0.06448297, -0.06448297],\r\n         [-0.06448297, -0.06448297],\r\n         [-0.06448297, -0.06448297]],\r\n \r\n        [[-0.06448297, -0.06448297],\r\n         [-0.06448297, -0.06448297],\r\n         [-0.06448297, -0.06448297]]], dtype=float32), array([[0.06280098, 0.06280098],\r\n        [0.06280098, 0.06280098],\r\n        [0.06280098, 0.06280098]], dtype=float32), array([[ 0.7273016 ,  0.7273016 ],\r\n        [-0.81298965,  0.7273016 ],\r\n        [ 0.7273016 ,  0.31250054]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.9667386 , -0.9667386 ],\r\n         [-0.62575763, -0.9667386 ],\r\n         [ 0.2294148 , -0.9667386 ]],\r\n \r\n        [[-0.9667386 , -0.9667386 ],\r\n         [-0.9667386 , -0.9667386 ],\r\n         [-0.9667386 , -0.9667386 ]],\r\n \r\n        [[-0.9667386 , -0.9667386 ],\r\n         [-0.9667386 , -0.9667386 ],\r\n         [-0.9667386 , -0.9667386 ]]], dtype=float32), array([[-0.7181849 ,  0.27146652],\r\n        [-0.6544955 , -0.8447196 ],\r\n        [ 0.27146652,  0.27146652]], dtype=float32), array([[-0.5017104, -0.5017104],\r\n        [-0.5017104, -0.5017104],\r\n        [-0.5017104, -0.5017104]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.4377519 , -0.4377519 ],\r\n         [-0.62575763, -0.4377519 ],\r\n         [ 0.2294148 , -0.4377519 ]],\r\n \r\n        [[-0.4377519 , -0.4377519 ],\r\n         [-0.4377519 , -0.4377519 ],\r\n         [-0.4377519 , -0.4377519 ]],\r\n \r\n        [[-0.4377519 , -0.4377519 ],\r\n         [-0.4377519 , -0.4377519 ],\r\n         [-0.4377519 , -0.4377519 ]]], dtype=float32), array([[-1.3770291e-04, -1.3770291e-04],\r\n        [-8.1298965e-01, -1.3770291e-04],\r\n        [-1.3770291e-04,  6.2800981e-02]], dtype=float32), array([[-0.1372088, -0.1372088],\r\n        [-0.1372088, -0.4379044],\r\n        [-0.1372088,  0.3028629]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.4377519 , -0.4377519 ],\r\n         [-0.62575763, -0.4377519 ],\r\n         [ 0.2294148 , -0.4377519 ]],\r\n \r\n        [[-0.4377519 , -0.4377519 ],\r\n         [-0.4377519 , -0.4377519 ],\r\n         [-0.4377519 , -0.4377519 ]],\r\n \r\n        [[-0.4377519 , -0.4377519 ],\r\n         [-0.4377519 , -0.4377519 ],\r\n         [-0.4377519 , -0.4377519 ]]], dtype=float32), array([[-1.3770291e-04, -1.3770291e-04],\r\n        [-8.1298965e-01, -1.3770291e-04],\r\n        [-1.3770291e-04,  6.2800981e-02]], dtype=float32), array([[-0.1372088, -0.1372088],\r\n        [-0.1372088, -0.4379044],\r\n        [-0.1372088,  0.3028629]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.4377519 , -0.4377519 ],\r\n         [-0.62575763, -0.4377519 ],\r\n         [ 0.2294148 , -0.4377519 ]],\r\n \r\n        [[-0.4377519 , -0.4377519 ],\r\n         [-0.4377519 , -0.4377519 ],\r\n         [-0.4377519 , -0.4377519 ]],\r\n \r\n        [[-0.4377519 , -0.4377519 ],\r\n         [-0.4377519 , -0.4377519 ],\r\n         [-0.4377519 , -0.4377519 ]]], dtype=float32), array([[-1.3770291e-04, -1.3770291e-04],\r\n        [-8.1298965e-01, -1.3770291e-04],\r\n        [-1.3770291e-04,  6.2800981e-02]], dtype=float32), array([[-0.1372088, -0.1372088],\r\n        [-0.1372088, -0.4379044],\r\n        [-0.1372088,  0.3028629]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.4377519 , -0.4377519 ],\r\n         [-0.62575763, -0.4377519 ],\r\n         [ 0.2294148 , -0.4377519 ]],\r\n \r\n        [[-0.4377519 , -0.4377519 ],\r\n         [-0.4377519 , -0.4377519 ],\r\n         [-0.4377519 , -0.4377519 ]],\r\n \r\n        [[-0.4377519 , -0.4377519 ],\r\n         [-0.4377519 , -0.4377519 ],\r\n         [-0.4377519 , -0.4377519 ]]], dtype=float32), array([[-1.3770291e-04, -1.3770291e-04],\r\n        [-8.1298965e-01, -1.3770291e-04],\r\n        [-1.3770291e-04,  6.2800981e-02]], dtype=float32), array([[-0.1372088, -0.1372088],\r\n        [-0.1372088, -0.4379044],\r\n        [-0.1372088,  0.3028629]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.4377519 , -0.4377519 ],\r\n         [-0.62575763, -0.4377519 ],\r\n         [ 0.2294148 , -0.4377519 ]],\r\n \r\n        [[-0.4377519 , -0.4377519 ],\r\n         [-0.4377519 , -0.4377519 ],\r\n         [-0.4377519 , -0.4377519 ]],\r\n \r\n        [[-0.4377519 , -0.4377519 ],\r\n         [-0.4377519 , -0.4377519 ],\r\n         [-0.4377519 , -0.4377519 ]]], dtype=float32), array([[-1.3770291e-04, -1.3770291e-04],\r\n        [-8.1298965e-01, -1.3770291e-04],\r\n        [-1.3770291e-04,  6.2800981e-02]], dtype=float32), array([[-0.1372088, -0.1372088],\r\n        [-0.1372088, -0.4379044],\r\n        [-0.1372088,  0.3028629]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.7181849 ,  0.27146652],\r\n        [-0.6544955 , -0.8447196 ],\r\n        [ 0.27146652,  0.27146652]], dtype=float32), array([[-0.5017104, -0.5017104],\r\n        [-0.5017104, -0.5017104],\r\n        [-0.5017104, -0.5017104]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.7181849 ,  0.27146652],\r\n        [-0.6544955 , -0.8447196 ],\r\n        [ 0.27146652,  0.27146652]], dtype=float32), array([[-0.5017104, -0.5017104],\r\n        [-0.5017104, -0.5017104],\r\n        [-0.5017104, -0.5017104]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[0.06280098, 0.06280098],\r\n        [0.06280098, 0.06280098],\r\n        [0.06280098, 0.06280098]], dtype=float32), array([[ 0.7273016 ,  0.7273016 ],\r\n        [-0.81298965,  0.7273016 ],\r\n        [ 0.7273016 ,  0.31250054]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[0.06280098, 0.06280098],\r\n        [0.06280098, 0.06280098],\r\n        [0.06280098, 0.06280098]], dtype=float32), array([[ 0.7273016 ,  0.7273016 ],\r\n        [-0.81298965,  0.7273016 ],\r\n        [ 0.7273016 ,  0.31250054]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=2, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[0.9559304, 0.9559304],\r\n        [0.9559304, 0.9559304],\r\n        [0.9559304, 0.9559304]], dtype=float32), array([[-1.3770291e-04, -6.0486221e-01],\r\n        [-1.3770291e-04, -1.3770291e-04],\r\n        [-1.3770291e-04,  8.8494122e-02]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.75024533, -0.75024533],\r\n        [-0.75024533, -0.75024533],\r\n        [-0.75024533, -0.75024533]], dtype=float32), array([[-0.37570858,  0.31514454],\r\n        [-0.12539388, -0.37570858],\r\n        [-0.07394399, -0.13174728]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.75024533, -0.75024533],\r\n        [-0.75024533, -0.75024533],\r\n        [-0.75024533, -0.75024533]], dtype=float32), array([[-0.37570858,  0.31514454],\r\n        [-0.12539388, -0.37570858],\r\n        [-0.07394399, -0.13174728]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.75024533, -0.75024533],\r\n        [-0.75024533, -0.75024533],\r\n        [-0.75024533, -0.75024533]], dtype=float32), array([[-0.37570858,  0.31514454],\r\n        [-0.12539388, -0.37570858],\r\n        [-0.07394399, -0.13174728]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.75024533, -0.75024533],\r\n        [-0.75024533, -0.75024533],\r\n        [-0.75024533, -0.75024533]], dtype=float32), array([[-0.37570858,  0.31514454],\r\n        [-0.12539388, -0.37570858],\r\n        [-0.07394399, -0.13174728]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.75024533, -0.75024533],\r\n        [-0.75024533, -0.75024533],\r\n        [-0.75024533, -0.75024533]], dtype=float32), array([[-0.37570858,  0.31514454],\r\n        [-0.12539388, -0.37570858],\r\n        [-0.07394399, -0.13174728]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[-1.3720880e-01, -1.2534568e-01],\r\n        [-1.3770291e-04, -4.3790439e-01],\r\n        [-1.3720880e-01,  3.0286291e-01]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=2, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[-1.3770291e-04, -6.0486221e-01],\r\n        [-1.3770291e-04, -1.3770291e-04],\r\n        [-1.3770291e-04,  8.8494122e-02]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[-1.3720880e-01, -1.2534568e-01],\r\n        [-1.3770291e-04, -4.3790439e-01],\r\n        [-1.3720880e-01,  3.0286291e-01]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=2, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[-1.3770291e-04, -6.0486221e-01],\r\n        [-1.3770291e-04, -1.3770291e-04],\r\n        [-1.3770291e-04,  8.8494122e-02]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[-1.3720880e-01, -1.2534568e-01],\r\n        [-1.3770291e-04, -4.3790439e-01],\r\n        [-1.3720880e-01,  3.0286291e-01]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[ 0.7273016 ,  0.7273016 ],\r\n        [-0.81298965,  0.7273016 ],\r\n        [ 0.7273016 ,  0.31250054]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[ 0.7273016 ,  0.7273016 ],\r\n        [-0.81298965,  0.7273016 ],\r\n        [ 0.7273016 ,  0.31250054]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=2, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[-1.3770291e-04, -6.0486221e-01],\r\n        [-1.3770291e-04, -1.3770291e-04],\r\n        [-1.3770291e-04,  8.8494122e-02]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[ 0.7273016 , -0.81298965],\r\n        [ 0.7273016 ,  0.7273016 ],\r\n        [ 0.7273016 ,  0.31250054]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=2, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[0.31514454, 0.10083199],\r\n        [0.31514454, 0.31514454],\r\n        [0.31514454, 0.31514454]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=2, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[0.31514454, 0.10083199],\r\n        [0.31514454, 0.31514454],\r\n        [0.31514454, 0.31514454]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=2, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[-1.3770291e-04, -6.0486221e-01],\r\n        [-1.3770291e-04, -1.3770291e-04],\r\n        [-1.3770291e-04,  8.8494122e-02]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=2, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[ 0.31514454, -0.10083199],\r\n        [ 0.31514454,  0.31514454],\r\n        [ 0.31514454,  0.31514454]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[ 0.7273016 , -0.12539388],\r\n        [ 0.7273016 ,  0.7273016 ],\r\n        [ 0.7273016 ,  0.31250054]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=2, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[ 0.31514454, -0.10083199],\r\n        [ 0.31514454,  0.31514454],\r\n        [ 0.31514454,  0.31514454]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[ 0.7273016 , -0.12539388],\r\n        [ 0.7273016 ,  0.7273016 ],\r\n        [ 0.7273016 ,  0.31250054]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=2, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[ 0.31514454, -0.10083199],\r\n        [ 0.31514454,  0.31514454],\r\n        [ 0.31514454,  0.31514454]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[ 0.7273016 , -0.12539388],\r\n        [ 0.7273016 ,  0.7273016 ],\r\n        [ 0.7273016 ,  0.31250054]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=2, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[ 0.31514454, -0.10083199],\r\n        [ 0.31514454,  0.31514454],\r\n        [ 0.31514454,  0.31514454]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[ 0.7273016 , -0.12539388],\r\n        [ 0.7273016 ,  0.7273016 ],\r\n        [ 0.7273016 ,  0.31250054]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=2, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[ 0.31514454, -0.10083199],\r\n        [ 0.31514454,  0.31514454],\r\n        [ 0.31514454,  0.31514454]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[ 0.7273016 , -0.12539388],\r\n        [ 0.7273016 ,  0.7273016 ],\r\n        [ 0.7273016 ,  0.31250054]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[ 0.02449792, -0.725019  ],\r\n        [-0.6544955 , -0.92966264],\r\n        [-0.8234622 , -0.8234622 ]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 689, in evaluate_test_data\r\n    result = self.execute(data, collect=True)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 604, in execute\r\n    result = self.test_runner(data, run)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/executors.py"", line 58, in default_new_style_executor\r\n    return function(data)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 600, in run\r\n    return test(*args, **kwargs)\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/sequence_ops_test.py"", line 189, in test_gather_padding\r\n    end_pad_width=st.integers(min_value=0, max_value=2),\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 542, in test\r\n    result = self.test(*args, **kwargs)\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/sequence_ops_test.py"", line 207, in test_gather_padding\r\n    reference=partial(_gather_padding_ref, start_pad_width, end_pad_width))\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/hypothesis_test_util.py"", line 575, in assertReferenceChecks\r\n    output_blob_name,\r\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/_private/utils.py"", line 1443, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/_private/utils.py"", line 780, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=0.0001, atol=0.0001\r\nOutput start_padding is not matching the reference\r\n(mismatch 100.0%)\r\n x: array(-4620624399421145088)\r\n y: array([[0., 0.],\r\n       [0., 0.],\r\n       [0., 0.]])\r\n\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[-0.6544955, -0.725019 ],\r\n        [-0.6544955, -0.6544955],\r\n        [-0.6544955, -0.6544955]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[-0.6544955, -0.725019 ],\r\n        [-0.6544955, -0.6544955],\r\n        [-0.6544955, -0.6544955]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[-0.4379044 , -0.725019  ],\r\n        [-0.4379044 , -0.4379044 ],\r\n        [-0.4379044 ,  0.03525195]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[-0.4379044 , -0.725019  ],\r\n        [-0.4379044 , -0.4379044 ],\r\n        [-0.4379044 ,  0.03525195]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[-0.4379044 , -0.725019  ],\r\n        [ 0.03525195, -0.4379044 ],\r\n        [-0.4379044 , -0.4379044 ]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[-1.3720880e-01, -7.2501898e-01],\r\n        [-1.3770291e-04, -4.3790439e-01],\r\n        [-1.3720880e-01,  3.0286291e-01]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[-1.3720880e-01, -7.2501898e-01],\r\n        [-1.3770291e-04, -4.3790439e-01],\r\n        [-1.3720880e-01,  3.0286291e-01]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[ 0.02449792, -0.725019  ],\r\n        [ 0.550836  ,  0.02449792],\r\n        [ 0.02449792,  0.02449792]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[-1.3720880e-01, -7.2501898e-01],\r\n        [-1.3770291e-04, -4.3790439e-01],\r\n        [-1.3720880e-01,  3.0286291e-01]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[ 0.7273016 , -0.725019  ],\r\n        [-0.31250054,  0.7273016 ],\r\n        [ 0.7273016 ,  0.7273016 ]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[-1.3720880e-01, -7.2501898e-01],\r\n        [-1.3770291e-04, -4.3790439e-01],\r\n        [-1.3720880e-01,  3.0286291e-01]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[ 0.7273016 , -0.725019  ],\r\n        [-0.31250054,  0.7273016 ],\r\n        [ 0.7273016 ,  0.7273016 ]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[-1.3720880e-01, -7.2501898e-01],\r\n        [-1.3770291e-04, -4.3790439e-01],\r\n        [-1.3720880e-01,  3.0286291e-01]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[ 0.7273016 , -0.725019  ],\r\n        [-0.31250054,  0.7273016 ],\r\n        [ 0.7273016 ,  0.7273016 ]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[-1.3720880e-01, -7.2501898e-01],\r\n        [-1.3770291e-04, -4.3790439e-01],\r\n        [-1.3720880e-01,  3.0286291e-01]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[ 0.7273016 , -0.725019  ],\r\n        [-0.31250054,  0.7273016 ],\r\n        [ 0.7273016 ,  0.7273016 ]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[-1.3720880e-01, -7.2501898e-01],\r\n        [-1.3770291e-04, -4.3790439e-01],\r\n        [-1.3720880e-01,  3.0286291e-01]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[ 0.7273016 , -0.725019  ],\r\n        [-0.31250054,  0.7273016 ],\r\n        [ 0.7273016 ,  0.7273016 ]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[-0.8234622 , -0.725019  ],\r\n        [-0.6544955 , -0.92966264],\r\n        [-0.8234622 , -0.8234622 ]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 689, in evaluate_test_data\r\n    result = self.execute(data, collect=True)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 604, in execute\r\n    result = self.test_runner(data, run)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/executors.py"", line 58, in default_new_style_executor\r\n    return function(data)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 600, in run\r\n    return test(*args, **kwargs)\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/sequence_ops_test.py"", line 189, in test_gather_padding\r\n    end_pad_width=st.integers(min_value=0, max_value=2),\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 542, in test\r\n    result = self.test(*args, **kwargs)\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/sequence_ops_test.py"", line 207, in test_gather_padding\r\n    reference=partial(_gather_padding_ref, start_pad_width, end_pad_width))\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/hypothesis_test_util.py"", line 575, in assertReferenceChecks\r\n    output_blob_name,\r\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/_private/utils.py"", line 1443, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/_private/utils.py"", line 780, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=0.0001, atol=0.0001\r\nOutput start_padding is not matching the reference\r\n(mismatch 100.0%)\r\n x: array(-4620624399421145088)\r\n y: array([[0., 0.],\r\n       [0., 0.],\r\n       [0., 0.]])\r\n\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=1, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[-0.5017104, -0.725019 ],\r\n        [-0.6544955, -0.5017104],\r\n        [-0.5017104, -0.5017104]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[-0.8234622 , -0.725019  ],\r\n        [-0.6544955 , -0.92966264],\r\n        [-0.8234622 , -0.8234622 ]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 689, in evaluate_test_data\r\n    result = self.execute(data, collect=True)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 604, in execute\r\n    result = self.test_runner(data, run)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/executors.py"", line 58, in default_new_style_executor\r\n    return function(data)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 600, in run\r\n    return test(*args, **kwargs)\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/sequence_ops_test.py"", line 189, in test_gather_padding\r\n    end_pad_width=st.integers(min_value=0, max_value=2),\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 542, in test\r\n    result = self.test(*args, **kwargs)\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/sequence_ops_test.py"", line 207, in test_gather_padding\r\n    reference=partial(_gather_padding_ref, start_pad_width, end_pad_width))\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/hypothesis_test_util.py"", line 575, in assertReferenceChecks\r\n    output_blob_name,\r\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/_private/utils.py"", line 1443, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/_private/utils.py"", line 780, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=0.0001, atol=0.0001\r\nOutput start_padding is not matching the reference\r\n(mismatch 100.0%)\r\n x: array(-4620624399421145088)\r\n y: array([[0., 0.],\r\n       [0., 0.],\r\n       [0., 0.]])\r\n\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=2, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[-0.25073355, -0.725019  ],\r\n        [-0.6544955 , -0.25073355],\r\n        [-0.25073355, -0.25073355]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[-0.8234622 , -0.725019  ],\r\n        [-0.6544955 , -0.92966264],\r\n        [-0.8234622 , -0.8234622 ]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 689, in evaluate_test_data\r\n    result = self.execute(data, collect=True)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 604, in execute\r\n    result = self.test_runner(data, run)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/executors.py"", line 58, in default_new_style_executor\r\n    return function(data)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 600, in run\r\n    return test(*args, **kwargs)\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/sequence_ops_test.py"", line 189, in test_gather_padding\r\n    end_pad_width=st.integers(min_value=0, max_value=2),\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 542, in test\r\n    result = self.test(*args, **kwargs)\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/sequence_ops_test.py"", line 207, in test_gather_padding\r\n    reference=partial(_gather_padding_ref, start_pad_width, end_pad_width))\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/hypothesis_test_util.py"", line 575, in assertReferenceChecks\r\n    output_blob_name,\r\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/_private/utils.py"", line 1443, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/_private/utils.py"", line 780, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=0.0001, atol=0.0001\r\nOutput start_padding is not matching the reference\r\n(mismatch 100.0%)\r\n x: array(-4620624399421145088)\r\n y: array([[0., 0.],\r\n       [0., 0.],\r\n       [0., 0.]])\r\n\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=2, end_pad_width=2, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[-0.3693131, -0.725019 ],\r\n        [-0.6544955, -0.3693131],\r\n        [-0.3693131, -0.3693131]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[-0.37570858, -0.725019  ],\r\n        [-0.6544955 ,  0.9936301 ],\r\n        [-0.37570858, -0.37570858]], dtype=float32)), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=0, args=(array([3], dtype=int32), array([[[-0.71080756, -0.71080756],\r\n         [-0.62575763, -0.71080756],\r\n         [ 0.2294148 , -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]],\r\n \r\n        [[-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756],\r\n         [-0.71080756, -0.71080756]]], dtype=float32), array([[-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036],\r\n        [-0.50764036, -0.50764036]], dtype=float32), array([[-0.8234622 , -0.725019  ],\r\n        [-0.6544955 , -0.31397155],\r\n        [-0.8234622 , -0.8234622 ]], dtype=float32)), gc=device_type: 1, dc=[, device_type: 1])\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 689, in evaluate_test_data\r\n    result = self.execute(data, collect=True)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 604, in execute\r\n    result = self.test_runner(data, run)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/executors.py"", line 58, in default_new_style_executor\r\n    return function(data)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 600, in run\r\n    return test(*args, **kwargs)\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/sequence_ops_test.py"", line 189, in test_gather_padding\r\n    end_pad_width=st.integers(min_value=0, max_value=2),\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 542, in test\r\n    result = self.test(*args, **kwargs)\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/sequence_ops_test.py"", line 207, in test_gather_padding\r\n    reference=partial(_gather_padding_ref, start_pad_width, end_pad_width))\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/hypothesis_test_util.py"", line 575, in assertReferenceChecks\r\n    output_blob_name,\r\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/_private/utils.py"", line 1443, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/_private/utils.py"", line 780, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=0.0001, atol=0.0001\r\nOutput start_padding is not matching the reference\r\n(mismatch 100.0%)\r\n x: array(-4620624399421145088)\r\n y: array([[0., 0.],\r\n       [0., 0.],\r\n       [0., 0.]])\r\n\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=0, args=(array([], dtype=int32), array([], dtype=float32), 0.0, 0.0), gc=device_type: 1, dc=[, device_type: 1])\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 689, in evaluate_test_data\r\n    result = self.execute(data, collect=True)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 604, in execute\r\n    result = self.test_runner(data, run)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/executors.py"", line 58, in default_new_style_executor\r\n    return function(data)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 600, in run\r\n    return test(*args, **kwargs)\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/sequence_ops_test.py"", line 189, in test_gather_padding\r\n    end_pad_width=st.integers(min_value=0, max_value=2),\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 542, in test\r\n    result = self.test(*args, **kwargs)\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/sequence_ops_test.py"", line 207, in test_gather_padding\r\n    reference=partial(_gather_padding_ref, start_pad_width, end_pad_width))\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/hypothesis_test_util.py"", line 575, in assertReferenceChecks\r\n    output_blob_name,\r\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/_private/utils.py"", line 1443, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/_private/utils.py"", line 780, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=0.0001, atol=0.0001\r\nOutput start_padding is not matching the reference\r\n(mismatch 100.0%)\r\n x: array(-4620624399421145088)\r\n y: array(0.)\r\n\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=1, end_pad_width=0, args=(array([], dtype=int32), array([], dtype=float32), 0.0, 0.0), gc=, dc=[, device_type: 1])\r\nTrying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=0, args=(array([], dtype=int32),\r\n array([], dtype=float32),\r\n -0.6964012176709845,\r\n -0.5087437279503267), gc=device_type: 1, dc=[, device_type: 1])\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 689, in evaluate_test_data\r\n    result = self.execute(data, collect=True)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 604, in execute\r\n    result = self.test_runner(data, run)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/executors.py"", line 58, in default_new_style_executor\r\n    return function(data)\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 600, in run\r\n    return test(*args, **kwargs)\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/sequence_ops_test.py"", line 189, in test_gather_padding\r\n    end_pad_width=st.integers(min_value=0, max_value=2),\r\n  File ""/usr/local/lib/python2.7/dist-packages/hypothesis/core.py"", line 542, in test\r\n    result = self.test(*args, **kwargs)\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/operator_test/sequence_ops_test.py"", line 207, in test_gather_padding\r\n    reference=partial(_gather_padding_ref, start_pad_width, end_pad_width))\r\n  File ""/usr/local/caffe2/lib/python2.7/dist-packages/caffe2/python/hypothesis_test_util.py"", line 575, in assertReferenceChecks\r\n    output_blob_name,\r\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/_private/utils.py"", line 1443, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/_private/utils.py"", line 780, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=0.0001, atol=0.0001\r\nOutput start_padding is not matching the reference\r\n(mismatch 100.0%)\r\n x: array(-4618794431967920128)\r\n y: array(0.)\r\n\r\nFalsifying example: test_gather_padding(self=<caffe2.python.operator_test.sequence_ops_test.TestSequenceOps testMethod=test_gather_padding>, start_pad_width=0, end_pad_width=0, args=(array([], dtype=int32), array([], dtype=float32), 0.0, 0.0), gc=device_type: 1, dc=[, device_type: 1])\r\n\r\nYou can reproduce this example by temporarily adding @reproduce_failure(\'3.66.8\', \'AXicY2DAAIwMAAAXAAI=\') as a decorator on your test case\r\n generated xml file: /var/lib/jenkins/workspace/caffe2_tests/python/result.xml -\r\n', '']",0,0
274,pytorch,2129,open,[Feature request] truncated normal initializer(sampler),"There has been a recent discussion on the discuss about truncated normal initializer [link](https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/15).
I have implemented one on my own, but the code is basically paraphrasing the tensorflow code, which I feel like may cause problem. If you feel good to add this, I can submit a PR once you tell me if it's ok to paraphrase or what's the alternative.",enhancement module: initialization triaged,"['I think this issue should be reopened, since the PR was not merged in the end and the truncated normal is still not available in PyTorch. @soumith \r\n\r\nRelated: https://github.com/pytorch/pytorch/issues/31945']",[],[],0,0
275,pytorch,7569,open,[Caffe2]How to set lr_mult and decay_mult in Conv layer?,"In facebookresearch/detectron, I see a conv layer is add by:

conv_rpn_fpn = model.Conv(
         bl_in,
         'conv_rpn_fpn' + slvl,
         dim_in,
         dim_out,
         kernel=3,
         pad=1,
         stride=1,
         weight_init=gauss_fill(0.01),
         bias_init=const_fill(0.0)
 )

However, I don't find any parameter statement about lr_mult and decay_mult like that in a caffe conv layer. Could you please give me an example? Thanks a lot!







If you have a question or would like help and support, please ask at our
[forums](https://discuss.pytorch.org/).

If you are submitting a feature request, please preface the title with [feature request].
If you are submitting a bug report, please fill in the following details.

## Issue description

Provide a short description.

## Code example

Please try to provide a minimal example to repro the bug.
Error messages and stack traces are also helpful.

## System Info
Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


- PyTorch or Caffe2:
- How you installed PyTorch (conda, pip, source):
- Build command you used (if compiling from source):
- OS:
- PyTorch version:
- Python version:
- CUDA/cuDNN version:
- GPU models and configuration:
- GCC version (if compiling from source):
- CMake version:
- Versions of any other relevant libraries:
",caffe2,[],"['\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n']",[],0,0
276,pytorch,14844,open,Negative indexing for nn.Embedding inputs,"## üöÄ Feature
<!-- A clear and concise description of the feature proposal -->

Negative indexing for nn.Embedding inputs

## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->

I found both
- 
- 

allow negative indexing for padding.

How about we allow negative indexing for  inputs as well?

## Example



cc @albanD @mruberry",function request module: nn needs design triaged,"['Maybe replacing index_select with indexing is sufficient', 'Hi, I am facing the same Runtime error while running a code ,I am a beginner in using pytorch so could \r\n you guys help me to understand what is wrong happening in the code and how to solve it?', '@zou3519 But is that the expected behavior here? I am not sure about this but in the above example, if you wrap negative values, then 4 and -1 become the same thing. But -1 should be padding and 4 is an input and so they should be different.\r\n\r\n\r\n@Rishav09 The problem is that one of the element you give as input to the embedding is larger than the size of the embedding. You need to check that your inputs are correct: between 0 and input_size-1.', 'Even though I guess that could be confusing, it can be interesting to add.\r\nThe goal would just be to add support for negative indices either by replacing the `index_select` in the cpp by something that allows negative indices. Or preprocess the indices before giving them to `index_select` to ensure they are all in the correct range.']","['python\r\nfrom torch import nn\r\nfrom torch.nn.utils.rnn import pad_sequence\r\nimport torch\r\n\r\ninput = [torch.LongTensor([1,2,3]),\r\n         torch.LongTensor([1,4]),\r\n         torch.LongTensor([2])]\r\n\r\npadded_input = pad_sequence(input, batch_first=True, padding_value=-1)\r\n# tensor([[ 1,  2,  3],\r\n#         [ 1,  4, -1],\r\n#         [ 2, -1, -1]])\r\n\r\nembedding = nn.Embedding(5, 10, padding_idx=-1)\r\n\r\nembedded = embedding(padded_input)\r\n# RuntimeError: index out of range at /Users/soumith/code/builder/wheel/pytorch-src/aten/src/TH/generic/THTensorMath.cpp:352\r\n']","['nn.Embedding', 'nn.utils.rnn.pad_sequence', 'nn.Embedding']",0,0
277,pytorch,9484,open,WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode. WARNING:root:Debug message: libcurand.so.9.0: cannot open shared object file: No such file or directory Segmentation fault (core dumped),"HI,
I have GPU and CUDA, CuDNN, and NCCL. My OS is Ubuntu 16.04, I followed this tutorial to install Coffe2 with GPU support (
conda install -c caffe2 caffe2-cuda9.0-cudnn7
) and the installation finished successfully but this command: python2 -c 'from caffe2.python import workspace; print(workspace.NumCudaDevices())'

returns: WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.
WARNING:root:Debug message: libcurand.so.9.0: cannot open shared object file: No such file or directory
Segmentation fault (core dumped)
Any Idea how can I solve it??
TNX

$nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2018 NVIDIA Corporation
Built on Tue_Jun_12_23:07:04_CDT_2018
Cuda compilation tools, release 9.2, V9.2.148

$ conda -V
conda 4.5.8

 nvidia-smi
Tue Jul 17 01:58:01 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.130                Driver Version: 384.130                   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Quadro M520         Off  | 00000000:02:00.0 Off |                  N/A |
| N/A   41C    P0    N/A /  N/A |    298MiB /  2002MiB |      2%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     21578      G   /usr/lib/xorg/Xorg                           234MiB |
|    0     22157      G   compiz                                        62MiB |
+-----------------------------------------------------------------------------+

",caffe2,"[""I have solved this problem by doing the below command:\r\n\r\nsudo cp /usr/local/cuda-9.0/lib64/libcurand.so.9.0 /usr/local/lib/libcurand.so.9.0 && sudo ldconfig\r\n\r\nthen I run the script again and it shows that other libxxxx.so can't not be found, then I did the command above again to copy the libxxxx.so from  /usr/local/cuda-9.0/lib64/ into /usr/local/lib/\r\nagain and again , when I copied several libxxxx.so , then it runs correctly.\r\nI think some libxxxx.so must be in /usr/local/lib so that it can be found"", ""Do you have cuda-9.0 or cuda 9.2?\r\nI tried those commands and same for others but finally I got this:\r\nWARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\r\nWARNING:root:Debug message: /usr/local/cuda-9.2/lib64/libcurand.so.9.0: version `libcurand.so.9.0' not found (required by /home/samira/anaconda2/lib/python2.7/site-packages/caffe2/python/caffe2_pybind11_state_gpu.so)\r\nSegmentation fault (core dumped)"", ""I am using **_### cuda9.0 and cudnn7.1.4 on Ubuntu16.04_**\r\nand I installed caffe2 pre-build Binaries after I have installed cuda and cudnn by the command bellow:\r\nconda install -c caffe2 caffe2-cuda9.0-cudnn7\r\nI can run the script in my terminal by when I run the script from pycharm , I encountered the WARNING that you've met\r\nI suggest that you should **_### install and configure your cuda and cudnn correctly_** because I have met many errors when I tried to install caffe2 without config cuda and cudnn correctly\r\nThe correct version of cuda and cudnn may be the keypoint\r\n\r\n\r\n"", 'You can try cuda9.0 and cudnn7.1.4 just like me', 'Same error here. have installed cuda cudnn and also nvcc. getting same error for cuda 8 cudnn 7 nvcc 2.2. Any solutions?\r\n']",[],[],0,0
278,pytorch,15260,open,MultiGPU for gru,"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->

During runtime of GRU under multi-GPU environment, there is a RuntimeError: Expected hidden size (3, 64, 12), got (3, 16, 12) where the first, second, and third arguments are the number of GRU layers, batch size and number of hidden units respectively. My model run well under single-GPU environment.

In Pytorch Forum, a question ""DataParallel LSTM/GRU wrong hidden batch size (8 GPUs)"" has been asked. One solution is to set batch_first to True for gru. But the error was still there after the setting. The correct solution is to store the hidden state inside the model rather than return it like the below code block(from user AnodyneCodeAsher Newcomer):




<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->
My wrong code sample:

Please help to fix this bug. Thanks a lot.

## Environment
 - PyTorch Version 1,0:
 - OS Ubuntu 16.04:
 - How you installed PyTorch:
 - Python version: 3.6
 - CUDA/cuDNN version: 9.0





cc @zou3519",module: data parallel module: rnn triaged,"['I have encountered similar problems while using LSTM. If I store the hidden state inside the model as the above correct solution suggestes, my 1080Ti will throw out of memory error.\r\n\r\n## Environment\r\n  - PyTorch Version: 1.0\r\n  - OS: Ubuntu 16.04\r\n  - How you installed PyTorch: `pip`\r\n  - Python version: 3.5\r\n  - CUDA/cuDNN version: 8.0', '@YYRancho First, please make sure your code is correct which would not blow up the memory. If that is the case, please try to reduce the dimension of your hidden state or batch size. And the problem I have mentioned above is in the case of multi-GPU, single GPU works well.', '@MasKong Thanks! The OOM error is indeed my fault. I have corrected my code and it works well.', ""Don't know if this is still relevant, but it's worth checking to see if this still repros.""]","['python\r\nclass LSTM(nn.Module):\r\n    def __init__(self, initial_state):\r\n        super(LSTM, self).__init__()\r\n\r\n        self.lstm = nn.LSTM(\r\n            ...\r\n            batch_first=True)\r\n        self.hn = initial_state\r\n            \r\n    def forward(self, input):\r\n        output, hn = self.lstm(input, self.hn)\r\n        self.hn = hn\r\n        return output\r\n', 'python\r\nclass LSTM(nn.Module):\r\n    def __init__(self, initial_state):\r\n        super(LSTM, self).__init__()\r\n\r\n        self.lstm = nn.LSTM(\r\n            ...\r\n            batch_first=True)\r\n            \r\n    def forward(self, input, hidden):\r\n        output, hn = self.lstm(input, hidden)\r\n        return output, hn\r\n']",['pip'],0,0
279,pytorch,4689,open,switch CUDA svd and qr to using cuSolver,"Currently we use MAGMA for these solvers, which uses a mix of CPU and GPU. Wonder if cuSolver will be faster, and we can avoid the CPU path entirely.

cc: @ngimel just want to confirm, cuSolver (dense) wont use the CPU right?

cc @ngimel @vincentqb @vishwakftw @SsnL @jianyuh",module: cuda module: linear algebra triaged,"[""Checked with cuSolver team, qr is GPU only, for svd they have CPU path (gesvd) and GPU-only path (gesvdj). They don't benchmark against magma, so YMMV. "", ""@soumith \r\nI'm sorry to go a bit off topic but I can't find reference about using gpu supported svd/qr. Is it possible to force MAGMA to run the factorization on the gpu?"", ""@PiotrSokol MAGMA's implementation of qr and svd has CPU calls. There are no alternative in MAGMA for these calls afaik. So no."", '@SsnL  Thanks!\r\n\r\n@soumith  Concerning the benchmarking, cupy uses cusolver as a backend and I compared it, though not very dilligently, to pytorch/MAGMA. MAGMA is ~2.5 faster on my setup, which is a 46 core Xeon CPU E5-2690 v4 @ 2.60GHz  and a Tesla P100 GPU.']",[],[],0,0
280,pytorch,29842,open,CI timeout after running test_async_grad_guard_with_grad (jit.test_async.TestAsync),"pytorch_linux_xenial_cuda9_cudnn7_py3_test
https://app.circleci.com/jobs/github/pytorch/pytorch/3600910



@suo 




cc @ezyang @gchanan @zou3519 @jerryzh168 @suo",high priority module: flaky-tests oncall: jit triage review triaged,[],['\r\nNov 14 19:29:53 test_async_grad_guard_with_grad (jit.test_async.TestAsync) ... ok\r\nToo long with no output (exceeded 1h30m0s)\r\n'],[],0,0
281,pytorch,23512,open,Build reconfiguration should consistently honor env variables,"Per discussion here: https://github.com/pytorch/pytorch/pull/23323#issuecomment-515168182

We currently have

(plainly == no additional build options passed in to setup.py)

for clean tree:

- configuration only: 
- configuration + build: invoke setup.py plainly

For rebuild:

- reconfiguration without rebuild:  (unreliable), or edit CMakeCache.txt and run cmake directly
- rebuild without reconfiguration: invoke setup.py plainly, all build options persist
- reconfiguration + rebuild:  (unreliable), or edit CMakeCache.txt and invoke setup.py plainly

We need to make the two ""unreliable"" spots ""reliable"".",module: build triaged,"['Yes please. @xuhdev do you plan to work on this?', '@colesbury Yes, but there are more cleanup to do before this can be done (i.e., lots of detections should be moved to CMake, see #21702). This is current set as a placeholder so we can bear this in mind while continuing build cleanup.']",[],"['--cmake-only', '--cmake-only --cmake', '--cmake']",0,0
282,pytorch,20704,open,Remove unpack() in torch/csrc/nn/type_checks.h and its caller functions in the codebase,The  function in torch/csrc/nn/type_checks.h is not used anymore because we had already killed torch.legacy.nn and all the normal NN ops go through ATen bindings right now. A proper removal of the  function would also involve removing all of its caller functions.,good first issue module: internals module: nn triaged,"[""I'd like to take this. Question: are `check_type` and friends in torch/csrc/nn/type_checks.h still relevant?"", '@yifuwang I think we can try to remove `unpack()` first, and then as a second step we can try to remove `check_type` and friends and see if the test complains.', '@yf225 it seems to me that the proper removal of `unpack()` would involve removing the logic for generating torch/csrc/nn/THNN.cpp and torch/csrc/nn/THCUNN.cpp (and maybe even torch._thnn), since without the unpacks those wrappers don\'t seem to make sense. Am I understanding it correctly? Or is there a meaningful intermediate change in which those wrappers should be kept around?\r\n\r\nAn example of the generated wrappers:\r\n```\r\nTH_API void THNN_CudaDoubleAbs_updateOutput(void*, THCudaDoubleTensor*, THCudaDoubleTensor*);\r\n\r\nPyObject * CudaDoubleAbs_updateOutput(PyObject *_unused, PyObject *args)\r\n{\r\n  HANDLE_TH_ERRORS\r\n  int __argcount = args ? PyTuple_Size(args) : 0;\r\n\r\n    if (__argcount == 3 &&\r\n          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&\r\n          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&\r\n          THNN_CudaDoubleTensor_Check(PyTuple_GET_ITEM(args, 2))) {\r\n\r\n      SpecializedDeviceGuard device_guard(get_device(args));\r\n\r\n      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));\r\n      THCudaDoubleTensor* arg_input = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));\r\n      THCudaDoubleTensor* arg_output = THNN_CudaDoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));\r\n\r\n      PyThreadState *_save = NULL;\r\n      try {\r\n        Py_UNBLOCK_THREADS;\r\n        THNN_CudaDoubleAbs_updateOutput(arg_state, arg_input, arg_output);\r\n        Py_BLOCK_THREADS;\r\n        Py_RETURN_NONE;\r\n      } catch (...) {\r\n        if (_save) {\r\n          Py_BLOCK_THREADS;\r\n        }\r\n        throw;\r\n      }\r\n\r\n  } else {\r\n    THPUtils_invalidArguments(args, NULL, ""CudaDoubleAbs_updateOutput"", 1, ""(int state, torch.cuda.DoubleTensor input, torch.cuda.DoubleTensor output)"");\r\n    return NULL;\r\n  }\r\n  END_HANDLE_TH_ERRORS\r\n}\r\n```', ""@yifuwang If that's the case I think we can remove all of them - as long as all the NN tests pass we should be in good shape."", 'I would like to tackle this', 'Hey @yf225, i would like to work on this issue']",[],"['unpack()', 'unpack()']",0,0
283,pytorch,17850,open,Build fails for caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/mkldnn/Conv.cpp.o; possibly MKL-DNN issue,"## üêõ Bug

PyTorch fails to build with an issue related to MKL-DNN. I previously built MKL-DNN by ing from their GitHub repo.

Error trace is below:



## Build command


## Environment

 - PyTorch Version (e.g., 1.0): Master branch
 - OS (e.g., Linux): Debian:Stretch
 - How you installed PyTorch (, , source): source
 - Build command you used (if compiling from source): see above
 - Python version: 3.6.5
 - CUDA/cuDNN version: NA
 - GPU models and configuration: NA
 - Any other relevant information: NA
",caffe2,[],"[""bash\r\n[91mIn file included from /opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:37:\r\n/opt/pytorch/aten/src/ATen/mkldnn/Runtime.h: In constructor ‚Äòat::native::Stream::Stream()‚Äô:\r\n/opt/pytorch/aten/src/ATen/mkldnn/Runtime.h:42:40: error: ‚Äòmkldnn::stream::kind‚Äô has not been declared\r\n   Stream():_cpu_stream(mkldnn::stream::kind::eager) {}\r\n                                        ^~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp: In function ‚Äòat::Tensor at::native::mkldnn_convolution(const at::Tensor&, const at::Tensor&, const at::Tensor&, c10::IntArrayRef, c10::IntArrayRef, c10::IntArrayRef, int64_t)‚Äô:\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:98:29: error: ‚Äòmkldnn::memory::format‚Äô has not been declared\r\n   auto format_any = memory::format::any;\r\n                             ^~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:99:30: error: ‚Äòmkldnn::memory::format‚Äô has not been declared\r\n   auto format_nchw = memory::format::nchw;\r\n                              ^~~~~~\r\n\x1b[0m\x1b[91m/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:100:42: error: ‚Äòmkldnn::memory::format‚Äô has not been declared\r\n   auto format_weight = (g!= 1) ? memory::format::goihw : memory::format::oihw;\r\n                                          ^~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:100:66: error: ‚Äòmkldnn::memory::format‚Äô has not been declared\r\n   auto format_weight = (g!= 1) ? memory::format::goihw : memory::format::oihw;\r\n                                                                  ^~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:101:27: error: ‚Äòmkldnn::memory::format‚Äô has not been declared\r\n   auto format_x = memory::format::x;\r\n                           ^~~~~~\r\n\x1b[0m\x1b[91m/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:131:21: error: no matching function for call to ‚Äòmkldnn::memory::memory(<brace-enclosed initializer list>, void*)‚Äô\r\n     input.data_ptr());\r\n                     ^\r\nIn file included from /opt/pytorch/aten/src/ATen/mkldnn/Runtime.h:3,\r\n                 from /opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:37:\r\n/usr/local/include/mkldnn.hpp:900:5: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory::desc&, const mkldnn::engine&)‚Äô\r\n     memory(const desc &md, const engine &aengine)\r\n     ^~~~~~\r\n/usr/local/include/mkldnn.hpp:900:5: note:   no known conversion for argument 1 from ‚Äò<brace-enclosed initializer list>‚Äô to ‚Äòconst mkldnn::memory::desc&‚Äô\r\n/usr/local/include/mkldnn.hpp:889:5: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory::desc&, const mkldnn::engine&, void*)‚Äô\r\n     memory(const desc &md, const engine &aengine, void *ahandle) {\r\n     ^~~~~~\r\n/usr/local/include/mkldnn.hpp:889:5: note:   candidate expects 3 arguments, 2 provided\r\n/usr/local/include/mkldnn.hpp:577:8: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory&)‚Äô\r\n struct memory: public handle<mkldnn_memory_t> {\r\n        ^~~~~~\r\n/usr/local/include/mkldnn.hpp:577:8: note:   candidate expects 1 argument, 2 provided\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:133:22: error: no matching function for call to ‚Äòmkldnn::memory::memory(<brace-enclosed initializer list>, void*)‚Äô\r\n     weight.data_ptr());\r\n                      ^\r\nIn file included from /opt/pytorch/aten/src/ATen/mkldnn/Runtime.h:3,\r\n                 from /opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:37:\r\n/usr/local/include/mkldnn.hpp:900:5: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory::desc&, const mkldnn::engine&)‚Äô\r\n     memory(const desc &md, const engine &aengine)\r\n     ^~~~~~\r\n/usr/local/include/mkldnn.hpp:900:5: note:   no known conversion for argument 1 from ‚Äò<brace-enclosed initializer list>‚Äô to ‚Äòconst mkldnn::memory::desc&‚Äô\r\n/usr/local/include/mkldnn.hpp:889:5: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory::desc&, const mkldnn::engine&, void*)‚Äô\r\n     memory(const desc &md, const engine &aengine, void *ahandle) {\r\n     ^~~~~~\r\n/usr/local/include/mkldnn.hpp:889:5: note:   candidate expects 3 arguments, 2 provided\r\n/usr/local/include/mkldnn.hpp:577:8: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory&)‚Äô\r\n struct memory: public handle<mkldnn_memory_t> {\r\n        ^~~~~~\r\n/usr/local/include/mkldnn.hpp:577:8: note:   candidate expects 1 argument, 2 provided\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:135:22: error: no matching function for call to ‚Äòmkldnn::memory::memory(<brace-enclosed initializer list>, void*)‚Äô\r\n     output.data_ptr());\r\n                      ^\r\nIn file included from /opt/pytorch/aten/src/ATen/mkldnn/Runtime.h:3,\r\n                 from /opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:37:\r\n/usr/local/include/mkldnn.hpp:900:5: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory::desc&, const mkldnn::engine&)‚Äô\r\n     memory(const desc &md, const engine &aengine)\r\n     ^~~~~~\r\n/usr/local/include/mkldnn.hpp:900:5: note:   no known conversion for argument 1 from ‚Äò<brace-enclosed initializer list>‚Äô to ‚Äòconst mkldnn::memory::desc&‚Äô\r\n/usr/local/include/mkldnn.hpp:889:5: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory::desc&, const mkldnn::engine&, void*)‚Äô\r\n     memory(const desc &md, const engine &aengine, void *ahandle) {\r\n     ^~~~~~\r\n/usr/local/include/mkldnn.hpp:889:5: note:   candidate expects 3 arguments, 2 provided\r\n/usr/local/include/mkldnn.hpp:577:8: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory&)‚Äô\r\n struct memory: public handle<mkldnn_memory_t> {\r\n        ^~~~~~\r\n/usr/local/include/mkldnn.hpp:577:8: note:   candidate expects 1 argument, 2 provided\r\n\x1b[0m\x1b[91m/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:139:36: error: ‚Äòusing element_type = struct mkldnn::convolution_forward::primitive_desc‚Äô {aka ‚Äòstruct mkldnn::convolution_forward::primitive_desc‚Äô} has no member named ‚Äòsrc_primitive_desc‚Äô; did you mean ‚Äòprimitive_desc‚Äô?\r\n   auto input_pd = conv_forward_pd->src_primitive_desc();\r\n                                    ^~~~~~~~~~~~~~~~~~\r\n                                    primitive_desc\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:140:23: error: unable to deduce ‚Äòauto‚Äô from ‚Äòinput_usr_memory‚Äô\r\n   auto input_memory = input_usr_memory;\r\n                       ^~~~~~~~~~~~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:141:56: error: ‚Äòprimitive_desc‚Äô is not a member of ‚Äòmkldnn::memory‚Äô\r\n   if (input_usr_memory.get_primitive_desc() != memory::primitive_desc(input_pd)) {\r\n                                                        ^~~~~~~~~~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:146:37: error: ‚Äòusing element_type = struct mkldnn::convolution_forward::primitive_desc‚Äô {aka ‚Äòstruct mkldnn::convolution_forward::primitive_desc‚Äô} has no member named ‚Äòweights_primitive_desc‚Äô; did you mean ‚Äòprimitive_desc‚Äô?\r\n   auto weight_pd = conv_forward_pd->weights_primitive_desc();\r\n                                     ^~~~~~~~~~~~~~~~~~~~~~\r\n                                     primitive_desc\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:147:24: error: unable to deduce ‚Äòauto‚Äô from ‚Äòweight_usr_memory‚Äô\r\n   auto weight_memory = weight_usr_memory;\r\n                        ^~~~~~~~~~~~~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:148:57: error: ‚Äòprimitive_desc‚Äô is not a member of ‚Äòmkldnn::memory‚Äô\r\n   if (weight_usr_memory.get_primitive_desc() != memory::primitive_desc(weight_pd)) {\r\n                                                         ^~~~~~~~~~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:153:37: error: ‚Äòusing element_type = struct mkldnn::convolution_forward::primitive_desc‚Äô {aka ‚Äòstruct mkldnn::convolution_forward::primitive_desc‚Äô} has no member named ‚Äòdst_primitive_desc‚Äô; did you mean ‚Äòprimitive_desc‚Äô?\r\n   auto output_pd = conv_forward_pd->dst_primitive_desc();\r\n                                     ^~~~~~~~~~~~~~~~~~\r\n                                     primitive_desc\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:154:24: error: unable to deduce ‚Äòauto‚Äô from ‚Äòoutput_usr_memory‚Äô\r\n   auto output_memory = output_usr_memory;\r\n                        ^~~~~~~~~~~~~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:155:57: error: ‚Äòprimitive_desc‚Äô is not a member of ‚Äòmkldnn::memory‚Äô\r\n   if (output_usr_memory.get_primitive_desc() != memory::primitive_desc(output_pd)) {\r\n                                                         ^~~~~~~~~~~~~~\r\n\x1b[0m\x1b[91m/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:163:22: error: no matching function for call to ‚Äòmkldnn::memory::memory(<brace-enclosed initializer list>, void*)‚Äô\r\n       bias.data_ptr()));\r\n                      ^\r\nIn file included from /opt/pytorch/aten/src/ATen/mkldnn/Runtime.h:3,\r\n                 from /opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:37:\r\n/usr/local/include/mkldnn.hpp:900:5: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory::desc&, const mkldnn::engine&)‚Äô\r\n     memory(const desc &md, const engine &aengine)\r\n     ^~~~~~\r\n/usr/local/include/mkldnn.hpp:900:5: note:   no known conversion for argument 1 from ‚Äò<brace-enclosed initializer list>‚Äô to ‚Äòconst mkldnn::memory::desc&‚Äô\r\n/usr/local/include/mkldnn.hpp:889:5: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory::desc&, const mkldnn::engine&, void*)‚Äô\r\n     memory(const desc &md, const engine &aengine, void *ahandle) {\r\n     ^~~~~~\r\n/usr/local/include/mkldnn.hpp:889:5: note:   candidate expects 3 arguments, 2 provided\r\n/usr/local/include/mkldnn.hpp:577:8: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory&)‚Äô\r\n struct memory: public handle<mkldnn_memory_t> {\r\n        ^~~~~~\r\n/usr/local/include/mkldnn.hpp:577:8: note:   candidate expects 1 argument, 2 provided\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:176:35: error: ‚Äòstruct mkldnn::stream‚Äô has no member named ‚Äòsubmit‚Äô\r\n   Stream::Instance().get_stream().submit(net);\r\n                                   ^~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp: In function ‚Äòat::Tensor at::native::mkldnn_convolution_backward_input(c10::IntArrayRef, const at::Tensor&, const at::Tensor&, c10::IntArrayRef, c10::IntArrayRef, c10::IntArrayRef, int64_t, bool)‚Äô:\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:209:29: error: ‚Äòmkldnn::memory::format‚Äô has not been declared\r\n   auto format_any = memory::format::any;\r\n                             ^~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:210:30: error: ‚Äòmkldnn::memory::format‚Äô has not been declared\r\n   auto format_nchw = memory::format::nchw;\r\n                              ^~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:211:42: error: ‚Äòmkldnn::memory::format‚Äô has not been declared\r\n   auto format_weight = (g!= 1) ? memory::format::goihw : memory::format::oihw;\r\n                                          ^~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:211:66: error: ‚Äòmkldnn::memory::format‚Äô has not been declared\r\n   auto format_weight = (g!= 1) ? memory::format::goihw : memory::format::oihw;\r\n                                                                  ^~~~~~\r\n\x1b[0m\x1b[91m/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:251:27: error: no matching function for call to ‚Äòmkldnn::memory::memory(<brace-enclosed initializer list>, void*)‚Äô\r\n     grad_output.data_ptr());\r\n                           ^\r\nIn file included from /opt/pytorch/aten/src/ATen/mkldnn/Runtime.h:3,\r\n                 from /opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:37:\r\n/usr/local/include/mkldnn.hpp:900:5: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory::desc&, const mkldnn::engine&)‚Äô\r\n     memory(const desc &md, const engine &aengine)\r\n     ^~~~~~\r\n/usr/local/include/mkldnn.hpp:900:5: note:   no known conversion for argument 1 from ‚Äò<brace-enclosed initializer list>‚Äô to ‚Äòconst mkldnn::memory::desc&‚Äô\r\n/usr/local/include/mkldnn.hpp:889:5: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory::desc&, const mkldnn::engine&, void*)‚Äô\r\n     memory(const desc &md, const engine &aengine, void *ahandle) {\r\n     ^~~~~~\r\n/usr/local/include/mkldnn.hpp:889:5: note:   candidate expects 3 arguments, 2 provided\r\n/usr/local/include/mkldnn.hpp:577:8: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory&)‚Äô\r\n struct memory: public handle<mkldnn_memory_t> {\r\n        ^~~~~~\r\n/usr/local/include/mkldnn.hpp:577:8: note:   candidate expects 1 argument, 2 provided\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:253:22: error: no matching function for call to ‚Äòmkldnn::memory::memory(<brace-enclosed initializer list>, void*)‚Äô\r\n     weight.data_ptr());\r\n                      ^\r\nIn file included from /opt/pytorch/aten/src/ATen/mkldnn/Runtime.h:3,\r\n                 from /opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:37:\r\n/usr/local/include/mkldnn.hpp:900:5: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory::desc&, const mkldnn::engine&)‚Äô\r\n     memory(const desc &md, const engine &aengine)\r\n     ^~~~~~\r\n/usr/local/include/mkldnn.hpp:900:5: note:   no known conversion for argument 1 from ‚Äò<brace-enclosed initializer list>‚Äô to ‚Äòconst mkldnn::memory::desc&‚Äô\r\n/usr/local/include/mkldnn.hpp:889:5: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory::desc&, const mkldnn::engine&, void*)‚Äô\r\n     memory(const desc &md, const engine &aengine, void *ahandle) {\r\n     ^~~~~~\r\n/usr/local/include/mkldnn.hpp:889:5: note:   candidate expects 3 arguments, 2 provided\r\n/usr/local/include/mkldnn.hpp:577:8: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory&)‚Äô\r\n struct memory: public handle<mkldnn_memory_t> {\r\n        ^~~~~~\r\n/usr/local/include/mkldnn.hpp:577:8: note:   candidate expects 1 argument, 2 provided\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:255:26: error: no matching function for call to ‚Äòmkldnn::memory::memory(<brace-enclosed initializer list>, void*)‚Äô\r\n     grad_input.data_ptr());\r\n                          ^\r\nIn file included from /opt/pytorch/aten/src/ATen/mkldnn/Runtime.h:3,\r\n                 from /opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:37:\r\n/usr/local/include/mkldnn.hpp:900:5: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory::desc&, const mkldnn::engine&)‚Äô\r\n     memory(const desc &md, const engine &aengine)\r\n     ^~~~~~\r\n/usr/local/include/mkldnn.hpp:900:5: note:   no known conversion for argument 1 from ‚Äò<brace-enclosed initializer list>‚Äô to ‚Äòconst mkldnn::memory::desc&‚Äô\r\n/usr/local/include/mkldnn.hpp:889:5: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory::desc&, const mkldnn::engine&, void*)‚Äô\r\n     memory(const desc &md, const engine &aengine, void *ahandle) {\r\n     ^~~~~~\r\n/usr/local/include/mkldnn.hpp:889:5: note:   candidate expects 3 arguments, 2 provided\r\n/usr/local/include/mkldnn.hpp:577:8: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory&)‚Äô\r\n struct memory: public handle<mkldnn_memory_t> {\r\n        ^~~~~~\r\n/usr/local/include/mkldnn.hpp:577:8: note:   candidate expects 1 argument, 2 provided\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:259:48: error: ‚Äòusing element_type = struct mkldnn::convolution_backward_data::primitive_desc‚Äô {aka ‚Äòstruct mkldnn::convolution_backward_data::primitive_desc‚Äô} has no member named ‚Äòdiff_dst_primitive_desc‚Äô; did you mean ‚Äòprimitive_desc‚Äô?\r\n   auto grad_output_pd = conv_backward_data_pd->diff_dst_primitive_desc();\r\n                                                ^~~~~~~~~~~~~~~~~~~~~~~\r\n                                                primitive_desc\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:260:29: error: unable to deduce ‚Äòauto‚Äô from ‚Äògrad_output_usr_memory‚Äô\r\n   auto grad_output_memory = grad_output_usr_memory;\r\n                             ^~~~~~~~~~~~~~~~~~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:261:62: error: ‚Äòprimitive_desc‚Äô is not a member of ‚Äòmkldnn::memory‚Äô\r\n   if (grad_output_usr_memory.get_primitive_desc() != memory::primitive_desc(grad_output_pd)) {\r\n                                                              ^~~~~~~~~~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:266:43: error: ‚Äòusing element_type = struct mkldnn::convolution_backward_data::primitive_desc‚Äô {aka ‚Äòstruct mkldnn::convolution_backward_data::primitive_desc‚Äô} has no member named ‚Äòweights_primitive_desc‚Äô; did you mean ‚Äòprimitive_desc‚Äô?\r\n   auto weight_pd = conv_backward_data_pd->weights_primitive_desc();\r\n                                           ^~~~~~~~~~~~~~~~~~~~~~\r\n                                           primitive_desc\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:267:24: error: unable to deduce ‚Äòauto‚Äô from ‚Äòweight_usr_memory‚Äô\r\n   auto weight_memory = weight_usr_memory;\r\n                        ^~~~~~~~~~~~~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:268:57: error: ‚Äòprimitive_desc‚Äô is not a member of ‚Äòmkldnn::memory‚Äô\r\n   if (weight_usr_memory.get_primitive_desc() != memory::primitive_desc(weight_pd)) {\r\n                                                         ^~~~~~~~~~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:273:47: error: ‚Äòusing element_type = struct mkldnn::convolution_backward_data::primitive_desc‚Äô {aka ‚Äòstruct mkldnn::convolution_backward_data::primitive_desc‚Äô} has no member named ‚Äòdiff_src_primitive_desc‚Äô; did you mean ‚Äòprimitive_desc‚Äô?\r\n   auto grad_input_pd = conv_backward_data_pd->diff_src_primitive_desc();\r\n                                               ^~~~~~~~~~~~~~~~~~~~~~~\r\n                                               primitive_desc\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:274:28: error: unable to deduce ‚Äòauto‚Äô from ‚Äògrad_input_usr_memory‚Äô\r\n   auto grad_input_memory = grad_input_usr_memory;\r\n                            ^~~~~~~~~~~~~~~~~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:275:57: error: ‚Äòprimitive_desc‚Äô is not a member of ‚Äòmkldnn::memory‚Äô\r\n   if (grad_input_memory.get_primitive_desc() != memory::primitive_desc(grad_input_pd)) {\r\n                                                         ^~~~~~~~~~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:288:35: error: ‚Äòstruct mkldnn::stream‚Äô has no member named ‚Äòsubmit‚Äô\r\n   Stream::Instance().get_stream().submit(net);\r\n                                   ^~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp: In function ‚Äòstd::tuple<at::Tensor, at::Tensor> at::native::mkldnn_convolution_backward_weights(c10::IntArrayRef, const at::Tensor&, const at::Tensor&, c10::IntArrayRef, c10::IntArrayRef, c10::IntArrayRef, int64_t, bool)‚Äô:\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:326:29: error: ‚Äòmkldnn::memory::format‚Äô has not been declared\r\n   auto format_any = memory::format::any;\r\n                             ^~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:327:30: error: ‚Äòmkldnn::memory::format‚Äô has not been declared\r\n   auto format_nchw = memory::format::nchw;\r\n                              ^~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:328:42: error: ‚Äòmkldnn::memory::format‚Äô has not been declared\r\n   auto format_weight = (g!= 1) ? memory::format::goihw : memory::format::oihw;\r\n                                          ^~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:328:66: error: ‚Äòmkldnn::memory::format‚Äô has not been declared\r\n   auto format_weight = (g!= 1) ? memory::format::goihw : memory::format::oihw;\r\n                                                                  ^~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:329:27: error: ‚Äòmkldnn::memory::format‚Äô has not been declared\r\n   auto format_x = memory::format::x;\r\n                           ^~~~~~\r\n\x1b[0m\x1b[91m/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:375:21: error: no matching function for call to ‚Äòmkldnn::memory::memory(<brace-enclosed initializer list>, void*)‚Äô\r\n     input.data_ptr());\r\n                     ^\r\nIn file included from /opt/pytorch/aten/src/ATen/mkldnn/Runtime.h:3,\r\n                 from /opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:37:\r\n/usr/local/include/mkldnn.hpp:900:5: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory::desc&, const mkldnn::engine&)‚Äô\r\n     memory(const desc &md, const engine &aengine)\r\n     ^~~~~~\r\n/usr/local/include/mkldnn.hpp:900:5: note:   no known conversion for argument 1 from ‚Äò<brace-enclosed initializer list>‚Äô to ‚Äòconst mkldnn::memory::desc&‚Äô\r\n/usr/local/include/mkldnn.hpp:889:5: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory::desc&, const mkldnn::engine&, void*)‚Äô\r\n     memory(const desc &md, const engine &aengine, void *ahandle) {\r\n     ^~~~~~\r\n/usr/local/include/mkldnn.hpp:889:5: note:   candidate expects 3 arguments, 2 provided\r\n/usr/local/include/mkldnn.hpp:577:8: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory&)‚Äô\r\n struct memory: public handle<mkldnn_memory_t> {\r\n        ^~~~~~\r\n/usr/local/include/mkldnn.hpp:577:8: note:   candidate expects 1 argument, 2 provided\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:377:27: error: no matching function for call to ‚Äòmkldnn::memory::memory(<brace-enclosed initializer list>, void*)‚Äô\r\n     grad_output.data_ptr());\r\n                           ^\r\nIn file included from /opt/pytorch/aten/src/ATen/mkldnn/Runtime.h:3,\r\n                 from /opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:37:\r\n/usr/local/include/mkldnn.hpp:900:5: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory::desc&, const mkldnn::engine&)‚Äô\r\n     memory(const desc &md, const engine &aengine)\r\n     ^~~~~~\r\n/usr/local/include/mkldnn.hpp:900:5: note:   no known conversion for argument 1 from ‚Äò<brace-enclosed initializer list>‚Äô to ‚Äòconst mkldnn::memory::desc&‚Äô\r\n/usr/local/include/mkldnn.hpp:889:5: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory::desc&, const mkldnn::engine&, void*)‚Äô\r\n     memory(const desc &md, const engine &aengine, void *ahandle) {\r\n     ^~~~~~\r\n/usr/local/include/mkldnn.hpp:889:5: note:   candidate expects 3 arguments, 2 provided\r\n/usr/local/include/mkldnn.hpp:577:8: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory&)‚Äô\r\n struct memory: public handle<mkldnn_memory_t> {\r\n        ^~~~~~\r\n/usr/local/include/mkldnn.hpp:577:8: note:   candidate expects 1 argument, 2 provided\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:379:27: error: no matching function for call to ‚Äòmkldnn::memory::memory(<brace-enclosed initializer list>, void*)‚Äô\r\n     grad_weight.data_ptr());\r\n                           ^\r\nIn file included from /opt/pytorch/aten/src/ATen/mkldnn/Runtime.h:3,\r\n                 from /opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:37:\r\n/usr/local/include/mkldnn.hpp:900:5: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory::desc&, const mkldnn::engine&)‚Äô\r\n     memory(const desc &md, const engine &aengine)\r\n     ^~~~~~\r\n/usr/local/include/mkldnn.hpp:900:5: note:   no known conversion for argument 1 from ‚Äò<brace-enclosed initializer list>‚Äô to ‚Äòconst mkldnn::memory::desc&‚Äô\r\n/usr/local/include/mkldnn.hpp:889:5: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory::desc&, const mkldnn::engine&, void*)‚Äô\r\n     memory(const desc &md, const engine &aengine, void *ahandle) {\r\n     ^~~~~~\r\n/usr/local/include/mkldnn.hpp:889:5: note:   candidate expects 3 arguments, 2 provided\r\n/usr/local/include/mkldnn.hpp:577:8: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory&)‚Äô\r\n struct memory: public handle<mkldnn_memory_t> {\r\n        ^~~~~~\r\n/usr/local/include/mkldnn.hpp:577:8: note:   candidate expects 1 argument, 2 provided\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:384:44: error: ‚Äòusing element_type = struct mkldnn::convolution_backward_weights::primitive_desc‚Äô {aka ‚Äòstruct mkldnn::convolution_backward_weights::primitive_desc‚Äô} has no member named ‚Äòsrc_primitive_desc‚Äô; did you mean ‚Äòprimitive_desc‚Äô?\r\n   auto input_pd = conv_backward_weight_pd->src_primitive_desc();\r\n                                            ^~~~~~~~~~~~~~~~~~\r\n                                            primitive_desc\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:385:23: error: unable to deduce ‚Äòauto‚Äô from ‚Äòinput_usr_memory‚Äô\r\n   auto input_memory = input_usr_memory;\r\n                       ^~~~~~~~~~~~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:386:56: error: ‚Äòprimitive_desc‚Äô is not a member of ‚Äòmkldnn::memory‚Äô\r\n   if (input_usr_memory.get_primitive_desc() != memory::primitive_desc(input_pd)) {\r\n                                                        ^~~~~~~~~~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:391:50: error: ‚Äòusing element_type = struct mkldnn::convolution_backward_weights::primitive_desc‚Äô {aka ‚Äòstruct mkldnn::convolution_backward_weights::primitive_desc‚Äô} has no member named ‚Äòdiff_dst_primitive_desc‚Äô; did you mean ‚Äòprimitive_desc‚Äô?\r\n   auto grad_output_pd = conv_backward_weight_pd->diff_dst_primitive_desc();\r\n                                                  ^~~~~~~~~~~~~~~~~~~~~~~\r\n                                                  primitive_desc\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:392:29: error: unable to deduce ‚Äòauto‚Äô from ‚Äògrad_output_usr_memory‚Äô\r\n   auto grad_output_memory = grad_output_usr_memory;\r\n                             ^~~~~~~~~~~~~~~~~~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:393:62: error: ‚Äòprimitive_desc‚Äô is not a member of ‚Äòmkldnn::memory‚Äô\r\n   if (grad_output_usr_memory.get_primitive_desc() != memory::primitive_desc(grad_output_pd)) {\r\n                                                              ^~~~~~~~~~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:398:50: error: ‚Äòusing element_type = struct mkldnn::convolution_backward_weights::primitive_desc‚Äô {aka ‚Äòstruct mkldnn::convolution_backward_weights::primitive_desc‚Äô} has no member named ‚Äòdiff_weights_primitive_desc‚Äô; did you mean ‚Äòdiff_weights_desc‚Äô?\r\n   auto grad_weight_pd = conv_backward_weight_pd->diff_weights_primitive_desc();\r\n                                                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n                                                  diff_weights_desc\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:399:29: error: unable to deduce ‚Äòauto‚Äô from ‚Äògrad_weight_usr_memory‚Äô\r\n   auto grad_weight_memory = grad_weight_usr_memory;\r\n                             ^~~~~~~~~~~~~~~~~~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:400:62: error: ‚Äòprimitive_desc‚Äô is not a member of ‚Äòmkldnn::memory‚Äô\r\n   if (grad_weight_usr_memory.get_primitive_desc() != memory::primitive_desc(grad_weight_pd)) {\r\n                                                              ^~~~~~~~~~~~~~\r\n/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:407:27: error: no matching function for call to ‚Äòmkldnn::memory::memory(<brace-enclosed initializer list>, void*)‚Äô\r\n       grad_bias.data_ptr()));\r\n                           ^\r\nIn file included from /opt/pytorch/aten/src/ATen/mkldnn/Runtime.h:3,\r\n                 from /opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:37:\r\n/usr/local/include/mkldnn.hpp:900:5: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory::desc&, const mkldnn::engine&)‚Äô\r\n     memory(const desc &md, const engine &aengine)\r\n     ^~~~~~\r\n/usr/local/include/mkldnn.hpp:900:5: note:   no known conversion for argument 1 from ‚Äò<brace-enclosed initializer list>‚Äô to ‚Äòconst mkldnn::memory::desc&‚Äô\r\n/usr/local/include/mkldnn.hpp:889:5: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory::desc&, const mkldnn::engine&, void*)‚Äô\r\n     memory(const desc &md, const engine &aengine, void *ahandle) {\r\n     ^~~~~~\r\n/usr/local/include/mkldnn.hpp:889:5: note:   candidate expects 3 arguments, 2 provided\r\n/usr/local/include/mkldnn.hpp:577:8: note: candidate: ‚Äòmkldnn::memory::memory(const mkldnn::memory&)‚Äô\r\n struct memory: public handle<mkldnn_memory_t> {\r\n        ^~~~~~\r\n/usr/local/include/mkldnn.hpp:577:8: note:   candidate expects 1 argument, 2 provided\r\n\x1b[0m\x1b[91m/opt/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:421:35: error: ‚Äòstruct mkldnn::stream‚Äô has no member named ‚Äòsubmit‚Äô\r\n   Stream::Instance().get_stream().submit(net);\r\n                                   ^~~~~~\r\n\x1b[0m[ 64%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUFloatType.cpp.o\r\ncaffe2/CMakeFiles/caffe2.dir/build.make:2841: recipe for target 'caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/mkldnn/Conv.cpp.o' failed\r\n\x1b[91mmake[2]: *** [caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/mkldnn/Conv.cpp.o] Error 1\r\nmake[2]: *** Waiting for unfinished jobs....\r\n\x1b[0m\x1b[91mmake[1]: *** [caffe2/CMakeFiles/caffe2.dir/all] Error 2\r\n\x1b[0mCMakeFiles/Makefile2:2107: recipe for target 'caffe2/CMakeFiles/caffe2.dir/all' failed\r\nMakefile:138: recipe for target 'all' failed\r\n\x1b[91mmake: *** [all] Error 2\r\n\x1b[0mBuilding wheel torch-1.1.0a0+742568e\r\n-- Building version 1.1.0a0+742568e\r\n"", 'bash \r\ncd /opt && git clone --recursive https://github.com/pytorch/pytorch && \\\r\n    cd /opt/pytorch && \\\r\n    git submodule update --init && \\\r\n    cd /opt/pytorch && ls && \\\r\n    sed -i \'s/""Use MKLDNN"" OFF/""Use MKLDNN"" ON/g\' CMakeLists.txt && \\\r\n    sed -i \'s/""Use DISTRIBUTED"" OFF/""Use DISTRIBUTED"" ON /g\' CMakeLists.txt && \\\r\n    sed -i \'s/for parallel code"" OFF/for parallel code"" ON /g\' CMakeLists.txt && \\\r\n    PYTHON_EXECUTABLE=/opt/conda/bin/python \\\r\n    PYTHON_LIBRARY=/opt/conda/lib/libpython3.6m.so \\\r\n    PYTHON_INCLUDE_DIR=/opt/conda/include/python3.6m \\\r\n    FULL_CAFFE2=1 \\\r\n    USE_OPENMP=1 \\\r\n    USE_MKL=1 \\\r\n    USE_MKLDNN=1 \\\r\n    USE_MKLML=1 \\\r\n    USE_SYSTEM_EIGEN_INSTALL=1 \\\r\n    USE_ZMQ=1 \\\r\n    USE_DISTRIBUTED=1 \\\r\n    BUILD_TEST=0 \\\r\n    MKLDNN_LIBRARY=/usr/local/lib \\\r\n    MKLDNN_INCLUDE_DIR=/usr/local/include \\\r\n    MKLDNN_LIB_DIR=/usr/local/lib \\\r\n    python setup.py install \r\n']","['cmake', 'conda', 'pip']",0,0
284,pytorch,28206,open,torch.utils.tensorboard.SummaryWriter.add_graph do not support non-tensor inputs,"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:

1.Run my script below:


and you can see the trace(take bug 3 as an example):

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->
writer.add_graph should run normally.

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


 Collecting environment information...
PyTorch version: 1.3.0
Is debug build: No
CUDA used to build PyTorch: None
OS: Mac OSX 10.14.6
GCC version: Could not collect
CMake version: Could not collect
Python version: 3.7
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
Versions of relevant libraries:
[pip] numpy==1.17.2
[pip] torch==1.3.0
[pip] torchvision==0.4.1
[conda] torch                     1.3.0                    pypi_0    pypi
[conda] torchvision               0.4.1                    pypi_0    pypi

## Additional context

<!-- Add any other context about the problem here. -->
1.TensorboardX.SummaryWriter.add_graph has the same bug as torch.utils.tensorboard
2.Besides this bug, I hope add_graph could accept not only a tuple as positional arguments, but also a dict as keyword arguments for the model.forward()'s input",oncall: visualization,"['cc @orionr ', 'cc @sanekmelnikov, @natalialunova and @lanpa', ""addressed here already: https://github.com/lanpa/tensorboardX/issues/520\r\n\r\nOnce JIT team supports this, it's easy to visualize it.""]","['python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.utils.tensorboard import SummaryWriter\r\n# from tensorboardX import SummaryWriter\r\n\r\n# bug 1: bool type inputs\r\nclass Net_1(nn.Module):\r\n    def __init__(self, dropout=0.5):\r\n        super(Net_1, self).__init__()\r\n        self.fc1 = nn.Linear(120, 84)\r\n        self.fc2 = nn.Linear(84, 10)\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, x, use_dropout=False):\r\n        x = F.relu(self.fc1(x))\r\n        if use_dropout:\r\n            x = self.dropout(x)  # or other operations ....\r\n        x = F.relu(self.fc2(x))\r\n        return x\r\n\r\nwith SummaryWriter(""bugs"") as w:\r\n    net = Net_1()\r\n    input_x = torch.randn((2,120))\r\n    w.add_graph(net, (input_x, True))\r\n\r\n\r\n# bug 2: None type inputs (might be argument\'s default value)\r\nclass Net_2(nn.Module):\r\n    def __init__(self):\r\n        super(Net_2, self).__init__()\r\n        self.fc1 = nn.Linear(120, 84)\r\n        self.fc2 = nn.Linear(120, 84)\r\n        self.fc3 = nn.Linear(120, 84)\r\n        self.fc4 = nn.Linear(84, 10)\r\n\r\n    def forward(self, x, y=None, z=None):\r\n        x = F.relu(self.fc1(x))\r\n        if y is not None:\r\n            y = F.relu(self.fc2(y))\r\n            x = x + y\r\n        if z is not None:\r\n            z = F.relu(self.fc3(z))\r\n            x = x + z\r\n        x = F.relu(self.fc4(x))\r\n        return x\r\n\r\nwith SummaryWriter(""bugs"") as w:\r\n    net = Net_2()\r\n    input_x = torch.randn((2,120))\r\n    input_y = None\r\n    input_z = torch.randn((2,120))\r\n    w.add_graph(net, (input_x, input_y, input_z))\r\n\r\n\r\n# bug 3: List type inputs (dict, or other python build-in types like int,str,... may also meet this question)\r\nclass Net_3(nn.Module):\r\n    def __init__(self):\r\n        super(Net_3, self).__init__()\r\n        self.fc_list = [nn.Linear(120, 120) for _ in range(10)]\r\n        self.fc_n = nn.Linear(120, 10)\r\n\r\n    def forward(self, x, index:list=None):\r\n        if index is not None:\r\n            for i in index:\r\n                x = F.relu(self.fc_list[i](x))\r\n        x = F.relu(self.fc_n(x))\r\n        return x\r\n\r\nwith SummaryWriter(""bugs"") as w:\r\n    net = Net_3()\r\n    input_x = torch.randn((2, 120))\r\n    index = [1, 5, 1, 7, 0]\r\n    w.add_graph(net, (input_x, index))\r\n\r\n', '\r\nError occurs, No graph saved\r\nTraceback (most recent call last):\r\n  File ""<input>"", line 1, in <module>\r\n  File ""/Applications/PyCharm.app/Contents/helpers/pydev/_pydev_bundle/pydev_umd.py"", line 197, in runfile\r\n    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script\r\n  File ""/Applications/PyCharm.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile\r\n    exec(compile(contents+""\\n"", file, \'exec\'), glob, loc)\r\n  File ""/Users/wangyuanzheng/Downloads/xxxxxxx/project/albert_pytorch/dev/add_graph_bug.py"", line 25, in <module>\r\n    w.add_graph(net, (input_x, True))\r\n  File ""/Users/wangyuanzheng/anaconda3/envs/CCFBigData-torch/lib/python3.7/site-packages/torch/utils/tensorboard/writer.py"", line 682, in add_graph\r\n    self._get_file_writer().add_graph(graph(model, input_to_model, verbose))\r\n  File ""/Users/wangyuanzheng/anaconda3/envs/CCFBigData-torch/lib/python3.7/site-packages/torch/utils/tensorboard/_pytorch_graph.py"", line 239, in graph\r\n    raise e\r\n  File ""/Users/wangyuanzheng/anaconda3/envs/CCFBigData-torch/lib/python3.7/site-packages/torch/utils/tensorboard/_pytorch_graph.py"", line 234, in graph\r\n    trace = torch.jit.trace(model, args)\r\n  File ""/Users/wangyuanzheng/anaconda3/envs/CCFBigData-torch/lib/python3.7/site-packages/torch/jit/__init__.py"", line 858, in trace\r\n    check_tolerance, _force_outplace, _module_class)\r\n  File ""/Users/wangyuanzheng/anaconda3/envs/CCFBigData-torch/lib/python3.7/site-packages/torch/jit/__init__.py"", line 997, in trace_module\r\n    module._c._create_method_from_trace(method_name, func, example_inputs, var_lookup_fn, _force_outplace)\r\nRuntimeError: Type \'Tuple[Tensor, bool]\' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced (toTraceableIValue at ../torch/csrc/jit/pybind_utils.h:298)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 135 (0x110c479e7 in libc10.dylib)\r\nframe #1: torch::jit::toTraceableIValue(pybind11::handle) + 1280 (0x110246740 in libtorch_python.dylib)\r\nframe #2: torch::jit::toTypedStack(pybind11::tuple const&) + 31 (0x1102e7edf in libtorch_python.dylib)\r\nframe #3: void pybind11::cpp_function::initialize<torch::jit::script::initJitScriptBindings(_object*)::$_16, void, torch::jit::script::Module&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, pybind11::function, pybind11::tuple, pybind11::function, bool, pybind11::name, pybind11::is_method, pybind11::sibling>(torch::jit::script::initJitScriptBindings(_object*)::$_16&&, void (*)(torch::jit::script::Module&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, pybind11::function, pybind11::tuple, pybind11::function, bool), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::\'lambda\'(pybind11::detail::function_call&)::__invoke(pybind11::detail::function_call&) + 147 (0x11031e4e3 in libtorch_python.dylib)\r\nframe #4: pybind11::cpp_function::dispatcher(_object*, _object*, _object*) + 3372 (0x10fe57d3c in libtorch_python.dylib)\r\n<omitting python frames>\r\n', '\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n']",[],0,0
285,pytorch,14839,open,Test OpenCV4 in CI,"It's not tested, evidence https://github.com/pytorch/pytorch/pull/14356

cc @ezyang @seemethere @malfet @walterddr @pytorch/pytorch-dev-infra",module: ci triaged,[],[],[],0,0
286,pytorch,28323,open,[quantization] fix the seed for hypothesis in CI,"Currently the hypothesis is semi-random on CI. We need to fix the seed to something constant.

cc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100",oncall: quantization triaged,"[""Wouldn't that result in less coverage? Currently every run of hypothesis in CI tries some random examples from search space. If random seed is fixed, it will try the same examples. Wouldn't it? "", 'They are derandomized I think, as long as you import TestCase in common_utils.py', ""> Wouldn't that result in less coverage? Currently every run of hypothesis in CI tries some random examples from search space. If random seed is fixed, it will try the same examples. Wouldn't it?\r\n\r\nMy idea is that we run a fixed seed on CI in order ot make it deterministic, but locally we force it to run with a different seed every time. Ideas?""]",[],[],0,0
287,pytorch,6257,open,"[feature request] adding a nonzero element ""in-place"" in sparse tensor","First mentioned here, but turns out to be non-existent. 
https://github.com/pytorch/pytorch/pull/6225#pullrequestreview-108887248

This operation would simply add a new index and a new value to a sparse tensor. 

cc @vincentqb @aocsa @nikitaved @pearu @mruberry",module: sparse triaged,"[""This is not much different from in-place addition, and it's already there. "", 'It is actually different, as it requires resizing indices and vals tensors. ']",[],[],0,0
288,pytorch,21015,open,How to use Infiniband for cpu-cluster with backend gloo?,"Now I'm trying to build pytorch from source for my cpu-cluster with backend gloo.
After installing pytorch, I got this information from install summay:

In my cluster, the network interface ""eno1"" represents Ethernet, and ""ib0"" represents Infiniband.
I set the environment variable , and distributed pytorch works fine. But when I set , it will cause some error.

What should I do?
Thanks.",oncall: distributed triaged,"['CC @pietern ', '@pietern \r\nCould you please give me some help?\r\nThanks.', '@sth1997 What kind of error? If you have an ib0 interface it is most likely for IPoIB, which is unrelated to whether you compile with ibverbs or not (since that is only used for actual IB, not IPoIB).', '@sth1997 @pietern How did you compile with USE_MPI. I installed openmpi under system path. I run the following command:\r\n`USE_DISTRIBUTED=1 USE_GLOO_IBVERBS=1 USE_GLOO=1 python3 setup.py.`\r\nBut it shows no mpi found.\r\n```\r\n-- MPI operators skipped due to no MPI support\r\n--   USE_DISTRIBUTED       : True\r\n--     USE_MPI             : OFF\r\n--     USE_GLOO            : ON\r\n--     USE_GLOO_IBVERBS    : OFF\r\n```\r\nCould you please give me some help?', '@401qingkong You can try setting `USE_MPI=1` and searching the CMake output for mentions of MPI. Perhaps your MPI distribution cannot be found by the build system.', 'If we want to use RDMA not IPOIB with gloo backend, which paramters need to be set. We set the GLOO_SOCKET_IFNAME=ib0, it may use the IPOIB not rdma. Thank you:) ', '> @sth1997 What kind of error? If you have an ib0 interface it is most likely for IPoIB, which is unrelated to whether you compile with ibverbs or not (since that is only used for actual IB, not IPoIB).\r\n\r\nSo how to use the actual IB, not IPoIB with gloo or mpi backend, could you give me some advice? Thanks in advance.']",['\r\n --   USE_DISTRIBUTED       : True\r\n --     USE_MPI             : ON\r\n --     USE_GLOO            : ON\r\n --     USE_GLOO_IBVERBS    : 1\r\n'],"['GLOO_SOCKET_IFNAME=eno1', 'GLOO_SOCKET_IFNAME=ib0']",0,0
289,pytorch,21135,open,[cmake build] can't build pytorch with install mkl library ,"### pytorch can't be built with  installed mkl library 
 - PyTorch version : master branch 
 - OS : CentOS-7.5

I had installed intel . So when I built  from the scratch, the build system couldn't find the installed  libraries.

I found  the mkl-related module, which was  . -related enviroments  were set to default path as the following:

In this way, I couldn't build  through  with the installed  libraries. Is there any effective method to build  with installed . 
### What I did to build with installed 
In order to build  with installed  and accept environments from the command line, I did modify the  as the following: 

And to accept the environments from the command line, the file  was modified as the following: 
`
--- a/tools/build_pytorch_libs.py
+++ b/tools/build_pytorch_libs.py
@@ -228,6 +228,18 @@ def run_cmake(version,
     if os.getenv('MKL_TBB'):
         cmake_defines(cmake_args, INTEL_MKL_TBB=check_env_flag('MKL_TBB'))

+    if os.getenv('INTEL_COMPILER_DIR'):
+        cmake_defines(cmake_args, INTEL_COMPILER_DIR=os.getenv('INTEL_COMPILER_DIR'))
+
+    if os.getenv('INTEL_MKL_DIR'):
+        cmake_defines(cmake_args, INTEL_MKL_DIR=os.getenv('INTEL_MKL_DIR'))
+
+    if os.getenv('INTEL_MKL_SEQUENTIAL'):
+        cmake_defines(cmake_args, INTEL_MKL_SEQUENTIAL=check_env_flag('INTEL_MKL_SEQUENTIAL'))
+
+    if os.getenv('INTEL_MKL_TBB'):
+        cmake_defines(cmake_args, INTEL_MKL_TBB=check_env_flag('INTEL_MKL_TBB'))
+
     mkldnn_threading = os.getenv('MKLDNN_THREADING')
     if mkldnn_threading:
         cmake_defines(cmake_args, MKLDNN_THREADING=mkldnn_threading)",module: build triaged,[],"[' cmake\r\n 32 # Intel Compiler Suite\r\n 33 SET(INTEL_COMPILER_DIR ""/opt/intel"" CACHE STRING\r\n 34   ""Root directory of the Intel Compiler Suite (contains ipp, mkl, etc.)"")\r\n 35 SET(INTEL_MKL_DIR ""/opt/intel/mkl"" CACHE STRING\r\n 36   ""Root directory of the Intel MKL (standalone)"")\r\n 37 SET(INTEL_MKL_SEQUENTIAL OFF CACHE BOOL\r\n 38   ""Force using the sequential (non threaded) libraries"")\r\n 39 SET(INTEL_MKL_TBB OFF CACHE BOOL\r\n 40   ""Force using TBB library"")\r\n', ' cmake\r\n--- a/cmake/Modules/FindMKL.cmake\r\n+++ b/cmake/Modules/FindMKL.cmake\r\n@@ -30,13 +30,13 @@ INCLUDE(CheckTypeSize)\r\n INCLUDE(CheckFunctionExists)\r\n\r\n # Intel Compiler Suite\r\n-SET(INTEL_COMPILER_DIR ""/opt/intel"" CACHE STRING\r\n+SET(INTEL_COMPILER_DIR ${INTEL_COMPILER_DIR} CACHE STRING\r\n   ""Root directory of the Intel Compiler Suite (contains ipp, mkl, etc.)"")\r\n-SET(INTEL_MKL_DIR ""/opt/intel/mkl"" CACHE STRING\r\n+SET(INTEL_MKL_DIR ${INTEL_MKL_DIR} CACHE STRING\r\n   ""Root directory of the Intel MKL (standalone)"")\r\n-SET(INTEL_MKL_SEQUENTIAL OFF CACHE BOOL\r\n+SET(INTEL_MKL_SEQUENTIAL ${INTEL_MKL_SEQUENTIAL} CACHE BOOL\r\n   ""Force using the sequential (non threaded) libraries"")\r\n-SET(INTEL_MKL_TBB OFF CACHE BOOL\r\n+SET(INTEL_MKL_TBB ${INTEL_MKL_TBB} CACHE BOOL\r\n   ""Force using TBB library"")\r\n']","['MKL', 'pytorch', 'MKL', 'cmake/Modules/FindMKL.cmake', 'MKL', 'pytorch', 'setup.py', 'MKL', 'pytorch', 'MKL', 'MKL', 'pytorch', 'MKL', 'cmake/Modules/FindMKL.cmake', 'tools/build_pytorch_libs.py', '']",0,0
290,pytorch,16316,closed,Please remove pytorch from pypi,"## üêõ Bug

I tried to install pytorch using pipenv (see https://github.com/pypa/pipenv/issues/3476).

You have put in a fancy thing which is supposed to open the browser. It did not. Instead there was no output and I believed that the installation was broken.

## To Reproduce

Steps to reproduce the behavior:

1. pipenv install pytorch

You will see something like this:



## Expected behavior

If you don't want people installing it from pypi then don't put it there.
Don't break dependency management tools.

## Environment

Collecting environment information...
PyTorch version: N/A  
Is debug build: N/A                    
CUDA used to build PyTorch: N/A                
                                        
OS: Ubuntu 18.04 LTS            
GCC version: (Ubuntu 7.3.0-16ubuntu3) 7.3.0
CMake version: version 3.5.1   
                       
Python version: 3.7      
Is CUDA available: N/A                  
CUDA runtime version: Could not collect
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] Could not collect
[conda] Could not collect

## Additional context

I can understand your desire to be helpful. Trying to make a non interactive activity (dependency management) into an interactive activity is inevitably going to result in situations like this.

Please just make the installation fail promptly and noisily.",,"['pipenv install torch\r\n\r\nthats the correct command.\r\nthe package pytorch was put as a placeholder because several people confused as to why pip install pytorch was failing. we‚Äôll try to improve the message.', 'Ah right thank you.', ""Closing due to age and apparent resolution. If you're still experiencing an issue please file a new issue, @bw-matthew ""]",['\r\n‚ûú pipenv install pytorch\r\nInstalling pytorch‚Ä¶\r\n‚†á Installing...\r\n'],[],0,0
291,pytorch,31596,open,pytorch forward hangs in multiprocess environment,"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->
This a very peculiar bug I am encountering. I am using MTCNN face detector from https://github.com/TreB1eN/InsightFace_Pytorch/  on pytorch 1.3.1+cpu

The face detection works fine when API is called from __main__ thread. But if I forked a new process and initialized MTCNN as well as face detector model (MobileFaceNet), it hangs in first net forward call (in MTCNN). If load_state_dict for MobileFaceNet is commented out, again it works fine.

## To Reproduce

Steps to reproduce the behavior:

1. Clone https://github.com/TreB1eN/InsightFace_Pytorch/
2. Try following code

3. If test function is called in __main__ thread, this code works fine.
4. If  line is removed, again code works fine

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Environment
pkg | version
--- | ---
Python | 3.7.4
torch |                            1.3.1+cpu   (pip)
Pillow |                           5.4.1      
opencv-python |                    4.1.1.26   
OS | ubuntu 18.04, linux 4.15.0-72-generic
",module: multiprocessing triaged,[],"[""\r\nfrom mtcnn import MTCNN\r\nfrom model import MobileFaceNet\r\nimport torch\r\nfrom PIL import Image\r\nfrom torch.multiprocessing import Process\r\nimport cv2\r\nimport time\r\n\r\n# mtcnn = MTCNN()\r\nmyfacenet = MobileFaceNet(512)\r\nfacenet_dict = torch.load('./model_mobilefacenet.pth', map_location = torch.device('cpu'))\r\nout = myfacenet.load_state_dict(facenet_dict)\r\n\r\ndef test():\r\n    mtcnn = MTCNN()\r\n    img = cv2.imread('29.png')\r\n    img2 = Image.fromarray(img)\r\n    print(mtcnn.align_multi(img2, 1, 100))\r\n\r\nif __name__ == '__main__':\r\n    p = Process(target=test)\r\n    p.start()\r\n    p.join()\r\n""]",['out = myfacenet.load_state_dict(facenet_dict)'],0,0
292,pytorch,13304,open,ASSERT FAILED at /opt/conda/conda-bld/pytorch-nightly_1539602533843/work/aten/src/ATen/core/blob.h:79,"## üêõ Bug
Hello Great programmers:
        When I was using FAIR's platform Detectron to do training with *e2e_mask_rcnn_R-101-FPN_3x_gn.yaml* config file, I faced this issue which indicated me to report one BUG to Pytorch.

<!-- A clear and concise description of what the bug is. -->
1generalized_rcnn
## Additional context

<!-- Add any other context about the problem here. -->
By the way, *e2e_mask_rcnn_R-50-FPN_1x.yaml* config works fine for me.

Waiting your response, thank you .",caffe2,"['Have the same problem. Have you managed how to fix it?', '@gf19880710, Same error is generated with ""e2e_mask_rcnn_R-50-FPN_1x.yaml"" also. ', 'The problem is already trained model available in OUTPUT_DIR which create conflict with test_model somehow. after moving already trained model it is working fine.', ""> \r\n> \r\n> The problem is already trained model available in OUTPUT_DIR which create conflict with test_model somehow. after moving already trained model it is working fine.\r\n\r\nHey, I'm experiencing the same error while running at inference time, so no test model that can conflict. Any clue what could be going wrong?"", '@tleers , Just check have you trained same model with any other configuration? if so, mode files from output directory to other directory.', 'I am experiencing the same issue during inference time, have not trained the same model with any other configuration, and there is nothing in OUTPUT_DIR (made a new one). Does anyone know how to solve this?', 'Hi @arjun-kava, @qvks, @tleers , \r\nI am getting a similar kind of error on FAIR\'s detectron.  \r\n\r\n```\r\nASSERT FAILED at /pytorch/aten/src/ATen/core/blob.h:77, please report a bug to PyTorch. wrong type for the Blob instance. Blob contains nullptr (uninitialized) while caller expects caffe2::Tensor.\r\nOffending Blob name: gpu_0/conv1_w.\r\nError from operator: \r\ninput: ""gpu_0/data"" input: ""gpu_0/conv1_w"" output: ""gpu_0/conv1"" name: """" type: ""Conv"" arg { name: ""kernel"" i: 7 } arg { name: ""order"" s: ""NCHW"" } arg { name: ""pad"" i: 3 } arg { name: ""stride"" i: 2 } arg { name: ""exhaustive_search"" i: 0 } device_option { device_type: 1 device_id: 0 } engine: ""CUDNN"" (Get at /pytorch/aten/src/ATen/core/blob.h:77)\r\n**** And then similar frame# XYZ traceback*****\r\n```\r\nI am using e2e_faster_rcnn_R-50-FPN_1x.yaml. I have trained FASTER-RCNN with FPN on a custom dataset with 12 classes. Also, there is no other trained model in OUTPUT_DIR.\r\n\r\n I am using google colab, so I have CUDA 10.0 with CUDNN 7.501 environment available. \r\nCould someone fix this issue? Is there some issue with the layer name or is it a bug of pytorch? (The error trace says ""please report a bug to PyTorch"")', ""Hi. Not sure this might help but basically I'm getting this same error, Detectron inference with pretrained model from Model Zoo, and --cfg configs/12_2017_baselines/e2e_keypoint_rcnn_R-50-FPN_1x.yaml\r\n\r\n`[E net_async_base.cc:382] IsType<T>() INTERNAL ASSERT FAILED at /opt/conda/conda-bld/pytorch-nightly_1562648889042/work/aten/src/ATen/core/blob.h:77, please report a bug to PyTorch. wrong type for the Blob instance. Blob contains nullptr (uninitialized) while caller expects caffe2::Tensor.\r\nOffending Blob name: gpu_0/conv_rpn_fpn2_w.`\r\n\r\nI'm using Python 2.7.14 Anaconda, Ubuntu 18.04, and using this same machine for the other ML training project, so CUDA and CuDNN should be working properly.""]","['\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.\r\n1.\r\n1.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\nno exception or bug for training and testing.\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n', '\r\ngengfeng@ai-work-4:~/Downloads$ python collect_env.py \r\nCollecting environment information...\r\nPyTorch version: 1.0.0.dev20181015\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 5.5.0-12ubuntu1) 5.5.0 20171010\r\nCMake version: version 3.11.4\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: GPU 0: GeForce GTX 1080\r\nNvidia driver version: 390.87\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-9.0/lib64/libcudnn.so\r\n/usr/local/cuda-9.0/lib64/libcudnn.so.7\r\n/usr/local/cuda-9.0/lib64/libcudnn.so.7.2.1\r\n/usr/local/cuda-9.0/lib64/libcudnn_static.a\r\n/usr/local/cuda-9.1/lib64/libcudnn.so\r\n/usr/local/cuda-9.1/lib64/libcudnn.so.7.1.3\r\n/usr/local/cuda-9.1/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] cuda91                    1.0                  h4c16780_0    pytorch\r\n[conda] pytorch-nightly           1.0.0.dev20181015 py3.6_cuda9.0.176_cudnn7.1.2_0    pytorch\r\n[conda] torch                     0.4.0                     <pip>\r\n[conda] torchvision               0.2.1                     <pip>\r\n']","['', '\r\n[I net_dag_utils.cc:102] Operator graph pruning prior to chain compute took: 6.1526e-05 secs\r\n[I net_dag_utils.cc:102] Operator graph pruning prior to chain compute took: 5.3939e-05 secs\r\n[I net_dag_utils.cc:102] Operator graph pruning prior to chain compute took: 7.264e-06 secs\r\n[I net_async_base.h:198] Using specified CPU pool size: 4; NUMA node id: -1\r\n[I net_async_base.h:203] Created new CPU pool, size: 4; NUMA node id: -1\r\n[E net_async_base.cc:422] IsType<T>() ASSERT FAILED at /opt/conda/conda-bld/pytorch-nightly_1539602533843/work/aten/src/ATen/core/blob.h:79, please report a bug to PyTorch. wrong type for the Blob instance. Blob contains nullptr (uninitialized) while caller expects caffe2::Tensor.\r\nOffending Blob name: gpu_0/conv1_gn_s.\r\nError from operator: \r\ninput: ""gpu_0/conv1"" input: ""gpu_0/conv1_gn_s"" input: ""gpu_0/conv1_gn_b"" output: ""gpu_0/conv1_gn"" output: ""gpu_0/conv1_gn_mean"" output: ""gpu_0/conv1_gn_std"" name: """" type: ""GroupNorm"" arg { name: ""use_cudnn"" i: 1 } arg { name: ""cudnn_exhaustive_search"" i: 0 } arg { name: ""group"" i: 32 } arg { name: ""epsilon"" f: 1e-05 } device_option { device_type: 1 device_id: 0 } (Get at /opt/conda/conda-bld/pytorch-nightly_1539602533843/work/aten/src/ATen/core/blob.h:79)\r\nframe #0: <unknown function> + 0x277d775 (0x7fa4c1984775 in /home/gengfeng/anaconda3/lib/python3.6/site-packages/caffe2/python/../../torch/lib/libcaffe2_gpu.so)\r\nframe #1: <unknown function> + 0x1321685 (0x7fa4c0528685 in /home/gengfeng/anaconda3/lib/python3.6/site-packages/caffe2/python/../../torch/lib/libcaffe2_gpu.so)\r\nframe #2: caffe2::AsyncNetBase::run(int, int) + 0x16e (0x7fa4ec15f4ee in /home/gengfeng/anaconda3/lib/python3.6/site-packages/caffe2/python/../../torch/lib/libcaffe2.so)\r\nframe #3: <unknown function> + 0x1259972 (0x7fa4ec16e972 in /home/gengfeng/anaconda3/lib/python3.6/site-packages/caffe2/python/../../torch/lib/libcaffe2.so)\r\nframe #4: <unknown function> + 0x124d1cb (0x7fa4ec1621cb in /home/gengfeng/anaconda3/lib/python3.6/site-packages/caffe2/python/../../torch/lib/libcaffe2.so)\r\nframe #5: <unknown function> + 0xafc5c (0x7fa4f6227c5c in /home/gengfeng/anaconda3/bin/../lib/libstdc++.so.6)\r\nframe #6: <unknown function> + 0x76db (0x7fa4fd2806db in /lib/x86_64-linux-gnu/libpthread.so.0)\r\nframe #7: clone + 0x3f (0x7fa4fcfa988f in /lib/x86_64-linux-gnu/libc.so.6)\r\n,  op GroupNorm\r\nWARNING workspace.py: 187: Original python traceback for operator ', ' in network ', ' in exception above (most recent call last):\r\nWARNING workspace.py: 192:   File ""tools/train_net.py"", line 132, in <module>\r\nWARNING workspace.py: 192:   File ""tools/train_net.py"", line 117, in main\r\nWARNING workspace.py: 192:   File ""tools/train_net.py"", line 127, in test_model\r\nWARNING workspace.py: 192:   File ""/home/gengfeng/Desktop/projects/DETECTRON/detectron/core/test_engine.py"", line 128, in run_inference\r\nWARNING workspace.py: 192:   File ""/home/gengfeng/Desktop/projects/DETECTRON/detectron/core/test_engine.py"", line 108, in result_getter\r\nWARNING workspace.py: 192:   File ""/home/gengfeng/Desktop/projects/DETECTRON/detectron/core/test_engine.py"", line 159, in test_net_on_dataset\r\nWARNING workspace.py: 192:   File ""/home/gengfeng/Desktop/projects/DETECTRON/detectron/core/test_engine.py"", line 235, in test_net\r\nWARNING workspace.py: 192:   File ""/home/gengfeng/Desktop/projects/DETECTRON/detectron/core/test_engine.py"", line 328, in initialize_model_from_cfg\r\nWARNING workspace.py: 192:   File ""/home/gengfeng/Desktop/projects/DETECTRON/detectron/modeling/model_builder.py"", line 124, in create\r\nWARNING workspace.py: 192:   File ""/home/gengfeng/Desktop/projects/DETECTRON/detectron/modeling/model_builder.py"", line 89, in generalized_rcnn\r\nWARNING workspace.py: 192:   File ""/home/gengfeng/Desktop/projects/DETECTRON/detectron/modeling/model_builder.py"", line 229, in build_generic_detection_model\r\nWARNING workspace.py: 192:   File ""/home/gengfeng/Desktop/projects/DETECTRON/detectron/modeling/optimizer.py"", line 54, in build_data_parallel_model\r\nWARNING workspace.py: 192:   File ""/home/gengfeng/Desktop/projects/DETECTRON/detectron/modeling/model_builder.py"", line 169, in _single_gpu_build_func\r\nWARNING workspace.py: 192:   File ""/home/gengfeng/Desktop/projects/DETECTRON/detectron/modeling/FPN.py"", line 63, in add_fpn_ResNet101_conv5_body\r\nWARNING workspace.py: 192:   File ""/home/gengfeng/Desktop/projects/DETECTRON/detectron/modeling/FPN.py"", line 104, in add_fpn_onto_conv_body\r\nWARNING workspace.py: 192:   File ""/home/gengfeng/Desktop/projects/DETECTRON/detectron/modeling/ResNet.py"", line 48, in add_ResNet101_conv5_body\r\nWARNING workspace.py: 192:   File ""/home/gengfeng/Desktop/projects/DETECTRON/detectron/modeling/ResNet.py"", line 99, in add_ResNet_convX_body\r\nWARNING workspace.py: 192:   File ""/home/gengfeng/Desktop/projects/DETECTRON/detectron/modeling/ResNet.py"", line 264, in basic_gn_stem\r\nWARNING workspace.py: 192:   File ""/home/gengfeng/Desktop/projects/DETECTRON/detectron/modeling/detector.py"", line 450, in ConvGN\r\nWARNING workspace.py: 192:   File ""/home/gengfeng/anaconda3/lib/python3.6/site-packages/caffe2/python/cnn.py"", line 165, in SpatialGN\r\nWARNING workspace.py: 192:   File ""/home/gengfeng/anaconda3/lib/python3.6/site-packages/caffe2/python/brew.py"", line 107, in scope_wrapper\r\nWARNING workspace.py: 192:   File ""/home/gengfeng/anaconda3/lib/python3.6/site-packages/caffe2/python/helpers/normalization.py"", line 206, in spatial_gn\r\nTraceback (most recent call last):\r\n  File ""tools/train_net.py"", line 132, in <module>\r\n    main()\r\n  File ""tools/train_net.py"", line 117, in main\r\n    test_model(checkpoints[\'final\'], args.multi_gpu_testing, args.opts)\r\n  File ""tools/train_net.py"", line 127, in test_model\r\n    check_expected_results=True,\r\n  File ""/home/gengfeng/Desktop/projects/DETECTRON/detectron/core/test_engine.py"", line 128, in run_inference\r\n    all_results = result_getter()\r\n  File ""/home/gengfeng/Desktop/projects/DETECTRON/detectron/core/test_engine.py"", line 108, in result_getter\r\n    multi_gpu=multi_gpu_testing\r\n  File ""/home/gengfeng/Desktop/projects/DETECTRON/detectron/core/test_engine.py"", line 159, in test_net_on_dataset\r\n    weights_file, dataset_name, proposal_file, output_dir, gpu_id=gpu_id\r\n  File ""/home/gengfeng/Desktop/projects/DETECTRON/detectron/core/test_engine.py"", line 258, in test_net\r\n    model, im, box_proposals, timers\r\n  File ""/home/gengfeng/Desktop/projects/DETECTRON/detectron/core/test.py"", line 66, in im_detect_all\r\n    model, im, cfg.TEST.SCALE, cfg.TEST.MAX_SIZE, boxes=box_proposals\r\n  File ""/home/gengfeng/Desktop/projects/DETECTRON/detectron/core/test.py"", line 158, in im_detect_bbox\r\n    workspace.RunNet(model.net.Proto().name)\r\n  File ""/home/gengfeng/anaconda3/lib/python3.6/site-packages/caffe2/python/workspace.py"", line 219, in RunNet\r\n    StringifyNetName(name), num_iter, allow_fail,\r\n  File ""/home/gengfeng/anaconda3/lib/python3.6/site-packages/caffe2/python/workspace.py"", line 180, in CallWithExceptionIntercept\r\n    return func(*args, **kwargs)\r\nRuntimeError: IsType<T>() ASSERT FAILED at /opt/conda/conda-bld/pytorch-nightly_1539602533843/work/aten/src/ATen/core/blob.h:79, please report a bug to PyTorch. wrong type for the Blob instance. Blob contains nullptr (uninitialized) while caller expects caffe2::Tensor.\r\nOffending Blob name: gpu_0/conv1_gn_s.\r\nError from operator: \r\ninput: ""gpu_0/conv1"" input: ""gpu_0/conv1_gn_s"" input: ""gpu_0/conv1_gn_b"" output: ""gpu_0/conv1_gn"" output: ""gpu_0/conv1_gn_mean"" output: ""gpu_0/conv1_gn_std"" name: """" type: ""GroupNorm"" arg { name: ""use_cudnn"" i: 1 } arg { name: ""cudnn_exhaustive_search"" i: 0 } arg { name: ""group"" i: 32 } arg { name: ""epsilon"" f: 1e-05 } device_option { device_type: 1 device_id: 0 } (Get at /opt/conda/conda-bld/pytorch-nightly_1539602533843/work/aten/src/ATen/core/blob.h:79)\r\nframe #0: <unknown function> + 0x277d775 (0x7fa4c1984775 in /home/gengfeng/anaconda3/lib/python3.6/site-packages/caffe2/python/../../torch/lib/libcaffe2_gpu.so)\r\nframe #1: <unknown function> + 0x1321685 (0x7fa4c0528685 in /home/gengfeng/anaconda3/lib/python3.6/site-packages/caffe2/python/../../torch/lib/libcaffe2_gpu.so)\r\nframe #2: caffe2::AsyncNetBase::run(int, int) + 0x16e (0x7fa4ec15f4ee in /home/gengfeng/anaconda3/lib/python3.6/site-packages/caffe2/python/../../torch/lib/libcaffe2.so)\r\nframe #3: <unknown function> + 0x1259972 (0x7fa4ec16e972 in /home/gengfeng/anaconda3/lib/python3.6/site-packages/caffe2/python/../../torch/lib/libcaffe2.so)\r\nframe #4: <unknown function> + 0x124d1cb (0x7fa4ec1621cb in /home/gengfeng/anaconda3/lib/python3.6/site-packages/caffe2/python/../../torch/lib/libcaffe2.so)\r\nframe #5: <unknown function> + 0xafc5c (0x7fa4f6227c5c in /home/gengfeng/anaconda3/bin/../lib/libstdc++.so.6)\r\nframe #6: <unknown function> + 0x76db (0x7fa4fd2806db in /lib/x86_64-linux-gnu/libpthread.so.0)\r\nframe #7: clone + 0x3f (0x7fa4fcfa988f in /lib/x86_64-linux-gnu/libc.so.6)\r\n\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n\r\n\r\nMy own configs *e2e_mask_rcnn_R-101-FPN_3x_gn.yaml*\r\nMODEL:\r\n  TYPE: generalized_rcnn\r\n  CONV_BODY: FPN.add_fpn_ResNet101_conv5_body\r\n  NUM_CLASSES: 2\r\n  FASTER_RCNN: True\r\n  MASK_ON: True\r\nNUM_GPUS: 1\r\nSOLVER:\r\n  WEIGHT_DECAY: 0.0001\r\n  LR_POLICY: steps_with_decay\r\n  BASE_LR: 0.002\r\n  GAMMA: 0.1\r\n  MAX_ITER: 70000\r\n  STEPS: [0, 40000, 60000]\r\nFPN:\r\n  FPN_ON: True\r\n  MULTILEVEL_ROIS: True\r\n  MULTILEVEL_RPN: True\r\n  USE_GN: True  # Note: use GN on the FPN-specific layers\r\nRESNETS:\r\n  STRIDE_1X1: False  # default True for MSRA; False for C2 or Torch models\r\n  TRANS_FUNC: bottleneck_gn_transformation  # Note: this is a GN bottleneck transform\r\n  STEM_FUNC: basic_gn_stem  # Note: this is a GN stem\r\n  SHORTCUT_FUNC: basic_gn_shortcut  # Note: this is a GN shortcut\r\nFAST_RCNN:\r\n  ROI_BOX_HEAD: fast_rcnn_heads.add_roi_Xconv1fc_gn_head  # Note: this is a Conv GN head\r\n  ROI_XFORM_METHOD: RoIAlign\r\n  ROI_XFORM_RESOLUTION: 7\r\n  ROI_XFORM_SAMPLING_RATIO: 2\r\nMRCNN:\r\n  ROI_MASK_HEAD: mask_rcnn_heads.mask_rcnn_fcn_head_v1up4convs_gn  # Note: this is a GN mask head\r\n  RESOLUTION: 28  # (output mask resolution) default 14\r\n  ROI_XFORM_METHOD: RoIAlign\r\n  ROI_XFORM_RESOLUTION: 14  # default 7\r\n  ROI_XFORM_SAMPLING_RATIO: 2  # default 0\r\n  DILATION: 1  # default 2\r\n  CONV_INIT: MSRAFill  # default GaussianFill\r\nTRAIN:\r\n  WEIGHTS: https://s3-us-west-2.amazonaws.com/detectron/ImageNetPretrained/47592356/R-101-GN.pkl  # Note: a GN pre-trained model\r\n  DATASETS: (\'coco_labelme_train\',)\r\n  SCALES: (700,)\r\n  MAX_SIZE: 1333\r\n  BATCH_SIZE_PER_IM: 64\r\n  RPN_PRE_NMS_TOP_N: 2000  # Per FPN level\r\nTEST:\r\n  DATASETS: (\'coco_labelme_val\',)\r\n  SCALE: 700\r\n  MAX_SIZE: 1333\r\n  NMS: 0.5\r\n  RPN_PRE_NMS_TOP_N: 1000  # Per FPN level\r\n  RPN_POST_NMS_TOP_N: 1000\r\nOUTPUT_DIR: .\r\n', '']",0,0
293,pytorch,6998,open,[pytorch] Not handling python reload properly,"Trying to reload some modules cause crashes:

On python 2.7 (crash with ubuntu release, error messages from debug build).

type->tp_flags & (1L<<9)' failed.
Aborted (core dumped)

>>> from torch import autograd
>>> reload(autograd)
terminate called after throwing an instance of 'std::runtime_error'
  what():  generic_type: cannot initialize type ""ProfilerEvent"": an object with that name is already defined
Aborted (core dumped)
`

@colesbury can you see an obvious reason for this? Or should I look into more details?",module: crash todo triaged,"[""We use static C++ state in a bunch of places, and restarting the interpreter doesn't cause a full destruct -> construct cycle. That's probably what's causing the segfault."", 'Hi,\r\n\r\nThere is no workaround at the moment unfortunately.\r\nCould you explain what is your usecase? and why you need to reload pytorch?', ""I deleted my comment because it seemed that my error was caused due to some error on my side.\r\nI'm not entirely sure what caused it, but I believe that due to bad refractoring some import statements included import of the __init__ file.\r\nFor example, if i remember correctly, there were these import statements:\r\n```from torch.__init__ import <something>```\r\n```from torch.autograd import __init__```\r\n\r\nOnce i deleted those the issue disappeared :) \r\nSo I thought it was better if i'll delete my comment here.\r\n "", 'This is still an error with python 3, but now the exception is different:\r\n\r\n```\r\nC:\\Users\\almenon>python\r\nPython 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:54:40) [MSC v.1900 64 bit (AMD64)] on win32\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import torch\r\n>>> from importlib import reload\r\n>>> reload(torch)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""C:\\Users\\caleb\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py"", line 166, in reload\r\n    _bootstrap._exec(spec, module)\r\n  File ""<frozen importlib._bootstrap>"", line 618, in _exec\r\n  File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module\r\n  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed\r\n  File ""C:\\Users\\caleb\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\torch\\__init__.py"", line 67, in <module>\r\n    if all([not os.path.exists(os.path.join(p, \'nvToolsExt64_1.dll\')) for p in dll_paths]):\r\nTypeError: all(): argument \'input\' (position 1) must be Tensor, not list\r\n```\r\n\r\nWindows 10, python 3.6.4. Packages are below:\r\n```\r\ntorch==1.8.1+cu102\r\ntorchvision==0.9.1+cu102\r\ntorchaudio===0.8.1\r\n-f https://download.pytorch.org/whl/torch_stable.html\r\nnumpy==1.19.5\r\n```']",['\r\n\r\n'],"['', '\r\n>>> import torch\r\n>>> reload(torch)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/alban/workspace/dev-pytorch/torch/__init__.py"", line 288, in <module>\r\n    from . import _torch_docs, _tensor_docs, _storage_docs\r\nImportError: cannot import name _torch_docs\r\n>>> exit()\r\npython: ../Objects/typeobject.c:2739: type_dealloc: Assertion ', '']",0,0
294,pytorch,3990,open,Raise an error when using magma built against wrong version of cuda,"Previous instances of this: https://github.com/pytorch/pytorch/issues/3018

I myself was affected by this bug when a server I was working on upgraded from cuda 8 to cuda 9. I dutifully rebuilt PyTorch but forgot to uninstall magma-cuda80: instant hang on CUDA initialization.",module: binaries module: build triaged,"['+1, I also ran into this. Was very confusing.\r\n\r\nWorth mentioning this [comment](https://github.com/pytorch/pytorch/issues/3018#issuecomment-336634336) from Adam here too, for those wondering about performance:\r\n\r\n""Magma is not used to boost the performance. It\'s only that without it some linear algebra functions (linear system solvers, Cholesky decomposition, and similar) will not work. You won\'t find any difference unless you use those."" ', 'If anyone is interested, I built a cuda 10 magma conda package.\r\n\r\nYou can find it here: https://anaconda.org/cpbotha/magma-cuda10\r\n\r\nMore details here: https://vxlabs.com/2018/11/04/pytorch-1-0-preview-nov-4-2018-packages-with-full-cuda-10-support-for-your-ubuntu-18-04-x86_64-systems/', 'This struck @fwillo in #15433 ', 'as far as I can tell, we cannot actually detect this purely from binary bits (unless we patch Magma itself to encode what CUDA it was built against.']",[],[],0,0
295,pytorch,12646,open,Caffe2 Installation inside Pytorch,"i am getting this error message (please see the attached) after run the command ""python setup.py install"":
![pytorch_question](https://user-images.githubusercontent.com/8035201/46946663-b2551580-d0a2-11e8-9f9e-afd34fd2454d.png). 
I followed this office guide of pytorch: https://caffe2.ai/docs/getting-started.html?platform=ubuntu&configuration=compile.
But the weird thing is I can print the coffee2 sucessfully
![coffe2](https://user-images.githubusercontent.com/8035201/46946767-03fda000-d0a3-11e8-8de5-4ccbba7c3d36.png)

i would like to ask if does anybody knows how to fix this?

",caffe2,"[""Maybe you have already installed caffe2 from anaconda's pre-built binaries like so https://caffe2.ai/docs/getting-started.html?platform=ubuntu&configuration=prebuilt\r\n\r\nPlease provide the full error message instead of a printscreen."", 'Hi @mhubii, thank for your reply. Yes I have already installed caffe2 from anaconda\'s pre-built binaries. But I wasn\'t aware of it. Now how can I fix the issue?\r\n\r\nhere is my full error message:\r\n\r\nBuilding wheel torch-1.0.0a0+6dd7194\r\nrunning install\r\nsetup.py::run()\r\nrunning build_deps\r\nsetup.py::build_deps::run()\r\n+ SYNC_COMMAND=cp\r\n++ command -v rsync\r\n+ \'[\' -x /usr/bin/rsync \']\'\r\n+ SYNC_COMMAND=\'rsync -lptgoD\'\r\n+ USE_CUDA=0\r\n+ USE_ROCM=0\r\n+ USE_NNPACK=0\r\n+ USE_MKLDNN=0\r\n+ USE_GLOO_IBVERBS=0\r\n+ CAFFE2_STATIC_LINK_CUDA=0\r\n+ RERUN_CMAKE=1\r\n+ [[ 8 -gt 0 ]]\r\n+ case ""$1"" in\r\n+ USE_CUDA=1\r\n+ shift\r\n+ [[ 7 -gt 0 ]]\r\n+ case ""$1"" in\r\n+ USE_NNPACK=1\r\n+ shift\r\n+ [[ 6 -gt 0 ]]\r\n+ case ""$1"" in\r\n+ break\r\n+ CMAKE_INSTALL=\'make install\'\r\n+ BUILD_SHARED_LIBS=ON\r\n+ USER_CFLAGS=\r\n+ USER_LDFLAGS=\r\n+ [[ -n \'\' ]]\r\n+ [[ -n \'\' ]]\r\n+ [[ -n \'\' ]]\r\n++ uname\r\n+ \'[\' Linux == Darwin \']\'\r\n+++ dirname ../tools/build_pytorch_libs.sh\r\n++ cd ../tools/..\r\n+++ pwd\r\n++ printf \'%q\\n\' /home/bapvn/work/caffe2-pytorch\r\n+ BASE_DIR=/home/bapvn/work/caffe2-pytorch\r\n+ TORCH_LIB_DIR=/home/bapvn/work/caffe2-pytorch/torch/lib\r\n+ INSTALL_DIR=/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install\r\n+ THIRD_PARTY_DIR=/home/bapvn/work/caffe2-pytorch/third_party\r\n+ CMAKE_VERSION=cmake\r\n+ C_FLAGS=\' -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include""   -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/TH"" -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/THC""   -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/THS"" -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/THCS""   -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/THNN"" -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/THCUNN""\'\r\n+ C_FLAGS=\' -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include""   -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/TH"" -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/THC""   -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/THS"" -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/THCS""   -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/THNN"" -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/THCUNN"" -DOMPI_SKIP_MPICXX=1\'\r\n+ LDFLAGS=\'-L""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/lib"" \'\r\n+ LD_POSTFIX=.so\r\n++ uname\r\n+ [[ Linux == \\D\\a\\r\\w\\i\\n ]]\r\n+ [[ 0 -eq 1 ]]\r\n+ LDFLAGS=\'-L""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/lib""  -Wl,-rpath,$ORIGIN\'\r\n+ CPP_FLAGS=\' -std=c++11 \'\r\n+ GLOO_FLAGS=\'-DBUILD_TEST=OFF \'\r\n+ THD_FLAGS=\r\n+ NCCL_ROOT_DIR=/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install\r\n+ [[ 1 -eq 1 ]]\r\n+ GLOO_FLAGS+=\'-DUSE_CUDA=1 -DNCCL_ROOT_DIR=/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install\'\r\n+ [[ 0 -eq 1 ]]\r\n+ CWRAP_FILES=\'/home/bapvn/work/caffe2-pytorch/torch/lib/ATen/Declarations.cwrap;/home/bapvn/work/caffe2-pytorch/torch/lib/THNN/generic/THNN.h;/home/bapvn/work/caffe2-pytorch/torch/lib/THCUNN/generic/THCUNN.h;/home/bapvn/work/caffe2-pytorch/torch/lib/ATen/nn.yaml\'\r\n+ CUDA_NVCC_FLAGS=\' -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include""   -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/TH"" -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/THC""   -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/THS"" -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/THCS""   -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/THNN"" -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/THCUNN"" -DOMPI_SKIP_MPICXX=1\'\r\n+ [[ -z \'\' ]]\r\n+ CUDA_DEVICE_DEBUG=0\r\n+ \'[\' -z \'\' \']\'\r\n++ getconf _NPROCESSORS_ONLN\r\n+ MAX_JOBS=6\r\n+ BUILD_TYPE=Release\r\n+ [[ -n \'\' ]]\r\n+ [[ -n \'\' ]]\r\n+ echo \'Building in Release mode\'\r\nBuilding in Release mode\r\n+ mkdir -p /home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install\r\n+ for arg in \'""$@""\'\r\n+ [[ nccl == \\n\\c\\c\\l ]]\r\n+ pushd /home/bapvn/work/caffe2-pytorch/third_party\r\n~/work/caffe2-pytorch/third_party ~/work/caffe2-pytorch/build\r\n+ build_nccl\r\n+ mkdir -p build/nccl\r\n+ pushd build/nccl\r\n~/work/caffe2-pytorch/third_party/build/nccl ~/work/caffe2-pytorch/third_party ~/work/caffe2-pytorch/build\r\n+ [[ 1 -eq 1 ]]\r\n+ cmake ../../nccl -DCMAKE_MODULE_PATH=/home/bapvn/work/caffe2-pytorch/cmake/Modules_CUDA_fix -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install \'-DCMAKE_C_FLAGS= -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include""   -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/TH"" -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/THC""   -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/THS"" -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/THCS""   -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/THNN"" -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/THCUNN"" -DOMPI_SKIP_MPICXX=1 \' \'-DCMAKE_CXX_FLAGS= -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include""   -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/TH"" -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/THC""   -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/THS"" -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/THCS""   -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/THNN"" -I""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/THCUNN"" -DOMPI_SKIP_MPICXX=1  -std=c++11  \' -DCMAKE_SHARED_LINKER_FLAGS= -DCMAKE_UTILS_PATH=/home/bapvn/work/caffe2-pytorch/cmake/public/utils.cmake -DNUM_JOBS=6\r\n-- Autodetected CUDA architecture(s): 6.1\r\n-- Set NVCC_GENCODE for building NCCL: -gencode=arch=compute_61,code=sm_61\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /home/bapvn/work/caffe2-pytorch/third_party/build/nccl\r\n+ make install -j6\r\n[100%] Built target nccl\r\nInstall the project...\r\n-- Install configuration: ""Release""\r\n-- Up-to-date: /home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/include/nccl.h\r\n+ mkdir -p /home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/lib\r\n+ find lib -name \'libnccl.so*\'\r\n+ xargs -I \'{}\' rsync -lptgoD \'{}\' /home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/lib/\r\n+ popd\r\n~/work/caffe2-pytorch/third_party ~/work/caffe2-pytorch/build\r\n+ \'[\' -f ./nccl/nccl/src/nccl.h \']\'\r\n+ popd\r\n~/work/caffe2-pytorch/build\r\n+ for arg in \'""$@""\'\r\n+ [[ caffe2 == \\n\\c\\c\\l ]]\r\n+ [[ caffe2 == \\g\\l\\o\\o ]]\r\n+ [[ caffe2 == \\c\\a\\f\\f\\e\\2 ]]\r\n+ build_caffe2\r\n+ [[ -z \'\' ]]\r\n+ EXTRA_CAFFE2_CMAKE_FLAGS=()\r\n+ [[ -n \'\' ]]\r\n+ [[ -n /home/bapvn/anaconda3/lib/python3.6/site-packages ]]\r\n+ EXTRA_CAFFE2_CMAKE_FLAGS+=(""-DCMAKE_PREFIX_PATH=$CMAKE_PREFIX_PATH"")\r\n+ [[ 1 -eq 1 ]]\r\n+ cmake /home/bapvn/work/caffe2-pytorch -DPYTHON_EXECUTABLE=/home/bapvn/anaconda3/bin/python -DPYTHON_LIBRARY=/home/bapvn/anaconda3/lib/libpython3.6m.so.1.0 -DPYTHON_INCLUDE_DIR=/home/bapvn/anaconda3/include/python3.6m -DBUILDING_WITH_TORCH_LIBS=ON -DTORCH_BUILD_VERSION=1.0.0a0+6dd7194 -DCMAKE_BUILD_TYPE=Release -DBUILD_TORCH=ON -DBUILD_PYTHON=ON -DBUILD_SHARED_LIBS=ON -DBUILD_BINARY=OFF -DBUILD_TEST=ON -DINSTALL_TEST=ON -DBUILD_CAFFE2_OPS=ON -DONNX_NAMESPACE=onnx_torch -DUSE_CUDA=1 -DUSE_NUMPY= -DCAFFE2_STATIC_LINK_CUDA=0 -DUSE_ROCM=0 -DUSE_NNPACK=1 -DUSE_LEVELDB=OFF -DUSE_LMDB=OFF -DUSE_OPENCV=OFF -DUSE_FFMPEG=OFF -DUSE_GLOG=OFF -DUSE_GFLAGS=OFF -DUSE_SYSTEM_EIGEN_INSTALL=OFF -DCUDNN_INCLUDE_DIR=/usr/include/ -DCUDNN_LIB_DIR=/usr/lib/x86_64-linux-gnu/ -DCUDNN_LIBRARY=/usr/lib/x86_64-linux-gnu/libcudnn.so.7 -DUSE_MKLDNN=0 -DMKLDNN_INCLUDE_DIR= -DMKLDNN_LIB_DIR= -DMKLDNN_LIBRARY= -DCMAKE_INSTALL_PREFIX=/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install -DCMAKE_C_FLAGS= -DCMAKE_CXX_FLAGS= \'-DCMAKE_EXE_LINKER_FLAGS=-L""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/lib""  -Wl,-rpath,$ORIGIN \' \'-DCMAKE_SHARED_LINKER_FLAGS=-L""/home/bapvn/work/caffe2-pytorch/torch/lib/tmp_install/lib""  -Wl,-rpath,$ORIGIN \' -DCMAKE_PREFIX_PATH=/home/bapvn/anaconda3/lib/python3.6/site-packages\r\n-- std::exception_ptr is supported.\r\n-- NUMA is available\r\n-- Current compiler supports avx2 extention. Will build perfkernels.\r\n-- Building using own protobuf under third_party per request.\r\n-- Use custom protobuf build.\r\n-- Caffe2 protobuf include directory: $<BUILD_INTERFACE:/home/bapvn/work/caffe2-pytorch/third_party/protobuf/src>$<INSTALL_INTERFACE:include>\r\n-- The BLAS backend of choice:MKL\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel_lp64: /home/bapvn/anaconda3/lib/libmkl_intel_lp64.so\r\n--   Library mkl_gnu_thread: /home/bapvn/anaconda3/lib/libmkl_gnu_thread.so\r\n--   Library mkl_core: /home/bapvn/anaconda3/lib/libmkl_core.so\r\n--   Library gomp: -fopenmp\r\n--   Library pthread: /usr/lib/x86_64-linux-gnu/libpthread.so\r\n--   Library m: /usr/lib/x86_64-linux-gnu/libm.so\r\n--   Library dl: /usr/lib/x86_64-linux-gnu/libdl.so\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel_lp64: /home/bapvn/anaconda3/lib/libmkl_intel_lp64.so\r\n--   Library mkl_gnu_thread: /home/bapvn/anaconda3/lib/libmkl_gnu_thread.so\r\n--   Library mkl_core: /home/bapvn/anaconda3/lib/libmkl_core.so\r\n--   Library gomp: -fopenmp\r\n--   Library pthread: /usr/lib/x86_64-linux-gnu/libpthread.so\r\n--   Library m: /usr/lib/x86_64-linux-gnu/libm.so\r\n--   Library dl: /usr/lib/x86_64-linux-gnu/libdl.so\r\nCMake Warning at cmake/Dependencies.cmake:76 (message):\r\n  MKL could not be found.  Defaulting to Eigen\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:202 (include)\r\n\r\n\r\n-- Brace yourself, we are building NNPACK\r\n-- Failed to find LLVM FileCheck\r\n-- git Version: v1.4.0-505be96a\r\n-- Version: 1.4.0\r\n-- Performing Test HAVE_STD_REGEX -- success\r\n-- Performing Test HAVE_GNU_POSIX_REGEX -- failed to compile\r\n-- Performing Test HAVE_POSIX_REGEX -- success\r\n-- Performing Test HAVE_STEADY_CLOCK -- success\r\n-- Found Numa  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libnuma.so)\r\n-- Using third party subdirectory Eigen.\r\nPython 3.6.3 :: Anaconda custom (64-bit)\r\n-- Found PythonInterp: /home/bapvn/anaconda3/bin/python (found suitable version ""3.6.3"", minimum required is ""2.7"") \r\n-- NumPy ver. 1.13.3 found (include: /home/bapvn/anaconda3/lib/python3.6/site-packages/numpy/core/include)\r\n-- Found PythonInterp: /home/bapvn/anaconda3/bin/python (found version ""3.6.3"") \r\n-- System pybind11 found\r\n-- pybind11l include dirs: /home/bapvn/anaconda3/include/python3.6m/home/bapvn/anaconda3/include/python3.6m\r\n-- MPI support found\r\n-- MPI compile flags: \r\n-- MPI include path: /usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent/usr/lib/openmpi/include/openmpi/opal/mca/event/libevent2021/libevent/include/usr/lib/openmpi/include/usr/lib/openmpi/include/openmpi\r\n-- MPI LINK flags path:  -Wl,-rpath  -Wl,/usr/lib/openmpi/lib  -Wl,--enable-new-dtags\r\n-- MPI libraries: /usr/lib/openmpi/lib/libmpi_cxx.so/usr/lib/openmpi/lib/libmpi.so\r\nCMake Warning at cmake/Dependencies.cmake:455 (message):\r\n  OpenMPI found, but it is not built with CUDA support.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:202 (include)\r\n\r\n\r\n-- Caffe2: CUDA detected: 9.0\r\n-- Caffe2: CUDA nvcc is: /usr/local/cuda/bin/nvcc\r\n-- Caffe2: CUDA toolkit directory: /usr/local/cuda\r\n-- Caffe2: Header version is: 9.0\r\n-- Found cuDNN: v7.3.1  (include: /usr/include/, library: /usr/lib/x86_64-linux-gnu/libcudnn.so.7)\r\nCMake Error at cmake/public/cuda.cmake:313 (message):\r\n  CUDA 9.0 is not compatible with std::tuple from GCC version >= 6.  Please\r\n  upgrade to CUDA 9.2 or use the following option to use another version (for\r\n  example):\r\n\r\n    -DCUDA_HOST_COMPILER=/usr/bin/gcc-5\r\n\r\nCall Stack (most recent call first):\r\n  cmake/Dependencies.cmake:492 (include)\r\n  CMakeLists.txt:202 (include)\r\n\r\n\r\n-- Configuring incomplete, errors occurred!\r\nSee also ""/home/bapvn/work/caffe2-pytorch/build/CMakeFiles/CMakeOutput.log"".\r\nSee also ""/home/bapvn/work/caffe2-pytorch/build/CMakeFiles/CMakeError.log"".\r\nFailed to run \'bash ../tools/build_pytorch_libs.sh --use-cuda --use-nnpack nccl caffe2 libshm gloo c10d THD\'\r\n', '@zou3519 thank for adding the label man.', ""Hm I don't know. Maybe try what the compiler is complaining about, run:\r\n\r\n```\r\nexport CUDA_HOST_COMPILER=/usr/bin/gcc-5\r\npython setup.py install \r\n```"", '@mhubii yes, I did try it but It did not work. \r\n\r\nThank you for your help.', 'What is the output of `ls /usr/bin | grep gcc`?', '@mhubii here is it:\r\nc89-gcc\r\nc99-gcc\r\ngcc\r\ngcc-4.8\r\ngcc-4.9\r\ngcc-5\r\ngcc-6\r\ngcc-ar\r\ngcc-ar-4.8\r\ngcc-ar-4.9\r\ngcc-ar-5\r\ngcc-ar-6\r\ngcc-nm\r\ngcc-nm-4.8\r\ngcc-nm-4.9\r\ngcc-nm-5\r\ngcc-nm-6\r\ngcc-ranlib\r\ngcc-ranlib-4.8\r\ngcc-ranlib-4.9\r\ngcc-ranlib-5\r\ngcc-ranlib-6\r\nx86_64-linux-gnu-gcc\r\nx86_64-linux-gnu-gcc-4.8\r\nx86_64-linux-gnu-gcc-4.9\r\nx86_64-linux-gnu-gcc-5\r\nx86_64-linux-gnu-gcc-6\r\nx86_64-linux-gnu-gcc-ar\r\nx86_64-linux-gnu-gcc-ar-4.8\r\nx86_64-linux-gnu-gcc-ar-4.9\r\nx86_64-linux-gnu-gcc-ar-5\r\nx86_64-linux-gnu-gcc-ar-6\r\nx86_64-linux-gnu-gcc-nm\r\nx86_64-linux-gnu-gcc-nm-4.8\r\nx86_64-linux-gnu-gcc-nm-4.9\r\nx86_64-linux-gnu-gcc-nm-5\r\nx86_64-linux-gnu-gcc-nm-6\r\nx86_64-linux-gnu-gcc-ranlib\r\nx86_64-linux-gnu-gcc-ranlib-4.8\r\nx86_64-linux-gnu-gcc-ranlib-4.9\r\nx86_64-linux-gnu-gcc-ranlib-5\r\nx86_64-linux-gnu-gcc-ranlib-6\r\n\r\nThank you so much for your support!', ""Hm I don't know if I can help you there. It seems like you somehow need to compile your code with gcc-5. Maybe try to remove the `build` folder from your `pytorch` folder. Change your default compiler and rebuild pytorch.\r\n""]",[],[],0,0
296,pytorch,8837,open,Inconsistency in implementation of _LRScheduler ,"I've noticed an odd behavior when attempting to write my own scheduler based on . 

If you write a custom get_lr() to work based on self.last_epoch, its impossible to differentiate the 
0th and the 1st epoch. 

Here is a minimal working example:



This results in the output



You can see the last epoch is asked to set the learning rate based on the last epoch being 0 twice. This LRScheduler class takes last_epoch as an argument, so it knows how to set the LR for the previous epoch. By default last_epoch=-1, because the first epoch is 0 and no epoch has run yet. On construction it then calls  with , which means the step function sets the learning rate for epoch 0. Then last_epoch is reset to -1 immediately after, so the next call to step also sets the learning rate for epoch 0. 

A fix would simply remove the + 1 from , but this might break existing implementations of  which wouldn't expect  being set to a negative number. 

I think a more intuitive implementation of this class might track the current epoch rather than the previous one. This would be a backwards incompatible change, but I think it would improve the overall quality of torch. I'm willing to give a re-implementation a shot if it sounds like a good idea to the maintainers. 

cc @ezyang @albanD @zou3519 @gqchen @pearu @nikitaved @soulitzer @mruberry @jbschlosser @vincentqb",module: nn module: optimizer needs research triaged,[],"[""python\r\n\r\ndef mwe():\r\n    # Assuming optimizer has two groups.\r\n    import torch.optim.lr_scheduler\r\n    import netharn as nh\r\n    model = nh.models.ToyNet2d()\r\n    optimizer = torch.optim.SGD(model.parameters(), lr=10)\r\n\r\n    class DummySchedule(torch.optim.lr_scheduler._LRScheduler):\r\n        def get_lr(self):\r\n            print('Set LR based on self.last_epoch = {!r}'.format(self.last_epoch))\r\n            self._current_lr = self.last_epoch\r\n            return [self.last_epoch]\r\n\r\n    # Initialize the optimizer with epoch 0's LR\r\n    # self = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda x: x)\r\n    self = DummySchedule(optimizer)\r\n    for epoch in range(3):\r\n        print('------')\r\n        print('Run epoch = {!r}'.format(epoch))\r\n        # Pretend we run epoch 0\r\n        print('Training with self._current_lr = {!r}'.format(self._current_lr))\r\n        # Pretend epoch 0 has finished, so step the scheduler.\r\n        # self.step(epoch=epoch)\r\n        self.step()\r\n"", '\r\nSet LR based on self.last_epoch = 0\r\n------\r\nRun epoch = 0\r\nTraining with self._current_lr = 0\r\nSet LR based on self.last_epoch = 0\r\n------\r\nRun epoch = 1\r\nTraining with self._current_lr = 0\r\nSet LR based on self.last_epoch = 1\r\n------\r\nRun epoch = 2\r\nTraining with self._current_lr = 1\r\nSet LR based on self.last_epoch = 2\r\n']","['torch.optim.lr_scheduler._LRScheduler', 'step', 'last_epoch + 1', 'self.step(last_epoch + 1)', 'get_lr()', 'self.last_epoch']",0,0
297,pytorch,8430,open,batchnorm2d  track_running_stats,"when I load a pretrained model, Runtime error occurred:  Missing key(s) in state_dict: bn.num_batches_tracked in every BatchNorm2d layer",module: serialization triaged,"['did you happen to train the model using a build from github at some point?', 'I have the same problem, trained the model with pytorch 0.4.0 and tried to load it with 0.4.1.', 'I have the same problem, trained the model with pytorch 0.3.1 and tried to load it with 0.4.0.', 'Can confirm this is still an issue. Trying to load model with 0.4.1', '@pfriesch @Ai-is-light @LukasMosser Can you provide your model definition, `list(state_dict.keys())`, and `getattr(state_dict, _metadata, None)` please? Thanks!', ""Hello! I have the same issue when trying to load in `0.4.1.post2` model trained in _mayve_ `0.3` (authors of the code didn't write requirements file so I have to infer version myself, but it is not 0.3 + because code still have Variables and volatiles).\r\nIn my case, `getattr(model.state_dict(), '_metadata', None)` returned \r\n```\r\nOrderedDict([('', {'version': 1}),\r\n             ('drop', {'version': 1}),\r\n             ('idrop', {'version': 1}),\r\n             ('rdrop', {'version': 1}),\r\n             ('encoder', {'version': 1}),\r\n             ('parser', {'version': 1}),\r\n             ('parser.drop', {'version': 1}),\r\n             ('parser.gate', {'version': 1}),\r\n             ('parser.gate.0', {'version': 1}),\r\n             ('parser.gate.1', {'version': 1}),\r\n             ('parser.gate.2', {'version': 2}),\r\n             ('parser.gate.3', {'version': 1}),\r\n             ('parser.gate.4', {'version': 1}),\r\n             ('parser.gate.5', {'version': 1}),\r\n             ('parser.gate.6', {'version': 1}),\r\n             ('reader', {'version': 1}),\r\n             ('reader.0', {'version': 1}),\r\n             ('reader.0.drop', {'version': 1}),\r\n             ('reader.0.memory_rnn', {'version': 1}),\r\n             ('reader.0.memory_rnn.ih', {'version': 1}),\r\n             ('reader.0.memory_rnn.ih.0', {'version': 1}),\r\n             ('reader.0.memory_rnn.ih.1', {'version': 1}),\r\n             ('reader.0.memory_rnn.hh', {'version': 1}),\r\n             ('reader.0.memory_rnn.hh.0', {'version': 1}),\r\n             ('reader.0.memory_rnn.hh.1', {'version': 1}),\r\n             ('reader.0.memory_rnn.c_norm', {'version': 1}),\r\n             ('reader.0.memory_rnn.drop', {'version': 1}),\r\n             ('reader.0.projector_summ', {'version': 1}),\r\n             ('reader.0.projector_summ.0', {'version': 1}),\r\n             ('reader.0.projector_summ.1', {'version': 1}),\r\n             ('reader.0.projector_summ.2', {'version': 1}),\r\n             ('reader.1', {'version': 1}),\r\n             ('reader.1.drop', {'version': 1}),\r\n             ('reader.1.memory_rnn', {'version': 1}),\r\n             ('reader.1.memory_rnn.ih', {'version': 1}),\r\n             ('reader.1.memory_rnn.ih.0', {'version': 1}),\r\n             ('reader.1.memory_rnn.ih.1', {'version': 1}),\r\n             ('reader.1.memory_rnn.hh', {'version': 1}),\r\n             ('reader.1.memory_rnn.hh.0', {'version': 1}),\r\n             ('reader.1.memory_rnn.hh.1', {'version': 1}),\r\n             ('reader.1.memory_rnn.c_norm', {'version': 1}),\r\n             ('reader.1.memory_rnn.drop', {'version': 1}),\r\n             ('reader.1.projector_summ', {'version': 1}),\r\n             ('reader.1.projector_summ.0', {'version': 1}),\r\n             ('reader.1.projector_summ.1', {'version': 1}),\r\n             ('reader.1.projector_summ.2', {'version': 1}),\r\n             ('predictor', {'version': 1}),\r\n             ('predictor.drop', {'version': 1}),\r\n             ('predictor.projector_pred', {'version': 1}),\r\n             ('predictor.projector_pred.0', {'version': 1}),\r\n             ('predictor.projector_pred.1', {'version': 1}),\r\n             ('predictor.projector_pred.2', {'version': 1}),\r\n             ('predictor.ffd', {'version': 1}),\r\n             ('predictor.ffd.0', {'version': 1}),\r\n             ('predictor.ffd.1', {'version': 1}),\r\n             ('predictor.ffd.2', {'version': 2}),\r\n             ('predictor.ffd.3', {'version': 1}),\r\n             ('decoder', {'version': 1})])\r\n```\r\nand `list(model.state_dict().keys())` returned \r\n```['encoder.weight',\r\n 'parser.gate.1.weight',\r\n 'parser.gate.1.bias',\r\n 'parser.gate.2.weight',\r\n 'parser.gate.2.bias',\r\n 'parser.gate.2.running_mean',\r\n 'parser.gate.2.running_var',\r\n 'parser.gate.5.weight',\r\n 'parser.gate.5.bias',\r\n 'reader.0.memory_rnn.ih.0.weight',\r\n 'reader.0.memory_rnn.ih.0.bias',\r\n 'reader.0.memory_rnn.ih.1.gamma',\r\n 'reader.0.memory_rnn.ih.1.beta',\r\n 'reader.0.memory_rnn.hh.0.weight',\r\n 'reader.0.memory_rnn.hh.0.bias',\r\n 'reader.0.memory_rnn.hh.1.gamma',\r\n 'reader.0.memory_rnn.hh.1.beta',\r\n 'reader.0.memory_rnn.c_norm.gamma',\r\n 'reader.0.memory_rnn.c_norm.beta',\r\n 'reader.0.projector_summ.1.weight',\r\n 'reader.0.projector_summ.1.bias',\r\n 'reader.1.memory_rnn.ih.0.weight',\r\n 'reader.1.memory_rnn.ih.0.bias',\r\n 'reader.1.memory_rnn.ih.1.gamma',\r\n 'reader.1.memory_rnn.ih.1.beta',\r\n 'reader.1.memory_rnn.hh.0.weight',\r\n 'reader.1.memory_rnn.hh.0.bias',\r\n 'reader.1.memory_rnn.hh.1.gamma',\r\n 'reader.1.memory_rnn.hh.1.beta',\r\n 'reader.1.memory_rnn.c_norm.gamma',\r\n 'reader.1.memory_rnn.c_norm.beta',\r\n 'reader.1.projector_summ.1.weight',\r\n 'reader.1.projector_summ.1.bias',\r\n 'predictor.projector_pred.1.weight',\r\n 'predictor.projector_pred.1.bias',\r\n 'predictor.ffd.1.weight',\r\n 'predictor.ffd.1.bias',\r\n 'predictor.ffd.2.weight',\r\n 'predictor.ffd.2.bias',\r\n 'predictor.ffd.2.running_mean',\r\n 'predictor.ffd.2.running_var',\r\n 'decoder.weight',\r\n 'decoder.bias']\r\n```\r\nmaybe there is some work around to set this attribute in newer version of pyotrch without changing source code?\r\n"", 'Thanks @mojesty . However, what I need is the results on the state_dict you are trying to load. Not the one of the module you are loading onto.', '@SsnL while trying to fix this problem, I dag into the cource code of `torch.load`, found lots of interesting stuff (`MAGIC_NUMBER` constant, for example) and understood that BatchNorm1D is simply forward-incompatible, as in 0.3.0 and 0.3.1 there is no `track_running_stats` attribute. To me, switching to older version solved the problem. Thanks!\r\n\r\n', ""@mojesty I'm glad to know that you found a workaround. Yes, switching to an old version works. But in the new version there is specific BC code to support load old state dict.. So could you please post those two results so I can know what is wrong? 0.4+ versions have much simpler API and many bug fixes so I want people to be able to use 0.4+ at lease."", ""@SsnL  Hi, I have the same problem when I load the model which is trained on Pytorch0.4.\r\nThe error is ‚Äònum_batches_tracked'.\r\nWhen the model is created on 0.4.1, there are loads of  '*.num_batches_tracked'  parameters, which do not exist on 0.4.0.\r\nI have to use 0.4.1, so how can I load the pretrained model?\r\n\r\n""]",[],[],0,0
298,pytorch,19160,open,Suggest model.eval() in torch.no_grad (and vice versa),"## üìö Documentation

  and  are both commonly used in evaluating a model.

[Confusion exists about whether setting  also sets ](https://discuss.pytorch.org/t/does-model-eval-with-torch-set-grad-enabled-is-train-have-the-same-effect-for-grad-history/17183/5?u=ataraxy)

Would you accept a PR which suggests usage of the other, with words like:

* If evaluating a model's performance, using Module.eval() may also be useful.
* If evaluating a model's performance, using autograd.no_grad may also be useful.",module: docs triaged,"['Need more discussions before making a decision. I see this is a good idea but it will affect many current PyTorch users.', 'I think the doc change is fine.', ""yea, let's do the doc change. @HaleTom would accept a  PR for this."", ""@soumith  I'll get on to it then.\r\n\r\n\r\n---------\r\n\r\nRelated:  For convenience, would you consider allowing something like:\r\n\r\n```\r\nModule.eval(grad=False)\r\nModule.train(grad=True)\r\n```\r\n\r\nTo turn off and on gradient tracking, respectively?\r\n\r\n(Of course, the default values of `grad=` should be backward compatible.)\r\n\r\nShall I raise a separate issue for this? I'd also be happy to look into it."", ""No, I think gradient mode is completely independent of eval/train mode of modules. We should point that in the docs, but I don't think an argument like that is a good idea."", 'Thanks @apaszke, good point.', 'To me, it makes sense to include a shortcut that combines both `model.eval()` and `torch.no_grad()`, because scenarios when this combination is needed happen very often.\r\n\r\nP.S. Shameless plug: in the library I developed there is one: [`zero.training.Eval`](https://yura52.github.io/zero/reference/training.html#eval). However, I still think it is worth adding something similar to PyTorch itself.', ""Thanks for the bump on this!  I'll get something submitted in the next week or so."", 'Just a thought, what if we also make `model.eval()` a context manager? So people can do things like \r\n```py\r\nwith model.eval(), torch.no_grad():\r\n  val_acc = val(model)\r\n```', '> Just a thought, what if we also make `model.eval()` a context manager?\r\n\r\nThe only difference it makes is that one can omit `model.train()`, because it becomes the ""default"" mode if `model.eval()` is **always** used as a context-manager, right? As for me, I will anyway use `.train()` and `.eval()` everywhere just before using the model, because I cannot know if some other code changes (or not) my model\'s mode. So, with my current experience, I don\'t see how I can benefit from the `model.eval` as a context-manager.\r\n\r\nAs for [my suggestion](https://github.com/pytorch/pytorch/issues/19160#issuecomment-690961244) above, the only point was to make the ""eval-and-no-grad"" combination less verbose and easier to use correctly.', ""The exact reason for my proposal above is that it is less verbose than the current way... Combining context managers is really easy. I agree with @apaszke that we should have independent options for independent settings. So making `.eval` a ctx manager seems both respecting that principle and make users' lives easier.""]",[],"['model.eval()', 'with torch.no_grad', 'model.eval()', 'autograd.no_grad']",0,0
299,pytorch,24045,open,"torch.{save,load} data corruption when serializing a Module with __{get,set}state__","Repro:



Output



cc @ezyang @gchanan @zou3519",high priority module: serialization quansight-nack triaged,"['The data itself is fine, the issue is that `pickle.load` is the one that calls `__setstate__`, but that happens before we load the tensors from the file, so they\'re filled with garbage when `__setstate__` is run\r\n\r\nThe `__setstate__` call happens on 573, but the tensor loading doesn\'t happen until 580\r\nhttps://github.com/pytorch/pytorch/blob/12ac9171dbf8d19cd2750f6190bd79f4a70d6013/torch/serialization.py#L573-L582\r\n\r\nThis can be verified by unpickling twice, once to find the start of the tensor data, then again to call setstate while the tensors are correctly initialized:\r\n\r\n```diff\r\ndiff --git a/torch/serialization.py b/torch/serialization.py\r\nindex 783563a1c..8e8b10759 100644\r\n--- a/torch/serialization.py\r\n+++ b/torch/serialization.py\r\n@@ -568,6 +568,8 @@ def _load(f, map_location, pickle_module, **pickle_load_args):\r\n         raise RuntimeError(""Invalid protocol version: %s"" % protocol_version)\r\n \r\n     _sys_info = pickle_module.load(f, **pickle_load_args)\r\n+\r\n+    data_pos = f.tell()\r\n     unpickler = pickle_module.Unpickler(f, **pickle_load_args)\r\n     unpickler.persistent_load = persistent_load\r\n     result = unpickler.load()\r\n@@ -581,4 +583,9 @@ def _load(f, map_location, pickle_module, **pickle_load_args):\r\n         if offset is not None:\r\n             offset = f.tell()\r\n \r\n+    f.seek(data_pos)\r\n+    unpickler = pickle_module.Unpickler(f, **pickle_load_args)\r\n+    unpickler.persistent_load = persistent_load\r\n+    result = unpickler.load()\r\n+\r\n     return result\r\n```\r\n\r\nwhich results in \r\n```\r\nstorage  0.4029526114463806\r\n 0.12256866693496704\r\n 0.8267606496810913\r\n 0.057990312576293945\r\n[torch.FloatStorage of size 4]\r\nstorage  -8.972253477331376e+35\r\n 4.5766407844848526e-41\r\n -8.972253477331376e+35\r\n 4.5766407844848526e-41\r\n[torch.FloatStorage of size 4]\r\nstorage  0.4029526114463806\r\n 0.12256866693496704\r\n 0.8267606496810913\r\n 0.057990312576293945\r\n[torch.FloatStorage of size 4]\r\n```\r\n\r\nI have a fix in #24794, but I don\'t know how it will impact legacy serialization / if it\'s safe', '@driazati is this fixed?', ""It's fix-able but involves some fragile assumptions that would probably get broken if someone is manually editing a `torch.save`d file since we have to manually load some of the `torch.save` output to find the file offsets in the binary for each tensor."", '@driazati So this issue is on hold since solving https://github.com/pytorch/pytorch/issues/26567 will solve this one as a side-effect?', ""Right, some of the work is landed already, so if you `torch.save(my_object, my_file, _use_new_zipfile_serialization=True)` you can get the behavior described in #26567. You can follow along in #32244 to see when that is made the default so you wouldn't need the flag anymore.""]","[""\r\nimport torch\r\nimport tempfile\r\n\r\n\r\nclass Foo(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.x = torch.rand(3, 4)\r\n\r\n    def __getstate__(self):\r\n        print('storage', self.x.storage())\r\n        print('cdata', self.x.storage()._cdata)\r\n        print('data', self.x)\r\n        return (self.x,)\r\n\r\n    def __setstate__(self, state):\r\n        x = state[0]\r\n        print('storage', x.storage())\r\n        print('cdata', x.storage()._cdata)\r\n        print('data', x)\r\n\r\nfoo = Foo()\r\nwith tempfile.NamedTemporaryFile() as f:\r\n    torch.save(foo, f)\r\n    f.seek(0)\r\n    loaded = torch.load(f)\r\n\r\n"", '\r\nstorage  0.616813063621521\r\n 0.1769922971725464\r\n 0.8948532938957214\r\n 0.48996198177337646\r\n 0.05472159385681152\r\n 0.5072327852249146\r\n 0.2782403826713562\r\n 0.28143006563186646\r\n 0.34611016511917114\r\n 0.08622455596923828\r\n 0.3336881399154663\r\n 0.06343936920166016\r\n[torch.FloatStorage of size 12]\r\ncdata 94388766643008\r\ndata tensor([[0.6168, 0.1770, 0.8949, 0.4900],\r\n        [0.0547, 0.5072, 0.2782, 0.2814],\r\n        [0.3461, 0.0862, 0.3337, 0.0634]])\r\nstorage  -6.13554555064281e-24\r\n 3.079493505200218e-41\r\n -6.0064247976762345e-24\r\n 3.079493505200218e-41\r\n 1.401298464324817e-45\r\n 0.0\r\n 4.0678076049753325e-31\r\n 6.206981819115192e-36\r\n 6.206981819115192e-36\r\n 6.206981819115192e-36\r\n 6.206981819115192e-36\r\n 2.7953761007662624e-20\r\n[torch.FloatStorage of size 12]\r\ncdata 94388766662208\r\ndata tensor([[-6.1355e-24,  3.0795e-41, -6.0064e-24,  3.0795e-41],\r\n        [ 1.4013e-45,  0.0000e+00,  4.0678e-31,  6.2070e-36],\r\n        [ 6.2070e-36,  6.2070e-36,  6.2070e-36,  2.7954e-20]])\r\n']",[],0,0
300,pytorch,5157,open,BCELoss - weight parameter shape incorrect,"The  parameter of  seems to be incorrectly defined when using a multi-dimensional input and target. Related [forum thread](https://discuss.pytorch.org/t/binary-cross-entropy-weights/13299).

The documentation defines  as:
> If given, has to be a Tensor of size ‚Äúnbatch‚Äù.

However, this example throws an error:


A workaround is to  the  tensor to match the number of dimensions:


Internally,  is called and fails in the first code snippet.
The second code snippet applied a weighting for each batch element, which is fine.

Should we automatically unsqueeze the weight tensor, if input and target are multi-dimensional?

Also, how should we handle class weighting?
If we just pass 2 weights as the  tensor, the code successfully runs, but does not apply class weighting, which might mislead some users:


A workaround would be:


What is wanted behavior of the  parameter in : class or batch weighting?
Both cases have some issues at the moment in my opinion.
I would like to fix this issue, but I would like to hear some opinions on the right behavior.
Class weighting would be consistent with other loss functions like , but maybe batch weighting is a more common use case for .

PyTorch version:  (installed from source)

",module: loss module: nn triaged,"[""Good catch, @ptrblck!\r\n\r\nBased on how BCELoss [is implemented](https://github.com/pytorch/pytorch/blob/ccf4dc15250534e6906aaca696f94b430a4da443/aten/src/THNN/generic/BCECriterion.c#L17), the weights look like they should be per-element weights, not class weights. Correct me if I'm wrong, but I don't think BCELoss has a concept of per-class weights because the inputs and targets are values between 0 and 1.\r\n\r\nIn this respect the current behavior makes sense (weights of size `(4,)` are not broadcastable to the input size of `(4, 2, 2)`). I think the best thing to do is to fix the docs."", ""Thanks for the response @zou3519 !\r\nYeah, you are right. Class weighting won't work for float targets. I just compared it to other loss functions and was wondering if class weighting would be useful for an imbalanced binary classification problem. `NLLLoss` is probably more useful in this setting. \r\n\r\nDo you think the method should automatically check, if `weight` is for batch or element weighting, i.e. `[4]` or `[4, 2, 2]`, and broadcast it if necessary? "", ""If I'm understanding this correctly, let's say we have inputs and targets of size [4, 2, 2]. You are proposing that if someone sends in a weight of size [4], we expand it to size (4, 2, 2) (or something like that).\r\n\r\nWhat would you say the use case of this is? I think it's more common that people look for per-element weights but if there is a use case this would be a good thing to have."", 'I thought about a use case where you would like to weight different instances of the current batch. Something like [sample importance](https://openreview.net/pdf?id=r1IRctqxg). \r\nHowever, this could also be achieved using `reduce=False` and a weight matrix.\r\n\r\nIn the other use case using per-element weights, one would need to reconstruct the `BCELoss` class in each iteration with its new weight matrix? Is it a cheap operation?\r\n\r\nWhat do you think about changing the docs to:\r\n`` \r\nweight (Tensor, optional) ‚Äì a manual rescaling weight given to the loss of each element. If given, has to be same shape as the input.\r\n``', '@zou3519 What do you think about multi-label classification using BCELoss?\r\nIt seems the `weights` can be used as a class weight in this case.\r\nRelated to [this](https://discuss.pytorch.org/t/what-is-the-difference-between-bcewithlogitsloss-and-multilabelsoftmarginloss/14944) thread:\r\n\r\n```\r\nx = Variable(torch.randn(10, 3))\r\ny = Variable(torch.FloatTensor(10, 3).random_(2))\r\n\r\n# double the loss for class 1\r\nclass_weight = torch.FloatTensor([1.0, 2.0, 1.0])\r\n# double the loss for last sample\r\nelement_weight = torch.FloatTensor([1.0]*9 + [2.0]).view(-1, 1)\r\nelement_weight = element_weight.repeat(1, 3)\r\n\r\nbce_criterion = nn.BCEWithLogitsLoss(weight=None, reduce=False)\r\nmulti_criterion = nn.MultiLabelSoftMarginLoss(weight=None, reduce=False)\r\n\r\nbce_criterion_class = nn.BCEWithLogitsLoss(weight=class_weight, reduce=False)\r\nmulti_criterion_class = nn.MultiLabelSoftMarginLoss(weight=class_weight, reduce=False)\r\n\r\nbce_criterion_element = nn.BCEWithLogitsLoss(weight=element_weight, reduce=False)\r\nmulti_criterion_element = nn.MultiLabelSoftMarginLoss(weight=element_weight, reduce=False)\r\n\r\nbce_loss = bce_criterion(x, y)\r\nmulti_loss = multi_criterion(x, y)\r\n\r\nbce_loss_class = bce_criterion_class(x, y)\r\nmulti_loss_class = multi_criterion_class(x, y)\r\n\r\nbce_loss_element = bce_criterion_element(x, y)\r\nmulti_loss_element = multi_criterion_element(x, y)\r\n\r\nprint(bce_loss - multi_loss)\r\nprint(bce_loss_class - multi_loss_class)\r\nprint(bce_loss_element - multi_loss_element)\r\n```', 'Any updates on this? @zou3519 @ptrblck \r\nIssue still remains and I would like to use it to assign weights to every example based on the labelset frequency in my multilabel classification problem. But like @ptrblck mentioned the only way I can get it to work is if I reconstruct the loss object for every batch ']","['\r\nx = Variable(torch.randn(4, 2, 2))\r\ny = Variable(torch.Tensor(4, 2, 2).random_(2))\r\noutput = F.sigmoid(x)\r\n\r\n# Create weight according to doc in BCELoss\r\nweight = torch.randn(4)\r\ncriterion_weighted = nn.BCELoss(weight=weight)\r\nloss_weighted = criterion_weighted(output, y) # Error!\r\n', '\r\n# Unsqueeze weight tensor\r\nweight = torch.randn(4, 1, 1)\r\ncriterion_weighted = nn.BCELoss(weight=weight)\r\nloss_weighted = criterion_weighted(output, y)\r\n', ""\r\n# Create class weights\r\nweight = torch.FloatTensor([0.1, 0.9]) \r\n\r\n# Internally, weight is expanded as\r\nsize = _infer_size(weight.size(), y.size())\r\nweight_expanded = weight.expand(size) \r\nprint(weight_expanded) # This is not, what we wanted as class weights!\r\n\r\ncriterion_weighted = nn.BCELoss(weight=weight)\r\nloss_weighted = criterion_weighted(output, y)\r\ncriterion_nonreduced = nn.BCELoss(reduce=False)\r\nloss_unreduced = criterion_nonreduced(output, y)\r\nloss_weighted_manual = (Variable(weight_expanded) * loss_unreduced).mean()\r\n\r\nif loss_weighted == loss_weighted_manual:\r\n    print('Class weighting failed')\r\n"", '\r\nweight_ = weight[y.data.view(-1).long()].view_as(y)\r\ncriterion = nn.BCELoss(reduce=False)\r\nloss = criterion(output, y)\r\nloss_class_weighted = loss * Variable(weight_)\r\n']","['weight', 'BCELoss', 'weight', 'unsqueeze', 'weight', '_infer_size', 'weight', 'weight', 'BCELoss', 'NLLLoss', 'BCELoss', '0.4.0a0+492e26f']",0,0
301,pytorch,7944,open,Better error message in DataChannelTCP::_receive,"## Issue description

The following code will produce an error from the TCP distributed backend:



Error:



This is a bit misleading, since the sizes of the tensors are the same but the types are different. I think it would be better to give a message along the lines of:



I'd be happy to submit a PR for this if others agree that the original message should be changed.",module: backend triaged,"[""We'll surely accept the PR, but keep in mind that you really shouldn't use this backend, and it's even likely to stop being supported in the future.""]","[""python\r\nimport os\r\nimport torch\r\nimport torch.distributed as dist\r\nimport sys\r\n\r\nos.environ['MASTER_ADDR'] = '127.0.0.1'\r\nos.environ['MASTER_PORT'] = '29500'\r\ndist.init_process_group('tcp', rank=int(sys.argv[1]), world_size=2)\r\n\r\n\r\nif dist.get_rank() == 0:\r\n\tt = torch.arange(9)\r\n\tdist.send(tensor=t, dst=1)\r\nelse:\r\n\tt = torch.zeros(9)\r\n\tdist.recv(tensor=t, src=0)\r\n"", '\r\nTraceback (most recent call last):\r\n  File ""/Users/shendrickson/pytorch_dist.py"", line 19, in <module>\r\n    dist.recv(tensor=t, src=0)\r\n  File ""/Users/shendrickson/anaconda2/envs/torchdev/lib/python3.6/site-packages/torch/distributed/__init__.py"", line 230, in recv\r\n    return torch._C._dist_recv(tensor, src)\r\nRuntimeError: Tensor sizes do not match\r\n', '\r\nRuntimeError: Expected to receive 72 bytes, but got 36 bytes instead. Are tensors of same size and type?\r\n']",[],0,0
302,pytorch,12181,open,Network surgery for transfer fails,"## Per the pytorch/caffe2 Readme I am asking here.

I would like to use an existing network definition and weights from the model zoo as the backbone for a new network. In this specific example the architecture will be squeezenet, and the new network simply has a different shape for the top parameterized layers ['conv10_w', 'conv10_b'], to accommodate a different set of classes from Imagenet. 

Unfortunately, it is not clear from the documentation, tutorials, or examples how to achieve this (to me). Some OS notes: I have built caffe2+OpenCV from source with the current master, into a python2.7.12 virtualenv, cuda 9.0, cuDNN 7.0. 

I wrote a script ( based on https://nbviewer.jupyter.org/gist/kyamagu/6cff70840c10ca374e069a3a7eb00cb4/dogs-vs-cats.ipynb )
that I think should do this: https://gist.github.com/johncorring/d735675e75add96fbdfbcc40fa00f3ba

I get the following error message:
Traceback (most recent call last):
  File ""dogsvscats.py"", line 184, in <module>
    shtyp = workspace.InferShapesAndTypes([train_model.net])
  File ""/home/john/Code/pytorch/build/caffe2/python/workspace.py"", line 258, in InferShapesAndTypes
    blobdesc_prototxt = C.infer_shapes_and_types_from_workspace(net_protos)
MemoryError: std::bad_alloc

which isn't very helpful (especially since cross referencing against caffe2 docs doesn't yield anything).

When I comment out the offending line and try to continue to training I recieve a seg fault that I have narrowed down to coming from line 204, workspace.RunNet(train_model.net). lldb returns the following stack trace:

 thread #1: tid = 9130, 0x00007fffaa112240 libcaffe2.sovoid caffe2::math::CopyMatrix<float, caffe2::CPUContext>(int, int, float const*, int, int, float*, int, int, caffe2::CPUContext*) + 208
    frame #1: 0x00007fffaa11392f libcaffe2.socaffe2::ConvOp<float, caffe2::CPUContext>::RunOnDeviceWithOrderNCHW()::{lambda(caffe2::Tensor*)#1}::operator()(caffe2::Tensor*) const + 1169
    frame #3: 0x00007fffaa3f77f8 libcaffe2.socaffe2::ConvPoolOpBase<caffe2::CPUContext>::RunOnDevice() + 301
    frame #5: 0x00007fffa9fb52e5 libcaffe2.socaffe2::SimpleNet::Run() + 460
    frame #7: 0x00007fffaa0aeb8a libcaffe2.sovoid pybind11::cpp_function::initialize<caffe2::python::addGlobalMethods(pybind11::module&)::{lambda(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, bool)#21}, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, bool, pybind11::name, pybind11::scope, pybind11::sibling>(caffe2::python::addGlobalMethods(pybind11::module&)::{lambda(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, bool)#21}&&, bool (*)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, bool), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call) + 311
    frame #9: 0x00007fffab160220 caffe2_pybind11_state_gpu.soPyEval_EvalFrameEx + 29342
    frame #11: 0x00000000004b9ab6 pythonPyEval_EvalFrameEx + 24639
    frame #13: 0x00000000004b9ab6 pythonPyEval_EvalFrameEx + 22711
    frame #15: 0x00000000004b9ab6 python??? + 63
    frame #17: 0x00000000004e5422 pythonPyRun_SimpleFileExFlags + 390
    frame #19: 0x0000000000493ae2 python__libc_start_main(main=(python_start + 41


",caffe2,[],[],"[""void caffe2::math::CopyMatrix<float, caffe2::CPUContext>(int, int, float const*, int, int, float*, int, int, caffe2::CPUContext*) + 208, name = 'python', stop reason = signal SIGSEGV: address access protected (fault address: 0xb15400000)\r\n  * frame #0: 0x00007fffaa112240 libcaffe2.so"", 'void caffe2::math::Im2Col<float, caffe2::CPUContext, (caffe2::StorageOrder)2>(int, int, int, int, int, int, int, int, int, int, int, int, int, float const*, float*, caffe2::CPUContext*, int) + 1087\r\n    frame #2: 0x00007fffaa3f52b1 libcaffe2.so', 'caffe2::ConvOp<float, caffe2::CPUContext>::RunOnDeviceWithOrderNCHW() + 2712\r\n    frame #4: 0x00007fffaa1c93ed libcaffe2.so', 'caffe2::Operator<caffe2::CPUContext>::Run(int) + 229\r\n    frame #6: 0x00007fffaa09275c libcaffe2.so', 'caffe2::Workspace::RunNet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 954\r\n    frame #8: 0x00007fffab11a277 caffe2_pybind11_state_gpu.so', 'pybind11::cpp_function::dispatcher(_object*, _object*, _object*) + 3552\r\n    frame #10: 0x00000000004c30ce python', 'PyEval_EvalCodeEx + 774\r\n    frame #12: 0x00000000004c1e6f python', 'PyEval_EvalCodeEx + 774\r\n    frame #14: 0x00000000004c16e7 python', 'PyEval_EvalCodeEx + 774\r\n    frame #16: 0x00000000004eb30f python', 'PyRun_FileExFlags + 130\r\n    frame #18: 0x00000000004e3cd6 python', 'Py_Main + 1554\r\n    frame #20: 0x00007ffff7810830 libc.so.6', 'main), argc=2, argv=0x00007fffffffda18, init=<unavailable>, fini=<unavailable>, rtld_fini=<unavailable>, stack_end=0x00007fffffffda08) + 240 at libc-start.c:291\r\n    frame #21: 0x00000000004933e9 python']",0,0
303,pytorch,18434,open,improve jit error message for legacy constructor,"Reported by @jph00

> new_tensor is a legacy constructor and is not supported in the JIT

It'd be helpful to know in the error message what the non-legacy alternative is.",module: docs module: onnx triaged,"[""@soumith i got the same error too when i tried to convert `Mozilla TTS` project to `ONNX` format.\r\n\r\nI am trying to deploy `TTS` using `onnx` and `Tensorflow serving`.  Like\r\n\r\n```\r\nmodel_path = 'best_model.pth.tar'\r\nmodel_config = 'config.json'\r\n\r\nuse_cuda = False\r\n\r\nconfig = load_config(model_config)\r\nap = AudioProcessor(**config.audio)\r\n\r\ninput_adapter = lambda sen: text_to_sequence(sen, [config.text_cleaner])\r\ninput_size = len(symbols)\r\n\r\ntest_model = Tacotron(input_size, config.embedding_size, ap.num_freq, ap.num_mels, config.r)\r\nif use_cuda:\r\n    cp = torch.load(model_path)\r\nelse:\r\n    cp = torch.load(model_path, map_location=lambda storage, loc: storage)\r\ntest_model.load_state_dict(cp['model'])\r\nif use_cuda:\r\n    test_model.cuda()\r\ntest_model.eval()\r\n\r\ndummpy_input = torch.ones(1, MAX_LEN, dtype=torch.long)\r\n\r\ntorch.onnx.export(test_model, args=dummpy_input, f='test.onnx', verbose=True)\r\n```\r\n\r\nIt shown me an error like \r\n\r\n```\r\n/home/data/peter/TTS/layers/tacotron.py:209: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  if x.size(-1) == self.in_features:\r\n/home/data/peter/TTS/layers/tacotron.py:219: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  assert x.size(1) == self.conv_bank_features * len(self.conv1d_banks)\r\n/home/data/peter/TTS/layers/tacotron.py:226: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator add_. This might cause the trace to be incorrect, because all other views that also reference this data will not not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.\r\n  x += inputs\r\n/home/data/peter/TTS/layers/tacotron.py:363: TracerWarning: new_zeros is a legacy constructor and is not supported in the JIT.\r\n  initial_memory = self.memory_init(inputs.data.new_zeros(B).long())\r\n/home/data/peter/TTS/layers/tacotron.py:366: TracerWarning: new_zeros is a legacy constructor and is not supported in the JIT.\r\n  attention_rnn_hidden = self.attention_rnn_init(inputs.data.new_zeros(B).long())\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-46-fae7353f3a28> in <module>()\r\n----> 1 torch.onnx.export(test_model, args=dummpy_input, f='test.onnx', verbose=True)\r\n      2 \r\n      3 # model = test_model\r\n      4 # # text_input = torch.from_numpy(seq).unsqueeze(0).long()\r\n      5 \r\n\r\n/data/zeng_ruihong/anaconda3/lib/python3.6/site-packages/torch/onnx/__init__.py in export(*args, **kwargs)\r\n     25 def export(*args, **kwargs):\r\n     26     from torch.onnx import utils\r\n---> 27     return utils.export(*args, **kwargs)\r\n     28 \r\n     29 \r\n\r\n/data/zeng_ruihong/anaconda3/lib/python3.6/site-packages/torch/onnx/utils.py in export(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type)\r\n    102             operator_export_type = OperatorExportTypes.ONNX\r\n    103     _export(model, args, f, export_params, verbose, training, input_names, output_names,\r\n--> 104             operator_export_type=operator_export_type)\r\n    105 \r\n    106 \r\n\r\n/data/zeng_ruihong/anaconda3/lib/python3.6/site-packages/torch/onnx/utils.py in _export(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, example_outputs, propagate)\r\n    279                                                training, input_names,\r\n    280                                                output_names, operator_export_type,\r\n--> 281                                                example_outputs, propagate)\r\n    282 \r\n    283     # TODO: Don't allocate a in-memory string for the protobuf\r\n\r\n/data/zeng_ruihong/anaconda3/lib/python3.6/site-packages/torch/onnx/utils.py in _model_to_graph(model, args, f, verbose, training, input_names, output_names, operator_export_type, example_outputs, propagate)\r\n    222             raise RuntimeError('\\'forward\\' method must be a script method')\r\n    223     else:\r\n--> 224         graph, torch_out = _trace_and_get_graph_from_model(model, args, training)\r\n    225         params = list(_unique_state_dict(model).values())\r\n    226 \r\n\r\n/data/zeng_ruihong/anaconda3/lib/python3.6/site-packages/torch/onnx/utils.py in _trace_and_get_graph_from_model(model, args, training)\r\n    190     # training mode was.)\r\n    191     with set_training(model, training):\r\n--> 192         trace, torch_out = torch.jit.get_trace_graph(model, args, _force_outplace=True)\r\n    193 \r\n    194     if orig_state_dict_keys != _unique_state_dict(model).keys():\r\n\r\n/data/zeng_ruihong/anaconda3/lib/python3.6/site-packages/torch/jit/__init__.py in get_trace_graph(f, args, kwargs, _force_outplace)\r\n    195     if not isinstance(args, tuple):\r\n    196         args = (args,)\r\n--> 197     return LegacyTracedModule(f, _force_outplace)(*args, **kwargs)\r\n    198 \r\n    199 \r\n\r\n/data/zeng_ruihong/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    487             result = self._slow_forward(*input, **kwargs)\r\n    488         else:\r\n--> 489             result = self.forward(*input, **kwargs)\r\n    490         for hook in self._forward_hooks.values():\r\n    491             hook_result = hook(self, input, result)\r\n\r\n/data/zeng_ruihong/anaconda3/lib/python3.6/site-packages/torch/jit/__init__.py in forward(self, *args)\r\n    250         try:\r\n    251             trace_inputs = _unflatten(all_trace_inputs[:len(in_vars)], in_desc)\r\n--> 252             out = self.inner(*trace_inputs)\r\n    253             out_vars, _ = _flatten(out)\r\n    254             torch._C._tracer_exit(tuple(out_vars))\r\n\r\n/data/zeng_ruihong/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    485             hook(self, input)\r\n    486         if torch._C._get_tracing_state():\r\n--> 487             result = self._slow_forward(*input, **kwargs)\r\n    488         else:\r\n    489             result = self.forward(*input, **kwargs)\r\n\r\n/data/zeng_ruihong/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py in _slow_forward(self, *input, **kwargs)\r\n    475         tracing_state._traced_module_stack.append(self)\r\n    476         try:\r\n--> 477             result = self.forward(*input, **kwargs)\r\n    478         finally:\r\n    479             tracing_state.pop_scope()\r\n\r\n/data/zeng_ruihong/tts_test_hupo_v3/TTS/models/tacotron.py in forward(self, characters, mel_specs, mask)\r\n     37         # batch x time x dim*r\r\n     38         mel_outputs, alignments, stop_tokens = self.decoder(\r\n---> 39             encoder_outputs, mel_specs, mask)\r\n     40         # Reshape\r\n     41         # batch x time x dim\r\n\r\n/data/zeng_ruihong/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    485             hook(self, input)\r\n    486         if torch._C._get_tracing_state():\r\n--> 487             result = self._slow_forward(*input, **kwargs)\r\n    488         else:\r\n    489             result = self.forward(*input, **kwargs)\r\n\r\n/data/zeng_ruihong/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py in _slow_forward(self, *input, **kwargs)\r\n    475         tracing_state._traced_module_stack.append(self)\r\n    476         try:\r\n--> 477             result = self.forward(*input, **kwargs)\r\n    478         finally:\r\n    479             tracing_state.pop_scope()\r\n\r\n/data/zeng_ruihong/tts_test_hupo_v3/TTS/layers/tacotron.py in forward(self, inputs, memory, mask)\r\n    404         t = 0\r\n    405         memory_input, attention_rnn_hidden, decoder_rnn_hiddens,\\\r\n--> 406             current_context_vec, attention, attention_cum = self._init_states(inputs)\r\n    407         while True:\r\n    408             if t > 0:\r\n\r\n/data/zeng_ruihong/tts_test_hupo_v3/TTS/layers/tacotron.py in _init_states(self, inputs)\r\n    367         decoder_rnn_hiddens = [\r\n    368             self.decoder_rnn_inits(inputs.data.new_tensor([idx]*B).long())\r\n--> 369             for idx in range(len(self.decoder_rnns))\r\n    370         ]\r\n    371         current_context_vec = inputs.data.new(B, self.in_features).zero_()\r\n\r\n/data/zeng_ruihong/tts_test_hupo_v3/TTS/layers/tacotron.py in <listcomp>(.0)\r\n    367         decoder_rnn_hiddens = [\r\n    368             self.decoder_rnn_inits(inputs.data.new_tensor([idx]*B).long())\r\n--> 369             for idx in range(len(self.decoder_rnns))\r\n    370         ]\r\n    371         current_context_vec = inputs.data.new(B, self.in_features).zero_()\r\n\r\nTypeError: mul(): argument 'other' (position 1) must be Tensor, not list\r\n```\r\n\r\nHope someone could give me a hint. Thanks a lot."", 'cc : @houseroad ', 'So what is the recommended alternative to `Tensor.new_empty()`?']",[],[],0,0
304,pytorch,13636,closed,Conv3d with cudnn error,"When I use **torch.backends.cudnn.benchmark = True** in Conv3d, it returned error 
**RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR**

I tired **re -rf ~/.nv** but it doesn't work. The error only disappears when **torch.backends.cudnn.benchmark = False**.

Ubuntu 16.04.4 LTS Tesla P100 Cuda 9.0 cudnn 7.1 
pytorch 1.0.0a0+d03c6ba from source and 0.4.1 from pip both tried.",,"['Could we get a repro? What are your input sizes?', 'Closing due to age and lack of reproduction; if this issue still occurs on the latest version of PyTorch please file a new issue']",[],[],0,0
305,pytorch,7343,open,[memory leak] [PyTorch] .backward(create_graph=True),"

leaks with  but not with .

Discovered when running code in #7270 

cc @ezyang @albanD @zou3519 @gqchen @pearu @nikitaved",module: autograd module: memory usage triaged,"[""That's a known issue, that's hard to solve in general. The problem is that we get a reference cycle in C++, which used to be broken by having a `weak_ptr` in the `AccumulateGrad`, which is now a strong reference. One way to get around it would be to disallow `create_graph` with `.backward` (and only use it with `.grad`).\r\n\r\nIIRC the reference cycle is `x -> x.grad -> x.grad.grad_fn = TanhBackwardBackward -> x`"", '@apaszke just beat me to describing the issue. To expand on his point, I think we need to disable history on `Tensor.grad`. So you would need to use `torch.autograd.grad` instead of `Tensor.backward()`', 'related https://github.com/pytorch/pytorch/pull/24368', 'Note that since then, we added a couple warnings to the doc to try and deter users from doing this.\r\nAlso note that people that use the new `zero_grad(set_to_none=True)` will break the cycle and avoid the leak !', ""@albanD \r\n\r\n> Note that since then, we added a couple warnings to the doc to try and deter users from doing this.\r\n> Also note that people that use the new `zero_grad(set_to_none=True)` will break the cycle and avoid the leak !\r\n\r\n@albanD this fix doesn't work for me -- do you know why this might be the case?"", 'Can you open a topic on the forum https://discuss.pytorch.org/ with details of your training loop and what you do?']","[""python\r\n>>> import torch\r\n>>> import gc\r\n>>> _ = torch.randn(1, device='cuda')\r\n>>> del _\r\n>>> torch.cuda.synchronize()\r\n>>> gc.collect()\r\n0\r\n>>> print(torch.cuda.memory_allocated())\r\n865280\r\n>>> x = torch.randn(1, device='cuda', requires_grad=True)\r\n>>> y = x.tanh()\r\n>>> y.backward(torch.ones_like(y), create_graph=True)\r\n>>> del x, y\r\n>>> torch.cuda.synchronize()\r\n>>> gc.collect()\r\n0\r\n>>> print(torch.cuda.memory_allocated())\r\n867328\r\n""]","['y  = x.tanh()', 'y = x  + 1']",0,0
306,pytorch,12322,open,[Caffe2] Segmentation fault (core dumped) while import caffe2.python.core,"## üêõ Bug

When I want to import caffe2.pytho.core, I get a Segmentation fault.

I followed the install guide for ubuntu 16 with prebuilt binaries and also installed nccl, I've been browsing past issues but can't find a solution.

please help me

## To Reproduce

Steps to reproduce the behavior:

1. Install NCCL 2.3.5
2. conda install -c caffe2 caffe2-cuda9.0-cudnn7
3. import caffe2.python.core

ERROR : 



## Environment


Followed : [](https://caffe2.ai/docs/getting-started.html?platform=ubuntu&configuration=prebuilt)

Cmake : I installed with anaconda I can't find the cmake output




",caffe2,"['I get the same after compiling pytorch+caffe2 with GPU support from scratch (`NO_TEST=1 USE_FFMPEG=1 USE_OPENCV=1 USE_LEVELDB=0 USE_LMDB=1 python setup.py install`) in a new anaconda environment. I get the segmentation fault after `from caffe2.python import workspace` (may be the same) and with `import caffe2.python.core`. ', '@kateiyas sorry for the slow response here. Can you drop the CMake output (basically stdout when running `NO_TEST=1 USE_FFMPEG=1 USE_OPENCV=1 USE_LEVELDB=0 USE_LMDB=1 python setup.py install`) here? That should help us debug. Thanks.']","['\r\nWARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\r\nSegmentation fault (core dumped)\r\n', '\r\nuname -a\r\nLinux realAI 4.4.0-131-generic #157-Ubuntu SMP Thu Jul 12 15:51:36 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n', '\r\nwhich python\r\n/home/mputtnam/anaconda3/bin/python\r\n\r\nwhich pip\r\n/home/mputtnam/anaconda3/bin/pip\r\n\r\necho $PYTHONPATH\r\n<blank>\r\n', '\r\nnvidia-smi\r\nThu Oct  4 08:35:22 2018\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 396.54                 Driver Version: 396.54                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Quadro K4000        Off  | 00000000:02:00.0  On |                  N/A |\r\n| 30%   38C    P8    11W /  87W |    419MiB /  3010MiB |      3%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Quadro K4000        Off  | 00000000:81:00.0 Off |                  N/A |\r\n| 30%   33C    P8    10W /  87W |      1MiB /  3018MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Quadro K4000        Off  | 00000000:82:00.0 Off |                  N/A |\r\n| 30%   32C    P8    10W /  87W |      1MiB /  3018MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1974      G   /usr/lib/xorg/Xorg                           210MiB |\r\n|    0      2955      G   compiz                                       180MiB |\r\n|    0      4925      G   /usr/bin/nvidia-settings                      24MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n', '\r\nnvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2015 NVIDIA Corporation\r\nBuilt on Tue_Aug_11_14:27:32_CDT_2015\r\nCuda compilation tools, release 7.5, V7.5.17\r\n\r\nls -lah /usr/local/cuda\r\nlrwxrwxrwx 1 root root 8 Oct  3 08:35 /usr/local/cuda -> cuda-9.0\r\n\r\n\r\n']",[],0,0
307,pytorch,934,open,dataloader parallels over elements vs over batches,"With current design of , if we set , 8 batches of data will be prepared in advance, each worker works on one batch. This is not very efficient especially when one batch is large/expensive to process; and normally we only need 1-2 batches ahead of time . Why didn't we go with the design where all workers work on one batch at a time? 

cc @SsnL",feature module: dataloader todo triaged,"['It is totally up to you to write your own `Dataset` which will for instance make use of `multiprocessing`. A good point is to do the hard work when the `Dataset` is instantiated. Also when `__getitem__()` of your `Dataset` is called, your processes will do the job.']",[],"['dataloader', 'num_workers=8']",0,0
308,pytorch,1362,open,Feature Request: noise contrastive estimation/negative sampling,"There isn't a standard loss function implementing this, even though it's pretty common. I am perfectly willing to implement it myself, if nobody else feels like it. It shouldn't be terribly complicated. I would structure it something like:
(N, C)C = num_classes(N)0 <= targets[i] <= C-1
The API for tensorflow also uses a parameter  so that by setting it to  you can switch to a negative sampling objective from nce. Is this worth doing?

Is this something that already exists? Is this something worth implementing?",feature module: nn triaged,"[""It doesn't exist and we'd be very happy to accept a PR! I'm not familiar with NCE so I can't assess the API until you send a PR, but it looks reasonable. "", ""If there's anything I can do to help on this I'd be happy to contribute. Code review, bug fixes, testing, etc."", '@nsaphra one small suggestion is that you dont need to subclass from `torch.nn.modules.loss._Loss`, subclassing from `nn.Module` should be fine.\r\nHaving a NCELoss would be great :)', 'Is this helpful? https://github.com/analvikingur/pytorch_NEG_loss', ""@cigrainger That's a very specific form of negative loss, and not really what I was hoping to make, since I would love to have a general way of combining my nce loss with a multilayer architecture. I would love some advice about how to effectively test my loss function, though, and I would definitely benefit from a code review.\r\n\r\n@soumith Why don't I need to subclass? Do you think that there is a documentation/readability benefit from using a loss specific superclass? I guess I'm not sure what the advantage of using `Module` vs. `_Loss`."", ""The `_Loss` subclass is currently tailored to the set of loss functions that wrap existing C fwd+bwd implementations from the Lua Torch backend library. Since you're most likely implementing in pure PyTorch (unless there are performance issues there that you can't get around), it's probably simpler to just subclass `Module`, whose ordinary behavior is to use the PyTorch implementation you provide in `forward` and rely on autograd for the backward pass.\r\n(I think what you're bringing up is effectively the difference between structural and nominative subtyping, and Python seems to lean towards structural)."", ""@nsaphra I have written an NCELoss module that takes the same parameters as `nn.nllloss`. The only difference is the unnormalized probability without softmax . But actually the NCE module should replace the traditional linear decoder at the output layer right before softmax, only in this way the complexity decreases from **|V|** to **k+1**.\r\nSuch a module is stateful because it stores the decoder matrix, and, in my opinion it's beyond a **simple** 'Loss' function.\r\nCan I have an early access to your working code?"", 'Hi @Stonesjtu, would you please share your NCELoss module? I plan to use ice loss in my current project. Thanks!', '@tonyyuango  Pushed my training scripts to github. Check this out.\r\nhttps://github.com/Stonesjtu/Pytorch-NCE/tree/nce', '@Stonesjtu Thank you so much!', ""Sorry, I was out of office for a while. I still think it would be useful to have this in the standard library, and I don't think that the tensorflow license allows basing the implementation on their code. IANAL, but I do think it's all right to use their api to identify what common loss functions might be worth adding beyond this one. Does anyone know?"", ""@nsaphra is @Stonesjtu 's implementation fine with you functionally? We can integrate that or a form of that into the standard library."", ""@nsaphra  I'll take a look at the NCE_loss API in `tensorflow`.\r\n\r\n@soumith is there any design docs for the components of `PyTorch`, or any blogs explaining how `Pytorch` works in a deeper manner?"", ""@tonyyuango Have you timed your NCE implementation?\r\n\r\nBefore I came across this thread, I had implemented a version [here](https://github.com/parthaca/examples/tree/master/word_language_model)\r\nbut it turned out to be slower than CE ([details here](https://discuss.pytorch.org/t/nce-loss-for-large-output-vocabularies/3415)). I haven't figured out if it is because of the index_select() and repeat() operations."", '@Stonesjtu Sorry, mis-directed my previous comment. Please read the comment above.\r\n', 'It turned out that `torch.multinomial()` was the culprit. I added an implementation of a more efficient sampling method described [here](https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/). This makes NCE a bit faster than CE on the 10K PTB dataset and much faster on larger vocabularies. More details in this [thread](https://discuss.pytorch.org/t/nce-loss-for-large-output-vocabularies/3415).', 'Thanks @parthaca and @Stonesjtu! Your implementations help me a lot! I\'m new to both pytorch and deep learning, and I just transfer to python from Java, so I encountered some difficulties when reading your code. It would be appreciated if you could clarify them.\r\n\r\nI found @parthaca uses the same parameters (weight and bias) for both encoder and nce decoder (""self.decoder.weight = self.encoder.weight"" in model.py). I\'m not quite clear about the reason. Does it mean that the input and output spaces must be the same? \r\n\r\nI implemented my nce by following @Stonesjtu \'s code as it came out a little earlier, but I found the parameters of the decoder don\'t change during model fitting. Perhaps it\'s because the decoder is a part of the loss function and its parameters are not involved in the backward propagation (please correct me if I\'m wrong). Thus I put the decoder into the RNN model and then it worked.', 'Q1: It depends on you wether to tie weights of encoder and decoder.\nQ2: You should add the parameters of nce to the optimizer, which means you will optimize both your original model and NCE output module. \n> On 2 Jun 2017, at 1:57 PM, tonyyuango <notifications@github.com> wrote:\n> \n> Thanks @parthaca <https://github.com/parthaca> and @Stonesjtu <https://github.com/stonesjtu>! Your implementations help me a lot! I\'m new to both pytorch and deep learning, and I just transfer to python from Java, so I encountered some difficulties when reading your code. It would be appreciated if you could clarify them.\n> \n> I found @parthaca <https://github.com/parthaca> uses the same parameters (weight and bias) for both encoder and nce decoder (""self.decoder.weight = self.encoder.weight"" in model.py). I\'m not quite clear about the reason. Does it mean that the input and output spaces must be the same?\n> \n> I implemented my nce by following @Stonesjtu <https://github.com/stonesjtu> \'s code as it came out a little earlier, but I found the parameters of the decoder don\'t change during model fitting. Perhaps it\'s because the decoder is a part of the loss function and its parameters are not involved in the backward propagation (please correct me if I\'m wrong). Thus I put the decoder into the RNN model and then it worked.\n> \n> ‚Äî\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub <https://github.com/pytorch/pytorch/issues/1362#issuecomment-305696922>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AEWFDKgwlCtADMKk1Hjnppuc3wGJD9f-ks5r_6QsgaJpZM4NJQ9A>.\n> \n\n', 'Are you guys interested in a simple contrastive loss fn as well? Originally proposed in Hadsell et al 2006. Something like [this](https://gist.github.com/harveyslash/725fcc68df112980328951b3426c0e0b#file-contrastive-loss-py)  from [here](https://hackernoon.com/facial-similarity-with-siamese-networks-in-pytorch-9642aa9db2f7)', '@Stonesjtu are there any plans to integrate your work into PyTorch?', 'I struggled a lot with the choice of API. Currently I found two implementations on `Tensorflow` and `dpnn`. Do you have any suggestions.\r\n\r\nRef:\r\nNCE on tensorflow: https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss\r\nNCE on dpnn: https://github.com/Element-Research/dpnn#nn.NCEModule', 'Not really, I am not experienced enough. However, I would recommend making a PR, as I looked through your repo and it seems to be mostly functional? I guess it would be easier to discuss the API and review the PR when submitted.', ""Has there been any progress on this? @Stonesjtu I see you've kept your implementation up-to-date, any plans to submit it as a PR?"", ""@ddehueck I will open a PR this weekend, but I should take long time to discuss the API. Another solution is that I can provide a pip package so you don't have to clone the source code."", ""@Stonesjtu That would be great. I think having this built-in to PyTorch would be ideal and I'm happy to help with any API discussion in the PR."", 'Does your NCE Loss have log uniform candidate sampling? ', '@Santosh-Gupta No, because I found unigram sampling gives better PPL than log-uniform for LM on large scale dataset. Another reason is the time complexity of unigram sampling is hugely reduced by using alias method.', ""Was the similarity and analogy properties of the embeddings tested? I find with Unigram sampling the most common words tend to dominate, though I haven't trained a very large scale dataset. "", ""If you mean the norm of the embedding vector of high-frequency words, yes. I've done some word similarity benchmarks, not much difference between sampled loss and cross-entropy loss."", 'Any progress on the PR?']",[],"['', 'python\r\nclass NCELoss(torch.nn.modules.loss._Loss):\r\n    r""""""Noise contrastive estimation loss function.\r\n    Args:\r\n        num_classes: int number of classes for the output layer\r\n        num_sampled: int number of samples to extract from noise distribution\r\n        noise_sampler: () -> int function\r\n            Function to generate k class labels according to noise distribution.\r\n            By default, noise will be assumed log uniform unigram.\r\n\r\n    Shape:\r\n        - Input: :math:', ' where ', '\r\n        - Target: :math:', ' where each value is ', '\r\n    """"""\r\n', '', 'subtract_log_q', 'False']",0,0
309,pytorch,13711,closed,"In eval(),  BatchNorm running_mean is different between different GPUs","## üêõ Bug

<!-- A clear and concise description of what the bug is. -->
After train , when test the model, the BatchNorm is different between different GPUs, it does not do broadcast in eval()?

## To Reproduce

Steps to reproduce the behavior:

1.
1.
1.

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


 - PyTorch Version (e.g., 1.0):
 - OS (e.g., Linux):
 - How you installed PyTorch (, , source):
 - Build command you used (if compiling from source):
 - Python version:
 - CUDA/cuDNN version:
 - GPU models and configuration:
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->
",,"['Please give us more information /a repro.', ""I means after run val after train, in loading a checkpoint , the running_mean is same between different gpus. But when running val after train, the running_mean is different between GPUs.\r\nYou can reproduction by running the same image in different gpus.\r\nOr it is a character , not a bug? when you save the checkpoint in rank 0, it just save the rank0 GPU's\r\nBatchNorm parameters\r\n\r\n"", ""Closing due to age; if you're still encountering this issue on the latest PyTorch please open a new issue with an updated reproduction""]","['\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n']","['conda', 'pip']",0,0
310,pytorch,30987,open,Remove `.data`,"Even though it is not documented, many users still use it. And it leads to many bugs in user code.
So we should remove it completely to prevent this.
The expected steps are:
- [ ] Add a new api to make a shallow copy that does not share version? Or a cat + unbind function that does not share version counter if that is enough.
- [ ] Remove the use of .data in all our internal code
  - [ ] 
  - [ ]  and 
  - [ ] 
  - [ ]  Blocky by https://github.com/pytorch/pytorch/pull/30258#issuecomment-558344600 (not blocked ones were done in #31479)
  - [ ]  Use that requires the new api mentioned above.
  - [x]  #31480 
  - [ ]  Simple ones done in #31481 and #31482
  - [ ] 
  - [ ] 
  - [ ] 
  - [x] *ignored* 
- [ ] Add warning to every call to 
- [ ] Actually remove  ?


cc @ezyang @SsnL @albanD @zou3519 @gqchen",actionable better-engineering enhancement module: autograd triaged,"['I currently have to use `.data` to circumvent version tracking for implementing invertible inplace ops (InplaceBatchNorm + InplaceLeakyRelu). I wish there was a way around that.', ""That would be the first point. If you just want to get a copy that does not share version counter. But that new api will only do this. And won't have all the other side-effects of `.data`. That should fit your use case right?"", 'Yes I think so! :)', 'I\'m curious is `.data` ""redundant"" or ""internal"". When building custom logger, optimizer and etc, I often use `a.data = b.data` where `b` was a clone of `a`. Literally, this should mean ""attach the data of `b` back to `a` without copying"", but I\'m not sure did pytorch actually do this.\r\n\r\nOne good thing to have `.data` is that you can do these kind of attaching/detaching conveniently (while you cannot attach an `np.ndarray` to another one without actually replacing the array object). Another good thing is a short-hand notation for `torch.no_grad`', 'cc @colesbury ', '@ZisIsNotZis it is ""historical"". `.data` was the Tensor wrapped by a Variable. But since Variable disappeared. It does not make sense anymore. It allows you to do too many things and in general shoot you in the foot.\r\nFor your use case, you should use `a.set_(b)` which is a specialized function to set a Tensor in another inplace. Or just `a = b` if you don\'t need to change `a` inplace.', ""Hey @albanD \r\nHow does it affect the p.grad.data and p.data used by the optimisers?\r\ne.g. https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py#L64\r\nJust in case I'm writing my own optim ;) Cheers"", 'Hi,\r\n\r\nFor optimizers, the simplest solution is to remove all the `.data` and add the `@torch.no_grad()` decorator to the `step()` function.', ""You should document every thing `.data` currently allows you to do, to make sure we have a preferred workaround.  For example, one thing I don't see listed here is changing the dtype of a tensor in place."", 'Currently `.data` have few use. The main ones are below:\r\n- `y = x.data` is often used to get a new Tensor that does not share the history of `x` but looks at the same memory. This should be replaced with `y = x.detach()`\r\n- `x.data.copy_(foo)` is used to change the content of `x` without the autograd knowing about it. this should be preferably replaced by \r\n```\r\nwith torch.no_grad():\r\n  x.copy_(foo)\r\n```\r\n- `y = x.data` is sometimes used (in `torch/cuda/` for example) to get a new Tensor that share the same data as `x` but does not share the version counter (and so has a different behavior wrt inplace operations). This is the use for which we want to add a new api.\r\n- `x.data = y` is sometimes used (in nn.Module `_apply` function) to mutate the Tensor `x` inplace. This changes the properties of the Tensor x (type, storage, layout etc) inplace. We should disallow such modification altogether.', 'about last point: sometimes reinterpret (changing dtype) is useful and can be made explicit, but maybe this is unrelated to inplace change of dtype: https://github.com/pytorch/pytorch/issues/29013', 'But that last point would not allow you to do this as it only populates with all the properties (including storage) of the other Tensor. So it makes a ""copy"" of the metadata `y` into `x`.\r\nBut I agree that a reinterpret would be useful feature !', '@albanD I just checked that the only `.data` used in tensorboard is the following, which is guarded by a type check.\r\n\r\n\r\n```python\r\ndef _prepare_pytorch(x):\r\n    if isinstance(x, torch.autograd.Variable):\r\n        x = x.data\r\n    x = x.cpu().numpy()\r\n    return x\r\n```', ""@lanpa thanks for looking into this !\r\nGiven that `torch.autograd.Variable` do not exist anymore (it's litteratlly an empty class [here](https://github.com/pytorch/pytorch/blob/14593f077f9cb248cca85fd18b598d14c47d5d4e/torch/autograd/variable.py#L10)) this check is a bit out-dated.\r\nBut if you send a PR (add me as a reviewer) to replace it with `x = x.detach().cpu().numpy()`, that will work.\r\n"", 'Is it really necessary to detach in the code above?', 'If a Tensor that requires gradient is passed to this function, the call to `.numpy()` will fail.\r\nGiven the original code, I guessed a Tensor that requires gradient can be passed. If it is never the case, then you can ignore the detach() and simply remove the `if` statement indeed.', ""I noticed that there is renewed interest in removing `.data`. If work hasn't already begun on this, I plan on working on removing its usage in the optimizers."", ""I started removing `.data` from the optimizers (see #33640). As a quick note, using the `@torch.no_grad` decorator for the `step` method can fail as the `closure` function may call `backward`. If the decorator is used, then `closure` will error, as I found when running the tests. This can be worked around by the end-user with `torch.enable_grad()`, so it's only an issue for the optimizers included with PyTorch.\r\n\r\nI am also running into a further problem with my current strategy for removing `.data` from the optimizers. It seems to fail in the distributed case specifically, and I could use some advice on how to fix it or further test it. The shallow copy is complaining of in-place modifications, and I think it's related to `torch.no_grad()` being thread-local."", ""Hi,\r\n\r\nYes, given all the confusion it creates for new users and the danger it is, we're trying to remove it !\r\nI'll answer on the PR.\r\n\r\nBut one of the dangerous side-effect of using `.data` is that it does not share the version counter of the original Tensor. And so the inplace verification we have in the autograd engine do not work.\r\nRemoving the `.data` adds these check back (which is good), but in some cases they are too strict. See the first point in the PR about adding a new function to do a shallow copy without sharing the version counter for advanced user.""]",[],"['benchmarks/', 'docs/source/scripts/build_activation_images.py', 'docs/source/notes/extending.rst', 'test/', 'torch/autograd/', 'torch/cuda/', 'torch/jit', 'torch/nn', 'torch/optim', 'torch/utils/tensorboard/', 'torch/tensor.py', 'caffe2/', '.data', '.data']",0,0
311,pytorch,9983,open,[feature request] Add matrix functions,"Add matrix power as implemented by [numpy.linalg.matrix_power](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.matrix_power.html), matrix exponential as implemented by [scipy.linalg.expm](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.linalg.expm.html), and matrix logarithm as implemented by [scipy.linalg.logm](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.linalg.logm.html).


- [x] matrix_power
- [x] matrix_exp
- [ ] matrix_log
- [ ] matrix_sqrt

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @jianyuh @nikitaved @pearu @mruberry @heitorschueroff @walterddr @IvanYashchuk @xwang233 @Lezcano @rgommers",function request high priority module: linear algebra module: numpy triaged,"['Matrix power can be easily implemented in PyTorch by following the `numpy` implementation, which [translates straight away to PyTorch, see implementation here](https://github.com/numpy/numpy/blob/0124839917feb4f124d07dbfbf79915f01624a3a/numpy/linalg/linalg.py#L602-L636).\r\nMatrix exponential seems [a bit more involved though](https://github.com/scipy/scipy/blob/f179b25f00fbb5d645a2e4595d1d908fb499d84c/scipy/sparse/linalg/matfuncs.py)', 'Are these worth porting into ATen? If so, I can take it up.', 'Yes, please!', 'A generalization based on SVD (https://en.wikipedia.org/wiki/Matrix_function) may also be useful (for simple graph signal processing): diagonalize the matrix somehow (svd by default, maybe some optimized svd for symmetric matrices), apply the user-supplied function to eigenvalues, re-multiply the factors back.\r\n\r\nFor diagonalizable matrices, this way of doing matrix power may also be faster (for large powers). This would also enable matrix square root, though for matrix square root there seem to be specialized approx algorithms based on newton iteration.', '@zou3519 @fmassa I went through the SciPy implementation for `matrixexp` (named `expm` in SciPy), and have a few questions:\r\n- the implementation is added for sparse arrays as well. Should I be considering that, or is that supposed to be left for later?\r\n- the implementation seems to take advantage of upper triangular matrices, and there are some optimized calculations done for the same. Should I take the same approach with `ATen` or is a general implementation sufficient?\r\nThank you.', 'Upvote for this, matrix exponential `expm`, logarithm `logm` and (fractional) matrix power `powm` which includes square root (with negative powers support)', ""The approach by @vadimkantorov seems to make most sense IMO because there's already a (differentiable) PyTorch function for SVD, and after that the exponentiation is just elemwise exp for the diagonal.\r\n\r\n~There's also an elegant (and probably useful) expression for the gradient. I don't actually know how the pytorch gradients are defined (yet), but I could probably dig up/work out the math for the gradient if there's interest for that...~\r\nEDIT: oops actually it's probably not useful in practice... anyway, the SVD + elemwise exp seems good"", 'Already implemented', ""OK great to hear that @SsnL! Can you point out the function (also for reference to anyone else that happens to stumble on this thread)? Can't find it in the docs..."", 'This issue is partially resolved: `matrix_power` is implemented, whereas `matrix_exp` is not yet done.', '@harpone here it is for reference https://pytorch.org/docs/master/torch.html?highlight=matrix_power#torch.matrix_power\r\nNote that it is only available in the nightly build, not in 0.4.1', 'Sorry about that. I updated the issue title and desc to reflect the current status :)', '@SsnL It would be nice if matrix_power also supported non-integer powers like 0.5 and other cases. It may though require a full svd approach or some custom Newton-step approximation (saw it in some paper for a fast matrix square root estimation without doing an svd)', '@vadimkantorov Yep that would be nice. Do you still have a link to the paper?', '@SsnL For Newton-step for matrix square root: https://arxiv.org/abs/1707.06772', 'Since this involves a paper, maybe can this go into `contrib`?', 'I think a larger feature to decide is about supporting non-integer powers in matrix_power.\r\n\r\n(The paper is just about a particular faster matrix square root approximation scheme - I agree, it can go to contrib.)', 'Can developers please tell us that by when we should expect the implementations of expm and logm functions in pytorch? \r\n', 'This might be helpful\r\nhttps://github.com/SkafteNicki/cuda_expm\r\n\r\nand CC @SkafteNicki', 'Would this allow back-prop with functions involving matrix_expo?', 'That guy seems to have implemented this as well. I tried to build his module but gut stuck with linkage issues. The pure pytorch example was working correctly ', '@ferrine I never got the cuda implementation to work, because I got stuck with some linkage to cuSOLVER. The pure pytorch example works fine and can back-prop gradients. The cuda implementation was just to get a faster implementation, since I use matrix exponentials quite a bit myself. ', ""I've managed to implement (with a simple test) matrix exponential using torch script here:\r\nhttps://github.com/ferrine/geoopt/blob/master/geoopt/linalg/_expm.py\r\nI can open a PR to add that to pytorch master as well"", 'Just in case you need a taste for the limitations of various matrix exponential algorithms: https://www.cs.cornell.edu/cv/ResearchPDF/19ways+.pdf', 'Seems that the feature request has been completed. Feel free to re-open it if you have any questions.', 'hmm i don‚Äôt think this exists in pytorch tho. @zhangguanheng66 ', '@SsnL I thought you already implemented, as mentioned above. Anyways, I will re-open it and triage.', 'Any updates on the development of `expm`, `logm`, `sqrtm`?', ""The formula for the gradient of the exponential is derived in Proposition 6.1 here:\r\nhttps://arxiv.org/abs/1909.09501\r\nand an implementation for 32-bit skew-symmetric matrices can be found here: \r\nhttps://github.com/Lezcano/expRNN/blob/e29e9ec4e1d419763f2edbdd983f479177771f05/trivializations.py#L10\r\n\r\nThe following things would be missing, which should be very easy to implement modulo making some design choices:\r\n- Implement the exponential for general matrices following Proposition 6.1 in the paper.\r\n- Generalise the implementation to correctly handle batches of matrices of the same size.\r\n\r\nDesign choices:\r\n- Choose how to compute the inverse needed in the gradient formula in general, as one of the most compelling use cases of the exponential is when its argument is skew-symmetric, and in this case computing the inverse is just computing the transpose. It would not be a good design to choose to compute the inverse, as it is much more costly and it may incur in unnecessary precision errors.\r\n- Leave the 32 bit implementation, or implement both a 32 and 64 bit implementation, or just the 64 bit implementation?\r\n- Just use a pade7 (or pade13 for 64-bits) when handling batches of matrices (it might incur in a loss of precision when the elements in the batch have small entries) or split the batch into the subbatches of matrices that need the different approximations and then put them together again (more computationally costly).\r\n\r\nIf someone tells me which of these implementation decisions follow closer the current Pytorch guidelines, I'm happy to implement them myself and PR them"", '@vishwakftw could you guide @Lezcano ', ""@soumith I'd be happy to.\r\n\r\n@Lezcano perhaps you could try implementing the matrix exponential first, and then we can follow up with the gradient implementation.\r\n\r\nRegarding your design choices: it would be preferable to have separate implementations based on precision. IIRC, we would have to make one more approximation for the 64-bit case."", 'I put together an implementation of the exponential of matrices and its gradients for arbitrary matrices, which chooses between the 32 and the 64 bit algorithm, and does some optimization when the input is a skew symmetric matrix. It also implements some (very) minimal tests \r\nhttps://github.com/Lezcano/expm\r\nIt does not implement `expm` for batches of matrices. This could be done with a bit of effort, if one wants to go for precision (choosing the best Pade approximant for each matrix in the batch) or it should be almost straightforward if one chooses to take the maximum degree Pade approximant for the whole batch.\r\n', 'I think we should go with the maximum degree Pade approximation for all matrices in the batch.\r\n\r\n@Lezcano we have our forward implementations in C++ residing in aten/src/ATen/native/LinearAlgebra.cpp (or a separate file MatrixFunctions.cpp) The derivatives go in tools/autograd/templates/Functions.cpp.\r\n\r\nWould it be possible for you to port your code to C++? staticmethods will have to be converted to static functions in the file you choose to add them in.', ""I could do that, but I am afraid that I don't have the time now. I will have some time for this again in December, but I will be quite busy until then.\r\nI will leave the implementation there for now (it should be trivial to extend to batches in Pytorch) in case someone wants to go ahead and port the code. If not, I will do it in December."", ""@Lezcano thanks. If I find time, I'll try doing it myself, or I'll leave it to anyone else to do it too. Otherwise, we'll wait for you.\r\n\r\nFeel free to reach out with any questions you might have about the implementation anytime."", 'Instead of hunting for the ""best"" implementation and keep this feature being delayed, is there anything preventing us from adding a naive implementation first?  Something like:\r\n```python\r\ndef matrix_exp(matrix, symmetric=False):\r\n    if not symmetric:\r\n        raise NotImplementedError(\'matrix exp for non-symmetric matrix is not implemented\')\r\n    e, V = torch.symeig(matrix, eigenvectors=True)\r\n    return V.t() @ torch.diag(e).exp() @ V\r\n```\r\n\r\nIn general, all the matrix functions, as long as this function could be written as a power series, could be implemented using the above method\r\n```python\r\ndef matrix_func(matrix, func, symmetric=False):\r\n    if not symmetric:\r\n        raise NotImplementedError(\'matrix exp for non-symmetric matrix is not implemented\')\r\n    e, V = torch.symeig(matrix, eigenvectors=True)\r\n    return V.t() @ func(torch.diag(e)) @ V\r\n```\r\n\r\nWe can always improve later.', 'BTW, I have a working jit scripted version of this if it is relevant\r\nhttps://github.com/geoopt/geoopt/blob/master/geoopt/linalg/_expm.py\r\nBatched is here:\r\nhttps://github.com/geoopt/geoopt/blob/master/geoopt/linalg/batch_linalg.py#L50', '@ferrine Your implementation of the forward pass is correct, as I see that is an adaptation of numpy, as it is the one presented above, but it is missing the implementation of an approximation of the gradient.\r\n@zasdfgbnm  an implementation using the eigendecomposition, not only just works for symmetric matrices, but the algorithm is rather poor in terms of numerical stability.\r\n\r\nA stronger way to go forward would be taking the implementation in\r\nhttps://github.com/Lezcano/expm\r\nand translating it to C++. As I said, I might find time to do this during Christmas, but if anyone wants to chip in before that, any help is welcome. If we are willing to take an implementation in pure pytorch, rather than in C++, the implementation in that repo is already usable. In that repo there is also an implementation of the following algorithm, which should be faster in GPUs\r\nhttp://personales.upv.es/serblaza/2018Expm_Rev.pdf\r\n\r\nTo approximate the gradient, the current implementation works, but it would be more about 2x more efficient implementing the algorithm algorithm detailed in:\r\nhttps://pdfs.semanticscholar.org/e537/b375e59ca61ee5243c0dfeeefa9329f63725.pdf\r\nThis algorithm is implemented in scipy in the function `scipy.linalg.expm_frechet`, and the method `SPS`\r\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.expm_frechet.html?highlight=frechet', 'Any updates on this? ', 'I just updated today the implementation in the repo to handle batches of matrices. \r\nhttps://github.com/Lezcano/expm\r\nIt would be a matter of porting it to ATen and writing the accompanying testing. Alas, I do not have the time at the moment to do so, but maybe someone could chip in? Most of the work is already done, as the algorithms are already there and I have tested them.', ':red_circle: I am very disappointed on ~~_pytorch_~~ :confused:  for not implementing matrix log (logm in matlab), which is a very essential tool in machine learning and tensor manipulation. I am thinking to switch back to matlab.', ""I'm bumping the priority on this feature due to the amount of activity in this thread"", 'Is anyone working on it? I could give it a try.', ""Not that I know. The current state of affairs is that we have an implementation that works for batches and all in \r\nhttps://github.com/Lezcano/expm\r\n(in particular, the Taylor one, as it is faster in GPU), and that should be ported to ATen.\r\n\r\nThe implementation currently uses a poor man's version of `torch.matrix_power` that accepts a sequence of indices in the exponent term. A good implementation of the matrix exponential would go through extending first `torch.matrix_power` to handle a vector (or a tensor) in its second argument. Feel free to pop me a message if you have any questions about the algorithm."", 'Any updates on this?', '@mdanb , sorry, been busy. I want to finish something before jumping to this one. It is right next on my bucket list.', 'For what is worth, regarding the `matrix_sqrt` and `matrix_log`, both implementations rely on the real Schur form, as implemented in Scipy, so that should be implemented in PyTorch. As all these functions are analytic, the same algorithm used to compute the gradient for the `expm` as implemented in the repo above would work for these two (see lines 263 and 286 in https://github.com/Lezcano/expm/blob/master/pytorch_expm/expm_taylor.py)', '@Lezcano , thank you! You recommend the method from http://personales.upv.es/serblaza/2018Expm_Rev.pdf , right? Any recommendations for the backward step?', ""The backward step can be implemented the same way for all these functions, as they are all analytic, the algorithm employed is the same. It is based on this paper:\r\nhttps://dl.acm.org/doi/10.1137/S0895479895283409\r\nand it's implemented in pure PyTorch for a generic analytic function here:\r\nhttps://github.com/Lezcano/expm/blob/master/pytorch_expm/expm_taylor.py#L263\r\n\r\nAbout the forward method for the `expm`, yes, that's the paper I think that would works best for GPUs. I implemented their algorithm here, implementing also an exact backward step (the one mentioned before using Mathias' algorithm):\r\nhttps://github.com/Lezcano/expm/blob/master/pytorch_expm/expm_taylor.py\r\nI already implemented it to run as fast as I could, but maybe you find some other optimizations. Probably the function `matrix_power_two_batch` is the one that could be optimized the most (this is just a batched version of `matrix_power`, so it would be best to just implement this in `torch.matrix_power`)."", '@Lezcano , correct me if I am wrong, but it seems to me the paper decides the parameters based on the matrix 1-induced norm, not the vector 1-norm? It looks like in your code you use  l1 norm for a single matrix, and Frobenius norm for batches...', 'You are completely right! That frobenius norm was left there, as some previous version of torch had a bug when selecting a `p =/= 2` and selecting some custom `dim`. And yes, both norms should really be the `1`-operator norm according to the paper. It would be good to have this on Pytorch anyway, so it might be good to add it to `torch.norm`? At least the simple ones, like operator norms 1, 2, and \\infty.\r\n\r\n**Edit:** I changed the implementation in the repo to use operator norm 1 in both single and batched matrices', 'It seems to me the paper has a typo in `expm2`, there I would expect the condition `||A|| <= theta`.\r\n\r\nAlso, thinking about precision it seems to me we can just go with `T_18`. The values of `theta` are pretty small fort `T_i, i < 18` to be realizable in practice. Why? I would assume that in the context of neural networks most matrices are coming from the Gaussian distribution (say, Xavier initialization). The expected operator norm 1 has a lower bound of an expectation of max Gaussians, and in this case (Xavier initialization) it is at least `Theta(sqrt(log n))`, where `n` is the size of a matrix.\r\n\r\nSo, I propose to stick to `T_18`, it is just much more likely, and makes the implementation easier to understand/debug, and portable across CPU/CUDA (only high level uniform batch operations).', 'For batching, the current implementation is already done just with T_18 (and handling two trivial cases separately). At the moment, it is just for the case of one matrix when the other cases are used. \r\n\r\nOn the other hand, although I do think that for some cases just T_18 is going to be useful, there are other cases where the other polynomials save quite a bit of time. I do not think that the Xavier initialization (or some other standard initialization) will be used without further constraints together with the exponential as, in the case of one variable, the exponential of matrices makes the eigenvalues of the matrices ""explode"".\r\n\r\nOne example when it is not bad conditioned is when doing optimisation over the orthogonal matrices. In this case, one way to do this (and a quite efficient one) is to sample a random orthogonal matrix B, and optimising the function `A -> Bexpm(A-A.t())` with initial condition `A = 0`. In this case, if the norm of `A` remains small during some reasonable time (which is usually the case), cheaper approximations save quite a bit of time.', 'It is quite unlikely that Gassian random matrices have large eigenvalues, actually.\r\nWhat is your use-case, by the way?\r\n\r\nAnyway, I do believe it is imperative to bring the feature first, and then optimize as needed.\r\nThe efficient batched version (treating each matrix separately) will require device-specific code for sure, and it gets more involved there with TensorIterator and stuff...', 'True, Gaussian random matrices do not have large eigenvalues on average, but if you backprop throu `expm`, the gradients will make them very large in no time, same as it happens with the regular exponential.\r\n\r\nMy use case is exactly the one I outlined above, but in more generality: Optimisation on manifolds. Optimisation with orthogonal constraints is just one instance of it.\r\n\r\nAnd yes, you are right, maybe it is best to have a version that can be shipped, and then, from there, implement the other optimisations. I believe that this should not be too difficult after the first version is finished.', '@Lezcano , could you please have a look at the mathematical part of my PR? It is ready for review.', ""I left a review in the PR---it's looking very good.\r\nWe would just be missing a way to implement the batches in parallel as, at the moment, that sequential operation will surely be a bottleneck for reasonable batches. I have also sent an email to the authors of the paper asking them about that numerical mismatch.\r\n\r\nIt would be good if someone with experience in AT and in optimising these processes could take a look at it, as I am not very familiar with AT. cc: @vishwakftw ?"", 'When it comes to the matrix square root, would you be interested in the negative square root? I guess it is useful to have, especially for psd matrices, coordinate transformations, decorrelations, what do you think? It also makes the implementation a package deal if implemented via the matrix sign function.', 'I have not looked into the implementation of the square root and related functions, but they would surely be very useful! I think that, if it\'s implemented in terms of the real Schur decomposition would allow to directly implement arbitrary fractional powers, although this could be a separate algorithm.\r\n\r\nAs PyTorch does not currently support complex numbers, this would have to be a real Schur decomposition, for which some algorithms exist, although I don\'t know how fast are they on GPU. In any case, having an implementation of the real Schur decomposition would make ""fairly straightforward"" the implementation of both A^p where p is a rational number and the logarithm. Higham has several articles on these type of algorithms which are the (CPU) standard, cf.\r\n\r\n    Nicholas J. Higham and Lijing lin (2011) ‚ÄúA Schur-Pade Algorithm for Fractional Powers of a Matrix.‚Äù SIAM Journal on Matrix Analysis and Applications, 32 (3). pp. 1056-1078. ISSN 0895-4798\r\n\r\n    Awad H. Al-Mohy and Nicholas J. Higham (2012) ‚ÄúImproved Inverse Scaling and Squaring Algorithms for the Matrix Logarithm.‚Äù SIAM Journal on Scientific Computing, 34 (4). C152-C169. ISSN 1095-7197\r\n\r\nThis second one is ""similar"" to his article about using Pad√© approximants to approximate the matrix exponential.\r\n', 'Amazing, but all these things can be implemented with a sign function (fractional powers)! So, to bring things to the table fast, I will probably concentrate on this path.', '> As PyTorch does not currently support complex numbers\r\n\r\nWe support complex on master!', '@ezyang , unfortunately, some of the fundamental functionality is missing yet :(\r\n```python\r\nIn [1]: import torch\r\n\r\nIn [2]: a = torch.randn(3, 3, dtype=torch.complex128)\r\n\r\nIn [3]: torch.matmul(a, a)\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-3-7a0abd112090> in <module>\r\n----> 1 torch.matmul(a, a)\r\n\r\nRuntimeError: _th_addmm_out not supported on CPUType for ComplexDouble\r\n\r\n```', 'Yeah, addmm has been blocked on getting ported to ATen first :( https://github.com/pytorch/pytorch/issues/24679', 'I will try to use algorithms which do not rely on the explicit complex structure, and the matrix sign function seems like a good solution. The complex support will be enabled automatically once `matmul` supports complex numbers.', ""Are there any updates on this? I'm looking forward to the matrix log!"", 'Sorry, no update on matrix log or matrix sqrt. We are planning to review better SciPy linear algebra compatibility in the near future, however.', ""@nikitaved has done work on matrix sqrt, which should be unblocked once his `linalg.lstsq` PR is in - it's still a big job after that though, so no ETA. No one has worked on matrix log AFAIK. "", 'also looking forward to matrix sqrt! thanks for sharing an update on this', ""The issue with the matrix sqrt that I see is its implementation for CUDA, it is not so straightforward to translate the SciPy's implementation to the gpu (no Schur decomposition, dependence in loops). It is possible, however, to implement an iterative algorithm either only for the GPU, or for both the CPU and CUDA. If we agree on iterative algorithms, then I could start working on it.""]",[],[],0,0
312,pytorch,30965,open,JIT breaks with postponed annotations,"Targetting the correct issue this time, sorry for the noise

## üêõ Bug

As per [PEP 563 (Postponed Evaluation of Annotations)](https://www.python.org/dev/peps/pep-0563), typing annotations are not automatically evaluated as definition time starting with python 3.7 when using .

The solution is to avoid using   directly in [](https://github.com/pytorch/pytorch/blob/master/torch/jit/_recursive.py#L74) but to call [](https://docs.python.org/3.7/library/typing.html#typing.get_type_hints__future__` call will also be correctly evaluated. Should I make a PR?

## Testcase



This fails with traceback



cc @suo",oncall: jit triaged,"['We would be very interested in a PR that fixes this behavior :). However, we need to keep in mind that we support the last three major versions of Python (3.6, 3.7, and 3.8), so it may complicate the issue.\r\n\r\ncc @driazati who may be interested', ""Actually, I have just checked and `get_type_hints` is available in 3.6, so I'll have a go at it :-)""]","['python\r\nfrom __future__ import annotations\r\nfrom typing_extensions import Final\r\nimport torch\r\nimport torch.jit\r\n\r\nclass ModuleWithFinal(torch.nn.Module):\r\n    const: Final[int]\r\n    def __init__(self, const):\r\n        super().__init__()\r\n        self.const = const\r\n        \r\nmod = ModuleWithFinal(0)\r\ntorch.jit.script(mod)\r\n', 'text\r\n  File ""testcase.py"", line 14, in <module>\r\n    torch.jit.script(mod)\r\n  File ""torch/jit/__init__.py"", line 1256, in script\r\n    return torch.jit._recursive.recursive_script(obj)\r\n  File ""torch/jit/_recursive.py"", line 534, in recursive_script\r\n    return create_script_module(nn_module, infer_methods_to_compile(nn_module))\r\n  File ""torch/jit/_recursive.py"", line 293, in create_script_module\r\n    concrete_type = concrete_type_store.get_or_create_concrete_type(nn_module)\r\n  File ""torch/jit/_recursive.py"", line 236, in get_or_create_concrete_type\r\n    concrete_type_builder = infer_concrete_type_builder(nn_module)\r\n  File ""torch/jit/_recursive.py"", line 131, in infer_concrete_type_builder\r\n    if torch._jit_internal.is_final(ann):\r\n  File ""torch/_jit_internal.py"", line 645, in is_final\r\n    return ann.__module__ == \'typing_extensions\' and \\\r\nAttributeError: \'str\' object has no attribute \'__module__\'\r\n']","['from __future__ import annotations', '__annotations__', 'jit._recursive.infer_concrete_type_builder()', 'typing.get_type_hints()', ') instead, which is available since Python 3.7 and has the nice benefit that type hints that were given as strings (which is possible even without the ']",0,0
313,pytorch,11850,open,[Caffe2/Bug] Cannot enable MKL-DNN,"## Solution
A solution is found for the problem, but maybe a bug in Caffe2

MKL-DNN can be enabled by first compiling the PyTorch. Then turn on MKL by

and recompile PyTorch. Then MKL-DNN is enabled.

## Issue description

Trying to accelerate caffe2 inference with MKL-DNN.

The MKL-DNN lib can be detected:

But mkl operators are not compiled

And MKL-DNN cannot be found after installation since

returns False.

Also, I try to enable MKL by changing the CMakeLists.txt:

It acutally cause an error:


## System Info
PyTorch version: 1.0.0a0+98aebed
Is debug build: No
CUDA used to build PyTorch: None

OS: Ubuntu 18.04.1 LTS
GCC version: (Ubuntu 7.3.0-16ubuntu3) 7.3.0
CMake version: version 3.10.2

Python version: 2.7
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip] numpy (1.15.1)
[pip] torch (1.0.0a0+98aebed)
[conda] Could not collect

",caffe2,"['Please check with `-DBLAS=""MKL""`. ', 'Hi @deepali-c , Thanks for the response!\r\n\r\n```-DBLAS=""MKL""``` seems does not change the compilation.\r\n\r\nI add this option in the caffe2 build instruction:\r\n```bash\r\ndiff --git a/tools/build_pytorch_libs.sh b/tools/build_pytorch_libs.sh\r\nindex 37d816775..65e639359 100755\r\n--- a/tools/build_pytorch_libs.sh\r\n+++ b/tools/build_pytorch_libs.sh\r\n@@ -300,6 +300,7 @@ function build_caffe2() {\r\n                       -DCUDNN_INCLUDE_DIR=$CUDNN_INCLUDE_DIR \\\r\n                       -DCUDNN_LIB_DIR=$CUDNN_LIB_DIR \\\r\n                       -DCUDNN_LIBRARY=$CUDNN_LIBRARY \\\r\n+                      -DBLAS=""MKL"" \\\r\n                       -DUSE_MKLDNN=$USE_MKLDNN \\\r\n                       -DMKLDNN_INCLUDE_DIR=$MKLDNN_INCLUDE_DIR \\\r\n                       -DMKLDNN_LIB_DIR=$MKLDNN_LIB_DIR \\\r\n```', 'Hi @deepali-c I have [solved the problem](https://github.com/pytorch/pytorch/issues/11850#issue-361548155), but it might be a bug in Caffe2.', 'I guess the problem is as follows, resided in Caffe2 ...\r\n\r\n\r\n```\r\n CMake Warning at caffe2/CMakeLists.txt:198 (add_library):\r\n   Cannot generate a safe runtime search path for target caffe2 because there\r\n   is a cycle in the constraint graph:\r\n\r\n     dir 0 is [/usr/local/cuda/lib64]\r\n     dir 1 is [/usr/local/lib]\r\n       dir 4 must precede it due to runtime library [libmkldnn.so.0]\r\n     dir 2 is [/usr/lib/x86_64-linux-gnu/openmpi/lib]\r\n     dir 3 is [/opt/intel/mkl/lib/intel64]\r\n     dir 4 is [....../pytorch/build/lib]\r\n       dir 1 must precede it due to runtime library [libmkldnn.so.0]\r\n\r\n   Some of these libraries may not be found correctly.\r\n```\r\n', '@jiapei100 Truly, I think some contributors are working on solving this problem, e.g., [this issue](https://github.com/pytorch/pytorch/pull/11853). I am trying to do some performance profiling of MKL-DNN so I am now writing my own code based on MKL-DNN directly.']","['bash\r\ndiff --git a/CMakeLists.txt b/CMakeLists.txt\r\nindex 827121b1f..235d00bd6 100644\r\n--- a/CMakeLists.txt\r\n+++ b/CMakeLists.txt\r\n@@ -111,6 +111,7 @@ option(USE_SYSTEM_EIGEN_INSTALL\r\n option(USE_TENSORRT ""Using Nvidia TensorRT library"" OFF)\r\n option(USE_ZMQ ""Use ZMQ"" OFF)\r\n option(USE_ZSTD ""Use ZSTD"" OFF)\r\n+option(USE_MKL ""Use MKL"" ON)\r\n option(USE_MKLDNN ""Use MKLDNN"" OFF)\r\n option(USE_IDEEP ""Use IDEEP interface in MKL BLAS"" ON)\r\n option(USE_MKLML ""Use MKLML interface in MKL BLAS"" ON)\r\n', '\r\n-- Found MKLDNN: /home/jiecaoyu/.local/include  \r\n-- Found MKLDNN      (include: /home/jiecaoyu/.local/include, library: /home/jiecaoyu/.local/lib/libmkldnn.so)\r\n', '\r\n-- Excluding mkl operators as we are not using mkl\r\n', 'python\r\nfrom caffe2.python import workspace\r\nworkspace.C.has_mkldnn\r\n', 'bash\r\ndiff --git a/CMakeLists.txt b/CMakeLists.txt\r\nindex 827121b1f..235d00bd6 100644\r\n--- a/CMakeLists.txt\r\n+++ b/CMakeLists.txt\r\n@@ -111,6 +111,7 @@ option(USE_SYSTEM_EIGEN_INSTALL\r\n option(USE_TENSORRT ""Using Nvidia TensorRT library"" OFF)\r\n option(USE_ZMQ ""Use ZMQ"" OFF)\r\n option(USE_ZSTD ""Use ZSTD"" OFF)\r\n+option(USE_MKL ""Use MKL"" ON)\r\n option(USE_MKLDNN ""Use MKLDNN"" OFF)\r\n option(USE_IDEEP ""Use IDEEP interface in MKL BLAS"" ON)\r\n option(USE_MKLML ""Use MKLML interface in MKL BLAS"" ON)\r\n', '\r\n...\r\n/home/jiecaoyu/LIBS/pytorch/aten/src/TH/THAllocator.cpp:397:1: error: prototype for ‚ÄòTHMapAllocator::THMapAllocator(WithFd, const char*, int, int)‚Äô does not match any in class ‚ÄòTHMapAllocator‚Äô\r\n THMapAllocator::THMapAllocator(WithFd, const char *filename, int fd, int flags) {\r\n ^~~~~~~~~~~~~~\r\nIn file included from /home/jiecaoyu/LIBS/pytorch/aten/src/TH/THAllocator.cpp:1:0:\r\n/home/jiecaoyu/LIBS/pytorch/aten/src/TH/THAllocator.h:41:3: error: candidates are: THMapAllocator::THMapAllocator(THMapAllocator&&)\r\n   THMapAllocator(THMapAllocator&&) = delete;\r\n   ^~~~~~~~~~~~~~\r\n/home/jiecaoyu/LIBS/pytorch/aten/src/TH/THAllocator.h:39:3: error:                 THMapAllocator::THMapAllocator(const THMapAllocator&)\r\n   THMapAllocator(const THMapAllocator&) = delete;\r\n   ^~~~~~~~~~~~~~\r\n/home/jiecaoyu/LIBS/pytorch/aten/src/TH/THAllocator.h:38:3: error:                 THMapAllocator::THMapAllocator(WithFd, const char*, int, int, size_t)\r\n   THMapAllocator(WithFd, const char *filename, int fd, int flags, size_t size);\r\n   ^~~~~~~~~~~~~~\r\n/home/jiecaoyu/LIBS/pytorch/aten/src/TH/THAllocator.cpp:393:1: error:                 THMapAllocator::THMapAllocator(const char*, int, size_t)\r\n THMapAllocator::THMapAllocator(const char *filename, int flags, size_t size) {\r\n ^~~~~~~~~~~~~~\r\n/home/jiecaoyu/LIBS/pytorch/aten/src/TH/THAllocator.cpp:401:52: error: destructors may not have parameters\r\n THMapAllocator::~THMapAllocator(THMapAllocator* ctx) {}\r\n                                                    ^\r\n/home/jiecaoyu/LIBS/pytorch/aten/src/TH/THAllocator.cpp:401:1: error: redefinition of ‚ÄòTHMapAllocator::~THMapAllocator()‚Äô\r\n THMapAllocator::~THMapAllocator(THMapAllocator* ctx) {}\r\n ^~~~~~~~~~~~~~\r\nIn file included from /home/jiecaoyu/LIBS/pytorch/aten/src/TH/THAllocator.cpp:1:0:\r\n...\r\n']",[],0,0
314,pytorch,10170,open,build error,"## Issue description
Failed to build the caffe2

## Code example

CMake 3.11.3

## System Info
- Caffe2:

******** Summary ********
General:
  CMake version         : 3.11.3
  CMake command         : C:/Program Files/CMake/bin/cmake.exe
  Git version           : v0.1.11-9628-gf908b2b91-dirty
  System                : Windows
  C++ compiler          : C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe
  C++ compiler version  : 19.0.24215.1
  BLAS                  : Eigen
  CXX flags             : /DWIN32 /D_WINDOWS /W3 /GR /EHsc -DONNX_NAMESPACE=onnx_c2 /MP /bigobj
  Build type            : Release
  Compile definitions   : 
  CMAKE_PREFIX_PATH     : 
  CMAKE_INSTALL_PREFIX  : C:/dev/pytorch/build/install

  BUILD_CAFFE2          : ON
  BUILD_ATEN            : OFF
  BUILD_BINARY          : ON
  BUILD_CUSTOM_PROTOBUF : ON
    Link local protobuf : ON
  BUILD_DOCS            : OFF
  BUILD_PYTHON          : OFF
  BUILD_SHARED_LIBS     : ON
  BUILD_TEST            : OFF
  USE_ASAN              : OFF
  USE_ATEN              : OFF
  USE_CUDA              : ON
    CUDA static link    : OFF
    USE_CUDNN           : ON
    CUDA version        : 8.0
    cuDNN version       : 7.0.5
    CUDA root directory : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0
    CUDA library        : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0/lib/x64/cuda.lib
    cudart library      : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0/lib/x64/cudart_static.lib
    cublas library      : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0/lib/x64/cublas.lib;C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0/lib/x64/cublas_device.lib
    cufft library       : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0/lib/x64/cufft.lib
    curand library      : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0/lib/x64/curand.lib
    cuDNN library       : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0/lib/x64/cudnn.lib
    nvrtc               : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0/lib/x64/nvrtc.lib
    CUDA include path   : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0/include
    NVCC executable     : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0/bin/nvcc.exe
    CUDA host compiler  : $(VCInstallDir)bin
    USE_TENSORRT        : OFF
  USE_ROCM              : OFF
  USE_EIGEN_FOR_BLAS    : ON
  USE_FFMPEG            : OFF
  USE_GFLAGS            : ON
  USE_GLOG              : ON
  USE_GLOO              : OFF
  USE_LEVELDB           : OFF
  USE_LITE_PROTO        : OFF
  USE_LMDB              : OFF
  USE_METAL             : OFF
  USE_MKL               : 
  USE_MOBILE_OPENGL     : OFF
  USE_MPI               : OFF
  USE_NCCL              : OFF
  USE_NERVANA_GPU       : OFF
  USE_NNPACK            : OFF
  USE_OBSERVERS         : OFF
  USE_OPENCL            : OFF
  USE_OPENCV            : OFF
  USE_OPENMP            : OFF
  USE_PROF              : OFF
  USE_REDIS             : OFF
  USE_ROCKSDB           : OFF
  USE_ZMQ               : OFF
  Public Dependencies  : Threads::Threads;gflags;glog::glog
  Private Dependencies : cpuinfo;onnxifi_loader
Configuring done
Generating done
",caffe2,[],[],[],0,0
315,pytorch,18496,open,Can't compile c10::optional values() or operator-> in .cu file,"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->

Our implementation of optional doesn't seem to always be compilable in .cu files.  Compiling this commit: https://github.com/gchanan/pytorch/commit/6666ff1083a55e90b230d2269c13a3d30af8c0f4

gives me the following error:


I did a bit of digging and the problem seems to be with the constexpr functions, e.g.:
https://github.com/pytorch/pytorch/blob/654e59fcac4a9d4bf0b48306e1d7f7be5b7e40b1/c10/util/Optional.h#L600-L604

basically, for nvcc to compile this in a .cu file,  has to be a .  This doesn't seem possible with the constraint that bad_optional_access derives from  because  does not have  constructors.

Note that this only applies to calls to .  Calls to  seem to fail for other reasons that I didn't dig into.  But  passes fine, so the solution is to use that until we fix the issue.",,['Is this only on CUDA 10.1?\r\nSame bug as https://github.com/davisking/dlib/commit/02ed083c4c8f2d9c5edb5df52523326d13b9d7ae'],[],"['\r\nOptional.h(602): error: identifier ""Typeinfo for  ::c10::bad_optional_access"" is undefined in device code\r\n', 'bad_optional_access(...)', 'constexpr', 'std::exception', 'std::exception', 'constexpr', 'value()', 'operator->', 'operator*']",0,0
316,pytorch,1051,open,DataParallel does not correctly handle running variables in batch norm,"Running variables accumulated on non-master GPUs are lost, and in the next step running variables accumulated on the master GPU are broadcast to non-master GPUs. Convergence-wise it does not make much difference;-), but it is still a bug. 
Also, device_ids in DataParallel should be checked to not contain the same values, otherwise stuff like this will break: 
https://github.com/pytorch/pytorch/blob/master/torch/backends/cudnn/rnn.py#L38-L43
If creating replicas on the same GPU is actually intended to be supported, then the above referenced lines that create a dictionary indexed by current_device constitute a bug. 
Both this issues ultimately are caused but not well defined behaviour of DataParallel with respect to stateful modules, of which batchnorm and recurrent nets with cudnn backend are examples. 

cc @ngimel @albanD @mruberry",module: cuda module: data parallel triaged,"[""This does seem bad.  For large networks like resnet50 where you have small batches, each GPU has a poor estimate of the norm, especially at the beginning of training.  We have found that allowing each GPU to run it's own batch norm can be more stable and better, calculating a total batch norm across all GPUs.  Final convergence is likely not impacted much since the final norm will work itself out after a few 10s of epochs.  \r\n\r\nBut I agree with @ngimel that this looks like a bug in general for anything with state even if we may argue about batch norm specifically."", 'It would show up in testing only, no impact on training whatsoever (running vars are not used in training). ', '1. It\'s probably simpler to disallow specifying the same device multiple times in DataParallel. I\'m not sure there are sufficient use cases to warrant supporting it.\r\n\r\n2. It would be better to accumulate running_mean/var, but this requires autograd to support ""backward"" ops on things that aren\'t optimized by SGD.', ""2. I agree that accumulating running_mean/var across GPUs is not trivial in the current autograd model. But even if we decide not to accumulate them (arguably, over many epochs even the master GPU will see enough different samples to come up with good running estimates), current implementation broadcasting them to all the GPUs only to immediately discard whatever updates other GPUs made to them makes no sense. May be they should be not buffers, but some members of module ```__dict__```, like, e.g. random states in rnns. \r\nAlso, I think module replication in DataParallel should be better documented. Random states in rnn work now only because ```__dict__``` is shallow copied in replicate, it would break if ```__dict__``` were deep copied. But what if some module expects ```__dict__``` to be deep copied to work properly? There's nothing in the docs to say that it wouldn't work.  "", 'Shallow copy is mentioned in the last paragraph of the [`DataParallel` docs](http://pytorch.org/docs/nn.html#torch.nn.DataParallel).', 'I read it as referring to inputs, not to module attributes. ', '2. We could replace them with ""dummy"" buffers on the correct GPU during training. I\'m not sure it will save enough time to make it a worthwhile optimization. At one point, we didn\'t broadcast the running_mean/var which led to other replicas using the original buffers (on the wrong GPU) which slowed everything down.', ""Yes, that's what we had for random states in rnn, that's why rnn module now has a dict with rng state per GPU. Broadcasting buffers is ~1.5% of the runtime for resnet with 64/GPU, would be more for smaller batch sizes or future architectures. "", 'Sorry, but I don\'t fully understand what you mean. When I use ""register_buffer"" to get a buffer and I update it during forward, do you mean that I cannot get the correct corresponding value on each GPU if I use DataParallel? Does it mean all the GPUs have to use the buffers on GPU:0?']",[],[],0,0
317,pytorch,4181,open,Fused RNN refactor plan,"This ticket is to track our plan to refactor the fused RNN API (which makes use of the CuDNN implementation of RNNs).

**Why is this difficult?** There are a number of factors which make fused RNNs unusual, compared to most of the other differentiable operations in PyTorch

* It requires an unusually large, structured series of weights. Most differentiable operators have a fixed number of weights, but an RNN for an entire sequence must have weights for every layer of the RNN. To make matters worse, each layer needs not one but two tensors; the weight and the bias. No other operator in PyTorch behaves like this.

* CuDNN requires these weights and inputs to be packed in a particular way.  The required packing operations are frequently reported by users as an extremely confusing aspect of PyTorch.

* Not only do the weights vary depending on the type of RNN, so do the hidden tensors. If you are an LSTM, you have both hx and cx; for other RNNs, only hx is needed.

* RNN with dropout is a stateful API, which requires dropout descriptors to be passed between invocations to handle randomness.

**What do we want to do?** Here are the desired goals of the RNN refactor:

* Make CuDNN RNN available from ATen

**Design ideas.**

* The ATen API will take two tensors,  and , with  being undefined tensor for non-LSTM networks.

cc @csarofeen @ptrblck",module: cudnn triaged,"[""You don't actually need weights for every element of the time sequence, weights are reused across time steps. Otherwise, we hear you ;-)"", 'May #4145 be a candidate for this refactoring process as well?', ""Possible; I haven't studied the CuDNN constraints closely enough to know if this is possible."", ""@ezyang I think instead of **weights**, you are talking about the **intermediate (temporary) variables** introduced by the RNN's computing. "", 'Yes I do, thanks for the clarification! EDIT: Actually, I just mean layers!', 'This is blocked on #4564, because the unpacked RNN signature requires both TensorList and Tensor.']",[],"['hx', 'cx', 'cx']",0,0
318,pytorch,25381,open,regarding builtin_function_or_method,"## üöÄ Feature
<!-- A clear and concise description of the feature proposal -->
some way to see the code for builtin_function_or_method
## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->
consider this scenario
1) I am using google colab, I want to see the implementation of nn.Dropout
2) I ctrl+click on nn.Dropout, it takes me to dropout.py, where I find Dropout class, which returns

3) so, I ctrl+click on F.dropout, it takes me to functional.py, where I see this

4) now, I am stuck, because this is a builtin, and only way for me to look at its code, is to search for this in github.
5) same thing happens when I want to see the implementation of nn.CrossMapLRN2d, it redirects to torch.group_norm, which is builtin.
6) similarly for view, permute, this way to understand what these builtins are doing becomes difficult

because of this I raised an issue in google colab, to provide a way to look at builtin_function_or_method, they told 

""
No, there's no way to get directly to the source in this case.

In particular, for a .so, it could be a cython file, or it could be another kind of C extension (built via vanilla C/C++ code, or SWIG'd in, or something else completely). All we have is the installed package -- there's no way for us to trace that back to source, and in cases like this (torch is distributed as a .whl file) the source isn't included.
""


## Pitch

<!-- A clear and concise description of what you want to happen. -->

## Alternatives

<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->

## Additional context

<!-- Add any other context or screenshots about the feature request here. -->
",feature low priority triaged,"[""Unlikely we will ever find resources to implement this feature. I'm leaving it open in case of open source community contributions.""]",[],"['\r\nF.dropout(input, self.p, self.training, self.inplace)\r\n', '\r\nreturn (_VF.dropout_(input, p, training)\r\n            if inplace\r\n            else _VF.dropout(input, p, training))\r\n']",0,0
319,pytorch,24915,open,Shared Dataset Functionality,"## üöÄ Feature

We want to build a unified data pipeline interface that offers building blocks for others to build on with the following objectives:
* Standardize datasets across domains.
* Offer flexible building blocks that can be combine to obtain other datasets.
* Enable datasets that do not fit in memory.
* Share code among domains.
* Facilitate parallel loading and processing of data.
* Decouple data loading and preprocessing/transformation.
* Offer static typing for datasets

## Motivation

* The Domains currently each have their own non-standard dataset structure that may also download the data. This duplicate efforts and adds complexity to the user.
* A common bottleneck when generating datasets is reading the data. We want to offer an interface that enables reading the data and running initial preprocessing while maximizing available computing resources utilization.
* We may want to leverage specialize libraries such as NVIDIA DALI.

## Additional Information

* [torch.utils.data](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/)
* [tf.data](https://www.tensorflow.org/beta/guide/data) (e.g. uses dictionary for data point iteration)
* fast.ai's [basic_data](https://docs.fast.ai/basic_data.html) and [data_block](https://docs.fast.ai/data_block.html)
* [tnt](https://github.com/pytorch/tnt/blob/master/torchnet/dataset/dataset.py)
* [torchnet](https://github.com/torchnet/torchnet/tree/master/dataset)
* [~~torchdata~~](https://pypi.org/project/torchdata/)

Datasets:
* pytorch/text#624 pytorch/text#610 pytorch/audio#303 new datasets in domains
* pytorch/vision#1193 wants to select which metadata to return
* Internal: [overview](https://fb.quip.com/vlWwA35cmq0t) [torchtext](https://fb.quip.com/LncwAsC1cUZt) [core](https://fb.quip.com/B0PeACndlZEE) [torchvision](https://fb.quip.com/WGsUApsce6xN)
* [safe datasets](https://github.com/msamogh/nonechucks)

Dataloader:
* [torchaudio background iterator](https://github.com/pytorch/audio/blob/master/torchaudio/datasets/utils.py#L314)
* #24915 wants to re-use worker processes
* [FastDataLoader](https://github.com/pytorch/pytorch/issues/15849#issuecomment-573921048)
* [python 3.8 shared memory](https://docs.python.org/3/library/multiprocessing.shared_memory.html)
* Internal: [torchdata](https://fb.quip.com/ekJJAsYqMG7X) [gil](https://docs.google.com/document/d/1InJP79dWTIYj-xGVU65Y2r-K2HeL6t2l1xGDfKTU4Rw/edit#) [experiment](https://fb.quip.com/imVLAOdyJfAI) [DataLoader+Iterable](https://fb.workplace.com/groups/2162019300778793/permalink/3398854433474998/)

Features:
* #12672 wants to move collate_fn functionality to datasets
* #26547 wants distributed random sampling
* #28743 for sampler for iterable datasets
* pytorch/vision#1315 wants to apply an instance of random transform sequence to many images

cc @SsnL @fmassa @zhangguanheng66 @vincentqb @mrshenli ",better-engineering module: dataloader triaged,"['An additional wish: unify transforms interface. One idea: make them regular autograd functions / modules. This enables writing them more shared between domains (imagine various forms of cutout / warps, synthetic noise), moving them from cpu to gpu, performing them at other parts of the model, etc', 'all datasets are not moving to core pytorch. If you want, create a `torchdata` for it and make domain APIs depend on `torchdata`. ', '> all datasets are not moving to core pytorch. If you want, create a `torchdata` for it and make domain APIs depend on `torchdata`.\r\n\r\nOne of the plans is to keep the raw data links and utils in pytorch/data and have the preprocessing func and associated ops in each domain. ', 'cc @SsnL  - what is your take on this?', ""I'm wonder what functionalities are planned for `torchdata`. Most of the proposed features sound already doable nowadays. I think additional explanation on what the exact needs are and why the current code can't do the job will be helpful for discussion.\r\n\r\nFor example,\r\n\r\n>  a core download feature, within PyTorch\r\n\r\nsounds similar to `torch.hub`'s download utility function\r\n\r\n> a core raw data loading (e.g. ByteTensor iterator), within PyTorch\r\n\r\n sounds doable just with the current data loading infra.\r\n"", '@soumith Any thoughts on my suggestion about `transforms` interface? (i.e. making them normal pytorch functions / modules to support better reuse)', '@vadimkantorov \r\n> Any thoughts on my suggestion about transforms interface? (i.e. making them normal pytorch functions / modules to support better reuse)\r\n\r\nYes, I agree, and torchvision is going to be doing this, see https://github.com/pytorch/vision/issues/1375', 'Thoughts from offline chat with @taylorgordon20\r\n\r\nHow to help in debugging in a natural python way?\r\n* Could use maps (e.g. `Map(read, get_line)`) instead of generators.\r\n* Do we want static type analysis?\r\n* Enforce structure between generators?\r\n* `tensorflow_datasets/image/coco.py` has ""features"" for typing\r\n* [facebook pyre](https://pyre-check.org/) may support templating and [typed dictionary](https://www.python.org/dev/peps/pep-0589/) check in functions? \r\n\r\nWishes?\r\n* Batched\r\n* Lazy better than Eager for data loading\r\n* Async preferred over Sync (e.g. use CPU while GPU is busy, mutiple hosts)\r\n* Prefer imperative over declarative\r\n* Serializable (for pre-processing in particular) can be done in a few ways: torchscript, etc\r\n* Shuffle, Sample\r\n* Save, cache transformed data, memoization (available for free for flows?)']",[],[],0,0
320,pytorch,28245,open,PyTorch RPC should expose critical metrics to the application.,"## üöÄ Feature

Context for Model Parallel: https://github.com/pytorch/pytorch/issues/23110

## Motivation

When applications are using complex distributed primitives like RPC, RRef and Distributed Autograd, debugging issues can be cumbersome. We should have a way of exposing metrics to applications. This could simply be a  API that returns information about various things. The full list of metrics needs to be decided, although a few examples could be number of owner rrefs, number of user rrefs, RPC latency, Distributed autograd latency etc.

cc @ezyang @gchanan @zou3519 @jerryzh168 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528",feature module: rpc triaged,"['@smessmer I believe you are supposed to put at least one module on an issue before marking it as triaged.', 'So... is model parallel the same team as distributed? If not, we should promote model-parallel to a top level label, same as distributed. ', 'Updated `topic: model-parallel` to `module: rpc`', 'If the high-priority label means that it should be shipped by the next release of pytorch, then we should remove the label, since this will not be ready in time for the release', ""@rohan-varma no, it doesn't necessarily mean that. It's more of a way to make sure we don't forget about these bugs; we do metrics on high priority issues, we don't on non-high priority issues.""]",[],"['def get_metrics() -> Dict[str, int]']",0,0
321,pytorch,2001,open,Implement similar PyTorch function as model.summary() in keras?," in keras gives a very fine visualization of your model and it's very convenient when it comes to debugging the network. Can we try to implement something like it in PyTorch?

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @albanD @mruberry",feature function request module: nn triaged,"[""What kind of information that's not in `str(model)` would you like to see? Output shapes?"", 'Sure you can! Here\'s something to get you started. (Adapted from other code, so it\'s not tested in the wild). Note that you HAVE to know the input size, and you HAVE to make a forward pass through the network. Those are the only reqs I think.\r\n\r\n```python\r\n    def summary(input_size, model):\r\n        def register_hook(module):\r\n            def hook(module, input, output):\r\n                class_name = str(module.__class__).split(\'.\')[-1].split(""\'"")[0]\r\n                module_idx = len(summary)\r\n\r\n                m_key = \'%s-%i\' % (class_name, module_idx+1)\r\n                summary[m_key] = OrderedDict()\r\n                summary[m_key][\'input_shape\'] = list(input[0].size())\r\n                summary[m_key][\'input_shape\'][0] = -1\r\n                summary[m_key][\'output_shape\'] = list(output.size())\r\n                summary[m_key][\'output_shape\'][0] = -1\r\n\r\n                params = 0\r\n                if hasattr(module, \'weight\'):\r\n                    params += th.prod(th.LongTensor(list(module.weight.size())))\r\n                    if module.weight.requires_grad:\r\n                        summary[m_key][\'trainable\'] = True\r\n                    else:\r\n                        summary[m_key][\'trainable\'] = False\r\n                if hasattr(module, \'bias\'):\r\n                    params +=  th.prod(th.LongTensor(list(module.bias.size())))\r\n                summary[m_key][\'nb_params\'] = params\r\n                \r\n            if not isinstance(module, nn.Sequential) and \\\r\n               not isinstance(module, nn.ModuleList) and \\\r\n               not (module == model):\r\n                hooks.append(module.register_forward_hook(hook))\r\n        \r\n        # check if there are multiple inputs to the network\r\n        if isinstance(input_size[0], (list, tuple)):\r\n            x = [Variable(th.rand(1,*in_size)) for in_size in input_size]\r\n        else:\r\n            x = Variable(th.rand(1,*input_size))\r\n\r\n        # create properties\r\n        summary = OrderedDict()\r\n        hooks = []\r\n        # register hook\r\n        model.apply(register_hook)\r\n        # make a forward pass\r\n        model(x)\r\n        # remove these hooks\r\n        for h in hooks:\r\n            h.remove()\r\n\r\n        return summary\r\n```\r\nHere\'s an example of Keras summary, FYI:\r\n\r\n<img width=""562"" alt=""screen shot 2017-07-07 at 12 55 01 pm"" src=""https://user-images.githubusercontent.com/13004360/27968068-89234e14-6313-11e7-92dc-92c31b8e28b9.png"">\r\n', '@apaszke `str(output)` works well for most cases but missed out on some `output_shape` is one of them\r\n@ncullen93 The code looks/works well. Can we think about a PR with some code refactoring to this?', 'Hello, I am new to pytorch and contributing to it. Can I try this one out?', ""@aditya1702  yeah sure, as my information goes till now we don't have any functions to see the output_size and something similiar to keras model.summary"", ""I think it would be useful to add hooks to the output of str(model).\r\nI use str(model) in my logging system and currently I can't know whether my models used weight normalizations or not since it's a hook."", 'After @ncullen93 posted his code, I added model summary to my local build of pytorch. Recently decided to clean the code up a little and make a PR. Can hopefully merge soon.  ', 'Hi @isaykatsman the implementation is nice. Thanks.', ""@ncullen93 \r\nIt that 'Variable' at the line `x = Variable(th.rand(1,*input_size))` a package?\r\nHow can I deal with this error?\r\n```\r\nglobal name 'Variable' is not defined\r\n```"", '`from torch.autograd import Variable`', 'I did some [modification ](https://gist.github.com/HTLife/b6640af9d6e7d765411f8aa9aa94b837)on the print out style to fit the style of Keras.  Hope it helps.\r\n\r\n![_031618_024018_pm](https://user-images.githubusercontent.com/4699179/37507094-f99ace62-2927-11e8-8162-eb2f87ac8c91.jpg)\r\n![_031618_023944_pm](https://user-images.githubusercontent.com/4699179/37507096-fb00beba-2927-11e8-9beb-e8ebb3d52aaa.jpg)\r\n\r\n\r\n(2018/05/18 update)\r\nsksq96 re-organized the code into python package [sksq96/pytorch-summary](https://github.com/sksq96/pytorch-summary)', ""This doesn't work for LSTMs where the output is a tuple."", '@ncullen93  It does not work with sequential() function. Do you have a solution for this?\r\n\r\n```\r\nclass Discriminator(nn.Module):\r\n    def __init__(self, in_channels=3):\r\n        super(Discriminator, self).__init__()\r\n\r\n        def discriminator_block(in_filters, out_filters, normalize=True):\r\n            """"""Returns downsampling layers of each discriminator block""""""\r\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\r\n            if normalize:\r\n                layers.append(nn.InstanceNorm2d(out_filters))\r\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\r\n            return layers\r\n\r\n        self.model = nn.Sequential(\r\n            *discriminator_block(in_channels, 64, normalize=False),\r\n            *discriminator_block(64, 128),\r\n            *discriminator_block(128, 256),\r\n            *discriminator_block(256, 512),\r\n            nn.ZeroPad2d((1, 0, 1, 0)),\r\n            nn.Conv2d(512, 1, 4, padding=1)\r\n        )\r\n\r\n    def forward(self, img):\r\n        return self.model(img)\r\n\r\n```\r\n\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File "".\\test.py"", line 118, in <module>\r\n    summary(model, (3, 28, 28))\r\n  File ""C:\\Users\\bikas\\Anaconda3\\lib\\site-packages\\torchsummary\\torchsummary.py"", line 57, in summary\r\n    model(x)\r\n  File ""C:\\Users\\bikas\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py"", line 491, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File "".\\test.py"", line 93, in forward\r\n    return self.model(img)\r\n  File ""C:\\Users\\bikas\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py"", line 491, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""C:\\Users\\bikas\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py"", line 91, in forward\r\n    input = module(input)\r\n  File ""C:\\Users\\bikas\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py"", line 493, in __call__\r\n    hook_result = hook(self, input, result)\r\n  File ""C:\\Users\\bikas\\Anaconda3\\lib\\site-packages\\torchsummary\\torchsummary.py"", line 26, in hook\r\n    params += torch.prod(torch.LongTensor(list(module.weight.size())))\r\nAttributeError: \'NoneType\' object has no attribute \'size\'\r\n```', ""I like the tree format of the builtin `__repr__()` but wanted to show the number of parameters, like: \r\n\r\n![image](https://user-images.githubusercontent.com/266841/42837079-013e5d74-89cb-11e8-9b49-25197774e743.png)\r\n\r\n```python\r\nimport sys\r\nfrom functools import reduce\r\n\r\nfrom torch.nn.modules.module import _addindent\r\n\r\ndef summary(model, file=sys.stderr):\r\n    def repr(model):\r\n        # We treat the extra repr like the sub-module, one item per line\r\n        extra_lines = []\r\n        extra_repr = model.extra_repr()\r\n        # empty string will be split into list ['']\r\n        if extra_repr:\r\n            extra_lines = extra_repr.split('\\n')\r\n        child_lines = []\r\n        total_params = 0\r\n        for key, module in model._modules.items():\r\n            mod_str, num_params = repr(module)\r\n            mod_str = _addindent(mod_str, 2)\r\n            child_lines.append('(' + key + '): ' + mod_str)\r\n            total_params += num_params\r\n        lines = extra_lines + child_lines\r\n\r\n        for name, p in model._parameters.items():\r\n            total_params += reduce(lambda x, y: x * y, p.shape)\r\n\r\n        main_str = model._get_name() + '('\r\n        if lines:\r\n            # simple one-liner info, which most builtin Modules will use\r\n            if len(extra_lines) == 1 and not child_lines:\r\n                main_str += extra_lines[0]\r\n            else:\r\n                main_str += '\\n  ' + '\\n  '.join(lines) + '\\n'\r\n\r\n        main_str += ')'\r\n        if file is sys.stderr:\r\n            main_str += ', \\033[92m{:,}\\033[0m params'.format(total_params)\r\n        else:\r\n            main_str += ', {:,} params'.format(total_params)\r\n        return main_str, total_params\r\n\r\n    string, count = repr(model)\r\n    if file is not None:\r\n        print(string, file=file)\r\n    return count\r\n```\r\n"", ""> What kind of information that's not in `str(model)` would you like to see? Output shapes?\r\n\r\nTotal parameters and total FLOPS."", 'Can i draw the forward  pass graph similar to torchviz ? \r\nIf i summary the model and get the visulization, that would be good.\r\n', ""NameError: name 'th' is not defined\r\nFor example in the line: x = [Variable(th.rand(1,*in_size)).type(dtype) for in_size in input_size]\r\nWhat is missing?"", ""@zamnius , here are the complete list of imports:\r\n\r\n```\r\nfrom torch.autograd import Variable\r\nimport torch as th\r\nfrom torch import nn as nn\r\nfrom collections import OrderedDict\r\nfrom model import UNet3D\r\n```\r\n@ShuvenduBikash , this is probably what you figured out:\r\nThe lines 15-22 should be like this as all layers dont have bias.\r\n\r\n```\r\n                if hasattr(module,'weight') and module.weight is not None:\r\n                    params += th.prod(th.LongTensor(list(module.weight.size())))\r\n                    if module.weight.requires_grad:\r\n                        summary[m_key]['trainable'] = True\r\n                    else:\r\n                        summary[m_key]['trainable'] = False\r\n                if hasattr(module,'bias') and module.bias is not None:\r\n                    params +=  th.prod(th.LongTensor(list(module.bias.size())))\r\n\r\n```"", ""I don't remember why this was originally in the low-priority bin, but I'm tentatively marking this as triage review for discussion on whether or not this should be high-pri"", ""Removing high-pri for now since this is provided by [pytorch-summary](https://github.com/sksq96/pytorch-summary), which appears to work for most cases described here (including `nn.Sequential` models).\r\n\r\nAs of now, I don't see huge benefits to maintaining this within PyTorch core and would prefer to keep it in the separate repo, but I'm open to being convinced otherwise if it doesn't support your use case, etc.""]",[],['model.summary'],0,0
322,pytorch,18998,open,torch.from_PIL() Request ?,"## üöÄ Feature
A simple method for transforming a PIL images directly into torch tensor.

## Motivation
It's frustrating to use transforms for a simple conversion between a PIL image and torch tensors and in the same time it's very easy to get tensor from numpy through  

## Pitch

Giving it a PIL images of any type and it returns a torch tensor

## Alternatives
I have considered two:
1. Transforms
2. convert it first to numpy then to torch
",feature module: vision triaged,"[""This should really not be PIL specific, but should use Python's buffer interface. It's definitely a feature that we want, but it's _very_ hard to implement reliable across all the Python versions we support."", 'Python 2 is in its way for deprecation, if this would help ^ ^']",['torch.from_numpy()'],[],0,0
323,pytorch,15771,open,Implicit conversion error in caffe2,"Hello,

While compiling generated sources(caffe2.pb.cc)  from caffe2.proto we are getting the implicit conversion error
protobuf version=v3.5.2
compiler:
 CC=aarch64-linux-android-clang \
 CXX=aarch64-linux-android-clang++




Thank you,


",caffe2,"['@orionr could you take a look? ', 'Hi @VinayKarnam - you should be using the pinned protobuf version we have in the submodule at third_party/protobuf. Is that not the case here? In fact, we try and hide protobuf from all external users of the PyTorch and Caffe2 libraries because protobuf can be so fickle. Please confirm the above. Also, what command are you using to build? Thanks.']","[""\r\n/home/armnn1/armnn-devenv/armnn/src/armnnCaff2Parser/proto/caffe2.pb.cc:2269:9: error: implicit conversion changes signedness: '::google::protobuf::int32' (aka 'int') to\r\n      'google::protobuf::uint32' (aka 'unsigned int') [-Werror,-Wsign-conversion]\r\n        static_cast< ::google::protobuf::int32>(\r\n        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/home/armnn1/armnn-devenv/armnn/src/armnnCaff2Parser/proto/caffe2.pb.cc:2282:9: error: implicit conversion changes signedness: '::google::protobuf::int32' (aka 'int') to\r\n      'google::protobuf::uint32' (aka 'unsigned int') [-Werror,-Wsign-conversion]\r\n        static_cast< ::google::protobuf::int32>(\r\n        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/home/armnn1/armnn-devenv/armnn/src/armnnCaff2Parser/proto/caffe2.pb.cc:2326:9: error: implicit conversion changes signedness: '::google::protobuf::int32' (aka 'int') to\r\n      'google::protobuf::uint32' (aka 'unsigned int') [-Werror,-Wsign-conversion]\r\n        static_cast< ::google::protobuf::int32>(\r\n        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/home/armnn1/armnn-devenv/armnn/src/armnnCaff2Parser/proto/caffe2.pb.cc:2339:9: error: implicit conversion changes signedness: '::google::protobuf::int32' (aka 'int') to\r\n      'google::protobuf::uint32' (aka 'unsigned int') [-Werror,-Wsign-conversion]\r\n        static_cast< ::google::protobuf::int32>(\r\n        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/home/armnn1/armnn-devenv/armnn/src/armnnCaff2Parser/proto/caffe2.pb.cc:3017:9: error: implicit conversion changes signedness: '::google::protobuf::int32' (aka 'int') to\r\n      'google::protobuf::uint32' (aka 'unsigned int') [-Werror,-Wsign-conversion]\r\n        static_cast< ::google::protobuf::int32>(\r\n        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n""]",[],0,0
324,pytorch,24770,closed,Migrate `sort` from the TH to Aten (CPU),"Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review.",better-engineering module: porting module: sorting and selection triaged,"[""For information, we already have topk and kth_value ported to ATen in https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Sorting.cpp\r\n\r\nAlso, I wonder if this task shouldn't be tackled via a generalization of TensorIterator that takes a vector and returns a vector, instead of taking a number and returning a number?"", 'It looks like topk and kth_value indeed do not utilize TensorIterator. I am thinking about fixing that. ']",[],[],0,0
325,pytorch,27694,open,Deployment training model at C + + end,"When the model does not contain a custom layer, it can be deployed directly on the C + + side using JIT mechanism and libtorch library.

But my model contains a custom c++ and CUDA layer. Now that I'm deploying the model on the c++ side, do I need to compile the custom c++ and CUDA layers into the libtorch library?
Thank you!!!

cc @suo @yf225",module: cpp module: docs needs reproduction oncall: jit triaged,"['Hello @huang229 - could you add a code snippet and more information (as detailed in the issue templates) for this?', ""@cpuhrsch Thank you very much. Sorry, there's too much code. It's a little inconvenient. I trained a target detection model, but this model contains a 3D linear interpolation layer that I defined. The 3D linear interpolation layer is written in cuda. Now I want to deploy the trained target detection model to c++ end of windows platform. I don't know how to deploy the model with custom layer to c++ end with JIT and libtorch.\r\n\r\nYour platform provides a custom layer in the C + + side of the application. But under ubuntu, there is no example under windows. I try to use Python setup. py install to compile scripts under windows, and then use JIT mechanism to invoke them on the C + + side. But I did not compile successfully using Python setup. py install. Is it convenient to provide the following examples under windows?\r\nThe links are as follows:\r\nhttps://github.com/pytorch/extension-script\r\n\r\nBelow is a part of my model, which includes a 3D linear interpolation layer(self.roi_align_*). Its purpose is to intercept an area on the feature map and interpolate the specified size. The other layers are convolution and deconvolution.  Now that the model has been trained, I hope to deploy the model at the c++ end, which is more conducive to application, but I don't know how to deploy the model with custom layer using JIT mechanism and libtorch library.\r\n    def forward(self, box_coord, input, netconv3, netconv5, netconv7, netconv9, netconv11):\r\n\r\n        #input, boxes, crop_depth, crop_height, crop_width\r\n        method = 1\r\n        netconv9_roiAlign = self.roi_align_4(netconv9, box_coord, 4, 2, 2, method)  # 16\r\n        segnet_dconv2 = self.segnet_dconv2(netconv9_roiAlign)\r\n\r\n        netconv7_roiAlign = self.roi_align_3(netconv7, box_coord, 8, 4, 4, method)#32\r\n        seg_concat2 = torch.cat([segnet_dconv2, netconv7_roiAlign], 1)\r\n        segnet_conv3 = self.segnet_conv3(seg_concat2)\r\n        segnet_conv4 = self.segnet_conv4(segnet_conv3)\r\n\r\n        segnet_dconv3 = self.segnet_dconv3(segnet_conv4)\r\n\r\n        netconv5_roiAlign = self.roi_align_2(netconv5, box_coord, 16, 8, 8, method)#64\r\n        seg_concat3 = torch.cat([segnet_dconv3, netconv5_roiAlign], 1)\r\n        segnet_conv5 = self.segnet_conv5(seg_concat3)\r\n        segnet_conv6 = self.segnet_conv6(segnet_conv5)\r\n\r\n        segnet_dconv4 = self.segnet_dconv4(segnet_conv6)\r\n        concat_input = torch.cat([netconv3, input], dim=1)\r\n        snetconv3_roiAlign = self.roi_align_1(concat_input, box_coord, 32, 16, 16, method) #128\r\n        seg_concat4 = torch.cat([segnet_dconv4, snetconv3_roiAlign], 1)\r\n        segnet_conv7 = self.segnet_conv7(seg_concat4)\r\n\r\n        segnet_conv8 = self.segnet_conv8_32(segnet_conv7)\r\n\r\n        mask_out = torch.sigmoid(segnet_conv8)\r\n\r\nThe roi_Align call is as followsÔºö\r\n\r\nclass roi_Align(Function):\r\n\r\n     @torch.jit.script_method\r\n     def forward(ctx, input, boxs, crop_depth, crop_height, crop_width, method):\r\n         batch_size, channel_num, img_depth, img_height, img_width = input.shape\r\n         ctx.save_for_backward = [input, boxs, int(crop_depth), int(crop_height), int(crop_width), int(method)]\r\n        for i in range(batch_size):\r\n            feature_map =input[i, ...].permute(1, 2, 3, 0)\r\n            boxs_ = boxs.data.cpu()[i,:,0 : 6]/ (cfg.IMAGE_SIZE - 1) * (feature_map.shape[-2] - 1)\r\n            fea_cpu = feature_map.data.cpu()\r\n            outputs = crop_and_resize_cuda.forward(fea_cpu.cuda(), boxs_.cuda(), int(crop_depth), int(crop_height), int(crop_width), int(method))\r\n            outputs = outputs.permute(0, 4, 1, 2, 3)\r\n            if i == 0:\r\n                output_map = outputs\r\n            else:\r\n                output_map = torch.cat([output_map, outputs], axis = 0)\r\n        return output_map #output_map.cuda()\r\n"", 'cc: @suo\r\n\r\n', 'Hi @huang229, thank you for your detailed reply. Have you had a look at our [tutorial](https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html) on how to build custom extensions that are accessible from TorchScript? You will need to build a shared library that is linked into your C++ executable, but otherwise it should all work.', '@suo Thank you very much for your reply. I have seen this application example for a long time, but it is an application under Ubuntu system. I need an application instance under Windows system. Do you know if it is convenient for me?']",[],[],0,0
326,pytorch,28515,open,Get rid of libc10.so,"I finally have a good reason to merge libc10.so into libtorch.so (and corresponding libc10_cuda.so into libtorch_cuda.so, etc.): I am trying to devirtualize access to AutogradMeta, but because TensorImpl lives in c10 and AutogradMeta lives in torch, I cannot do this as the destructor would have to cross a dynamic library boundary. By absorbing c10 into torch I will be able to do this.

Some dangers: putting c10 into libtorch might push Windows build over max library size. See https://github.com/pytorch/pytorch/issues/27215

Other alternatives I thought of:
* Move TensorImpl back to ATen/core
* Keep the virtual interface to AutogradMeta

Some alternatives that don't work
* Move AutogradMeta into c10. AutogradMeta must be declared after Tensor as it contains a field Tensor, and we cannot replace that field with an  as the public API of tensor  returns a mutable  reference. (Though, it might be possible to fix this up by just moving TensorImpl out of c10)

cc @ezyang @gchanan @zou3519 @jerryzh168 @dzhulgakov @smessmer ",module: build triaged,"[""One hiccup along the way: caffe2 perfkernels depends on c10/util/Half.h. It looks like it's fine if you don't explicitly track this dependency (and perfkernels already depend on caffe2 headers) but it's something to be aware of."", 'This PR is the payoff of doing this: https://github.com/pytorch/pytorch/pull/28526']",[],"['intrusive_ptr<TensorImpl>', 'x.grad()', 'Tensor&']",0,0
327,pytorch,25417,open,Add a mode to check input tensor sizes in allreduce_coalesced,"@jfc4050 added CPU  in #24949. The current version flattens all input tensors into one, and then allreduce that tensor. It works, even if, say, process 0 provides a tensor of size 4 and process 1 provides two tensors of size 2 each. This is a reasonable shortcut to avoid using additional communications to check tensor sizes, but it will be good to add size checking in the default mode, and asking users to explicitly set  to they indeed want to skip that.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera",feature oncall: distributed triaged,"[""This is somewhat overlapping with the idea of having a process group that does this type of debugging, as a wrapper over the standard functionality (see #22071 for the issue). Then each collective will first allgather the metadata of the arguments that were passed, so that all processes can confirm they are valid, before running the collective. If one or more processes use the wrong args (e.g. wrong dtype, sizes, strides, etc), then all processes can raise an error.\r\n\r\nI like that better in comparison to adding a flag to every collective to toggle whether or not this check should be enabled. Moreover, if it defaults to on, then performance will be poor out of the box, and users need to be aware of this flag for improved performance. I would rather have this special mode exist, where the goal is correctness and debugging, and not performance. Then if we get any type of error during normal operation that can hint at some form of desynchronization, we can mention this mode in the error message, and have users use that mode to figure out what's wrong. Doing that type of debugging with a flag per collective means you're likely to miss one or two when you need it, and pull out your hair while stilling seeing timeouts/weird behavior.""]",[],"['allreduce_coalesced', 'check_sizes=False']",0,0
328,pytorch,11980,open,[Enhancement] Increase user-friendliness of dataset.random_split,"## Issue description

Currently, when using the [random_split function](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataset.py)  the parameters that need to be given are:
- dataset
- list that contains the lengths of splits to be produced

This means a user has to calculate these upfront and add them to the function as parameters like this:




Wouldn't it be better if the second parameter was a list that contains the percentages in which the users wants the split to happen, like this:
 .

The result of this change would be cleaner code for the user and also I believer in a more natural way of creating the splits.

I would like to pick this up as a pull request!





cc @VitalyFedyunin @ejguan",enhancement module: data triaged,"['@romanovacca this sounds good. Please feel free to pick it up', ""@zou3519 ,I have created a solution. This is my first PR and I looked at the 'contributing' document, but don't know how I can unit test my changes using any existing tests. What I did now was locally run this and tested some scenarios and that went well. So is there anything else I can do before creating a PR?"", ""That's a good point, I'm not sure how to test randomness. Feel free to put up a PR and I'll take a look at it."", ""NB on the analogous functionality from Spark: https://spark.apache.org/docs/2.3.1/api/scala/index.html#org.apache.spark.rdd.RDD@randomSplit(weights:Array[Double],seed:Long):Array[org.apache.spark.rdd.RDD[T]]\r\n\r\nJust an example of an analogous API and implementation. In particular, I would highlight that the Spark implementation makes the choice of normalizing weights which don't sum to zero. An alternative for this normalization of weights functionality could be to simply throw an error, depending on the workflow desired."", '@zou3519 PR is here: [#12068](https://github.com/pytorch/pytorch/pull/12068) . This is my first PR, so if I did it wrong let me know!\r\n\r\n@jeffreyksmithjr I do not understand what you mean, could you explain it?', ""FWIW the PR got NACKed by apaszke in review, so I don't know if this request is dead or if there's another way to do it that resolves his concern.""]",[],"['train_size = int(0.8 * len(dataset))', 'test_size = len(dataset) - train_size', 'train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size,test_size])', 'train_dataset, test_dataset = torch.utils.data.random_split(dataset,[0.8,0.2])']",0,0
329,pytorch,26714,closed,"spectral_norm used in RNN causes ""parameter types mismatch"" in GPU","## Issue description
spectral_norm used in  is okay. But when it's used in ,there will be a 
model = network().cuda()
## Environment
 - PyTorch Version (e.g., 1.0): 1.1.0
 - Python version: 3.6.8
 - CUDA/cuDNN version:  cuda9.0


",module: nn triaged,"['CC @pbelevich as it is related to the new `_cudnn_rnn_flatten_weight` code.', 'I came across the same Bug, did this problem be solved? @VitalyFedyunin @pbelevich @oh-y ', 'Closing as I cannot reproduce this on master.']","['\r\n\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## Code\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n']","['nn.Linear', 'nn.RNN', '', '\r\nRuntimeError while running ', '\r\n\r\nTraceback (most recent call last):\r\n  File ""/anaconda3/envs/env1/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3296, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File ""<ipython-input-2-66814b2fa07a>"", line 1, in <module>\r\n    runfile(\'example.py\')\r\n  File ""/home/.pycharm_helpers/pydev/_pydev_bundle/pydev_umd.py"", line 197, in runfile\r\n    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script\r\n  File ""/home/.pycharm_helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile\r\n    exec(compile(contents+""\\n"", file, \'exec\'), glob, loc)\r\n  File ""example.py"", line 17, in <module>\r\n    model = network().cuda()\r\n  File ""/anaconda3/envs/env1/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 265, in cuda\r\n    return self._apply(lambda t: t.cuda(device))\r\n  File ""/anaconda3/envs/env1/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 193, in _apply\r\n    module._apply(fn)\r\n  File ""/anaconda3/envs/env1/lib/python3.6/site-packages/torch/nn/modules/rnn.py"", line 127, in _apply\r\n    self.flatten_parameters()\r\n  File ""/anaconda3/envs/env1/lib/python3.6/site-packages/torch/nn/modules/rnn.py"", line 123, in flatten_parameters\r\n    self.batch_first, bool(self.bidirectional))\r\nRuntimeError: param_from.type() == param_to.type() ASSERT FAILED at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:541, please report a bug to PyTorch. parameter types mismatch\r\npython\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nclass network (nn.Module):\r\n    def __init__(self):\r\n        super(network, self).__init__()\r\n        self.sn_rnn = nn.utils.spectral_norm(nn.RNN(20,10,1),name=\'weight_hh_l0\')\r\n        # self.fc = nn.utils.spectral_norm(nn.Linear(20,10), name=\'weight\')\r\n    def forward (self,x):\r\n        # x = F.tanh(self.fc(x))\r\n        out,_ = self.sn_rnn(x)\r\n        return F.log_softmax(out, dim=1)\r\n\r\nif __name__ == \'__main__\':\r\n    x = torch.randn(2, 10, 20).cuda()\r\n    model = network().cuda()\r\n    out= model(x)\r\n    print(\'end\')\r\n', '']",0,0
330,pytorch,24600,closed,Migrate `multi_margin_loss` from the TH to Aten (CUDA),"Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review.",better-engineering module: loss module: porting triaged,[],[],[],0,0
331,pytorch,29692,open,[FR] add generator= kwarg support for torch.randn and torch.rand,All stochastic functions should accept . These should as well.,enhancement module: random triaged,"['Can you elaborate on this issue? ', '@mruberry I added short explanation. Let me know if it is not sufficient.', 'Thank you.']",[],['generator='],0,0
332,pytorch,28938,open,Cannot select version in the tutorials page,"## üìö Documentation

The documentation pages can be viewed for different versions using the [versions page](https://pytorch.org/docs/versions.html). However, when navigating to the ""Tutorials"", the versions cannot be selected. This is a problem for the experimental parts (s.a. quantization), as we update the tutorials in master, which cannot be viewed.",module: docs triaged,['Is it possible to move this issue to pytorch/tutorials?'],[],[],0,0
333,pytorch,10714,open,[feature request][caffe2] extend FC/FCTranspose op to handle 2d bias.,"Currently, FC/FCTranspose only accepts 1d bias. The FC's implementation will do the broadcast to create the 2d bias from 1d bias, but we could just provide the whole 2d bias. Then, we can do the better optimization for onnx Gemm op with caffe2 backend.

I will try to implement this idea and create a pull request later.

@houseroad @bddppq 
",caffe2,"['#10770 ', 'Discussed with @houseroad .\r\nI will add a new op gemm to support 2d bias in caffe2.']",[],[],0,0
334,pytorch,14672,closed,Inconsistent behavior for log_prob when values are outside of support,"## üêõ Bug

In Scipy, if you try to calculate the log probability of a value outside of the given distribution's support, scipy will return .

The Uniform Distribution for torch follows this behavior, as shown below:

However, this behavior does not extend to other distributions that have a constrained support, such as the exponential, beta, and gamma, or discrete distributions such as the geometric. 









These four are the distributions that I've tested; I'm unsure if there are more.  Is this discrepancy in behavior expected?  Please let me know if I've missed something.

### System Info
Collecting environment information...
PyTorch version: 0.4.1
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 18.10
GCC version: (Ubuntu 8.2.0-7ubuntu1) 8.2.0
CMake version: version 3.12.1

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: GeForce GTX 960M
Nvidia driver version: 415.13
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] numpy (1.15.4)
[pip] numpydoc (0.8.0)
[pip] torch (0.4.1)
[conda] pytorch                   0.4.1            py37ha74772b_0


cc @fritzo @neerajprad @alicanb @nikitaved",module: distributions triaged,"['Hi @chentc777,\r\n\r\nYou can consider setting `validate_args` to `True` for the distributions individually, or by setting a global flag using:\r\n```\r\ntorch.distributions.Distribution.set_default_validate_args(True)\r\n```\r\n\r\nThis will raise a `ValueError` if the values are outside the support.\r\n\r\n```python\r\n>>> torch.distributions.Distribution.set_default_validate_args(False)\r\n>>> Beta(torch.tensor(1.),torch.tensor(1.)).log_prob(torch.tensor(5.))\r\ntensor(nan)\r\n>>> Exponential(torch.tensor(2.)).log_prob(torch.tensor(-5.))\r\ntensor(11.6094)\r\n>>> Gamma(torch.tensor(1.),torch.tensor(1.)).log_prob(torch.tensor(-1.))\r\ntensor(nan)\r\n>>> Geometric(torch.tensor(5.)).log_prob(torch.tensor(-1.))\r\ntensor(nan)\r\n```\r\n\r\n```python\r\n>>> torch.distributions.Distribution.set_default_validate_args(True)\r\n>>> Beta(torch.tensor(1.),torch.tensor(1.)).log_prob(torch.tensor(5.))\r\nValueError: The value argument must be within the support\r\n>>> Exponential(torch.tensor(2.)).log_prob(torch.tensor(-5.))\r\nValueError: The value argument must be within the support\r\n>>> Gamma(torch.tensor(1.),torch.tensor(1.)).log_prob(torch.tensor(-1.))\r\nValueError: The value argument must be within the support\r\n>>> Geometric(torch.tensor(5.)).log_prob(torch.tensor(-1.))\r\nValueError: The parameter probs has invalid values\r\n```', 'Closing as validation is now enabled by default.  I believe erroring is probably the more common use case in deep learning.  To reproduce SciPy behavior of returning negative infinity, you should be able to interleave based on `.support.check(-)`:\r\n```py\r\nd = LogNormal(torch.zeros(9), torch.ones(9), validate_args=False)\r\nx = torch.randn(9)  # incorrectly unconstrained\r\nunsafe_log_prob = d.log_prob(x)  # may have NANs\r\nsafe_log_prob = torch.where(\r\n    d.support.check(x),\r\n    unsafe_log_prob,\r\n    torch.full_like(unsafe_log_prob, -math.inf),\r\n)\r\n```']","['python\r\n> Uniform(torch.tensor(0.),torch.tensor(1.)).log_prob(torch.tensor(5.))\r\n> tensor(-Inf)\r\n', 'python \r\n> Beta(torch.tensor(1.),torch.tensor(1.)).log_prob(torch.tensor(5.))\r\n> tensor(nan)\r\n', 'python\r\n> Exponential(torch.tensor(2.)).log_prob(torch.tensor(-5.))\r\n> tensor(11.6094)\r\n', 'python\r\n> Gamma(torch.tensor(1.),torch.tensor(1.)).log_prob(torch.tensor(-1.))\r\n> tensor(nan)\r\n', 'python\r\n> Geometric(torch.tensor(5.)).log_prob(torch.tensor(-1.))\r\n> tensor(nan)\r\n']",['-inf'],0,0
335,pytorch,14112,open,[sparse] add descriptions and examples for methods at torch.sparse doc page,"- There is no descriptions / examples for the methods listed at at doc page
- It will be much easier for new comers to learn how to use sparse if we improve doc a little bit

cc @aocsa @nikitaved @pearu @mruberry @IvanYashchuk",module: sparse triaged,"[""How's this going? Any suggestions on the best way to approach [this problem](https://github.com/pytorch/pytorch/issues/14489)?""]",[],[],0,0
336,pytorch,20997,open,ReduceLROnPlateau will fail when add new parameter group to the optimizer,"## üêõ Bug

ReduceLROnPlateau will fail when add new parameter group to the optimizer.
The  function will raise list index out of range error

## To Reproduce

Steps to reproduce the behavior:

1. initialize optimizer: 
1. initialize scheduler: 
1. add parameter group to the optimizer: 
1. raise error when the learning rate should be changed

## Expected behavior

when add a parameter group to the optimizer, the  attribute of the scheduler should be updated to avoid this error.

## Environment

 - PyTorch Version (1.1.0):",module: optimizer triaged,[],[],"['_reduce_lr', 'optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), args.lr)', ""scheduler = ReduceLROnPlateau(optimizer, patience=1, factor=0.1, verbose=True, mode='max')"", ""optimizer.add_param_group({'params':unfreezed_params, 'lr':lr})"", 'min_lrs']",0,0
337,pytorch,22687,open,DispatchStub should report what operator it failed to find kernel for,"In #22681 we get an unhelpful error message because it just says that CUDA kernels is missing, it doesn't say what is missing. It would be good of dispatch stub gave more information. Maybe we can pass in a string constant to the template parameter and update the macro to stringify the struct name to pass in.

cc @cpuhrsch @VitalyFedyunin @colesbury ",module: cpu module: internals triaged,[],[],[],0,0
338,pytorch,27406,open,[jit] String frontend doesn't support default arg values,"

cc @suo",oncall: jit triaged,[],[],[],0,0
339,pytorch,15163,closed,doc error:torch.nn.NLLLoss,"if size_average=True and reduce=True,
loss(x,class)=‚àíweights[class]‚àóx[class]/N",,"[""Please describe the issue in more details. Otherwise we can't understand what you mean."", 'DOC:\r\nif size_average=True and reduce=True,\r\nloss(x,class)=‚àíweights[class]‚àóx[class]/(‚àën=1toN (Wyn))\r\n‚àën=1toN (Wyn) ‚â†N', 'Use the issue template.']",[],[],0,0
340,pytorch,17126,open,Support callables in scripted functions,"## üöÄ Feature
Allow for callables (notably other scripted functions or ScriptModules) to be passed to scripted functions (Currently fails with ).

## Motivation

Attempting to tease out fusions from larger codebases (in my case maskrcnn_benchmark) where some blocks of code may not be scriptable, but subsets of those blocks may be. In a concrete example, I have a module which contains loop over sets of inputs & modules, where  is seen.  is itself a ScriptModule, and I'd like to be able to add the  to a fused group that's already being generated within that ScriptModule. However, there are several things that prevent the outermost module from being scripted. Pulling out the relevant code to a separate function like:


Should allow the desired fusion, but is currently not supported.

## Pitch

Allow callables (at least ScriptModules) to be passed as arguments to scripted functions and called.

## Alternatives

My specific use-case disappears as the scripting supports a certain level of python, but I would expect there to be other use-cases.

## Additional context

This is a proxy code I wrote to make sure that the error I was seeing wasn't from the larger application. This fails with . 




cc @suo",jit-backlog oncall: jit,"['Thanks for the detailed feature request! This is on our radar but not sure it will happen anytime soon.', 'We are currently building support for user-defined types in TorchScript. So soon you will be able to implement a method on your class and pass it around in script functions.', '@suo what is the status of the support for user-defined types?']","['\r\n@torch.jit.script\r\ndef impl(m, x. y):\r\n    return m(x) + y\r\n', '\r\nimport torch\r\n\r\nclass Bias(torch.jit.ScriptModule):\r\n    def __init__(self, num_channels):\r\n        super(Bias, self).__init__()\r\n\r\n        self.bias = torch.nn.Parameter(torch.zeros(num_channels))\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, x):\r\n        return x + self.bias.reshape(1, -1, 1, 1)\r\n\r\n@torch.jit.script\r\ndef fwd_impl(b, x, y):\r\n    return b(x) + y\r\n\r\n# Note: Not-scriptable in real app\r\nclass OuterModule(torch.nn.Module):\r\n    def __init__(self, n):\r\n        super(OuterModule, self).__init__()\r\n  \r\n        self.bias = Bias(n)\r\n\r\n    def forward(self, x, y):\r\n        return fwd_impl(self.bias, x, y)\r\n\r\nn, c, h, w = 32, 4, 16, 16\r\nx = torch.randn(n, c, h, w).cuda()\r\ny = torch.randn(n, c, h, w).cuda()\r\nm = OuterModule(c).cuda()\r\n\r\nwith torch.no_grad():\r\n    z = m(x, y)\r\n']","['cannot call a value', 'z = module(x) + y', 'module', '+', 'cannot call a value']",0,0
341,pytorch,27034,open,RuntimeError:[enforce fail at context.h:48] option.device_type() ==PROTO_CPU. 1vs0,"
Traceback (most recent call last):
File ""/usr/VMZ-master-1/tools/train_net.py"", line 586, in
main()
File ""/usr/VMZ-master-1/tools/train_net.py"", line 581, in main
Train(args)
File ""/usr/VMZ-master-1/tools/train_net.py"", line 404, in Train
workspace.CreateNet(test_model.net)
File ""/root/pytorch/build/caffe2/python/workspace.py"", line 181, in CreateNet
StringifyProto(net), overwrite,
File ""/root/pytorch/build/caffe2/python/workspace.py"", line 215, in CallWithExceptionIntercept
return func(args, kwargs)
RuntimeError: [enforce fail at context.h:48] option.device_type() == PROTO_CPU. 1 vs 0
frame #0: c10::ThrowEnforceNotMet(char const, int, char const, std::__cxx11::basic_string<char, std::char_traits, std::allocator > const&, void const) + 0x78 (0x7fed19c32178 in /usr/local/lib/libc10.so)
frame #1: + 0x2686d70 (0x7fecdcf03d70 in /usr/local/lib/libtorch.so)
frame #2: + 0x2723fec (0x7fecdcfa0fec in /usr/local/lib/libtorch.so)
frame #3: + 0x3aff5ee (0x7fecde37c5ee in /usr/local/lib/libtorch.so)
frame #4: std::_Function_handler<std::unique_ptr<caffe2::OperatorBase, std::default_deletecaffe2::OperatorBase > (caffe2::OperatorDef const&, caffe2::Workspace*), std::unique_ptr<caffe2::OperatorBase, std::default_deletecaffe2::OperatorBase > ()(caffe2::OperatorDef const&, caffe2::Workspace)>::_M_invoke(std::_Any_data const&, caffe2::OperatorDef const&, caffe2::Workspace*&&) + 0x23 (0x7fed1a4b5433 in /root/pytorch/build/caffe2/python/caffe2_pybind11_state_gpu.so)
frame #5: + 0x236c25c (0x7fecdcbe925c in /usr/local/lib/libtorch.so)
frame #6: caffe2::CreateOperator(caffe2::OperatorDef const&, caffe2::Workspace*, int) + 0x328 (0x7fecdcbea528 in /usr/local/lib/libtorch.so)
frame #7: caffe2::dag_utils::prepareOperatorNodes(std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*) + 0x2ad (0x7fecdcbda06d in /usr/local/lib/libtorch.so)
frame #8: caffe2::AsyncNetBase::AsyncNetBase(std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*) + 0x24d (0x7fecdcbb670d in /usr/local/lib/libtorch.so)
frame #9: caffe2::AsyncSchedulingNet::AsyncSchedulingNet(std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*) + 0x9 (0x7fecdcbbb5b9 in /usr/local/lib/libtorch.so)
frame #10: + 0x23410ae (0x7fecdcbbe0ae in /usr/local/lib/libtorch.so)
frame #11: std::_Function_handler<std::unique_ptr<caffe2::NetBase, std::default_deletecaffe2::NetBase > (std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*), std::unique_ptr<caffe2::NetBase, std::default_deletecaffe2::NetBase > ()(std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace)>::_M_invoke(std::_Any_data const&, std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*&&) + 0x23 (0x7fecdcbbdf83 in /usr/local/lib/libtorch.so)
frame #12: caffe2::CreateNet(std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*) + 0x4a5 (0x7fecdcbb0495 in /usr/local/lib/libtorch.so)
frame #13: caffe2::Workspace::CreateNet(std::shared_ptr<caffe2::NetDef const> const&, bool) + 0x103 (0x7fecdcc2fe23 in /usr/local/lib/libtorch.so)
frame #14: caffe2::Workspace::CreateNet(caffe2::NetDef const&, bool) + 0x91 (0x7fecdcc30d61 in /usr/local/lib/libtorch.so)
frame #15: + 0x57906 (0x7fed1a4ad906 in /root/pytorch/build/caffe2/python/caffe2_pybind11_state_gpu.so)
frame #16: + 0x57bd2 (0x7fed1a4adbd2 in /root/pytorch/build/caffe2/python/caffe2_pybind11_state_gpu.so)
frame #17: + 0x99e3d (0x7fed1a4efe3d in /root/pytorch/build/caffe2/python/caffe2_pybind11_state_gpu.so)

frame #33: __libc_start_main + 0xe7 (0x7fed1e897b97 in /lib/x86_64-linux-gnu/libc.so.6)

",caffe2 triaged,[],[],[],0,0
342,pytorch,23054,open,ConcatDataset returns different error messages setting out of range plus index and minus index.,"## üêõ Bug

ConcatDataset returns different error messages setting out of range plus index and minus index. 

## To Reproduce

## Expected behavior
I think it's better that x[100] and x[-100] have the same error message.

My way to solve this is to chage [these lines](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataset.py#L197-L200) like this.


## Environment
 - PyTorch Version (e.g., 1.0): 1.1.0
 - OS (e.g., Linux): Ubuntu 18.04
 - How you installed PyTorch (, , source): pip
 - Python version: Python 3.7.3
 - CUDA/cuDNN version: None
 - GPU models and configuration: None

",low priority module: docs triaged,[],"['\r\n>>> import torch\r\n>>> from torch.utils import data\r\n>>> x=data.ConcatDataset((range(10),range(10)))\r\n>>> x[-100]\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/mueda/code/python/rc/doc/.venv/lib/python3.7/site-packages/torch/utils/data/dataset.py"", line 78, in __getitem__\r\n    raise ValueError(""absolute value of index should not exceed dataset length"")\r\nValueError: absolute value of index should not exceed dataset length\r\n>>> x[100]\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/mueda/code/python/rc/doc/.venv/lib/python3.7/site-packages/torch/utils/data/dataset.py"", line 85, in __getitem__\r\n    return self.datasets[dataset_idx][sample_idx]\r\nIndexError: list index out of range\r\n', '\r\n        if idx < 0:\r\n            idx = len(self) + idx\r\n        if not (0 <= idx < len(self)):\r\n            raise ValueError(""absolute value of index should not exceed dataset length"")\r\n']","['conda', 'pip']",0,0
343,pytorch,31557,open,[docs] F.ctc_loss docs to warn clearly about invalid inf-causing inputs; zero_infinity to become enabled by default,"**UPD** summary of all the long discussion for further discoverability:
1. F.ctc_loss will produce inf loss if presented with invalid unalignable examples
2. Such invalid examples may be generated by official usage code example if one is extremely unlucky or if one twists the dimension sizes a little bit
3. When presented with invalid examples, sum and mean reduction modes by default cause the whole batch loss to be **inf**

Proposals:
1. Have docs warn clearly about conditions on valid examples
2. Have docs warn clearly that the official usage example may produce invalid examples / fix the official code example
3. Enable zero_infinity = True by default or at least in reduction modes sum/mean

**BELOW IS THE ORIGINAL ISSUE DESCRIPTION**



Docs specify that targets can't be blank. If same consecutive labels are not supported, I think it should be explicitly mentioned in docs. And maybe docs should specify some workaround to encode consecutive same-valued targets (given that blank to separate them isn't allowed by docs).

Current docs: ",module: docs triaged,"[""Thanks for the suggestion! It'd be great if you wanted to submit a fix for this. "", '@mruberry One needs insight about CTC impl which I don‚Äôt have: can it be fixed or not. So in this case I can‚Äôt submit a fix.', 'This is potentially a bug in CTC. @mruberry could you please recategorize this?', '@t-vi would you have any comments about proper encoding of repeated labels? As far as I can see (at least the CPU impl) adds epsilons after every other target label automatically, so repeated labels should theoretically be ok.', '\nThe algorithm behind the function(s) is relatively vanilla the alignment but in logspace.\nYou never have a problem with target label repetitions, you just give it the target that you want. Only the input has the special encoding wrt repetitions and blanks.\nMy apologies for being blunt here, but based on the thread, I can recommend reading up on the ctc loss. Maybe the parameters become clearer after studying the original article or the distill pub exposition.', '@t-vi For context: I\'ve read the distill tutorial and other materials (including the original Graves paper) many times. I\'ve been working on a CTC-based speech recognition system for a few months and have had no problems with the CTC implementation in PyTorch (I used a special ""repeat"" character though) and am capable of providing the ctc_loss\'s arguments and learn a speechrec system with them. Ditching Baidu\'s warp-ctc has been possible thanks to your contributions to PyTorch!\r\n\r\nAs you know, I\'ve been experimenting with an equivalent formulation of CTC in https://discuss.pytorch.org/t/manually-call-f-ctc-loss-backward-method/61634/6. During the tests I\'ve found that even the official PyTorch CTC docs example will print **inf** if the number of labels is reduced from 20 to 2 (i.e. we have 1-letter alphabet + blank char):\r\n```python\r\nimport torch\r\nimport torch.nn.functional as F\r\n\r\ntorch.manual_seed(1)\r\nlog_probs = torch.randn(50, 16, 20).log_softmax(2).detach().requires_grad_()\r\ntargets = torch.randint(1, 2, (16, 30), dtype=torch.long)\r\ninput_lengths = torch.full((16,), 50, dtype=torch.long)\r\ntarget_lengths = torch.randint(10,30,(16,), dtype=torch.long)\r\nloss = F.ctc_loss(log_probs, targets, input_lengths, target_lengths)\r\nprint(loss)\r\n# tensor(inf, grad_fn=<MeanBackward0>)\r\n```\r\n\r\nThe only thing I did is to replace 20 by 2 in target generation, above is the official sample from https://pytorch.org/docs/stable/nn.functional.html?highlight=ctc_loss#torch.nn.functional.ctc_loss . The same effect (**inf**) in the official example can be achieved without changing number of labels - by increasing the number of target time steps from 30 to 50 and having all target_lengths length 50 as well (then the chance of having at least two labels repeated is high enough)\r\n\r\nI\'m sorry if these elements were not clear from the comments above.', 'This is not a bug but correct mathematical behaviour.', ""If this is correct mathematical behavior, shouldn't this aspect be documented? Increasing just number of target time steps in the official example also causes inf:\r\n```python\r\nimport torch\r\nimport torch.nn.functional as F\r\n\r\ntorch.manual_seed(1)\r\nlog_probs = torch.randn(50, 16, 20).log_softmax(2).detach().requires_grad_()\r\ntargets = torch.randint(1, 20, (16, 50), dtype=torch.long)\r\ninput_lengths = torch.full((16,), 50, dtype=torch.long)\r\ntarget_lengths = torch.randint(49,50,(16,), dtype=torch.long)\r\nloss = F.ctc_loss(log_probs, targets, input_lengths, target_lengths)\r\n\r\nprint(loss)\r\n# tensor(inf, grad_fn=<MeanBackward0>)\r\n```\r\n\r\nMeanwhile I will try to reimpl the forward pass in Python to try to understand the cause."", '@t-vi You are right. The situation seems to have nothing to do with repeated labels, but with the fact that input length 2 cannot possibly be aligned to a target sequence of two repeated labels. The behavior is mathematically correct. Btw I reimplemented \r\n\r\nBelow are some proposals about changes to the docs:\r\n\r\nHow about adding to zero_infinity description sth like: ""e.g. this can happen if aligning length-2 input to a sequence \'aa\' "".\r\n\r\nAnd maybe adding a comment in the code example:\r\n`# this snippet does not check that alignment of inputs to targets is possible, so if you are super-unlucky you can get inf loss`\r\n\r\nIn the official code example if one is unfortunate to get 30 `a`\'s as targets. 50-length input won\'t be enough to be alignable.\r\n\r\nThis is a bit confusing to receive inf on a good batch with one bad example - especially in default `sum` or `mean` reduction mode. A proposal: enable zero_infinity by default, at least with sum/mean reduction modes.\r\n\r\n(vectorized and loop-based impls used for debugging: https://gist.github.com/vadimkantorov/c1aa417cffa1450b03716c740795f107)', ""I would have appreciated if you had clearly stated that you are wanting to feed inputs and targets such that the input sequence is mathematically impossible to align to the targets and this is why you get an infinite loss rather than being vague about it.\r\nI'm sure there are bugs to be fixed and improvements to be made and it'd be good to improve the user experience, but I'm not certain that the suggestions in this issue would be on the top of my list. If someone else wants to submit a PR and find a reviewer to like it, then it is all well.\r\n"", '@t-vi If I had understood this root cause from the get-go, I would not have wasted everyone‚Äôs time! Please be ensured, I wrote this in good faith! The proof is I had posted a repro.\r\n\r\nI twisted the official example very minorly and found it produces inf (and if one is extremely unlucky, even without any modifications it is theoretically possible to have it generate a bad example). Yes, the un-alignable examples are correlated with long examples which are correlated with repeated labels. Yes, my original cause analysis was false, but I did not invoke repeated labels on purpose to waste your time!\r\n\r\nTo be clear, I do not ‚Äúwant to feed inputs that are mathematically unalignable‚Äù per se, I only wish that docs are clear about the condition when this may happen (and this can happen by a minor twist of the official example!) and be fool-proof by default if this happens. \r\n\r\nSorry again for not finding the root cause myself from the get-go! ', ""> I'm not certain that the suggestions in this issue would be on the top of my list.\r\n\r\nGot it! I''ll rename the issue properly, so it's discoverable if someone hits it"", '@t-vi Could you please re-open the issue (for a discussion at some later point)?\r\n\r\nI maybe will submit a docs PR at some point as well.']","['python\r\nimport torch\r\nimport torch.nn.functional as F\r\n\r\ndef test_ctc(C, B = 1, T = 2, full = False):\r\n    log_probs = torch.randn(B, C, T).log_softmax(dim = 1)\r\n    input_lengths = torch.full((B,), T, dtype=torch.long)\r\n\r\n    target_lengths = torch.full((B,), T, dtype = torch.long)\r\n    if full:\r\n        targets = torch.full((B, T), C - 1, dtype = torch.long)\r\n    else:\r\n        targets = torch.randint(1, C, (B, T), dtype=torch.long)\r\n\r\n    loss = F.ctc_loss(log_probs.permute(2, 0, 1), targets, input_lengths, target_lengths)\r\n    print(float(loss))\r\n\r\ntest_ctc(C = 64, full = False)\r\n# 4.894557952880859\r\n\r\ntest_ctc(C = 2, full = False)\r\n# inf\r\n\r\ntest_ctc(C = 64, full = True)\r\n# inf\r\n']",['Each element in the target sequence is a class index. And the target index cannot be blank (default=0)'],0,0
344,pytorch,9945,open,WERROR=1 doesn't work with FULL_CAFFE2,It seems to toggle too many warnings for Caffe2 and then  fails.,caffe2,[],[],['-Werror'],0,0
345,pytorch,20165,open,torch.nn.threshold cannot accept tensor as a threshold,"## üöÄ Feature
I think it will be useful if we can pass threshold as a tensor to the threshold function. in this way we can compute the backprop of the threshold tensor and use it in training process.

right now the threshold function just takes non-tensor threshold",enhancement module: nn triaged,"['We could do this.\r\n\r\n<del> But the gradient wrt to the threshold tensor will always be zero btw.</del> no that is wrong', ""Btw, isn't this just `max`?"", 'I think that can so useful.\r\nIt wont always be zero, it might be a little complicated though...\r\n\r\nhmmm why `max`?', 'i was trying to do it with `clamp` but i couldnot make that work because i think the clamp doesnot take tensor either', '`max(tensor, threshold)[i]` gives you `tensor[i]` if `tensor[i] > threshold[i]`, and `threshold[i]` otherwise.', 'I see, yes it is kinda similar. but when you use `max` you threshold them with the threshold value but in the threshold function you have the ability to threshold them with whatever value you desire\r\n', 'I think you still threshold with arbitrary values.\n\nOn Mon, May 6, 2019 at 12:49 Alireza <notifications@github.com> wrote:\n\n> I see, yes it is kinda similar. but when you use max you threshold them\n> with the max value but in the threshold function you have the ability to\n> threshold them with whatever value you desire\n>\n> ‚Äî\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/20165#issuecomment-489690174>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABLJMZNTYATRXWXNWKVLWUDPUBORTANCNFSM4HLBN5MQ>\n> .\n>\n', 'maybe im making a mistake. can you please provide a simple snippet example?\r\nin torch.threshold you have the freedom to choose what value to put instead of the elements that are less than the threshold.\r\nwhen i tried max it was replacing the values below the threshold with the threshold value', ""You can get the same using `torch.where(inp >= threshold, inp, value)`, if you JIT it, it'll be fast, too. To be honest, I think the `threshold != value` case (i.e. when it's not `max`) seems pretty exotic."", '@t-vi Nope, `torch.where(inp >= threshold, inp, value)` will kill the gradient flow.  I used it before, `nn.threshold `is the only thing that works fine, also `max` works as well. but they dont accept tensor for the threshold', 'How so? You get a gradient for inp and for value, but not for threshold, but that is expected.', '@t-vi \r\nActually, you are right I guess.\r\nIs it what you mean?\r\n\r\n```\r\n\r\nA = torch.rand(1,2,3,3)\r\nB = torch.mean(torch.rand(2,2))\r\nC= torch.zeros(A.size())\r\nD = torch.where(A >= B, A,C)\r\n```\r\n\r\nI was looking at [this](https://discuss.pytorch.org/t/gradients-of-torch-where/26835) and that is why i said it can kill gradient flow.\r\n\r\nIn addition, if we do it like that, it might give an error that you cannot do it inplace...']",[],[],0,0
346,pytorch,6265,open,[Caffe2] mobile_exporter init_net has code calling information,"**The init_net file generated from the mobile_exporter function has code calling information,**
There is fragment information in my generated init_net file:

‚ñ†146 ""GivenTensorFill*	
shape0*
values-6}‚ñ†?-
{‚ïì>-$‚îî?R‚îê  File ""Style_torch.py"", line 186, in <module>
    init_net, predict_net = mobile_exporter.Export(c2_workspace, c2_model, c2_model.external_input)
  File ""/home/wguo/lib/temp1/caffe2/build/caffe2/python/predictor/mobile_exporter.py"", line 86, in Export
    add_tensor(init_net, blob_name, blob)
  File ""/home/wguo/lib/temp1/caffe2/build/caffe2/python/predictor/mobile_exporter.py"", line 53, in add_tensor
    utils.MakeArgument(""values"", values),
*  ",caffe2,['Can you post all of the information that was in the template when you make a new issue? We need that info to understand what the problem is.'],[],[],0,0
347,pytorch,18645,open,c++ Windows  compile error ,"visual studio2017  ÔºõReference directory  O:\pytorch\libtorch\include\torch\csrc\api\include  
  O:\pytorch\libtorch\include   library directory O:\pytorch\libtorch\lib .... 
At the 101 of document rnn.h. There's a mistake.have error :cast to function type is illegal
 pytorch is https://download.pytorch.org/libtorch/cu100/libtorch-win-shared-with-deps-latest.zip
cuda is 10.0
windwos 10
visual studio 2017
release x64
cl.exe 14.16.27023


I think this is a compiler error. To this, there are Windows installation tutorials, you can compare the difference. Select the compilation environment and version.
",has workaround module: build module: windows needs reproduction triaged,"['Do you also get the error in the nightly build?\r\nLooks like this was fixed already.', 'no i in the stable 1.0. isn`t nightly .\r\nwhat it is"" Looks like this was fixed already"".', 'I mean, I think this error did exist but it no longer exists in the latest master.\r\nTo confirm, can you try the nightly build?', 'i try.', 'is this problem solved now? I compiled the source code yesterday, but still encounter the problem.', '@asa008 can you include a log of your build?', 'error occured in rnn.h:\r\n\r\n```\r\n  // The function signature of `rnn_relu`, `rnn_tanh` and `gru`. \r\n  using RNNFunctionSignature = std::tuple<Tensor, Tensor>(     \r\n     const Tensor&,\r\n     const Tensor&,\r\n     TensorList,\r\n     bool,\r\n     int64_t,\r\n     double,\r\n     bool,\r\n     bool,\r\n     bool);\r\n```  \r\n\t  \r\nthen, according to https://discuss.pytorch.org/t/c2066-error-in-rnn-h/37150/2  \r\nto modify above as following, solved:\r\n\r\n```\r\nusing RNNFunctionSignature = std::function<std::tuple<Tensor, Tensor>(\r\n      const Tensor&,\r\n      const Tensor&,\r\n      TensorList,\r\n      bool,\r\n      int64_t,\r\n      double,\r\n      bool,\r\n     bool,\r\n     bool)>;\r\n```\r\n\r\n@gchanan', '@asa008 Can you paste what the contents of `generic_forward` (right below) are? It should look like this:\r\n\r\n```\r\n  /// A generic `forward()` used for RNN and GRU (but not LSTM!). Takes the ATen\r\n  /// RNN function as first argument.\r\n  RNNOutput generic_forward(\r\n      std::function<RNNFunctionSignature> function,\r\n      const Tensor& input,\r\n      Tensor state);\r\n```\r\n\r\nCan you paste the exact contents of your error message? (A screenshot is OK too.)']",[],[],0,0
348,pytorch,19969,open,libtorch Segmentation Fault: RHEL 7 - easy to reproduce,"## üêõ Bug

tl; dr; I traced a (non-pretrained) resnet18 model in python and saved this to a .pt. I successfully loaded this .pt in a C++ program. Model inference seems to work. When the C++ program exits, I encounter a segmentation fault.

## To Reproduce

Here are code snippets to minimally reproduce the issue:

(1) First, generate the model by running the following python script. If successful, you should see a 0 print out. I'm able to run this code with no problems.



(2) Here is the C++ program I run that generates a segmentation fault. To run, do 



When I run the C++ program, here's the output I receive:



## Things I've tried
- I've tried running the above C++ code on the CPU (so, replacing  with ). This doesn't result in a segmentation fault.

- I've tried running the above Python/C++ code on both CPU, GPU for a simple feedforward net (30 inputs -> 20 hiddens -> ReLU -> 2 outputs -> Sigmoid); this doesn't result in a segmentation fault. Running with  on CPU, I don't see any issues. On GPU, however here's what I get:



## Expected behavior

In running with hard.pt, I expect the code to run without any segfaults.

## Environment

 - PyTorch Version (e.g., 1.0): 1.1.0.dev20190425
 - OS (e.g., Linux): Red Hat Enterprise Linux Server 7.4 (Maipo)
 - How you installed PyTorch (, , source): 
 - Build command you used (if compiling from source): 
 - Python version: 
 - CUDA/cuDNN version: 8.0.44/7
 - GPU models and configuration: 

Versions of relevant libraries:
[pip] numpy==1.15.4
[pip] numpydoc==0.8.0
[pip] torch==1.0.1.post2
[pip] torchvision==0.2.2
[conda] blas                      1.0                         mkl
[conda] mkl                       2019.1                      144
[conda] mkl-service               1.1.2            py37he904b0f_5
[conda] mkl_fft                   1.0.6            py37hd81dba3_0
[conda] mkl_random                1.0.2            py37hd81dba3_0
[conda] pytorch                   1.0.1           py3.7_cuda8.0.61_cudnn7.1.2_2    pytorch
[conda] torchvision               0.2.2                      py_3    pytorch

## Additional context

I've tried following the instructions to resolve this issue here: https://github.com/pytorch/pytorch/issues/12705 but haven't seen success; I've tried building pytorch from source + linking my C++ code to the generated libs, but this doesn't work either.

cc @ezyang @gchanan @zou3519 @ngimel",has workaround high priority module: cuda triaged,"['The same bug occur to me, do you solve it?', ""@SpringSJTU - haven't solved it yet, but haven't actively been working on this."", 'same bug here,\r\n\r\n`torch::Tensor out_tensor = module->forward(inputs).toTensor();`\r\n\r\nthe out_tensor type is [ Variable[CUDAType]{1,10} ], and Segmentation Fault is followed.', 'The same bug. So sad.', 'The same problem. QAQ \r\nDoes anyone have a solution?', 'Encountered the same bug. Anyone solved it yet?', 'I solved it. Use pytorch nightly build üéâ', 'Add ""cudaDeviceSynchronize();"" before and after you do inference. it solved my issue. Also It didn\'t change the speed.', '@vusraju can you paste your codes here? ', ""can't post it. But, just try to synchronize before and after your function/code that deals with gpu computations (forward in this case) and see if it works."", '[ Variable[CUDAType]{1,5} ]\r\nSegmentation fault (core dumped)\r\n', 'same problem, sad', 'for me, the fault is caused from the cmake file dependencies.  I complied the opencv with cuda-on option and add this to my project.\r\nfollowing cmake code cause error:\r\nset(DEP_LIBS ${CUDA_LIBRARIES}  ${TORCH_LIBRARIES} ${OpenCV_LIBS})\r\nchanged this to:\r\nset(DEP_LIBS ${TORCH_LIBRARIES}  ${CUDA_LIBRARIES} ${OpenCV_LIBS})\r\nis ok.\r\nmay this help you!\r\n ', ""@karthiksn Have you solved the problems? I met the same question with you! I'm looking for ward to hearing from you soon!"", ""Haven't solved this, but also haven't worked with this in over a year :)"", ""This is still affecting 1.5 and it appears to be some sort of problem when you link against cudart in the wrong order or something like that. I am not sure if there are downstream consequences of doing this, but if you change your cmake line to\r\n\r\n```\r\ntarget_link_libraries(example-app torch)\r\n```\r\n\r\n(instead of `$TORCH_LIBRARIES`) that seems to reduce the number of times we load cudart enough so that the problem goes away.\r\n\r\nAlso, weirdly, if you run your example-app as `./example-app` inside the directory it was built in there is no segfault, but if you run it in a different directory `build/example-app` it segfaults. I haven't checked if this is a CUDA 9 specific problem."", ""Here is an ldd from an executable that doesn't segfault:\r\n\r\n```\r\nlinux-vdso.so.1 =>  (0x00007fffbe1f0000)\r\nlibtorch.so => /home/ezyang/local/test/libtorch/lib/libtorch.so (0x00007f8dd5eb2000)\r\nlibtorch_cpu.so => /home/ezyang/local/test/libtorch/lib/libtorch_cpu.so (0x00007f8dc7116000)\r\nlibtorch_cuda.so => /home/ezyang/local/test/libtorch/lib/libtorch_cuda.so (0x00007f8d97dee000)\r\nlibc10.so => /home/ezyang/local/test/libtorch/lib/libc10.so (0x00007f8d97b94000)\r\nlibcudart.so.9.2 => /usr/local/cuda-9.2/lib64/libcudart.so.9.2 (0x00007f8d9792a000)\r\nlibnvToolsExt.so.1 => /usr/local/cuda-9.2/lib64/libnvToolsExt.so.1 (0x00007f8d97721000)\r\nlibstdc++.so.6 => /lib64/libstdc++.so.6 (0x00007f8d9741a000)\r\nlibgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00007f8d97204000)\r\nlibc.so.6 => /lib64/libc.so.6 (0x00007f8d96e36000)\r\nlibgomp-7c85b1e2.so.1 => /home/ezyang/local/test/libtorch/lib/libgomp-7c85b1e2.so.1 (0x00007f8d96c0c000)\r\nlibpthread.so.0 => /lib64/libpthread.so.0 (0x00007f8d969f0000)\r\nlibrt.so.1 => /lib64/librt.so.1 (0x00007f8d967e8000)\r\nlibdl.so.2 => /lib64/libdl.so.2 (0x00007f8d965e4000)\r\nlibm.so.6 => /lib64/libm.so.6 (0x00007f8d962e2000)\r\nlibcudart-72ec04ea.so.9.2 => /home/ezyang/local/test/libtorch/lib/libcudart-72ec04ea.so.9.2 (0x00007f8d96075000)\r\n/lib64/ld-linux-x86-64.so.2 (0x00007f8dd60c6000)\r\nlibc10_cuda.so => /home/ezyang/local/test/libtorch/lib/libc10_cuda.so (0x00007f8d95e46000)\r\nlibnvToolsExt-3965bdd0.so.1 => /home/ezyang/local/test/libtorch/lib/libnvToolsExt-3965bdd0.so.1 (0x00007f8d95c3c000)\r\n```\r\n\r\nand an ldd from an executable that does:\r\n\r\n```\r\nlinux-vdso.so.1 =>  (0x00007fff72318000)\r\nlibtorch.so => /home/ezyang/local/test/libtorch/lib/libtorch.so (0x00007fe6f3670000)\r\nlibc10_cuda.so => /home/ezyang/local/test/libtorch/lib/libc10_cuda.so (0x00007fe6f3441000)\r\nlibc10.so => /home/ezyang/local/test/libtorch/lib/libc10.so (0x00007fe6f31e7000)\r\nlibnvrtc.so.9.2 => /usr/local/cuda-9.2/lib64/libnvrtc.so.9.2 (0x00007fe6f1bdd000)\r\nlibnvToolsExt.so.1 => /usr/local/cuda-9.2/lib64/libnvToolsExt.so.1 (0x00007fe6f19d4000)\r\nlibcudart.so.9.2 => /usr/local/cuda-9.2/lib64/libcudart.so.9.2 (0x00007fe6f176a000)\r\nlibtorch_cpu.so => /home/ezyang/local/test/libtorch/lib/libtorch_cpu.so (0x00007fe6e29ce000)\r\nlibtorch_cuda.so => /home/ezyang/local/test/libtorch/lib/libtorch_cuda.so (0x00007fe6b36a6000)\r\nlibstdc++.so.6 => /lib64/libstdc++.so.6 (0x00007fe6b339f000)\r\nlibgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00007fe6b3189000)\r\nlibc.so.6 => /lib64/libc.so.6 (0x00007fe6b2dbb000)\r\nlibcudart-72ec04ea.so.9.2 => /home/ezyang/local/test/libtorch/lib/libcudart-72ec04ea.so.9.2 (0x00007fe6b2b4e000)\r\nlibm.so.6 => /lib64/libm.so.6 (0x00007fe6b284c000)\r\nlibgomp-7c85b1e2.so.1 => /home/ezyang/local/test/libtorch/lib/libgomp-7c85b1e2.so.1 (0x00007fe6b2622000)\r\nlibpthread.so.0 => /lib64/libpthread.so.0 (0x00007fe6b2406000)\r\n/lib64/ld-linux-x86-64.so.2 (0x00007fe6f3884000)\r\nlibdl.so.2 => /lib64/libdl.so.2 (0x00007fe6b2202000)\r\nlibrt.so.1 => /lib64/librt.so.1 (0x00007fe6b1ffa000)\r\nlibnvToolsExt-3965bdd0.so.1 => /home/ezyang/local/test/libtorch/lib/libnvToolsExt-3965bdd0.so.1 (0x00007fe6b1df0000)\r\n```"", ""I'm using LibTorch 1.4. \r\nAfter changing the linking order of CUDA and LibTorch libraries in my own makefile, it seems to work. Here's my solution.\r\nthis will cause segmentation fault when leaving:\r\n```\r\n# cuda\r\nINCLUDE_DIRS    += -I /usr/local/cuda/include/\r\nEXTERNAL_LIBS   += -L/usr/local/cuda/lib64 -lcuda -lcudart -lcublas -lcurand -lcudnn -Wl,-rpath,/usr/local/cuda/lib64\r\n\r\n# torch\r\nINCLUDE_DIRS    += -I ./third_party/torch/include\r\nINCLUDE_DIRS    += -I ./third_party/torch/include/torch/csrc/api/include\r\nEXTERNAL_LIBS   += -L./lib/torch/ -ltorch -lc10 -lc10_cuda -Wl,-rpath,./lib/torch/\r\n```\r\nthis wouldn't:\r\n```\r\n# libtorch\r\nINCLUDE_DIRS    += -I ./third_party/torch/include\r\nINCLUDE_DIRS    += -I ./third_party/torch/include/torch/csrc/api/include\r\nEXTERNAL_LIBS   += -L./lib/torch/ -ltorch -lc10 -lc10_cuda -Wl,-rpath,./lib/torch/\r\n# cuda\r\nINCLUDE_DIRS    += -I /usr/local/cuda/include/\r\nEXTERNAL_LIBS   += -L/usr/local/cuda/lib64 -lcuda -lcudart -lcublas -lcurand -lcudnn -Wl,-rpath,/usr/local/cuda/lib64\r\n```\r\n---\r\nI've checked core file after the segmentation fault, and I find that this fault is happened in cudnn. maybe there's some implicit confliction between libtorch and cudnn?"", 'Even i got the same error for my YOLOV3 model but then after 2 full days of learning about seg faults and debugging using gdb debugger in linux ,the seg fault got resolved when i built using Libtorch nightly version.\r\nPreviously i got seg fault for Libtorch 1.5 with cu 10.1\r\nDo try and let me know for the updates.,\r\n@TheBobbyliu @ezyang @sflc6 ', '@padmasreenagarajan \r\nCan you give more details on how do you solve the error ""segmentation fault(core dumped)""? \r\nI am facing the same problem now. Your inputs will be very helpful.\r\n\r\nEnvironment details:\r\n    OS: SUSE Linux Enterprise Server 12 SP3\r\n    Libtorch: Preview(Nightly) \r\n    CUDA: V10.2\r\n    cuDNN:  v8.0.3\r\n', ""Hello @Jelly123456 ,\r\n\r\n    This is a bug associated with C++ API-LibTorch (Stable) -1.5.1 when destructing cudnn handle pool (in CUDA-10.1)\r\n\r\n**SOLUTION:**\r\n                      This is solved in LibTorch Preview (Nightly) Build.\r\n\r\nSince Nightly build is a research package and a stable version of PyTorch 1.6 was released that time, I have generated the build files with LibTorch 1.6 stable and executed all the test images without any errors (Segmentation fault is resolved)\r\n\r\nSo, I would suggest you to install the latest PyTorch version and the corresponding LibTorch stable version.\r\nIf it still doesn't work, try installing CUDA 10.1 and let me know your issues if any!\r\n\r\nRegards,\r\nPadmasree N."", '@Jelly123456 Can you preproduce the crash using sample in the issue description? Or something more involved?']","['import torch\r\nimport torchvision\r\n\r\n# An instance of your model.\r\nmodel = torchvision.models.resnet18()\r\n\r\n# An example input you would normally provide to your model\'s forward() method.\r\nexample = torch.rand(1, 3, 224, 224).cuda()\r\n\r\n# Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.\r\nmodel.cuda()\r\n\r\nbefore = model(example)\r\nprint(before.shape)\r\n\r\ntraced_script_module = torch.jit.trace(model, example)\r\ntraced_script_module.save(\'hard.pt\')\r\n\r\nload_module = torch.jit.load(""hard.pt"")\r\nafter = load_module(example)\r\n\r\nprint(torch.sum(before - after))\r\n', '\r\n#define _GLIBCXX_USE_CXX11_ABI 0\r\n#include <torch/script.h> // One-stop header.\r\n\r\n#include <iostream>\r\n#include <memory>\r\n#include <vector>\r\n\r\nint main(int argc, const char* argv[]) {\r\n  if (argc != 2) {\r\n    std::cerr << ""usage: example-app <path-to-exported-script-module>\\n"";\r\n    return -1;\r\n  }\r\n\r\n  torch::Tensor tensor = torch::rand({1, 3, 224, 224}, torch::dtype(torch::kFloat32).device(torch::kCUDA));\r\n\r\n  // run\r\n  auto module = torch::jit::load(argv[1], torch::kCUDA);\r\n  assert(module != nullptr);\r\n  std::cout << ""ok"" << std::endl;\r\n\r\n  std::vector<torch::jit::IValue> inputs;\r\n  inputs.push_back(tensor);\r\n\r\n  auto out = module->forward(inputs).toTensor();\r\n  std::cout << out << std::endl;\r\n  std::cout << ""done"" << std::endl;\r\n}\r\n', '\r\n(base) [build]$ rm -rf *; cmake -DCMAKE_PREFIX_PATH=/home/ec2-user/libtorch/ ..; make -j; ./example-app ../hard.pt\r\n-- The C compiler identification is GNU 5.3.1\r\n-- The CXX compiler identification is GNU 5.3.1\r\n-- Check for working C compiler: /opt/rh/devtoolset-4/root/usr/bin/cc\r\n-- Check for working C compiler: /opt/rh/devtoolset-4/root/usr/bin/cc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /opt/rh/devtoolset-4/root/usr/bin/c++\r\n-- Check for working CXX compiler: /opt/rh/devtoolset-4/root/usr/bin/c++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Looking for pthread.h\r\n-- Looking for pthread.h - found\r\n-- Looking for pthread_create\r\n-- Looking for pthread_create - not found\r\n-- Looking for pthread_create in pthreads\r\n-- Looking for pthread_create in pthreads - not found\r\n-- Looking for pthread_create in pthread\r\n-- Looking for pthread_create in pthread - found\r\n-- Found Threads: TRUE\r\n-- Found CUDA: /usr/local/cuda (found version ""8.0"")\r\n-- Caffe2: CUDA detected: 8.0\r\n-- Caffe2: CUDA nvcc is: /usr/local/cuda/bin/nvcc\r\n-- Caffe2: CUDA toolkit directory: /usr/local/cuda\r\n-- Caffe2: Header version is: 8.0\r\n-- Found CUDNN: /usr/local/cuda/include\r\n-- Found cuDNN: v7.1.3  (include: /usr/local/cuda/include, library: /usr/local/cuda/lib64/libcudnn.so)\r\n-- Autodetected CUDA architecture(s):  3.7 3.7 3.7 3.7 3.7 3.7 3.7 3.7\r\n-- Added CUDA NVCC flags for: -gencode;arch=compute_37,code=sm_37\r\n-- Found torch: /home/ec2-user/libtorch/lib/libtorch.so\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /home/ec2-user/example-app/build\r\nScanning dependencies of target example-app\r\n[ 50%] Building CXX object CMakeFiles/example-app.dir/example-app.cpp.o\r\n[100%] Linking CXX executable example-app\r\n[100%] Built target example-app\r\nok\r\nColumns 1 to 10-0.1336 -0.7583 -0.0227  0.0581  0.0842 -0.2745  0.5487  1.0180  0.0602  0.0120\r\n...\r\n...\r\n...\r\nColumns 991 to 1000-0.6911  1.0201  0.2691  0.2674 -0.4418  0.1372  0.3234  0.1259 -0.3892  0.1433\r\n[ Variable[CUDAType]{1,1000} ]\r\ndone\r\nSegmentation fault\r\n', ""==60061== Memcheck, a memory error detector\r\n==60061== Copyright (C) 2002-2015, and GNU GPL'd, by Julian Seward et al.\r\n==60061== Using Valgrind-3.11.0 and LibVEX; rerun with -h for copyright info\r\n==60061== Command: ./example-app ../simple.pt\r\n==60061==\r\n==60061== Warning: set address range perms: large range [0x395de000, 0x66b46000) (defined)\r\n==60061== Warning: noted but unhandled ioctl 0x30000001 with no size/direction hints.\r\n==60061==    This could cause spurious value errors to appear.\r\n==60061==    See README_MISSING_SYSCALL_OR_IOCTL for guidance on writing a proper wrapper.\r\n==60061== Warning: noted but unhandled ioctl 0x27 with no size/direction hints.\r\n==60061==    This could cause spurious value errors to appear.\r\n==60061==    See README_MISSING_SYSCALL_OR_IOCTL for guidance on writing a proper wrapper.\r\n==60061== Warning: noted but unhandled ioctl 0x7ff with no size/direction hints.\r\n==60061==    This could cause spurious value errors to appear.\r\n==60061==    See README_MISSING_SYSCALL_OR_IOCTL for guidance on writing a proper wrapper.\r\n==60061== Warning: noted but unhandled ioctl 0x25 with no size/direction hints.\r\n==60061==    This could cause spurious value errors to appear.\r\n==60061==    See README_MISSING_SYSCALL_OR_IOCTL for guidance on writing a proper wrapper.\r\n==60061== Warning: noted but unhandled ioctl 0x17 with no size/direction hints.\r\n==60061==    This could cause spurious value errors to appear.\r\n==60061==    See README_MISSING_SYSCALL_OR_IOCTL for guidance on writing a proper wrapper.\r\n==60061== Warning: set address range perms: large range [0x1000000000, 0x8a00000000) (noaccess)\r\n==60061== Warning: set address range perms: large range [0x8a00000000, 0xa200000000) (noaccess)\r\n==60061== Warning: noted but unhandled ioctl 0x19 with no size/direction hints.\r\n==60061==    This could cause spurious value errors to appear.\r\n==60061==    See README_MISSING_SYSCALL_OR_IOCTL for guidance on writing a proper wrapper.\r\n==60061== Warning: noted but unhandled ioctl 0x21 with no size/direction hints.\r\n==60061==    This could cause spurious value errors to appear.\r\n==60061==    See README_MISSING_SYSCALL_OR_IOCTL for guidance on writing a proper wrapper.\r\n==60061== Warning: noted but unhandled ioctl 0x1b with no size/direction hints.\r\n==60061==    This could cause spurious value errors to appear.\r\n==60061==    See README_MISSING_SYSCALL_OR_IOCTL for guidance on writing a proper wrapper.\r\nok\r\n 0.4127  0.4883\r\n[ Variable[CUDAType]{1,2} ]\r\ndone\r\n==60061==\r\n==60061== HEAP SUMMARY:\r\n==60061==     in use at exit: 1,247,853,758 bytes in 1,312,577 blocks\r\n==60061==   total heap usage: 2,763,800 allocs, 1,451,223 frees, 1,808,988,116 bytes allocated\r\n==60061==\r\n==60061== LEAK SUMMARY:\r\n==60061==    definitely lost: 440 bytes in 7 blocks\r\n==60061==    indirectly lost: 2,344 bytes in 24 blocks\r\n==60061==      possibly lost: 403,300,908 bytes in 155,594 blocks\r\n==60061==    still reachable: 844,550,066 bytes in 1,156,952 blocks\r\n==60061==                       of which reachable via heuristic:\r\n==60061==                         stdstring          : 438,097 bytes in 6,137 blocks\r\n==60061==         suppressed: 0 bytes in 0 blocks\r\n==60061== Rerun with --leak-check=full to see details of leaked memory\r\n==60061==\r\n==60061== For counts of detected and suppressed errors, rerun with: -v\r\n==60061== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)"", 'GPU models and configuration:\r\nGPU 0: Tesla K80\r\nGPU 1: Tesla K80\r\nGPU 2: Tesla K80\r\nGPU 3: Tesla K80\r\nGPU 4: Tesla K80\r\nGPU 5: Tesla K80\r\nGPU 6: Tesla K80\r\nGPU 7: Tesla K80\r\n']","['./example-app /path/to/hard.pt', 'kCUDA', 'kCPU', 'valgrind', 'conda', 'pip', 'conda', 'N/A', '3.7']",0,0
349,pytorch,11389,open,[distributions] Torch distribution samplers slow on expanded parameters,"## Issue description
We use expanded tensors as distribution parameters in many cases where we dynamically broadcast the parameters at runtime. While working on a related (https://github.com/pytorch/pytorch/pull/11341) PR, I noticed that sampling can be slow when using expanded tensors as distribution parameters. I have narrowed this slowdown to the native torch samplers.

While this is likely expected behavior, it raises the question of:
 (a) whether we should be doing anything inside of distributions to ensure that parameter tensors are contiguous, and if so,
 (b) under what conditions should we ensure contiguity - always by default, or have it be controllable by the user via an optional keyword argument. If we use the same instance to draw multiple samples, it is worth the one time cost of calling  on the distribution parameters (I think, given the relatively low overhead of , we can probably make it the default). 

## Profiling code


cc. @fritzo, @vishwakftw 


cc @fritzo @neerajprad @alicanb @vishwakftw @nikitaved",module: distributions todo triaged,[],"['python\r\nIn [36]: m1, s1 = torch.ones(2, 10, 10000), torch.ones(2, 10, 10000)\r\n\r\nIn [37]: m2, s2 = torch.ones(10000).expand([2, 10, 10000]), torch.ones(10, 10000).expand([2, 10, 10000])\r\n\r\nIn [38]: m3, s3 = m2.contiguous(), s2.contiguous()\r\n\r\nIn [39]: m1.is_contiguous(), m2.is_contiguous(), m3.is_contiguous()\r\nOut[39]: (True, False, True)\r\n\r\nIn [40]: %timeit -n 1000 -r 10 torch.normal(m1, s1)\r\n1.53 ms ¬± 47 ¬µs per loop (mean ¬± std. dev. of 10 runs, 1000 loops each)\r\n\r\nIn [41]: %timeit -n 1000 -r 10 torch.normal(m2, s2)\r\n1.98 ms ¬± 18 ¬µs per loop (mean ¬± std. dev. of 10 runs, 1000 loops each)\r\n\r\nIn [42]: %timeit -n 1000 -r 10 torch.normal(m3, s3)\r\n1.5 ms ¬± 36.6 ¬µs per loop (mean ¬± std. dev. of 10 runs, 1000 loops each)\r\n\r\nIn [43]: %timeit m2.contiguous()\r\n134 ¬µs ¬± 1.25 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10000 loops each)\r\n']","['.contiguous', '.contiguous']",0,0
350,pytorch,15284,open,index_add_ with scalar values instead of tensors,"Hi, I was wondering whether we could update functions like  so that the third argument () can also be a scalar value?",enhancement module: advanced indexing triaged,['Seems like a reasonable ask.'],[],"['index_add_(dim, index, tensor)', 'tensor']",0,0
351,pytorch,14659,closed,error: module multiprocessing.util' has no attribute '_flush_std_streams,"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:

1. I followed the tutorial code: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html
1. run without change anything, but error comes out. even I download the official code, error still exist.
1. seems like cannot enumerate trainloader. once do enumerate(trainloader), error appears: ""module multiprocessing.util' has no attribute '_flush_std_streams""

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

PyTorch version: 0.4.1
Is debug build: No
CUDA used to build PyTorch: 8.0.61

OS: Ubuntu 16.04 LTS
GCC version: (Ubuntu 4.9.3-13ubuntu2) 4.9.3
CMake version: version 3.5.1

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: Tesla P100-PCIE-16GB
GPU 1: Tesla P100-PCIE-16GB
GPU 2: Tesla P100-PCIE-16GB
GPU 3: Tesla P100-PCIE-16GB
GPU 4: Tesla P100-PCIE-16GB
GPU 5: Tesla P100-PCIE-16GB
GPU 6: Tesla P100-PCIE-16GB
GPU 7: Tesla P100-PCIE-16GB

Nvidia driver version: 384.90
cuDNN version: Probably one of the following:
/usr/local/MATLAB/R2016b/bin/glnxa64/libcudnn.so.4.0.7

Versions of relevant libraries:
[pip] numpy (1.15.4)
[pip] torch (0.4.1)
[pip] torchvision (0.2.1)
[conda] cuda80                    1.0                  h205658b_0    pytorch
[conda] pytorch                   0.4.1           py36_cuda8.0.61_cudnn7.1.2_1  [cuda80]  pytorch
[conda] torchvision               0.2.1                    py36_1    pytorch

## Additional context

<!-- Add any other context about the problem here. -->
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-12-a647eac09f6e> in <module>
     21         plt.title('Batch from dataloader')
     22 
---> 23 for i_batch, sample_batched in enumerate(dataloader):
     24     print(i_batch, sample_batched['image'].size(),
     25           sample_batched['landmarks'].size())

~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py in __iter__(self)
    499 
    500     def __iter__(self):
--> 501         return _DataLoaderIter(self)
    502 
    503     def __len__(self):

~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py in __init__(self, loader)
    287             for w in self.workers:
    288                 w.daemon = True  # ensure that the worker exits on process exit
--> 289                 w.start()
    290 
    291             _update_worker_pids(id(self), tuple(w.pid for w in self.workers))

~/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py in start(self)
    103                'daemonic processes are not allowed to have children'
    104         _cleanup()
--> 105         self._popen = self._Popen(self)
    106         self._sentinel = self._popen.sentinel
    107         # Avoid a refcycle if the target function holds an indirect

~/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py in _Popen(process_obj)
    221     @staticmethod
    222     def _Popen(process_obj):
--> 223         return _default_context.get_context().Process._Popen(process_obj)
    224 
    225 class DefaultContext(BaseContext):

~/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py in _Popen(process_obj)
    275         def _Popen(process_obj):
    276             from .popen_fork import Popen
--> 277             return Popen(process_obj)
    278 
    279     class SpawnProcess(process.BaseProcess):

~/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py in __init__(self, process_obj)
     15 
     16     def __init__(self, process_obj):
---> 17         util._flush_std_streams()
     18         self.returncode = None
     19         self._launch(process_obj)

AttributeError: module 'multiprocessing.util' has no attribute '_flush_std_streams'

===================================================================
I test the code on windows, still got the error: [Errno 32] Broken pipe",,"['The access is from within python multiprocessing lib. So there is likely something wrong with your python install.', 'Have you solved this problem? I meet the same problem. When I set `num_workers` as default value, the error disappeared.', ""Closing this for now as stale; feel free to reopen if it's reproducible on a modern version of PyTorch.""]",[],[],0,0
352,pytorch,17231,open,testing.assert_allclose should assert tensors have the same devices,"## üêõ Bug

This unit test should pass:

        ind = tor.LongTensor([110, 125, 235, 333, 404]).cuda()
        nd_shape = tor.LongTensor([10, 10, 10]).cuda()
        xy = array_to_nd_index(ind, nd_shape)
        result = [[1, 1, 0],
                  [1, 2, 5],
                  [2, 3, 5],
                  [3, 3, 3],
                  [4, 0, 4]]
        tortest.assert_allclose(result, xy)

However, I get:

        RuntimeError: expected type torch.LongTensor but got torch.cuda.LongTensor

Here's the solution I wrote in my own assert function, though it may be a bit permissive:

        if a.is_cuda and not b.is_cuda:
            b = b.cuda()
        elif b.is_cuda and not a.is_cuda:
            a = a.cuda()

## Environment

 - PyTorch Version (e.g., 1.0):    1.0.1
 - OS (e.g., Linux):    Windows
 - How you installed PyTorch (, , source):    pip
 - Python version:    3.6
 - CUDA/cuDNN version:    0.9
",module: testing triaged,"[""On it's face, this seems like a reasonable UX improvement, but there are some problems.\r\n\r\n1. Should we convert to CPU, or convert to CUDA? There are merits in both directions. Conversion to CUDA (as you suggested) will lead to a faster allclose computation but we have to ship the result back to CPU to actually report if it worked or not. Conversion to CPU saves on CUDA space.\r\n2. We're going to soon have other device types besides CUDA running around. What should happen in those cases?\r\n\r\nWe don't really have any precedent for this case, so it's hard to say what to do."", 'We should never change the device, right? We can simply check that the device of all arguments matches, and use this for all returns as well. That solves the multi-device case too.', ""Asserting the devices match might be a good idea too. It's a little clear for that already, but it could be clearer.\r\n\r\nIf it was automatically put in a device, I'd prefer the device with the most memory, since it wouldn't have a memory error. So, I think,  server>cpu>gpu."", ""@SimLeek Yeah, why don't we assert the devices match. Do you want to submit the PR for this?"", ""Yeah. Sorry it took so long. I'll be submitting a PR soon.\r\n\r\nEdit: Well, not today. I switched to Ubuntu since it's easier to develop code, but windows is being a pain."", ""I've updated the title of this issue and its labels. We would accept a PR implementing this, although I expect the design of torch.testing may change in the near future. ""]",[],"['conda', 'pip']",0,0
353,pytorch,12609,open,Request for stripped down / inference only pytorch wheels,"## üöÄ Feature
Creating a precompiled pytorch wheel file that is trimmed down, inference only version.

## Motivation

Right now pytorch wheels are on average ~400MB zipped -> 1.+ GB unzipped, which is not a big deal for training & prototyping as generally the wheels are only installed once - but that's not the case for productionizing using service providers like sagemaker / algorithmia / etc.

## Pitch

If we can create a trimmed down, potentially inference only capable wheel file - we can directly improve the load time performance of these algorithms in serverless algorithm delivery environments, which could directly pytorch's ability to compete in the HPC serverless marketplace.

## Alternatives

We could also provide a clear way for users to create their own wheels, by simplifying and documenting the build process somewhat to enable optional features during the compilation process. 

## Additional context

Full disclosure, I'm an employee at Algorithmia and this change would make my life much easier :smile: 


cc @malfet @seemethere @walterddr",module: build triaged,"['Do you need CPU only, or CUDA as well? A CPU wheel is substantially smaller than the CUDA one.', 'As edward said, the CPU-only wheel is 10x smaller, would it be sufficient for your purposes? you can download it from the website by selecting ""CUDA: None""', 'CPU only definitely gets us somewhere - although we still find it slower to download and unpack than some other ML frameworks. However, we do also want to ensure that our users can access a gpu enabled wheel as well, while not sacrificing load time performance anywhere nearly as severely as they are today. \r\nAt the moment, because of how long the pytorch GPU wheel takes to zip & unzip on our infrastructure, inference algorithms (think autoencoders & other DNN models) that depend on the GPU wheel are nearly impossible to use.', ""I think it'd be generally a good idea for us to pave the wheel-building codepath. Users can very easily, without our intervention, get stripped down builds by, for example, specifying only the architectures they care about. It will be much harder to omit backward kernels, however.""]",[],[],0,0
354,pytorch,31591,open,"when i try to export a pytorch model to ONNX, got RuntimeError: output of traced region did not have observable data dependence with trace inputs; this probably indicates your program cannot be understood by the tracer.","## üêõ Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:

/Users/duke/opt/anaconda3/bin/python /Volumes/Êó†ÊûÅÂ±±/project/code/pytorch2ONNX.py
/Volumes/Êó†ÊûÅÂ±±/project/code/models/anchors.py:24: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  image_shape = np.array(image_shape)
/Volumes/Êó†ÊûÅÂ±±/project/code/models/anchors.py:35: TracerWarning: torch.from_numpy results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.
  return torch.from_numpy(all_anchors.astype(np.float32))
Traceback (most recent call last):
  File ""/Volumes/Êó†ÊûÅÂ±±/project/code/pytorch2ONNX.py"", line 39, in <module>
    save2ONNX(args)
  File ""/Volumes/Êó†ÊûÅÂ±±/project/code/pytorch2ONNX.py"", line 27, in save2ONNX
    torch.onnx.export(model, x, export_ONNX_file)
  File ""/Users/duke/opt/anaconda3/lib/python3.7/site-packages/torch/onnx/__init__.py"", line 143, in export
    strip_doc_string, dynamic_axes, keep_initializers_as_inputs)
  File ""/Users/duke/opt/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py"", line 66, in export
    dynamic_axes=dynamic_axes, keep_initializers_as_inputs=keep_initializers_as_inputs)
  File ""/Users/duke/opt/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py"", line 382, in _export
    fixed_batch_size=fixed_batch_size)
  File ""/Users/duke/opt/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py"", line 249, in _model_to_graph
    graph, torch_out = _trace_and_get_graph_from_model(model, args, training)
  File ""/Users/duke/opt/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py"", line 206, in _trace_and_get_graph_from_model
    trace, torch_out, inputs_states = torch.jit.get_trace_graph(model, args, _force_outplace=True, _return_inputs_states=True)
  File ""/Users/duke/opt/anaconda3/lib/python3.7/site-packages/torch/jit/__init__.py"", line 275, in get_trace_graph
    return LegacyTracedModule(f, _force_outplace, return_inputs, _return_inputs_states)(*args, **kwargs)
  File ""/Users/duke/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File ""/Users/duke/opt/anaconda3/lib/python3.7/site-packages/torch/jit/__init__.py"", line 356, in forward
    torch._C._tracer_exit(tuple(out_vars))
RuntimeError: output of traced region did not have observable data dependence with trace inputs; this probably indicates your program cannot be understood by the tracer.

Process finished with exit code 1


<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->
##Here is the code
import torch
import torch.onnx
from torch.autograd import Variable
from models.retinanet import resnet34, resnet50
from build_network import build_network
input_model = '../user_data/model_data/model_at_epoch_1.dat'
output_dir = 'torch_onnx_model.onnx'
dummy_input = torch.randn(2, 3, 800, 800)
checkpoint = torch.load(input_model)
model, _ = build_network(snapshot=None, backend='retinanet')
model.load_state_dict(checkpoint['state_dict'])
torch.onnx.export(model, dummy_input, output_dir)
print('Done')



## Expected behavior

The ONNX should be exported successfully.

## Environment
Python 3.7.4
pytorch '1.3.1'
macos 10.15




cc @suo",needs reproduction oncall: jit triaged,"[""Thanks for the report! As the error message indicates, the tracer detected that the output of your model didn't have any relationship to the input. Without seeing your model, it's not possible for us to tell why the tracer failed in this instance; can you post a small script that we can run to reproduce your issue?"", 'Hi, I just wanted to confirm since I\'ve been getting this error as well. By ""having a relationship to the input"" do you mean that the output tensor should be in the same computational graph as the input? ', ""Yeah, basically the tracer is detecting that‚Äîbased on its understanding‚Äîthe input doesn't affect the output at all. So changing the input would not lead to a change in output.\r\n\r\nSince that is rarely actually the case (because there's not much use for a function that does not depend on its inputs), we consider it a red flag that the tracer messed up in some way. "", 'I have the same problem', 'so do i, same problem']",[],[],0,0
355,pytorch,28472,open,at::Tensor::data() is deprecated but no other way is suggested for cpp extensions,"## üêõ Bug

Even the official [docs for cpp extensions](https://pytorch.org/tutorials/advanced/cpp_extension.html) use  but I get deprecation warning.

## To Reproduce

>  warning: ‚ÄòT* at::Tensor::data() const [with T = float]‚Äô is deprecated [-Wdeprecated-declarations]                                                                                                                               /home/ehazar/miniconda3/envs/py3_night/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:312:1: note: declared here                                                                                                                                                 T * data() const {


## Expected behavior

Provide a non-deprecated way for cpp extensions. Similarly, no alternative to / nor  is provided for cpp extensions.

## Environment

Collecting environment information...
PyTorch version: 1.4.0.dev20191018
Is debug build: No
CUDA used to build PyTorch: 10.0

OS: Ubuntu 16.04.6 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: GeForce GTX 1070
Nvidia driver version: 418.40.04
cuDNN version: /usr/local/lib/libcudnn.so.5.1.10

Versions of relevant libraries:
[pip] numpy==1.17.2
[pip] torch==1.4.0.dev20191018
[pip] torchvision==0.5.0a0+155c504
[conda] Could not collect


cc @yf225",OSS contribution wanted better-engineering module: cpp-extensions module: docs triaged,"['I use .data_ptr to replace .data. There is no warning then. But i am not sure whether this is the suggedted api.', 'As @shiyemin mentioned, `.data_ptr<scalar_t>()` is the recommended approach now.\r\nFor `AT_CHECK`, use `TORCH_CHECK`.\r\nFor `torch::jit::RegisterOperators()`, use `torch::RegisterOperators()`']",[],"['gates.data<scalar_t>()', 'AT_CHECK', 'AT_ASSERTM', 'torch::jit::RegisterOperators()']",0,0
356,pytorch,7773,open,[feature request] Add Local Contrast Normalization ,"## Issue description
As mentioned in this paper  :- http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf
I noticed that Local Response Norm is present. This is will be a good addition too.
I have an implementation ready and can create a PR soon, if approved.


cc @albanD @mruberry",enhancement module: nn triaged,"[""I also believe it's a good idea to add this, especially if it's already implemented :)\r\nLCN is still useful in certain cases."", 'I am interesting in the implementation, can your give the code gist for reference? @dibyadas ', 'Hi @guanfuchen  Sure. Here is the [link](https://github.com/dibyadas/Visualize-Normalizations/blob/master/LocalContrastNorm.ipynb). It works but can be optimized further I believe. ', 'Is this feature still desired?']",[],[],0,0
357,pytorch,20117,open,[FR] [RFC] add Sequential.append & .extend,"A common pattern people use in  of a  is to build a  first and then feed it into  because  doesn't support many handy methods existing on . E.g., 



This is totally unnecessary if we can just provide  and  on .",feature module: nn triaged,"['FYI: [chainer.Sequential](https://docs.chainer.org/en/latest/reference/generated/chainer.Sequential.html#chainer.Sequential) supports those methods and could be a reference.', ""@crcrpar Thanks for the reference! Chainer also has `insert`, `pop`, and `repeat`, all of which could be useful. I'm curious about `flatten` though, what's the use of that? :)"", 'TL; DR: the method is for better performance.\r\n\r\nIn Chainer, `Link`, the base class of `Sequential`, `Chain`, and concrete NN layers w/ learnable parameters such as Conv & BN has `repeat` method that creates a new `Sequential` object from itself [docs](https://docs.chainer.org/en/stable/reference/generated/chainer.Link.html#chainer.Link.repeat).\r\ne.g. `Linear(...).repeat(2)` is equivalent to `Sequential(Linear(...), Linear(...))`.\r\n\r\nSo, `Sequential(LayerA(), LayerB()).repeat(2)` returns nested model of `Sequential(Sequential(LayerA(), LayerB()), Sequential(LayerA(), LayerB())`, not `Sequential(LayerA(), LayerB(), LayerA(), LayerB())`.\r\n\r\nBut the latter is better from the perspective of overhead & readability.\r\nHere `flatten()` method can take care of this.', 'I see. Thanks!', 'Can I work on this? I would like to implement the features discussed here :)', ""I think it's reasonable to add that, but I'm not convinced about the `.repeat()` and `.flatten()` API. Let's leave that out."", '@codexetreme yes, please go ahead!', 'Yep I agree with @apaszke . `append`, `extend`, `insert`, and `pop` would be great!', ""Cool, ill work on `append`, `extend`, `insert`, `pop`. I'll submit a basic PR then keep pushing work to it and ask you guys if it's going the way it's supposed to ( since this is only my second issue so far, so I will need some input)"", 'I agree. `repeat` and `flatten` might confuse users.']","['py\r\ndef __init__(self, logres):\r\n    super().__init__():\r\n    layers = [\r\n          nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\r\n          nn.ReLU(inplace=True),\r\n          nn.MaxPool2d(kernel_size=3, stride=2),\r\n    ]\r\n\r\n    for _ in range(logres):\r\n         layers.extend([\r\n              nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\r\n              nn.ReLU(inplace=True),\r\n              nn.MaxPool2d(kernel_size=3, stride=2),\r\n        ])\r\n    self.layers = nn.Sequential(*layers)\r\n']","['__init__', 'nn.Module', 'list', 'nn.Sequential', 'Sequential', 'list', '.append', '.extend', 'Sequential']",0,0
358,pytorch,8386,open,Properly release NCCL resources,"Right now we're simply leaking them, because NCCL segfaults if you attempt to destroy them after the driver is unloaded.

One strategy to fix it would be to use an  handler.

See #8352 for more context.",module: nccl triaged,"[""Just FYI, it seems NCCL segfaults even if the driver hasn't been unloaded. I tried adding a check before calling ncclCommDestroy: \r\n\r\n`if (cudaGetDeviceCount(&dummy_var) != cudaSuccess) {\r\n`\r\n\r\nAnd I'm getting dummy_var as 8 (I have 8 GPUs in the box), which means CUDA driver is not unloaded, but NCCL is still segfaulting. "", 'Still a thing in the codebase']",[],['atexit'],0,0
359,pytorch,14366,open,How to use model.net.Clip?,"I want to clip loss using Clip op like this model.net.Clip([input,output],  0,10)  but it dosen't work!!
the Operators Catalog of caffe2 is too simple. Does anyone can help me? Thank you very much!!!",caffe2,[],[],[],0,0
360,pytorch,11978,open,[Caffe2] Attempting to install Caffe2 in Google Colab,"## Issue description

Hello, I'm trying to install Caffe2 in Google Colab so I can work with Detectron.
I've tried installing from Source however it takes too long (more than 2 hours) so it is impractical for Colab. Thus, I'm forced to install from binaries using Anaconda.

I install anaconda without much issues.



However, I'm trying to follow the [recommendation](https://caffe2.ai/docs/faq.html#why-is-caffe2-not-working-as-expected-in-anaconda) to install Caffe2 on a different environment than the base one.
I manage to create the environment just fine, however, trying to activate it does not work. In particular using:  seems to do nothing. So it seems that's out.

So, I go and install Caffe2 using the base anaconda environment. First, I have to add some code to add the anaconda binaries to the PATH, as it seems the usual way to assign environmental variables in Colab seem to not work.

Then installation as usual:


It installs without any issues. However, when trying to run the test code:

It returns:


I understand this is probably some PYTHONPATH [issue](https://github.com/facebookresearch/Detectron/blob/master/INSTALL.md#caffe2), however, I can't seem to find the ""build"" folder in caffe2.
If I search for caffe2 directory ()  after installing I only get the following results:

If I search in the directories of each result I get:

ls /root/anaconda2/pkgs/caffe2-cuda9.0-cudnn7-0.8.dev-py27_2018.08.26/lib/python2.7/site-packages/caffe2:

ls /root/anaconda2/pkgs/caffe2-cuda9.0-cudnn7-0.8.dev-py27_2018.08.26/include/caffe2:

ls /root/anaconda2/lib/python2.7/site-packages/caffe2:

ls /root/anaconda2/include/caffe2:

None of them has a build directory.

So finally I have some questions:

- **Was the Caffe2 installation successful?** I wonder given that I cannot find the 'build' directory.
- **Is this only a PYTHONPATH variable issue?** If so, to which folder should I point it?

I'm sorry if the question is noobish. But I'm a bit stuck. Thanks in advance!

### System Settings

- PyTorch or Caffe2: Caffe2
- How you installed PyTorch (conda, pip, source): conda
- Build command you used (if compiling from source):
- OS: Linux, whatever Colab's using.
- PyTorch version:
- Python version: 2.7
- CUDA/cuDNN version: 9.0 and 7.0
- GPU models and configuration:  Tesla K80
- GCC version (if compiling from source):
- CMake version:
- Versions of any other relevant libraries:
",caffe2,"['I have the same problem of lunching the Detectron and installing Caffe2 in Colab. Any progress on that?', 'In the end, I just did it using the provided docker image on our own\nserver. Sorry!\n\nOn Thu, Dec 6, 2018 at 4:23 PM Dene <notifications@github.com> wrote:\n\n> I have the same problem of lunching the Detectron and installing Caffe2 in\n> Colab. Any progress on that?\n>\n> ‚Äî\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/11978#issuecomment-444996684>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHneoFWXoOEB4WmfIrX9AS7ZXbiQ89z2ks5u2W6UgaJpZM4W1Xg8>\n> .\n>\n', ""I've managed to run Detectron (Caffe2 installed) in Google Colab, you can find the notebook here. https://github.com/Dene33/Detectron/blob/master/notebooks/detectron(google_colab).ipynb\r\n\r\nI also sent a pull request to the main repo, so hope to see it there as well."", ""Thanks so much!!!\n\nOn Fri, Dec 7, 2018 at 10:45 AM Dene <notifications@github.com> wrote:\n\n> I've managed to run Detectron (Caffe2 installed) in Google Colab, you can\n> find the notebook here.\n> https://github.com/Dene33/Detectron/blob/master/notebooks/detectron(google_colab).ipynb\n>\n> I also sent a pull request to the main repo, so hope to see it there as\n> well.\n>\n> ‚Äî\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/11978#issuecomment-445236850>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHneoFCE-Rr2I-L4sbxCs4dcY-w19HrPks5u2nD8gaJpZM4W1Xg8>\n> .\n>\n"", 'Resolving anaconda.org (anaconda.org)... 104.16.4.213, 104.16.2.213, 104.16.3.213, ...\r\nConnecting to anaconda.org (anaconda.org)|104.16.4.213|:443... connected.\r\nHTTP request sent, awaiting response... 404 NOT FOUND\r\n2019-02-03 16:31:27 ERROR 404: NOT FOUND.', ""i tried this but it didn't work\r\n> I've managed to run Detectron (Caffe2 installed) in Google Colab, you can find the notebook here. https://github.com/Dene33/Detectron/blob/master/notebooks/detectron(google_colab).ipynb\r\n> \r\n> I also sent a pull request to the main repo, so hope to see it there as well.\r\n\r\nit gave me this error:\r\n--2019-10-08 19:19:41--  https://anaconda.org/pytorch/pytorch-nightly/1.0.0.dev20181206/download/linux-64/pytorch-nightly-1.0.0.dev20181206-py2.7_cuda9.2.148_cudnn7.4.1_0.tar.bz2\r\nResolving anaconda.org (anaconda.org)... 104.17.93.24, 104.17.92.24, 2606:4700::6811:5c18, ...\r\nConnecting to anaconda.org (anaconda.org)|104.17.93.24|:443... connected.\r\nHTTP request sent, awaiting response... 404 NOT FOUND\r\n2019-10-08 19:19:41 ERROR 404: NOT FOUND.\r\n\r\ntar (child): pytorch-nightly-1.0.0.dev20181206-py2.7_cuda9.2.148_cudnn7.4.1_0.tar.bz2: Cannot open: No such file or directory\r\ntar (child): Error is not recoverable: exiting now\r\ntar: Child returned status 2\r\ntar: Error is not recoverable: exiting now\r\ncp: cannot stat 'lib/python2.7/site-packages/*': No such file or directory"", ""> I've managed to run Detectron (Caffe2 installed) in Google Colab, you can find the notebook here. https://github.com/Dene33/Detectron/blob/master/notebooks/detectron(google_colab).ipynb\r\n> \r\n> I also sent a pull request to the main repo, so hope to see it there as well.\r\n\r\nI have also tried this but no joy, running into the same errors as the two above, anyone had any luck?""]","['\r\nimport os\r\nos.environ[\'PATH\'] += "":/root/anaconda2/bin""\r\n', ""\r\n# To check if Caffe2 build was successful\r\npython2 -c 'from caffe2.python import core'\r\n\r\n"", '\r\nTraceback (most recent call last):\r\n  File ""<string>"", line 1, in <module>\r\nImportError: No module named caffe2.python\r\n', '\r\n/root/anaconda2/pkgs/caffe2-cuda9.0-cudnn7-0.8.dev-py27_2018.08.26/lib/python2.7/site-packages/caffe2\r\n/root/anaconda2/pkgs/caffe2-cuda9.0-cudnn7-0.8.dev-py27_2018.08.26/include/caffe2\r\n/root/anaconda2/lib/python2.7/site-packages/caffe2\r\n/root/anaconda2/include/caffe2\r\n', '\r\ncontrib  distributed  __init__.py   perfkernels  python\r\ncore\t experiments  __init__.pyc  proto\r\n', '\r\ncontrib   distributed  mkl\t  onnx\t       predictor  sgd\t      video\r\ncore\t  experiments  mobile\t  operators    proto\t  share\r\ncuda_rtc  ideep        mpi\t  opt\t       python\t  transforms\r\ndb\t  image        observers  perfkernels  queue\t  utils\r\n', '\r\ncontrib  distributed  __init__.py   perfkernels  python\r\ncore\t experiments  __init__.pyc  proto\r\n', '\r\ncontrib   distributed  mkl\t  onnx\t       predictor  sgd\t      video\r\ncore\t  experiments  mobile\t  operators    proto\t  share\r\ncuda_rtc  ideep        mpi\t  opt\t       python\t  transforms\r\ndb\t  image        observers  perfkernels  queue\t  utils\r\n']","['!bash ./Anaconda2-5.2.0-Linux-x86_64.sh', 'source activate test_caffe2', 'conda install -c caffe2 caffe2-cuda9.0-cudnn7', ""find / -type d -name 'caffe2'""]",0,0
361,pytorch,31277,open,nn.MultiHeadAttention with different similarity measures,"## üöÄ Feature
<!-- A clear and concise description of the feature proposal -->

## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->

## Pitch

<!-- A clear and concise description of what you want to happen. -->

Current nn.MultiHeadAttention uses matrix multiplication similarity, i.e., (Q@K.t()), but variants of this similarity are not available directly, for example, 

dot product similarity, i.e., 
, 

additive similarity, 
, 

general dot product similarity, 
.

These variants should also be in PyTorch.


## Alternatives

<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->

## Additional context

<!-- Add any other context or screenshots about the feature request here. -->


cc @zhangguanheng66",function request oncall: transformer/mha triaged,"['cc @zhangguanheng66 what do you think?', 'Good point. We plan to break the long MultiheadAttention function into multiple building blocks. In that case, you could replace the scaled-product part with a custom one.']","['(Q*K.t())', '(Wq*Q.t() + Wk*K.t())', '(Q*W*K.t())']",[],0,0
362,pytorch,25045,open,[Distance functions] F.pdist backward CUDA invalid configuration,"## üêõ Bug

After passing the input through a  function, the gradient *w.r.t.* to the input can not be correctly calculated on a cuda device.

## To Reproduce

Steps to reproduce the behavior:

3. When increasing the tensor size and running

the error message is:


<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


 - PyTorch Version: **1.2.0**
 - OS (e.g., Linux): **CentOS 7.6.1810 (Core)**
 - How you installed PyTorch (, , source): **conda**
 - Build command you used (if compiling from source): N/A
 - Python version: **3.6.7**
 - CUDA/cuDNN version: **V10.0.130**
 - GPU models and configuration: **GeForce GTX 1080**
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->

cc @ngimel",module: cuda module: distance functions triaged,"[""A leaf node that collects gradient: `x = torch.rand(dim, 4, requires_grad=True)`\r\n\r\nAn intermediate result that does not collect gradient: `x = torch.rand(dim, 4, requires_grad=True).to('cuda')`\r\n\r\nUse `x = torch.rand(dim, 4, requires_grad=True, device=device)` instead."", ""Another solution can be found [here](https://discuss.pytorch.org/t/grad-is-none-even-when-requires-grad-true/29826): \r\nIf you want to retain those gradients, call `w.retain_grad()` before calling `backward()`.\r\nThe result is as follows:\r\n\r\n```\r\ntensor([[-7.7181e-04,  1.0914e-03,  2.4808e-04, -1.8249e-03],\r\n        [ 9.2315e-04,  1.4243e-03,  4.7810e-04, -2.3060e-04],\r\n        [ 1.6089e-03,  7.2334e-04,  1.6088e-03,  2.2344e-04],\r\n        ...,\r\n        [ 1.6721e-03, -2.4401e-04,  1.1168e-03,  1.3291e-03],\r\n        [ 1.4105e-03,  6.3654e-04,  1.3851e-03, -8.1672e-04],\r\n        [-1.5194e-03, -9.8220e-04, -1.0435e-03, -6.5006e-05]])\r\ntensor([[ 9.4154e-04, -1.8999e-03, -7.8445e-04,  3.2105e-04],\r\n        [-1.9154e-03, -8.5609e-05,  9.3095e-04,  6.7951e-04],\r\n        [ 4.1913e-04, -4.2681e-04,  3.1998e-04, -2.8135e-04],\r\n        ...,\r\n        [-1.7634e-03,  2.1089e-04, -4.7719e-04, -1.3069e-03],\r\n        [-7.2096e-04, -1.1096e-03, -1.3573e-03,  1.0148e-03],\r\n        [-5.4195e-04,  6.4787e-04,  1.8998e-03, -1.0437e-03]], device='cuda:0')\r\n```"", 'Thanks @SsnL @AlbertHuyb, the first problem that there is no gradient for the input tensor on GPU has been solved. However, I met another problem in the backward pass. When running:\r\n```python\r\nimport torch\r\nimport torch.nn.functional as F\r\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\r\nprint(""device: "", device)\r\n\r\ndim = 1024\r\nx = torch.rand(dim, 4, requires_grad=True, device=device)\r\ny = torch.ones(dim, 1, requires_grad=False, device=device)\r\nvdist = F.pdist(x, 2)\r\ntdist = F.pdist(y, 1)\r\n\r\nvdist = 1/(1+vdist**2)\r\nloss = F.binary_cross_entropy(vdist, tdist)\r\nloss.backward()\r\nprint(x.grad.data)\r\n```\r\nthe `loss.backward()` works properly and produce:\r\n```\r\ntensor([[-8.7084e-04,  2.3595e-04, -2.6945e-04,  1.4838e-03],\r\n        [ 1.1696e-03, -7.8929e-04, -8.4497e-04, -1.6160e-03],\r\n        [-1.5014e-03,  1.7565e-03,  5.9908e-05,  9.8752e-05],\r\n        ...,\r\n        [ 2.3797e-04, -1.0391e-03, -6.5520e-04, -1.7515e-03],\r\n        [ 1.7547e-03, -1.3912e-03, -4.2819e-04, -1.1457e-04],\r\n        [-9.2908e-04,  2.9623e-04, -1.9417e-03,  8.6289e-04]], device=\'cuda:0\')\r\n```\r\nHowever, when changing the `dim` to 2048, the result is:\r\n```\r\nTraceback (most recent call last):\r\n  File ""test_pdist.py"", line 16, in <module>\r\n    test(torch.device(\'cuda\'), 4096)\r\n  File ""test_pdist.py"", line 12, in test\r\n    loss.backward()\r\n  File ""/n/home/user/app/anaconda3/envs/myenv/lib/python3.6/site-packages/torch/tensor.py"", line 118, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File ""/n/home/user/app/anaconda3/envs/myenv/lib/python3.6/site-packages/torch/autograd/__init__.py"", line 93, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: CUDA error: invalid configuration argument\r\n```\r\nCould you please help check this? Thanks again.', '@zudi-lin This looks like a CUDA config issue. Can you reproduce this on another CUDA machine?', ""@SsnL Do we have any updates on this? I am having the same issue if the dimension is larger than about 1450 (didn't check what the precise cut is, but it works if dim is less than 1300)."", 'i am having the same invalid cuda config doing pdist with tensor torch.Size([1870, 512])\r\nalso it apparently doesnt support half tensors', 'I also see the same issue if the dimension is larger than about 1450. I checked that computing pdist using `torch.norm((x - x[:, None]), p=2.0, dim=2)` works so it is not an issue with the input or running out of memory.\r\n', '@SsnL can this be reopened? I can confirm that this is still a problem in 1.4.0.\r\n\r\nThe following code reliable reproduces the issue. The exact size threshold is 1449. The naive version works fine so it is not a resource issue.\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nprint(torch.__version__)\r\n\r\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\r\nprint(device)\r\nif torch.cuda.is_available(): print(torch.cuda.get_device_name())\r\n\r\ndef spike():\r\n  N = 1449 # fails\r\n\r\n  x0 = torch.rand(N, 3, requires_grad=True, device=device)\r\n  x1 = x0.detach().clone()\r\n  x1.requires_grad=True\r\n\r\n  # backward fails for N > 1448\r\n  d0 = F.pdist(x0)\r\n  l0 = torch.sum(d0) / (0.5 * N * (N - 1))\r\n  l0.backward() \r\n  print(l0)\r\n  \r\n  # works\r\n  d1 = torch.norm(x1[:, None] - x1, dim=2, p=2.0)\r\n  l1 = torch.sum(d1) / (1.0 * N * (N - 1))\r\n  l1.backward()\r\n  print(l1)\r\n\r\nspike()\r\n```\r\n\r\noutput:\r\n\r\n```\r\n1.4.0\r\ncuda:0\r\nTesla P100-PCIE-16GB\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-3-8f569d2fda32> in <module>()\r\n     28   print(l1)\r\n     29 \r\n---> 30 spike()\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\r\n     97     Variable._execution_engine.run_backward(\r\n     98         tensors, grad_tensors, retain_graph, create_graph,\r\n---> 99         allow_unreachable=True)  # allow_unreachable flag\r\n    100 \r\n    101 \r\n\r\nRuntimeError: CUDA error: invalid configuration argument\r\n```', 'cc @ptrblck @emcastillo. Probably a thing similar to #31167 should be done here. As it stands, it is a resource issue, because likely pdist backward on cuda is very inefficiently implemented. ', '#31593 relaxed the condition for the forward and backward passes for `pdist`.\r\n@kleinhenz could you try the nightly binary, please?\r\n\r\n@ngimel I agree that the performance could be improved, but we should track in in a separate bug.', ""@ptrblck yes, it does work on nightly. thanks! Probably should have checked that first. Performance does still leave something to be desired compared to the naive solution.\r\n\r\n```\r\n1.6.0.dev20200416+cu101\r\ncuda:0\r\nTesla P100-PCIE-16GB\r\ninput size = torch.Size([2000, 3])\r\npdist elapsed = 32.213985443115234\r\ntensor(0.6534, device='cuda:0', grad_fn=<DivBackward0>)\r\nnaive elapsed = 2.1415040493011475\r\ntensor(0.6534, device='cuda:0', grad_fn=<DivBackward0>)\r\n```""]","['python\r\nimport torch\r\nimport torch.nn.functional as F\r\n\r\ndef test(device, dim=1024):\r\n    x = torch.rand(dim, 4, requires_grad=True).to(device)\r\n    y = torch.ones(dim, 1).to(device)\r\n    vdist = F.pdist(x, 2)\r\n    tdist = F.pdist(y, 1)\r\n\r\n    vdist = 1/(1+vdist**2)\r\n    loss = F.binary_cross_entropy(vdist, tdist)\r\n    loss.backward()\r\n    print(x.grad.data)\r\n', ""python\r\ntest(torch.device('cuda'), dim=4096)\r\n"", '\r\nTraceback (most recent call last):\r\n  File ""test_pdist.py"", line 16, in <module>\r\n    test(torch.device(\'cuda\'), 4096)\r\n  File ""test_pdist.py"", line 12, in test\r\n    loss.backward()\r\n  File ""/n/home/user/app/anaconda3/envs/myenv/lib/python3.6/site-packages/torch/tensor.py"", line 118, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File ""/n/home/user/app/anaconda3/envs/myenv/lib/python3.6/site-packages/torch/autograd/__init__.py"", line 93, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: CUDA error: invalid configuration argument\r\n', '\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n']","['torch.nn.functional.pdist', 'conda', 'pip']",0,0
363,pytorch,15608,open,[Caffe2] How to switch to test phase?,"Hi, I just spent 1 hour looking at the documentation and some tutorials on the website, and googling ; but I cannot figure out how I can put a net in testing phase. I would like an equivalent in Caffe2 of .",caffe2,"['You can look at the MNIST tutorial. To run a trained net on a testing set of data, load a new model into the workspace without adding training operators and without re-initializing the init parameters in order to keep the ones already in the workspace from training. Then you can pass the testing data into the net like you did with training.', 'I was more interested in disabling dropout for example, or speeding up the inference by disabling the gradient computation.']",[],['net.set_phase_test()'],0,0
364,pytorch,26646,open,[RFC] Drop Windows CUDA 9.2 support,"@peterjc123 and I propose that we drop CUDA 9.2 binary build support for Windows, meaning that we only provide CUDA 10.1 binaries for Windows. Here is our reasoning:

* We strongly prefer to have a single CI provider on which we build all binaries. At the moment, CircleCI is that provider; however, it only supports a single Visual Studio version, that is too new for CUDA 9.2. To run CUDA 9.2 builds on CircleCI, we would have to install both CUDA 9.2 (they provide CUDA 10.1) but also an older version of MSVC which is compatible with CUDA 9.2.
* To workaround this for CUDA 9.2 builds today, we currently do these builds on Azure. However, the agents provided by Microsoft to build nightlies are facing problems, and the release of GitHub Actions has tied up most of the resources available to resolve these problems.

We will not drop from-source support for CUDA 9.2; users are always welcome to build from source if they need this configuration. However, building from source on Windows is quite painful, and @peterjc123 is concerned for LibTorch users, where we rely on the user's environment (as opposed to packaging CUDA for them).

If you are a Windows CUDA 9.2 user that would be unduly affected by this change, please comment on this issue. Thank you!

cc @ezyang @peterjc123",module: binaries module: cuda module: windows triaged,"['cc @soumith ', 'Some more points:\r\n1. The download count of CUDA 9.2 packages on Windows is low, which is only near 5k~6k, which is only nearly 10% of that for CUDA 10.0.\r\n2. The nightlies for CUDA 9.2 on Windows never worked, but no related issues could be found.\r\n3. The cuda 9.2 package is non-existent for Windows in Anaconda Cloud. Although we used that in the numba channel, it leads to the complicated install command.', 'only a single Windows CUDA version seems good to me.\r\nGreenlight from my side!', ""Well, looks like Microsoft has fixed the agents and the upload issue is fixed. I don't think we need to do this in a hurry now.""]",[],[],0,0
365,pytorch,3025,closed,Function request: np.isin,"I believe that should be useful to have a function similar to https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.in1d.html, that compares a tensor element-wise with a list of possible values. (I'm using it to filter labels/classes in some classifiers).

Expected behavior:



Now it's possible to implement it by iterating over the filter, and storing the results in a tensor with the OR operator. But a faster implementation is possible with TH/THC

My current implementation:

(N, S)(S)batch X samplessamples

cc @mruberry @rgommers @heitorschueroff",function request module: bootcamp module: numpy triaged,"['If the issue is still open, Can I take this up?', 'Sure! Also, instead of implementing `in1d`, I think we should implement the more general `isin`, as recommended by [the numpy documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.in1d.html)\r\nHere is a slightly more runtime efficient (but memory-hungry and suboptimal) implementation (requires PyTorch compiled from master):\r\n```python\r\ndef isin(ar1, ar2):\r\n    return (ar1[..., None] == ar2).any(-1)\r\n```', 'I think this interface is very useful. \r\nSo I hope it can be added as soon as possible.', '@CamiWilliams When I run your proposed `isin` implementation (see #26144) with `torch` 1.2.0 on MacOS 10.14.6, I get the warning:\r\n\r\n`../aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.`\r\n\r\nIf you change:\r\n\r\n`return result.type(torch.ByteTensor)`\r\n\r\nto:\r\n\r\n`return result.type(torch.BoolTensor)`\r\n\r\nthe warning goes away.  Could you provide more detail on why you went with `ByteTensor` here?  I want to make sure I did not miss something.', 'When will `torch.isin` functionality be added in pytorch or is there already something like this which is not documented?\r\nhttps://github.com/pytorch/pytorch/pull/26144\r\n', 'assign to myself for now. This might be a good task for onboading team member.', 'Another work-around for `in1d` which is reasonably fast; memory consumption depends on maximum element of `a1` and `ar2`.\r\n\r\n```python\r\ndef in1d(ar1, ar2):\r\n    mask = ar2.new_zeros((max(ar1.max(), ar2.max()) + 1,), dtype=torch.bool)\r\n    mask[ar2.unique()] = True\r\n    return mask[ar1]\r\n```', ""Quick fyi: following NumPy's recommendation we should implement isin first, and maybe not implement in1d at all."", 'Although it looks like numpy is currently just wrapping `in1d`:\r\n\r\nhttps://github.com/numpy/numpy/blob/da887a666ad975ece7fb7465005aa99c0ddef8d2/numpy/lib/arraysetops.py#L709-L711']","['\r\n>>> a = torch.LongTensor([[1,2,3],[1,1,2],[3,5,1]])\r\n>>> a\r\n 1  2  3\r\n 1  1  2\r\n 3  5  1\r\n[torch.LongTensor of size 3x3]\r\n>>> a.in(torch.LongTensor([1, 2, 5]))\r\n 1 1 0\r\n 1 1 1\r\n 0 1 1\r\n']","['', '\r\n@utils.tensorfy(0, 1, tensor_klass=torch.LongTensor)\r\ndef filter_labels(y, labels):\r\n    """"""Utility used to create a mask to filter values in a tensor.\r\n\r\n    Args:\r\n        y (list, torch.Tensor): tensor where each element is a numeric integer\r\n            representing a label.\r\n        labels (list, torch.Tensor): filter used to generate the mask. For each\r\n            value in ', 'y', ' its mask will be ""1"" if its value is in ', 'labels', ',\r\n            ""0"" otherwise"".\r\n\r\n    Shape:\r\n        y: can have any shape. Usually will be :math:', ' or :math:', ',\r\n            containing ', ' or just a list of ', '.\r\n        labels: a flatten list, or a 1D LongTensor.\r\n\r\n    Returns:\r\n        mask (torch.ByteTensor): a binary mask, with ""1"" with the respective value from ', 'y', ' is\r\n        in the ', 'labels', ' filter.\r\n\r\n    Example::\r\n\r\n        >>> a = torch.LongTensor([[1,2,3],[1,1,2],[3,5,1]])\r\n        >>> a\r\n         1  2  3\r\n         1  1  2\r\n         3  5  1\r\n        [torch.LongTensor of size 3x3]\r\n        >>> classification.filter_labels(a, [1, 2, 5])\r\n         1  1  0\r\n         1  1  1\r\n         0  1  1\r\n        [torch.ByteTensor of size 3x3]\r\n        >>> classification.filter_labels(a, torch.LongTensor([1]))\r\n         1  0  0\r\n         1  1  0\r\n         0  0  1\r\n        [torch.ByteTensor of size 3x3]\r\n    """"""\r\n    mapping = torch.zeros(y.size()).byte()\r\n\r\n    for label in labels:\r\n        mapping = mapping | y.eq(label)\r\n\r\n    return mapping\r\n\r\n', '']",0,0
366,pytorch,3158,open,[Feature Request] Sparse-Dense elementwise Multiplication ,"Hi, thanks again for the great work.

I would like to raise an issue to discussion how to implement the following features:
Dense[m,1] * Sparse[m,n] -> Sparse[m,n]
Sparse[m,n] * Dense[1,n] -> Sparse[m,n]
Sparse[m*n] * Dense[b,1,n] -> Sparse[b,m,n]

these features are fully supported by the [cuSPARSE Level 3](https://developer.nvidia.com/cusparse), as for the CPU side, I'm not quite sure weather MKL was satisfied. 

I would like to help on the implementation if needed, could anyone give me a brief guideline that what have be done and what still need to be do?



 

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @aocsa @nikitaved @pearu @mruberry @vincentqb",module: sparse triaged,"['Just to clarify: are you looking for an elementwise product, or want to do matrix multiplication? From the shapes I infer it‚Äôs the first one, but I can‚Äôt find any related functions in cuSPARSE', ""I'm looking for element-wise product. I'm misunderstood that  [this fuction](http://docs.nvidia.com/cuda/cusparse/index.html#cusparse-lt-t-gt-csrmm) was describing an element-wise production. \r\n\r\nfor `Dense[m,1] * Sparse[m,n] -> Sparse[m,n]` and `Sparse[m,n] * Dense[1,n] -> Sparse[m,n]` mainly have two issues:\r\n         1. SparseTensor doesn't offer cmul(DenseTensor), which can be solved by convert Dense to Sparse first.\r\n         2. Sparse * Sparse required the two tensor have [exactly same size ](https://github.com/pytorch/pytorch/blob/master/torch/lib/THCS/generic/THCSTensorMath.cu#L411). To my understanding if two tensor was broadcastable, that should also work?\r\n\r\nand for `Sparse[m*n] * Dense[b,1,n] -> Sparse[b,m,n]`. If cuSPARSE didn't offer something like `bspaddcmul `I'm guessing I can only using an for loop on Dense[b,1,n] right?"", ""1. Converting dense tensors to sparse is a bad idea. It will take a lot more memory than the original dense tensor and will be extremely slow. We should write specialized kernels for this.\r\n2. That's true, although I don't think our current broadcasting code supports sparse tensors. In general, broadcasting for sparse tensors is much harder and more expensive than for dense tensors, because you actually need to repeat all the values, making the tensor much larger"", ""is there any plan that when should pytorch support CSR/CSC format? most of these operation will be cheap if using CSR/CSC.\r\n\r\nfor these special case, i guess we don't need really allocate more memories since every value should be repeated and only be used as a intermedia value? \r\n\r\nBesides, if the cuSPARSE and MKL didn't offer these implementation, does that mean that we need more effort to utilize the multiprocess? "", 'I don\'t think we\'ll be supporting other formats. They require new kernels for all math operations, and it\'s a lot of code to write and maintain.\r\n\r\nThat\'s true, but only if you special case the ""broadcasted"" operators in the backend. There\'s no way to easily ""expand"" the sparse tensor outside of a math method. It\'s because all non-zero values have to be listed explicitly, and expanding actually adds more non-zero values.\r\n\r\nNo, that\'s not true. We\'re using OpenMP in our code, which makes it very easy to paralellize math operations', 'hey, \r\n\r\nI am also struggling with the same issue. I want to do element-wise multiplication of sparse and dense matrices with auto_grad = True. \r\nDid you find some solution for the same?\r\n\r\nthanks in Advance.', 'I ran into the same issue, even with the latest Pytorch build (1.8)\r\n\r\nSimple example code:\r\n\r\nA = torch.sparse_coo_tensor([[0,1],[0,1]], [1.0, 2.0])\r\nB = torch.tensor([[2.0, 3.0]])\r\nA*B # fails, but A.to_dense()*B works fine', 'Bumping priority based on user activity. ', 'We would accept a PR implementing this behavior. ', 'I was looking this issue and it seems feasible.  To focus on one goal, first I will implement the element-wise product between sparse tensor and dense tensor. \r\n\r\ncc @mruberry, @rgommers  ', 'The issue description says that this should always result in sparse output. That sounds sensible, but there was a lot of discussion on the semantics of this elsewhere. @pearu is this actionable right now?', '> The issue description says that this should always result in sparse output. That sounds sensible, but there was a lot of discussion on the semantics of this elsewhere. @pearu is this actionable right now?\r\n\r\nIt has been suggested [torch.sparse design principles] that ""Binary functions operating on a COO tensor and a strided tensor generally produce strided tensors"" that differs from the wished behavior indeed. So, the answer is no until the semantics of element-wise multiplication is confirmed.\r\n\r\nA little background: While for element-wise multiplication, `COO * Strided -> COO` sounds sensible then for element-wise addition, `COO + Strided -> Strided` is inevitable. Hence the general suggestion for binary operations.\r\n\r\nThe  `COO * Strided -> COO` behavior could be achieved/implemented via `torch.mul(coo, strided, layout=torch.coo_layout)` or `torch.mul(coo, strided, out=out)` where `out` is a COO tensor with the same indices set as `coo` has.', ""@mruberry this is blocked as well as lower-prio than other items on the sparse tracking issue (gh-44634), so I'll remove the high-prio label from this and add it to that roll-up."", ""> @mruberry this is blocked as well as lower-prio than other items on the sparse tracking issue ([gh-44634](https://github.com/pytorch/pytorch/issues/44634)), so I'll remove the high-prio label from this and add it to that roll-up.\r\n\r\nOK -- let's talk about this on Friday, too. I don't want us to feel blocked on a tracker. The tracker's supposed to help us!"", 'It\'s not blocked on the tracker, that\'s secondary here. It\'s blocked on ""we don\'t yet all agree on the semantics"", and resolving that is currently on the TODO list for after the CSR/GCS format work is landed.']",[],[],0,0
367,pytorch,28549,open,Expose DifferentiableGraphBackward to python,"## üöÄ Feature

Currently, there is no easy way of visualizing the graph generated by autodiff in jit. We want something that could do



## Additional context

In autograd, the  for  of a JIT graph is defined as

 
in 

This class does not have python correspondence, therefore python only views it as a general . We need to create a Python class for it so that we could access additional information.

**Please confirm if this feature sounds useful and if yes, assign me to this issue and I will work on it.**

cc @suo @ezyang @SsnL @albanD @zou3519 @gqchen",feature module: autograd oncall: jit triaged,"[""Note that the graph exposed here is not eager autograd but symbolic autodiff in JIT. But yea I think it's good to have if not much effort is necessary. "", '@ailzhang Yes, I understand it is not eager autograd. I am looking at the performance on the backward pass with jit and feeling that this feature would be very helpful for debugging.', 'FYI: you can find [here](https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/functions/init.cpp) an example of how to create a custom Node with additional available field.', '@albanD Thanks!', 'cc: @zdevito as well to see whether this fits the plan. ', ""Improving the debuggability of the JIT is a good thing. There are some things to keep in mind though when designing any interface around this. First, DifferentiableGraphBackward is an implementation detail of the JIT and should remain that way. It is used when part of the graph is symbolically differentiable, but there is no guarantee that a single call to the backward of the output of a script function will result in the execution of one DifferentiableGraphBackward op. In cases where none of the graph is symbolically differentiable, then there will be no instances of this op. In cases where the whole graph is symbolically differentiable, there will be a single one, and in partial cases, there will often be multiple instances of this op type in the graph.  In many cases, the final value of a script function won't have a `grad_fn` that is a DifferentiableGraphOp, though there may be autograd graph nodes inside the trace of that thing which do have these values. What we can do is improve how the `grad_fn` prints for when a DifferentiableGraphOp does exists. In this case, it would great if calling `print(x.grad_fn)` prints out the underlying graph as long as the connectivity information (the things inside the attributes of the Node* representing the op that show how the graph hooks up with captured variables). That would help understanding how the JIT constructed an autograd graph and show how it integrates into the existing autograd tape.""]","['python\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef f(a):\r\n    return (a + 100) * 5\r\n\r\na = torch.tensor(0., requires_grad=True)\r\nresult = f(a)\r\n\r\nprint(result.grad_fn.graph)\r\n', '\r\nstruct DifferentiableGraphBackward : public autograd::Node\r\n']","['Node', 'grad_fn', 'torch/csrc/jit/graph_executor.cpp', 'CppFunction']",0,0
368,pytorch,6171,open,Compute csr representation on torch.sparse.Tensor.coalesce for faster sparse matrix multiplication ,"On the code on sparse matrix-dense matrix multiplication
https://github.com/pytorch/pytorch/blob/master/aten/src/THS/generic/THSTensorMath.c#L305
and 
https://github.com/pytorch/pytorch/blob/master/aten/src/THCS/generic/THCSTensorMath.cu#L58 , 
it first computes csr representation, then perform sparse matrix-dense matrix multiplication based on that representation, then destroy the csr representation. 
If we repeatedly perform matrix multiplication on the same sparse matrix, computing csr representation is redundant. This could be resolved by computing csr representation only once when coalescing  and save it as a member of  or  ( https://github.com/pytorch/pytorch/blob/master/aten/src/THS/generic/THSTensor.cpp#L428 and
https://github.com/pytorch/pytorch/blob/master/aten/src/THCS/generic/THCSTensor.cu#L38 ). This would enhance speed of sparse matrix-dense matrix multiplication.

Will this be a good modification? Shall I try to make a pull request on this one?



cc @vincentqb @aocsa @nikitaved @pearu @mruberry",module: sparse triaged,"['This may be a good starting point for https://github.com/pytorch/pytorch/issues/3158, https://github.com/pytorch/pytorch/issues/4247, and https://github.com/pytorch/pytorch/issues/5262 . ', 'CuSparse libraries support multiple sparse matrix formats such as COO, CSR, ELLPACK etc where a different format might make sense depending on the sparse matrix property. Given that, would it be possible to allow the user to control the sparse matrix layout to begin with? Like say we extend .layout option beyond just sparse_coo to allow addition of other sparse matrix formats. Is that easy to extend? I dont know enough about how these tensors are implemented to know that myself.   ', ""Yes, this makes sense (indeed, with the new `torch.layout` API bits, we have planned for it), but no, it's not so easy to support. It basically entails adding a new backend to PyTorch, and there are a lot of moving bits to making this happen, especially because we are rewriting how all of this works internally (C10)."", ""@kose-y Did you find a workaround ? I'm thinking about trying to adapt the function `THCSTensor_(spaddmm)`, and wrapping the new one in the same way, so that I can call it from PyTorch. Did you start doing something of the sort by any chance ?\r\nI also noticed that the files you link have disappeared now in master. So, you should maybe update the links in your first comment, to v0.4.0 for example."", 'Starting from `master`, my first attempt consists of splitting this function: https://github.com/pytorch/pytorch/blob/bc66d982482e6d6c586d12de5176d9d4bf38eec5/aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu#L44\r\n\r\nin 2 parts (see code below), so that I can call the first part at the very beginning, and then the second part every time I need to do a matmat product.\r\n\r\nI know it\'s not very pretty, but I\'m just trying to write something that could work, while waiting for the next release of PyTorch.\r\nNow I need to do the wrapping in order to be able to call `sp_coo_to_csr` from PyTorch, and replace the calls to `s_addmm_out_sparse_dense_cuda` by `s_addmm_out_spcsr_dense_cuda`, and I\'m not sure how to do that...\r\n\r\n@ezyang, can I have some guidance on if my functions could potentially work, and if the wrapping would be easy ? Also, do you have a timeline for when this issue will be adressed ?\r\n\r\n```c++\r\n// Takes a SparseTensor (in COO) and returns the CSR representation as a tuple (or array) of Tensors \r\n// sp_sizes is a vector containing the important sizes of the sparse matrix : [m, k, nnz]\r\nArrayRef<Tensor> sp_coo_to_csr(const SparseTensor& sparse_, ArrayRef<int64_t>* sp_sizes) {\r\n#ifndef __HIP_PLATFORM_HCC__\r\n  AT_CHECK(sparse_.is_cuda(), ""addmm: expected \'mat1\' to be CUDA, but got CPU"");\r\n\r\n  AT_CHECK(_check_device({sparse_}));\r\n\r\n  // TODO: This error message seems awfully opaque\r\n  AT_CHECK(sparse_._sparseDims() == 2, ""addmm: 2D tensor expected, got "", sparse_._sparseDims(), ""D tensor"");\r\n  AT_CHECK(sparse_._denseDims() == 0, ""addmm: scalar values expected, got "", sparse_._denseDims(), ""D values"");\r\n\r\n  // mxk * kxn = mxn\r\n  int64_t m = sparse_.size(0);\r\n  int64_t k = sparse_.size(1);\r\n  int64_t nnz = sparse._nnz();\r\n\r\n  sp_sizes[0] = m;\r\n  sp_sizes[1] = k;\r\n  sp_sizes[2] = nnz;\r\n\r\n  SparseTensor sparse = sparse_.coalesce();\r\n\r\n  LongTensor indices = sparse._indices();\r\n  Tensor values = sparse._values();\r\n\r\n  LongTensor rowIndices = indices.select(0, 0);\r\n  LongTensor colIndices = indices.select(0, 1);\r\n  IntTensor csr = _to_csr_int(rowIndices, m, nnz);\r\n  IntTensor colIndicesInt = at::empty({colIndices.size(0)}, indices.type().toScalarType(kInt));\r\n  colIndicesInt.copy_(colIndices);\r\n\r\n  return {colIndicesInt, csr, values};\r\n#else\r\n  AT_ERROR(""s_addmm_out_sparse_dense_cuda: HIP not supported"");\r\n#endif\r\n}\r\n\r\n// Takes the sparse matrix as a array of Tensors, and an array of sizes\r\nTensor& s_addmm_out_spcsr_dense_cuda(Tensor& r_, const Tensor& t, const ArrayRef<Tensor> sparse_, const ArrayRef<int64_t> sp_sizes, const Tensor& dense, Scalar beta, Scalar alpha) {\r\n#ifndef __HIP_PLATFORM_HCC__\r\n  AT_ASSERT(t.is_cuda()); // dispatch argument\r\n  AT_CHECK(r_.is_cuda(), ""addmm: expected \'out\' to be CUDA, but got CPU"");\r\n  AT_CHECK(dense.is_cuda(), ""addmm: expected \'mat2\' to be CUDA, but got CPU"");\r\n\r\n  // Get back the info on the sparse matrix\r\n  IntTensor colIndicesInt = sparse_[0];\r\n  IntTensor csr = sparse_[1];\r\n  Tensor values = sparse_[2];\r\n\r\n  // mxk * kxn = mxn\r\n  int64_t m = sp_sizes[0];\r\n  int64_t k = sp_sizes[1];\r\n  int64_t nnz = sp_sizes[2];\r\n  int64_t n = dense.size(1);\r\n\r\n  AT_CHECK(_check_device({colIndicesInt,csr,values}));\r\n  AT_CHECK(_check_device({r_, t, dense}));\r\n\r\n  // TODO: This error message seems awfully opaque\r\n  AT_CHECK(dense.dim() == 2, ""addmm: 2D tensor expected, got "", dense.dim(), ""D tensor"");\r\n\r\n\r\n  AT_CHECK(t.size(0) == m,\r\n\t  ""addmm: Argument #1 (t): Expected dim 0 size "", m, "", got "", t.size(0));\r\n  AT_CHECK(t.size(1) == n,\r\n\t  ""addmm: Argument #1 (t): Expected dim 1 size "", n, "", got "", t.size(1));\r\n  AT_CHECK(dense.size(0) == k,\r\n\t  ""addmm: Argument #3 (dense): Expected dim 0 size "", k, "", got "", dense.size(0));\r\n\r\n  r_.resize_({m, n});\r\n\r\n  // No half support, so we don\'t have to use CUDATypeConversion\r\n  Tensor r__;\r\n  AT_DISPATCH_FLOATING_TYPES(\r\n\t  values.type(), ""addmm_sparse_cuda"", [&] {\r\n\t\tscalar_t cast_beta = beta.to<scalar_t>();\r\n\t\tscalar_t cast_alpha = alpha.to<scalar_t>();\r\n\t\tif (cast_beta == 0) {\r\n\t\t  r_.zero_();\r\n\t\t} else if (cast_beta == 1) {\r\n\t\t  if (!isSameTensor(t, r_)) {\r\n\t\t\tr_.copy_(t);\r\n\t\t  }\r\n\t\t} else {\r\n\t\t  at::mul_out(r_, t, beta);\r\n\t\t}\r\n\r\n\t\t/* r_ */\r\n\t\tif(r_.stride(0) == 1 && r_.stride(1) == r_.size(0)) {\r\n\t\t  r__ = r_;\r\n\t\t} else {\r\n\t\t  // TODO: how... strange\r\n\t\t  r__ = r_.transpose(0, 1).clone();\r\n\t\t  r__.transpose_(0, 1);\r\n\t\t}\r\n\r\n\t\t/* dense */\r\n\t\tTensor dense_;\r\n\t\tchar transpose_dense;\r\n\t\tif(dense.stride(0) == 1 && dense.stride(1) == dense.size(0)) {\r\n\t\t  transpose_dense = \'n\';\r\n\t\t  dense_ = dense;\r\n\t\t} else if(dense.stride(1) == 1 && dense.stride(0) != dense.size(1)) {\r\n\t\t  transpose_dense = \'t\';\r\n\t\t  dense_ = dense;\r\n\t\t} else {\r\n\t\t  transpose_dense = \'t\';\r\n\t\t  dense_ = dense.contiguous();\r\n\t\t}\r\n\r\n\t\tsparse::cuda::csrmm2(\r\n\t\t  \'n\',\r\n\t\t  transpose_dense,\r\n\t\t  m,\r\n\t\t  n,\r\n\t\t  k,\r\n\t\t  nnz,\r\n\t\t  cast_alpha,\r\n\t\t  values.data<scalar_t>(),\r\n\t\t  csr.data<int32_t>(),\r\n\t\t  colIndicesInt.data<int32_t>(),\r\n\t\t  dense_.data<scalar_t>(),\r\n\t\t  (transpose_dense == \'n\' ? dense_.stride(1) : dense_.stride(0)),\r\n\t\t  cast_beta,\r\n\t\t  r__.data<scalar_t>(),\r\n\t\t  r__.stride(1));\r\n\r\n\t  });\r\n\r\n  r_.copy_(r__);\r\n  return r_;\r\n#else\r\n  AT_ERROR(""s_addmm_out_sparse_dense_cuda: HIP not supported"");\r\n#endif\r\n}\r\n\r\n```\r\n', ""@matthieuheitz \r\nMy initial try is in #6225 , which is before the release of 0.4.0, but haven't tried since then."", 'Happy new year ! :tada: \r\nAny updates on this issue ?']",[],"['THSTensor', 'THCSTensor']",0,0
369,pytorch,20323,open,Support size to `torch.normal`,"This would be more consistent with numpy.



cc @mruberry @rgommers @heitorschueroff",function request module: numpy triaged,"['yes - I support this\r\n\r\nthe current API is inconsistent with the rest of the API an norms set by bumpy\r\n\r\nalso the size should/could be a tuple\r\n\r\n```\r\ntorch.normal(0.0, 4.0, size=(2,5,4))\r\n```', 'Thanks, we will consider this.\r\n\r\nFor now, can you get unblocked by doing something like this?\r\n\r\n```python\r\ntorch.randn(2, 5, 4) * 4.0 \r\n```\r\n\r\nJust trying to make sure I understand your use case correctly.']","['python\r\ntorch.normal(0.0, 4.0, size=5)\r\n']",[],0,0
370,pytorch,24145,open,sccache crashes when building `Distribution.cu` on Windows,"There are many occurences of this build error in Azure Pipelines.
https://dev.azure.com/pytorch/PyTorch/_build/results?buildId=3891
https://dev.azure.com/pytorch/PyTorch/_build/results?buildId=3901
https://dev.azure.com/pytorch/PyTorch/_build/results?buildId=3695

Any ideas, @yf225?",module: build module: build warnings triaged,"['Based on offline discussion with @peterjc123 , this is only reproducible with CUDA 10. I suggested trying out the solutions in https://github.com/mozilla/sccache/issues/256 (e.g. increasing timeout value). Also if the error can be reproduced locally I can help debug.\r\n', ""@yf225 @peterjc123 What's the default timeout?"", '@pietern The default value is 600. https://github.com/mozilla/sccache/blob/master/src/server.rs#L68', ""Now it is becoming worse. Some more errors:\r\n```\r\n[269/1930] Building C object confu-deps\\cpuinfo\\CMakeFiles\\cpuinfo.dir\\src\\init.c.obj\r\nFAILED: confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/init.c.obj \r\nC:\\w\\1\\s\\windows\\tmp_bin\\sccache.exe  cl  /nologo -DCPUINFO_LOG_LEVEL=2 -DTH_BLAS_MKL -D_OPENMP_NOFORCE_MANIFEST -I..\\..\\third_party\\cpuinfo\\src -I..\\..\\third_party\\cpuinfo\\include -I..\\..\\third_party\\cpuinfo\\deps\\clog\\include -I..\\..\\third_party\\protobuf\\src -IC:\\w\\1\\s\\windows\\mkl\\include /DWIN32 /D_WINDOWS /W3 /EHa /MDd /Zi /Ob0 /Od /RTC1   /MDd /showIncludes /Foconfu-deps\\cpuinfo\\CMakeFiles\\cpuinfo.dir\\src\\init.c.obj /Fdconfu-deps\\cpuinfo\\CMakeFiles\\cpuinfo.dir\\cpuinfo.pdb /FS -c ..\\..\\third_party\\cpuinfo\\src\\init.c\r\nsccache: encountered fatal error\r\nsccache: error : failed to store `init.c.obj` to cache\r\nsccache:  cause: failed to store `init.c.obj` to cache\r\nsccache:  cause: failed to zip up compiler outputs\r\nsccache:  cause: The process cannot access the file because it is being used by another process. (os error 32)\r\n[270/1930] Building CXX object third_party\\protobuf\\cmake\\CMakeFiles\\libprotobuf.dir\\__\\src\\google\\protobuf\\util\\internal\\protostream_objectsource.cc.obj\r\ncl : Command line warning D9025 : overriding '/EHs' with '/EHa'\r\n[271/1930] Building C object confu-deps\\cpuinfo\\CMakeFiles\\cpuinfo.dir\\src\\x86\\info.c.obj\r\n[272/1930] Building C object confu-deps\\cpuinfo\\CMakeFiles\\cpuinfo.dir\\src\\api.c.obj\r\nFAILED: confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/api.c.obj \r\nC:\\w\\1\\s\\windows\\tmp_bin\\sccache.exe  cl  /nologo -DCPUINFO_LOG_LEVEL=2 -DTH_BLAS_MKL -D_OPENMP_NOFORCE_MANIFEST -I..\\..\\third_party\\cpuinfo\\src -I..\\..\\third_party\\cpuinfo\\include -I..\\..\\third_party\\cpuinfo\\deps\\clog\\include -I..\\..\\third_party\\protobuf\\src -IC:\\w\\1\\s\\windows\\mkl\\include /DWIN32 /D_WINDOWS /W3 /EHa /MDd /Zi /Ob0 /Od /RTC1   /MDd /showIncludes /Foconfu-deps\\cpuinfo\\CMakeFiles\\cpuinfo.dir\\src\\api.c.obj /Fdconfu-deps\\cpuinfo\\CMakeFiles\\cpuinfo.dir\\cpuinfo.pdb /FS -c ..\\..\\third_party\\cpuinfo\\src\\api.c\r\nsccache: encountered fatal error\r\nsccache: error : failed to store `api.c.obj` to cache\r\nsccache:  cause: failed to store `api.c.obj` to cache\r\nsccache:  cause: failed to zip up compiler outputs\r\nsccache:  cause: The process cannot access the file because it is being used by another process. (os error 32)\r\n```\r\nLooks like /Zi is appearing again for C sources."", '@yf225 Have you rebuilt sccache? However, the issue is still there and it only occurs when trying to build the CUDA 10.0 binaries through CMD.']","['\r\n[737/1919] Building NVCC (Device) object caffe2/CMakeFiles/torch.dir/__/aten/src/ATen/native/cuda/torch_generated_Distributions.cu.obj\r\nFAILED: caffe2/CMakeFiles/torch.dir/__/aten/src/ATen/native/cuda/torch_generated_Distributions.cu.obj \r\ncmd.exe /C ""cd /D C:\\w\\1\\s\\windows\\pytorch\\build\\build\\caffe2\\CMakeFiles\\torch.dir\\__\\aten\\src\\ATen\\native\\cuda && C:\\w\\1\\s\\windows\\conda\\envs\\py3\\Library\\bin\\cmake.exe -E make_directory C:/w/1/s/windows/pytorch/build/build/caffe2/CMakeFiles/torch.dir/__/aten/src/ATen/native/cuda/. && C:\\w\\1\\s\\windows\\conda\\envs\\py3\\Library\\bin\\cmake.exe -D verbose:BOOL=OFF -D build_configuration:STRING=Release -D generated_file:STRING=C:/w/1/s/windows/pytorch/build/build/caffe2/CMakeFiles/torch.dir/__/aten/src/ATen/native/cuda/./torch_generated_Distributions.cu.obj -D generated_cubin_file:STRING=C:/w/1/s/windows/pytorch/build/build/caffe2/CMakeFiles/torch.dir/__/aten/src/ATen/native/cuda/./torch_generated_Distributions.cu.obj.cubin.txt -P C:/w/1/s/windows/pytorch/build/build/caffe2/CMakeFiles/torch.dir/__/aten/src/ATen/native/cuda/torch_generated_Distributions.cu.obj.Release.cmake""\r\nDistributions.cu\r\ncl : Command line warning D9025 : overriding \'/EHs\' with \'/EHa\'\r\ncl : Command line warning D9025 : overriding \'/EHa\' with \'/EHs\'\r\nDistributions.cu\r\ncl : Command line warning D9025 : overriding \'/EHs\' with \'/EHa\'\r\ncl : Command line warning D9025 : overriding \'/EHa\' with \'/EHs\'\r\nDistributions.cu\r\ncl : Command line warning D9025 : overriding \'/EHs\' with \'/EHa\'\r\ncl : Command line warning D9025 : overriding \'/EHa\' with \'/EHs\'\r\nDistributions.cu\r\ncl : Command line warning D9025 : overriding \'/EHs\' with \'/EHa\'\r\ncl : Command line warning D9025 : overriding \'/EHa\' with \'/EHs\'\r\nDistributions.cu\r\ncl : Command line warning D9025 : overriding \'/EHs\' with \'/EHa\'\r\ncl : Command line warning D9025 : overriding \'/EHa\' with \'/EHs\'\r\nDistributions.cu\r\ncl : Command line warning D9025 : overriding \'/EHs\' with \'/EHa\'\r\ncl : Command line warning D9025 : overriding \'/EHa\' with \'/EHs\'\r\nDistributions.cu\r\ncl : Command line warning D9025 : overriding \'/EHs\' with \'/EHa\'\r\ncl : Command line warning D9025 : overriding \'/EHa\' with \'/EHs\'\r\nDistributions.cu\r\nerror: failed to execute compile\r\ncaused by: error reading compile response from server\r\ncaused by: Failed to read response header\r\ncaused by: An existing connection was forcibly closed by the remote host. (os error 10054)\r\nCMake Error at torch_generated_Distributions.cu.obj.Release.cmake:279 (message):\r\n  Error generating file\r\n  C:/w/1/s/windows/pytorch/build/build/caffe2/CMakeFiles/torch.dir/__/aten/src/ATen/native/cuda/./torch_generated_Distributions.cu.obj\r\n']",[],0,0
371,pytorch,15298,open,Momentum problem (1-momentum is correct?) in BatchNorm2d,"## üêõ Bug

In BatchNorm2d' doc, the explanation of momentum is:


where  is the newly computed batch statistics,  is the estimated statistics in history.
While in my experiments, when momentum set to 0, the batchnorm operates just use the newly computed statistics.

So I guess maybe the following explanation is correct:



## To Reproduce

 is a torch.Tensor with shape (8, 3, 299, 299), which read from an image
 is the model load from 
I have set the momentum as 0 in [Inception v3 model struct](https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py):



Then I calculate the mean and var:


The result is that  while 

## Expected behavior

Expected  while 

## Environment

 - PyTorch Version 0.4.1:
 - Windows:
 - pip:
 - Python version: 3.6.5


cc @brianjo @mruberry @albanD @jbschlosser",module: docs module: nn todo triaged,"['momentum has no effect on training mode output though.', 'Yes. While it is suggested to set momentum to 0.1 (default), I think modify the doc and the default value as 0.9 is better.']","['python\r\nclass BasicConv2d(nn.Module):\r\n\r\n    def __init__(self, in_channels, out_channels, **kwargs):\r\n        super(BasicConv2d, self).__init__()\r\n        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\r\n        self.bn = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0)\r\n\r\n    def forward(self, x):\r\n        x = self.conv(x)\r\n        x = self.bn(x)\r\n        return F.relu(x, inplace=True)\r\n', ' python\r\nconv = mol.Conv2d_1a_3x3.conv(b_x)\r\ndim = conv.shape\r\nm = (conv.sum(0).sum(1).sum(1)/(dim[0]*dim[2]*dim[3])).view(1, -1, 1, 1)\r\nvar = (((conv-m)**2).sum(0).sum(1).sum(1)/(dim[0]*dim[2]*dim[3]-1)).view(1, -1, 1, 1) + 0.001\r\ntmp = (conv - m) / torch.sqrt(var)\r\ntmp = tmp * mol.Conv2d_1a_3x3.bn.weight.view(1, -1, 1, 1) + mol.Conv2d_1a_3x3.bn.bias.view(1, -1, 1, 1)\r\nbn = mol.Conv2d_1a_3x3.bn(conv)\r\nprint((bn-tmp).mean(), (bn-tmp).std())\r\n']","['\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momemtum} \\times x_t$', 'x_t', '\\hat{x}', '\\hat{x}_\\text{new} = \\text{momemtum\\times \\hat{x} + (1 - \\text{momentum}) \\times x_t$', 'b_x', 'mol', 'Inception V3', 'bn=tmp', 'momentum = 0', 'bn=tmp', 'momentum = 1']",0,0
372,pytorch,25034,open,"make add_module accept tuples as well or change containers(ModuleList, Sequential, etc) to allow this","## üöÄ Feature
<!-- A clear and concise description of the feature proposal -->

## Motivation
The API is not consistent with the way architectures are made. That is you can do some actions using a set of methods, but cant do the same actions using a slightly different version of the same methods. To be more clear for example, currently if you want to alter/use a part of an existing architecture you may use   like this : 
1. method 1 :    

2. or a slightly different version 

However, the very time you  try to use the named version of same method (i.e ), you no longer can do the same thing and will face the error : 

in the process of using only  method in such situations, you lose the information originally placed in the architecture, meaning you no longer have access to the modules using their names. 
The source of the issue lies in  the  method which interestingly accepts a name and a module by default. 

  
## Pitch
allow for ,  and alikes that accept modules, also accept named modules. meaning these two snippets be interchangeable : 

using modules only ()

using tuple ()




cc @SsnL",enhancement module: nn triaged,[],"['\r\nmodel = torchvision.models.resnet18(pretrained=True)\r\nmodel_features = nn.ModuleList(model.children())[ : -1] \r\n...\r\n', '\r\nmodel = torchvision.models.resnet18(pretrained=True)\r\nmodel_features = list(model.children())[ : -1]\r\nmodel_features = nn.Sequential(*model_features) \r\n...\r\n', '\r\nmodel = torchvision.models.resnet18(pretrained=True)\r\nmodel_features = nn.ModuleList(model.children())[ : -1] \r\n', '\r\nmodel = torchvision.models.resnet18(pretrained=True)\r\nmodel_features = nn.ModuleList(model.named_children())[ : -1] \r\n']","['model.children()', 'model.named_children', 'TypeError: tuple is not a Module subclass', 'children()', 'add_module()', 'nn.ModuleList()', 'nn.Sequential()', 'children()', 'named_children()']",0,0
373,pytorch,14996,open,as_tensor does not use the device of the default tensor type,"## üêõ Bug

According to the [doc](https://pytorch.org/docs/stable/torch.html?highlight=torch%20as_tensor#torch.as_tensor) for , the input should be copied to a cuda device if the default tensor type is a cuda tensor. 

> Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.

It does copy to cuda device if the input is a numpy array, but not if the input is a cpu tensor. 

## To Reproduce

Steps to reproduce the behavior:



## Expected behavior

The device should be the 'cuda' device in both cases.

## Environment

PyTorch version: 1.0.0
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 18.04.1 LTS
GCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
CMake version: version 3.10.2

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration:
GPU 0: GeForce GTX 1070
GPU 1: GeForce GTX 1080

Nvidia driver version: 390.77
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] Could not collect
[conda] blas                      1.0                         mkl
[conda] mkl                       2018.0.3                      1
[conda] mkl_fft                   1.0.6            py37h7dd41cf_0
[conda] mkl_random                1.0.1            py37h4414c95_1
[conda] pytorch                   1.0.0           py3.7_cuda9.0.176_cudnn7.4.1_1    pytorch
[conda] torchvision               0.2.1                      py_2    pytorch



cc @ngimel @jlin27 @mruberry",module: cuda module: docs triaged,"['The behavior in both cases is correct according to the clause in the docs: ""If the data is already a Tensor with the same dtype and device, no copy will be performed.""  But the documentation for the `device` property doesn\'t specify that.', 'In the example I gave, the device of the input tensor is cpu and the expected device of output tensor is cuda, so the clause you have quoted does not apply and copying from cpu to cuda is the expected behaviour.', ""@gchanan I pointed this out on #45367 but I'm repeating it here as that issue was already closed when I commented on it. The behavior is not consistent with the documentation. One way to make it consistent would be to fix the documentation. It should read:\r\n\r\n> device (torch.device, optional) - the desired device of returned tensor. Default: if None __and input is not a torch tensor__, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. __If None and input is a tensor, uses the device of the input tensor.__\r\n\r\n(edits in bold)\r\n\r\nThe other way would be to fix the code. I would prefer that because it simplifies the behavior but it breaks backward compatibility.\r\n\r\n"", ""I created a PR to clarify the docs. Separately, I think we should deprecate `torch.set_default_tensor_type`. I'll create a separate issue to discuss that, however, after the 1.7 branch cut. ""]","[""\r\nIn [1]: import torch\r\n\r\nIn [2]: import numpy as np\r\n\r\nIn [3]: torch.set_default_tensor_type(torch.cuda.FloatTensor)\r\n\r\nIn [4]: torch.as_tensor(np.random.randn(4)).device\r\nOut[4]: device(type='cuda', index=0)\r\n\r\nIn [5]: torch.as_tensor(torch.randn(4, device='cpu')).device\r\nOut[5]: device(type='cpu')\r\n""]",['torch.as_tensor'],0,0
374,pytorch,24645,open,Migrate `thnn_conv2d_forward` from the TH to Aten (CUDA),"Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review.",better-engineering module: convolution module: porting triaged,[],[],[],0,0
375,pytorch,3542,open,"Minor: docker build fails on submodule because of gitdir with absolute path name (problem with git v 2.7/2.8, fixed in git v 2.9)","error message is 

Reason:  pytorch was cloned with --recursive (as recommended) into   and the submodule stores in file 

the absolute pathname as

But when 
 
is run to copy  everything into  the absolute pathname won't match - hence the error.

This is apparently a know problem in git, see https://lwn.net/Articles/691223/  and fixed in v 2.9 
So may be you can just put a note on the page telling people about this to save them time. 
",module: build triaged,"['I am getting the same error, so is the only solution to install git v 2.9 ? can we change the lines in the docker file to fix this ?', ""I don't think changing the dockerfile to adapt to a broken git logic makes sense. But yes,  git version 2.9 or newer should do the trick without any other changes. "", 'thanks, so it might be a good idea to make the docker install `git` version >= 2.9. Using the current `DockerFile`, it seems that it installs an older version of `git`. Is it easy to change the docker to have it install version >= 2.9 ?', ""Hi, @utke1 . I changed the Dockerfile to install git 2.9 and git 2.19. But I still got a similar error message as follows. \r\n\r\n```\r\nStep 9/12 : RUN git submodule update --init\r\n ---> Running in 673898e16516\r\nfatal: not a git repository (or any of the parent directories): .git\r\nThe command '/bin/sh -c git submodule update --init' returned a non-zero code: 128\r\n```\r\n\r\nIt would be great if you could give me some advice."", 'Hi! I am trying to build an image on Google Cloud with this [dockerfile](https://raw.githubusercontent.com/pytorch/pytorch/master/docker/pytorch/Dockerfile) and got the same error. Any help will be appreciated! Basically I typed \r\n\r\ngcloud builds submit --tag  gcr.io/an_image_name\r\n\r\nand got a fatal error:\r\n\r\nStep 9/13 : RUN git submodule update --init\r\n ---> Running in 8c3312e41bdc\r\nfatal: Not a git repository (or any of the parent directories): .git\r\nThe command \'/bin/sh -c git submodule update --init\' returned a non-zero code: 128\r\nERROR\r\nERROR: build step 0 ""gcr.io/cloud-builders/docker"" failed: exit status 128', 'I am running into this and it does not get fixed with a git upgrade of the docker version or host machine (I fail to see why that would be). ']","[""\r\nRemoving intermediate container dee615013001\r\nStep 9/14 : COPY . .\r\n ---> 489276daed51\r\nStep 10/14 : RUN git submodule update --init\r\n ---> Running in ca195ce4b458\r\nfatal: Not a git repository: /data/work/Git/pytorch/.git/modules/torch/lib/gloo\r\nUnable to find current revision in submodule path 'torch/lib/gloo'                                      \r\nThe command '/bin/sh -c git submodule update --init' returned a non-zero code: 1\r\n"", '/data/work/Git/', '/data/work/Git/pytorch/.git/modules/torch/lib/gloo/.git', '\r\ngitdir: /data/work/Git/pytorch/.git/modules/torch/lib/gloo\r\n', '\r\nCOPY . . \r\n', '/opt']",[],0,0
376,pytorch,26551,open,No way to disable mse_loss broadcasting warning,"## üêõ Bug

Hi, 

torch.nn.functional.mse_loss always throws a warning when using it with two tensors of different shapes.
There are legitimate reasons for wanting to use differently shaped tensor and taking advantage of the standard broadcasting behaviour of pytorch, so there needs to be a way to disable that warning.

## To Reproduce



## Expected behavior

There should be an optional parameter to disable the warning, like , .
As discussed here: https://github.com/pytorch/pytorch/issues/16045#issuecomment-476266780

## Additional context

There are legitimate cases for wanting to calculate the MSE between differently shaped tensors.
For example I need to calculate the difference between each sample and a subset of other samples. So I need to calculate the MSE between tensors shaped like:
(number of samples, dimension of a sample, 1) and (number of samples, dimension of a sample, number of samples in the subset).
I understand that safeguards need to be put in-place to avoid misleading people, as discussed in the original issue (https://github.com/pytorch/pytorch/issues/16045) but broadcasting is a Pytorch staple and users shouldn't have to re-implement basic functions like the MSE to make use of its full power.

Cheers
",enhancement module: nn triaged,"['Yes, patches welcome. cc @gchanan who mentioned this in the original issue', 'FYI, as a workaround you can use:\r\n\r\na) `F.mse_loss(a, b.expand_as(a))` in your example\r\nb) The [`warning`](https://docs.python.org/3/library/warnings.html) module to suppress warnings']","['\r\nimport torch\r\nimport torch.nn.functional as F\r\n\r\na = torch.randn(5, 4)\r\nb = torch.randn(5, 1)\r\n\r\nF.mse_loss(a, b)\r\n']","['warn_broadcasting=True', 'error_broadcasting=False']",0,0
377,pytorch,28329,open,No in-place version of where(),"## üöÄ Feature
There is no way to perform the functionality of  in-place. This feature would either add a  method to  or an  parameter to the existing , or both.

## Motivation

All of the usual reasons for doing operations in-place.

## Pitch

As stated above, add a  method to  or an  parameter to the existing .

## Alternatives

One could use the out-of-place  for a less efficient alternative.

Note that the  method does not have the same functionality as an in-place .
",OSS contribution wanted function request module: sorting and selection triaged,"['The in place version would choose the src tensor if the condition is true and take an additional tensor of the same size as argument to choose from if the condition is false, I assume?\r\n\r\nThis seems like a reasonable addition.', 'Correct, with the usual broadcasting semantics for the condition and second tensor. The function is basically a no-op for elements where the condition is true and a copy where the condition is false.', 'Are you interested in submitting a PR for this?', 'Yes.\r\n\r\nWhat is the criterion for adding an underscore method to `Tensor`? There are some point-wise operations that do not have one yet also have a function with an `out` argument. A good example is max: there is no `.max_` method on `Tensor`, but there is a `torch.max` function with an `out` argument.\r\n', '@gchanan Can you comment on this?', ""From what I see:\r\n- inplace operation (`*_`) are only possible if the output is the same size as the input.\r\n- `out=` construction is always possible as the Tensor given to out will be resized to the need of the function (potentially being backed by new memory after the resize).\r\n\r\nSo if you want an inplace version, you want a `*_` method.\r\nThe `out=` construction does not guarantee that the result will be written inplace in `out`. Only that this Tensor will contain the result after the op.\r\n\r\nAlso `max` is a bit special because even though we could have an inplace version of `torch.max(t1, t2)`, we can't have an inplace version of `torch.max(t1, dim=0)` because the output is not the same size as the input."", 'I see, so in this case it would make sense to add both `Tensor.where_()` and `torch.where(..., out)`.', ""If we had only one, `Tensor.where_()` feels more useful.\r\n\r\nI think there aren't that many people that make use of the `out=` api anymore (mainly because it does not support autograd). And with the good allocators we have now (especially on gpu), reusing buffers is not as useful as it used to be."", ""I agree with @albanD's take.\r\n\r\nAlso the `out=` API is broken, see e.g. https://github.com/pytorch/pytorch/issues/8989, so we are generally hesitant to add to it unless there is a good reason (and since inplace covers your usecase, there isn't really a good reason here).""]",[],"['torch.where()', '.where_()', 'Tensor', 'out', 'torch.where()', '.where_()', 'Tensor', 'out', 'torch.where()', 'torch.where()', '.masked_scatter_()', 'where()']",0,0
378,pytorch,23301,open,Error while using Libtorch + OpenCV + Qt Creator,"I have the following configuration in the .pro file

 

OpenCV works absolutely fine **without** ""QMAKE_CXXFLAGS += -D_GLIBCXX_USE_CXX11_ABI=**0**"". With this, however, I get this following errors:

![Screenshot_2019-07-24_14-35-50](https://user-images.githubusercontent.com/17313248/61799072-b6307800-ae2a-11e9-9bb3-e8b8068c8a87.png)

OpenCV works fine **with** ""QMAKE_CXXFLAGS += -D_GLIBCXX_USE_CXX11_ABI=**1**"" as well. But it throws a different set of errors:

![Screenshot_2019-07-24_14-38-10](https://user-images.githubusercontent.com/17313248/61799101-c183a380-ae2a-11e9-9da8-ff815f721dde.png)

Setting ""QMAKE_CXXFLAGS += -D_GLIBCXX_USE_CXX11_ABI=**0**"" has been recommended for Libtorch in most of the forums to avoid the errors above.

What could be a solution or some solutions to work around this?
(I am a newbie to both Libtorch and Qt Creator.)",module: cpp triaged,[],['\r\n    TEMPLATE = app\r\n    CONFIG += console c++11\r\n    CONFIG -= app_bundle\r\n    CONFIG -= qt\r\n    CONFIG += thread\r\n    \r\n    SOURCES += main.cpp\r\n    \r\n    INCLUDEPATH += /usr/local/include/opencv4\r\n    LIBS += -L/usr/local/lib/\r\n    LIBS += -lopencv_core\r\n    LIBS += -lopencv_highgui\r\n    LIBS += -lopencv_imgproc\r\n    LIBS += -lopencv_videoio\r\n\r\n    QMAKE_CXXFLAGS += -D_GLIBCXX_USE_CXX11_ABI=0\r\n    \r\n    INCLUDEPATH += /path/to/libtorch/include\r\n    INCLUDEPATH += /path/to/libtorch/include/torch/csrc/api/include\r\n    LIBS += -L/path/to/libtorch/lib\r\n    LIBS += -ltorch -lc10\r\n'],[],0,0
379,pytorch,18776,open,Value of torch.backends.cudnn.benchmark Baked into JIT-Traced Modules ( 150x slowdown on ConvTranspose2d() ) [jit] [libtorch] [cudnn] ,"## üêõ Bug

If you trace a module with  and load that script module in C++ via LibTorch, the resulting behavior in C++ depends on whether or not the  flag was set. Calls to  from the C++ API at runtime appear to have no effect.

## To Reproduce

**NOTE**: I was not able to verify this issue still exists on the latest nightly (20190402) because it appears the latest nightly (at least on Windows) cannot run JIT-traced models. Even the simplest model gives the following error:



1) Run the python script below:  or 
2) Compile + run the C++ code below.
3) Observe:
  a) Average time per call. I see ~0.8ms in the python script and either ~0.8 or ~120ms in C++ depending on the flag used in python. In either case, C++ sets benchmarking ON. (GTX 1080)
  b) Kernel run by CuDNN. w/either setting of the flag, the python code runs . With the flag ON, it runs  once (taking ~120ms) and then chooses the faster . If the flag was ON in python, C++ also chooses  but if the flag was OFF in python, it always chooses  regardless of the flag setting in C++.

I observed the choice of kernel using .

Python Script ():


C++ Code:


## Expected Behavior

I would expect that either: 1) the C++ setting of  should be respected (choosing the correct algorithm) or 2) at least print a warning that it is being overridden by the value of the flag at trace time.

## Additional Info
I printed the JIT graphs generated with benchmarking ON/OFF and got the following with the flag OFF:


The only change when the flag is ON is that register %17 is 1 instead of 0. I suppose this is where the ""hardcoding"" of the flag might be happening?

## Environment
Python code was run on Linux, C++ code was run on Windows

 - PyTorch Version (e.g., 1.0): 1.0.0.dev20190311 on linux, 2336f0ba0 on Windows
 - OS (e.g., Linux): Fedora 29, Windows 10 1809
 - How you installed PyTorch (, , source): conda (pytorch-nightly)
 - Python version: 3.7
 - CUDA/cuDNN version: CUDA 10, cuDNN 7.4.2
 - GPU models and configuration: Titan RTX (linux), GTX 1080 (windows)

cc @suo",oncall: jit triaged,"[""Yea in the current implementation it's expected behavior for the loaded module using a flag as constant set in trace phase. We could potentially add a warning for it. \r\nOn the other hand, is it possible for us to detect these backend related flags in runtime, and remove them as an input argument for `_convolution`? cc: @ngimel @apaszke @zdevito "", ""If you adjust it to not trace `_convolution`, and instead trace the function that wraps around it, you won't hardcode the flag."", ""I think the way the code is currently structured, there are a lot of call sites of `_convolution`, so this may be annoying to do. One possibility is to add another intermediate function between those call sites and `_convolution` which doesn't have the backend flag."", ""@ezyang In tracing we deliberately not to trace the function wraps around `_convolution` here. https://github.com/pytorch/pytorch/blob/173f224570017b4b1a3a1a13d0bff280a54d9cd9/tools/autograd/gen_variable_type.py#L41\r\nI'm not very familiar with the reason behind it though. "", 'we need the backend flag to compute backward correctly.', ""@soumith we don't. I wrote symbolic AD for batch norm which allows for multiple backends. I think the idea was that someone might have code like:\r\n```py\r\ntorch.backends.cudnn.enabled = False\r\nsome_op() # cuDNN bug\r\ntorch.backends.cudnn.enabled = True\r\n```\r\nAnd we wanted to preserve this semantics. I think we should save those flags when tracing."", 'Can we distinguish the case where the user explicitly set the flag during the run of the model, from the case where the flag is set outside the model?\r\n\r\nIn specific, I mean, we should trace/script `torch.backends.cudnn.enabled = False` as an operator `aten::setBenchmarkCuDNN(false)` or `prim::setBenchmarkCuDNN(true)`, and remove the `benchmark` argument from `aten::convolution`.\r\n\r\nIn such design\r\n\r\n```python\r\ntorch.backends.cudnn.enabled = False\r\n\r\n@script\r\ndef model(x):\r\n    return F.convolution(x, .....)\r\n```\r\n\r\nwill respect whatever the flag is set at the context where the model is executed, while\r\n\r\n```python\r\n@script\r\ndef model(x):\r\n    torch.backends.cudnn.enabled = False\r\n    return F.convolution(x, .....)\r\n```\r\nwill always run without cuDNN regardless of the context.']","['\r\nINVALID_ARGUMENT:: Cannot find field. (deserialize at ..\\torch\\csrc\\jit\\import.cpp:108)\r\n(no backtrace available)\r\n', 'python\r\nimport sys\r\nimport time\r\n\r\nimport torch as th\r\nth.backends.cudnn.benchmark = bool(int(sys.argv[1]))\r\n\r\nmod = th.nn.ConvTranspose2d(8, 3, 4, 2, 1).cuda()\r\ninp = th.zeros(1, 8, 512, 512).cuda()\r\n\r\nmod(inp); mod(inp); mod(inp)\r\n\r\nsmod = th.jit.trace(mod, (inp,), check_trace=False)\r\nsmod.save(""smod.ptj"")\r\n\r\nN = 1000\r\nth.cuda.synchronize()\r\nstart = time.time()\r\nfor _ in range(N):\r\n    mod(inp)\r\n    th.cuda.synchronize()\r\nend = time.time()\r\nprint(""Time (ms):"", 1000*(end-start)/N)\r\n', 'c++\r\n#include <chrono>\r\n#include <iostream>\r\n\r\n#include <c10/cuda/CUDAGuard.h>\r\n#include <torch/script.h>\r\n#include <torch/torch.h>\r\n\r\n#include <cuda_runtime_api.h>\r\n\r\nint main() {\r\n  at::globalContext().setBenchmarkCuDNN(true);\r\n  auto nograd = torch::NoGradGuard();\r\n\r\n  try {\r\n    auto mod = torch::jit::load(""smod.ptj"");\r\n    mod->to(torch::kCUDA);\r\n    torch::jit::Stack input_stack = {torch::zeros({1, 8, 512, 512}, torch::kCUDA)};\r\n\r\n    mod->forward(input_stack);\r\n    mod->forward(input_stack);\r\n    mod->forward(input_stack);\r\n\r\n    const int N = 100;\r\n    cudaDeviceSynchronize();\r\n    const auto start = std::chrono::high_resolution_clock::now();\r\n    for (int i = 0; i < N; ++i) {\r\n      mod->forward(input_stack);\r\n      cudaDeviceSynchronize();\r\n    }\r\n    const auto end = std::chrono::high_resolution_clock::now();\r\n    const float elapsed = std::chrono::duration<float, std::milli>(end - start).count() / N;\r\n    std::cout << ""Time (ms): "" << elapsed << std::endl;\r\n  } catch (c10::Error e) {\r\n    std::cerr << e.what() << std::endl;\r\n    return 1;\r\n  }\r\n  return 0;\r\n}\r\n', '\r\ngraph(%input : Float(1, 8, 512, 512),\r\n      %1 : Float(8, 3, 4, 4),\r\n      %2 : Float(3)):\r\n  %3 : int = prim::Constant[value=2](), scope: ConvTranspose2d\r\n  %4 : int = prim::Constant[value=2](), scope: ConvTranspose2d\r\n  %5 : int[] = prim::ListConstruct(%3, %4), scope: ConvTranspose2d\r\n  %6 : int = prim::Constant[value=1](), scope: ConvTranspose2d\r\n  %7 : int = prim::Constant[value=1](), scope: ConvTranspose2d\r\n  %8 : int[] = prim::ListConstruct(%6, %7), scope: ConvTranspose2d\r\n  %9 : int = prim::Constant[value=1](), scope: ConvTranspose2d\r\n  %10 : int = prim::Constant[value=1](), scope: ConvTranspose2d\r\n  %11 : int[] = prim::ListConstruct(%9, %10), scope: ConvTranspose2d\r\n  %12 : bool = prim::Constant[value=1](), scope: ConvTranspose2d\r\n  %13 : int = prim::Constant[value=0](), scope: ConvTranspose2d\r\n  %14 : int = prim::Constant[value=0](), scope: ConvTranspose2d\r\n  %15 : int[] = prim::ListConstruct(%13, %14), scope: ConvTranspose2d\r\n  %16 : int = prim::Constant[value=1](), scope: ConvTranspose2d\r\n  %17 : bool = prim::Constant[value=0](), scope: ConvTranspose2d\r\n  %18 : bool = prim::Constant[value=0](), scope: ConvTranspose2d\r\n  %19 : bool = prim::Constant[value=1](), scope: ConvTranspose2d\r\n  %20 : Float(1, 3, 1024, 1024) = aten::_convolution(%input, %1, %2, %5, %8, %11, %12, %15, %16, %17, %18, %19), scope: ConvTranspose2d\r\n  return (%20)\r\n']","['torch.jit.trace(...)', 'torch.backends.cudnn.benchmark', 'at::globalContext().setBenchmarkCuDNN(true/false)', 'python test.py 0', 'python test.py 1', 'cudnn::detail::dgrad_engine<...>', 'cudnn::detail::dgrad2d_alg1_1<...>', 'dgrad_engine', 'dgrad_engine', 'dgrad2d_alg1_1', 'nvprof python test.py 0/1', 'test.py', 'at::globalContext().setBenchmarkCuDNN(true)', 'conda', 'pip']",0,0
380,pytorch,20056,open,Creation of too big multidimensional array returns empty tensor.,"## üêõ Bug

When trying to create a tensor of too many dimensions it simply returns an empty tensor with its shape is the dimensions I passed even though it should have contained the value .

## To Reproduce

Steps to reproduce the behavior:

Number of 2's in the following example is .

Output: 



Another related issue



These are  2's. 

Output: 



When they're  2's. The output changes to:



<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

I expected to see the aforementioned dimensions filled with  if it's possible, or an exception that says a value error.

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


## Additional context

I suspect that the desired behavior is not happening because tensors seem to be basing on NumPy  which accepts only  dims.

Also, I have just found this [line](https://github.com/pytorch/pytorch/blob/7ddd5d06ed07a50b94aa6b2fdffa2f667d677c4b/aten/src/TH/THGeneral.cpp#L184).



I think the integer division of those two elements results in the error. ",module: error checking module: tensor creation triaged,"[""@soumith I'm not sure If I should be mentioning you, but I just wanted to know the updates on this. I did not receive any comments."", ""I dont think it's been resolved yet. I dont think anyone is actively working on it at the moment either"", ""@soumith I could work on it if it's confirmed as a bug. "", 'cc: @gchanan ', ""I was able to confirm the first issue on master, if you would like to look into fixing it, @andrewnaguib we would be happy to give you guidance.\r\n\r\nI wasn't able to reproduce the second issue, it looks to have been fixed:\r\n```\r\n      1 torch.ones((2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2),\r\n----> 2 dtype=torch.uint8)\r\n\r\nRuntimeError: [enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 4611686018427387904 bytes. Error code 12 (Cannot allocate memory)\r\n```\r\n\r\nThe third issue still seems relevant, though:\r\n```\r\nIn [5]: torch.ones((2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2\r\n   ...: , 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2),\r\n   ...: dtype=torch.uint8)\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-5-796b36549665> in <module>()\r\n      1 torch.ones((2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2),\r\n----> 2 dtype=torch.uint8)\r\n\r\nRuntimeError: [enforce fail at CPUAllocator.cpp:47] ((ptrdiff_t)nbytes) >= 0. alloc_cpu() seems to have been called with negative number: 9223372036854775808\r\n```"", 'Yes, I would. Thanks!  I will look into the 1st and 3rd issues, and will ensure I have the latest version (anyways) and recheck the 2nd. Where do you suggest I start looking? ', ""I don't think this would be something easy fixing. IIRC  TensorIterator has assumptions on the maximum number of dimensions that a Tensor can have."", '@fmassa Are you saying that the first issue has a direct connection with changing the maximum number of dimensions? I only thought the intention would be raising a `RuntimeError`. Also, where the maximum number of dims defined? \r\n', 'raising a `RuntimeError` sounds good.  Silently giving you an empty tensor is bad.', '@gchanan Yeah. However, does that mean a tensor has a maximum number of dims? ', ""I'm not sure if we generally enforce that at construction time.  Do you get the same behavior if you use `torch.empty` ?"", 'Here is an example where we enforce a max number of dimensions\r\nhttps://github.com/pytorch/pytorch/blob/eabfca3577ea85df2d68bdf747c62dd4a5fff5cf/aten/src/ATen/cuda/detail/OffsetCalculator.cuh#L15\r\nThis affect most point wise operations in PyTorch ', ""But I think I misunderstood the point in this issue. If raising an error at creation time is the goal, then I'm ok with it. But making PyTorch support tensors with arbitrarily large number of dimensions is more complicated. It used to be supported in the past, but we dropped support for it last year"", '@gchanan Yes, same behavior exist with `torch.empty`. Also, confirmed the 1st and 3rd issues on the master (at the time of writing this reply).  \r\n\r\nI believe the both issues happens because of the unexpected behavior of casting:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/4976208e739a05a7cf6309f33f3b07ad0cd8e50e/c10/core/CPUAllocator.cpp#L48\r\n\r\nBecause of an overflow; since if you try `std::numeric_limits<std::ptrdiff_t>::max()`, the result will be `9223372036854775807` (x64) which is less than `9223372036854775808`. `ptrdiff_t` unexpectedly returns the negative corresponding. \r\n\r\nAlso, if you try to case any number greater than >2<sup>64</sup>, `ptrdiff_t` will simply return `0` which is, indeed, `True` in the `>=0` part. (`=`) is unnecessary anyways because it is caught in:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/4976208e739a05a7cf6309f33f3b07ad0cd8e50e/c10/core/CPUAllocator.cpp#L42-L44\r\n\r\nCan you confirm, please?\r\n\r\nShould I add unit tests for these issues? \r\n\r\n---\r\n\r\n@fmassa How does the `MAX_DIMS` you mentioned differ from the one in:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/e44b2b72bd4ccecf9c2f6c18d09c11eff446b5a3/torch/csrc/utils/tensor_new.cpp#L46\r\n\r\nAlthough it is more reasonable to raise an error message at the current situation, what limits supporting larger number of dims (are the release notes for dropping support exist for checking?)? Also, why it is not explicitly stated in the docs? \r\n\r\n', '@gchanan can you confirm, please? ', '@soumith Would it be possible to have someone just to confirm on the issue resolution in my previous comment so that I can prepare a PR?', '@ngimel is looking into it.', ""There are several issues here\r\n1) MAX_DIMS=128 in tensor_new.cpp is definitely wrong, pytorch doesn't support 128-dim tensors anywhere else, as @fmassa pointed out the number of dimensions on cuda e.g. is limited to 25. We should change MAX_DIMS in tensor_new.cpp. \r\n2) @andrewnaguib is correct, there's no reason casting nbytes to ptrdiff_t for comparison https://github.com/pytorch/pytorch/blob/4976208e739a05a7cf6309f33f3b07ad0cd8e50e/c10/core/CPUAllocator.cpp#L48. `posix_memalign` expects `size_t` argument. On a 64-bit system max value for size_t is 2**64-1, and max_value for ptrdiff_t is 2**63-1. Avoiding this cast will make the check correctly fire when number of elements is between 2**63 and 2**64. Comparison to 0 is also not necessary, as it was taken care of before\r\n3) That said, we still cannot guarantee that `nbytes` computation will be correct because it can overflow in size_t, and we didn't find a palatable way to address it, see #42582 and linked PR. \r\n""]","['Python\r\ntorch.ones((2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2),\r\ndtype=torch.uint8)\r\n', 'Python\r\ntensor([],\r\n       size=(2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2),\r\n       dtype=torch.uint8)\r\n', 'Python\r\ntorch.ones((2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2),\r\ndtype=torch.uint8)\r\n', 'Python\r\nRuntimeError: $ Torch: not enough memory: you tried to allocate 0GB. Buy new RAM! at /opt/conda/conda-bld/pytorch_1549628766161/work/aten/src/TH/THGeneral.cpp:201\r\n', 'Python\r\nRuntimeError: $ Torch: invalid memory size -- maybe an overflow? at /opt/conda/conda-bld/pytorch_1549628766161/work/aten/src/TH/THGeneral.cpp:188\r\n', '\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n', '\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Arch Linux\r\nGCC version: (crosstool-NG 1.23.0.449-a04d0) 7.3.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.105\r\nGPU models and configuration: GPU 0: GeForce GTX 1050 Ti with Max-Q Design\r\nNvidia driver version: 418.56\r\ncuDNN version: /usr/lib/libcudnn.so.7.5.0\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.2\r\n[pip3] torch==1.0.1.post2\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] _tflow_select             2.3.0                       mkl  \r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.1                      144  \r\n[conda] mkl_fft                   1.0.10           py36ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py36hd81dba3_0  \r\n[conda] pytorch                   1.0.1           py3.6_cuda9.0.176_cudnn7.4.2_2    pytorch\r\n[conda] tensorflow                1.12.0          mkl_py36h69b6ba0_0  \r\n[conda] tensorflow-base           1.12.0          mkl_py36h3c3e929_0  \r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n', 'cpp\r\nTHError(""$ Torch: not enough memory: you tried to reallocate %dGB. Buy new RAM!"", size/1073741824);\r\n']","['1', '>63', '62', '63', '1', 'ndarray', '32']",0,0
381,pytorch,10746,open,[Caffe2] Failed to build dispatch_test. Error LNK2001: unresolved external symbol,"## Issue description

Getting LNK2001 on Windows for .


## System Info
- PyTorch or Caffe2: C2
- How you installed PyTorch (conda, pip, source): src
- Build command you used (if compiling from source): cmake
- OS: Win10
- PyTorch version: master
- VS version (if compiling from source): 2017
- CMake version: 3.12",caffe2,[],"['\r\npytorch\\build\\caffe2\\core\\dispatch\\dispatch_test.vcxproj"" (default target) (56) ->\r\n(Link target) -> \r\n  KernelRegistration.obj : error LNK2001: unresolved external symbol ""__declspec(dllimport) private: __cdecl caffe2::TypeIdentifier::TypeIdentifier(unsigned short)"" (__imp_??0TypeIdentifier@caffe2@@AEAA@G@Z) [pytorch\\build\\caffe2\\core\\dispatch\\dispatch_test.vcxproj]\r\n  OpSchema.obj : error LNK2001: unresolved external symbol ""__declspec(dllimport) private: __cdecl caffe2::TypeIdentifier::TypeIdentifier(unsigned short)"" (__imp_??0TypeIdentifier@caffe2@@AEAA@G@Z) [pytorch\\build\\caffe2\\core\\dispatch\\dispatch_test.vcxproj]\r\n  OpSchemaRegistration.obj : error LNK2001: unresolved external symbol ""__declspec(dllimport) private: __cdecl caffe2::TypeIdentifier::TypeIdentifier(unsigned short)"" (__imp_??0TypeIdentifier@caffe2@@AEAA@G@Z) [pytorch\\build\\caffe2\\core\\dispatch\\dispatch_test.vcxproj]\r\n  OpSchema_test.obj : error LNK2001: unresolved external symbol ""__declspec(dllimport) private: __cdecl caffe2::TypeIdentifier::TypeIdentifier(unsigned short)"" (__imp_??0TypeIdentifier@caffe2@@AEAA@G@Z) [pytorch\\build\\caffe2\\core\\dispatch\\dispatch_test.vcxproj]\r\n  Dispatcher.obj : error LNK2001: unresolved external symbol ""__declspec(dllimport) private: __cdecl caffe2::TypeIdentifier::TypeIdentifier(unsigned short)"" (__imp_??0TypeIdentifier@caffe2@@AEAA@G@Z) [pytorch\\build\\caffe2\\core\\dispatch\\dispatch_test.vcxproj]\r\n  DispatchKey.obj : error LNK2001: unresolved external symbol ""__declspec(dllimport) private: __cdecl caffe2::TypeIdentifier::TypeIdentifier(unsigned short)"" (__imp_??0TypeIdentifier@caffe2@@AEAA@G@Z) [pytorch\\build\\caffe2\\core\\dispatch\\dispatch_test.vcxproj]\r\n  DispatchTable.obj : error LNK2001: unresolved external symbol ""__declspec(dllimport) private: __cdecl caffe2::TypeIdentifier::TypeIdentifier(unsigned short)"" (__imp_??0TypeIdentifier@caffe2@@AEAA@G@Z) [pytorch\\build\\caffe2\\core\\dispatch\\dispatch_test.vcxproj]\r\n  pytorch\\build\\bin\\RelWithDebInfo\\dispatch_test.exe : fatal error LNK1120: 1 unresolved externals [pytorch\\build\\caffe2\\core\\dispatch\\dispatch_test.vcxproj]\r\n\r\n']",['dispatch_test'],0,0
382,pytorch,31657,open,Use of Sequence collections for abstract classes in Dataset,"## üöÄ Feature
Using the sequence (or any other similar Python abstract class) for the  class in order to tackle the note in lines 30-32 of :

def __len__(self)__len__

By using the  abstract classes, we can see in the official Python documentation that they have the exact abstract methods that the  class expects, i.e.  and .

## Motivation

My motivation is three-fold: (1) tackling the note mentioned above left by the PyTorch contributors, (2) let the code be auto-documented‚Äîsince accessing this code states explicitly that you need to define those two methods and, (3) using Python abstract classes instead of inheritting from object (which is a rather deprecated practice for when Python did not have abstract classes capabilities).

## Pitch

Implement the Dataset class using new Python abstract classes capabilities, or at least discuss about it.

## Alternatives

I currently do not have alternative proposals.

## Additional context

.


cc @SsnL",feature module: dataloader triaged,[],[],"['Dataset', 'torch.utils.data.dataset', '', 'python\r\n    # No ', ' default?\r\n    # See NOTE [ Lack of Default ', ' in Python Abstract Base Classes ]\r\n    # in pytorch/torch/utils/data/sampler.py\r\n', '', 'Sequence', 'Dataset', '__getitem__', '__len__', 'None']",0,0
383,pytorch,24675,closed,Migrate `_multinomial_alias_setup` from the TH to Aten (CPU),"Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review.",better-engineering module: porting triaged,"['`_multinomial_alias_setup` no longer exists, as of #50489.']",[],[],0,0
384,pytorch,215,open,missing tensor constructors,"Missing while writing docs

- (size, stride)
- (storage, storageOffset, size, stride)",low priority module: docs todo triaged,"['Why would you ever want to create a tensor with specified sizes and strides? üòï\nSecond one can be easily emulated with `set_`, but I can add it to the constructor if you want\n', 'hmmm not sure how useful it is, yea. I can only think of it being useful when one wants to construct zero-strided tensors.\n', ""Yeah, but it's an edge case and `torch.Tensor(1).expand(10)` gives you the same result, while being much more readable.\n"", 'true! prob not needed then.\n', '@soumith I can work on this issue. Can you give me some pointers on what to look at, if I want to work on this issue?']",[],[],0,0
385,pytorch,12555,closed,add nvidia driver version to environment collection script,"https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py currently misses collecting nvidia driver version.
Would be useful to have that.",,"['It seems to be there here https://github.com/pytorch/pytorch/blob/master/torch/utils/collect_env.py#L254 .\r\n\r\nAre you expecting something else?', 'nevermind, my bad.']",[],[],0,0
386,pytorch,10751,closed,[docs] Error in documentation for fft normalization,"The primary fft equation in the documentation [here](https://github.com/pytorch/pytorch/blob/8013dac43d2acb592cab75317f17d4f9c5b9eb6a/torch/_torch_docs.py#L5303) is 

However, from experimentation it seems the  1/N factor  should not be there for the forward transform. 

Here is the relevant line:
https://github.com/pytorch/pytorch/blob/8013dac43d2acb592cab75317f17d4f9c5b9eb6a/torch/_torch_docs.py#L5303

The inverse transform equation (with the 1/N) is consistent with the behavior of the implementation.

Note, the documentation is accurate for , , but this only makes sense when if that 1/N factor isn't there in the main equation above. 

Note, the documentation for  is also accurate for , but again this only makes sense if the 1/N factor isn't there in the main equation above.

Lastly, the documentation for  is a mix of two problems. [This line](https://github.com/pytorch/pytorch/blob/8013dac43d2acb592cab75317f17d4f9c5b9eb6a/torch/_torch_docs.py#L5493) says 

This says ""multiply"" instead of ""divide"" (as in fft). This is consistent with the primary equation, but it is not consistent with the implementation. It should be ",,[],"['   X[\\omega_1, \\dots, \\omega_d] =\r\n        \\frac{1}{\\prod_{i=1}^d N_i} \\sum_{n_1=0}^{N_1} \\dots \\sum_{n_d=0}^{N_d} x[n_1, \\dots, n_d]\r\n         e^{-j\\ 2 \\pi \\sum_{i=0}^d \\frac{\\omega_i n_i}{N_i}},\r\n', 'If normalized is set to True, this normalizes the result by multiplying it with [sqrt(N)]']","['\\frac{1}{\\prod_{i=1}^d N_i}', 'normalized=True', 'If normalized is set to True, this normalizes the result by dividing by [1/sqrt(N)]', 'ifft', 'normalized=True', 'rfft', 'divide']",0,0
387,pytorch,17397,closed,How does pytorch count batch size in SGD with multiGPU when it does Batch Norm?,"## ‚ùì Questions and Help
When pytorch does the batch normalization in SGD, what batch size it use? Per GPU batch size, or total batch size over all GPUs?
For example, suppose I set the per-GPU batch size as 32, and I use 8 GPUs. When Pytorch does batch normalization, what batch size it use, 32 or 32 x 8?
My concern comes from a facebook's paper ""Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour"". If Pytorch's implementation is the same with what this paper describes, I can modify the parameters of my model accordingly; if not, I need to know. Thanks!
",,['please use https://discuss.pytorch.org for questions'],[],[],0,0
388,pytorch,12608,closed,ONNX model export issue,"Hi,

I would like to get some general ideas on exporting ONNX model when model accepts both input sequence and output labels as arguments. How would you set up Variables to traverse through the model to export ONNX?

Thanks.
",,['Closed as out-of-date.'],[],[],0,0
389,pytorch,347,closed,cudnn 3d and 1d modules,"we only added bindings for 2d modules. finish up.
Assigning to Sam.",high priority,['Implemented in #359.'],[],[],0,0
390,pytorch,8981,closed,Intermittent failure of TestCollectEnv.test_expect,"The failing diff usually looks something like:



I thought this was some weird buffering problem, but then I looked more carefully at the test, and one of the regexes looks VERY suspicious:



If the size of the short version hash varies, this regex will gobble more or less. It really shouldn't be hardcoded as wildcard periods.

CC @zou3519 ",,[],"['\r\n06:19:31   Versions of relevant libraries:\r\n06:19:31   [pip] numpy (1.14.X)\r\n06:19:31 - [pip] torch (0.5.0a0\r\n06:19:31 + [pip] torch (0.5.0a0)\r\n06:19:31 ?                     +\r\n', ""\r\n        version_hash_regex = re.compile(r'(a\\d+)\\+.......')\r\n        result = re.sub(version_hash_regex, r'\\1', info_output).strip()\r\n""]",[],0,0
391,pytorch,6518,closed,[PyTorch] torch.Tensor and torch.Tensor.dtype have confusing repr,"
I expect to at least see  print  by default.

cc @gchanan ",,"['`torch.Tensor` is now a class, not an alias of the default tensor type.  This seems fine to me.']","[""python\r\n>>> torch.Tensor\r\n<class 'torch.Tensor'>\r\n>>> torch.Tensor.dtype\r\n<attribute 'dtype' of 'torch._C._TensorBase' objects>\r\n""]","['torch.Tensor.dtype', 'torch.float32']",0,0
392,pytorch,21119,closed,One libtorch FindTorch.cmake module to support multi-configs(both debug and release),"## üêõ Bug

Libtorch is excellent to provide cmake for multiple configs on windows. 

  * Download here (Release version): 
    https://download.pytorch.org/libtorch/cu90/libtorch-win-shared-with-deps-latest.zip
  * Download here (Debug version): 
    https://download.pytorch.org/libtorch/cu90/libtorch-win-shared-with-deps-debug-latest.zip

However, it is quite inconvenient. Usually, I need to copy different versions to switch between different configurations, which is annoying during development. Can we support both configs inside **one FindTorch.cmake**?

",,"['no, we explicitly dont aim to cover that. The binary files are already pretty large.']",[],[],0,0
393,pytorch,3231,closed,.numpy() conversion problem,"Converting an ndarray to a buffer and back works as expected, but not if the ndarray was given by  where t is a 

This works:



This doesn't:

",,"['Seems like this is a numpy issue, `frombuffer()` assumes float64, specifying dtype=np.float32 will work.\r\n\r\n```python\r\na = torch.arange(0, 100)\r\nb = np.frombuffer(bytes(a.numpy()), dtype=np.float32)\r\nprint(b.shape)\r\n```\r\n\r\nPlease close this issue.']","['python\r\nb = np.frombuffer(bytes(np.arange(0, 100)), dtype=float)\r\nprint(b.shape)\r\n', 'python\r\na = torch.arange(0, 100)\r\nb = np.frombuffer(bytes(a.numpy()), dtype=float)\r\nprint(b.shape)\r\n']","['t.numpy()', 'torch.Tensor']",0,0
394,pytorch,28775,closed,[feature request] Whether or not any planning to support mips64 arch environments,"

## üöÄ Feature
pytorch can be compiled in  arch successfully, and then utilize related API to develop :+1: 

## Motivation
I'm working on compiling pytorch in mips64 arch, but it doesn't work


I have tried add  in  and , same error happened.

## Pitch
pytorch can be compiled in  arch

<!-- A clear and concise description of what you want to happen. -->

## Alternatives
1) When I see warning info in  for , I plan to fake match rule in , for example:

The ultimate result will also failed since  . So, whether or not  support  arch, too? Thanks!
",feature triaged,"['After investigating and learning, I add MIPS arch define in `aten/src/TH/vector/simd.h`, `third_party/sleef/src/arch/helperpurec_scalar.h` with `debian:stretch` docker image. Finally, it does work!\r\n```\r\nroot@mips64:~/pytorch# ls -lh dist/torch-1.4.0a0+f067088-cp37-cp37m-linux_mips64.whl \r\n-rw-r--r-- 1 root root 25M Mar  8 05:35 dist/torch-1.4.0a0+f067088-cp37-cp37m-linux_mips64.whl\r\n\r\nroot@mips64:/# python -c ""from distutils.util import get_platform; import torch; print(\'pytorch is ok in {}\'.format(get_platform()))""\r\npytorch is ok in linux-mips64\r\n\r\n>>> import torch\r\n>>> x = torch.empty(5, 3)\r\n>>> print(x)\r\nError in cpuinfo: processor architecture is not supported in cpuinfo\r\ntensor([[-1.1392e+28,  3.5733e-43, -1.1392e+28],\r\n        [ 3.5733e-43, -6.6617e+27,  3.5733e-43],\r\n        [-6.6571e+27,  3.5733e-43, -6.6598e+27],\r\n        [ 3.5733e-43, -6.6571e+27,  3.5733e-43],\r\n        [-6.6598e+27,  3.5733e-43, -6.6572e+27]])\r\n```\r\n\r\nSince machine resources isn\'t enough, so only `pytest test_autograd.py`, test display is ok:https://user-images.githubusercontent.com/19144683/76158082-df4d1100-614c-11ea-9394-d9e02c9b4089.png\r\n\r\nFollowing https://github.com/pytorch/builder/blob/master/check_binary.sh:\r\n```\r\n>>> from caffe2.python import core\r\nWARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\r\n# expected result, not support MKL\r\n>>> torch.backends.mkl.is_available()\r\nFalse\r\n\r\nroot@mips64:/tmp/venv# install_root=""/tmp/venv/lib/python3.7/site-packages/torch""\r\nroot@mips64:/tmp/venv# g++ example-app.cpp -I$install_root/include -I$install_root/include/torch/csrc/api/include -L$install_root/lib -ltorch -std=gnu++14 -o example-app -lc10 -Wl,--no-as-needed -Wl,-R$install_root/lib \r\nroot@mips64:/tmp/venv# ./example-app \r\nWarning: Simple test passed! (main at example-app.cpp:4)\r\n```\r\n\r\n@zhangguanheng66 I don\'t know about pytorch realization process, related smock testing passed. About above error, any impact on usage later? Thanks!', 'I will close because of no any response, and see: https://github.com/uddmorningsun/mips-applications-in-python/tree/master/pytorch', '@uddmorningsun\r\n‰Ω†ÁºñËØëÊàêÂäü‰∫ÜÔºü', '@nmanhong \r\nÊòØÁöÑÔºå‰∫åËøõÂà∂ wheel ÂåÖËßÅ https://github.com/uddmorningsun/mips-applications-in-python/releases  ÁªÜËäÇ‰ø°ÊÅØÂèÇËÄÉ README.md Âíå Áõ∏Â∫îÁöÑ CHANGELOG.md\r\n\r\nYes, binary wheel packages see https://github.com/uddmorningsun/mips-applications-in-python/releases Details refer to README.md or CHANGELOG.md']","['\r\n(pytorch-venv) [root@lx-app-1 build]# git branch\r\n* v1.3.0\r\n\r\n(pytorch-venv) [root@lx-app-1 build]# cmake -DUSE_DISTRIBUTED=OFF -DBLAS=ATLAS -DAtlas_BLAS_LIBRARY=/usr/lib64/atlas/ -DAtlas_CBLAS_INCLUDE_DIR=/usr/include/atlas-mips64el-base/ -DAtlas_CBLAS_LIBRARY=/usr/lib64/atlas/USE_MPI=OFF -DUSE_FBGEMM=OFF -DCMAKE_SYSTEM_PROCESSOR=mips64  -DUSE_PYTORCH_QNNPACK=OFF -DBUILD_TEST=ON -DUSE_NNPACK=OFF -DUSE_QNNPACK=OFF -DUSE_NUMA=OFF  -DBUILD_CAFFE2_MOBILE=OFF  -DBUILD_BINARY=ON -DUSE_OPENCV=ON -DUSE_CUDA=OFF -DCMAKE_BUILD_TYPE=Release ..\r\n(pytorch-venv) [root@lx-app-1 build]# make -j $(nproc)\r\n... ...\r\nCMake Warning at third_party/cpuinfo/CMakeLists.txt:71 (MESSAGE):\r\n  Target processor architecture ""mips64"" is not supported in cpuinfo.\r\n  cpuinfo will compile, but cpuinfo_initialize() will always fail.\r\n... ...\r\n\r\n... ...\r\n[  6%] Generating contrib/playground/resnetdemo/override_no_test_model_no_checkpoint.py\r\n[  6%] Building C object sleef/src/libm/CMakeFiles/sleefpurecfma_scalar.dir/sleefsimdsp.c.o\r\n[  6%] Building C object sleef/src/libm/CMakeFiles/sleefdetpurec_scalar.dir/sleefsimddp.c.o\r\n[  6%] Building C object sleef/src/libm/CMakeFiles/sleefpurec_scalar.dir/sleefsimdsp.c.o\r\n[  6%] Building C object sleef/src/libm/CMakeFiles/sleefdetpurecfma_scalar.dir/sleefsimdsp.c.o\r\n[  6%] Built target c10\r\n[  6%] Building C object sleef/src/libm/CMakeFiles/sleefdetpurecfma_scalar.dir/sleefsimddp.c.o\r\n[  6%] Generating contrib/playground/resnetdemo/rendezvous_filestore.py\r\nIn file included from /home/yancy/pytorch/third_party/sleef/src/libm/sleefsimdsp.c:187:0:\r\n/home/yancy/pytorch/third_party/sleef/src/arch/helperpurec_scalar.h:56:2: error: #error FP_FAST_FMA or FP_FAST_FMAF not defined\r\n #error FP_FAST_FMA or FP_FAST_FMAF not defined\r\n... ...\r\n', '\r\n... ...\r\n-ELSEIF(NOT CMAKE_SYSTEM_PROCESSOR MATCHES ""^(i[3-6]86|AMD64|x86(_64)?|armv[5-8].*|aarch64)$"")\r\n+ELSEIF(NOT CMAKE_SYSTEM_PROCESSOR MATCHES ""^(i[3-6]86|AMD64|x86(_64)?|armv[5-8].*|aarch64|mips64|mips64el)$"")\r\n... ...\r\n']","['mips64', '-I /home/yancy/pytorch/third_party/sleef/src/arch/', 'build/CMakeCache.txt', 'make rebuild_cache', 'mips64', 'cmake ...', 'third_party/cpuinfo', 'third_party/cpuinfo/CMakeLists.txt', 'cpuinfo', 'cpuinfo', 'mips64']",0,0
395,pytorch,1981,closed,"[Controversial] `pyyaml` is 6 years old, might be best to switch to `ruamel.yaml`?","Commit history from pyyaml, as far as I can tell, http://pyyaml.org/log/ 

<img width=""733"" alt=""screen shot 2017-07-05 at 10 34 54 am"" src=""https://user-images.githubusercontent.com/123560/27858562-9e33dc44-616d-11e7-9525-0cf771d24b31.png"">

Commit history from , https://bitbucket.org/ruamel/yaml/commits/all

<img width=""1355"" alt=""screen shot 2017-07-05 at 10 35 36 am"" src=""https://user-images.githubusercontent.com/123560/27858592-bd36de8e-616d-11e7-8213-548f81909bd2.png"">

ruamel.yaml supports yaml 1.2, and handles round-trip, including comments.",todo,"[""it's one of those things where if there's a super smooth transition, then why not...\r\n\r\nHave you tried `ruamel.yaml` as a drop-in replacement?"", ""PyYAML has had it's last commit made in February this year: https://github.com/yaml/pyyaml"", ""> PyYAML has had it's last commit made in February this year: https://github.com/yaml/pyyaml\r\n\r\nFair enough :-)\r\n\r\nCoupled with\r\n\r\n> Have you tried ruamel.yaml as a drop-in replacement?\r\n\r\nYes... but only with `pip`. With `conda`, I've also tried, but seems to be packaged into a different namespace.\r\n\r\n=> I think I'll close this for now.""]",[],['ruamel.yaml'],0,0
396,pytorch,5949,closed,Bug Report re-initializing weights after layer initialization ,"
Why is this allowed? considering the fact that it changes the entire layer 




This gives a dimension mismatch error
",,"[""Here's one reason: people sometimes want to e.g. enlarge the output size, and this allows them to replace the weights. You can always add an assert that the weights you assign have the same shape if you want to be safe. Thanks for the suggestion, but closing as wontfix.""]","['\r\nx = Variable( torch.randn(1,10))\r\nlayer = nn.Linear(10,10,bias=False)\r\nlayer.weight.data = torch.randn(100,100)\r\n', '\r\nprint(layer)\r\nLinear(in_features=10, out_features=10, bias=False) \r\n\r\n', '\r\nx = torch.randn(1,10)\r\nlayer(Variable(x))\r\n']",[],0,0
397,pytorch,12365,closed,"[JIT] mytuple[0], mytuple[i] does not work in JIT","Current, the following code gives an error:

With error message:
",oncall: jit,"['PR is coming to fix this, will comment when it lands. ', 'I was not aware of https://github.com/pytorch/pytorch/pull/11492, so I created https://github.com/pytorch/pytorch/pull/12203 . I will leave https://github.com/pytorch/pytorch/pull/12203 open for now for my personal usage, but please please also close https://github.com/pytorch/pytorch/pull/12203 when https://github.com/pytorch/pytorch/pull/11492 get merged.', 'Hi - tuple indexing PR has landed. We only support indexing with a constant literal: mytuple[0] will work, but mytuple[i] will not. \r\n\r\n']","['python\r\n@torch.jit.script\r\ndef f(x):\r\n    return torch.sort(x)[0]\r\n', '\r\nTraceback (most recent call last):\r\n  File ""quicktest.py"", line 3, in <module>\r\n    @torch.jit.script\r\n  File ""/Users/gaoxiang/anaconda3/lib/python3.6/site-packages/torch/jit/__init__.py"", line 589, in script\r\n    graph = _jit_script_compile(ast, rcb)\r\nRuntimeError: gatherable->type()->isSubtypeOf(DynamicType::get()) ASSERT FAILED at /Users/administrator/nightlies/2018_09_28/wheel_build_dirs/conda_3.6/conda/conda-bld/pytorch-nightly_1538131714608/work/torch/csrc/jit/script/compiler.cpp:1819, please report a bug to PyTorch. (emitBasicGather at /Users/administrator/nightlies/2018_09_28/wheel_build_dirs/conda_3.6/conda/conda-bld/pytorch-nightly_1538131714608/work/torch/csrc/jit/script/compiler.cpp:1819)\r\nframe #0: torch::jit::script::to_ir::emitSimpleExpr(std::__1::shared_ptr<torch::jit::script::Tree> const&) + 2371 (0x10c3e5e53 in libtorch.dylib)\r\nframe #1: torch::jit::script::to_ir::emitSugaredExpr(torch::jit::script::Expr, unsigned long) + 1446 (0x10c3dd326 in libtorch.dylib)\r\nframe #2: torch::jit::script::to_ir::emitExpr(torch::jit::script::Expr) + 69 (0x10c3dddd5 in libtorch.dylib)\r\nframe #3: torch::jit::script::to_ir::getNamedValues(std::__1::vector<std::__1::shared_ptr<torch::jit::script::Tree>, std::__1::allocator<std::__1::shared_ptr<torch::jit::script::Tree> > >, bool) + 1216 (0x10c3e6a40 in libtorch.dylib)\r\nframe #4: torch::jit::script::to_ir::getValues(std::__1::vector<std::__1::shared_ptr<torch::jit::script::Tree>, std::__1::allocator<std::__1::shared_ptr<torch::jit::script::Tree> > >, bool) + 64 (0x10c3eec30 in libtorch.dylib)\r\nframe #5: torch::jit::script::to_ir::getValues(torch::jit::script::List<torch::jit::script::Expr>, bool) + 86 (0x10c3d8156 in libtorch.dylib)\r\nframe #6: torch::jit::script::to_ir::to_ir(torch::jit::script::Def, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, torch::jit::script::Method&, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, torch::jit::script::Method&> > >&, std::__1::function<std::__1::shared_ptr<torch::jit::script::SugaredValue> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, torch::jit::script::Method&, torch::jit::SourceRange const&)> const&, std::__1::shared_ptr<torch::jit::script::SugaredValue>, torch::jit::script::Method&) + 2754 (0x10c3d5082 in libtorch.dylib)\r\nframe #7: std::__1::__function::__func<torch::jit::script::defineMethodsInModule(torch::jit::script::Module&, std::__1::vector<torch::jit::script::Def, std::__1::allocator<torch::jit::script::Def> > const&, std::__1::vector<std::__1::function<std::__1::shared_ptr<torch::jit::script::SugaredValue> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, torch::jit::script::Method&, torch::jit::SourceRange const&)>, std::__1::allocator<std::__1::function<std::__1::shared_ptr<torch::jit::script::SugaredValue> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, torch::jit::script::Method&, torch::jit::SourceRange const&)> > > const&, std::__1::shared_ptr<torch::jit::script::SugaredValue>)::$_5, std::__1::allocator<torch::jit::script::defineMethodsInModule(torch::jit::script::Module&, std::__1::vector<torch::jit::script::Def, std::__1::allocator<torch::jit::script::Def> > const&, std::__1::vector<std::__1::function<std::__1::shared_ptr<torch::jit::script::SugaredValue> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, torch::jit::script::Method&, torch::jit::SourceRange const&)>, std::__1::allocator<std::__1::function<std::__1::shared_ptr<torch::jit::script::SugaredValue> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, torch::jit::script::Method&, torch::jit::SourceRange const&)> > > const&, std::__1::shared_ptr<torch::jit::script::SugaredValue>)::$_5>, void (torch::jit::script::Method&)>::operator()(torch::jit::script::Method&) + 111 (0x10c3d452f in libtorch.dylib)\r\nframe #8: torch::jit::script::Method::ensure_defined() + 175 (0x10c3f7fbf in libtorch.dylib)\r\nframe #9: torch::jit::script::defineMethodsInModule(torch::jit::script::Module&, std::__1::vector<torch::jit::script::Def, std::__1::allocator<torch::jit::script::Def> > const&, std::__1::vector<std::__1::function<std::__1::shared_ptr<torch::jit::script::SugaredValue> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, torch::jit::script::Method&, torch::jit::SourceRange const&)>, std::__1::allocator<std::__1::function<std::__1::shared_ptr<torch::jit::script::SugaredValue> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, torch::jit::script::Method&, torch::jit::SourceRange const&)> > > const&, std::__1::shared_ptr<torch::jit::script::SugaredValue>) + 2520 (0x10c3bfa28 in libtorch.dylib)\r\nframe #10: torch::jit::script::compileFunction(torch::jit::script::Def, std::__1::function<std::__1::shared_ptr<torch::jit::script::SugaredValue> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, torch::jit::script::Method&, torch::jit::SourceRange const&)> const&) + 620 (0x10c3c405c in libtorch.dylib)\r\nframe #11: void pybind11::cpp_function::initialize<torch::jit::script::initJitScriptBindings(_object*)::$_21, std::__1::shared_ptr<torch::jit::Graph>, torch::jit::script::Def const&, std::__1::function<pybind11::function (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >)>, pybind11::name, pybind11::scope, pybind11::sibling>(torch::jit::script::initJitScriptBindings(_object*)::$_21&&, std::__1::shared_ptr<torch::jit::Graph> (*)(torch::jit::script::Def const&, std::__1::function<pybind11::function (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >)>), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::\'lambda\'(pybind11::detail::function_call&)::__invoke(pybind11::detail::function_call&) + 408 (0x10937fe58 in _C.cpython-36m-darwin.so)\r\nframe #12: pybind11::cpp_function::dispatcher(_object*, _object*, _object*) + 3308 (0x108fc570c in _C.cpython-36m-darwin.so)\r\n<omitting python frames>\r\nframe #25: start + 1 (0x7fff67c8f085 in libdyld.dylib)\r\n']",[],0,0
398,pytorch,15764,closed,torch.argsort descends wrongly,"## üêõ Bug
given a large amount of numbers (say, a list containing 32768 numbers), I found that the argsort behaves wrongly when setting the descend=False.
## To Reproduce
I want to calculate the similarity of word embeddings, and I managed to achieve that by batched-similarity calculation. I expect the most similarity to an embedding should be itself, thus the highest rank should be 1,2... accordingly, and vice versa. 
now say we have torch.tensor object  (in shape batch_size * 32768), I want to extract top similarities along the last dimension.

the code gives the result of:

> get_top = torch.argsort(similarity, dim=1)
> [20485, 30807, 27706,  ...,   117,     6,     **1**],
>  [29628, 16835, 23989,  ...,  6773, 10377,     **2**],
>    ...,
>  [30120, 23853, 24914,  ...,   118,    50,    **47**],
>  [23528, 23663, 25283,  ...,    78,    77,    **48**],
>  [31738, 17062, 29731,  ...,  1240,    79,    **49**]

while:

> top8 = torch.argsort(similarity, dim=1, descending=True)
> tensor([[**21820**, 21822, 21816,  ..., 10909, 10910, 32725],
>         [**32497**, 28158, 20713,  ..., 14889, 12632, 16177],
>         [**17110**, 17460, 18861,  ..., 14628, 12113,    18],
>         ...,
>         [**22335**, 21440, 26360,  ..., 14870,  8292, 13842],
>         [**20379**, 16479, 28973,  ..., 11770, 11327, 13454],
>         [**23388**, 19492, 16577,  ...,  7874,  5219, 13230]])
the bold part implys the main problems. and I assume it's not simply because of the accuracy of float numebrs.
## Expected behavior
> top8 = torch.argsort(similarity, dim=1, descending=True)
> tensor([
>         [**1**,  blablabla ],
>         [**2**, blablabla],
>         ...,
>         [**47**, blablabla],
>         [**48**, blablabla],
>         [**49**, blablabla]])
<!-- A clear and concise description of what you expected to happen. -->

## Environment
 - PyTorch Version: 1.0 stable
 - OS (e.g., Linux): Linux
 - How you installed PyTorch (, , source): pip install
 - Python version: 3.6.3

",cherry-picked high priority,"[""Hmmm it's correct when I randomly initiate a tensor. Could you provide more context, or a simple example? \r\n\r\n```\r\nIn [12]: a = torch.randn(3, 4)\r\n\r\nIn [14]: a\r\nOut[14]:\r\ntensor([[ 1.4786,  1.2917,  1.0115,  0.1842],\r\n        [-0.5320,  0.2807, -0.0722,  0.6151],\r\n        [ 1.3300, -0.6290,  0.0619,  0.3336]])\r\n\r\nIn [16]: torch.argsort(a, dim=1)\r\nOut[16]:\r\ntensor([[3, 2, 1, 0],\r\n        [0, 2, 1, 3],\r\n        [1, 2, 3, 0]])\r\n\r\nIn [17]: torch.argsort(a, dim=1, descending=True)\r\nOut[17]:\r\ntensor([[0, 1, 2, 3],\r\n        [3, 1, 2, 0],\r\n        [0, 3, 2, 1]])\r\n```"", ""Does your tensor contain repeated entries? In yes, then that's probably the problem: I believe we don't guarantee which order repeated entries will get output in argsort"", ""@ailzhang  I think it's with the NaN value. because the PADing embedding is all zero, thus the 0 index is always NaN. does NaN need to be properly treated? I think the argsort() and topk() just manage this fine."", ""@fmassa no. btw, the multiple-entries thing does not explain why the top similarity index of a vec is not itself. \r\nI tried torch.tensor.topk(), it yields topk results just as the argsort() 's last k results (in the opposite order of course). problem seems to happen when you try to deal with NaN entries."", '@ailzhang \r\n[log.zip](https://github.com/pytorch/pytorch/files/2732960/log.zip)\r\n\r\n```\r\n      a = numpy.load(""log"")\r\n      print(a)\r\n      a=torch.tensor(a)\r\n      print (a.topk(8, dim=0)[1])\r\n      print(torch.argsort(a, dim=0).narrow(0,a.shape[0]-8,8))\r\n      print(torch.argsort(a, dim=0, descending=True).narrow(0,0,8))\r\n```\r\n\r\n> [        nan  0.07907724 -0.05215565 ..., -0.02669582  0.10268118\r\n>   0.0064341 ]\r\n> tensor([ 50, 118,  47,  13, 144,  17,  20, 328])\r\n> tensor([328,  20,  17, 144,  13,  47, 118,  50])\r\n> tensor([20929, 25714, 18326, 20942, 25476, 24726, 22254, 24916])', 'Okay I think the problem is caused by `NaN`. Here\'s a simpler repro:\r\n```\r\nIn [33]: a = torch.tensor([3, float(\'NaN\'), 2])\r\n\r\nIn [34]: torch.argsort(a, dim=0)\r\nOut[34]: tensor([0, 1, 2]) # wrong!\r\n\r\nIn [36]: a = torch.tensor([1, float(\'NaN\'), 2])\r\n\r\nIn [37]: torch.argsort(a, dim=0, descending=True)\r\nOut[37]: tensor([0, 1, 2]) # wrong!\r\n```\r\nSimilarly if we remove the leading `Nan` in the array you provided, the problem is gone.  \r\n```\r\nIn [38]: a = numpy.load(""log"")\r\n\r\nIn [39]: a = torch.tensor(a)\r\n\r\nIn [40]: b = a[1:]\r\n\r\nIn [41]: b\r\nOut[41]: tensor([ 0.0791, -0.0522,  0.0226,  ..., -0.0267,  0.1027,  0.0064])\r\n\r\nIn [49]: print(torch.argsort(b, dim=0, descending=True).narrow(0,0,8))\r\ntensor([ 49, 117,  46,  12, 143,  16,  19, 327])\r\n\r\nIn [50]: print(torch.argsort(b, dim=0).narrow(0,b.shape[0]-8,8))\r\ntensor([327,  19,  16, 143,  12,  46, 117,  49])\r\n```\r\nActually the same problem exists for `torch.sort` as well, that `nan` breaks the monotonic increasing in the middle, but it was hidden in the printed sequence. \r\n```\r\nIn [51]: torch.sort(a)\r\nOut[51]:\r\n(tensor([-0.2427, -0.2274, -0.2272,  ...,  0.5829,  0.7206,  1.0000]),\r\n tensor([26606, 25901, 32186,  ...,    47,   118,    50]))\r\nIn [57]: torch.sort(a)[0][16360:16365]\r\nOut[57]: tensor([ 0.2460,  0.2769,     nan, -0.2238, -0.2159])\r\n```', 'FYI numpy handles this case by always putting `nan` at the end the sorted sequence.  We should probably follow numpy behavior. \r\n\r\n```\r\nIn [14]: a\r\nOut[14]:\r\narray([        nan,         inf,        -inf, ..., -0.02669582,\r\n        0.10268118,  0.0064341 ], dtype=float32)\r\n\r\nIn [15]: numpy.sort(a)\r\nOut[15]:\r\narray([       -inf, -0.24265206, -0.22741349, ...,  1.        ,\r\n               inf,         nan], dtype=float32)\r\n\r\nIn [16]: numpy.argsort(a)\r\nOut[16]: array([    2, 26606, 25901, ...,    50,     1,     0])\r\n```\r\n\r\nLooks like we only handle `inf` correctly, but not `ninf` and `nan`. \r\n```\r\nIn [19]: b = torch.tensor(a)\r\n\r\nIn [20]: torch.sort(b)\r\nOut[20]:\r\n(tensor([-0.2427, -0.2274, -0.2272,  ...,  0.7206,  1.0000,     inf]),\r\n tensor([26606, 25901, 32186,  ...,   118,    50,     1]))\r\n```', 'cc: @umanwizard ', 'Taking a look', ""FWIW, PyTorch matches vanilla python behavior (which is that sorting with NaNs produces an undefined result).\r\n```\r\n>>> sorted([3, float('NaN'), 2])\r\n[3, nan, 2]\r\n```""]","['\r\nget_top = torch.argsort(similarity, dim=1)\r\nget_top = torch.argsort(similarity, dim=1, descending=True) \r\n']","['similarity', 'conda', 'pip']",0,0
399,pytorch,3573,closed,Mac OS X build with Python 3.6 fails,"One representative error:



The cause of the problem is aa911939a328eff55c9b28b39ed3c43507ba8a2a:



It seems that on clang, changing the type parameter here is sufficient to cause template instantiation to fail.

Maybe the easiest way to fix this is to write a more portable version of PyInt_FromLong (and friends) which always returns .",,[],"['\r\ntorch/csrc/autograd/functions/init.cpp:220:37: error: address of overloaded function \'getTupleAttr\' does not match required type \'_object *(_object *, void *)\'\r\n  {(char*)""output_padding"", (getter)getTupleAttr<ConvBackwardBackward, std::vector<int>, ConvParams,\r\n                                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntorch/csrc/autograd/functions/init.cpp:82:11: note: candidate template ignored: invalid explicitly-specified argument for template parameter \'Convert\'\r\nPyObject* getTupleAttr(PyObject* obj, void* _unused)\r\n', '\r\n   {(char*)""output_padding"", (getter)getTupleAttr<ConvForward, std::vector<int>, ConvParams,\r\n-                                         &ConvParams::output_padding, long, PyInt_FromLong>, NULL, NULL, NULL},\r\n+                                         &ConvParams::output_padding, int64_t, PyInt_FromLong>, NULL, NULL, NULL},\r\n']",['int64_t'],0,0
400,pytorch,27769,closed,Nvidia cuda documentation link renders wrong,"Some of our docs link to ""CUDA documentation"" render incorrectly:

![image](https://user-images.githubusercontent.com/5652049/66680833-414d2d00-ec26-11e9-8347-5da72167ccb5.png)


cc @ezyang @gchanan @zou3519",high priority module: docs triage review,"[""Seems high priority given it's super easy to fix."", 'Fixed by https://github.com/pytorch/pytorch/pull/27782']",[],[],0,0
401,pytorch,20675,closed,Windows 10 CUDA 9 CUDNN 7.5 Pytorch 1.1 CUDNN_STATUS_EXECUTION_FAILED,"## ‚ùì Questions and Help

### Please note that this issue tracker is not a help form and this issue will be closed.

We have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:

- [Discussion Forum](https://discuss.pytorch.org/)
Firstly I tested tensorflow-gpu 1.12, it works on CUDA GPU well. 
Then I followed pytorch tutorial to setup a simple CIFAR10 net, it works well on cpu. I tested cuda&cudnn is_available, both of them return True, however the gpu-enabled CIFAR10 net failed on gpu with exception CUDNN_STATUS_EXECUTION_FAILED.

Inputs is cudnn acceptable:  True
Traceback (most recent call last):
  File ""D:/Python/pytorch/tutorial/tut_4_classifier_gpu.py"", line 97, in <module>
    outputs = net(inputs)
  File ""D:\Python\Python36\lib\site-packages\torch\nn\modules\module.py"", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File ""D:/Python/pytorch/tutorial/tut_4_classifier_gpu.py"", line 62, in forward
    x = self.pool(F.relu(self.conv1(x)))
  File ""D:\Python\Python36\lib\site-packages\torch\nn\modules\module.py"", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File ""D:\Python\Python36\lib\site-packages\torch\nn\modules\conv.py"", line 320, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED
",module: windows,"['I am sure I installed pytorch 1.1 with compitable version of cuda by pip3 install *.whl', ""Some essential information (like code, something about your env) is missing so it is hard to say what's wrong there. BTW, please use our [Discussion Forum](https://discuss.pytorch.org/) for user questions."", 'My gpu card is RTX 2070. And the issue was solved when I upgraded my CUDA from 9.0 to 10.0. Thanks.']",[],[],0,0
402,pytorch,4896,closed,Second order derivative in neural network w.r.t. input encounter all zero,"









the values of y somehow are all zero
while in none-neural network case







the values of y are all normal
How to fix this? ",,"['There is no ""fix"", zero is the correct answer in this case. In here, gradient of the loss with respect to the input doesn\'t depend on the value of that input, so it is zero.', 'We found that actually vgg16 is a linear model, so its second order derivatives are all zero. if we modified the network to resnet18 this will work. somehow stupid question, anyway thank you for the time.']",[],"['import torch', 'from torch.autograd import grad,Variable', 'input = Variable(torch.rand(1,3,224,224), requires_grad=True).cuda()', 'w = Variable(torch.rand(1,802816)).cuda()', 'net = vgg16_withoutfc().cuda()', 'output = net(input)', 'jacob = grad(output, input, grad_outputs=w, create_graph=True)[0]', 'y = grad(jacob.sum(), input, create_graph=True)[0]', 'x = Variable(torch.rand(2,2), requires_grad=True).cuda()', 'z = Variable(torch.rand(2,2)).cuda()', 'n = x**2', 'jacob = grad(n, x, grad_outputs=z, create_graph=True)[0]', 'y = grad(jacob.sum(), x, create_graph=True)[0]']",0,0
403,pytorch,26081,closed,RNN fails to compile due to USE_FBGEMM=OFF,"## üêõ Bug

Compiling libtorch without FBGEMM causes a compile error in RNN.

> /d/dev/pytorch/aten/src/ATen/native/RNN.cpp:286:57: error: ‚ÄòPackedLinearWeight‚Äô was not declared in this scope

The struct exists in aten/src/ATen/native/quantized/cpu/fbgemm_utils.h

Workaround was to #ifdef certain sections of code. See diff:


## To Reproduce

Steps to reproduce the behavior:

1. cmake -DUSE_FBGEMM=OFF ..
2. Compile error

## Expected behavior

Compiles

## Environment

 - PyTorch Version (e.g., 1.0): master
 - OS (e.g., Linux): Linux x64
 - How you installed PyTorch (, , source): source
 - Build command you used (if compiling from source): cmake -DUSE_FBGEMM=OFF ..
 - Python version: 3.6
 - CUDA/cuDNN version: N/A
 - GPU models and configuration: N/A
 - Any other relevant information: N/A

cc @jerryzh168 @jianyuh @dzhulgakov @zou3519",module: rnn oncall: quantization,"[""Thanks for reporting this issue! This is due to the recent PR: https://github.com/pytorch/pytorch/pull/25877. We have seen this issue (https://github.com/pytorch/pytorch/pull/25877#discussion_r323457658) and the PR is updated with `#ifdef USE_FBGEMM`, but not sure why an older version of PR is merged.\r\n\r\nPlease rebase to the latest master branch and there shouldn't be such build error any longer. https://github.com/pytorch/pytorch/pull/26079 reverted the above PR and a new PR (#26084) is going to be merged."", 'Closing as https://github.com/pytorch/pytorch/pull/26084 has landed.']","['Diff\r\ndiff --git a/aten/src/ATen/native/RNN.cpp b/aten/src/ATen/native/RNN.cpp\r\nindex b9913888ed..774b6fcc74 100644\r\n--- a/aten/src/ATen/native/RNN.cpp\r\n+++ b/aten/src/ATen/native/RNN.cpp\r\n@@ -275,6 +275,7 @@ static std::vector<QuantizedCellParams> gather_quantized_params(TensorList param\r\n   return result;\r\n }\r\n \r\n+#ifdef USE_FBGEMM\r\n static std::vector<QuantizedCellParamsDynamic> gather_quantized_params_dynamic(\r\n     TensorList params) {\r\n   static at::Tensor undefined;\r\n@@ -291,6 +292,7 @@ static std::vector<QuantizedCellParamsDynamic> gather_quantized_params_dynamic(\r\n   }\r\n   return result;\r\n }\r\n+#endif\r\n \r\n static std::vector<QuantizedCellParamsFP16> gather_quantized_params_fp16(\r\n     TensorList params) {\r\n@@ -1043,6 +1045,7 @@ Tensor rnn_relu_cell(\r\n   return SimpleCell<relu_f, CellParams>{}(input, hx, CellParams{w_ih, w_hh, b_ih, b_hh});\r\n }\r\n \r\n+#ifdef USE_FBGEMM\r\n // Quantized implementations\r\n //\r\n // These implementations use FBGEMM to do the i2h and h2h linear layers with\r\n@@ -1094,6 +1097,7 @@ std::tuple<Tensor, Tensor, Tensor> quantized_lstm(\r\n   return results;\r\n \r\n }\r\n+#endif\r\n \r\n #define DEFINE_QUANTIZED_RNN_CELL(name, hx_type, cell_type, return_type, prepare_hx_fn) \\\r\n return_type name( \\\r\n\r\n']","['conda', 'pip']",0,0
404,pytorch,17163,closed,Support torch.zeros with HalfTensor (half),"

This might be nontrivial to do because zeros is implemented in terms of vectorized fill on CPU, so you'd have to work out how to make that code work for half.",,"[""But... zero is just vector fill of all zero bits! So maybe with some casting trick this isn't that hard."", ""I remember that the reason why this isn't implemented was because we didn't want to implement halftensor operations on CPU tensors, but I don't remember why that is"", ""In general it's hard to implement everything in half on CPU; especially doing so efficiently. It's mostly a question of slowly expanding capabilities as makes sense."", 'I believe this has been addressed by https://github.com/pytorch/pytorch/commit/e0b44cac1feb55e4e5386fa35c223bbd8af3bb82.']","['\r\n>>> import torch\r\n>>> torch.zeros(2, dtype=torch.half)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\nRuntimeError: _th_zero_ is not implemented for type torch.HalfTensor\r\n']",[],0,0
405,pytorch,11989,closed,RuntimeError: CUDA error: unknown error,"## Issue description
I upgrade CUDA from 8.0 to 9.0, and reinstall cudnn and pytorch.
Then following error happen:
RuntimeError: CUDA error: unknown error

## Code example


Traceback (most recent call last):
  File ""example.py"", line 3, in <module>
    x = torch.tensor([1., 2.], device=device)
RuntimeError: CUDA error: unknown error

## System Info
Collecting environment information...
PyTorch version: 0.4.1
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 16.04.5 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.5
Is CUDA available: Yes
CUDA runtime version: 9.0.176
GPU models and configuration: GPU 0: TITAN Xp
Nvidia driver version: 384.130
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.3.0
/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a

Versions of relevant libraries:
[pip] Could not collect
[conda] Could not collect
",,['Somehow the problem can be fixed following this advice:\r\n[https://github.com/pjreddie/darknet/issues/98#issuecomment-348441285](https://github.com/pjreddie/darknet/issues/98#issuecomment-348441285)'],"[""\r\nimport torch\r\ndevice = torch.device('cuda')\r\nx = torch.tensor([1., 2.], device=device)\r\n""]",[],0,0
406,pytorch,24718,closed,Migrate `l1_loss_backward` from the TH to Aten (CPU),"Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review.",better-engineering module: porting triaged,[],[],[],0,0
407,pytorch,3001,closed,Trying to do advanced indexing using a Variable causes a hang,"**Minimal test case:**

    import torch
    x = torch.autograd.Variable(torch.FloatTensor([[0.1, 0.2], [0.3, 0.4]]))
    idx = torch.autograd.Variable(torch.LongTensor([1, 0]))
    print(x[idx, :])

**Expected output:**

Either:

    Variable containing:
     0.3000  0.4000
     0.1000  0.2000
    [torch.FloatTensor of size 2x2]

(the above is what is printed if I replace the last line with )

...or an immediate exception traceback informing me that advanced indices must be a  and not a .

**Actual output:**

The program initially hangs. *After* I interrupt the program with ^C, the following exception is displayed:

    Traceback (most recent call last):
      File ""loop.py"", line 5, in <module>
        print(x[idx, :])
      File ""/home/.../lib/python3.5/site-packages/torch/autograd/variable.py"", line 76, in __getitem__
        return Index.apply(self, key)
      File ""/home/.../lib/python3.5/site-packages/torch/autograd/_functions/tensor.py"", line 16, in forward
        result = i.index(ctx.index)
    IndexError: When performing advanced indexing the indexing objects must be LongTensors or convertible to LongTensors

**More details:**

From some digging around in pdb, it looked like [](https://github.com/pytorch/pytorch/blob/master/torch/autograd/variable.py#L78) was tail-calling itself via C code. While I couldn't identify the full stack, it looks like it eventually made its way to [this attempt to convert the  index to a ](https://github.com/pytorch/pytorch/blob/master/torch/csrc/generic/Tensor.cpp#L785). Sure enough, this also hangs:

    torch.LongTensor(idx)

Perhaps the  constructor is trying to treat the  as a iterable, and getting stuck because iterating through a  returns a sequence of ? (Happy to dig further, if someone can point me to the logic for initializing a .)

Python 3.5.2, GCC 5.4.0 on Linux,  = '0.2.0_3', no CUDA",high priority,"['Hi @futurulus - would it be possible for you to try this on `master`? I just built a clean copy of PyTorch and the code snippet succeeds for me. You may not have a version of PyTorch that includes https://github.com/pytorch/pytorch/pull/2590, which addresses this issue.', 'Ah, it works on `master` for me too. Awesome.\r\n\r\nThis:\r\n\r\n    torch.LongTensor(idx)\r\n\r\nstill spins for a while but eventually comes back with\r\n\r\n    Traceback (most recent call last):\r\n      File ""<stdin>"", line 1, in <module>\r\n    RuntimeError: already counted a million dimensions in a given sequence. Most likely your items are also sequences and there\'s no way to infer how many dimension should the tensor have\r\n\r\nIt seems natural to me that a Tensor constructor should know to reach into `.data` if it sees a Variable, but perhaps there are good reasons for it not to. In any case, there\'s an easy workaround (just use `idx.data` instead of `idx`) and a googlable error message.\r\n\r\nThanks for the quick reply!', '@futurulus - just to be clear, you are talking about trying to create a `LongTensor` from `idx` which is a `LongTensor` wrapped in a `Variable`?', 'Yes, exactly.\r\n\r\nMore realistically, I might not know whether `idx` is a list, numpy array, `LongTensor`, or [edit: `LongTensor` wrapped in a] `Variable`, but I need to create a `LongTensor` from it regardless. `torch.LongTensor(idx)` seems like an idiomatic way to do that, but currently if I expect the input might be a `Variable`, I need a special case.', ""@futurulus I see. With #2633 coming in the future I think it doesn't merit adding some special workaround for now, given that handling it in your code should be easy enough. However, we should not have a hang when passing unexpected inputs to the `LongTensor` constructor. I will take a look."", 'fixed via #3163 ']",[],"['print(x[idx])', 'Tensor', 'Variable', 'Variable.__getitem__', 'Variable', 'LongTensor', 'LongTensor', 'Variable', 'Variable', 'Variables', 'LongTensor', 'torch.__version__']",0,0
408,pytorch,15544,closed,python api ok but c++ api bad,"## ‚ùì Questions and Help

### Please note that this issue tracker is not a help form and this issue will be closed.

We have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:

- [Discussion Forum](https://discuss.pytorch.org/)

i build pytorch v1 source code in a docker container without gpu, but has cuda.
commit and save to a docker image.
run this docker image on another machine with GPU, occur below:
 pytorch python api is ok:
![image](https://user-images.githubusercontent.com/17922949/50450451-20526e80-0969-11e9-8158-029ae2700ff5.png)
but pytorch c++ api is bad:
![image](https://user-images.githubusercontent.com/17922949/50450462-32cca800-0969-11e9-9701-fea278df8f1f.png)


gcc version : 4.8.4
cuda : 9.0.176
nvidia driver : 384.81
cudnn : 7.0.3

why ? hope to get help!

  


",module: cpp,"['how to solve?\r\n\r\ninitialize error:CUDA error (35): CUDA driver version is insufficient for CUDA runtime version.\r\n\r\nbut my cuda version is the same.', 'fix it by lib linking']",[],[],0,0
409,pytorch,8282,closed,Loaded network with load_state_dict has different shape but works anyway,"After it was verified on [discuss.pytorch](https://discuss.pytorch.org/t/loaded-network-has-different-shape-but-works-anyway/19398) that this is indeed unwanted behaviour, I am forwarding this to you:

## Issue description

I trained a model with among others had the following layer:

and then saved it to a file with state_dict and torch.save.

Then, when I wanted to load that model using load_state_dict, by accident the same layer was setup as follows:


Nevertheless, the model was loaded without error. It seemed that the weights were just duplicated 32 times, but I have not verified. So the question is how this is consistent with API documentation. I have not found a statement that says load_state_dict would somehow fix shape inconsistencies automatically. It seems you have a documentation vs reality mismatch here. (Now you need to decide which one to fix)

## Code example



Provided by discuss .pytorch user ptrblck

## System Info
pytorch 0.4 release

",todo,"['we should check for exact shape match at https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/module.py#L636', ""We'd like to have an option (e.g., when using `load_state_dict(strict=False)`) to disable this check (and return the messages like it's already doing).\r\nLoading a state-dict with a few mismatched values is common in tasks like transfer learning.\r\n\r\nRight now, `strict=False` does the following: it allows loading a dict with missing parameters. It allows loading a dict with more parameters than needed. Why not also allow loading a dict with parameters of mismatched shape?"", '@ppwwyyxx Could you open a new issue for this? I agree that `strict=False` should do what you described. But generally, for state dict manipulation, I would suggest changing the state dict that is being loaded, rather than using `strict=False` because the later almost always disables more checks than you would like to.']","[""\r\npath = './test_model.pth'\r\n\r\nmodel = nn.Conv2d(64, 1, 3, 1, 1)\r\ntorch.save(model.state_dict(), path)\r\n\r\nmodel = nn.Conv2d(64, 32, 3, 1, 1)\r\nmodel.load_state_dict(torch.load(path))\r\n\r\nfor w in model.weight[:, :, 0, 0]:\r\n    print(w)\r\n""]","['final_layer.append(nn.Conv2d(64, 1, kernel_size=1))', 'final_layer.append(nn.Conv2d(64, 32, kernel_size=1))']",0,0
410,pytorch,20296,closed,the capacity of memory when compiling pytorch from source,"how large the capacity of memory is needed when compiling pytorch from source ?

I've got about 5GB available when compiling in ubuntu 18.04.  Each time I compile with **python setup.py install**, the compiling program will occupy all the capacity of memory. Then my screen is stuck. After a while,  the screen becomes dark and my laptop reboots.... 

Is there any way to setup some parameters to use less memory when compiling ? Many thanks.",module: build triaged,"['One thing to try is reduce the parallelism of the build. This can be done by setting `MAX_JOBS=1`, for example.', 'Sorry.  MAX_JOBS=1 doesn\'t work out. I\'ve set ""export MAX_JOBS=1"" in shell, before typing ""python setup.py install"".  Still, out of memoy. \r\nI\'ve even changed all the ""MAX_JOBS"" parameters in the project to 1. However, it doesn\'t work out.  \r\n\r\nAnd **I\'ve found that there were many ""cicc"" and ""cc1plus"" commands when I used top command in shell**. So I think ""MAX_JOBS"" cannot reduce the number of threads used to build the project.', ""Hmm. Can you post your compile log? Here's the logic which handles `MAX_JOBS`\r\n\r\n```\r\ndef build_caffe2(version,\r\n                 cmake_python_library,\r\n                 build_python,\r\n                 rerun_cmake,\r\n                 build_dir):\r\n    my_env = create_build_env()\r\n    build_test = not check_negative_env_flag('BUILD_TEST')\r\n    max_jobs = os.getenv('MAX_JOBS', None)\r\n    cmake_cache_file = 'build/CMakeCache.txt'\r\n    if rerun_cmake and os.path.isfile(cmake_cache_file):\r\n        os.remove(cmake_cache_file)\r\n    if not os.path.exists(cmake_cache_file) or (USE_NINJA and not os.path.exists('build/build.ninja')):\r\n        run_cmake(version,\r\n                  cmake_python_library,\r\n                  build_python,\r\n                  build_test,\r\n                  build_dir,\r\n                  my_env)\r\n    if IS_WINDOWS:\r\n        build_cmd = ['cmake', '--build', '.', '--target', 'install', '--config', build_type, '--']\r\n        if USE_NINJA:\r\n            # sccache will fail if all cores are used for compiling\r\n            j = max(1, multiprocessing.cpu_count() - 1)\r\n            if max_jobs is not None:\r\n                j = min(int(max_jobs), j)\r\n            build_cmd += ['-j', str(j)]\r\n            check_call(build_cmd, cwd=build_dir, env=my_env)\r\n        else:\r\n            j = max_jobs or str(multiprocessing.cpu_count())\r\n            build_cmd += ['/maxcpucount:{}'.format(j)]\r\n            check_call(build_cmd, cwd=build_dir, env=my_env)\r\n    else:\r\n        if USE_NINJA:\r\n            ninja_cmd = ['ninja', 'install']\r\n            if max_jobs is not None:\r\n                ninja_cmd += ['-j', max_jobs]\r\n            check_call(ninja_cmd, cwd=build_dir, env=my_env)\r\n        else:\r\n            max_jobs = max_jobs or str(multiprocessing.cpu_count())\r\n            check_call(['make', '-j', str(max_jobs), 'install'], cwd=build_dir, env=my_env)\r\n```\r\n\r\nSo what I am expecting to see is a call to ninja/make with `-j 1`.""]",[],[],0,0
411,pytorch,3882,closed, Leaking dataloader,"I‚Äôve met a strange memory leak when I tried to implement ‚ÄúImproved Training of Wasserstein GANs‚Äù. I‚Äôm getting OOM in the middle of second epoch both on CPU and GPU. Memory usage seems to increase after each batch, the profiling of CPU version points on the for loop over dataloader. Here is the kinda minimal example:



From the discussion on https://discuss.pytorch.org/t/leaking-dataloader/10418 I've found out that the code runs fine on  torch 0.2.0_3, while I'm using https://github.com/pytorch/pytorch/tree/8ebf18b5b1d57ef16c24366649e720867f394a98 built from source, with CUDA 8 and cudnn7, so I supposed it should be some recently introduced bug",,"['There was a memory leak (https://github.com/pytorch/pytorch/issues/3818) bug in the master branch last week regarding convolutions. You may have hit the same one. 0.3 branch works OK.\r\n', 'This is a known memory leak on double backward. It is not related to dataloader. The issue is tracked at #3835 and #3824 .\r\n\r\n0.3 branch works fine.', 'fixed with https://github.com/pytorch/pytorch/pull/3970']","['python\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nimport os\r\nimport random\r\n\r\nimport torch\r\nimport torch.backends.cudnn as cudnn\r\nimport torch.nn.parallel\r\nimport torch.optim as optim\r\nimport torch.utils.data\r\nimport torchvision.datasets as dset\r\nimport torchvision.transforms as transforms\r\nfrom torch import autograd\r\nfrom torch import nn\r\nfrom torch.autograd import Variable\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\'--dataroot\', required=True, help=\'path to dataset\')\r\nparser.add_argument(\'--workers\', type=int, help=\'number of data loading workers\', default=4)\r\nparser.add_argument(\'--batchSize\', type=int, default=128, help=\'input batch size\')\r\nparser.add_argument(\'--imageSize\', type=int, default=32, help=\'the height / width of the input image to network\')\r\nparser.add_argument(\'--nz\', type=int, default=128, help=\'size of the latent z vector\')\r\nparser.add_argument(\'--num_gen_filters\', type=int, default=32, help=\'# of gen filters in first conv layer\')  # 64\r\nparser.add_argument(\'--num_disc_filters\', type=int, default=32, help=\'# of discrim filters in first conv layer\')\r\nparser.add_argument(\'--niter\', type=int, default=25, help=\'number of epochs to train for\')\r\nparser.add_argument(\'--lr\', type=float, default=1e-4, help=\'learning rate, default=0.0002\')\r\nparser.add_argument(\'--beta1\', type=float, default=0.5, help=\'beta1 for adam. default=0.5\')\r\nparser.add_argument(\'--cuda\', action=\'store_true\', default=False, help=\'enables cuda\')\r\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'number of GPUs to use\')\r\nparser.add_argument(\'--netG\', default=\'\', help=""path to netG (to continue training)"")\r\nparser.add_argument(\'--netD\', default=\'\', help=""path to netD (to continue training)"")\r\nparser.add_argument(\'--outf\', default=\'./result\', help=\'folder to output images and model checkpoints\')\r\nparser.add_argument(\'--manualSeed\', type=int, default=1, help=\'manual seed\')\r\n\r\nopt = parser.parse_args()\r\ntorch.backends.cudnn.benchmark = True\r\n\r\n\r\nclass Generator(nn.Module):\r\n    def __init__(self, num_gen_filters, nz=128, num_channels=3):\r\n        super(Generator, self).__init__()\r\n        self.num_gen_filters = num_gen_filters\r\n        self.preprocess = nn.Sequential(\r\n            nn.Linear(nz, 4 * 4 * 4 * num_gen_filters),\r\n            nn.BatchNorm2d(4 * 4 * 4 * num_gen_filters),\r\n            nn.ReLU(True),\r\n        )\r\n        self.block1 = nn.Sequential(\r\n            nn.ConvTranspose2d(4 * num_gen_filters, 2 * num_gen_filters, 2, stride=2),\r\n            nn.BatchNorm2d(2 * num_gen_filters),\r\n            nn.ReLU(True),\r\n        )\r\n        self.block2 = nn.Sequential(\r\n            nn.ConvTranspose2d(2 * num_gen_filters, num_gen_filters, 2, stride=2),\r\n            nn.BatchNorm2d(num_gen_filters),\r\n            nn.ReLU(True),\r\n        )\r\n        self.deconv_out = nn.ConvTranspose2d(num_gen_filters, num_channels, 2, stride=2)\r\n        self.tanh = nn.Tanh()\r\n\r\n    def forward(self, input):\r\n        output = self.preprocess(input)\r\n        output = output.view(-1, 4 * self.num_gen_filters, 4, 4)\r\n        output = self.block1(output)\r\n        output = self.block2(output)\r\n        output = self.deconv_out(output)\r\n        output = self.tanh(output)\r\n        return output.view(-1, 3, 32, 32)\r\n\r\n\r\nclass Discriminator(nn.Module):\r\n    def __init__(self, num_disc_filters, num_channels=3):\r\n        super(Discriminator, self).__init__()\r\n        self.num_disc_filters = num_disc_filters\r\n        self.main = nn.Sequential(\r\n            nn.Conv2d(num_channels, num_disc_filters, 3, 2, padding=1),\r\n            nn.LeakyReLU(),\r\n            nn.Conv2d(num_disc_filters, 2 * num_disc_filters, 3, 2, padding=1),\r\n            nn.LeakyReLU(),\r\n            nn.Conv2d(2 * num_disc_filters, 4 * num_disc_filters, 3, 2, padding=1),\r\n            nn.LeakyReLU(),\r\n        )\r\n\r\n        self.linear = nn.Linear(4 * 4 * 4 * num_disc_filters, 1)\r\n\r\n    def forward(self, input):\r\n        output = self.main(input)\r\n        output = output.view(-1, 4 * 4 * 4 * self.num_disc_filters)\r\n        output = self.linear(output)\r\n        return output\r\n\r\n\r\ndef main():\r\n    print(opt)\r\n\r\n    nz = opt.nz\r\n\r\n    try:\r\n        os.makedirs(opt.outf)\r\n    except OSError:\r\n        pass\r\n\r\n    if opt.manualSeed is None:\r\n        opt.manualSeed = random.randint(1, 10000)\r\n    print(""Random Seed: "", opt.manualSeed)\r\n    random.seed(opt.manualSeed)\r\n    torch.manual_seed(opt.manualSeed)\r\n    if opt.cuda:\r\n        torch.cuda.manual_seed_all(opt.manualSeed)\r\n\r\n    cudnn.benchmark = True\r\n\r\n    if torch.cuda.is_available() and not opt.cuda:\r\n        print(""WARNING: You have a CUDA device, so you should probably run with --cuda"")\r\n\r\n    dataset = dset.CIFAR10(root=opt.dataroot, download=True, transform=transforms.Compose(\r\n        [transforms.Resize(opt.imageSize), transforms.ToTensor(),\r\n         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ]))\r\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=opt.batchSize,\r\n                                             shuffle=True, num_workers=int(opt.workers))\r\n\r\n    netG = Generator(opt.num_gen_filters, nz)\r\n    print(netG)\r\n\r\n    netD = Discriminator(opt.num_disc_filters)\r\n    print(netD)\r\n\r\n    noise = torch.FloatTensor(opt.batchSize, nz, 1, 1)\r\n    fixed_noise = torch.FloatTensor(opt.batchSize, nz).normal_(0, 1)\r\n\r\n    if opt.cuda:\r\n        netD.cuda()\r\n        netG.cuda()\r\n        noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\r\n\r\n    # setup optimizer\r\n    optimizerD = optim.Adam(netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.9))\r\n\r\n    for epoch in range(opt.niter):\r\n        for _, data in enumerate(dataloader, 0):\r\n            for p in netG.parameters():  # reset requires_grad\r\n                p.requires_grad = False  # they are set to False below in netG update\r\n\r\n            # train with real\r\n            netD.zero_grad()\r\n            real_cpu, _ = data\r\n            batch_size = real_cpu.size(0)\r\n            if opt.cuda:\r\n                real_cpu = real_cpu.cuda()\r\n\r\n            noise.resize_(batch_size, nz).normal_(0, 1)\r\n            noisev = Variable(noise, volatile=True)\r\n            fake = Variable(netG(noisev).data)\r\n\r\n            interpolates = 0.5 * real_cpu + 0.5 * fake.data\r\n\r\n            if opt.cuda:\r\n                interpolates = interpolates.cuda()\r\n            interpolates = autograd.Variable(interpolates, requires_grad=True)\r\n\r\n            disc_interpolates = netD(interpolates)\r\n\r\n            gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\r\n                                      grad_outputs=torch.ones(\r\n                                          disc_interpolates.size()).cuda() if opt.cuda else torch.ones(\r\n                                          disc_interpolates.size()), create_graph=True, only_inputs=True)[0]\r\n\r\n            gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * 10\r\n\r\n            gradient_penalty.backward()\r\n            optimizerD.step()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n']",[],0,0
412,pytorch,15328,closed,"torch.gesv forward handles singleton dimension, but backward doesn't","## üêõ Bug

I'm running in to an issue where  accepts a  right hand side (with an  left hand side) in the forward pass, but the backward pass will fail. Unsqueezing the right hand side to  and then squeezing the output gets me the same size output (), but the backward pass also works.

## To Reproduce



## Expected behavior

Either (a) torch.gesv should require  to have the trailing singleton dimension in the forward pass (e.g. be  or (b) the backward pass should work if  doesn't have the singleton dimension.

## Environment

Reproducible on  installed via conda.
",,"['I am siding with torch.gesv requiring at least two dimensions. Another alternative would be to reshape the output to be of the same shape as the RHS tensor, but this needs to be done in TH/THC.', 'Funnily, 1-dim tensors for the RHS don‚Äôt work on CUDA. I have fixed this asymmetry locally by relaxing a check in TH, and this should resolve this issue. I‚Äôll send in a PR shortly.']","[""python\r\nimport torch\r\n\r\nA = torch.randn(5, 5, requires_grad=True)\r\nb = torch.randn(5, requires_grad=True)\r\n\r\nsolve = torch.gesv(b.unsqueeze(-1), A)[0].squeeze(-1) # Works\r\nsolve.sum().backward() # Works\r\n\r\nsolve2 = torch.gesv(b, A)[0] # Works\r\nsolve2.sum().backward() # Doesn't work\r\n"", 'python\r\nTraceback (most recent call last):\r\n  File ""gesv_bug.py"", line 10, in <module>\r\n    solve2.sum().backward() # Doesn\'t work\r\n  File ""/home/jrg365/anaconda3/lib/python3.6/site-packages/torch/tensor.py"", line 102, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File ""/home/jrg365/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py"", line 90, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: Function GesvBackward returned an invalid gradient at index 0 - got [5, 1] but expected shape compatible with [5]\r\n']","['torch.gesv', 'torch.Size([n])', 'n x n', 'torch.Size([n, 1])', 'torch.Size([n])', 'b', 'torch.Size([5, 1])', 'b', 'pytorch-1.0.0']",0,0
413,pytorch,5712,closed,Multiple versions of Dockerfiles fail to build,"PyTorch GitHub Issues Guidelines
--------------------------------

- OS: Mac
- PyTorch version: master, v0.3.1, v0.3.0, v0.2.0, v0.1.12
- How you installed PyTorch (conda, pip, source):
- Python version: N/A
- CUDA/cuDNN version: N/A
- GPU models and configuration: N/A
- GCC version (if compiling from source): N/A

Version 0.3.1 below:

This produces this:

UPDATE:
Same goes for v0.3.0

UPDATE:
Running  fails with below

UPDATE:
Running 0.1.12 gives this, similar to latest versions but different package:

UPDATE:
Running  gives this:
",,"[""Possibly this has to do with my Mac setup? I'm going to stop trying other versions now"", '@s1113950 You want me try building it?', 'Go for it @ashahba !', ""@ashahba could be due to the base image being `nvidia/cuda` not working on my machine. I have a gpu but it's not nvidia"", ""It's possible, but I'll find out shortly after I'm out of meeting with my team."", '@s1113950 your setup should be fine the only thing is you may need to pass `--build-arg NO_PROXY=$NO_PROXY`  and `--build-arg no_proxy=$no_proxy` info as well.\r\nI was able to build both `master` and also `v0.3.1` behind corporate proxy although it took a bit long.\r\n\r\n```\r\nSuccessfully installed pillow-5.0.0 torchvision-0.2.0\r\nCleaning up...\r\n ---> 63dce00d2bad\r\nRemoving intermediate container 02743c0ec397\r\nStep 13 : WORKDIR /workspace\r\n ---> Running in f1e33c97eaa3\r\n ---> 0ffa74b39fa5\r\nRemoving intermediate container f1e33c97eaa3\r\nStep 14 : RUN chmod -R a+w /workspace\r\n ---> Running in 32c626887f44\r\n ---> 3e6422c72b8f\r\nRemoving intermediate container 32c626887f44\r\nSuccessfully built 3e6422c72b8f\r\n$ docker images\r\nREPOSITORY          TAG                            IMAGE ID            CREATED              SIZE\r\npytorch             0.3.1                          3e6422c72b8f        About a minute ago   4.416 GB\r\n<none>              <none>                         85cab9281f05        28 minutes ago       3.648 GB\r\npytorch             latest                         074671e67a8e        43 minutes ago       5.174 GB\r\nnvidia/cuda         9.0-cudnn7-devel-ubuntu16.04   3ada27a3ae03        3 days ago           2.864 GB\r\nnvidia/cuda         8.0-cudnn6-devel-ubuntu16.04   547cf50ecba4        3 days ago           1.97 GB\r\n```\r\n\r\nAlso since for `v1.3.0` and possibly older versions `/opt/conda/bin` is not added to `PATH` calls to `conda` fail. So in order to build `v0.3.1` I had to make a small change:\r\n```\r\n$ git diff\r\ndiff --git a/Dockerfile b/Dockerfile\r\nindex cba8b33..b95f612 100644\r\n--- a/Dockerfile\r\n+++ b/Dockerfile\r\n@@ -24,7 +24,7 @@ RUN curl -o ~/miniconda.sh -O  https://repo.continuum.io/miniconda/Miniconda3-la\r\n #     /opt/conda/bin/conda install conda-build && \\\r\n      /opt/conda/bin/conda create -y --name pytorch-py$PYTHON_VERSION python=$PYTHON_VERSION numpy pyyaml scipy ipython mkl&& \\\r\n      /opt/conda/bin/conda clean -ya \r\n-ENV PATH /opt/conda/envs/pytorch-py$PYTHON_VERSION/bin:$PATH\r\n+ENV PATH /opt/conda/bin:/opt/conda/envs/pytorch-py$PYTHON_VERSION/bin:$PATH\r\n RUN conda install --name pytorch-py$PYTHON_VERSION -c soumith magma-cuda80\r\n # This must be done before pip so that requirements.txt is available\r\n WORKDIR /opt/pytorch\r\n```\r\n', ""I'm so Sorry @s1113950 .\r\nI'm getting the same error as you @s1113950 on both `master` and `v0.3.1` on my MacBook.\r\nThe build was successful on my workstation which is behind corporate proxy but this machine has a GPU so most likely the issue is lack of GPU on failing ones."", ""üëç I have a gpu but not an nvidia one. That's probably the issue I agree!""]","['\r\ndocker build --build-arg HTTP_PROXY=XXXX --build-arg HTTPS_PROXY=XXXX --build-arg http_proxy=XXXX --build-arg https_proxy=XXXX -t test:XXXXX https://github.com/pytorch/pytorch.git#v0.3.1\r\n', ""\r\nWARNING: The following packages cannot be authenticated!\r\n  libnccl2 libnccl-dev\r\nE: There were unauthenticated packages and -y was used without --allow-unauthenticated\r\nThe command '/bin/sh -c apt-get update && apt-get install -y --no-install-recommends          build-essential          cmake          git          curl          vim          ca-certificates          libnccl2=2.0.5-2+cuda8.0          libnccl-dev=2.0.5-2+cuda8.0          libjpeg-dev          libpng-dev &&     rm -rf /var/lib/apt/lists/*' returned a non-zero code: 100\r\n"", '\r\n CMakeFiles/THC.dir/build.make:217: recipe for target \'CMakeFiles/THC.dir/THC_generated_THCTensorMode.cu.o\' failed\r\n    make[2]: *** [CMakeFiles/THC.dir/THC_generated_THCTensorMode.cu.o] Error 1\r\n    make[2]: *** Waiting for unfinished jobs....\r\n    CMakeFiles/Makefile2:67: recipe for target \'CMakeFiles/THC.dir/all\' failed\r\n    make[1]: *** [CMakeFiles/THC.dir/all] Error 2\r\n    Makefile:127: recipe for target \'all\' failed\r\n    make: *** [all] Error 2\r\n    Running setup.py install for torch: finished with status \'error\'\r\nCleaning up...\r\n  Removing source in /tmp/pip-nvl2npin-build\r\nCommand ""/opt/conda/envs/pytorch-py35/bin/python -u -c ""import setuptools, tokenize;__file__=\'/tmp/pip-nvl2npin-build/setup.py\';f=getattr(tokenize, \'open\', open)(__file__);code=f.read().replace(\'\\r\\n\', \'\\n\');f.close();exec(compile(code, __file__, \'exec\'))"" install --record /tmp/pip-k2bx1enk-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-nvl2npin-build/\r\nException information:\r\nTraceback (most recent call last):\r\n  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/pip/basecommand.py"", line 215, in main\r\n    status = self.run(options, args)\r\n  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/pip/commands/install.py"", line 342, in run\r\n    prefix=options.prefix_path,\r\n  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/pip/req/req_set.py"", line 784, in install\r\n    **kwargs\r\n  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/pip/req/req_install.py"", line 878, in install\r\n    spinner=spinner,\r\n  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/pip/utils/__init__.py"", line 707, in call_subprocess\r\n    % (command_desc, proc.returncode, cwd))\r\npip.exceptions.InstallationError: Command ""/opt/conda/envs/pytorch-py35/bin/python -u -c ""import setuptools, tokenize;__file__=\'/tmp/pip-nvl2npin-build/setup.py\';f=getattr(tokenize, \'open\', open)(__file__);code=f.read().replace(\'\\r\\n\', \'\\n\');f.close();exec(compile(code, __file__, \'exec\'))"" install --record /tmp/pip-k2bx1enk-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-nvl2npin-build/\r\nLooking up ""https://pypi.python.org/pypi/pip/json"" in the cache\r\nNo cache entry available\r\nStarting new HTTPS connection (1): pypi.python.org\r\n""GET /pypi/pip/json HTTP/1.1"" 200 72983\r\nUpdating cache with response from ""https://pypi.python.org/pypi/pip/json""\r\nCaching b/c date exists and max-age > 0\r\nThe command \'/bin/sh -c TORCH_CUDA_ARCH_LIST=""3.5 5.2 6.0 6.1+PTX"" TORCH_NVCC_FLAGS=""-Xfatbin -compress-all""     CMAKE_PREFIX_PATH=""$(dirname $(which conda))/../""     pip install -v .\' returned a non-zero code: 1\r\n', ""\r\nWARNING: The following packages cannot be authenticated!\r\n  libcudnn6 libcudnn6-dev\r\nE: There were unauthenticated packages and -y was used without --allow-unauthenticated\r\nThe command '/bin/sh -c apt-get update && apt-get install -y --no-install-recommends          build-essential          cmake          git          curl          ca-certificates          libjpeg-dev          libpng-dev          libcudnn6=$CUDNN_VERSION-1+cuda8.0          libcudnn6-dev=$CUDNN_VERSION-1+cuda8.0 &&      rm -rf /var/lib/apt/lists/*' returned a non-zero code: 100\r\n"", '\r\n[ 19%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMode.cu.o\r\n    Killed\r\n    CMake Error at ATen_generated_THCTensorIndex.cu.o.cmake:267 (message):\r\n      Error generating file\r\n      /tmp/pip-fbuuiu10-build/torch/lib/build/aten/src/ATen/CMakeFiles/ATen.dir/__/THC/./ATen_generated_THCTensorIndex.cu.o\r\n\r\n\r\n    src/ATen/CMakeFiles/ATen.dir/build.make:154: recipe for target \'src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorIndex.cu.o\' failed\r\n    make[2]: *** [src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorIndex.cu.o] Error 1\r\n    make[2]: *** Waiting for unfinished jobs....\r\n    CMakeFiles/Makefile2:233: recipe for target \'src/ATen/CMakeFiles/ATen.dir/all\' failed\r\n    make[1]: *** [src/ATen/CMakeFiles/ATen.dir/all] Error 2\r\n    Makefile:127: recipe for target \'all\' failed\r\n    make: *** [all] Error 2\r\n    Running setup.py install for torch: finished with status \'error\'\r\nCleaning up...\r\n  Removing source in /tmp/pip-fbuuiu10-build\r\nCommand ""/opt/conda/bin/python -u -c ""import setuptools, tokenize;__file__=\'/tmp/pip-fbuuiu10-build/setup.py\';f=getattr(tokenize, \'open\', open)(__file__);code=f.read().replace(\'\\r\\n\', \'\\n\');f.close();exec(compile(code, __file__, \'exec\'))"" install --record /tmp/pip-giq0q4m5-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-fbuuiu10-build/\r\nException information:\r\nTraceback (most recent call last):\r\n  File ""/opt/conda/lib/python3.6/site-packages/pip/basecommand.py"", line 215, in main\r\n    status = self.run(options, args)\r\n  File ""/opt/conda/lib/python3.6/site-packages/pip/commands/install.py"", line 342, in run\r\n    prefix=options.prefix_path,\r\n  File ""/opt/conda/lib/python3.6/site-packages/pip/req/req_set.py"", line 784, in install\r\n    **kwargs\r\n  File ""/opt/conda/lib/python3.6/site-packages/pip/req/req_install.py"", line 878, in install\r\n    spinner=spinner,\r\n  File ""/opt/conda/lib/python3.6/site-packages/pip/utils/__init__.py"", line 707, in call_subprocess\r\n    % (command_desc, proc.returncode, cwd))\r\npip.exceptions.InstallationError: Command ""/opt/conda/bin/python -u -c ""import setuptools, tokenize;__file__=\'/tmp/pip-fbuuiu10-build/setup.py\';f=getattr(tokenize, \'open\', open)(__file__);code=f.read().replace(\'\\r\\n\', \'\\n\');f.close();exec(compile(code, __file__, \'exec\'))"" install --record /tmp/pip-giq0q4m5-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-fbuuiu10-build/\r\nLooking up ""https://pypi.python.org/pypi/pip/json"" in the cache\r\nNo cache entry available\r\nStarting new HTTPS connection (1): pypi.python.org\r\n""GET /pypi/pip/json HTTP/1.1"" 200 72983\r\nUpdating cache with response from ""https://pypi.python.org/pypi/pip/json""\r\nCaching b/c date exists and max-age > 0\r\nThe command \'/bin/sh -c TORCH_CUDA_ARCH_LIST=""3.5 5.2 6.0 6.1 7.0+PTX"" TORCH_NVCC_FLAGS=""-Xfatbin -compress-all""     CMAKE_PREFIX_PATH=""$(dirname $(which conda))/../""     pip install -v .\' returned a non-zero code: 1\r\n']","['v0.2.0', 'master']",0,0
414,pytorch,18689,closed,distributed.all_gather function stuck when using NCCL backend ,"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->
I am trying to use distributed.all_gather to gather gradients in multi nodes. but I found the all_gather function stuck and no error throw
## To Reproduce

Steps to reproduce the behavior:

1. set up env variable  , save the following code as 

2. run the code with  and 

btw, when I execute this code manually in , I found the all_gather quickly go through, but it stuck when trying to print tensor

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

process will print a list of tensor
<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:

PyTorch version: 1.0.1.post2
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 18.04.1 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CMake version: version 3.10.2

Python version: 3.5
Is CUDA available: Yes
CUDA runtime version: 9.0.176
GPU models and configuration:
GPU 0: Tesla K80
GPU 1: Tesla K80
GPU 2: Tesla K80
GPU 3: Tesla K80
GPU 4: Tesla K80
GPU 5: Tesla K80
GPU 6: Tesla K80
GPU 7: Tesla K80

Nvidia driver version: 384.145
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.3.1

Versions of relevant libraries:
[pip3] numpy==1.15.3
[pip3] torch==1.0.1.post2
[pip3] torchvision==0.2.1
[conda] Could not collect

## Additional context

<!-- Add any other context about the problem here. -->
",module: nccl oncall: distributed triaged,"['@lecoan\r\n\r\n1. Before init the process group, call `torch.cuda.set_device(args.rank)` to assign different GPUs to different processes. \r\n2. Change `tmp = [t.randn(5).cuda()] * 2` to `tmp = [t.randn(5).cuda() for _ in range(2)]`, otherwise you are using the same tensor to receive two outputs. \r\n\r\nDoes this solve your problem?', 'NCCL requires inputs to be stored on different GPUs. [link](https://docs.nvidia.com/deeplearning/sdk/nccl-archived/nccl_235/nccl-developer-guide/index.html)\r\n\r\n> When creating a communicator, a unique rank between 0 and n-1 has to be assigned to each of the nCUDA devices which are part of the communicator. Using the same CUDA device multiple times as different ranks of the same NCCL communicator is not supported and may lead to hangs.', 'I change tmp = [t.randn(5).cuda()] * 2 to tmp = [t.randn(5).cuda() for _ in range(2)], and now it works. thank you so much! @mrshenli \r\nhowever, if I don\'t use GPU (eg.  tmp = [t.randn(5)] * 2) and use Gloo, `all_gather` could work. I can\'t find why\r\n\r\n## use NCCL with `[t.randn(5).cuda()] * 2`\r\nCUDA_VISIBLE_DEVICES=6 python test.py --rank 0\r\nCUDA_VISIBLE_DEVICES=7 python test.py --rank 1\r\nI got:\r\n```bash\r\nTraceback (most recent call last):\r\n  File ""test.py"", line 21, in <module>\r\n    main()\r\n  File ""test.py"", line 17, in main\r\n    dist.all_gather(tmp, tensor)\r\n  File ""/usr/local/lib/python3.5/dist-packages/torch/distributed/distributed_c10d.py"", line 1027, in all_gather\r\n    work = _default_pg.allgather([tensor_list], [tensor])\r\nRuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:260, unhandled cuda error\r\n```\r\n\r\n## use Gloo with `[t.randn(5)] * 2`\r\npython test.py --rank 0\r\npython test.py --rank 1\r\n```bash\r\n[tensor([1., 1., 1., 1., 1.]), tensor([1., 1., 1., 1., 1.])]\r\n```', '@lecoan\r\n\r\nDo you mean something like the following? It does not hang but still doesn\'t work for me. The printed tensors are assigned the same value, instead of 0s and 1s, because they are the same tensor. Regarding the hanging behavior, Gloo won\'t hang even if input tensors are on the same GPU. \r\n\r\n```python\r\n    ws = 2\r\n    t.cuda.set_device(args.rank % 2)\r\n    dist.init_process_group(""gloo"", rank=args.rank, world_size=ws)\r\n    tmp = [t.randn(5)] * ws\r\n    print (tmp)\r\n    tensor = t.ones(5) * args.rank\r\n    dist.all_gather(tmp, tensor)\r\n    print(tmp)\r\n```', '> use Gloo with [t.randn(5)] * 2 ...\r\n\r\nSee my response above. If you use your original code, the printed result cannot tell if the output tensors use the same underlying memory space or not. To check that, input tensors on different processes should have different values (i.e., `tensor = t.ones(5) * args.rank`). ', '@lecoan may I close this issue?', '@mrshenli ok. thanks for your help']","['python\r\nimport torch.distributed as dist\r\nimport torch as t\r\n\r\nimport argparse\r\n\r\ndef main():\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\r\n            ""--rank"",\r\n            type=int,\r\n            help=""the rank of proc""\r\n            )\r\n    args = parser.parse_args()\r\n    dist.init_process_group(""nccl"", rank=args.rank, world_size=2)\r\n    tmp = [t.randn(5).cuda()] * 2\r\n    tensor = t.ones(5).cuda()\r\n    dist.all_gather(tmp, tensor)\r\n    print(tmp)\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n', '\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n']","['MASTER_ADDR', 'MASTER_PORT', 'main.py', 'python main.py --rank 0', 'python main.py --rank 1', 'ipython']",0,0
415,pytorch,23232,closed,Add collective communication APIs for Python objects. ,"## üöÄ Feature

Support , , , and  general python objects. Instead of overloading existing API, it might be clearer to make them explicit, e.g.: , , , and .


## Motivation

Sharing objects across processes is a very common use case. People actually started building their own solutions by serializing meta and object into tensors and then using tensor collective comm APIs to share them (see,  in #22490,  in #23228). 

## Pitch

Python object on different processes might not be of the same size. So this needs to be done in multiple steps:

1. Pickle local python object into byte tensors. 
2. Communicate the tensor sizes across processes. Then, each process allocates local buffer tensors using the max size.
3. Copy local tensor to the buffer tensor and communicate. 
4. Extract results from the buffer tensors and unpickle them back into Python objects.

cc @pietern @zhaojuanmao @pritamdamania87 
",feature oncall: distributed triaged,"['This came up in https://discuss.pytorch.org/t/gathering-dictionaries-of-distributeddataparallel/51381.', 'Another example of where `all_gather` is used with arbitrary Python objects is in [`maskrcnn_benchmark`](https://github.com/facebookresearch/maskrcnn-benchmark/blob/24c8c90efdb7cc51381af5ce0205b23567c3cd21/maskrcnn_benchmark/utils/comm.py#L48-L88)', '@rohan-varma This one might be interesting to you.']",[],"['gather', 'all_gather', 'broadcast', 'scatter', 'gather_object', 'all_gather_object', 'broadcast_object', 'scatter_object', '_broadcast_object ', '_collect_worker_names']",0,0
416,pytorch,14179,closed,Failed to run 'bash ../tools/build_pytorch_libs.sh --use-cuda --use-nnpack --use-mkldnn --use-qnnpack caffe2',"## üêõ Bug
-- Build files have been written to: /home/feng/pytorch/build
+ make install -j12
Scanning dependencies of target js_embed
Scanning dependencies of target benchmark
Scanning dependencies of target nccl_external
Scanning dependencies of target pthreadpool
Scanning dependencies of target clog
Scanning dependencies of target gtest
Scanning dependencies of target gloo
Scanning dependencies of target onnxifi_dummy
Scanning dependencies of target onnxifi_loader
Scanning dependencies of target libprotobuf-lite
Scanning dependencies of target libprotobuf
[  0%] Creating directories for 'nccl_external'
Scanning dependencies of target mkldnn
[  1%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/json_reporter.cc.o
[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/js_embed.dir/__/src/google/protobuf/compiler/js/embed.cc.o
[  1%] Building C object confu-deps/clog/CMakeFiles/clog.dir/src/clog.c.o
[  1%] No download step for 'nccl_external'
[  1%] No patch step for 'nccl_external'
[  1%] No update step for 'nccl_external'
[  1%] No configure step for 'nccl_external'
[  1%] Performing build step for 'nccl_external'
[  1%] Building C object third_party/onnx/CMakeFiles/onnxifi_loader.dir/onnx/onnxifi_loader.c.o
[  1%] Building C object third_party/onnx/CMakeFiles/onnxifi_dummy.dir/onnx/onnxifi_dummy.c.o
[  1%] Building C object confu-deps/pthreadpool/CMakeFiles/pthreadpool.dir/src/threadpool-pthreads.c.o
make[3]: warning: jobserver unavailable: using -j1.  Add '+' to parent make rule.
/home/feng/pytorch/third_party/QNNPACK/deps/clog/src/clog.c: In function ‚Äòclog_vlog_fatal‚Äô:
/home/feng/pytorch/third_party/QNNPACK/deps/clog/src/clog.c:120:4: warning: ignoring return value of ‚Äòwrite‚Äô, declared with attribute warn_unused_result [-Wunused-result]
    write(STDERR_FILENO, out_buffer, prefix_chars + format_chars + CLOG_SUFFIX_LENGTH);
    ^
/home/feng/pytorch/third_party/QNNPACK/deps/clog/src/clog.c: In function ‚Äòclog_vlog_error‚Äô:
/home/feng/pytorch/third_party/QNNPACK/deps/clog/src/clog.c:196:4: warning: ignoring return value of ‚Äòwrite‚Äô, declared with attribute warn_unused_result [-Wunused-result]
    write(STDERR_FILENO, out_buffer, prefix_chars + format_chars + CLOG_SUFFIX_LENGTH);
    ^
/home/feng/pytorch/third_party/QNNPACK/deps/clog/src/clog.c: In function ‚Äòclog_vlog_warning‚Äô:
/home/feng/pytorch/third_party/QNNPACK/deps/clog/src/clog.c:272:4: warning: ignoring return value of ‚Äòwrite‚Äô, declared with attribute warn_unused_result [-Wunused-result]
    write(STDERR_FILENO, out_buffer, prefix_chars + format_chars + CLOG_SUFFIX_LENGTH);
    ^
/home/feng/pytorch/third_party/QNNPACK/deps/clog/src/clog.c: In function ‚Äòclog_vlog_info‚Äô:
/home/feng/pytorch/third_party/QNNPACK/deps/clog/src/clog.c:348:4: warning: ignoring return value of ‚Äòwrite‚Äô, declared with attribute warn_unused_result [-Wunused-result]
    write(STDOUT_FILENO, out_buffer, prefix_chars + format_chars + CLOG_SUFFIX_LENGTH);
    ^
/home/feng/pytorch/third_party/QNNPACK/deps/clog/src/clog.c: In function ‚Äòclog_vlog_debug‚Äô:
/home/feng/pytorch/third_party/QNNPACK/deps/clog/src/clog.c:424:4: warning: ignoring return value of ‚Äòwrite‚Äô, declared with attribute warn_unused_result [-Wunused-result]
    write(STDOUT_FILENO, out_buffer, prefix_chars + format_chars + CLOG_SUFFIX_LENGTH);
    ^
[  1%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/algorithm.cc.o
[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/arena.cc.o
Generating nccl.h.in                           > /home/feng/pytorch/build/nccl/include/nccl.h
Compiling  init.cu                             > /home/feng/pytorch/build/nccl/obj/init.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[  1%] Building CXX object third_party/googletest/googletest/CMakeFiles/gtest.dir/src/gtest-all.cc.o
[  1%] Linking CXX executable ../../../bin/js_embed
[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/arenastring.cc.o
[  1%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/string_util.cc.o
[  1%] Linking C shared library ../../lib/libonnxifi_dummy.so
[  1%] Linking C static library ../../lib/libpthreadpool.a
[  1%] Linking C static library ../../lib/libclog.a
[  1%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/allgather.cc.o
[  1%] Linking C static library ../../lib/libonnxifi_loader.a
[  1%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/primitive.cpp.o
[  1%] Built target onnxifi_dummy
[  1%] Built target js_embed
[  1%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/pooling.cpp.o
[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/extension_set.cc.o
[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/arena.cc.o
[  1%] Built target onnxifi_loader
[  1%] Built target pthreadpool
[  1%] Built target clog
[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/arenastring.cc.o
Scanning dependencies of target python_copy_files
Scanning dependencies of target c10
[  1%] Building CXX object c10/CMakeFiles/c10.dir/DeviceType.cpp.o
[  1%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/engine.cpp.o
[  1%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/query.cpp.o
[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/generated_message_table_driven_lite.cc.o
[  1%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/allreduce.cc.o
[  1%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/softmax.cpp.o
[  1%] Building CXX object c10/CMakeFiles/c10.dir/Half.cpp.o
[  1%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/mkldnn_debug.cpp.o
[  1%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/batch_normalization.cpp.o
[  2%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/extension_set.cc.o
[  2%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/scratchpad.cpp.o
[  2%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/commandlineflags.cc.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/primitive_attr.cpp.o
[  3%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/generated_message_table_driven_lite.cc.o
[  3%] Building CXX object c10/CMakeFiles/c10.dir/Device.cpp.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/verbose.cpp.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/eltwise.cpp.o
[  3%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/sleep.cc.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/memory_desc_wrapper.cpp.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/primitive_iterator.cpp.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/lrn.cpp.o
[  3%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/allreduce_local.cc.o
[  3%] Building CXX object c10/CMakeFiles/c10.dir/Stream.cpp.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/memory.cpp.o
[  3%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/statistics.cc.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/inner_product.cpp.o
[  3%] Building CXX object c10/CMakeFiles/c10.dir/core/dispatch/OpSchema.cpp.o
[  3%] Building CXX object c10/CMakeFiles/c10.dir/core/dispatch/KernelRegistration.cpp.o
[  3%] Building CXX object c10/CMakeFiles/c10.dir/core/dispatch/DeviceId.cpp.o
[  3%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/broadcast.cc.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/stream.cpp.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/convolution_relu.cpp.o
[  3%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/generated_message_util.cc.o
[  3%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/io/coded_stream.cc.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/rnn.cpp.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/reorder.cpp.o
[  3%] Building CXX object c10/CMakeFiles/c10.dir/core/dispatch/DispatchKey.cpp.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[  3%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/generated_message_util.cc.o
[  3%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/context.cc.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/convolution.cpp.o
[  3%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/benchmark.cc.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/primitive_desc.cpp.o
[  3%] Building CXX object c10/CMakeFiles/c10.dir/core/dispatch/Dispatcher.cpp.o
[  3%] Building CXX object c10/CMakeFiles/c10.dir/core/dispatch/OpSchemaRegistration.cpp.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/deconvolution.cpp.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/utils.cpp.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_sse42_conv_kernel_f32.cpp.o
[  3%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/io/zero_copy_stream.cc.o
[  3%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/gather.cc.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/cpu_batch_normalization_utils.cpp.o
init.cu:52:1: warning: ‚ÄòncclNet‚Äô initialized and declared ‚Äòextern‚Äô
 ncclNet_t* ncclNet = NULL;
 ^
[  3%] Building CXX object c10/CMakeFiles/c10.dir/core/dispatch/DispatchTable.cpp.o
[  3%] Building CXX object c10/CMakeFiles/c10.dir/core/dispatch/LayoutId.cpp.o
[  3%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/reduce.cc.o
[  3%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/io/zero_copy_stream_impl_lite.cc.o
[  3%] Building CXX object c10/CMakeFiles/c10.dir/impl/DeviceGuardImplInterface.cpp.o
[  4%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/message_lite.cc.o
[  4%] Building CXX object c10/CMakeFiles/c10.dir/util/Type.cpp.o
[  4%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/io/coded_stream.cc.o
[  4%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/repeated_field.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[  4%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ref_lrn.cpp.o
[  4%] Building CXX object c10/CMakeFiles/c10.dir/util/Backtrace.cpp.o
[  4%] Building CXX object c10/CMakeFiles/c10.dir/util/Optional.cpp.o
[  4%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/scatter.cc.o
[  4%] Building CXX object c10/CMakeFiles/c10.dir/util/C++17.cpp.o
Compiling  ring.cu                             > /home/feng/pytorch/build/nccl/obj/ring.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[  5%] Building CXX object c10/CMakeFiles/c10.dir/util/SmallVector.cpp.o
[  5%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/csv_reporter.cc.o
[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/atomicops_internals_x86_gcc.cc.o
[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/atomicops_internals_x86_msvc.cc.o
[  5%] Building CXX object c10/CMakeFiles/c10.dir/util/LeftRight.cpp.o
[  5%] Building CXX object c10/CMakeFiles/c10.dir/util/flags_use_gflags.cpp.o
[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/io/zero_copy_stream.cc.o
[  5%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_common_1x1_conv_kernel.cpp.o
[  5%] Linking CXX static library ../../../lib/libgtest.a
[  5%] Built target gtest
Scanning dependencies of target ATEN_CPU_FILES_GEN_TARGET
[  5%] Generating ../aten/src/ATen/CPUByteType.cpp, ../aten/src/ATen/CPUByteType.h, ../aten/src/ATen/CPUCharType.cpp, ../aten/src/ATen/CPUCharType.h, ../aten/src/ATen/CPUCopy.cpp, ../aten/src/ATen/CPUDoubleType.cpp, ../aten/src/ATen/CPUDoubleType.h, ../aten/src/ATen/CPUFloatType.cpp, ../aten/src/ATen/CPUFloatType.h, ../aten/src/ATen/CPUGenerator.h, ../aten/src/ATen/CPUHalfType.cpp, ../aten/src/ATen/CPUHalfType.h, ../aten/src/ATen/CPUIntType.cpp, ../aten/src/ATen/CPUIntType.h, ../aten/src/ATen/CPULongType.cpp, ../aten/src/ATen/CPULongType.h, ../aten/src/ATen/CPUShortType.cpp, ../aten/src/ATen/CPUShortType.h, ../aten/src/ATen/Declarations.yaml, ../aten/src/ATen/Functions.h, ../aten/src/ATen/NativeFunctions.h, ../aten/src/ATen/RegisterCPU.cpp, ../aten/src/ATen/RegisterCPU.h, ../aten/src/ATen/SparseCPUByteType.cpp, ../aten/src/ATen/SparseCPUByteType.h, ../aten/src/ATen/SparseCPUCharType.cpp, ../aten/src/ATen/SparseCPUCharType.h, ../aten/src/ATen/SparseCPUDoubleType.cpp, ../aten/src/ATen/SparseCPUDoubleType.h, ../aten/src/ATen/SparseCPUFloatType.cpp, ../aten/src/ATen/SparseCPUFloatType.h, ../aten/src/ATen/SparseCPUIntType.cpp, ../aten/src/ATen/SparseCPUIntType.h, ../aten/src/ATen/SparseCPULongType.cpp, ../aten/src/ATen/SparseCPULongType.h, ../aten/src/ATen/SparseCPUShortType.cpp, ../aten/src/ATen/SparseCPUShortType.h, ../aten/src/ATen/TypeDefault.cpp, ../aten/src/ATen/TypeDefault.h, ../aten/src/ATen/TypeExtendedInterface.h, ../aten/src/ATen/CUDAByteType.cpp, ../aten/src/ATen/CUDAByteType.h, ../aten/src/ATen/CUDACharType.cpp, ../aten/src/ATen/CUDACharType.h, ../aten/src/ATen/CUDACopy.cpp, ../aten/src/ATen/CUDADoubleType.cpp, ../aten/src/ATen/CUDADoubleType.h, ../aten/src/ATen/CUDAFloatType.cpp, ../aten/src/ATen/CUDAFloatType.h, ../aten/src/ATen/CUDAGenerator.h, ../aten/src/ATen/CUDAHalfType.cpp, ../aten/src/ATen/CUDAHalfType.h, ../aten/src/ATen/CUDAIntType.cpp, ../aten/src/ATen/CUDAIntType.h, ../aten/src/ATen/CUDALongType.cpp, ../aten/src/ATen/CUDALongType.h, ../aten/src/ATen/CUDAShortType.cpp, ../aten/src/ATen/CUDAShortType.h, ../aten/src/ATen/RegisterCUDA.cpp, ../aten/src/ATen/RegisterCUDA.h, ../aten/src/ATen/SparseCUDAByteType.cpp, ../aten/src/ATen/SparseCUDAByteType.h, ../aten/src/ATen/SparseCUDACharType.cpp, ../aten/src/ATen/SparseCUDACharType.h, ../aten/src/ATen/SparseCUDADoubleType.cpp, ../aten/src/ATen/SparseCUDADoubleType.h, ../aten/src/ATen/SparseCUDAFloatType.cpp, ../aten/src/ATen/SparseCUDAFloatType.h, ../aten/src/ATen/SparseCUDAIntType.cpp, ../aten/src/ATen/SparseCUDAIntType.h, ../aten/src/ATen/SparseCUDALongType.cpp, ../aten/src/ATen/SparseCUDALongType.h, ../aten/src/ATen/SparseCUDAShortType.cpp, ../aten/src/ATen/SparseCUDAShortType.h
[  6%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/types.cc.o
[  6%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ref_deconvolution.cpp.o
[  7%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_sse42_1x1_conv_kernel_f32.cpp.o
[  7%] Built target python_copy_files
Scanning dependencies of target common
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/bytestream.cc.o
[  7%] Building C object sleef/src/common/CMakeFiles/common.dir/common.c.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/common.cc.o
[  7%] Built target common
Scanning dependencies of target mkrename
[  7%] Building C object sleef/src/libm/CMakeFiles/mkrename.dir/mkrename.c.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/common/linux.cc.o
[  7%] Linking C executable ../../bin/mkrename
[  7%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/reporter.cc.o
[  7%] Built target mkrename
[  7%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/complexity.cc.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/io/zero_copy_stream_impl_lite.cc.o
[  7%] Building CXX object c10/CMakeFiles/c10.dir/util/Array.cpp.o
[  7%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/cpu_sum.cpp.o
[  7%] Building CXX object c10/CMakeFiles/c10.dir/util/Logging.cpp.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/int128.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[  7%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/counter.cc.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/message_lite.cc.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/io_win32.cc.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/once.cc.o
[  7%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/sysinfo.cc.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/status.cc.o
[  7%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/colorprint.cc.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/statusor.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/common/logging.cc.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/mpi/context.cc.o
[  7%] Building CXX object c10/CMakeFiles/c10.dir/util/Exception.cpp.o
Compiling  bootstrap.cu                        > /home/feng/pytorch/build/nccl/obj/bootstrap.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/repeated_field.cc.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/stringpiece.cc.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/stringprintf.cc.o
[  7%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/timers.cc.o
[  7%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_common_convolution.cpp.o
[  7%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_common_conv_winograd_kernel_f32.cpp.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/rendezvous/context.cc.o
[  7%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/benchmark_register.cc.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/atomicops_internals_x86_gcc.cc.o
[  7%] Building CXX object c10/CMakeFiles/c10.dir/util/TypeList.cpp.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/structurally_valid.cc.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/strutil.cc.o
[  7%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/console_reporter.cc.o
[  7%] Building CXX object c10/CMakeFiles/c10.dir/util/typeid.cpp.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/atomicops_internals_x86_msvc.cc.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/rendezvous/file_store.cc.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/rendezvous/hash_store.cc.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/time.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/wire_format_lite.cc.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/bytestream.cc.o
[  7%] Building CXX object c10/CMakeFiles/c10.dir/util/TensorTypeId.cpp.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/rendezvous/prefix_store.cc.o
[  7%] Building CXX object c10/CMakeFiles/c10.dir/util/Metaprogramming.cpp.o
[  7%] Building CXX object c10/CMakeFiles/c10.dir/util/TypeTraits.cpp.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/rendezvous/store.cc.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/address.cc.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/buffer.cc.o
[  7%] Building CXX object c10/CMakeFiles/c10.dir/util/StringUtil.cpp.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/context.cc.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/common.cc.o
[  7%] Building CXX object c10/CMakeFiles/c10.dir/util/TensorTypeIdRegistration.cpp.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/device.cc.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/pair.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[  7%] Building CXX object c10/CMakeFiles/c10.dir/util/flags_use_no_gflags.cpp.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/unbound_buffer.cc.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/tcp/address.cc.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/tcp/buffer.cc.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/tcp/context.cc.o
[  7%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/gemm_inner_product.cpp.o
Compiling  transport.cu                        > /home/feng/pytorch/build/nccl/obj/transport.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[  7%] Linking CXX static library ../../../lib/libprotobuf-lite.a
[  7%] Built target libprotobuf-lite
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/tcp/device.cc.o
Scanning dependencies of target mkdisp
[  7%] Building C object sleef/src/libm/CMakeFiles/mkdisp.dir/mkdisp.c.o
[  7%] Linking C executable ../../bin/mkdisp
[  7%] Built target mkdisp
Scanning dependencies of target renamedsp256.h_generated
[  7%] Generating renamedsp256.h
[  7%] Built target renamedsp256.h_generated
Scanning dependencies of target dispavx.c_generated
[  7%] Generating dispavx.c
[  7%] Built target dispavx.c_generated
Scanning dependencies of target renameSSE2.h_generated
[  7%] Generating include/renamesse2.h
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/tcp/pair.cc.o
Generating renamesse2.h: mkrename 2 4 sse2
[  7%] Built target renameSSE2.h_generated
Scanning dependencies of target renameFMA4.h_generated
[  7%] Generating include/renamefma4.h
Generating renamefma4.h: mkrename 4 8 fma4
[  7%] Built target renameFMA4.h_generated
Scanning dependencies of target renameAVX2.h_generated
[  7%] Generating include/renameavx2.h
Generating renameavx2.h: mkrename 4 8 avx2
[  7%] Built target renameAVX2.h_generated
Scanning dependencies of target renameAVX2128.h_generated
[  7%] Generating include/renameavx2128.h
Generating renameavx2128.h: mkrename 2 4 avx2128
[  7%] Built target renameAVX2128.h_generated
Scanning dependencies of target renameSSE4.h_generated
[  8%] Generating include/renamesse4.h
Generating renamesse4.h: mkrename 2 4 sse4
[  8%] Built target renameSSE4.h_generated
Scanning dependencies of target ATEN_CUDA_FILES_GEN_TARGET
[  8%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/int128.cc.o
[  8%] Generating ../aten/src/ATen/CPUByteType.cpp, ../aten/src/ATen/CPUByteType.h, ../aten/src/ATen/CPUCharType.cpp, ../aten/src/ATen/CPUCharType.h, ../aten/src/ATen/CPUCopy.cpp, ../aten/src/ATen/CPUDoubleType.cpp, ../aten/src/ATen/CPUDoubleType.h, ../aten/src/ATen/CPUFloatType.cpp, ../aten/src/ATen/CPUFloatType.h, ../aten/src/ATen/CPUGenerator.h, ../aten/src/ATen/CPUHalfType.cpp, ../aten/src/ATen/CPUHalfType.h, ../aten/src/ATen/CPUIntType.cpp, ../aten/src/ATen/CPUIntType.h, ../aten/src/ATen/CPULongType.cpp, ../aten/src/ATen/CPULongType.h, ../aten/src/ATen/CPUShortType.cpp, ../aten/src/ATen/CPUShortType.h, ../aten/src/ATen/Declarations.yaml, ../aten/src/ATen/Functions.h, ../aten/src/ATen/NativeFunctions.h, ../aten/src/ATen/RegisterCPU.cpp, ../aten/src/ATen/RegisterCPU.h, ../aten/src/ATen/SparseCPUByteType.cpp, ../aten/src/ATen/SparseCPUByteType.h, ../aten/src/ATen/SparseCPUCharType.cpp, ../aten/src/ATen/SparseCPUCharType.h, ../aten/src/ATen/SparseCPUDoubleType.cpp, ../aten/src/ATen/SparseCPUDoubleType.h, ../aten/src/ATen/SparseCPUFloatType.cpp, ../aten/src/ATen/SparseCPUFloatType.h, ../aten/src/ATen/SparseCPUIntType.cpp, ../aten/src/ATen/SparseCPUIntType.h, ../aten/src/ATen/SparseCPULongType.cpp, ../aten/src/ATen/SparseCPULongType.h, ../aten/src/ATen/SparseCPUShortType.cpp, ../aten/src/ATen/SparseCPUShortType.h, ../aten/src/ATen/TypeDefault.cpp, ../aten/src/ATen/TypeDefault.h, ../aten/src/ATen/TypeExtendedInterface.h, ../aten/src/ATen/CUDAByteType.cpp, ../aten/src/ATen/CUDAByteType.h, ../aten/src/ATen/CUDACharType.cpp, ../aten/src/ATen/CUDACharType.h, ../aten/src/ATen/CUDACopy.cpp, ../aten/src/ATen/CUDADoubleType.cpp, ../aten/src/ATen/CUDADoubleType.h, ../aten/src/ATen/CUDAFloatType.cpp, ../aten/src/ATen/CUDAFloatType.h, ../aten/src/ATen/CUDAGenerator.h, ../aten/src/ATen/CUDAHalfType.cpp, ../aten/src/ATen/CUDAHalfType.h, ../aten/src/ATen/CUDAIntType.cpp, ../aten/src/ATen/CUDAIntType.h, ../aten/src/ATen/CUDALongType.cpp, ../aten/src/ATen/CUDALongType.h, ../aten/src/ATen/CUDAShortType.cpp, ../aten/src/ATen/CUDAShortType.h, ../aten/src/ATen/RegisterCUDA.cpp, ../aten/src/ATen/RegisterCUDA.h, ../aten/src/ATen/SparseCUDAByteType.cpp, ../aten/src/ATen/SparseCUDAByteType.h, ../aten/src/ATen/SparseCUDACharType.cpp, ../aten/src/ATen/SparseCUDACharType.h, ../aten/src/ATen/SparseCUDADoubleType.cpp, ../aten/src/ATen/SparseCUDADoubleType.h, ../aten/src/ATen/SparseCUDAFloatType.cpp, ../aten/src/ATen/SparseCUDAFloatType.h, ../aten/src/ATen/SparseCUDAIntType.cpp, ../aten/src/ATen/SparseCUDAIntType.h, ../aten/src/ATen/SparseCUDALongType.cpp, ../aten/src/ATen/SparseCUDALongType.h, ../aten/src/ATen/SparseCUDAShortType.cpp, ../aten/src/ATen/SparseCUDAShortType.h
[  8%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/tcp/unbound_buffer.cc.o
[  8%] Linking CXX shared library ../lib/libc10.so
[  8%] Built target c10
Scanning dependencies of target mkrename_gnuabi
[  8%] Building C object sleef/src/libm/CMakeFiles/mkrename_gnuabi.dir/mkrename_gnuabi.c.o
[  8%] Linking C executable ../../bin/mkrename_gnuabi
[  8%] Built target mkrename_gnuabi
Scanning dependencies of target mkmasked_gnuabi
[  8%] Building C object sleef/src/libm/CMakeFiles/mkmasked_gnuabi.dir/mkmasked_gnuabi.c.o
[  8%] Linking C executable ../../bin/mkmasked_gnuabi
[  8%] Built target mkmasked_gnuabi
Scanning dependencies of target mkalias
[  8%] Building C object sleef/src/libm/CMakeFiles/mkalias.dir/mkalias.c.o
Scanning dependencies of target arraymap
[  8%] Building C object sleef/src/common/CMakeFiles/arraymap.dir/arraymap.c.o
[  8%] Linking C executable ../../bin/mkalias
[  8%] Built target mkalias
[  8%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/io_win32.cc.o
Scanning dependencies of target torch_shm_manager
[  8%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/once.cc.o
[  8%] Building CXX object caffe2/torch/lib/libshm/CMakeFiles/torch_shm_manager.dir/manager.cpp.o
[  8%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/status.cc.o
[  8%] Built target arraymap
Scanning dependencies of target c10_utils_gpu
[  9%] Building CXX object caffe2/utils/CMakeFiles/c10_utils_gpu.dir/dummy.cpp.o
[  9%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/cpu_barrier.cpp.o
[  9%] Built target c10_utils_gpu
Scanning dependencies of target c10_utils_hip
[  9%] Building CXX object caffe2/utils/CMakeFiles/c10_utils_hip.dir/dummy.cpp.o
[  9%] Built target c10_utils_hip
Scanning dependencies of target c10_utils_cpu
[  9%] Building CXX object caffe2/utils/CMakeFiles/c10_utils_cpu.dir/dummy.cpp.o
[  9%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/simple_sum.cpp.o
[  9%] Built target c10_utils_cpu
Scanning dependencies of target cpuinfo
[  9%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/init.c.o
[  9%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/api.c.o
[  9%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/init.c.o
[  9%] Linking CXX static library ../../../lib/libbenchmark.a
[  9%] Built target benchmark
[  9%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/info.c.o
Scanning dependencies of target nnpack_reference_layers
[  9%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack_reference_layers.dir/src/ref/convolution-output.c.o
[  9%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/vendor.c.o
[  9%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/uarch.c.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[  9%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack_reference_layers.dir/src/ref/convolution-input-gradient.c.o
[  9%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ncsp_batch_normalization.cpp.o
[  9%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/name.c.o
[  9%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack_reference_layers.dir/src/ref/convolution-kernel.c.o
[  9%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/statusor.cc.o
[  9%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack_reference_layers.dir/src/ref/fully-connected-output.c.o
[  9%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/topology.c.o
[  9%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack_reference_layers.dir/src/ref/max-pooling-output.c.o
[  9%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/isa.c.o
[  9%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack_reference_layers.dir/src/ref/softmax-output.c.o
[  9%] Built target ATEN_CPU_FILES_GEN_TARGET
[  9%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack_reference_layers.dir/src/ref/relu-output.c.o
[  9%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ref_inner_product.cpp.o
[  9%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/cache/descriptor.c.o
[  9%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack_reference_layers.dir/src/ref/relu-input-gradient.c.o
[  9%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/cache/init.c.o
[  9%] Linking C static library ../../lib/libnnpack_reference_layers.a
[  9%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/cache/deterministic.c.o
[ 10%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/linux/init.c.o
[ 10%] Built target nnpack_reference_layers
Scanning dependencies of target gtest_main
[ 10%] Building CXX object third_party/googletest/googletest/CMakeFiles/gtest_main.dir/src/gtest_main.cc.o
[ 10%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/linux/cpuinfo.c.o
[ 10%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/linux/smallfile.c.o
[ 10%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/linux/multiline.c.o
[ 10%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/linux/current.c.o
[ 10%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/linux/cpulist.c.o
[ 10%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/linux/processors.c.o
Scanning dependencies of target benchmark_main
[ 10%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark_main.dir/benchmark_main.cc.o
[ 10%] Linking C static library ../../lib/libcpuinfo.a
[ 10%] Built target cpuinfo
Scanning dependencies of target onnxifi_wrapper
[ 10%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/stringpiece.cc.o
[ 10%] Building C object third_party/onnx/CMakeFiles/onnxifi_wrapper.dir/onnx/onnxifi_wrapper.c.o
[ 10%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/simple_concat.cpp.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 10%] Linking CXX executable ../../../../bin/torch_shm_manager
[ 10%] Linking C shared module ../../lib/libonnxifi.so
[ 10%] Built target onnxifi_wrapper
Scanning dependencies of target __aten_op_header_gen
[ 10%] Generating contrib/aten/aten_op.h
[ 10%] Built target torch_shm_manager
Scanning dependencies of target renameAVX.h_generated
[ 10%] Generating include/renameavx.h
Generating renameavx.h: mkrename 4 8 avx
[ 10%] Built target renameAVX.h_generated
[ 11%] Linking CXX static library ../../../lib/libgloo.a
Scanning dependencies of target renamedsp128.h_generated
[ 11%] Generating renamedsp128.h
[ 11%] Built target renamedsp128.h_generated
Scanning dependencies of target headers
[ 12%] Generating ../../../include/sleef.h
[ 12%] Built target gloo
Scanning dependencies of target dispsse.c_generated
[ 12%] Generating dispsse.c
Generating sleef.h: mkrename 2 4 __m128d __m128 __m128i __m128i __SSE2__
Generating sleef.h: mkrename 2 4 __m128d __m128 __m128i __m128i __SSE2__ sse2
Generating sleef.h: mkrename 2 4 __m128d __m128 __m128i __m128i __SSE2__ sse4
Generating sleef.h: mkrename 4 8 __m256d __m256 __m128i struct\ {\ __m128i\ x,\ y;\ } __AVX__
Generating sleef.h: mkrename 4 8 __m256d __m256 __m128i struct\ {\ __m128i\ x,\ y;\ } __AVX__ avx
Generating sleef.h: mkrename 4 8 __m256d __m256 __m128i struct\ {\ __m128i\ x,\ y;\ } __AVX__ fma4
Generating sleef.h: mkrename 4 8 __m256d __m256 __m128i __m256i __AVX__ avx2
Generating sleef.h: mkrename 2 4 __m128d __m128 __m128i __m128i __SSE2__ avx2128
Generating sleef.h: mkrename 8 16 __m512d __m512 __m256i __m512i __AVX512F__
Generating sleef.h: mkrename 8 16 __m512d __m512 __m256i __m512i __AVX512F__ avx512f
[ 12%] Built target dispsse.c_generated
[ 12%] Built target headers
Scanning dependencies of target sleefsse2
Scanning dependencies of target sleeffma4
[ 12%] Building C object sleef/src/libm/CMakeFiles/sleefsse2.dir/sleefsimdsp.c.o
[ 12%] Building C object sleef/src/libm/CMakeFiles/sleeffma4.dir/sleefsimdsp.c.o
Compiling  misc/group.cu                       > /home/feng/pytorch/build/nccl/obj/misc/group.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 12%] Linking CXX static library ../../../lib/libbenchmark_main.a
[ 12%] Linking CXX static library ../../../lib/libgtest_main.a
[ 12%] Built target benchmark_main
Scanning dependencies of target sleefavx2
[ 12%] Built target gtest_main
[ 12%] Building C object sleef/src/libm/CMakeFiles/sleefavx2.dir/sleefsimdsp.c.o
Scanning dependencies of target sleefavx2128
[ 12%] Building C object sleef/src/libm/CMakeFiles/sleefavx2128.dir/sleefsimdsp.c.o
[ 12%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/stringprintf.cc.o
[ 12%] Building C object sleef/src/libm/CMakeFiles/sleeffma4.dir/sleefsimddp.c.o
[ 12%] Building C object sleef/src/libm/CMakeFiles/sleefavx2128.dir/sleefsimddp.c.o
[ 12%] Building C object sleef/src/libm/CMakeFiles/sleefavx2.dir/sleefsimddp.c.o
[ 12%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/structurally_valid.cc.o
[ 12%] Building C object sleef/src/libm/CMakeFiles/sleefsse2.dir/sleefsimddp.c.o
[ 12%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_core_u8s8s32x_conv_kernel.cpp.o
[ 12%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/gemm_convolution.cpp.o
[ 12%] Built target sleeffma4
Scanning dependencies of target sleefsse4
[ 12%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/strutil.cc.o
[ 12%] Built target sleefavx2128
Scanning dependencies of target c10_utils_gpu_test
[ 12%] Building C object sleef/src/libm/CMakeFiles/sleefsse4.dir/sleefsimdsp.c.o
[ 12%] Linking CXX executable ../../bin/c10_utils_gpu_test
[ 12%] Built target sleefavx2
Scanning dependencies of target c10_utils_hip_test
[ 12%] Building CXX object caffe2/utils/CMakeFiles/c10_utils_hip_test.dir/dummy.cpp.o
[ 12%] Linking CXX executable ../../bin/c10_utils_hip_test
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 12%] Built target c10_utils_gpu_test
Scanning dependencies of target c10_utils_cpu_test
[ 12%] Linking CXX executable ../../bin/c10_utils_cpu_test
[ 12%] Built target c10_utils_hip_test
Scanning dependencies of target qnnpack
[ 12%] Building C object sleef/src/libm/CMakeFiles/sleefsse4.dir/sleefsimddp.c.o
[ 12%] Building C object confu-deps/QNNPACK/CMakeFiles/qnnpack.dir/src/init.c.o
[ 12%] Built target c10_utils_cpu_test
[ 12%] Generating src/x86_64-fma/2d-fourier-8x8.py.o
[ 12%] Building C object confu-deps/QNNPACK/CMakeFiles/qnnpack.dir/src/convolution.c.o
[ 12%] Built target sleefsse2
Scanning dependencies of target c10_registry_test
[ 12%] Building CXX object c10/test/CMakeFiles/c10_registry_test.dir/registry_test.cpp.o
[ 12%] Building C object confu-deps/QNNPACK/CMakeFiles/qnnpack.dir/src/deconvolution.c.o
[ 12%] Building C object confu-deps/QNNPACK/CMakeFiles/qnnpack.dir/src/fully-connected.c.o
[ 12%] Building C object confu-deps/QNNPACK/CMakeFiles/qnnpack.dir/src/sgemm/6x8-psimd.c.o
[ 12%] Building C object confu-deps/QNNPACK/CMakeFiles/qnnpack.dir/src/q8gemm/2x4c8-sse2.c.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 12%] Building C object confu-deps/QNNPACK/CMakeFiles/qnnpack.dir/src/q8gemm/4x4c2-sse2.c.o
[ 12%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_pool_kernel_f32.cpp.o
[ 12%] Building C object confu-deps/QNNPACK/CMakeFiles/qnnpack.dir/src/q8conv/4x4c2-sse2.c.o
[ 12%] Built target sleefsse4
Compiling  misc/nvmlwrap.cu                    > /home/feng/pytorch/build/nccl/obj/misc/nvmlwrap.o
Scanning dependencies of target c10_OpSchema_test
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 12%] Building CXX object c10/test/CMakeFiles/c10_OpSchema_test.dir/dispatch/OpSchema_test.cpp.o
[ 12%] Building C object confu-deps/QNNPACK/CMakeFiles/qnnpack.dir/src/q8dw/9c8-sse2.c.o
[ 12%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/time.cc.o
[ 12%] Linking C static library ../../lib/libqnnpack.a
[ 13%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/wire_format_lite.cc.o
[ 13%] Built target qnnpack
Scanning dependencies of target c10_InlineStreamGuard_test
[ 13%] Building CXX object c10/test/CMakeFiles/c10_InlineStreamGuard_test.dir/impl/InlineStreamGuard_test.cpp.o
[ 13%] Linking CXX executable ../../bin/c10_OpSchema_test
[ 13%] Built target c10_OpSchema_test
Scanning dependencies of target c10_StreamGuard_test
[ 13%] Building CXX object c10/test/CMakeFiles/c10_StreamGuard_test.dir/StreamGuard_test.cpp.o
[ 13%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/gemm_u8s8s32x_convolution.cpp.o
[ 13%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/any.cc.o
[ 13%] Linking CXX executable ../../bin/c10_registry_test
[ 13%] Built target c10_registry_test
Scanning dependencies of target c10_DeviceGuard_test
[ 13%] Linking CXX executable ../../bin/c10_StreamGuard_test
[ 13%] Building CXX object c10/test/CMakeFiles/c10_DeviceGuard_test.dir/DeviceGuard_test.cpp.o
[ 13%] Built target c10_StreamGuard_test
Scanning dependencies of target c10_TypeTraits_test
[ 13%] Building CXX object c10/test/CMakeFiles/c10_TypeTraits_test.dir/util/TypeTraits_test.cpp.o
[ 13%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/any.pb.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 13%] Generating src/x86_64-fma/2d-fourier-16x16.py.o
[ 13%] Built target ATEN_CUDA_FILES_GEN_TARGET
Scanning dependencies of target c10_Metaprogramming_test
[ 13%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/api.pb.cc.o
[ 13%] Building CXX object c10/test/CMakeFiles/c10_Metaprogramming_test.dir/util/Metaprogramming_test.cpp.o
[ 13%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_core_u8s8s32x_1x1_convolution.cpp.o
[ 13%] Linking CXX executable ../../bin/c10_TypeTraits_test
[ 13%] Built target c10_TypeTraits_test
Scanning dependencies of target c10_logging_test
[ 13%] Building CXX object c10/test/CMakeFiles/c10_logging_test.dir/logging_test.cpp.o
[ 13%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/compiler/importer.cc.o
[ 13%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx2_gemm_f32.cpp.o
[ 14%] Linking CXX executable ../../bin/c10_InlineStreamGuard_test
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 14%] Built target c10_InlineStreamGuard_test
Scanning dependencies of target c10_Array_test
[ 14%] Building CXX object c10/test/CMakeFiles/c10_Array_test.dir/util/Array_test.cpp.o
[ 14%] Linking CXX executable ../../bin/c10_DeviceGuard_test
Compiling  misc/ibvwrap.cu                     > /home/feng/pytorch/build/nccl/obj/misc/ibvwrap.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 14%] Built target c10_DeviceGuard_test
Scanning dependencies of target c10_InlineDeviceGuard_test
[ 14%] Building CXX object c10/test/CMakeFiles/c10_InlineDeviceGuard_test.dir/impl/InlineDeviceGuard_test.cpp.o
[ 14%] Linking CXX executable ../../bin/c10_Metaprogramming_test
[ 14%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/compiler/parser.cc.o
[ 14%] Built target c10_Metaprogramming_test
Scanning dependencies of target c10_typeid_test
[ 14%] Linking CXX executable ../../bin/c10_Array_test
[ 14%] Building CXX object c10/test/CMakeFiles/c10_typeid_test.dir/util/typeid_test.cpp.o
[ 14%] Built target c10_Array_test
Scanning dependencies of target c10_flags_test
[ 14%] Building CXX object c10/test/CMakeFiles/c10_flags_test.dir/flags_test.cpp.o
[ 14%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_pooling.cpp.o
[ 14%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/descriptor.cc.o
[ 14%] Linking CXX executable ../../bin/c10_logging_test
[ 14%] Built target c10_logging_test
Scanning dependencies of target c10_TypeList_test
Skipping _th_multinomial Because of Arg: Generator * (Generator*) 
Skipping _th_normal Because of Arg: Generator * (Generator*) 
Skipping _th_normal Because of Arg: Generator * (Generator*) 
Skipping _th_normal Because of Arg: Generator * (Generator*) 
Skipping _th_tensor Because of Arg: Storage (Storage) 
Skipping _th_tensor Because of Arg: Storage (Storage) 
Skipping rrelu_with_noise Because of Arg: Generator * (Generator*) 
Skipping rrelu_with_noise_forward Because of Arg: Generator * (Generator*) 
[ 14%] Building CXX object c10/test/CMakeFiles/c10_TypeList_test.dir/util/TypeList_test.cpp.o
Skipping thnn_conv_transpose2d_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) 
Skipping thnn_conv_transpose3d_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) 
Skipping thnn_conv2d_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) 
Skipping thnn_conv_depthwise2d_backward Because of Arg: std::array<bool,2> (std::array<bool,2>) 
Skipping thnn_conv3d_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) 
Skipping thnn_conv_dilated2d_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) 
Skipping thnn_conv_dilated3d_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) 
Skipping _cudnn_rnn_backward Because of Arg: std::array<bool,4> (std::array<bool,4>) 
Skipping _cudnn_init_dropout_state because it is a factory method
Skipping _fused_dropout Because of Arg: Generator * (Generator *) 
Skipping arange because it is a factory method
Skipping bartlett_window because it is a factory method
Skipping bernoulli Because of Arg: Generator * (Generator *) 
Skipping bernoulli Because of Arg: Generator * (Generator *) 
Skipping blackman_window because it is a factory method
Skipping clamp Because of Arg: c10::optional<Scalar> (Scalar) 
Skipping clamp Because of Arg: c10::optional<Scalar> (Scalar) 
Skipping _convolution_double_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) 
Skipping cudnn_convolution_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) 
Skipping cudnn_convolution_transpose_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) 
Skipping cumsum Because of Arg: ScalarType (ScalarType) 
Skipping cumprod Because of Arg: ScalarType (ScalarType) 
Skipping einsum Because of Arg: std::string (std::string) 
Skipping empty because it is a factory method
Skipping empty_like because it is a factory method
Skipping empty_strided because it is a factory method
Skipping eye because it is a factory method
Skipping full because it is a factory method
Skipping full_like because it is a factory method
Skipping hann_window because it is a factory method
Skipping hamming_window because it is a factory method
Skipping _cufft_set_plan_cache_max_size Because of Ret: void (void)
Skipping _cufft_clear_plan_cache Because of Ret: void (void)
Skipping linspace because it is a factory method
Skipping logspace because it is a factory method
Skipping log_softmax Because of Arg: ScalarType (ScalarType) 
Skipping mean Because of Arg: ScalarType (ScalarType) 
Skipping mean Because of Arg: ScalarType (ScalarType) 
Skipping mean Because of Arg: ScalarType (ScalarType) 
Skipping mkldnn_convolution_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) 
Skipping miopen_convolution_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) 
Skipping miopen_convolution_transpose_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) 
Skipping native_batch_norm_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) 
Skipping ones because it is a factory method
Skipping ones_like because it is a factory method
Skipping rand because it is a factory method
Skipping rand_like because it is a factory method
Skipping randint because it is a factory method
Skipping randint_like because it is a factory method
Skipping randn because it is a factory method
Skipping randn_like because it is a factory method
Skipping randperm because it is a factory method
Skipping range because it is a factory method
Skipping rrelu Because of Arg: Generator * (Generator *) 
Skipping softmax Because of Arg: ScalarType (ScalarType) 
Skipping sum Because of Arg: ScalarType (ScalarType) 
Skipping sum Because of Arg: ScalarType (ScalarType) 
Skipping sum Because of Arg: ScalarType (ScalarType) 
Skipping prod Because of Arg: ScalarType (ScalarType) 
Skipping prod Because of Arg: ScalarType (ScalarType) 
Skipping prod Because of Arg: ScalarType (ScalarType) 
Skipping zeros because it is a factory method
Skipping zeros_like because it is a factory method
Skipping _standard_gamma Because of Arg: Generator * (Generator *) 
Skipping poisson Because of Arg: Generator * (Generator *) 
Skipping sparse_coo_tensor because it is a factory method
Skipping _sparse_coo_tensor_unsafe because it is a factory method
Skipping _sparse_coo_tensor_with_dims because it is a factory method
Skipping _sparse_coo_tensor_with_dims_and_tensors because it is a factory method
Skipping sparse_mask Because of Arg: SparseTensorRef (SparseTensorRef) 
Skipping to because it is a factory method
Skipping data_ptr Because of Ret: void* (void*)
Skipping multinomial Because of Arg: Generator * (Generator *) 
Skipping normal Because of Arg: Generator * (Generator *) 
Skipping normal Because of Arg: Generator * (Generator *) 
Skipping normal Because of Arg: Generator * (Generator *) 
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 14%] Built target __aten_op_header_gen
Scanning dependencies of target sleefavx
[ 14%] Building C object sleef/src/libm/CMakeFiles/sleefavx.dir/sleefsimdsp.c.o
[ 14%] Linking CXX executable ../../bin/c10_InlineDeviceGuard_test
[ 14%] Linking CXX executable ../../bin/c10_flags_test
[ 14%] Built target c10_flags_test
[ 14%] Built target c10_InlineDeviceGuard_test
Scanning dependencies of target dispsse_obj
Scanning dependencies of target dispavx_obj
[ 14%] Building C object sleef/src/libm/CMakeFiles/dispsse_obj.dir/dispsse.c.o
[ 14%] Building C object sleef/src/libm/CMakeFiles/dispavx_obj.dir/dispavx.c.o
[ 14%] Linking CXX executable ../../bin/c10_TypeList_test
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 14%] Built target c10_TypeList_test
[ 15%] Building C object sleef/src/libm/CMakeFiles/sleefavx.dir/sleefsimddp.c.o
[ 15%] Linking CXX executable ../../bin/c10_typeid_test
[ 15%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_lrn_kernel_f32.cpp.o
[ 15%] Built target c10_typeid_test
Compiling  misc/rings.cu                       > /home/feng/pytorch/build/nccl/obj/misc/rings.o
[ 15%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/cpu_concat.cpp.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 15%] Built target dispsse_obj
[ 15%] Built target dispavx_obj
[ 15%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_core_u8s8s32x_convolution.cpp.o
[ 15%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/descriptor.pb.cc.o
[ 15%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/descriptor_database.cc.o
[ 15%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_core_i8i8_pooling.cpp.o
[ 15%] Built target sleefavx
Scanning dependencies of target sleef
[ 15%] Building C object sleef/src/libm/CMakeFiles/sleef.dir/sleefdp.c.o
[ 15%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/nchw_pooling.cpp.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 16%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/cpu_reducer.cpp.o
[ 16%] Building C object sleef/src/libm/CMakeFiles/sleef.dir/sleefsp.c.o
[ 16%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_common_gemm_f32.cpp.o
[ 16%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/duration.pb.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 16%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_dw_conv_kernel_f32.cpp.o
Compiling  misc/utils.cu                       > /home/feng/pytorch/build/nccl/obj/misc/utils.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 16%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_reorder.cpp.o
[ 16%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/cpu_engine.cpp.o
[ 16%] Building C object sleef/src/libm/CMakeFiles/sleef.dir/sleefld.c.o
[ 16%] Building C object sleef/src/libm/CMakeFiles/sleef.dir/sleefqp.c.o
[ 16%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/dynamic_message.cc.o
[ 16%] Linking C static library ../../lib/libsleef.a
[ 16%] Built target sleef
[ 16%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/empty.pb.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 16%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_core_convolution_winograd.cpp.o
Compiling  misc/enqueue.cu                     > /home/feng/pytorch/build/nccl/obj/misc/enqueue.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 16%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/extension_set_heavy.cc.o
[ 16%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_common_lrn.cpp.o
[ 16%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/field_mask.pb.cc.o
[ 16%] Generating src/x86_64-fma/2d-winograd-8x8-3x3.py.o
[ 16%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/generated_message_reflection.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 16%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/generated_message_table_driven.cc.o
[ 16%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/io/gzip_stream.cc.o
[ 16%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_sse42_convolution.cpp.o
[ 16%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/io/printer.cc.o
[ 16%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_common_convolution_winograd.cpp.o
[ 16%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/io/strtod.cc.o
[ 16%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx2_1x1_convolution.cpp.o
[ 16%] Generating src/x86_64-fma/blas/s8gemm.py.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 16%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/io/tokenizer.cc.o
Compiling  transport/p2p.cu                    > /home/feng/pytorch/build/nccl/obj/transport/p2p.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 16%] Generating src/x86_64-fma/blas/c8gemm.py.o
[ 16%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_eltwise.cpp.o
[ 16%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/io/zero_copy_stream_impl.cc.o
[ 16%] Generating src/x86_64-fma/blas/s4c6gemm.py.o
[ 16%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/map_field.cc.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/message.cc.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/reflection_ops.cc.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/service.cc.o
[ 17%] Generating src/x86_64-fma/blas/conv1x1.py.o
[ 17%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ref_pooling.cpp.o
[ 17%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx2_conv_kernel_f32.cpp.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 17%] Generating src/x86_64-fma/blas/sgemm.py.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/source_context.pb.cc.o
[ 17%] Generating src/x86_64-fma/max-pooling.py.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/struct.pb.cc.o
[ 17%] Generating src/x86_64-fma/relu.py.o
[ 17%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_dw_convolution.cpp.o
[ 17%] Generating src/x86_64-fma/softmax.py.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/mathlimits.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/substitute.cc.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/text_format.cc.o
Compiling  transport/shm.cu                    > /home/feng/pytorch/build/nccl/obj/transport/shm.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/timestamp.pb.cc.o
[ 17%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ref_batch_normalization.cpp.o
[ 17%] Generating src/x86_64-fma/blas/sdotxf.py.o
[ 17%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_inner_product.cpp.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/type.pb.cc.o
[ 17%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_reorder_utils.cpp.o
[ 17%] Generating src/x86_64-fma/blas/shdotxf.py.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/unknown_field_set.cc.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/delimited_message_util.cc.o
[ 17%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_core_u8s8s32x_1x1_conv_kernel.cpp.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/field_comparator.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/field_mask_util.cc.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/datapiece.cc.o
[ 17%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_batch_normalization.cpp.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/default_value_objectwriter.cc.o
[ 17%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_sse42_1x1_convolution.cpp.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/error_listener.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/field_mask_utility.cc.o
Compiling  transport/net.cu                    > /home/feng/pytorch/build/nccl/obj/transport/net.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 17%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_core_u8s8s32x_wino_convolution.cpp.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/json_escaping.cc.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/json_objectwriter.cc.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/json_stream_parser.cc.o
[ 18%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/object_writer.cc.o
[ 19%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx2_convolution.cpp.o
[ 19%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_common_1x1_convolution.cpp.o
Scanning dependencies of target nnpack
[ 19%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/proto_writer.cc.o
[ 19%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/init.c.o
[ 19%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/convolution-inference.c.o
[ 19%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/protostream_objectsource.cc.o
[ 19%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/fully-connected-inference.c.o
[ 20%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/pooling-output.c.o
[ 20%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/relu-output.c.o
[ 20%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/protostream_objectwriter.cc.o
[ 20%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/type_info.cc.o
[ 20%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/softmax-output.c.o
[ 20%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/fully-connected-output.c.o
[ 20%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/relu-input-gradient.c.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 20%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/convolution-input-gradient.c.o
[ 20%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/type_info_test_helper.cc.o
[ 20%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/convolution-kernel-gradient.c.o
[ 20%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/convolution-output.c.o
[ 20%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/x86_64-fma/softmax.c.o
[ 20%] Linking C static library ../../lib/libnnpack.a
[ 20%] Built target nnpack
[ 20%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/utility.cc.o
[ 20%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/cpu_reorder.cpp.o
[ 20%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/json_util.cc.o
[ 20%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/message_differencer.cc.o
[ 20%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/time_util.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 20%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/type_resolver_util.cc.o
Compiling  transport/net_socket.cu             > /home/feng/pytorch/build/nccl/obj/transport/net_socket.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 20%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/wire_format.cc.o
[ 20%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/wrappers.pb.cc.o
[ 20%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx2_1x1_conv_kernel_f32.cpp.o
[ 20%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_common_conv_kernel.cpp.o
[ 20%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/gemm_convolution_utils.cpp.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 20%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_lrn.cpp.o
[ 20%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ref_softmax.cpp.o
[ 20%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_transpose_src_utils.cpp.o
[ 20%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ref_rnn.cpp.o
[ 20%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ref_eltwise.cpp.o
[ 20%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ref_convolution.cpp.o
[ 20%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/nspc_batch_normalization.cpp.o
[ 20%] Linking CXX static library ../../../lib/libprotobuf.a
[ 20%] Built target libprotobuf
[ 20%] Generating ../../../../third_party/protobuf/src/google/protobuf/compiler/js/well_known_types_embed.cc
Scanning dependencies of target libprotoc
[ 20%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_core_conv_winograd_kernel_f32.cpp.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 20%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/code_generator.cc.o
[ 20%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/command_line_interface.cc.o
Compiling  transport/net_ib.cu                 > /home/feng/pytorch/build/nccl/obj/transport/net_ib.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 20%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_enum.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_enum_field.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_extension.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_field.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_file.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_generator.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_helpers.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_map_field.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_message.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_message_field.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_padding_optimizer.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_primitive_field.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_service.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_string_field.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_doc_comment.cc.o
Compiling  collectives/all_reduce.cu           > /home/feng/pytorch/build/nccl/obj/collectives/all_reduce.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_enum.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_enum_field.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_field_base.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_generator.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_helpers.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_map_field.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_message.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_message_field.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_primitive_field.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_reflection_class.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_repeated_enum_field.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_repeated_message_field.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_repeated_primitive_field.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_source_generator_base.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_wrapper_field.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_context.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_doc_comment.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_enum.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_enum_field.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_enum_field_lite.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_enum_lite.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_extension.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_extension_lite.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_field.cc.o
Compiling  collectives/all_gather.cu           > /home/feng/pytorch/build/nccl/obj/collectives/all_gather.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_file.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_generator.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_generator_factory.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_helpers.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_lazy_message_field.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_lazy_message_field_lite.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_map_field.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_map_field_lite.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_message.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_message_builder.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_message_builder_lite.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_message_field.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_message_field_lite.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_message_lite.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_name_resolver.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_primitive_field.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_primitive_field_lite.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_service.cc.o
Compiling  collectives/broadcast.cu            > /home/feng/pytorch/build/nccl/obj/collectives/broadcast.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_shared_code_generator.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_string_field.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_string_field_lite.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_enum.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_enum_field.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_extension.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_field.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_file.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_generator.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_helpers.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_map_field.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_message.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_message_field.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_primitive_field.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/js/js_generator.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/js/well_known_types_embed.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_enum.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_enum_field.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_extension.cc.o
Compiling  collectives/reduce.cu               > /home/feng/pytorch/build/nccl/obj/collectives/reduce.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_field.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_file.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_generator.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_helpers.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_map_field.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_message.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_message_field.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_oneof.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_primitive_field.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/php/php_generator.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/plugin.cc.o
[ 25%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/plugin.pb.cc.o
[ 25%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/python/python_generator.cc.o
[ 25%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/ruby/ruby_generator.cc.o
[ 25%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/subprocess.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 25%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/zip_writer.cc.o
Compiling  collectives/reduce_scatter.cu       > /home/feng/pytorch/build/nccl/obj/collectives/reduce_scatter.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 25%] Linking CXX shared library ../../../../lib/libmkldnn.so
[ 25%] Built target mkldnn
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 25%] Linking CXX static library ../../../lib/libprotoc.a
[ 25%] Built target libprotoc
Scanning dependencies of target protoc
[ 25%] Building CXX object third_party/protobuf/cmake/CMakeFiles/protoc.dir/__/src/google/protobuf/compiler/main.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 25%] Linking CXX executable ../../../bin/protoc
[ 25%] Built target protoc
Scanning dependencies of target gen_onnx_proto
[ 25%] Running C++/Python protocol buffer compiler on /home/feng/pytorch/caffe2/proto/prof_dag.proto
[ 25%] Running C++/Python protocol buffer compiler on /home/feng/pytorch/caffe2/proto/predictor_consts.proto
[ 25%] Running C++/Python protocol buffer compiler on /home/feng/pytorch/caffe2/proto/torch.proto
[ 25%] Running C++/Python protocol buffer compiler on /home/feng/pytorch/caffe2/proto/caffe2.proto
[ 25%] Running C++/Python protocol buffer compiler on /home/feng/pytorch/caffe2/proto/hsm.proto
[ 25%] Running C++/Python protocol buffer compiler on /home/feng/pytorch/caffe2/proto/metanet.proto
[ 25%] Running C++/Python protocol buffer compiler on /home/feng/pytorch/caffe2/proto/caffe2_legacy.proto
[ 25%] Running gen_proto.py on onnx/onnx.in.proto
Processing /home/feng/pytorch/third_party/onnx/onnx/onnx.in.proto
Writing /home/feng/pytorch/build/third_party/onnx/onnx/onnx_onnx_torch.proto
Compiling  all_reduce.cu                       > /home/feng/pytorch/build/nccl/obj/collectives/device/all_reduce_sum.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
Writing /home/feng/pytorch/build/third_party/onnx/onnx/onnx_onnx_torch.proto3
Writing /home/feng/pytorch/build/third_party/onnx/onnx/onnx.pb.h
generating /home/feng/pytorch/build/third_party/onnx/onnx/onnx_pb.py
[ 25%] Running C++ protocol buffer compiler on /home/feng/pytorch/build/third_party/onnx/onnx/onnx_onnx_torch.proto
Scanning dependencies of target Caffe2_PROTO
[ 25%] Building CXX object caffe2/proto/CMakeFiles/Caffe2_PROTO.dir/caffe2_legacy.pb.cc.o
[ 25%] Building CXX object caffe2/proto/CMakeFiles/Caffe2_PROTO.dir/caffe2.pb.cc.o
[ 25%] Building CXX object caffe2/proto/CMakeFiles/Caffe2_PROTO.dir/predictor_consts.pb.cc.o
[ 25%] Building CXX object caffe2/proto/CMakeFiles/Caffe2_PROTO.dir/prof_dag.pb.cc.o
[ 25%] Building CXX object caffe2/proto/CMakeFiles/Caffe2_PROTO.dir/hsm.pb.cc.o
[ 25%] Building CXX object caffe2/proto/CMakeFiles/Caffe2_PROTO.dir/metanet.pb.cc.o
[ 25%] Building CXX object caffe2/proto/CMakeFiles/Caffe2_PROTO.dir/torch.pb.cc.o
[ 25%] Built target gen_onnx_proto
[ 25%] Running gen_proto.py on onnx/onnx-operators.in.proto
Processing /home/feng/pytorch/third_party/onnx/onnx/onnx-operators.in.proto
Writing /home/feng/pytorch/build/third_party/onnx/onnx/onnx-operators_onnx_torch.proto
Writing /home/feng/pytorch/build/third_party/onnx/onnx/onnx-operators_onnx_torch.proto3
Writing /home/feng/pytorch/build/third_party/onnx/onnx/onnx-operators.pb.h
generating /home/feng/pytorch/build/third_party/onnx/onnx/onnx_operators_pb.py
[ 25%] Running C++ protocol buffer compiler on /home/feng/pytorch/build/third_party/onnx/onnx/onnx-operators_onnx_torch.proto
Scanning dependencies of target onnx_proto
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx_proto.dir/onnx/onnx_onnx_torch.pb.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx_proto.dir/onnx/onnx-operators_onnx_torch.pb.cc.o
[ 26%] Linking CXX static library ../../lib/libonnx_proto.a
[ 26%] Built target onnx_proto
Scanning dependencies of target onnx
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/version_converter/convert.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/function.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/controlflow/defs.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/data_type_utils.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/version_converter/helper.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/__/__/caffe2/onnx/torch_ops/schema.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/nn/defs.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/__/__/caffe2/onnx/torch_ops/defs.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/nn/old.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/logical/defs.cc.o
[ 26%] Built target Caffe2_PROTO
Scanning dependencies of target Caffe2_perfkernels_avx2
[ 26%] Building CXX object caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx2.dir/embedding_lookup_avx2.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/logical/old.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/reduction/defs.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/generator/defs.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/generator/old.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/tensor/defs.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/tensor/old.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/schema.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/traditionalml/defs.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/math/defs.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/math/old.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/rnn/defs.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/rnn/old.cc.o
[ 27%] Building CXX object caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx2.dir/math_cpu_avx2.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/experiments/defs.cc.o
[ 27%] Building CXX object caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx2.dir/common_avx2.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/experiments/experiments_functions.cc.o
[ 27%] Building CXX object caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx2.dir/typed_axpy_avx2.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/onnxifi_utils.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/common/ir_pb_converter.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/common/status.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/common/assertions.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/common/model_helpers.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/common/interned_strings.cc.o
[ 27%] Building CXX object caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx2.dir/embedding_lookup_fused_8bit_rowwise_avx2.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/checker.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/shape_inference/implementation.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/optimizer/pass_registry.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/optimizer/optimize.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/optimizer/pass_manager.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/optimizer/pass.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
Scanning dependencies of target caffe2_protos
[ 27%] Linking CXX static library ../lib/libcaffe2_protos.a
[ 27%] Built target caffe2_protos
Scanning dependencies of target Caffe2_perfkernels_avx
[ 27%] Building CXX object caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx.dir/adagrad_avx.cc.o
[ 28%] Building CXX object caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx.dir/typed_axpy_avx.cc.o
[ 28%] Building CXX object caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx.dir/common_avx.cc.o
[ 28%] Built target Caffe2_perfkernels_avx
[ 28%] Built target Caffe2_perfkernels_avx2
[ 28%] Linking CXX static library ../../lib/libonnx.a
[ 28%] Built target onnx
Scanning dependencies of target caffe2
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
Compiling  broadcast.cu                        > /home/feng/pytorch/build/nccl/obj/collectives/device/broadcast_sum.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
ptxas warning : Too big maxrregcount value specified 96, will be ignored
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
Compiling  reduce.cu                           > /home/feng/pytorch/build/nccl/obj/collectives/device/reduce_sum.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 28%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/DLConvertor.cpp.o
[ 28%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/TensorUtils.cpp.o
[ 28%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUGenerator.cpp.o
[ 28%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/TensorGeometry.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/UndefinedType.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/Utils.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/Context.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUGeneral.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUTypeDefault.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/SparseTensorImpl.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/ExpandUtils.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/detail/ComplexHooksInterface.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/detail/CPUGuardImpl.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/detail/CUDAHooksInterface.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/cpu/FlushDenormal.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/Storage.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/context_base.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/thread_pool.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/ATenCoreTest.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/StorageImpl.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/Formatting.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/LegacyTypeDispatch.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/register_symbols.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/VariableHooksInterface.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/Tensor.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/interned_strings.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/TensorOptions.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/Range.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/ivalue.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/Allocator.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/ATenGeneral.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/UndefinedTensorImpl.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/Scalar.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/blob.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/intrusive_ptr.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/TensorImpl.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/UniqueVoidPtr.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/type.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/OptionsGuard.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Embedding.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/SoftMax.cpp.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/DispatchStub.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Dropout.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/TensorCompare.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/LinearAlgebra.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/GridSampler.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/ReduceOps.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/UnaryOps.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Linear.cpp.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Indexing.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/PixelShuffle.cpp.o
Compiling  all_gather.cu                       > /home/feng/pytorch/build/nccl/obj/collectives/device/all_gather_sum.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/TensorIterator.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/LegacyBridge.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/TypeProperties.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Copy.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Loss.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/EmbeddingBag.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Normalization.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/WeightNorm.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Resize.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Convolution.cpp.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/RoiPooling.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/PackedSequence.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/LegacyDefinitions.cpp.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Distance.cpp.o
Compiling  reduce_scatter.cu                   > /home/feng/pytorch/build/nccl/obj/collectives/device/reduce_scatter_sum.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/TensorFactories.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/ConvolutionTBC.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/SummaryOps.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/TensorTransformations.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Unique.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/TensorIteratorReduce.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Pooling.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Distributions.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/SpectralOps.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Memory.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/TensorProperties.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Scalar.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/TensorShape.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/BinaryOps.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Activation.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/LossCTC.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/RNN.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/ConstantPadNd.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/TensorConversions.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/BatchLinearAlgebra.cpp.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/sparse/SparseTensorMath.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/sparse/SparseTensor.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/mkl/LinearAlgebra.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/mkl/SpectralOps.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/mkldnn/Conv.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUByteType.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUCharType.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUCopy.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUDoubleType.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUFloatType.cpp.o
Compiling  all_reduce.cu                       > /home/feng/pytorch/build/nccl/obj/collectives/device/all_reduce_prod.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUHalfType.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUIntType.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPULongType.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUShortType.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/RegisterCPU.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/SparseCPUByteType.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/SparseCPUCharType.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/SparseCPUDoubleType.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/SparseCPUFloatType.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/SparseCPUIntType.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/SparseCPULongType.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/SparseCPUShortType.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/TypeDefault.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THGeneral.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THAllocator.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THSize.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THStorageFunctions.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THTensor.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THTensorCopy.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THTensorRandom.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THTensorMath.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THTensorMoreMath.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THTensorEvenMoreMath.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THTensorConv.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THTensorLapack.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THBlas.cpp.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THLapack.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THLogAdd.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THRandom.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THFile.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THDiskFile.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THMemoryFile.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THVector.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/vector/AVX.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/vector/AVX2.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/THNN/init.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp.AVX2.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/Activation.cpp.AVX2.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/SoftMaxKernel.cpp.AVX2.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/GridSamplerKernel.cpp.AVX2.cpp.o
Compiling  broadcast.cu                        > /home/feng/pytorch/build/nccl/obj/collectives/device/broadcast_prod.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp.AVX2.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp.AVX2.cpp.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
Compiling  reduce.cu                           > /home/feng/pytorch/build/nccl/obj/collectives/device/reduce_prod.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp.AVX2.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/TensorCompareKernel.cpp.AVX2.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp.AVX.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/Activation.cpp.AVX.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/SoftMaxKernel.cpp.AVX.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/GridSamplerKernel.cpp.AVX.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp.AVX.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp.AVX.cpp.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp.AVX.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/TensorCompareKernel.cpp.AVX.cpp.o
Compiling  all_gather.cu                       > /home/feng/pytorch/build/nccl/obj/collectives/device/all_gather_prod.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp.DEFAULT.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/Activation.cpp.DEFAULT.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/SoftMaxKernel.cpp.DEFAULT.cpp.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/GridSamplerKernel.cpp.DEFAULT.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp.DEFAULT.cpp.o
Compiling  reduce_scatter.cu                   > /home/feng/pytorch/build/nccl/obj/collectives/device/reduce_scatter_prod.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp.DEFAULT.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp.DEFAULT.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/TensorCompareKernel.cpp.DEFAULT.cpp.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/mkldnn/Runtime.cpp.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/aten/aten_op.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/gloo/allgather_ops.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/gloo/allreduce_ops.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/gloo/barrier_ops.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/gloo/broadcast_ops.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/gloo/common.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/gloo/common_world_ops.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/gloo/context.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/gloo/reduce_scatter_ops.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/gloo/store_handler.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/script/lexer.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/script/compiler.cc.o
Compiling  all_reduce.cu                       > /home/feng/pytorch/build/nccl/obj/collectives/device/all_reduce_min.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/init_intrinsics_check.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/module.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/net_dag_utils.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/event.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/graph.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/plan_executor.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/int8_serialization.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/tensor_int8.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/context.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/prof_dag_counters.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/blob_stats.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/qtensor.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/net_simple.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/memonger.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/net.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/transform.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/operator_c10wrapper.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/context_base.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/common.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/blob_serialization.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/net_async_tracing.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/workspace.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/tensor_impl.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/init_omp.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/operator.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/init.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/operator_schema.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/tensor.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/net_async_base.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/net_simple_refcount.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/stats.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/types.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/allocator.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/net_dag.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/numa.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/net_async_scheduling.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/qtensor_serialization.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/db.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/proto_convert.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/proto_wrap.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/proto_utils.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/murmur_hash3.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/smart_tensor_printer.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/signal_handler.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/string_utils.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/threadpool/ThreadPool.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/cpuid.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/bench_utils.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/math_cpu.cc.o
Compiling  broadcast.cu                        > /home/feng/pytorch/build/nccl/obj/collectives/device/broadcast_min.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/math_utils.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/thread_name.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/threadpool/pthreadpool.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/threadpool/pthreadpool_impl.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/predictor/predictor.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/predictor/predictor_utils.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/predictor/predictor_config.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/nomnigraph/tests/test_util.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/nomnigraph/Representations/NeuralNet.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/db/create_db_op.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/db/protodb.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/distributed/file_store_handler.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
Compiling  reduce.cu                           > /home/feng/pytorch/build/nccl/obj/collectives/device/reduce_min.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/distributed/file_store_handler_op.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/distributed/store_handler.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/distributed/store_ops.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/utils/ideep_register.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/dropout_op.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/pool_op.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/momentum_sgd_op.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/local_response_normalization_op.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/relu_op.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/elementwise_sum_op.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/squeeze_op.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/utility_ops.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/conv_fusion_op.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/operator_fallback_ideep.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/concat_split_op.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/spatial_batch_norm_op.cc.o
Compiling  all_gather.cu                       > /home/feng/pytorch/build/nccl/obj/collectives/device/all_gather_min.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/fully_connected_op.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/conv_op.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/mpi/mpi_common.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
Compiling  reduce_scatter.cu                   > /home/feng/pytorch/build/nccl/obj/collectives/device/reduce_scatter_min.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/mpi/mpi_ops.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/observers/time_observer.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/observers/runcnt_observer.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/onnx/backend.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/onnx/onnx_exporter.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/onnx/backend_rep.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/onnx/helper.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/onnx/onnxifi_init.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/onnx/device.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/lpnorm_op.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/gru_unit_op.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sparse_normalize_op.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/while_op.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/unique_ops.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/feed_blob_op.cc.o
Compiling  all_reduce.cu                       > /home/feng/pytorch/build/nccl/obj/collectives/device/all_reduce_max.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/index_ops.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/clip_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_div_gradient_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/glu_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reservoir_sampling.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/dropout_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/resize_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/roi_align_rotated_gradient_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/tensor_protos_db_input.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sparse_to_dense_mask_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/pool_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/relu_n_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/instance_norm_gradient_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reciprocal_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/leaky_relu_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/distance_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/ensure_cpu_output_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/locally_connected_op_util.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/partition_ops.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/selu_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/do_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/lengths_reducer_fused_8bit_rowwise_ops.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/free_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/lengths_top_k_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/batch_bucketize_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/heatmap_max_keypoint_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/filler_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/softsign_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/conv_op_shared.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/enforce_finite_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/local_response_normalization_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/moments_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sparse_to_dense_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/data_couple.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/tt_linear_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/arg_ops.cc.o
Compiling  broadcast.cu                        > /home/feng/pytorch/build/nccl/obj/collectives/device/broadcast_max.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/assert_op.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
Compiling  reduce.cu                           > /home/feng/pytorch/build/nccl/obj/collectives/device/reduce_max.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/feature_maps_ops.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/if_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/acos_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/relu_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/accumulate_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/spatial_softmax_with_loss_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/listwise_l2r_op.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sigmoid_op.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/cast_op.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/integral_image_op.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/flexible_top_k.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_sub_op.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sequence_ops.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/remove_data_blocks_op.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/h_softmax_op.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/expand_squeeze_dims_op.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/slice_op.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reduce_front_back_max_ops.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/conv_gradient_op.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/stats_put_ops.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/bbox_transform_op.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/collect_and_distribute_fpn_rpn_proposals_op.cc.o
Compiling  all_gather.cu                       > /home/feng/pytorch/build/nccl/obj/collectives/device/all_gather_max.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reshape_op.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/prepend_dim_op.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/roi_align_gradient_op.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sqrt_op.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/segment_reduction_op.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_div_op.cc.o
Compiling  reduce_scatter.cu                   > /home/feng/pytorch/build/nccl/obj/collectives/device/reduce_scatter_max.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/byte_weight_dequant_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_sum_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/upsample_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/ensure_clipped_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/cosh_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/margin_ranking_criterion_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/roi_pool_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/tanh_gradient_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/lp_pool_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/pow_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/rowmul_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/rsqrt_op.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/roi_align_rotated_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/bisect_percentile_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/percentile_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/normalize_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sin_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/batch_moments_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/lstm_unit_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/jsd_op.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
Compiling  functions.cu                        > /home/feng/pytorch/build/nccl/obj/collectives/device/functions.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/pad_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sigmoid_gradient_op.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/string_ops.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/create_scope_op.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/lengths_reducer_rowwise_8bit_ops.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
nvlink fatal   : Internal error: reference to deleted section
Makefile:83: recipe for target '/home/feng/pytorch/build/nccl/obj/collectives/device/devlink.o' failed
make[5]: *** [/home/feng/pytorch/build/nccl/obj/collectives/device/devlink.o] Error 1
Makefile:45: recipe for target 'devicelib' failed
make[4]: *** [devicelib] Error 2
Makefile:25: recipe for target 'src.build' failed
make[3]: *** [src.build] Error 2
CMakeFiles/nccl_external.dir/build.make:110: recipe for target 'nccl_external-prefix/src/nccl_external-stamp/nccl_external-build' failed
make[2]: *** [nccl_external-prefix/src/nccl_external-stamp/nccl_external-build] Error 2
CMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/nccl_external.dir/all' failed
make[1]: *** [CMakeFiles/nccl_external.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/lengths_reducer_ops.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/deform_conv_op.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/gather_ranges_to_dense_op.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/conv_transpose_op_mobile.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/batch_matmul_op.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/crf_viterbi_op.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/thresholded_relu_op.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/replace_nan_op.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/mean_op.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/stylizer_ops.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/is_empty_op.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/channel_stats_op.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sinusoid_position_encoding_op.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/apmeter_op.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_ops.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/key_split_ops.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/square_root_divide_op.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reduce_ops.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/softmax_shared.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_mul_op.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/roi_align_op.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/minmax_gradient_ops.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/deform_conv_gradient_op.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reduce_front_back_sum_ops.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/transpose_op.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/text_file_reader_utils.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elu_op.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reduce_front_back_mean_ops.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/workspace_ops.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/text_file_reader.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/given_tensor_fill_op.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/utility_ops.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/communicator_op.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/find_duplicate_elements_op.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/abs_op.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/cbrt_op.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/batch_box_cox_op.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sqr_op.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/last_n_window_collector.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/tanh_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/length_split_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/gather_fused_8bit_rowwise_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/onnxifi_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/logit_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/boolean_mask_ops.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/stop_gradient.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/asin_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/batch_sparse_to_dense_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/channel_shuffle_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/counter_ops.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sinh_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_mul_gradient_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/ngram_ops.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/accuracy_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reduction_ops.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/matmul_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/rank_loss_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/softplus_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/conditional_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/loss_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/boolean_unmask_ops.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/weighted_multi_sampling_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/fused_rowwise_random_quantization_ops.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reciprocal_gradient_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/softmax_with_loss_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/numpy_tile_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/piecewise_linear_transform_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/space_batch_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/floor_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/log_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_logical_ops.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_add_gradient_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/ceil_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/index_hash_ops.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/weighted_sample_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/atomic_ops.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/one_hot_ops.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_ops_schema.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/merge_id_lists_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/shape_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/perplexity_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/normalize_l1_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/map_ops.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/concat_split_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/load_save_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/find_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/top_k.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/group_norm_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/spatial_batch_norm_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/tile_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/box_with_nms_limit_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/tan_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/channel_backprop_stats_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/fully_connected_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/zero_gradient_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/stump_func_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/fused_rowwise_8bit_conversion_ops.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/negative_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/softmax_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/half_float_ops.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/ctc_beam_search_decoder_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/given_tensor_byte_string_to_uint8_fill_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/onnx_while_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/negate_gradient_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/pool_gradient_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/flatten_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_ops_utils.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/summarize_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/minmax_ops.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/lengths_tile_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/im2col_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/generate_proposals_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/conv_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/stats_ops.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/order_switch_ops.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reverse_packed_segs_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/norm_planar_yuv_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/exp_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/cos_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/atan_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/ctc_greedy_decoder_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/scale_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/conv_transpose_gradient_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/instance_norm_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/prelu_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/batch_gather_ops.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/expand_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/cosine_embedding_criterion_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/pack_segments.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_add_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/multi_class_accuracy_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/copy_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/mod_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/dataset_ops.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/affine_channel_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/lengths_pad_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/spatial_batch_norm_gradient_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/cross_entropy_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/locally_connected_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/rmac_regions_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/variable_length_sequence_padding.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/hard_sigmoid_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/pack_rnn_sequence_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/fc_inference.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/conv_op_eigen.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/gather_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/swish_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_sub_gradient_op.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/conv_transpose_op.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quant_decode_op.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_linear_op.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/layer_norm_op.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/cube_op.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/layer_norm.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/batch_matmul.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/flatten.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/averaged_loss.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/add.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/batch_gather.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/filler.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/enforce_finite.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/expand_dims.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/stop_gradient.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/cast.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/mul.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/sigmoid.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/relu.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/fc.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/sparse_lengths_sum.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/concat.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/sigmoid_cross_entropy_with_logits.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/expand_dims_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/add_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/fc_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/concat_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/relu_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/batch_matmul_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/flatten_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/sparse_lengths_sum_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/sigmoid_cross_entropy_with_logits_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/sigmoid_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/filler_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/averaged_loss_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/cast_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/stop_gradient_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/batch_gather_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/enforce_finite_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/mul_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/rnn/recurrent_network_op.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/rnn/recurrent_network_executor.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/rnn/recurrent_network_blob_fetcher_op.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/init_qnnpack.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_add_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_average_pool_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_channel_shuffle_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_concat_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_conv_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_conv_transpose_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_dequantize_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_fc_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_flatten_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_given_tensor_fill_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_leaky_relu_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_max_pool_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_quantize_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_relu_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_reshape_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_resize_nearest_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_roi_align_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_slice_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_sigmoid_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_softmax_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/distributed.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/optimize_ideep.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/distributed_converter.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/dead_code_elim.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/mobile.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/optimizer.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/sink.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/onnxifi_transformer.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/passes.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/annotations.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/converter.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/backend_cutting.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/fusion.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/device.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/perfkernels/typed_axpy.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/perfkernels/fused_8bit_rowwise_embedding_lookup.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/perfkernels/adagrad.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/perfkernels/embedding_lookup.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/perfkernels/math_cpu_base.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/queue/queue_ops.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/queue/rebatching_queue_ops.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/queue/rebatching_queue.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/queue/blobs_queue.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/queue/blobs_queue_db.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/momentum_sgd_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/adam_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/learning_rate_adaption_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/clip_tensor_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/yellowfin_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/wngrad_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/gftrl_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/iter_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/learning_rate_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/ftrl_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/adadelta_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/lars_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/adagrad_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/rmsprop_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/share/contrib/nnpack/conv_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/share/contrib/depthwise/depthwise3x3_conv_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/transforms/single_op_transform.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/transforms/conv_to_nnpack_transform.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/transforms/pattern_net_transform.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/transforms/common_subexpression_elimination.cc.o
[ 57%] Linking CXX shared library ../lib/libcaffe2.so
[ 57%] Built target caffe2
Makefile:138: recipe for target 'all' failed
make: *** [all] Error 2
Failed to run 'bash ../tools/build_pytorch_libs.sh --use-cuda --use-nnpack --use-mkldnn --use-qnnpack caffe2'
<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:

1.
1.
1.

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


 - PyTorch Version (e.g., 1.0):
 - OS (e.g., Linux):
 - How you installed PyTorch (, , source):
 - Build command you used (if compiling from source):
 - Python version:
 - CUDA/cuDNN version:
 - GPU models and configuration:
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->
",,"['Who can help me ? thinks\r\nubuntu16.04 cuda8.0 cudnn6.0', 'duplicate of https://github.com/pytorch/pytorch/issues/14178\r\n\r\nplease stop opening the same issue again']","['\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n']","['conda', 'pip']",0,0
417,pytorch,20579,closed,Exception when adding custom scalars in tensorboard,"## üêõ Exception when adding custom scalars in tensorboard

An exception is raised when adding custom scalars using 



## To Reproduce
Minimal code to reproduce:


Running this results in:


## Environment

Please copy and paste the output from our


## Additional context

The exception message is quite clear, the required type is  but we have a , this happens [here](https://github.com/pytorch/pytorch/blob/09f22d10a695bfba8ffb3327b9920fd3358c00ee/torch/utils/tensorboard/summary.py#L381):


Should be enough to remove the surrounding square brackets, PR is coming.",module: tensorboard triaged,"['@baldassarreFe The fix is correct, will you make a PR?', ""@lanpa of course, I submitted the PR shortly after opening the issue and I saw you approved it yesterday. Let's just wait for a merge and we can close the issue ;)""]","[""python\r\nfrom torch.utils.tensorboard import SummaryWriter\r\n\r\nwith SummaryWriter() as writer:\r\n    writer.add_custom_scalars({'Stuff': {\r\n        'Losses': ['MultiLine', ['loss/(one|two)']],\r\n        'Metrics': ['MultiLine', ['metric/(three|four)']],\r\n    }})\r\n"", '\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 3, in <module>\r\n  File ""/some/path/python3.7/site-packages/torch/utils/tensorboard/writer.py"", line 733, in add_custom_scalars\r\n    self._get_file_writer().add_summary(custom_scalars(layout))\r\n  File ""/some/path/python3.7/site-packages/torch/utils/tensorboard/summary.py"", line 383, in custom_scalars\r\n    smd = SummaryMetadata(plugin_data=PluginData)\r\nTypeError: Parameter to MergeFrom() must be instance of same class: expected tensorboard.SummaryMetadata.PluginData got list.\r\n', '\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04) 7.4.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1050 Ti with Max-Q Design\r\nNvidia driver version: 418.43\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.3\r\n[pip] torch==1.1.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl_fft                   1.0.12           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] pytorch                   1.1.0           py3.7_cuda10.0.130_cudnn7.5.1_0    pytorch\r\n', ""python\r\n    PluginData = [SummaryMetadata.PluginData(plugin_name='custom_scalars')]  # <--\r\n    smd = SummaryMetadata(plugin_data=PluginData)\r\n""]","['TypeError: Parameter to MergeFrom() must be instance of same class: expected tensorboard.SummaryMetadata.PluginData got list.', 'tensorboard.SummaryMetadata.PluginData', 'list']",0,0
418,pytorch,6318,closed,Bug of ByteTensor,"Here is an example which I want to do:

But this code will encounter an issue, which raise

So how can I judge if a Tensor in a list?
",,"['Adding more context, I am able to reproduce this on master. `a in c` works but `b in c` throws the above error.', '@SsnL Ôºåthis is what I am saying, the `b in c` will raise an error ', ""So, the reason why it works in one case but not the other is the following.\r\nAccording to the python doc, [`x in y` can be translated to](https://docs.python.org/3/reference/expressions.html#membership-test-details)\r\n```python\r\nany(x is e or x == e for e in y)\r\n```\r\nin the first case, `a is c[0]` is True, but in the second case it's not, so it compares `b == c[0]`, which returns a 3d tensor which can't be cast to a single bool.\r\n\r\nYou have a few possibilities:\r\n1 - if you only want to see if the exact same tensor (no clones) is in the list\r\n```python\r\nif any(a is e for e in c):\r\n```\r\n2 - if you want to accept copies:\r\n```python\r\nif any(a is e or a.equal(e) for e in c):\r\n```\r\n\r\nPyTorch behavior is compatible with numpy's in this case btw, and I don't think we will be changing it.""]",['python\r\nimport torch\r\na = torch.randn(3)\r\nb = torch.randn(3)\r\nc = [a]\r\nif a in c:\r\n    .....\r\n......\r\nif b in c:\r\n    .....\r\n'],['RuntimeError: bool value of non-empty torch.ByteTensor objects is ambiguous'],0,0
419,pytorch,7645,closed,Max pooling behavior for nan values,"## Issue description

max pooling functions are not consistent with max functions.
Below an example, every max pooling (be it 1d, 2d or 3d, adaptive or not) acts the same, on cpu or on cuda.

Essentially, there are two fondamental differences :
 - max pooling of all  values is  while for  it's 
 - max pooling of nan and valid values is valid values, which means s get ignored, while for , as soon as there is a  value, the result is .

More generally, choosing explicetely how to deal with  as in numpy ([e.g.](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.nanmax.html)) could be a solution, but maybe this is related to CuDNN's max pooling ?

## Code example



## System Info

Built from latest sources (as of 05/17)
PyTorch version: 0.5.0a0+331a04d
Is debug build: No
CUDA used to build PyTorch: 9.1.85

OS: Ubuntu 16.04.4 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
CMake version: version 3.9.4

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.1.85
GPU models and configuration: GPU 0: Quadro M1000M
Nvidia driver version: 390.30
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.3
/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a

Versions of relevant libraries:
[conda] magma-cuda91              2.3.0                         1    pytorch
[conda] torch                     0.5.0a0+331a04d           <pip>
[conda] torch                     0.3.1b0+2b47480           <pip>
[conda] torch                     0.5.0a0+4251e38           <pip>

",,"[""Hi, @ClementPinard . Thank you for providing the example. I'll take a look."", 'Hmhm. I seem to get a double free in the `F.adaptive_max_pool1d(a, 1).backward()`...', 'Another question: Do we really want to provide an option to ignore NaN? My view is that ""if you have nan in your net, you\'re screwed"", so I would just return NaN there, personally.\r\n', 'My practical usecase is for KITTI groundtruth depthmaps and FlowMaps which are 2D sparse arrays.\r\nFor an algorithm that use FlowNet-like architecture that outputs predictions at multiple scale levels, we can either compare predictions to downscaled GT or upscaled predictions to GT.\r\n\r\nThe first being obviously less computationally expensive, the ignore NaN would help downscaling such sparse 2D maps.\r\n\r\nFor the moment we do something I find ugly, you can see it  [here](https://github.com/ClementPinard/FlowNetPytorch/blob/master/multiscaleloss.py#L19)\r\nEssentially it zeroes the `nan`s, takes the map and construct two maps of positive and negative values which are then maxpooled and added back together.\r\n\r\nI am actually open for a strict ""no-nan"" policy on pooling functions, but in that case better enforce it before someone writes a code that tries to benefit from maxpooling ignore `nan`s feature/bug\r\n\r\nand if you have a clever way of pooling sparse 2D tensors, I\'m open to it, but I guess it\'s a topic for pytorch forums ;)', ""Personally, I think it is more sane have NaN -> NaN in the pooling and offer a parametrizable `nan_to_num` (where you get to pick the values) that does `torch.where(torch.isnan(x), torch.tensor(float('-inf')), x)` or so.\r\n"", ""So the proposed fix does NaN -> NaN similar to max. I didn't try to fix gradients for the nan case. This would involve keeping the values and I don't think that is worth it (in particular because I would not expect the pooling layer to be last, and otherwise we'd probably get NaN as grad_out).\r\nIf you are reasonably happy with it, I'd move it to a PR."", '+1 for NaN -> NaN,   _""abyssus abyssum invocat""_ \r\nWe could also add an optional mask which would be a ByteTensor of the same size, specifying whether or not the considered pixel is used for the pooling, that could be used for any kind of pulling. An `ignore_nan` functionality would then be to provide the mask `input != input`', 'I could work with that.\r\nto continue to be picky, the potential drawback is that it won\'t work for other pooling methods such as average pooling or median pooling. The problem here is that the max operation inherently ignores nonmax values which can be leveraged for ""ignore some pixels"" operation, but it has a ""non universal"" feel to it since it woun\'t be as easy for other kinds of 2D operations.', 'To be clear, the fix is good for me, but I figured a related discussion on selective pooling (whether to ditch NaN values or anything you want to ignore) could happen (maybe not on this issue ?)\r\n\r\nIt would be convenient for my use case, but maybe for others too.', 'fixed via https://github.com/pytorch/pytorch/pull/7670']","[""python\r\na= torch.full((1,1,2), float('nan'))\r\nprint(a.max())  # tensor(nan)\r\nprint(F.adaptive_max_pool1d(a, 1)) # tensor([[[-3.4028e+38]]])\r\n\r\na.requires_grad=True\r\na.max().backward()\r\nprint(a.grad) # tensor([[[ 0., 0.]]])\r\nF.adaptive_max_pool1d(a, 1).backward()\r\nprint(a.grad) # tensor([[[ 0., 0.]]])\r\n\r\nwith torch.no_grad():\r\n    a[0,0,0] = 0\r\n\r\na.max().backward()\r\nprint(a.grad) # tensor([[[ 0., 0.]]])\r\nF.adaptive_max_pool1d(a, 1).backward()\r\nprint(a.grad) # tensor([[[ 1., 0.]]])\r\n""]","['nan', '-THINF', 'max', 'nan', 'nan', 'max', 'nan', 'nan', 'nan']",0,0
420,pytorch,22135,closed,[dataloader] type annotation is broken,In #19228 I did not update the type annotations. We should fix this.,module: dataloader module: typing triaged,"[""I've also found that bunch of optional parameters are not labeled with `Optional`:\r\n`dataloader.pyi:`\r\n```python\r\ndef __init__(self, dataset: Dataset[T_co], batch_size: int=..., shuffle: bool=..., sampler: Sampler[int]=...,\r\n                 num_workers: int=..., collate_fn: _collate_fn_t=..., pin_memory: bool=...,\r\n                 drop_last: bool=..., timeout: float=..., worker_init_fn: _worker_init_fn_t=...) -> None: ...\r\n\r\ndef __init__(self, dataset: Dataset[T_co], batch_sampler: Sampler[Sequence[int]]=..., num_workers: int=...,\r\n                 collate_fn: _collate_fn_t=..., pin_memory: bool=..., timeout: float=...,\r\n                 worker_init_fn: _worker_init_fn_t=...) -> None: ...\r\n```\r\n`dataloader.py`:\r\n```python\r\ndef __init__(self, dataset, batch_size=1, shuffle=False, sampler=None,\r\n                 batch_sampler=None, num_workers=0, collate_fn=default_collate,\r\n                 pin_memory=False, drop_last=False, timeout=0,\r\n                 worker_init_fn=None):\r\n```\r\n\r\nSo these are `sampler`, `batch_sampler` and `worker_init_fn`."", 'I think this is fixed by a couple recent PRs']",[],[],0,0
421,pytorch,24006,closed,result random,"abosolutely same pytorch code:
when I use 1 gpu to train the model, every time results are same
But when i use mutli gpus to train the model, every time result are random, 
pazzled!",,[],[],[],0,0
422,pytorch,22298,closed,[c++] Can't use Sequential inside Sequential ,"Hi. I was implementing MNASNet for C++ side in torchvision and I ran into this problem. apparently using  inside  is not possible. Here is the minimal code:



@yf225 Can you have a look?",module: cpp module: nn triaged,"['Apparently the problem is that nn::Sequential has templated forward function.', '@soumith Can anyone else have a look? This is keeping me from completing MNASNet', '@ShahriarSS I will look into it', '@ShahriarSS Could you share the MNASNet example that you are working on? `nn::Sequential` in C++ API is not nestable by design, but we might be able to find a workaround for this issue.', '@yf225 C++ version:\r\n```\r\n    layers->push_back(torch::nn::Conv2d(\r\n        Options(3, 32, 3).padding(1).stride(2).with_bias(false)));\r\n    layers->push_back(torch::nn::BatchNorm(\r\n        torch::nn::BatchNormOptions(32).momentum(BN_MOMENTUM)));\r\n    layers->push_back(torch::nn::Functional(modelsimpl::relu_));\r\n    layers->push_back(torch::nn::Conv2d(\r\n        Options(32, 32, 3).padding(1).stride(1).groups(32).with_bias(false)));\r\n    layers->push_back(torch::nn::BatchNorm(\r\n        torch::nn::BatchNormOptions(32).momentum(BN_MOMENTUM)));\r\n    layers->push_back(torch::nn::Functional(modelsimpl::relu_));\r\n    layers->push_back(torch::nn::Conv2d(\r\n        Options(32, 16, 1).padding(0).stride(1).with_bias(false)));\r\n    layers->push_back(torch::nn::BatchNorm(\r\n        torch::nn::BatchNormOptions(16).momentum(BN_MOMENTUM)));\r\n\r\n    layers->push_back(stack(16, depths[0], 3, 2, 3, 3, BN_MOMENTUM));\r\n    layers->push_back(stack(depths[0], depths[1], 5, 2, 3, 3, BN_MOMENTUM));\r\n    layers->push_back(stack(depths[1], depths[2], 5, 2, 6, 3, BN_MOMENTUM));\r\n    layers->push_back(stack(depths[2], depths[3], 3, 1, 6, 2, BN_MOMENTUM));\r\n    layers->push_back(stack(depths[3], depths[4], 5, 2, 6, 4, BN_MOMENTUM));\r\n    layers->push_back(stack(depths[4], depths[5], 3, 1, 6, 1, BN_MOMENTUM));\r\n\r\n    layers->push_back(torch::nn::Conv2d(\r\n        Options(depths[5], 1280, 1).padding(0).stride(1).with_bias(false)));\r\n    layers->push_back(torch::nn::BatchNorm(\r\n        torch::nn::BatchNormOptions(1280).momentum(BN_MOMENTUM)));\r\n    layers->push_back(torch::nn::Functional(modelsimpl::relu_));\r\n\r\n    classifier = torch::nn::Sequential(torch::nn::Dropout(dropout),\r\n                                       torch::nn::Linear(1280, num_classes));\r\n```\r\n\r\nPython code:\r\n```\r\n        layers = [\r\n            # First layer: regular conv.\r\n            nn.Conv2d(3, 32, 3, padding=1, stride=2, bias=False),\r\n            nn.BatchNorm2d(32, momentum=_BN_MOMENTUM),\r\n            nn.ReLU(inplace=True),\r\n            # Depthwise separable, no skip.\r\n            nn.Conv2d(32, 32, 3, padding=1, stride=1, groups=32, bias=False),\r\n            nn.BatchNorm2d(32, momentum=_BN_MOMENTUM),\r\n            nn.ReLU(inplace=True),\r\n            nn.Conv2d(32, 16, 1, padding=0, stride=1, bias=False),\r\n            nn.BatchNorm2d(16, momentum=_BN_MOMENTUM),\r\n            # MNASNet blocks: stacks of inverted residuals.\r\n            _stack(16, depths[0], 3, 2, 3, 3, _BN_MOMENTUM),\r\n            _stack(depths[0], depths[1], 5, 2, 3, 3, _BN_MOMENTUM),\r\n            _stack(depths[1], depths[2], 5, 2, 6, 3, _BN_MOMENTUM),\r\n            _stack(depths[2], depths[3], 3, 1, 6, 2, _BN_MOMENTUM),\r\n            _stack(depths[3], depths[4], 5, 2, 6, 4, _BN_MOMENTUM),\r\n            _stack(depths[4], depths[5], 3, 1, 6, 1, _BN_MOMENTUM),\r\n            # Final mapping to classifier input.\r\n            nn.Conv2d(depths[5], 1280, 1, padding=0, stride=1, bias=False),\r\n            nn.BatchNorm2d(1280, momentum=_BN_MOMENTUM),\r\n            nn.ReLU(inplace=True),\r\n        ]\r\n```\r\nThe problem is with the stack function that returns a Sequential. Here is stack:\r\n```\r\ntorch::nn::Sequential stack(int64_t input, int64_t output, int64_t kernel,\r\n                            int64_t stride, double exp_factor, int64_t repeats,\r\n                            double bn_momentum)\r\n{\r\n    assert(repeats >= 1);\r\n\r\n    torch::nn::Sequential seq;\r\n    seq->push_back(MNASNetInvertedResidual(input, output, kernel, stride,\r\n                                           exp_factor, bn_momentum));\r\n\r\n    for (int64_t i = 1; i < repeats; ++i)\r\n        seq->push_back(MNASNetInvertedResidual(output, output, kernel, 1,\r\n                                               exp_factor, bn_momentum));\r\n\r\n    return seq;\r\n}\r\n```\r\n\r\nI can wrap the inner sequential inside another Module but the output python model must be loadable in C++ model.', '@ShahriarSS Is the Python code also implemented by us? I think wrapping the inner sequential inside another Module would be a good solution for this problem, but the Python model needs to have the same structure as well.', '@yf225 yes the python model exists in torchvision. I can start a PR changing the python code. Should I?\r\n@fmassa  what do you think?', ""@fmassa Do you think it's okay to wrap any inner `nn.Sequential` inside another `nn.Module` for torchvision Python models? This is mostly because of a fundamental limitation in C++ frontend - in order to support the `nn::Sequential` returning any type, its `forward()` method has to be templatized, which prevents `nn::Sequential` from being nested.\r\n\r\nAn alternative approach would be to not support `nn::Sequential` returning any type (and only return Tensor type), but I think we should support it because the same works in Python frontend:\r\n```python\r\nclass M(torch.nn.Module):\r\n    def forward(self, x):\r\n        return 1\r\n\r\nmodel = torch.nn.Sequential(\r\n    torch.nn.Linear(3, 5),\r\n    M(),\r\n)\r\n\r\na = model(torch.randn(3))\r\n```"", '@yf225 How about this?\r\n\r\n```\r\nstruct StackSequentailImpl : torch::nn::SequentialImpl\r\n{\r\n    using SequentialImpl::SequentialImpl;\r\n\r\n    torch::Tensor forward(torch::Tensor x)\r\n    {\r\n        return SequentialImpl::forward(x);\r\n    }\r\n};\r\n\r\nTORCH_MODULE(StackSequentail);\r\n\r\nStackSequentail stack(int64_t input, int64_t output, int64_t kernel,\r\n                      int64_t stride, double exp_factor, int64_t repeats,\r\n                      double bn_momentum)\r\n```\r\n\r\nAnd then we can use StackSequential inside the main Sequential.', ""@ShahriarSS I think adding `StackSequential` as a custom module in the example is a good idea, as currently we don't want to un-templatize the forward function of `nn::Sequential`. In the future, if we have more user requests for nested `nn::Sequential` without custom wrapper, we might consider adding `nn::Sequential` with special return types, such as `nn::TensorSequential`, although it would be a divergence from the Python API because Python side doesn't have it."", ""> @fmassa Do you think it's okay to wrap any inner nn.Sequential inside another nn.Module for torchvision Python models? \r\n\r\nNo, we shouldn't do this for Python / torchvision\r\n\r\nI would actually propose that we de-templatize `nn::Sequential` and assume it returns a `Tensor`, and then introduce a `nn::MultiSequential` or something that allows returning anything"", '@yf225 What is the course of action here? If the changes that @soumith proposed are as simple as renaming ```nn::Sequential``` to ```nn::MultiSequential``` and adding ```nn::Sequential``` under, wrapping it I can make the changes.', ""@ShahriarSS I think we should take the following actions here:\r\n\r\n1. Rename the current `nn::Sequential` to `nn::AnySequential` (which might be more consistent with the naming of other modules such as `AnyModule`).\r\n2. Add a new `nn::Sequential` class that wraps `nn::AnySequential` and only returns a `Tensor` from its `forward` method.\r\n\r\nAlso, in order to notify existing `nn::Sequential` users what happens and help them migrate to the new `nn::AnySequential` API, ideally we should throw a `static_assert` error when the user does `float output = sequential->forward<float>(input)`, and in the error message tell them to change `sequential`'s class to `nn::AnySequential`."", '@yf225 I will start a PR fixing this. Also how will we do an ```static_assert``` when forward function of ```nn::Sequential``` is not a template? There will be a compilation error when using forward function as template.', '@ShahriarSS I am not entirely sure if this would work, but perhaps we could make `forward` function of `nn::Sequential` contain a template type with a default value, and then in the function body check if the template type is equal to the default value. This way we can catch the `float output = sequential->forward<float>(input)` use cases. Note that the return type of `forward` of `nn::Sequential` is still `Tensor`, so nesting `nn::Sequential` should work.', 'Discussed with @ShahriarSS offline, and the resolution is:\r\n\r\nIf you need to use `nn::Sequential`s inside each other you can subclass `nn::Sequential` and write a non-templatized forward function for it. You can checkout https://github.com/pytorch/vision/blob/2f46070f3cb1ea894d82578f3dc5677f82f34958/torchvision/csrc/models/mnasnet.cpp#L59 for an example on how to do this.\r\n\r\nDocs for this resolution is added by https://github.com/pytorch/pytorch/pull/23939.']","['nn::Sequential', 'nn::Sequential', '\r\ntorch::nn::Sequential layers;\r\nauto S = torch::nn::Sequential();\r\nlayers->push_back(S);\r\n']",[],0,0
423,pytorch,26189,closed,"[ONNX] bool comparison with 0,1 should export False,True constants","## üêõ Bug

Should be able to compare boolean to 0,1 and exporter maybe should then use Fale,True as constants. But [comparing bool to 0 results in error](https://github.com/pytorch/pytorch/issues/25805)

## To Reproduce



## Expected behavior

export 0 as False constant to be valid ONNX

## Environment

PyTorch version: 1.2.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 16.04.6 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CMake version: version 3.14.0

Python version: 2.7
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: GeForce GTX 1070
Nvidia driver version: 418.40.04
cuDNN version: /usr/local/lib/libcudnn.so.5.1.10

Versions of relevant libraries:
[pip] mictorch==0.0.1
[pip] numpy==1.16.4
[pip] torch==1.2.0
[pip] torchvision==0.4.0
[conda] Could not collect

## Additional context

Work around is:

",module: onnx triaged,"['PR for fix: https://github.com/pytorch/pytorch/pull/26292', '@dashesy this should be fixed. Could you please try and close this issue if possible?', 'This is fixed in the nightly, thanks']","['python\r\na = b > c\r\nna = a == 0\r\n', 'python\r\na = b > c\r\nna = a == torch.as_tensor(False)\r\n']",[],0,0
424,pytorch,16288,closed,[JIT] Unable to append a tuple of tensor to a list,"## üêõ Bug

JIT is unable to append a  to a 

## To Reproduce



",oncall: jit,"[""Empty lists are assumed to have the type `List[Tensor]`, for this you'd have to do `torch.jit.annotate(List[Tuple[Tensor, Tensor]], []`\r\n\r\nThis works on master for me:\r\n```python\r\nclass ListOfTupleOfTensor(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super(ListOfTupleOfTensor, self).__init__()\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, x):\r\n        # type: (Tensor) -> List[Tuple[Tensor, Tensor]]\r\n\r\n        returns = torch.jit.annotate(List[Tuple[Tensor, Tensor]], [])\r\n        for i in range(10):\r\n            returns.append((x, x))\r\n\r\n        return returns\r\n\r\nc = ListOfTupleOfTensor()\r\nprint(c.graph)\r\n```\r\n\r\nThe error message could definitely be better though, I'll leave this issue open to track that"", 'Do you need to import List, Tuple or Tensor?', 'The type annotation on `torch.jit.annotate` is parsed directly in the TorchScript compiler so technically no', 'So this will break when if I use PYTORCH_JIT=0?\r\n\r\nIs there a way to keep the code working with `PYTORCH_JIT=0`? Cos I often turn off PYTORCH JIT during debugging', 'This seems to do it \r\n\r\n```\r\nimport torch\r\nfrom torch.jit import Tensor\r\nfrom typing import List, Tuple\r\n\r\nclass ListOfTupleOfTensor(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super(ListOfTupleOfTensor, self).__init__()\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, x):\r\n        # type: (Tensor) -> List[Tuple[Tensor, Tensor]]\r\n\r\n        returns = torch.jit.annotate(List[Tuple[Tensor, Tensor]], [])\r\n        for i in range(10):\r\n            returns.append((x, x))\r\n\r\n        return returns\r\n\r\nc = ListOfTupleOfTensor()\r\nprint(c(torch.zeros(10)))\r\n\r\n```\r\n\r\n@driazati , does ^ look ok?', ""Yup that's the recommended way"", 'This seems to be resolved. Feel free to reopen if you have additional issues']","['\r\nimport torch\r\nclass ListOfTupleOfTensor(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super(ListOfTupleOfTensor, self).__init__()\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, x):\r\n        # type: (Tensor) -> List[Tuple[Tensor, Tensor]]\r\n\r\n        returns = []\r\n        for i in range(10):\r\n            returns.append((x, x))\r\n\r\n        return returns\r\n\r\n\r\nc = ListOfTupleOfTensor()\r\nprint c.graph\r\n', ""\r\nRuntimeError: \r\narguments for call are not valid:\r\n  \r\n  for operator aten::append(Tensor[] self, Tensor el) -> Tensor[]:\r\n  expected a value of type Tensor for argument 'el' but found (Tensor, Tensor)\r\n  @torch.jit.script_method\r\n  def forward(self, x):\r\n      # type: (Tensor) -> List[Tuple[Tensor, Tensor]]\r\n  \r\n      returns = []\r\n      for i in range(10):\r\n          returns.append((x, x))\r\n                          ~~~~ <--- HERE\r\n  \r\n      return returns\r\n  \r\n  for operator aten::append(int[] self, int el) -> int[]:\r\n  expected a value of type int[] for argument 'self' but found Tensor[]\r\n  @torch.jit.script_method\r\n  def forward(self, x):\r\n      # type: (Tensor) -> List[Tuple[Tensor, Tensor]]\r\n  \r\n      returns = []\r\n      for i in range(10):\r\n          returns.append((x, x))\r\n          ~~~~~~~~~~~~~~ <--- HERE\r\n  \r\n      return returns\r\n  \r\n  for operator aten::append(float[] self, float el) -> float[]:\r\n  expected a value of type float[] for argument 'self' but found Tensor[]\r\n  @torch.jit.script_method\r\n  def forward(self, x):\r\n      # type: (Tensor) -> List[Tuple[Tensor, Tensor]]\r\n  \r\n      returns = []\r\n      for i in range(10):\r\n          returns.append((x, x))\r\n          ~~~~~~~~~~~~~~ <--- HERE\r\n  \r\n      return returns\r\n  \r\n  for operator aten::append(t[] self, t el) -> t[]:\r\n  could not match type (Tensor, Tensor) to t in argument 'el': type variable 't' previously matched to type Tensor is matched to type (Tensor, Tensor)\r\n  @torch.jit.script_method\r\n  def forward(self, x):\r\n      # type: (Tensor) -> List[Tuple[Tensor, Tensor]]\r\n  \r\n      returns = []\r\n      for i in range(10):\r\n          returns.append((x, x))\r\n                          ~~~~ <--- HERE\r\n  \r\n      return returns\r\n  \r\n  for operator aten::append(bool[] self, bool el) -> bool[]:\r\n  expected a value of type bool[] for argument 'self' but found Tensor[]\r\n  @torch.jit.script_method\r\n  def forward(self, x):\r\n      # type: (Tensor) -> List[Tuple[Tensor, Tensor]]\r\n  \r\n      returns = []\r\n      for i in range(10):\r\n          returns.append((x, x))\r\n          ~~~~~~~~~~~~~~ <--- HERE\r\n  \r\n      return returns\r\nfor call at:\r\n@torch.jit.script_method\r\ndef forward(self, x):\r\n    # type: (Tensor) -> List[Tuple[Tensor, Tensor]]\r\n\r\n    returns = []\r\n    for i in range(10):\r\n        returns.append((x, x))\r\n        ~~~~~~~~~~~~~~ <--- HERE\r\n\r\n    return returns\r\n\r\n""]","['Tuple[Tensor, Tensor]', 'List[Tuple[Tensor, Tensor]]']",0,0
425,pytorch,4048,closed,"Feature request: ModuleDict, like ModuleList","Self-explanatory. Currently there is no easy way to maintain a dictionary as an attribute of a Module whose values are themselves Modules that need to be registered. Introducing a ModuleDict class that functions the same as a ModuleList, except it exposes a dictionary interface, would resolve this. This has the following practical benefits for users:

1) Checkpointing is much better with a ModuleDict that has strings as keys and Modules as values than a ModuleList. I have personally run into problems when I change the order of modules in my ModuleList and try to load old checkpoints, because the checkpoint key names are order-dependent with ModuleLists, so it breaks my code.

2) It combines the advantages of a ModuleList (dynamic assignment of Modules to a class) with the advantages of using named attributes (documentation / readability / sane module names in checkpoints).

Another pro is that this is very simple to implement. Here is a first pass, if there is support for this feature I will clean it up and submit this as a PR:

",,"['You should also allow the named_module() generator as an input to the update function.  Also would be good to have this with parameters as well.', 'Thanks for the proposal. I‚Äôm wondering if there‚Äôs any reason why you can just use nn.Module and regular attribute access for this purpose?', '@apaszke, I think you are right to say that the normal nn.Module should be used for this, but based on the documentation it is not obvious that one can use the nn.Module for this purpose.  I think if there was an example in the nn.ModuleList and/or nn.Module documentation that showed how nn.Module could be used as a ""ModuleList dictionary"", then one wouldn\'t need or request a nn.ModuleDict.', ""Here is another proposal:\r\n\r\nWhy not create a `nn.ModuleIterable` (or something like that) that wraps an iterable into a `nn.Module`, like what is currently done in `nn.ModuleList`, and deprecate `nn.ModuleList`?\r\n\r\nI'm not sure if it would be best to add a number of `nn.Module`s for different container types.\r\n\r\nWhat do you think?"", 'I‚Äôm not sure how does this solve the problem or how is it different than ModuleList? Can you please elaborate on your idea?', ""Implementation-wise this would be very similar to what we currently have for `nn.ModuleList`, eventually bookeeping the type of the iterable that was passed so that we can provide *nice* printings and indexing access.\r\n\r\nThis solves the problem because every iterable that you want to pass you just need to pass that you want to record in the model (for `.parameters()`, `.cuda()`, etc) to this common module.\r\n\r\nIs such a thing necessary? Not necessarily, but [it was already handy in the past](https://discuss.pytorch.org/t/list-of-nn-module-in-a-nn-module/219).\r\n\r\nDo we need such abstraction? Maybe not, as we can subclass a `nn.Module` to implement such funcionality. But for users, might be better to do\r\n```python\r\nbranch1 = {}\r\nbranch1['conv1'] = nn.Conv2d(...)\r\nbranch1['conv2'] = nn.Conv2d(...)\r\nbranch1['conv3'] = nn.Conv2d(...)\r\nself.branch1 = nn.FromIterable(branch1)\r\n\r\n# and then later on\r\nfor module_name in ['conv1', 'conv2', 'conv3']:\r\n# or for module in self.branch1 if we branch1 was OrderedDict\r\n    output = F.relu(self.branch1[module_name]) + 1.0\r\n```\r\n\r\ninstead of having to write a class just for holding the `conv1`/`conv2`/`conv3`.\r\n\r\nLet me know if I wasn't clear."", ""I prefer a dict interface to using regular attribute access with nn.Module because in my case, the names of the modules are only known at runtime. (They are dependent on the config file). I have a multi-headed network with several heads doing different tasks; each head is a Module that I'd like to keep as the value of a dictionary whose key is the task name.\r\n\r\nFurthermore, due to the way my data is labeled, I can only backprop through one head at a time. For each image I only have one of the tasks labeled. So each minibatch requires forwarding through a different subset of the network. This is a primary reason I'm using PyTorch, and I think multi-task learning setups like this will only get more common. To choose which subset of the network the data gets forwarded through, I use a string provided by the data loader: ideally the string would just be the key for the heads ModuleDict.\r\n\r\nIt's possible to do this using getattr and setattr, but given the amount of magic PyTorch does for these functions in nn.Module under the hood, it's preferable to have a simple dict abstraction. It seems like one of PyTorch's design goals is first-class support for dynamic graphs -- if that's true then a ModuleDict is a natural addition."", 'I agree it is not necessary, but does seem like a ""nice to have"" addition.\r\n\r\nAnother point, is that nn.ModuleDict is more natural than nn.ModuleList, because modules and parameters are stored in an OrderedDict already.', ""@fmassa I guess my confusion comes from the fact that `Iterable` in Python means just a (possibly lazy) sequence of items, with specified order and without the mapping structure. This is exactly what `ModuleList` provides.\r\n\r\nI wouldn't say that it's more natural - both data structures have their own different purposes, but after giving it some thought I think I'm ok with adding `ModuleDict`, so please send a PR."", ""@apaszke maybe my confusion comes from the fact that\r\n```python\r\nisinstance({}, collections.Iterable) == True\r\n```\r\nAnyway, my idea was just to unite all those new modules in a single one, so that we don't grow it for each new container type (`ModuleNamedTuple`, `ModuleSet`, etc).\r\n\r\nBut that's just a minor comment."", ""Well that's because iterable is just something you can iterate over, but e.g. iterating over a dict only gives you keys by default (iterable doesn't imply that you need to recover the full container structure). In general, every available container is different in some way, and we would probably want to keep these differences when enabling their use as containers for modules (e.g. `insert(n, item)` is only valid for lists, `keys` is only valid for dicts). I don't think there's a good way to unify them."", ""Also, lists and dicts are kind of fundamental. We won't be getting a named tuple or a set for sure (because comparison on modules is not very well defined, and you'd be better off using a dict with meaningful keys)."", ""I have a use case very similar to what has been described by @rkaplan . I may have N encoders and M decoders depending on the configuration file which defines several datasets let's say. I would like to select a specific encoder and/or decoder based on a string that'll change during training.\r\n\r\nThe easiest way I'd imagine for doing this is sth like below in the `init` of the custom `Module`:\r\n```\r\nself.encoders = {}\r\nfor enc in requested_encoders:\r\n  setattr(self, 'encoder_%s' % enc, RNN(..))\r\n  self.encoders[enc] = getattr(self, 'encoder_%s' % enc)\r\n\r\n# Or alternatively if the Module could (semi-)automatically detect dicts\r\nself.encoders = {enc:RNN(..) for enc in requested_encoders}\r\n```"", '@ozancaglayan this will break if you try to use it with `DataParallel`. We should just get a `ModuleDict`, but as a workaround, you can keep a module list, and make the dict map names to indices in that list (this will work with `DataParallel`)', 'Just out of curiosity, why does this break DataParallel?', 'because the `self.encoders` dict is the same in all replicas, but each will have different modules (because they need to hold different parameters). So all replicas will try to use the modules from the first one, which will give you a GPU idx mismatch error', ""Sounds good, I'll submit this as a PR sometime in the next few days"", 'Any hope for the PR soon? I find this thing quite useful.', ""Sorry for the delay, I kept having all sorts of trouble trying to get PyTorch to build on my Mac. I'll just spin up an EC2 instance and do it I guess. Thanks for the bump."", ""I am facing very similar issue with @rkaplan , where I am training attributes and having a head module for each attribute, the reason I prefer using ModuleDict is the attribute string contains multiple words, so if I don't map attribute names to some unique names, I can't use normal way to register each head.\r\n\r\nFinally i realized i can just use `add_module()` actually and maintain an OrderedDict in both labels and  features."", 'Sooo?', ""Hi, personally was looking for something like this when creating a custom  bidirectional rnn so that I could store an arbitrary number of dicts consisting of pairs of forwards and backwards cells in a modulelist and iterate over it in the forward loop.\r\n\r\n\r\n```\r\nnum_layers = 10\r\nlayers = nn.ModuleList()\r\n\r\nfor layer in range(num_layers):\r\n    final_layer =   layer == num_layers-1\r\n    paired_layerdict =  ModuleDict(\r\n                       {  'forw' : nn.LSTMCell( in_feats ,  hid_feats)  ,  \r\n                          'back' : nn.LSTMCell( in_feats ,  hid_feats)  }\r\n    ) \r\n    paired_layerdict.is_final_layer = final_layer \r\n    layers.append( paired_layerdict )\r\n```\r\nI worked around not having it by using zip and two module lists but then had to get round some issues later on that wouldnt normally be a problem  (eg the forward function wasnt certain whether we were currently in a forwards cell or a backwards cell and whether we were in the final layer). Got around it but overall  felt the final solution was a bit clunky and inelegant, and would have been cleaner with something like ModuleDict"", 'I think this can be closed now.']","['python\r\nclass ModuleDict(Module):\r\n    r""""""Holds submodules in a dict.\r\n    ModuleDict can be indexed like a regular Python dict, but modules it\r\n    contains are properly registered, and will be visible by all Module methods.\r\n    Arguments:\r\n        modules (dict, optional): a list of modules to add\r\n    Example::\r\n        TODO.\r\n    """"""\r\n\r\n    def __init__(self, modules=None):\r\n        super(ModuleDict, self).__init__()\r\n        if modules is not None:\r\n            self.update(modules)\r\n\r\n    def __getitem__(self, key):\r\n        return self._modules[key]\r\n\r\n    def __setitem__(self, key, module):\r\n        return setattr(self, key, module)\r\n\r\n    def __len__(self):\r\n        return len(self._modules)\r\n\r\n    def __iter__(self):\r\n        return iter(self._modules)\r\n\r\n    def keys(self):\r\n        return self._modules.keys()\r\n\r\n    def items(self):\r\n        return self._modules.items()\r\n\r\n    def values(self):\r\n        return self._modules.values()\r\n\r\n    def get(self, key, default=None):\r\n        if default is not None:\r\n            return self._modules.get(key, default)\r\n        return self._modules.get(key)\r\n\r\n    def update(self, modules):\r\n        r""""""Updates modules from a Python dict.\r\n        Arguments:\r\n            modules (dict): dict of modules to append\r\n        """"""\r\n        if not isinstance(modules, dict):\r\n            raise TypeError(""ModuleDict.update should be called with a ""\r\n                            ""dict, but got "" + type(modules).__name__)\r\n        for key, module in modules.items():\r\n            self.add_module(key, module)\r\n        return self\r\n']",[],0,0
426,pytorch,16806,closed,storage_initialized() ASSERT FAILED when I execute tensor.resize_(-1),"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->

When I execute , I get a run-time error message that tells me to report the bug: . Because  is not a valid argument to , it is reasonable that we get an error. However, the above error message seems to suggest that this error is not handled properly in the source code.  

## To Reproduce

Steps to reproduce the behavior:

1. 
2. 
3. 

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

A proper error message should have been issued. 

## Environment

 - PyTorch Version (e.g., 1.0): 1.0.0
 - OS (e.g., Linux): Windows 10
 - How you installed PyTorch (, , source): pip
 - Build command you used (if compiling from source): N/A
 - Python version: 3.6.2
 - CUDA/cuDNN version: N/A
 - GPU models and configuration: N/A
 - Any other relevant information: N/A
",module: bootcamp triaged,"['Thanks for the report! Happy to review a PR on this.', '@ezyang Hey, could I try working on this one? Seems simple enough to get started. :)', '@nmilosev yes go for it! Add me as reviewer when you do it', '@ezyang this seems to be fixed already.\r\n\r\n```\r\n>>> x = torch.Tensor([1, 2, 3, 4])\r\n>>> x.resize_(-1)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/milosevicn/contrib/pytorch/torch/tensor.py"", line 84, in __repr__\r\n    return torch._tensor_str._str(self)\r\n  File ""/home/milosevicn/contrib/pytorch/torch/_tensor_str.py"", line 310, in _str\r\n    tensor_str = _tensor_str(self, indent)\r\n  File ""/home/milosevicn/contrib/pytorch/torch/_tensor_str.py"", line 209, in _tensor_str\r\n    formatter = _Formatter(get_summarized_data(self) if summarize else self)\r\n  File ""/home/milosevicn/contrib/pytorch/torch/_tensor_str.py"", line 79, in __init__\r\n    tensor_view = tensor.reshape(-1)\r\nRuntimeError: Trying to create tensor with negative dimension -1: [-1]\r\n```\r\n\r\nProbably close this issue then, I will find something else suitable, or you can point me to it. I\'ve got the whole env ready :)', ""Thanks. Maybe take a look at this one: https://github.com/pytorch/pytorch/issues/14959 (there's a few parts, but one easy bit is putting the device guards in torch namespace, somewhere).""]",[],"['tensor.resize_(-1)', 'RuntimeError: storage_initialized() ASSERT FAILED at ..\\aten\\src\\ATen/core/TensorImpl.h:656, please report a bug to PyTorch', '-1', 'resize_()', 'import torch', 'x = torch.Tensor([1, 2, 3, 4])', 'x.resize_(-1)', 'conda', 'pip']",0,0
427,pytorch,7396,closed,[JIT][script] Implement `train` on ScriptModules,,oncall: jit,['Closed by https://github.com/pytorch/pytorch/pull/11505'],[],[],0,0
428,pytorch,27255,closed,[JIT] list comprehensions over tensors does not work,"## üêõ Bug

Following fails to compile:



List comprehensions over other iterables works; just not tensors.


cc @suo",jit-backlog oncall: jit,[],['\r\n@torch.jit.script\r\ndef fn(x):\r\n     return [img.shape[-2:] for img in zip(x)]\r\n'],[],0,0
429,pytorch,4872,closed,Memory leak CUDNN 7003,"I updated pytorch to 0.3 using conda. I realized that there is a memory leak in RAM in my experiments. Later, I tried to compile from source python with CUDNN 6... and 7005, other versions of CUDNN solved the problem. Is it possible to update CUDNN (from 7003 to 7005) in conda/pip packages? 

",,"['Any chance to get a small script that would let us reproduce the leak?', '@sanek777 0.3.1 will ship with 7005', '@soumith Okey, i think that this is good news. Closing problem.']",[],[],0,0
430,pytorch,3514,closed,Unable to use cat on torch.LongTensor,"Hello again,

I'm trying to concatenate two tensor together using ¬†applied on a Tensor that I have created from a Numpy array. Here is the example:



Instead if you try with this it works:


What's wrong?

Cheers,
Alessandro",,"[""@aleSuglia it's not a bug, `torch.zeros(x.size())` returns a `torch.FloatTensor` and `x` is a `torch.LongTensor`. You need to convert `x` to `long()` as well.\r\n\r\n`torch.cat((x, torch.zeros(x.size()).type_as(x)))` or alternatively `torch.cat((x, x.new(*x.size()).fill_(0)))`""]","[""\r\nIn [1]: import torch\r\n\r\nIn [2]: import numpy as np\r\n\r\nIn [3]: x = torch.LongTensor(np.random.randint(1, 20, (1, 1, 4)))\r\n\r\nIn [4]: torch.cat((x, torch.zeros(x.size())))\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-4-4ddca689077c> in <module>()\r\n----> 1 torch.cat((x, torch.zeros(x.size())))\r\n\r\nTypeError: cat received an invalid combination of arguments - got (tuple), but expected one of:\r\n * (sequence[torch.LongTensor] seq)\r\n      didn't match because some of the arguments have invalid types: (tuple)\r\n * (sequence[torch.LongTensor] seq, int dim)\r\n"", '\r\nIn [5]: a = torch.randn(1,1,4)\r\n\r\nIn [6]: a\r\nOut[6]:\r\n\r\n(0 ,.,.) =\r\n -1.1397  0.0766  1.3821 -0.4036\r\n[torch.FloatTensor of size 1x1x4]\r\n\r\nIn [7]: torch.cat((a, torch.zeros(a.size())))\r\nOut[7]:\r\n\r\n(0 ,.,.) =\r\n -1.1397  0.0766  1.3821 -0.4036\r\n\r\n(1 ,.,.) =\r\n  0.0000  0.0000  0.0000  0.0000\r\n[torch.FloatTensor of size 2x1x4]\r\n']",['torch.cat'],0,0
431,pytorch,31611,closed,the example program using libtorch is not linked against torch_cuda when USE_CUDA is ON,"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->

Looks like https://github.com/pytorch/pytorch/issues/15992 is coming back again after the split of the torch library.

## To Reproduce

Steps to reproduce the behavior:

1. The following script throws ""Torch is not linked against CUDA"" and gets 0 as output.

2. The following one is working correctly and gets 1 as output.

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

The behaviour keeps consistent in code snippet 1 and code snippet 2.

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


 - PyTorch Version (e.g., 1.0): master
 - OS (e.g., Linux): Windows
 - How you installed PyTorch (, , source): source
 - Build command you used (if compiling from source): python setup.py build
 - Python version: doesn't matter
 - CUDA/cuDNN version: doesn't matter
 - GPU models and configuration: doesn't matter
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->


cc @ezyang @gchanan @zou3519 @ngimel @peterjc123",high priority module: build module: cuda module: windows triaged,"['@ezyang ', 'Is this an issue in 1.4?  I will check.', ""This is not a problem with 1.4, which doesn't have the split change (it got reverted)."", 'This appears to be Windows specific because following the instructions in https://pytorch.org/tutorials/advanced/cpp_frontend.html I get output of ""1"" (correct)', ""I don't know when I'm going to get to this, unassigning myself for now."", 'I just found out that this issue could be solved by using the `/INCLUDE` switch. torch_cuda.dll contains the function signature `?warp_size@cuda@at@@YAHXZ` (`int __cdecl at::cuda::warp_size(void)`). So adding `/INCLUDE:""?warp_size@cuda@at@@YAHXZ""` to the linker flags will force the library/executable to link against `torch_cuda.dll`. What do you think, @ezyang ?\r\n\r\nReference:\r\nhttps://docs.microsoft.com/en-us/cpp/build/reference/include-force-symbol-references?view=vs-2019', ""@peterjc123 Hmm, while I'm not so hot about hard coding one specific symbol (I guess if we document it appropriately it's fine), I don't have any better ideas. SGTM."", ""@ezyang Recently, more and more users are complaining about this. It seems that they like to construct a VS project on their own instead of rely on CMake, which will need to set the flag manually. with MSVC, `#pragma comment` could be used for adding linker flags https://docs.microsoft.com/en-us/cpp/preprocessor/comment-c-cpp?view=vs-2019#linker. But I don't know where should I add it. Is there a header that only exist in CUDA build and will be included by `#include <torch/extension.h>`?"", '@peterjc123 Do you think maybe the strategy here might work? https://github.com/pytorch/vision/issues/2134#issuecomment-625075418\r\n\r\nThen they have to import a CUDA specific header, but that is all.', '@ezyang That one sounds more portable and both methods should be effective.', 'I am noticing the same issue with Linux.  torch::cuda::is_available() returns zero.  \r\n\r\nThere is a weird hack were if you create a dummy network and pass it to c++ data_parallel somehow CUDA is detected again. ', '@powderluv do you mind opening a new issue for this?', 'I have opened https://github.com/pytorch/pytorch/issues/42018']","['cpp\r\n#include <ATen/ATen.h>\r\n#include <torch/torch.h>\r\n#include <iostream>\r\n\r\nint main() {\r\n\tstd::cout << torch::cuda::is_available() << std::endl;\r\n\ttorch::Tensor tensor = at::tensor({ -1, 1 }, at::kCUDA);\r\n        return 0;\r\n}\r\n', 'cpp\r\n#include <ATen/ATen.h>\r\n#include <torch/torch.h>\r\n#include <iostream>\r\n#include <Windows.h>\r\n\r\nint main() {\r\n\tLoadLibraryA(""torch_cuda.dll"");\r\n\tstd::cout << torch::cuda::is_available() << std::endl;\r\n\ttorch::Tensor tensor = at::tensor({ -1, 1 }, at::kCUDA);\r\n        return 0;\r\n}\r\n', '\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n']","['conda', 'pip']",0,0
432,pytorch,2032,closed,Possible bug in RNN module,"
...................................................................................................

Sorry, I have to reopen this because I think this is a bug within the RNN module and I could't get help from discuss.pytorch

I run a similar experiment with one linear layer neural network and the output is correct. So a reasonable explanation would be the RNN module keep different set of parameters (for optimization reason?) and didn't mix up at the end of the batch training. The following code will duplicate this problem.

........................................................................................................

I encounter this problem when I am trying to implement seq2seq to familiarize with this new framework. This issue seems related to parameters sharing in mini-batch. I setup a dummy training set with only one mini-batch. This mini-batch has 3 data entries in it. All with the same input, and different outputs:

training_data = [
[[[4,5,1,7,8],[4,5,1,7,8],[4,5,1,7,8]], (input 1)
[[0],[0],[0]], (input 2)
[[1],[3],[5]]] (target)
]

In theory, the model will never learn this data because the contradiction. However, the loss reach near 0 after only a few hundred epochs. But if I split 3 data entries into 3 mini-batch, the model will not learn the data set which should be the correct result.

So the model must be keeping different set of parameter for each position in mini-batch? And the parameters are not updated to be the same after each mini-batch forward-backward? Can someone tell me if I misunderstood something?

",,"[""This is an interesting question (can't think of an answer right now, but I'd recommend inspecting/printing the actual tensors just before the classification happens and trying to trace back any differences). It would be better suited to https://discuss.pytorch.org, though."", ""closing the issue here based on @jekbradbury 's recommendation that the discussion is better moved to https://discuss.pytorch.org"", ""@jekbradbury @jekbradbury Yes, it's on https://discuss.pytorch.org. I will close this.\r\n@davidmascharka the init_hidden is already in the forward."", ""One possible explanation: \r\n\r\n`enc_inp: 3 x 5`\r\n\r\nAfter going through`Embedding` layer,\r\n\r\n`enc_inp: 3 x 5 x 10`\r\n\r\nIt is of size `batch x seq x features`. But in pytorch, the input to LSTM is of size `seq x batch x features`. You view it as `enc_embeds.view(-1, batch_size, self.embed_dim)`. That's where the problem arises. If you use a mini-batch of size 1, then the batch size doesn't affect the `view` process. Thus you didn't observe that weird behaviour. \r\n\r\nI haven't tested this theory, though."", ""Yes, you can try creating the LSTM with `batch_first=True` if that's the dimension order in your input."", '@ chenzhekl I think you are right. The inputs are probably distorted after view. I will check later.\r\n...............................................................................\r\nI test it and this is indeed the problem, view is not the equivalent of transpose in tensorflow. View in here simply create a new tensor and copy paste everything from the original tensor. Lesson learned']","['\r\nimport torch\r\nimport torch.autograd as autograd\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.optim as optim\r\n\r\n\r\n\r\ntraining_data = [\r\n\t[[[4,5,1,7,8],[4,5,1,7,8],[4,5,1,7,8]], [[0],[0],[0]], [[1],[3],[5]]]\r\n\t]\r\n\r\n\r\ndef prepare_sequence(all_seq):\r\n\treturn autograd.Variable(torch.LongTensor(all_seq)).cuda()\r\n\r\nclass Seq2seqLSTM(nn.Module):\r\n\r\n\tdef __init__(self, vocab_size, target_size, embed_dim, hidden_dim, num_layers):\r\n\r\n\t\tsuper(Seq2seqLSTM, self).__init__()\r\n\t\tself.hidden_dim = hidden_dim\r\n\t\tself.embed_dim = embed_dim\r\n\t\tself.vocab_size = vocab_size\r\n\t\tself.target_size = target_size\r\n\t\tself.num_layers = num_layers\r\n\r\n\t\tself.word_embeddings = nn.Embedding(vocab_size, embed_dim)\r\n\t\tself.encoder = nn.LSTM(embed_dim, hidden_dim, num_layers)\r\n\t\tself.decoder = nn.LSTM(embed_dim, hidden_dim, num_layers)\r\n\r\n\t\tself.curr_hidden = None\r\n\t\tself.hidden2tag = nn.Linear(hidden_dim, target_size)\r\n\r\n\tdef init_hidden(self, batch_size):\r\n\t\treturn (autograd.Variable(torch.zeros(self.num_layers, batch_size, self.hidden_dim)).cuda(),\r\n\t\t\tautograd.Variable(torch.zeros(self.num_layers, batch_size, self.hidden_dim)).cuda())\r\n\r\n\tdef forward(self, enc_seq, dec_seq):\r\n\r\n\t\tbatch_size = enc_seq.size()[0]\r\n\t\tself.curr_hidden = self.init_hidden(batch_size)\r\n\r\n\t\tenc_embeds = self.word_embeddings(enc_seq)\r\n\t\tdec_embeds = self.word_embeddings(dec_seq)\r\n\r\n\t\tenc_out, self.curr_hidden = self.encoder(\r\n\t\t\tenc_embeds.view(-1, batch_size, self.embed_dim), self.curr_hidden)\r\n\t\tdec_out, self.curr_hidden = self.decoder(\r\n\t\t\tdec_embeds.view(-1, batch_size, self.embed_dim), self.curr_hidden)\r\n\r\n\t\ttag_space = self.hidden2tag(dec_out.view(batch_size * len(dec_out), -1))\r\n\t\ttag_scores = F.log_softmax(tag_space)\r\n\t\treturn tag_scores\r\n\r\nEMBED_DIM = 10\r\nHIDDEN_DIM = 10\r\nVOCAB_SIZE = 10\r\nTARGET_SIZE = 10\r\nNUM_LAYERS = 2\r\n\r\nmodel = Seq2seqLSTM(VOCAB_SIZE, TARGET_SIZE, EMBED_DIM, HIDDEN_DIM, NUM_LAYERS).cuda()\r\nloss_function = nn.NLLLoss()\r\noptimizer = optim.Adam(model.parameters())\r\nprint model\r\n\r\nfor epoch in range(1000):\r\n\r\n\tone_batch = map(lambda x:prepare_sequence(x), training_data[epoch%len(training_data)])\r\n\tenc_inp = one_batch[0]\r\n\tdec_inp = one_batch[1]\r\n\ttarget = one_batch[2]\r\n\r\n\tmodel.zero_grad()\r\n\ttag_scores = model(enc_inp, dec_inp)\r\n\r\n\tloss = loss_function(tag_scores, target.view(-1))\r\n\tloss.backward()\r\n\toptimizer.step()\r\n\r\n\tprint loss\r\n']",[],0,0
433,pytorch,27512,closed,[JIT] floordiv not bound to tensor,"## üêõ Bug

floordiv is only bound for int, not tensor, which results in an incorrectly inserted cast..


cc @suo",oncall: jit,[],['\r\ndef test(pos):\r\n    return pos // 2\r\n\r\ninput = torch.tensor(4)\r\nprint(test(input)\r\n# tensor(2)\r\nprint(torch.jit.script(test)(input))\r\n# 2\r\n'],[],0,0
434,pytorch,2620,closed,nn.Embedding error when using max_norm,"
",,"['This has been fixed with the latest version in master, the PR fixing is in https://github.com/pytorch/pytorch/pull/2336', ""i'm cutting a minor release this week for the JIT, it should be in that release.""]","['\r\n>>> torch.__version__\r\n\'0.2.0_3\'\r\n>>> embedding = nn.Embedding(10, 3, max_norm=3, norm_type=2)\r\n>>> input = Variable(torch.LongTensor([[1,2,4,5],[4,3,2,9]]))\r\n>>> embedding(input)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""AAA/venv/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 224, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""AAA/venv/lib/python3.6/site-packages/torch/nn/modules/sparse.py"", line 94, in forward\r\n    self.scale_grad_by_freq, self.sparse\r\n  File ""AAA/venv/lib/python3.6/site-packages/torch/nn/_functions/thnn/sparse.py"", line 48, in forward\r\n    cls._renorm(indices, weight, max_norm, norm_type)\r\nTypeError: _renorm() missing 1 required positional argument: \'norm_type\'\r\n']",[],0,0
435,pytorch,1829,closed,cuda out of memory with size 1 tensor,"Hi, I have just built pytorch on OS X with CUDA support.

I'm having some trouble using CUDA features as it seems to run out of memory even with a size 1 tensor.


My laptop is fairly old, with an NVIDIA GeForce GT 650M 1024 MB. CUDA version is 8.0. Torch version is . OS X is 10.12.4. I am using Python 3.5.3 with Anaconda.",,"[""I experienced that some time ago as well. It's likely not the 1-element tensor that's causing the OOM, but rather THC state (that needs ~220MB) of memory. Rebooting can clear a bit of GPU memory. @ngimel do you know if there's any way to get GPU mem usage on a mac (there's no `nvidia-smi`)?"", ""cudaMemGetInfo memory should still work, I don't know about nvidia-smi alternative."", 'Thanks for the advice. I quickly wrote a small program to access cudaMemGetInfo\r\n```\r\n#include <stdio.h>\r\n#include <cuda_runtime_api.h>\r\n\r\nint main(int argc, char **argv)\r\n{\r\n\r\n  size_t free, total;\r\n\r\n  cudaMemGetInfo(&free, &total);\r\n\r\n  printf(""CUDA memory: free=%d, total=%d\\n"", (int)free, (int)total);\r\n\r\n  return 0;\r\n}\r\n```\r\nand compile it by running\r\n```\r\n> nvcc cuda_mem.cpp -o cuda_mem\r\n```\r\nWhen I run it, I get the following\r\n```\r\n> ./cuda_mem\r\nCUDA memory: free=1515520, total=1073414144\r\n```\r\nSo it seems indeed, there is very little memory free. Since the graphics card is used for the OS, is it possible it is just using up everything available ? (this is not a particularly big graphics card I think.)', 'Your OS is using up ~1GB of video memory. That is a lot. You could probably try switching to the intel gpu for the display and use nvidia gpu for the compute.', ""Thanks for the help. The memory of the GPU was used by my web browser. Things are now running fine. gfxCardStatus helped diagnose which application was hogging the GPU. I'm closing the issue."", 'Hello, I also came across this problem in an 8-GPU machine. I wrote two scripts to check the conditions of GPUs.\r\n\r\n```cpp\r\n#include <iostream>\r\n#include ""cuda.h""\r\n#include ""cuda_runtime_api.h""\r\n  \r\nusing namespace std;\r\n  \r\nint main( void ) {\r\n    int num_gpus;\r\n    size_t free, total;\r\n    cudaGetDeviceCount( &num_gpus );\r\n    for ( int gpu_id = 0; gpu_id < num_gpus; gpu_id++ ) {\r\n        cudaSetDevice( gpu_id );\r\n        int id;\r\n        cudaGetDevice( &id );\r\n        cudaMemGetInfo( &free, &total );\r\n        cout << ""GPU "" << id << "" memory: free="" << free << "", total="" << total << endl;\r\n    }\r\n    return 0;\r\n}\r\n```\r\n\r\nThis one gives reasonable results since I have not run any program in the GPUs. \r\n\r\n```bash\r\nGPU 0 memory: free=16488464384, total=16945512448\r\nGPU 1 memory: free=16488464384, total=16945512448\r\nGPU 2 memory: free=16488464384, total=16945512448\r\nGPU 3 memory: free=16488464384, total=16945512448\r\nGPU 4 memory: free=16488464384, total=16945512448\r\nGPU 5 memory: free=16488464384, total=16945512448\r\nGPU 6 memory: free=16488464384, total=16945512448\r\nGPU 7 memory: free=16488464384, total=16945512448\r\n```\r\n\r\nHowever, when I tried to create a `torch.cuda.FloatTensor` with only one element, one card (GPU 4) is out of memory...\r\n\r\n```python\r\nimport torch\r\nimport numpy as np\r\n \r\nif __name__ == \'__main__\':\r\n    x = np.random.randn(1)\r\n    try:\r\n        t = torch.cuda.FloatTensor(x)\r\n        print(\'Success!\')\r\n    except Exception as e:\r\n        print(e)\r\n```\r\n\r\nThe execution results are as follows.\r\n\r\n```bash\r\n$ CUDA_VISIBLE_DEVICES=0 python3 check.py \r\nSuccess!\r\n$ CUDA_VISIBLE_DEVICES=1 python3 check.py \r\nSuccess!\r\n$ CUDA_VISIBLE_DEVICES=2 python3 check.py \r\nSuccess!\r\n$ CUDA_VISIBLE_DEVICES=3 python3 check.py \r\nSuccess!\r\n$ CUDA_VISIBLE_DEVICES=4 python3 check.py \r\nCUDA error: out of memory\r\n$ CUDA_VISIBLE_DEVICES=5 python3 check.py \r\nSuccess!\r\n$ CUDA_VISIBLE_DEVICES=6 python3 check.py \r\nSuccess!\r\n$ CUDA_VISIBLE_DEVICES=7 python3 check.py \r\nSuccess!\r\n```']","[""\r\nipython --pylab\r\nPython 3.5.3 |Continuum Analytics, Inc.| (default, Mar  6 2017, 12:15:08)\r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 6.1.0 -- An enhanced Interactive Python. Type '?' for help.\r\nUsing matplotlib backend: MacOSX\r\n\r\nIn [1]: import torch\r\n\r\nIn [2]: torch.cuda.current_device()\r\nOut[2]: 0\r\n\r\nIn [3]: torch.cuda.device_count()\r\nOut[3]: 1\r\n\r\nIn [4]: torch.cuda.FloatTensor([1])\r\nTHCudaCheck FAIL file=/Users/scheibler/sandbox/pytorch/torch/lib/THC/generic/THCStorage.cu line=66 error=2 : out of memory\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-4-8352503114b9> in <module>()\r\n----> 1 torch.cuda.FloatTensor([1])\r\n\r\n~/anaconda/envs/python3/lib/python3.5/site-packages/torch/cuda/__init__.py in _lazy_new(cls, *args, **kwargs)\r\n    267     # We need this method only for lazy init, so we can remove it\r\n    268     del _CudaBase.__new__\r\n--> 269     return super(_CudaBase, cls).__new__(cls, *args, **kwargs)\r\n    270\r\n    271\r\n\r\nRuntimeError: cuda runtime error (2) : out of memory at /Users/scheibler/sandbox/pytorch/torch/lib/THC/generic/THCStorage.cu:66\r\n""]",['0.1.12+8d33603'],0,0
436,pytorch,2735,closed,nk,,,[],[],[],0,0
437,pytorch,26893,closed,onnx export: einsum not supported,"## üêõ Bug

Exporting models which use  in their  method does not work


## To Reproduce

This is a minimal example:


which gives 

s

cc @matthewfeickert @kratsg",module: onnx triaged,"['CC @houseroad ', ""I'm looking into adding the Einsum operator in ONNX: https://github.com/onnx/onnx/pull/2504\r\nWill update this once we have the operator ready."", '@neginraoof Any update on this?', '@ShakedDovrat - support for EinSum in ONNX exporter has been added. (https://github.com/pytorch/pytorch/pull/32716)', '@spandantiwari in which version? I still get this error in pytorch==1.4.0', 'This is in PyTorch master only. It is expected to be released with the next release. You can try it out using one of PyTorch nightly builds from pytorch.org.', ""Thank you. I've downloaded the current nightly build from pytorch.org and I still get the same error.\r\nI have installed:\r\ntorch==1.5.0.dev20200218\r\ntorchvision==0.6.0.dev20200219\r\nWhat else can I try?"", '@ShakedDovrat - this is the PR (https://github.com/pytorch/pytorch/pull/32716) adding support for Einsum in ONNX export. So I expect this to be supported. \r\nCan you please add an argument in your export API call: `opset_version=12`.\r\nThis is needed because Einsum is part of ONNX Opset 12 (to be released soon) and not previous opsets.', 'Thank you for your help.\r\nI\'ve added the `opset_version=12` argument and now get this error:\r\n```\r\nTraceback (most recent call last):\r\n  File ""temp.py"", line 9, in <module>\r\n    torch.onnx.export(model, dummy_input, output_model_path, verbose=True, input_names=[\'input\'], output_names=[\'output\'], opset_version=12)\r\n  File ""lib\\site-packages\\torch\\onnx\\__init__.py"", line 158, in export\r\n    custom_opsets, enable_onnx_checker)\r\n  File ""lib\\site-packages\\torch\\onnx\\utils.py"", line 68, in export\r\n    custom_opsets=custom_opsets, enable_onnx_checker=enable_onnx_checker)\r\n  File ""lib\\site-packages\\torch\\onnx\\utils.py"", line 469, in _export\r\n    fixed_batch_size=fixed_batch_size)\r\n  File ""lib\\site-packages\\torch\\onnx\\utils.py"", line 338, in _model_to_graph\r\n    fixed_batch_size=fixed_batch_size, params_dict=params_dict)\r\n  File ""lib\\site-packages\\torch\\onnx\\utils.py"", line 153, in _optimize_graph\r\n    graph = torch._C._jit_pass_onnx(graph, operator_export_type)\r\n  File ""lib\\site-packages\\torch\\onnx\\__init__.py"", line 189, in _run_symbolic_function\r\n    return utils._run_symbolic_function(*args, **kwargs)\r\n  File ""lib\\site-packages\\torch\\onnx\\utils.py"", line 717, in _run_symbolic_function\r\n    return op_fn(g, *inputs, **attrs)\r\n  File ""lib\\site-packages\\torch\\onnx\\symbolic_helper.py"", line 128, in wrapper\r\n    args = [_parse_arg(arg, arg_desc) for arg, arg_desc in zip(args, arg_descriptors)]\r\n  File ""lib\\site-packages\\torch\\onnx\\symbolic_helper.py"", line 128, in <listcomp>\r\n    args = [_parse_arg(arg, arg_desc) for arg, arg_desc in zip(args, arg_descriptors)]\r\n  File ""lib\\site-packages\\torch\\onnx\\symbolic_helper.py"", line 81, in _parse_arg\r\n    ""\', since it\'s not constant, please try to make ""\r\nRuntimeError: Failed to export an ONNX attribute \'onnx::Gather\', since it\'s not constant, please try to make things (e.g., kernel size) static if possible\r\n```\r\nGoogling it suggests it\'s a bug that was already fixed, so I\'m guessing opset version 12 is just not stable yet.', ""Hi, I also tried to modify opset version. It dose not work. Actually, on RTX2060, the program is stuck on the execution of the following code. I have to kill the process.\r\n`\r\ntorch.onnx.export(model=model,\r\n                              args=(input_ids, ),\r\n                              f=outf,\r\n                              input_names=['input'],\r\n                              output_names=['output'],\r\n                              dynamic_axes={\r\n                                  'input': [0, 1],\r\n                                  'output': [0, 1]\r\n                              },\r\n                             opset_version=12)\r\n`""]","['\r\nimport torch\r\nclass MyModel(torch.nn.Module):\r\n    def __init__(self):\r\n        super(MyModel,self).__init__()\r\n\r\n    def forward(self, inputs):\r\n        return torch.einsum(\'i->i\',inputs)\r\n\r\nmodel = MyModel()\r\ntorch.onnx.export(\r\n    model,\r\n    torch.randn(1),\r\n    ""pyhf.onnx"",\r\n    verbose=True,\r\n    input_names=[\'input\'],\r\n    output_names=[\'output\']\r\n)\r\n', '\r\n\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nI would expect this export to work\r\n\r\n## Environment\r\n\r\nMac OS X \r\n\r\n']","['einsum', 'forward()', '', '\r\nRuntimeError: Unsupported prim::Constant kind: ', '. Send a bug report.\r\n\r\npip freeze|grep torch\r\ntorch==1.2.0\r\n', '']",0,0
438,pytorch,24568,closed,Migrate `ge` and `ge_` from the TH to Aten (CUDA),"Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review.",better-engineering module: porting triaged,[],[],[],0,0
439,pytorch,31248,closed,"RuntimeError: cholesky_cuda: For batch 0: U(6,6) is zero, singular U.","## üêõ Bug when using 
Hi, I always get this RuntimeError during my training process:

##  Reproduce
There is a KL-loss term in my loss function, and I assume the two distributions are multivariate normal distributions, so I calculated it as follows:

## Environment
Here is my package version:
 - PyTorch 1.3.1  
 - Ubuntu 18.04
 - CUDA 9.2.148 with cuDNN 7.6.3_0 

## Additional 
I find my KL-loss falls in the range from 1e-6 to 1e-5.

 Any ideas to solve this problem? thx
",,"['Sorry, `log_var` is just an element-wise `log` of the covariance matrix? Ah, got it. Could you please share your reproduction script? Ideally with the fixed seed, if the parameters for the Gaussians are randomly sampled... Also, why do you add `mean` in the end? KL-divergence is already a scalar...', 'I guess Cholesky is used to circumvent the computation of the inverse covariance matrix in the quadratic form... But your covariance matrix is diagonal, so maybe it is possible to use this information before constructing the probability distribution object. @LiUzHiAn, do you know whether it is possible to do so, I mean just to provide a diagonal matrix to the `MultivariateNormal` constructor? Also, when the code fails, is it true that `q_log_var` and `p_log_var` have huge negative values?', ""Hi, @nikitaved \r\n\r\nIn my case, I assume the `p` and `q` are both Multivariate Gaussians as I mentioned above. Further, I assume each dimension is independent of others. So, each row in `log_var`  is the variances (i.e. [log( (sigma_1)^2), ... ,log( (sigma_N)^2)] ). Things are similar with `mu`` and other params.\r\n\r\nThen, with the `mu` and `sigma**2`, I can construct batch Multivariate Gaussians using the `torch.distributions.MultivariateNormal()`. As the Doc illustrating, the second param should be covariance matrix, and that's why I use `torch.diag_embed(torch.exp(q_log_var))` in advance.\r\n\r\nThe reason I use `mean()` in my KL loss is that the `p` and `q` here are batch distributions, with each row correspondingly."", 'Ok, did not notice the batch dimension. Anyway, clearly your covariance matrix becomes singular, and `MultivariateNormal` assumes positive-definite (full-rank) matrix. What about trying `LowRankMultivariteNormal` instead? https://pytorch.org/docs/stable/distributions.html?highlight=multivariatenormal#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal\r\n\r\nCholesky decomposition assumes that the matrix is positive-definite, and hence full-rank. This is not your case, and also the covariance matrix in your case has a simple structure, so no need to do any decompositions, you could find the (pseudo)inverse right away!\r\n\r\nSo, I do not think it is a bug. Let me know whether my suggestion works, and if so, we could close the issue.', ""I've written some test code and it seems that the batch dimension is supported.\r\n\r\nOk, let's leave this issue here. Thank you so much"", 'No problem, just let me know whether `LowRankMultivariateNormal` solves your issue.', '> Ok, did not notice the batch dimension. Anyway, clearly your covariance matrix becomes singular, and `MultivariateNormal` assumes positive-definite (full-rank) matrix. What about trying `LowRankMultivariteNormal` instead? https://pytorch.org/docs/stable/distributions.html?highlight=multivariatenormal#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal\r\n> \r\n> Cholesky decomposition assumes that the matrix is positive-definite, and hence full-rank. This is not your case, and also the covariance matrix in your case has a simple structure, so no need to do any decompositions, you could find the (pseudo)inverse right away!\r\n> \r\n> So, I do not think it is a bug. Let me know whether my suggestion works, and if so, we could close the issue.\r\n\r\nAlso @nikitaved , \r\n\r\n‚ÄúIn practice it may be necessary to add a small multiple of the identity matrix I to the covariance matrix for numerical reasons. This is because the eigenvalues of the matrix K0 can decay very rapidly and without this stabilization the Cholesky decomposition fails. The effect on the generated samples is to add additional independent noise of variance . From the context  can usually be chosen to have inconsequential effects on the samples, while ensuring numerical stability.‚Äù [(A.2 Gaussian Identities)](http://www.gaussianprocess.org/gpml/chapters/RWA.pdf). \r\n\r\nBlog from which I found this - [https://juanitorduz.github.io/multivariate_normal/](https://juanitorduz.github.io/multivariate_normal/)']","['\r\nFile ""train.py"", line 121, in train\r\n    loss_kl = kl_loss(mu, logvar, VAE_mu, VAE_logvar)\r\n  File ""/home/jessica/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""/home/jessica/anomaly_detection/ours/loss/losses.py"", line 70, in forward\r\n    p = torch.distributions.MultivariateNormal(p_mu, p_var)\r\n  File ""/home/jessica/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/distributions/multivariate_normal.py"", line 149, in __init__\r\n    self._unbroadcasted_scale_tril = torch.cholesky(covariance_matrix)\r\nRuntimeError: cholesky_cuda: For batch 0: U(6,6) is zero, singular U.\r\n', '\r\nclass KL_Loss(nn.Module):\r\n\tdef __init__(self):\r\n\t\tsuper(KL_Loss, self).__init__()\r\n\r\n\tdef forward(self, p_mu, p_log_var, q_mu, q_log_var):\r\n\t\t# [batch_size,d]  d is the dimension of my multivariate gaussian distribution\r\n\t\tassert q_mu.size() == p_mu.size()\r\n\t\tassert q_log_var.size() == p_log_var.size()\r\n\r\n\t\t# suppose the Neural Network esitimate log_var \r\n\t\tq_var = torch.diag_embed(torch.exp(q_log_var))\r\n\t\tp_var = torch.diag_embed(torch.exp(p_log_var))\r\n\t\t\r\n\t\tp = torch.distributions.MultivariateNormal(p_mu, p_var)\r\n\t\tq = torch.distributions.MultivariateNormal(q_mu, q_var)\r\n\t\tkl_loss = torch.distributions.kl_divergence(p, q).mean()\r\n\r\n\t\treturn kl_loss\r\n']","['torch.distributions.kl_divergence(p, q)']",0,0
440,pytorch,24686,closed,Migrate `clamp` and `clamp_` from the TH to Aten (CPU),"Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review.",better-engineering module: porting triaged,[],[],[],0,0
441,pytorch,12133,closed,How to initialize caffe2::Tensor,"## ‚ùì Questions and Help

### Please note that this issue tracker is not a help form and this issue will be closed.

We have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:

- [Discussion Forum](https://discuss.pytorch.org/)

I don't know how to initialize  Tensor(const vector<int64_t>& dims,const vector<T>& values, BaseContext* context), cause I dont't know how to init context.",,[],[],[],0,0
442,pytorch,2490,closed,libquadmath is repeated in TH/CMakeLists.txt,"in file 'torch/lib/TH/CMakeLists.txt', line 473 ~ 477,



libquadmath is repeated.
",,['thanks for reporting. fixed via https://github.com/pytorch/pytorch/commit/bd27f0b5a7183bbb42b024f88bd9058842c10f95'],"['\r\nInstall_Required_Library(${BLAS_openblas_LIBRARY})\r\nInstall_Required_Library(""${libpath}/libquadmath"")   \r\nInstall_Required_Library(""${libpath}/libgfortran"")   \r\nInstall_Required_Library(""${libpath}/libquadmath"")   \r\nInstall_Required_Library(""${libpath}/libgcc"")\r\n']",[],0,0
443,pytorch,5918,closed,Lint of .gitmodules and aten/.gitmodules are synchronized,see https://github.com/pytorch/pytorch/pull/5911,,[],[],[],0,0
444,pytorch,25859,closed,TORCH_WARN must not take out GIL (should buffer warnings),"At the moment, we have an attractively named  macro which you can use to report warnings from C++. What if I told you... that using this function could cause your code to take out a lock and all sorts of performance nastiness like that? Well, that's exactly what this macro does, because it calls a custom warning handler that, in the case of our Python extension, calls into Python code to file a Python warning. This is bad bad bad, for both performance reasons, and also because it could cause a deadlock (as to file a Python warning, we must acquire the GIL).

A better strategy is our Python handler must buffer warnings, and then when we (normally) return back to Python and acquire the GIL, actually report the warnings. The correct place to add this logic is probably where we also handle catching C++ exceptions and converting them into Python exceptions.",module: internals triaged,"['I\'m pretty ambivalent about this proposed change. I\'m concerned that this will make the warnings less useful: if something fails in between the TORCH_WARN and the ""normal"" return, the warning won\'t be printed.\r\n\r\nThe potential performance pitfalls can be handled with a std::call_once, like we do [here](https://github.com/pytorch/pytorch/blob/d1496183f59b59928ac7003503591560716f21c5/aten/src/THC/THCBlas.cu#L205-L207). We shouldn\'t be spamming warnings for a good user experience anyways.\r\n\r\nI don\'t see how warnings pose a deadlock issue that we wouldn\'t encounter anyways. The hazard appears to be that Python may acquire C++ land locks while still holding the GIL. This is unsafe regardless of TORCH_WARN. We have Tensor [destructors](https://github.com/pytorch/pytorch/blob/849c32f8e97af9f95e57b59d0db0aca6feced2d7/torch/csrc/utils/tensor_numpy.cpp#L167-L176) that acquire the GIL and it\'s hard to limit when/where those destructors may run.', '> if something fails in between the TORCH_WARN and the ""normal"" return, the warning won\'t be printed.\r\n\r\nMy thinking is that exception handling code that translates exceptions between C++ and Python would always flush warnings, even if there is an error. But it\'s true that if we segfault all warnings will be lost.\r\n\r\nI could also be convinced that the warn macro should be warn once by default everyone in C++.', '> My thinking is that exception handling code that translates exceptions between C++ and Python would always flush warnings, even if there is an error.\r\n\r\nOh, that makes sense. That makes me feel much better about the approach.', 'CC @albanD.', 'The PR that initially prompted this bug is #25319', 'Recapping in person discussion: we should probably also rename `TORCH_WARN` to `TORCH_WARN_SPAMALOT` and `TORCH_WARN_ONCE` to `TORCH_WARN`, to make the right thing easier to do.', '@albanD should this be closed?', 'Yes, the PR has been merged.']",[],['TORCH_WARN'],0,0
445,pytorch,10004,closed,Detaching gradients instead of using no_grad in spectral_norm,"Looking at [this](https://github.com/pytorch/pytorch/blob/3609977d7f4629f75dd1f8d904ddd5b52388124f/torch/nn/utils/spectral_norm.py#L53) code I've noticed that there is an easy way to make ,  differentiable wrt  (it is useful as normalizing constant  depends on ,  and ,  depend on ). 
If we  starting point of power iteration  we avoid cumulating gradients problem while allowing gradients pass through power iteration. As it adds some computational overhead it can be optional.
",,"[""The original Spectral Norm paper doesn't backprop through the left and right singular vectors, so didn't we. It's also unclear why detaching before the loop was reasonable because the _u vector is cached and reused."", 'According to https://discuss.pytorch.org/t/disconnected-gradient-in-pytorch/6341/5\r\ndetach will help to avoid a leak.', 'no_grad stops history tracking too.', 'I see, that was just a proposal to correctly pass gradients through power\niteration method\n\n–ø–Ω, 30 –∏—é–ª. 2018 –≥., 22:51 Tongzhou Wang <notifications@github.com>:\n\n> no_grad stops history tracking too.\n>\n> ‚Äî\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/10004#issuecomment-408988233>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ALKb7stu-73R-tHRskse_HEJonS0oNIZks5uL2PMgaJpZM4VmSQo>\n> .\n>\n', '@ferrine No worries. This is a perfectly valid thing to think about. I considered that too when we implemented SN. But _u and _v are cached at every iteration so it would be (nearly) impossible to compute exact gradient.']",[],"['u', 'v', 'W', 'sigma', 'u', 'v', 'u', 'v', 'W', 'detach', 'u']",0,0
446,pytorch,25171,closed,Relation between AVX and TH?,"Hi, I'm studying PyTorch internals, especially trying to find AVX implementation. (not AVX2.)

I built PyTorch from source with

What I've figured out is the AVX intrinsic function that is called for simple tensor addition.


I debugged with gdb, and set some breakpoints and succeeded it. Below is the result of  at gdb.


The AVX intrinsic  is called at #0

Now, I'm trying to find if there exists AVX acceleration of CNN, so I've debugged really simple code at [PyTorch tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py) with same procedure as above. But I cannot find any AVX implementation.

So now I'm wondering if I've missed some options when I built PyTorch. I've googled a lot, and looking inside the CMake system, I haven't figured it out yet.
One I noticed it is there are AVX.cpp in TH directory.

And after searching TH, I also noticed that I don't have libTH.so file. In [a blog in PyTorch homepage](https://pytorch.org/blog/a-tour-of-pytorch-internals-2/#backend-torch-and-vendor-libraries), there is libTH.so.1 at torch/lib. Below is my  result at that directory.


In addition, in [this blog](https://apaszke.github.io/torch-internals.html), there is ""simd"" directory in , but I don't have it. Is there anyone who can tell me about what I'm missing?



",module: convolution module: vectorization triaged,"[""> I'm trying to find if there exists AVX acceleration of CNN\r\n\r\nI did some search, and found that convolutions on CPU calls `thnn_conv2d` and `thnn_conv3d`.\r\n\r\nhttps://github.com/pytorch/pytorch/blob/f0a582b4808f8f5a43f73a42582e4908ff22d862/aten/src/ATen/native/Convolution.cpp#L689-L701\r\n\r\nWhich seems calls into `THTensorConv.cpp`, which does use the vectorized implementation. (@ezyang @colesbury Please correct me if I am wrong.)\r\n\r\nhttps://github.com/pytorch/pytorch/blob/365fc265719d8dae7ec345f013503491f221ae6f/aten/src/TH/generic/THTensorConv.cpp#L490\r\n\r\n> I also noticed that I don't have libTH.so file\r\n\r\nI believe that's all merged into `libtorch.so` now. If you do `nm libtorch.so` you should be able to see `THNN_` symbols."", ""Yes, although the convolutions will use MKL-DNN if available. Intel likes to use multiple names for the same product (and multiple products with the same name) so ideep, MKL-DNN, an DNNL all basically refer to the same thing. The binding code is here:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/mkldnn/Conv.cpp\r\n\r\nMKL-DNN is included as a GIT submodule. Source code is here:\r\n\r\nhttps://github.com/intel/mkl-dnn\r\n\r\nI'm not sure about MKL-DNN support for AVX (but not AVX2).  "", ""@colesbury, \r\n\r\nwhen it comes to performance library there's only one name and it's DNNL (was Intel MKL-DNN before Aug 19th this year). What is referred as ideep is a glue code between Pytorch and DNNL that will eventually go away.\r\n\r\n@jiseongg,\r\n\r\nIntel AVX-specific code in DNNL lives in jit_avx2_* or jit_uni_* implementations. Here's [an example](https://github.com/intel/mkl-dnn/blob/master/src/cpu/jit_avx2_conv_kernel_f32.cpp#L82) for 2D convolution kernel."", ""@mrshenli,\r\n \r\n> I did some search, and found that convolutions on CPU calls `thnn_conv2d` and `thnn_conv3d`.\r\n> ...\r\n> Which seems calls into `THTensorConv.cpp`, which does use the vectorized implementation.\r\n\r\nI'm not sure that tutorial code calls it. Functions in `THTensorConv.cpp` don't exist at all in my call stack. But I will do more research about it.\r\n\r\n>I believe that's all merged into libtorch.so now. If you do nm libtorch.so you should be able to see THNN_ symbols.\r\n\r\nIt seems you're right. I found symbols of source files at `TH` directory. Thank you. But I think my codes don't reach there, as I mentioned above.\r\n\r\n\r\n@colesbury,\r\n\r\nThank you, I set a breakpoint at that function and it succeeded. I've never known CPU executes PyTorch with mkl-dnn.\r\n\r\n\r\n@vpirogov,\r\n\r\nYes, It seems to be what I was looking for. But I'm not sure that my code execute it, so I'm trying to go deeper with my python code to figure out if it reaches there.""]","['bash\r\nDEBUG=1 NO_CUDA=1 python setup.py develop\r\n', 'python\r\nz = torch.add(x, y)\r\n', '\r\n#0 ~/pytorch/aten/src/ATen/cpu/vec256/vec256_double.h:203\r\n...\r\n#18 ~/pytorch/build/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp.AVX.cpp:29\r\n...\r\n#20 ~/pytorch/aten/src/ATen/native/BinaryOps.cpp:41\r\n', 'bash\r\nuser@user:~/pytorch/torch/lib\r\nc10d         libcaffe2_detectron_ops.so        libfbgemm.a     libprotobuf-lited.a  libshm.so           pkgconfig\r\ncmake        libcaffe2_module_test_dynamic.so  libgloo.a       libprotocd.a         libshm_windows      python3.6\r\nlibasmjit.a  libcaffe2_observers.so            libmkldnn.a     libpthreadpool.a     libsleef.a\r\nlibc10d.a    libclog.a                         libnnpack.a     libqnnpack.a         libtorch_python.so\r\nlibc10.so    libcpuinfo.a                      libprotobufd.a  libshm               libtorch.so\r\n']","['bt', '_mm256_add_pd(a, b)', '~/pytorch/aten/src/TH/vector/AVX.cpp', 'ls', 'TH/generic']",0,0
447,pytorch,24088,closed,"RuntimeError: ""eye"" not implemented for 'Bool' when on CPU","## üêõ Bug
torch.eye doesn't work for dtype torch.bool when the device is CPU, but it does work on GPU. 

## To Reproduce
>>> torch.eye(5, dtype=torch.bool, device=torch.device(""cpu""))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
RuntimeError: ""eye"" not implemented for 'Bool'

>>> torch.eye(5, dtype=torch.bool, device=torch.device(""cuda""))
tensor([[ True, False, False, False, False],
        [False,  True, False, False, False],
        [False, False,  True, False, False],
        [False, False, False,  True, False],
        [False, False, False, False,  True]], device='cuda:0')

## Expected behavior
I was expecting torch.eye for bool to be implemented for both CPU and CUDA. Maybe there are other torch functions with this issue as well?

## Environment
Collecting environment information...
PyTorch version: 1.2.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: Could not collect

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: GeForce GTX 1060
Nvidia driver version: 430.40
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.17.0
[pip3] torch==1.2.0
[pip3] torchvision==0.4.0
[conda] Could not collect

 - PyTorch Version (e.g., 1.0): 1.2
 - OS (e.g., Linux): Ubuntu 18.04
 - How you installed PyTorch (, , source): pip
 - Python version: 3.6.8
 - CUDA/cuDNN version: 10.1
 - GPU models and configuration: GTX 1060 (6GB)

## Additional context
I can workaround on CPU for now by casting to torch.bool after calling torch.eye.
",enhancement module: boolean tensor triaged,"['This would be nice to get fixed, given the changes to the bool mask behavior. \r\n\r\n@sdaulton ']",[],"['conda', 'pip']",0,0
448,pytorch,29117,closed,[tracking issue] RPC tests are flaky,"

cc @ezyang @gchanan @zou3519 @jerryzh168 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528",high priority module: flaky-tests module: rpc triage review,"['https://app.circleci.com/jobs/github/pytorch/pytorch/3442436', 'This may only be for ASAN.', '@aazzolini Who wrote this test recently.\r\n\r\nDoes this test need a retry mechanism? Should it be disabled on ASAN builds? ', ""This test shouldn't be flaky neither take 400+ seconds as it is very simple. I'm investigating."", 'https://circleci.com/gh/pytorch/pytorch/3467580', ""Turning this issue into a tracking issue, let's continue to post new examples of flakiness here to help @mrshenli and @aazzolini investigate."", 'https://app.circleci.com/jobs/github/pytorch/pytorch/3463855', 'https://app.circleci.com/jobs/github/pytorch/pytorch/3503307', 'Fixes are on the way #29069  #29306. Flakiness addressed in these two PRs have broad impact on almost all rpc/rref tests. Hopefully, this could give us a relief.', 'https://app.circleci.com/jobs/github/pytorch/pytorch/3518795', 'https://app.circleci.com/jobs/github/pytorch/pytorch/3521041', 'https://app.circleci.com/jobs/github/pytorch/pytorch/3529088\r\n(let me know if this is not useful)', 'closed by #29827 ']","['\r\n======================================================================\r\nNov 01 22:11:17 ERROR: test_call_method_on_rref (__main__.RpcTestWithFork)\r\nNov 01 22:11:17 ----------------------------------------------------------------------\r\nNov 01 22:11:17 Traceback (most recent call last):\r\nNov 01 22:11:17   File ""/var/lib/jenkins/workspace/test/common_distributed.py"", line 130, in wrapper\r\nNov 01 22:11:17     self._join_processes(fn)\r\nNov 01 22:11:17   File ""/var/lib/jenkins/workspace/test/common_distributed.py"", line 211, in _join_processes\r\nNov 01 22:11:17     self._check_return_codes(elapsed_time)\r\nNov 01 22:11:17   File ""/var/lib/jenkins/workspace/test/common_distributed.py"", line 230, in _check_return_codes\r\nNov 01 22:11:17     raise RuntimeError(\'Process {} terminated or timed out after {} seconds\'.format(i, elapsed_time))\r\nNov 01 22:11:17 RuntimeError: Process 0 terminated or timed out after 400.1577408313751 seconds\r\nNov 01 22:11:17 \r\nNov 01 22:11:17 ----------------------------------------------------------------------\r\n']",[],0,0
449,pytorch,7634,closed,cuda out of memory err:   when my gpu memory still has 4G left ," 

## Issue description

i was running a trained model  from github ,i watched the gpu memory, found that the script exited when the memory usage got 2G.
my gpu is 1060 with 6G memory ,how did that come?


PS: torch might has the 2G limit,but i haven't install that 


 

## System Info
RuntimeError: cuda runtime err(2): out of memory at /opt/conda/conda-bld/pytorch_1501953625411/work/pytorch-0.1.12/torch/libTHC/THCstorage.cu:66

 

- PyTorch or Caffe2:
- How you installed PyTorch (conda, pip, source):
- Build command you used (if compiling from source):
- OS: Ubuntu 14.04
- PyTorch version: pytorch 0.1.10
- Python version:  2.7
- CUDA/cuDNN version: 8.0/5.1 
- GCC version (if compiling from source): 4.9
 
 
",,['it can happen when you are allocating >4G'],[],[],0,0
450,pytorch,2117,closed,Add dynamic infer shape for nn.Linear,"It's quite inconvenient for user to calculate the shape of tensor manually. I think it would be better to entitle nn.Linear with the ability to infer shape from the input tensor.
One possible solution is like numpy's reshape function, which is nn.Linear(-1, hidden_size) or nn.Linear(None, hidden_size) means automatic inferring.

My preliminary implementation for this is at here:
https://gist.github.com/VoVAllen/1420e410e3dfd368b8dc9061ad0c206a

This implementation doesn't create any overhead during the training process. Also I think this kind of style can be used in other layers such as nn.Conv.
Feel free to add any comments. ",,"['Wrong implementation', 'we cannot have a dynamic inferring of the Linear layer weight sizes, because `parameters()` always have to return the full size of the linear layer. usually the optimizer is constructed with `model.parameters()` before a `forward` of the model is run.', ""I think it's possible to first add placeholder in model.parameter() and then add in the real parameter later in first forward process. Since nobody will execute optimization before first forward process.  Kind of Mock Parameter, build an intermediate wrapper over parameter in optimizer setting."", ""it's not just the optimizer. We actually charter into a few other side-effects. hogwild will be broken for the linear layers:\r\n- you create model\r\n- send model to all processes (the weights have `.share_memory_()` called on them)\r\n- then you train model in each of these subprocesses.\r\n\r\nThis will be broken as well, because once you share a Tensor through, you cant resize it (and if you replace the placeholder with a new Tensor, sharing is broken)."", ""Get it. Thank you for your explanation. It's more complicated than I thought. Hogwild's behavior is more likely a static graph construction. However I still feel regret that a dynamic graph framework cannot implement such infer shape things. Maybe a patch is implement this in single process situation and raise exception in share_memory process. But it's a bit ugly, may also counter your total design philosophy."", 'Hope you could find a better solution for this later']",[],[],0,0
451,pytorch,30627,closed,Everything under /usr/local/include is copied.,"## Bug

When I  to a custom location for packaging (previously set by ), I got everything under my  copied to this custom place.

This only happens on Ubuntu 18.04, and CentOS 7 seems fine with the same build script.
And I've only started to see this recently.
Builds from 2 weeks ago was still fine.

![image](https://user-images.githubusercontent.com/5203025/69997152-5ba9d580-1508-11ea-98c2-fa155ffc8d4e.png)

## Environment

 - PyTorch Version (e.g., 1.0): master
 - OS (e.g., Linux): Ubuntu 18.04 docker
 - How you installed PyTorch (, , source): source
 - Build command you used (if compiling from source): cmake+ninja
 - Python version: 3.6.9 (system default)
 - CUDA/cuDNN version: 10.2


cc @ezyang @gchanan @zou3519",high priority module: build triage review triaged,"[""If I delete the `<custom_path>/usr/local/*` and rerun `cmake --build . --target install` (so that there's nothing recompiled, just pure install), I still get all of them, indicating this is a install-specific issue."", 'Log for the second clean run of `cmake --build . --target install`.\r\n```\r\nroot@6b04d7f9297d:/tmp/scratch/pytorch/build# cmake --build . --target install\r\n[0/1] cd /tmp/scratch/pytorch/build && /usr/local/bin/cmake -P cmake_install.cmake\r\n-- Install configuration: ""Release""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/lib/libc10.so"" to ""$ORIGIN:/usr/local/lib""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/c10_DeviceGuard_test"" to ""$ORIGIN:/usr/local/lib""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/c10_StreamGuard_test"" to ""$ORIGIN:/usr/local/lib""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/c10_TensorTypeSet_test"" to ""$ORIGIN:/usr/local/lib""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/c10_InlineDeviceGuard_test"" to ""$ORIGIN:/usr/local/lib""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/c10_InlineStreamGuard_test"" to ""$ORIGIN:/usr/local/lib""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/c10_Array_test"" to ""$ORIGIN:/usr/local/lib""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/c10_C++17_test"" to ""$ORIGIN:/usr/local/lib""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/c10_ConstexprCrc_test"" to ""$ORIGIN:/usr/local/lib""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/c10_Half_test"" to ""$ORIGIN:/usr/local/lib""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/c10_LeftRight_test"" to ""$ORIGIN:/usr/local/lib""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/c10_Metaprogramming_test"" to ""$ORIGIN:/usr/local/lib""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/c10_TypeIndex_test"" to ""$ORIGIN:/usr/local/lib""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/c10_TypeList_test"" to ""$ORIGIN:/usr/local/lib""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/c10_TypeTraits_test"" to ""$ORIGIN:/usr/local/lib""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/c10_bfloat16_test"" to ""$ORIGIN:/usr/local/lib""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/c10_either_test"" to ""$ORIGIN:/usr/local/lib""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/c10_flags_test"" to ""$ORIGIN:/usr/local/lib""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/c10_intrusive_ptr_test"" to ""$ORIGIN:/usr/local/lib""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/c10_logging_test"" to ""$ORIGIN:/usr/local/lib""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/c10_ordered_preserving_dict_test"" to ""$ORIGIN:/usr/local/lib""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/c10_registry_test"" to ""$ORIGIN:/usr/local/lib""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/c10_string_view_test"" to ""$ORIGIN:/usr/local/lib""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/c10_tempfile_test"" to ""$ORIGIN:/usr/local/lib""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/c10_typeid_test"" to ""$ORIGIN:/usr/local/lib""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/lib/libc10_cuda.so"" to ""$ORIGIN:/usr/local/cuda/lib64:/usr/local/lib""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/c10_cuda_CUDATest"" to ""$ORIGIN:/usr/local/lib:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/lib/libcaffe2_nvrtc.so"" to ""$ORIGIN:/usr/local/cuda/lib64/stubs:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/lib/libtorch.so"" to ""$ORIGIN:/usr/local/cuda/lib64:/usr/local/lib:/opt/intel/mkl/lib/intel64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/List_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/TensorImpl_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/KernelFunction_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/kernel_function_legacy_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/kernel_function_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/kernel_functor_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/kernel_lambda_legacy_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/kernel_lambda_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/kernel_stackbased_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/op_registration_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/blob_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/common_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/context_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/event_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/graph_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/init_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/module_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/net_async_tracing_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/net_dag_utils_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/net_simple_refcount_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/net_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/observer_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/operator_schema_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/operator_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/parallel_net_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/stats_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/timer_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/transform_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/workspace_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""                                           [64/46195]-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/inline_container_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/fixed_divisor_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/math_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/fatal_signal_asan_no_sig_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/simple_queue_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/proto_utils_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/cpuid_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/smart_tensor_printer_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/cast_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/predictor_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/data_filler_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/AlgorithmsTest"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/BinaryMatchImplTest"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/GraphTest"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/MatchTest"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/NeuralNetTest"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/SubgraphMatcherTest"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/TarjansImplTest"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/TopoSortTest"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/time_observer_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/ssa_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/batch_matmul_op_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/boolean_unmask_ops_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/conv_transpose_op_mobile_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/elementwise_op_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/generate_proposals_op_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/generate_proposals_op_util_boxes_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/generate_proposals_op_util_nms_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/half_float_ops_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/string_ops_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/text_file_reader_utils_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/utility_ops_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/int8_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/int8_roi_align_op_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/backend_cutting_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/bound_shape_inference_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/converter_nomigraph_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/dead_code_elim_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/device_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/distributed_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/mobile_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/nnpack_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/depthwise3x3_conv_op_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/common_subexpression_elimination_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/conv_to_nnpack_transform_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/pattern_net_transform_test"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/blob_gpu_test"" to ""$ORIGIN:/usr/local/cuda/lib64:/usr/local/lib:/opt/intel/mkl/lib/intel64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/context_gpu_test"" to ""$ORIGIN:/usr/local/cuda/lib64:/usr/local/lib:/opt/intel/mkl/lib/intel64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/event_gpu_test"" to ""$ORIGIN:/usr/local/cuda/lib64:/usr/local/lib:/opt/intel/mkl/lib/intel64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/net_gpu_test"" to ""$ORIGIN:/usr/local/cuda/lib64:/usr/local/lib:/opt/intel/mkl/lib/intel64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/operator_gpu_test"" to ""$ORIGIN:/usr/local/cuda/lib64:/usr/local/lib:/opt/intel/mkl/lib/intel64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/math_gpu_test"" to ""$ORIGIN:/usr/local/cuda/lib64:/usr/local/lib:/opt/intel/mkl/lib/intel64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/conv_op_cache_cudnn_test"" to ""$ORIGIN:/usr/local/cuda/lib64:/usr/local/lib:/opt/intel/mkl/lib/intel64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/batch_matmul_op_gpu_test"" to ""$ORIGIN:/usr/local/cuda/lib64:/usr/local/lib:/opt/intel/mkl/lib/intel64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/elementwise_op_gpu_test"" to ""$ORIGIN:/usr/local/cuda/lib64:/usr/local/lib:/opt/intel/mkl/lib/intel64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/generate_proposals_op_gpu_test"" to ""$ORIGIN:/usr/local/cuda/lib64:/usr/local/lib:/opt/intel/mkl/lib/intel64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/generate_proposals_op_util_nms_gpu_test"" to ""$ORIGIN:/usr/local/cuda/lib64:/usr/local/lib:/opt/intel/mkl/lib/intel64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/operator_fallback_gpu_test"" to ""$ORIGIN:/usr/local/cuda/lib64:/usr/local/lib:/opt/intel/mkl/lib/intel64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/reshape_op_gpu_test"" to ""$ORIGIN:/usr/local/cuda/lib64:/usr/local/lib:/opt/intel/mkl/lib/intel64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/roi_align_op_gpu_test"" to ""$ORIGIN:/usr/local/cuda/lib64:/usr/local/lib:/opt/intel/mkl/lib/intel64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/test/utility_ops_gpu_test"" to ""$ORIGIN:/usr/local/cuda/lib64:/usr/local/lib:/opt/intel/mkl/lib/intel64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/test_jit"" to ""$ORIGIN:/usr/local/cuda/lib64:/usr/local/cuda/lib64/stubs:/usr/local/lib:/opt/intel/mkl/lib/intel64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/test_cpp_rpc"" to ""$ORIGIN:/usr/local/cuda/lib64:/usr/local/cuda/lib64/stubs:/usr/local/lib:/opt/intel/mkl/lib/intel64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/test_api"" to ""$ORIGIN:/usr/local/cuda/lib64:/usr/local/cuda/lib64/stubs:/usr/local/lib:/opt/intel/mkl/lib/intel64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/test_dist_autograd"" to ""$ORIGIN:/usr/local/cuda/lib64:/usr/local/cuda/lib64/stubs:/usr/local/lib:/opt/intel/mkl/lib/intel64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/lib/libcaffe2_detectron_ops_gpu.so"" to ""$ORIGIN:/usr/local/cuda/lib64:/usr/local/lib:/opt/intel/mkl/lib/intel64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/lib/libcaffe2_module_test_dynamic.so"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/lib/libcaffe2_observers.so"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/lib/libcaffe2_rocksdb.so"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/convert_caffe_image_db"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/convert_db"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/make_cifar_db"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/make_mnist_db"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/parallel_info"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/intra_inter_benchmark"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/at_launch_benchmark"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/predictor_verifier"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/print_registered_core_operators"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/run_plan"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/speed_benchmark"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/speed_benchmark_torch"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/split_db"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/db_throughput"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/core_overhead_benchmark"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/inspect_gpu"" to ""$ORIGIN:/usr/local/cuda/lib64:/usr/local/lib:/opt/intel/mkl/lib/intel64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/print_core_object_sizes_gpu"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/core_overhead_benchmark_gpu"" to ""$ORIGIN:/usr/local/cuda/lib64:/usr/local/lib:/opt/intel/mkl/lib/intel64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/convert_encoded_to_raw_leveldb"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/make_image_db"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/convert_image_to_tensor"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/caffe2_benchmark"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/convert_and_benchmark"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/tutorial_blob"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n-- Set runtime path of ""/tmp/scratch/pytorch/install.nZmHkpuFFa/root/usr/local/bin/dump_operator_names"" to ""$ORIGIN:/usr/local/lib:/opt/intel/mkl/lib/intel64:/usr/local/cuda/lib64""\r\n```', ""Note that this only applies to `/usr/local/include`, other dirs like `lib` and `bin` weren't affected."", ""I'm currently working around this by a `find | sed | xargs rm -rf` workflow."", 'Huh, that is surprising! Which version of CMake do you use?\r\n\r\ncc @kostmo @ezyang ', 'Should be latest. It is part of a larger build where we also build the latest cmake from source.\n\nSent from my iPhone\n\n> On Dec 3, 2019, at 00:44, Pieter Noordhuis <notifications@github.com> wrote:\n> \n> \ufeff\n> Huh, that is surprising! Which version of CMake do you use?\n> \n> cc @kostmo @ezyang\n> \n> ‚Äî\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n', '@xkszltl Do you think you could bisect this? That would be really helpful.', ""@ezyang \r\n\r\nDid some investigation but haven't found anything before running out of spare time and switched to the `find | sed | xargs rm` work-around"", ""Unfortunately I can't easily reproduce this exact problem but I *can* easily reproduce #29823, which I have a hunch is related. @xkszltl by any chance in your cmake output, do you see something like this:\r\n\r\n```\r\n-- System pybind11 found\r\n-- pybind11 include dirs: /usr/local/include\r\n```\r\n\r\nIf so I think what's happening is that the changes in #29659 caused the pybind11 include directory to always get installed, even for system installs of pybind11."", '@ngoldbaum Yes:\r\n![image](https://user-images.githubusercontent.com/5203025/70185949-ab6ed500-169f-11ea-9de6-33dbc3dec5ac.png)\r\n', ""Hmm, glad my hunch is leading somewhere!  Do you also see a `python3.6m` directory getting installed that shouldn't be there?"", ""There's no `/usr/include/python3.6m` but as in the very first screenshot of this thread, there's `/usr/local/python3.6m`, which contains the following:\r\n```\r\n/usr/local/python3.6m\r\n/usr/local/python3.6m/pyarena.h\r\n/usr/local/python3.6m/opcode.h\r\n/usr/local/python3.6m/warnings.h\r\n/usr/local/python3.6m/code.h\r\n/usr/local/python3.6m/boolobject.h\r\n/usr/local/python3.6m/setobject.h\r\n/usr/local/python3.6m/pyctype.h\r\n/usr/local/python3.6m/bitset.h\r\n/usr/local/python3.6m/Imaging.h\r\n/usr/local/python3.6m/node.h\r\n/usr/local/python3.6m/token.h\r\n/usr/local/python3.6m/accu.h\r\n/usr/local/python3.6m/pygetopt.h\r\n/usr/local/python3.6m/methodobject.h\r\n/usr/local/python3.6m/unicodeobject.h\r\n/usr/local/python3.6m/pgenheaders.h\r\n/usr/local/python3.6m/dynamic_annotations.h\r\n/usr/local/python3.6m/floatobject.h\r\n/usr/local/python3.6m/odictobject.h\r\n/usr/local/python3.6m/sysmodule.h\r\n/usr/local/python3.6m/eval.h\r\n/usr/local/python3.6m/listobject.h\r\n/usr/local/python3.6m/classobject.h\r\n/usr/local/python3.6m/import.h\r\n/usr/local/python3.6m/pydtrace.h\r\n/usr/local/python3.6m/iterobject.h\r\n/usr/local/python3.6m/graminit.h\r\n/usr/local/python3.6m/structmember.h\r\n/usr/local/python3.6m/object.h\r\n/usr/local/python3.6m/py_curses.h\r\n/usr/local/python3.6m/metagrammar.h\r\n/usr/local/python3.6m/pyexpat.h\r\n/usr/local/python3.6m/objimpl.h\r\n/usr/local/python3.6m/pyport.h\r\n/usr/local/python3.6m/ceval.h\r\n/usr/local/python3.6m/pylifecycle.h\r\n/usr/local/python3.6m/pythonrun.h\r\n/usr/local/python3.6m/pycapsule.h\r\n/usr/local/python3.6m/frameobject.h\r\n/usr/local/python3.6m/descrobject.h\r\n/usr/local/python3.6m/symtable.h\r\n/usr/local/python3.6m/complexobject.h\r\n/usr/local/python3.6m/pystrhex.h\r\n/usr/local/python3.6m/pymacro.h\r\n/usr/local/python3.6m/bytearrayobject.h\r\n/usr/local/python3.6m/patchlevel.h\r\n/usr/local/python3.6m/dtoa.h\r\n/usr/local/python3.6m/genobject.h\r\n/usr/local/python3.6m/bytesobject.h\r\n/usr/local/python3.6m/pydebug.h\r\n/usr/local/python3.6m/pystrtod.h\r\n/usr/local/python3.6m/errcode.h\r\n/usr/local/python3.6m/memoryobject.h\r\n/usr/local/python3.6m/sliceobject.h\r\n/usr/local/python3.6m/pystate.h\r\n/usr/local/python3.6m/ImagingUtils.h\r\n/usr/local/python3.6m/dictobject.h\r\n/usr/local/python3.6m/ast.h\r\n/usr/local/python3.6m/pyerrors.h\r\n/usr/local/python3.6m/Python-ast.h\r\n/usr/local/python3.6m/structseq.h\r\n/usr/local/python3.6m/bltinmodule.h\r\n/usr/local/python3.6m/compile.h\r\n/usr/local/python3.6m/pyatomic.h\r\n/usr/local/python3.6m/pythread.h\r\n/usr/local/python3.6m/ImPlatform.h\r\n/usr/local/python3.6m/pytime.h\r\n/usr/local/python3.6m/asdl.h\r\n/usr/local/python3.6m/moduleobject.h\r\n/usr/local/python3.6m/pyconfig.h\r\n/usr/local/python3.6m/bytes_methods.h\r\n/usr/local/python3.6m/ImDib.h\r\n/usr/local/python3.6m/pymacconfig.h\r\n/usr/local/python3.6m/marshal.h\r\n/usr/local/python3.6m/osmodule.h\r\n/usr/local/python3.6m/longintrepr.h\r\n/usr/local/python3.6m/weakrefobject.h\r\n/usr/local/python3.6m/osdefs.h\r\n/usr/local/python3.6m/pgen.h\r\n/usr/local/python3.6m/typeslots.h\r\n/usr/local/python3.6m/grammar.h\r\n/usr/local/python3.6m/codecs.h\r\n/usr/local/python3.6m/ucnhash.h\r\n/usr/local/python3.6m/funcobject.h\r\n/usr/local/python3.6m/modsupport.h\r\n/usr/local/python3.6m/fileutils.h\r\n/usr/local/python3.6m/Python.h\r\n/usr/local/python3.6m/fileobject.h\r\n/usr/local/python3.6m/enumobject.h\r\n/usr/local/python3.6m/abstract.h\r\n/usr/local/python3.6m/traceback.h\r\n/usr/local/python3.6m/tupleobject.h\r\n/usr/local/python3.6m/pyfpe.h\r\n/usr/local/python3.6m/cellobject.h\r\n/usr/local/python3.6m/parsetok.h\r\n/usr/local/python3.6m/datetime.h\r\n/usr/local/python3.6m/pyhash.h\r\n/usr/local/python3.6m/intrcheck.h\r\n/usr/local/python3.6m/pymath.h\r\n/usr/local/python3.6m/rangeobject.h\r\n/usr/local/python3.6m/longobject.h\r\n/usr/local/python3.6m/pystrcmp.h\r\n/usr/local/python3.6m/pymem.h\r\n/usr/local/python3.6m/namespaceobject.h\r\n```"", ""Ah, I missed that in your screenshot. I think that's pretty definitive proof #29823 is the same issue."", '@ngoldbaum \r\nConfirmed the issue has been fixed on my side.\r\nThanks!']",[],"['cmake --build . --target install', '-DCMAKE_INSTALL_PREFIX', '/usr/local/include', 'conda', 'pip']",0,0
452,pytorch,11076,closed,"[caffe2] testing, please ignore","If you have a question or would like help and support, please ask at our
[forums](https://discuss.pytorch.org/).

If you are submitting a feature request, please preface the title with [feature request].
If you are submitting a bug report, please fill in the following details.

## Issue description

Provide a short description.

## Code example

Please try to provide a minimal example to repro the bug.
Error messages and stack traces are also helpful.

## System Info
Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


- PyTorch or Caffe2:
- How you installed PyTorch (conda, pip, source):
- Build command you used (if compiling from source):
- OS:
- PyTorch version:
- Python version:
- CUDA/cuDNN version:
- GPU models and configuration:
- GCC version (if compiling from source):
- CMake version:
- Versions of any other relevant libraries:
",caffe2,[],"['\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n']",[],0,0
453,pytorch,31554,closed,RuntimeError: Error in dlopen or dlsym: libcaffe2_nvrtc.so: cannot open shared object file: No such file or directory,"RuntimeError: Error in dlopen or dlsym: libcaffe2_nvrtc.so: cannot open shared object file: No such file or directory

Environment: 
1. Pytorch 1.3.1
2. Linux
3. Pycharm

",,"['I found a solution: include the lib path of anaconda into LD_LIBRARY_PATH before running pycharm.\r\n\r\n**For my own case:** run the following code line in the command line.\r\n\r\nexport LD_LIBRARY_PATH=/data/xiaoshua/miniconda3/lib/python3.7/site-packages/torch/lib:$LD_LIBRARY_PATH\r\n\r\n**For your case:**  change the path of ""/data/xiaoshua/miniconda3/"" to your own directory. Then, run the above export code line.\r\n\r\n', 'Great to hear you found a solution! ']",[],[],0,0
454,pytorch,15262,closed,doc err:torch.nn.BCELoss,"pytorch 1.0
-------------
DOC:This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets y should be numbers between 0 and 1.
--------------
import torch 
import torch.nn.functional as F
b=[[0.2,0.1]]
c=[[1.,3.]]
aa = torch.tensor(b,requires_grad=True)
target = torch.tensor(c)
output = F.binary_cross_entropy(aa,target,reduce=True,size_average=True)
z=output.sum()
z.backward()
print(output)
-----------------------
tensor(4.1532, grad_fn=<BinaryCrossEntropyBackward>)
---------------
target value is 1 and 3.,not between 0 and 1.
",module: docs todo,['https://pytorch.org/docs/stable/nn.functional.html#binary-cross-entropy The docs changed and this issue can be closed.'],[],[],0,0
455,pytorch,22542,closed,Bug in CosineAnnealingWarmRestarts (T_mul instead of T_mult),"## üêõ Bug

The bug appears in PyTorch 1.1 in file lr_scheduler.py (line 686) for LR scheduler ""CosineAnnealingWarmRestarts"", where the parameter name is *T_mult* but in line 686 the name *T_mul* is used instead (python interpreter does not know variable *T_mul*).

## To Reproduce

The bug can be easily reproduced if the *T_mult* parameter of CosineAnnealingWarmRestart scheduler is not set properly (i.e. is < 1 or not an instance of int), causing Python to raise the error at line 686 (where *T_mul* is used instead of *T_mult*).

The following line will raise the error and cause the bug:


because 2.0 is not an instance of int.

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

Line 686 of lr_scheduler.py should be modified from

into 


## Environment

- PyTorch Version (e.g., 1.0): 1.1
 - OS (e.g., Linux): Linux
 - How you installed PyTorch (, , source): pip
 - Build command you used (if compiling from source):
 - Python version: 3.7
 - CUDA/cuDNN version: 10
 - GPU models and configuration: 1080 Ti
 - Any other relevant information: N/A",module: optimizer triaged,"['Hi @ldutu, thanks for reporting! Since you have found the solution, would you like to submit a PR for it? We will be happy to review and land. ', 'Hi @mrshenli , checking the latest version of the file, I found that this was already fixed by @rtqichen in commit 57948414acf87c846bf88cf5f194282b8f47c423 . Thanks!']",[],"['scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=100, T_mult=2.0)', 'raise ValueError(""Expected integer T_mul >= 1, but got {}"".format(T_mul))', 'raise ValueError(""Expected integer T_mult >= 1, but got {}"".format(T_mult))', 'conda', 'pip']",0,0
456,pytorch,5563,closed,Import Error : no module  named torch,"PyTorch GitHub Issues Guidelines
--------------------------------

We like to limit our issues to bug reports and feature requests. If you have a question or would like help and support, please visit our forums: https://discuss.pytorch.org/

If you are submitting a feature request, please preface the title with [feature request].

When submitting a bug report, please include the following information (where relevant):
- OS:
- PyTorch version:
- How you installed PyTorch (conda, pip, source):
- Python version:
- CUDA/cuDNN version:
- GPU models and configuration:
- GCC version (if compiling from source):

In addition, including the following information will also be very helpful for us to diagnose the problem:
- A script to reproduce the bug. Please try to provide as minimal of a test case as possible.
- Error messages and/or stack traces of the bug
- Context around what you are trying to do

![issue1](**https://user-images.githubusercontent.com/19254992/36958144-c011b122-205f-11e8-82c2-fe0b0e0ad4f3.png**)
image error running from terminal is at link in Bracket
OS 16.04 Pytorch version 0.31 Latest , installed using conda, python version 3.5/3.6 .
Cuda 8.0 Cudnn 5.1 . GCC version 5.4.0
I just cloned this repository https://github.com/thstkdgus35/EDSR-PyTorch and trying to run demo.sh file which runs main.py file which imports PyTorch. so the problem is **i am able to import torch from home directory in python3 shell but whenever i go to any folder/directory and run pytorch code or import torch in python3 shell it gives me error** of no module named torch (see image for details). Also **i have installed torch(Luajit) in home or default root directory**. so it might be possible reason.Please help with reference to my code in image.
![issue1](https://user-images.githubusercontent.com/19254992/36958482-2c7fe048-2062-11e8-86e8-b10f25f471cc.png)
",,"['You have installed `torch` with Python3, where script uses Python2 (`python` defaults to `python2`). Change it, and it should work.', 'I ran the command with python3 also and all python files in repository are in python3. You can check DIV2K_jpeg.py file in repository its in python3 for sure and same for other files.', 'I am getting the same issue. How did you resolve it?', 'how did u install your pytorch and which project you are working on ? solutions would depend .', 'Resolved, I added sys.path(/path_torch_directory) in my code.', 'Do u Know why its needed (if pytorch installed in root directory default/every other installation default too) ?', '@ankita-kalra could you please elaborate your answer please? when i run my source code it shows no module named torch, but\r\n![p80720-195030](https://user-images.githubusercontent.com/27885238/43001126-aaff0e98-8c56-11e8-8692-dc517807a464.jpg)\r\n already installed successfully.\r\n\r\n\r\n\r\n', 'in the first command you used `python3`, second command you used `python`.', 'hlo ..i successfully installed torch using (pip install https://download.pytorch.org/whl/cu90/torch-1.0.0-cp37-cp37m-win_amd64.whl )and conda and python(3.7)is same on my base root env and my activated env but on jupyter notebook when i type import torch it showing module not found error..please any suggestion', 'easly installation pytorch from here.\r\nhttps://pytorch.org/get-started/locally/', 'i compiled pytorch from source for GPU with CUDA 9 and CUDNN 7. After compiling when i tried to import torch . It throws No module named torch. i cloned pytorch into my code folder and compiled from there. Should i need to compile from home directory ', ""can't install torch on pypy3"", 'how to update pytorch or torch version in my environment(container)?', 'Add the path by: export PATH=~/anaconda3/bin:$PATH  \r\nbefore opening the python.']",[],[],0,0
457,pytorch,8039,closed,RuntimeError: cuda runtime error (30) : unknown error at /pytorch/aten/src/THC/THCGeneral.cpp:70,"## Issue description



## Code example



## System Info



 does not detect  even though it's installed:



",,"['is there something in your LD_LIBRARY_PATH that might override general RPATH? For example do you have cuda 9.2 in your LD_LIBRARY_PATH?', ""Sorry, I cannot replicate it anymore, as later I've reinstalled `pytorch` from the `Archlinux` repos and now it works like a charm. \r\n\r\nAnyway, it seems that the underlying problem was that the version from `pytorch.org` is compiled using `CUDA 9.1.85`, but on my system I had `CUDA 9.2.88`.\r\n\r\nThanks, Narunas\r\n"", ""The most possible reason is that the version of CUDA is different from the one needed by pytorch .Secondly, you may have no cudnn. What's more, the cudnn also need the right version of CUDA.\r\nI got the same Error at the same line,now it goes right. I have no cudnn before."", ""i had this error, indeed cuda was installed in an unknow location and the environement path not set properly. i reinstalled cuda with custom location and it resolved the problem. To me It looks like when the cuda installer suggested an installation directory that doesn't exist, it didn't create it and i didn't know where it was installed and could not set my path variables properly. ""]","['\r\n$ python pytorch.py \r\nTHCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=70 error=30 : unknown error\r\nTraceback (most recent call last):\r\n  File ""pytorch.py"", line 3, in <module>\r\n    a = torch.ones(3,3).to(\'cuda\')\r\n  File ""/usr/lib/python3.6/site-packages/torch/cuda/__init__.py"", line 161, in _lazy_init\r\n    torch._C._cuda_init()\r\nRuntimeError: cuda runtime error (30) : unknown error at /pytorch/aten/src/THC/THCGeneral.cpp:70\r\n', ""\r\nimport torch\r\n\r\na = torch.ones(3,3).to('cuda')\r\nb = torch.ones(3,3).to('cuda')\r\n\r\nprint(a + b)\r\n"", '\r\n$ python collect_env.py \r\nCollecting environment information...\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.1.85\r\n\r\nOS: Arch Linux\r\nGCC version: (GCC) 8.1.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: 9.2.88\r\nGPU models and configuration: GPU 0: GeForce GTX 970\r\nNvidia driver version: 396.24\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.14.3)\r\n[pip3] numpydoc (0.7.0)\r\n[pip3] torch (0.4.0)\r\n[pip3] torchvision (0.2.1)\r\n[conda] Could not collect\r\n', ""\r\n# pacman -Q | egrep 'cud(a|nn)'\r\ncuda 9.2.88.1-2\r\ncudnn 7.1.3-1\r\npycuda-headers 2017.1.1-5\r\npython-pycuda 2017.1.1-5\r\npython-tensorflow-opt-cuda 1.8.0-2\r\n""]","['collect_env.py', 'cuDNN']",0,0
458,pytorch,3225,closed,Subtracting ByteTensor doesn't work in this case.,"This snippet used to work, but now throws an error:

",,"['Hi,\r\nIt is not the substraction that does not work, it is the negation.\r\nA `ByteTensor` is an unsigned type, so doing `- byte_tensor` does not work.', 'Thanks for pointing this out. Perhaps the behavior of SubConstant should change? Here is the old behavior (0.1.12):\r\n\r\n```\r\ndef forward(self, a):\r\n    if self.sub_tensor:\r\n        if a.is_signed() and self.inplace:\r\n            self.mark_dirty(a)\r\n            return a.neg_().add_(self.constant)\r\n        else:\r\n            assert not self.inplace, ""can\'t perform (constant - tensor) "" \\\r\n                ""subtraction in-place on an unsigned type""\r\n            return a.new().resize_as_(a).fill_(self.constant).sub_(a)\r\n    else:\r\n        if self.inplace:\r\n            self.mark_dirty(a)\r\n            return a.sub_(self.constant)\r\n        else:\r\n            return a.sub(self.constant)\r\n```', 'It might be sufficient to update the docs as a breakage created by torch v0.2.0.', 'This was somewhat referred to in a different issue (https://github.com/pytorch/pytorch/issues/2543), but I think the problem is slightly different. There should be a simple way to invert a ""binary tensor"" (from 0101 -> 1010 for instance).', 'Fixed in #4075', 'üéÜ ']","[""\r\nIn [1]: import torch\r\n\r\nIn [2]: torch.__version__\r\nOut[2]: '0.2.0_4'\r\n\r\nIn [3]: from torch.autograd import Variable\r\n\r\nIn [4]: 1 - Variable(torch.cuda.ByteTensor(2).fill_(1))\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-3-d11efaae512d> in <module>()\r\n----> 1 1 - Variable(torch.cuda.ByteTensor(2).fill_(1))\r\n\r\n/home/dexter/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.pyc in __rsub__(self, other)\r\n    824\r\n    825     def __rsub__(self, other):\r\n--> 826         return SubConstant.apply(other, self)\r\n    827\r\n    828     def __mul__(self, other):\r\n\r\n/home/dexter/anaconda2/lib/python2.7/site-packages/torch/autograd/_functions/basic_ops.pyc in forward(ctx, a, b, inplace)\r\n    128                 return tensor.neg_().add_(constant)\r\n    129             else:\r\n--> 130                 return tensor.neg().add_(constant)\r\n    131\r\n    132     @staticmethod\r\n\r\nAttributeError: 'torch.cuda.ByteTensor' object has no attribute 'neg'\r\n""]",[],0,0
459,pytorch,19920,closed,CTC Decoder with KenLM Language Model,where is the implementation of ctc_decoder_with_kenlm in pytorch? I have found some 3rd party implementation but they are no longer supported by pytorch now?,,['we dont have such a thing in pytorch core'],[],[],0,0
460,pytorch,7498,closed,Production,How to deploy the pYtorch trained model to production ? ,,"['What do you mean by ""production""? The word itself is vague.', 'please use https://discuss.pytorch.org/']",[],[],0,0
461,pytorch,19601,closed,TestJit.test_cpp broken on master,"## üêõ Bug


https://circleci.com/gh/pytorch/pytorch/1423249?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link/console",oncall: jit,"['Seems to be this patch https://github.com/pytorch/pytorch/pull/19445 cc @eellison ', 'Unlanding the offending patch']","['\r\nApr 23 05:09:48 ======================================================================\r\nApr 23 05:09:48 ERROR: test_cpp (__main__.TestJit)\r\nApr 23 05:09:48 ----------------------------------------------------------------------\r\nApr 23 05:09:48 Traceback (most recent call last):\r\nApr 23 05:09:48   File ""/var/lib/jenkins/workspace/test/common_utils.py"", line 129, in wrapper\r\nApr 23 05:09:48     fn(*args, **kwargs)\r\nApr 23 05:09:48   File ""test_jit.py"", line 1727, in test_cpp\r\nApr 23 05:09:48     torch._C._jit_run_cpp_tests(run_cuda=False)\r\nApr 23 05:09:48 RuntimeError: \r\nApr 23 05:09:48 Schema not found for node. File a bug report.\r\nApr 23 05:09:48 Node: %3 : Tensor = aten::tensor(%2, %1, %0)\r\nApr 23 05:09:48 \r\nApr 23 05:09:48 Input types:float, int?, Device?\r\nApr 23 05:09:48 candidates were:\r\nApr 23 05:09:48   aten::tensor(float t, *, int? dtype=<default>, Device? device=<default>, bool requires_grad=<default>) -> Tensor\r\nApr 23 05:09:48   aten::tensor(int t, *, int? dtype=<default>, Device? device=<default>, bool requires_grad=<default>) -> Tensor\r\nApr 23 05:09:48   aten::tensor(bool t, *, int? dtype=<default>, Device? device=<default>, bool requires_grad=<default>) -> Tensor\r\nApr 23 05:09:48   aten::tensor(t[] data, *, int? dtype=<default>, Device? device=<default>, bool requires_grad=<default>) -> Tensor\r\nApr 23 05:09:48 graph():\r\nApr 23 05:09:48   %0 : Device? = prim::Constant()\r\nApr 23 05:09:48   %1 : int? = prim::Constant()\r\nApr 23 05:09:48   %2 : float = prim::Constant[value=1]()\r\nApr 23 05:09:48   %3 : Tensor = aten::tensor(%2, %1, %0)\r\nApr 23 05:09:48   %4 : Tensor[] = prim::ListConstruct(%3)\r\nApr 23 05:09:48   %5 : Tensor = aten::tensor(%2, %1, %0)\r\nApr 23 05:09:48   %6 : Tensor[] = prim::ListConstruct(%5)\r\nApr 23 05:09:48   %7 : (Tensor[], Tensor[]) = prim::TupleConstruct(%4, %6)\r\nApr 23 05:09:48   return (%7)\r\n']",[],0,0
462,pytorch,19024,closed,Loss explosion with DataParallel on WGAN models ,"See https://discuss.pytorch.org/t/huge-loss-with-dataparallel/40749 for the original report.

This seems to have regressed between 0.4.1 and 1.0 and needs to be debugged.

cc @mrshenli ",high priority oncall: distributed triaged,"['In the Pytorch version 1.0.1-py3.7_cuda8.0.61_cudnn7.1.2_2, I face the same problem.', 'After testing, the problem still existed in the newest PyTorch version 1.1.0.dev20190417', ""Yes, I have also met this issue when I use pytorch==1.0.1 version,  but if I  use 0.4.1 version everything is fine. So what's wrong?  This problem waste some of my time for debugging my program. "", '![image](https://user-images.githubusercontent.com/10510469/56457779-22c25380-63b2-11e9-9ff3-8264ca9cc309.png)\r\n\r\nSame code for different pytorch version while cause different loss.', 'I can also confirm that this issue is in the latest build of 1.1.0. \r\nCuda 10. Python 3.7.2.', 'I also see this occurring for pytorch 1.0.1  on python 3.7.3, with cuda10.0.130, cudnn7.4. \r\nLike @ailias everything works fine on 0.4.1 for me as well.', ""I'm struggling with the same issue with pytorch 1.0\r\n\r\nI've made a minimal example to reproduce this:\r\n```python\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.optim import Adam\r\nimport numpy as np\r\nimport os\r\n\r\ntorch.manual_seed(0)\r\nnp.random.seed(0)\r\ntorch.backends.cudnn.benchmark = True\r\ntorch.backends.cudnn.deterministic = True\r\ntorch.set_printoptions(precision=10)\r\n\r\n\r\ndiscriminator1 = nn.Linear(1, 1, bias=True).cuda()\r\n\r\ndiscriminator2 = nn.Linear(1, 1, bias=True)\r\ndiscriminator2.load_state_dict(discriminator1.state_dict())\r\ndiscriminator2 = nn.DataParallel(discriminator2).cuda()\r\n\r\noptim1 = Adam(discriminator1.parameters())\r\noptim2 = Adam(discriminator2.parameters())\r\n\r\ndef gradient_penalty(real_data, discriminator):\r\n    real_data = real_data.clone()\r\n    real_data.requires_grad=True\r\n    logits = discriminator(real_data).sum()\r\n    grad = torch.autograd.grad(\r\n        outputs=logits,\r\n        inputs=real_data,\r\n        grad_outputs=torch.ones(logits.shape).cuda(),\r\n        create_graph=True\r\n    )[0]\r\n    return grad.sum()\r\n\r\n# Define input\r\nx_fake = torch.randn((1, 1)).cuda()\r\n\r\n# Discriminator 1\r\npenalty = gradient_penalty(x_fake, discriminator1).sum()\r\noptim1.zero_grad()\r\npenalty.backward()\r\noptim1.step()\r\ny_out1 = discriminator1(x_fake)\r\n\r\n# Discriminator 2\r\npenalty = gradient_penalty(x_fake, discriminator2).sum()\r\noptim2.zero_grad()\r\npenalty.backward()\r\noptim2.step()\r\ny_out2 = discriminator2(x_fake)\r\n\r\nprint(y_out1 - y_out2)\r\n```\r\nOptimally, the difference between `y_out1` and `y_out2` should be 0.\r\n\r\nTested with the following NGC pytorch containers:\r\n\r\n- [nvcr.io/nvidia/pytorch:18.09-py3](https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel_18.09.html). This is based on 0.4.1 official release. The difference is 0.0, no issue with gradient penalty.\r\n- [nvcr.io/nvidia/pytorch:18.10-py3](https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel_18.11.html). This is based on a 1.0 preview (Master branch up to PR [11834](https://github.com/pytorch/pytorch/pull/11834) The difference is 0.0021787882, which is a significant difference.\r\nApparently it got broken somewhere between 0.4.1 release and the 1.0 preview.\r\n\r\nI also checked the newest pytorch version. Still broken.\r\n\r\nHope this helps!\r\n\r\n"", ""It's a bug. `DataParallel` now does not handle double backward properly. In the above example `discriminator2` does not get any update. \r\n\r\n```\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.optim import Adam\r\nimport numpy as np\r\nimport os\r\n\r\ntorch.backends.cudnn.benchmark = True\r\ntorch.backends.cudnn.deterministic = True\r\ntorch.set_printoptions(precision=10)\r\n\r\n\r\ndiscriminator1 = nn.Linear(1, 1, bias=True).cuda()\r\ndiscriminator2 = nn.Linear(1, 1, bias=True)\r\ndiscriminator2.load_state_dict(discriminator1.state_dict())\r\ndiscriminator2 = nn.DataParallel(discriminator2).cuda()\r\n\r\nlearning_rate = 0.5 # put any value here\r\noptim1 = Adam(discriminator1.parameters(), lr=learning_rate)\r\noptim2 = Adam(discriminator2.parameters(), lr=1000)\r\n\r\ndef gradient_penalty(real_data, discriminator):\r\n    real_data = real_data.clone()\r\n    real_data.requires_grad=True\r\n    logits = discriminator(real_data).sum()\r\n    grad = torch.autograd.grad(\r\n        outputs=logits,\r\n        inputs=real_data,\r\n        grad_outputs=torch.ones(logits.shape).cuda(),\r\n        create_graph=True\r\n    )[0]\r\n    return grad.sum()\r\n\r\n# Define input\r\nx_fake = torch.ones((1, 1)).cuda()\r\n\r\n# Discriminator 1\r\npenalty = gradient_penalty(x_fake, discriminator1).sum()\r\noptim1.zero_grad()\r\npenalty.backward()\r\noptim1.step()\r\n\r\n# Discriminator 2\r\npenalty = gradient_penalty(x_fake, discriminator2).sum()\r\noptim2.zero_grad()\r\npenalty.backward()\r\noptim2.step()\r\n\r\n# running on multiple GPUs this always prints something around 1e-7 \r\n# regardlesss of learning_rate and initialization\r\n# discriminator2 basically does not get any grad update\r\nprint(discriminator2.module.weight - discriminator1.weight - learning_rate)\r\n\r\n```"", ""Okay. I think I'm running into this same issue from DistributedDataParallel as well. The loss chart above looks strikingly similar to what I'm seeing when using a gradient penalty (not the WGAN-GP style though). The loss has some noise to it, but it does't trend up or down. And the model just simply doesn't learn. I've been banging my head against my desk for over a month now trying to debug my architecture then I found this issue. \r\n\r\nAfter just reimplementing my model in TensorFlow (its a 1:1 implementation, including the ability to swap weights between TF and PyTorch and get the exact same outputs) and Viola! It trains perfectly, as it should have been. The training methodology is exactly the same between implementations as well. It seems second order derivatives are broken. At least when using multi-GPU training (I can't run my model on a single GPU to test it so I'm not sure). \r\n\r\nAny progress being made on this? It seems very important. "", ""@mdlockyer, I'm not sure if it is the same issue with DistributedDataParallel. According to the [docs](https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel), DistributedDataParallel does not support torch.grad.autograd, which is required for gradient penalty\r\n\r\n> Okay. I think I'm running into this same issue from DistributedDataParallel as well. \r\n"", '@pietern @mrshenli any chance a fix gets into 1.2?', 'assigning @pietern to figure out if we should target this for 1.2 or not.', ""@thuyen Many thanks for the repro script.\r\n\r\nI'm running a bisect to figure out which commit introduced this."", ""I've bisected to #11611 being the culprit here.\r\n\r\n@resistor @apaszke @ezyang Any ideas?"", 'Assigning to @resistor for further investigation.', '@pietern A fix was attempted in #16888.', ""And the original issue for that fix at #16532. Let's continue the discussion there to consolidate.""]",[],[],0,0
463,pytorch,28868,closed,How to build caffe2 with ONNX opset version greater than 9?,"## ‚ùì Questions and Help

Hello,
I've currently worked with freshly merged feature pytorch/vision#1401 and won't able to find a way to make Caffe2 work with ONNX operation set 10?

Is there a way to build a Caffe2 from source with this opset?
 ",,"['Please use https://discuss.pytorch.org (https://discuss.pytorch.org/) for questions. Please kindly feel encouraged to reopen this issue, if this does not apply.']",[],[],0,0
464,pytorch,3822,closed,Building from source on master is broken,"Related to https://github.com/pytorch/pytorch/pull/3817.

When building from source on Ubuntu 16.04, I get the following.


Reverting the offending PR solves the problem.",,"[""I am experiencing similar errors:\r\n\r\n```\r\n-- Build files have been written to: /home/username/Codes/pytorch/torch/lib/build/THD\r\nScanning dependencies of target THD\r\n[  3%] Building CXX object CMakeFiles/THD.dir/base/ChannelUtils.cpp.o\r\n[  7%] Building CXX object CMakeFiles/THD.dir/base/Cuda.cpp.o\r\n[ 11%] Building CXX object CMakeFiles/THD.dir/base/DataChannel.cpp.o\r\n[ 14%] Building CXX object CMakeFiles/THD.dir/base/data_channels/DataChannelGloo.cpp.o\r\n[ 18%] Building CXX object CMakeFiles/THD.dir/base/DataChannelRequest.cpp.o\r\n[ 22%] Building CXX object CMakeFiles/THD.dir/base/TensorDescriptor.cpp.o\r\n[ 25%] Building CXX object CMakeFiles/THD.dir/base/data_channels/DataChannelMPI.cpp.o\r\n[ 29%] Building CXX object CMakeFiles/THD.dir/base/data_channels/DataChannelTCP.cpp.o\r\n[ 33%] Building CXX object CMakeFiles/THD.dir/base/data_channels/Store.cpp.o\r\n[ 37%] Building CXX object CMakeFiles/THD.dir/base/init_methods/InitMethod.cpp.o\r\n[ 40%] Building CXX object CMakeFiles/THD.dir/base/init_methods/InitMethodEnv.cpp.o\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp: In constructor ‚Äòthd::AutoGPU::AutoGPU(int)‚Äô:\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:139:27: error: ‚ÄòcudaGetDevice‚Äô was not declared in this scope\r\n     cudaGetDevice(&device_);\r\n                           ^\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:140:29: error: ‚ÄòcudaSetDevice‚Äô was not declared in this scope\r\n     cudaSetDevice(new_device);\r\n                             ^\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp: In destructor ‚Äòthd::AutoGPU::~AutoGPU()‚Äô:\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:147:26: error: ‚ÄòcudaSetDevice‚Äô was not declared in this scope\r\n     cudaSetDevice(device_);\r\n                          ^\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp: In member function ‚Äòat::Tensor thd::DataChannelMPI::_newLikeFlat(std::vector<at::Tensor>&) const‚Äô:\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:157:57: warning: narrowing conversion of ‚Äò((& t)->at::Tensor::is_cuda() ? (& t)->at::Tensor::get_device() : -1l)‚Äô from ‚Äòlong int‚Äô to ‚Äòint‚Äô inside { } [-Wnarrowing]\r\n   AutoGPU gpu_guard { t.is_cuda() ? t.get_device() : -1 };\r\n                                                         ^\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:158:44: warning: narrowing conversion of ‚Äò(& tensors)->std::vector<_Tp, _Alloc>::size<at::Tensor, std::allocator<at::Tensor> >()‚Äô from ‚Äòstd::vector<at::Tensor>::size_type {aka long unsigned int}‚Äô to ‚Äòlong int‚Äô inside { } [-Wnarrowing]\r\n   std::vector<int64_t> sizes { tensors.size() };  // sizes = [output.size()] + input.sizes()\r\n                                            ^\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:158:44: warning: narrowing conversion of ‚Äò(& tensors)->std::vector<_Tp, _Alloc>::size<at::Tensor, std::allocator<at::Tensor> >()‚Äô from ‚Äòstd::vector<at::Tensor>::size_type {aka long unsigned int}‚Äô to ‚Äòlong int‚Äô inside { } [-Wnarrowing]\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:160:10: error: ‚Äòinput‚Äô was not declared in this scope\r\n   return input.type().tensor(sizes);\r\n          ^\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp: In member function ‚Äòvirtual void thd::DataChannelMPI::scatter(std::vector<at::Tensor>&, at::Tensor&, thd::rank_type, THDGroup)‚Äô:\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:233:7: error: no match for ‚Äòoperator!‚Äô (operand type is ‚Äòat::Tensor‚Äô)\r\n   if (!output.contiguous())\r\n       ^\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:233:7: note: candidate: operator!(bool) <built-in>\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:233:7: note:   no known conversion for argument 1 from ‚Äòat::Tensor‚Äô to ‚Äòbool‚Äô\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp: In member function ‚Äòvirtual void thd::DataChannelMPI::reduce(at::Tensor&, THDReduceOp, thd::rank_type, THDGroup)‚Äô:\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:281:7: error: no match for ‚Äòoperator!‚Äô (operand type is ‚Äòat::Tensor‚Äô)\r\n   if (!data.contiguous())\r\n       ^\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:281:7: note: candidate: operator!(bool) <built-in>\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:281:7: note:   no known conversion for argument 1 from ‚Äòat::Tensor‚Äô to ‚Äòbool‚Äô\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp: In member function ‚Äòvirtual void thd::DataChannelMPI::broadcast(at::Tensor&, thd::rank_type, THDGroup)‚Äô:\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:299:7: error: no match for ‚Äòoperator!‚Äô (operand type is ‚Äòat::Tensor‚Äô)\r\n   if (!data.contiguous())\r\n       ^\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:299:7: note: candidate: operator!(bool) <built-in>\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:299:7: note:   no known conversion for argument 1 from ‚Äòat::Tensor‚Äô to ‚Äòbool‚Äô\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp: In member function ‚Äòvirtual void thd::DataChannelMPI::send(thd::Scalar&, thd::rank_type ‚Äô:\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:309:56: error: request for member ‚ÄòscalarType‚Äô in ‚Äò(& data)->thd::Scalar::type()‚Äô, which is of non-class type ‚Äòthd::RPCType‚Äô\r\n   MPI_Send(data.data(), 1, mpi_datatype.at(data.type().scalarType()),\r\n                                                        ^\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp: In member function ‚Äòvirtual void thd::DataChannelMPI::receive(thd::Scalar&, thd::rank_type)‚Äô:\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:324:17: error: ‚Äòstruct thd::Scalar‚Äô has no member named ‚Äòdata_ptr‚Äô\r\n   MPI_Recv(data.data_ptr(), 1, mpi_datatype.at(data.type().scalarType()),\r\n                 ^\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:324:60: error: request for member ‚ÄòscalarType‚Äô in ‚Äò(& data)->thd::Scalar::type()‚Äô, which is of non-class type ‚Äòthd::RPCType‚Äô\r\n   MPI_Recv(data.data_ptr(), 1, mpi_datatype.at(data.type().scalarType()),\r\n                                                            ^\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp: In member function ‚Äòvirtual void thd::DataChannelMPI::receive(at::Tensor&, thd::rank_type)‚Äô:\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:346:10: error: ‚Äòstatus‚Äô was not declared in this scope\r\n   return status.MPI_SOURCE;\r\n          ^\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:346:17: error: return-statement with a value, in function returning 'void' [-fpermissive]\r\n   return status.MPI_SOURCE;\r\n                 ^\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp: In member function ‚Äòvirtual thd::DataChannelMPI::RequestMPI* thd::DataChannelMPI::ireceive(at::Tensor&, thd::rank_type)‚Äô:\r\n/home/username/Codes/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:384:10: error: cannot convert ‚Äòstd::unique_ptr<thd::DataChannelMPI::RequestMPI>‚Äô to ‚Äòthd::DataChannelMPI::RequestMPI*‚Äô in return\r\n   return request;\r\n          ^\r\n[ 44%] Building CXX object CMakeFiles/THD.dir/base/init_methods/InitMethodFile.cpp.o\r\nCMakeFiles/THD.dir/build.make:206: recipe for target 'CMakeFiles/THD.dir/base/data_channels/DataChannelMPI.cpp.o' failed\r\nmake[2]: *** [CMakeFiles/THD.dir/base/data_channels/DataChannelMPI.cpp.o] Error 1\r\nmake[2]: *** Waiting for unfinished jobs....\r\nCMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/THD.dir/all' failed\r\nmake[1]: *** [CMakeFiles/THD.dir/all] Error 2\r\nMakefile:129: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\n```\r\n\r\nI am following the installation steps from the README.md file. \r\nI am using the following:\r\n```\r\nconda 4.3.30\r\nPython 3.5.4 :: Anaconda custom (64-bit)\r\nCUDA 8 and CUDNN 7\r\n```"", 'I just opened a PR that fixes it. Please try using that branch.', '@apaszke PR fixes the install issues. ', 'I also meet this problem. I can not find PR branch. \r\n\r\nWhat I do is compiling the binary source code. `sudo python3.5 setup.py install` \r\n', '@AlexHex7 you can do the following:\r\n\r\nI will assume you have all the dependencies mentioned at https://github.com/pytorch/pytorch#from-source\r\n```\r\ngit clone --recursive https://github.com/pytorch/pytorch.git\r\ncd pytorch\r\ngit pull origin pull/3831/head:build_fix\r\ngit checkout build_fix\r\nexport CMAKE_PREFIX_PATH=""$(dirname $(which conda))/../"" # [anaconda root directory]\r\npython setup.py install | tee build.log # Writes the build logs to build.log\r\n\r\n```', '@napsternxg  Thanks, solve it!', 'I also met the same problem.  @napsternxg Thanks for your solution and have solved it.', 'I met the same problem. But  head:build_fix branch doesn\'t work.\r\n\r\nrunning install\r\nrunning build_deps\r\n-- The C compiler identification is GNU 5.4.0\r\n-- The CXX compiler identification is GNU 5.4.0\r\n-- Check for working C compiler: /usr/bin/cc\r\n-- Check for working C compiler: /usr/bin/cc -- works\r\n-- Detecting C compiler ABI info\r\n-- Checking if C linker supports --verbose\r\n-- Checking if C linker supports --verbose - yes\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /usr/bin/c++\r\n-- Check for working CXX compiler: /usr/bin/c++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Checking if CXX linker supports --verbose\r\n-- Checking if CXX linker supports --verbose - yes\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Could NOT find CUDA (missing:  CUDA_TOOLKIT_ROOT_DIR CUDA_NVCC_EXECUTABLE CUDA_INCLUDE_DIRS CUDA_CUDART_LIBRARY) (Required is at least version ""5.5"")\r\n-- Found gcc >=5 and CUDA <= 7.5, adding workaround C++ flags\r\n-- Automatic GPU detection failed. Building for common architectures.\r\n-- Autodetected CUDA architecture(s): 3.0;3.5;5.0;5.2+PTX\r\n-- Could not find CUDA with FP16 support, compiling without torch.CudaHalfTensor\r\n-- Removing -DNDEBUG from compile flags\r\n-- Try OpenMP C flag = [-fopenmp]\r\n-- Performing Test OpenMP_FLAG_DETECTED\r\n-- Performing Test OpenMP_FLAG_DETECTED - Success\r\n-- Try OpenMP CXX flag = [-fopenmp]\r\n-- Performing Test OpenMP_FLAG_DETECTED\r\n-- Performing Test OpenMP_FLAG_DETECTED - Success\r\n-- Found OpenMP: -fopenmp  \r\n-- Compiling with OpenMP support\r\n-- MAGMA not found. Compiling without MAGMA support\r\n-- Could not find hardware support for NEON on this machine.\r\n-- No OMAP3 processor on this machine.\r\n-- No OMAP4 processor on this machine.\r\n-- Looking for cpuid.h\r\n-- Looking for cpuid.h - found\r\n-- Performing Test HAVE_GCC_GET_CPUID\r\n-- Performing Test HAVE_GCC_GET_CPUID - Success\r\n-- Performing Test NO_GCC_EBX_FPIC_BUG\r\n-- Performing Test NO_GCC_EBX_FPIC_BUG - Success\r\n-- Performing Test C_HAS_SSE1_1\r\n-- Performing Test C_HAS_SSE1_1 - Success\r\n-- Performing Test C_HAS_SSE2_1\r\n-- Performing Test C_HAS_SSE2_1 - Success\r\n-- Performing Test C_HAS_SSE3_1\r\n-- Performing Test C_HAS_SSE3_1 - Failed\r\n-- Performing Test C_HAS_SSE3_2\r\n-- Performing Test C_HAS_SSE3_2 - Success\r\n-- Performing Test C_HAS_SSE4_1_1\r\n-- Performing Test C_HAS_SSE4_1_1 - Failed\r\n-- Performing Test C_HAS_SSE4_1_2\r\n-- Performing Test C_HAS_SSE4_1_2 - Success\r\n-- Performing Test C_HAS_SSE4_2_1\r\n-- Performing Test C_HAS_SSE4_2_1 - Failed\r\n-- Performing Test C_HAS_SSE4_2_2\r\n-- Performing Test C_HAS_SSE4_2_2 - Success\r\n-- Performing Test C_HAS_AVX_1\r\n-- Performing Test C_HAS_AVX_1 - Failed\r\n-- Performing Test C_HAS_AVX_2\r\n-- Performing Test C_HAS_AVX_2 - Success\r\n-- Performing Test C_HAS_AVX2_1\r\n-- Performing Test C_HAS_AVX2_1 - Failed\r\n-- Performing Test C_HAS_AVX2_2\r\n-- Performing Test C_HAS_AVX2_2 - Failed\r\n-- Performing Test C_HAS_AVX2_3\r\n-- Performing Test C_HAS_AVX2_3 - Failed\r\n-- Performing Test CXX_HAS_SSE1_1\r\n-- Performing Test CXX_HAS_SSE1_1 - Success\r\n-- Performing Test CXX_HAS_SSE2_1\r\n-- Performing Test CXX_HAS_SSE2_1 - Success\r\n-- Performing Test CXX_HAS_SSE3_1\r\n-- Performing Test CXX_HAS_SSE3_1 - Failed\r\n-- Performing Test CXX_HAS_SSE3_2\r\n-- Performing Test CXX_HAS_SSE3_2 - Success\r\n-- Performing Test CXX_HAS_SSE4_1_1\r\n-- Performing Test CXX_HAS_SSE4_1_1 - Failed\r\n-- Performing Test CXX_HAS_SSE4_1_2\r\n-- Performing Test CXX_HAS_SSE4_1_2 - Success\r\n-- Performing Test CXX_HAS_SSE4_2_1\r\n-- Performing Test CXX_HAS_SSE4_2_1 - Failed\r\n-- Performing Test CXX_HAS_SSE4_2_2\r\n-- Performing Test CXX_HAS_SSE4_2_2 - Success\r\n-- Performing Test CXX_HAS_AVX_1\r\n-- Performing Test CXX_HAS_AVX_1 - Failed\r\n-- Performing Test CXX_HAS_AVX_2\r\n-- Performing Test CXX_HAS_AVX_2 - Success\r\n-- Performing Test CXX_HAS_AVX2_1\r\n-- Performing Test CXX_HAS_AVX2_1 - Failed\r\n-- Performing Test CXX_HAS_AVX2_2\r\n-- Performing Test CXX_HAS_AVX2_2 - Failed\r\n-- Performing Test CXX_HAS_AVX2_3\r\n-- Performing Test CXX_HAS_AVX2_3 - Failed\r\n-- SSE2 Found\r\n-- SSE3 Found\r\n-- AVX Found\r\n-- Performing Test HAS_C11_ATOMICS\r\n-- Performing Test HAS_C11_ATOMICS - Failed\r\n-- Performing Test HAS_MSC_ATOMICS\r\n-- Performing Test HAS_MSC_ATOMICS - Failed\r\n-- Performing Test HAS_GCC_ATOMICS\r\n-- Performing Test HAS_GCC_ATOMICS - Success\r\n-- Atomics: using GCC intrinsics\r\n-- Looking for sys/types.h\r\n-- Looking for sys/types.h - found\r\n-- Looking for stdint.h\r\n-- Looking for stdint.h - found\r\n-- Looking for stddef.h\r\n-- Looking for stddef.h - found\r\n-- Check size of void*\r\n-- Check size of void* - done\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel_lp64: /home/wilber/anaconda3/envs/python35/lib/libmkl_intel_lp64.so\r\n--   Library mkl_gnu_thread: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel_lp64: /home/wilber/anaconda3/envs/python35/lib/libmkl_intel_lp64.so\r\n--   Library mkl_intel_thread: /home/wilber/anaconda3/envs/python35/lib/libmkl_intel_thread.so\r\n--   Library mkl_core: /home/wilber/anaconda3/envs/python35/lib/libmkl_core.so\r\n--   Library gomp: -fopenmp\r\n--   Library pthread: /usr/lib/x86_64-linux-gnu/libpthread.so\r\n--   Library m: /usr/lib/x86_64-linux-gnu/libm.so\r\n--   Library dl: /usr/lib/x86_64-linux-gnu/libdl.so\r\n-- Looking for cblas_sgemm\r\n-- Looking for cblas_sgemm - not found\r\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel_lp64: /home/wilber/anaconda3/envs/python35/lib/libmkl_intel_lp64.so\r\n--   Library mkl_gnu_thread: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n--   Library mkl_intel_lp64: /home/wilber/anaconda3/envs/python35/lib/libmkl_intel_lp64.so\r\n--   Library mkl_intel_thread: /home/wilber/anaconda3/envs/python35/lib/libmkl_intel_thread.so\r\n--   Library mkl_core: /home/wilber/anaconda3/envs/python35/lib/libmkl_core.so\r\n--   Library iomp5: /home/wilber/anaconda3/envs/python35/lib/libiomp5.so\r\n--   Library pthread: /usr/lib/x86_64-linux-gnu/libpthread.so\r\n--   Library m: /usr/lib/x86_64-linux-gnu/libm.so\r\n--   Library dl: /usr/lib/x86_64-linux-gnu/libdl.so\r\n-- Looking for cblas_sgemm\r\n-- Looking for cblas_sgemm - found\r\n-- MKL library found\r\n-- Performing Test BLAS_F2C_DOUBLE_WORKS\r\n-- Performing Test BLAS_F2C_DOUBLE_WORKS - Failed\r\n-- Performing Test BLAS_F2C_FLOAT_WORKS\r\n-- Performing Test BLAS_F2C_FLOAT_WORKS - Success\r\n-- Performing Test BLAS_USE_CBLAS_DOT\r\n-- Performing Test BLAS_USE_CBLAS_DOT - Success\r\n-- Found a library with BLAS API (mkl).\r\n-- Found a library with LAPACK API. (mkl)\r\n-- Could NOT find CUDNN (missing:  CUDNN_INCLUDE_DIR CUDNN_LIBRARY) \r\n-- ignoring CUDA\r\n-- Using python found in /home/wilber/anaconda3/envs/python35/bin/python\r\n[\'/home/wilber/repository/pytorch/aten/src/THNN/generic/THNN.h\', \'/home/wilber/repository/pytorch/aten/src/THCUNN/generic/THCUNN.h\', \'/home/wilber/repository/pytorch/aten/src/ATen/nn.yaml\']\r\nATen Excluded: {\'bernoulli_\', \'bernoulli\'}\r\n-- Looking for clock_gettime in rt\r\n-- Looking for clock_gettime in rt - found\r\n-- Looking for mmap\r\n-- Looking for mmap - found\r\n-- Looking for shm_open\r\n-- Looking for shm_open - found\r\n-- Looking for shm_unlink\r\n-- Looking for shm_unlink - found\r\n-- Looking for malloc_usable_size\r\n-- Looking for malloc_usable_size - found\r\n-- Performing Test C_HAS_THREAD\r\n-- Performing Test C_HAS_THREAD - Success\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /home/wilber/repository/pytorch/torch/lib/build/aten\r\n[  1%] Generating ATen/CPUGenerator.h, ATen/CUDAGenerator.h, ATen/Declarations.yaml, ATen/CPUByteStorage.cpp, ATen/CPUByteStorage.h, ATen/CPUByteType.cpp, ATen/CPUByteType.h, ATen/CPUByteTensor.cpp, ATen/CPUByteTensor.h, ATen/CPUCharStorage.cpp, ATen/CPUCharStorage.h, ATen/CPUCharType.cpp, ATen/CPUCharType.h, ATen/CPUCharTensor.cpp, ATen/CPUCharTensor.h, ATen/CPUDoubleStorage.cpp, ATen/CPUDoubleStorage.h, ATen/CPUDoubleType.cpp, ATen/CPUDoubleType.h, ATen/CPUDoubleTensor.cpp, ATen/CPUDoubleTensor.h, ATen/CPUFloatStorage.cpp, ATen/CPUFloatStorage.h, ATen/CPUFloatType.cpp, ATen/CPUFloatType.h, ATen/CPUFloatTensor.cpp, ATen/CPUFloatTensor.h, ATen/CPUIntStorage.cpp, ATen/CPUIntStorage.h, ATen/CPUIntType.cpp, ATen/CPUIntType.h, ATen/CPUIntTensor.cpp, ATen/CPUIntTensor.h, ATen/CPULongStorage.cpp, ATen/CPULongStorage.h, ATen/CPULongType.cpp, ATen/CPULongType.h, ATen/CPULongTensor.cpp, ATen/CPULongTensor.h, ATen/CPUShortStorage.cpp, ATen/CPUShortStorage.h, ATen/CPUShortType.cpp, ATen/CPUShortType.h, ATen/CPUShortTensor.cpp, ATen/CPUShortTensor.h, ATen/CPUHalfStorage.cpp, ATen/CPUHalfStorage.h, ATen/CPUHalfType.cpp, ATen/CPUHalfType.h, ATen/CPUHalfTensor.cpp, ATen/CPUHalfTensor.h, ATen/SparseCPUByteType.cpp, ATen/SparseCPUByteType.h, ATen/SparseCPUByteTensor.cpp, ATen/SparseCPUByteTensor.h, ATen/SparseCPUCharType.cpp, ATen/SparseCPUCharType.h, ATen/SparseCPUCharTensor.cpp, ATen/SparseCPUCharTensor.h, ATen/SparseCPUDoubleType.cpp, ATen/SparseCPUDoubleType.h, ATen/SparseCPUDoubleTensor.cpp, ATen/SparseCPUDoubleTensor.h, ATen/SparseCPUFloatType.cpp, ATen/SparseCPUFloatType.h, ATen/SparseCPUFloatTensor.cpp, ATen/SparseCPUFloatTensor.h, ATen/SparseCPUIntType.cpp, ATen/SparseCPUIntType.h, ATen/SparseCPUIntTensor.cpp, ATen/SparseCPUIntTensor.h, ATen/SparseCPULongType.cpp, ATen/SparseCPULongType.h, ATen/SparseCPULongTensor.cpp, ATen/SparseCPULongTensor.h, ATen/SparseCPUShortType.cpp, ATen/SparseCPUShortType.h, ATen/SparseCPUShortTensor.cpp, ATen/SparseCPUShortTensor.h, ATen/Type.h, ATen/Type.cpp, ATen/Tensor.h, ATen/TensorMethods.h, ATen/Functions.h, ATen/Dispatch.h, ATen/Copy.cpp, ATen/NativeFunctions.h\r\n[\'/home/wilber/repository/pytorch/aten/src/THNN/generic/THNN.h\', \'/home/wilber/repository/pytorch/aten/src/THCUNN/generic/THCUNN.h\', \'/home/wilber/repository/pytorch/aten/src/ATen/nn.yaml\']\r\nATen Excluded: {\'bernoulli\', \'bernoulli_\'}\r\nScanning dependencies of target ATen\r\n[  2%] Building CXX object src/ATen/CMakeFiles/ATen.dir/CPUGenerator.cpp.o\r\n[  3%] Building CXX object src/ATen/CMakeFiles/ATen.dir/CUDAGenerator.cpp.o\r\n[  4%] Building CXX object src/ATen/CMakeFiles/ATen.dir/Context.cpp.o\r\n[  5%] Building CXX object src/ATen/CMakeFiles/ATen.dir/DLConvertor.cpp.o\r\n[  6%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ExpandUtils.cpp.o\r\n[  7%] Building CXX object src/ATen/CMakeFiles/ATen.dir/Formatting.cpp.o\r\n[  8%] Building CXX object src/ATen/CMakeFiles/ATen.dir/Scalar.cpp.o\r\n[  9%] Building CXX object src/ATen/CMakeFiles/ATen.dir/UndefinedTensor.cpp.o\r\n[ 10%] Building CXX object src/ATen/CMakeFiles/ATen.dir/UndefinedType.cpp.o\r\n[ 11%] Building CXX object src/ATen/CMakeFiles/ATen.dir/Utils.cpp.o\r\n[ 12%] Building CXX object src/ATen/CMakeFiles/ATen.dir/native/Indexing.cpp.o\r\n[ 13%] Building CXX object src/ATen/CMakeFiles/ATen.dir/native/NativeFunctions.cpp.o\r\n[ 15%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteStorage.cpp.o\r\n[ 16%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteType.cpp.o\r\n[ 17%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUByteTensor.cpp.o\r\n[ 18%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharStorage.cpp.o\r\n[ 19%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharType.cpp.o\r\n[ 20%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUCharTensor.cpp.o\r\n[ 21%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUDoubleStorage.cpp.o\r\n[ 22%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUDoubleType.cpp.o\r\n[ 23%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUDoubleTensor.cpp.o\r\n[ 24%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUFloatStorage.cpp.o\r\n[ 25%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUFloatType.cpp.o\r\n[ 26%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUFloatTensor.cpp.o\r\n[ 27%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUIntStorage.cpp.o\r\n[ 29%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUIntType.cpp.o\r\n[ 30%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUIntTensor.cpp.o\r\n[ 31%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPULongStorage.cpp.o\r\n[ 32%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPULongType.cpp.o\r\n[ 33%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPULongTensor.cpp.o\r\n[ 34%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUShortStorage.cpp.o\r\n[ 35%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUShortType.cpp.o\r\n[ 36%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUShortTensor.cpp.o\r\n[ 37%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUHalfStorage.cpp.o\r\n[ 38%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUHalfType.cpp.o\r\n[ 39%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/CPUHalfTensor.cpp.o\r\n[ 40%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/SparseCPUByteType.cpp.o\r\n[ 41%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/SparseCPUByteTensor.cpp.o\r\n[ 43%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/SparseCPUCharType.cpp.o\r\n[ 44%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/SparseCPUCharTensor.cpp.o\r\n[ 45%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/SparseCPUDoubleType.cpp.o\r\n[ 46%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/SparseCPUDoubleTensor.cpp.o\r\n[ 47%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/SparseCPUFloatType.cpp.o\r\n[ 48%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/SparseCPUFloatTensor.cpp.o\r\n[ 49%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/SparseCPUIntType.cpp.o\r\n[ 50%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/SparseCPUIntTensor.cpp.o\r\n[ 51%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/SparseCPULongType.cpp.o\r\n[ 52%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/SparseCPULongTensor.cpp.o\r\n[ 53%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/SparseCPUShortType.cpp.o\r\n[ 54%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/SparseCPUShortTensor.cpp.o\r\n[ 55%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/Type.cpp.o\r\n[ 56%] Building CXX object src/ATen/CMakeFiles/ATen.dir/ATen/Copy.cpp.o\r\n[ 58%] Building C object src/ATen/CMakeFiles/ATen.dir/__/TH/THGeneral.c.o\r\n[ 59%] Building C object src/ATen/CMakeFiles/ATen.dir/__/TH/THHalf.c.o\r\n[ 60%] Building C object src/ATen/CMakeFiles/ATen.dir/__/TH/THAllocator.c.o\r\n[ 61%] Building C object src/ATen/CMakeFiles/ATen.dir/__/TH/THSize.c.o\r\n[ 62%] Building C object src/ATen/CMakeFiles/ATen.dir/__/TH/THStorage.c.o\r\n[ 63%] Building C object src/ATen/CMakeFiles/ATen.dir/__/TH/THTensor.c.o\r\n[ 64%] Building C object src/ATen/CMakeFiles/ATen.dir/__/TH/THBlas.c.o\r\n[ 65%] Building C object src/ATen/CMakeFiles/ATen.dir/__/TH/THLapack.c.o\r\n[ 66%] Building C object src/ATen/CMakeFiles/ATen.dir/__/TH/THLogAdd.c.o\r\n[ 67%] Building C object src/ATen/CMakeFiles/ATen.dir/__/TH/THRandom.c.o\r\n[ 68%] Building C object src/ATen/CMakeFiles/ATen.dir/__/TH/THFile.c.o\r\n[ 69%] Building C object src/ATen/CMakeFiles/ATen.dir/__/TH/THDiskFile.c.o\r\n[ 70%] Building C object src/ATen/CMakeFiles/ATen.dir/__/TH/THMemoryFile.c.o\r\n[ 72%] Building C object src/ATen/CMakeFiles/ATen.dir/__/TH/THAtomic.c.o\r\n[ 73%] Building C object src/ATen/CMakeFiles/ATen.dir/__/TH/THVector.c.o\r\n[ 74%] Building C object src/ATen/CMakeFiles/ATen.dir/__/TH/generic/simd/convolve.c.o\r\n[ 75%] Building C object src/ATen/CMakeFiles/ATen.dir/__/TH/generic/simd/convolve5x5_sse.c.o\r\n[ 76%] Building C object src/ATen/CMakeFiles/ATen.dir/__/TH/vector/AVX.c.o\r\n[ 77%] Building C object src/ATen/CMakeFiles/ATen.dir/__/TH/generic/simd/convolve5x5_avx.c.o\r\n[ 78%] Building C object src/ATen/CMakeFiles/ATen.dir/__/THNN/init.c.o\r\n[ 79%] Building C object src/ATen/CMakeFiles/ATen.dir/__/THS/THSTensor.c.o\r\n[ 80%] Linking CXX shared library libATen.so\r\n[ 80%] Built target ATen\r\nScanning dependencies of target scalar_test\r\nScanning dependencies of target scalar_tensor_test\r\nScanning dependencies of target broadcast_test\r\nScanning dependencies of target basic\r\n[ 83%] Building CXX object src/ATen/test/CMakeFiles/scalar_tensor_test.dir/scalar_tensor_test.cpp.o\r\n[ 83%] Building CXX object src/ATen/test/CMakeFiles/basic.dir/basic.cpp.o\r\n[ 83%] Building CXX object src/ATen/test/CMakeFiles/scalar_test.dir/scalar_test.cpp.o\r\n[ 84%] Building CXX object src/ATen/test/CMakeFiles/broadcast_test.dir/broadcast_test.cpp.o\r\n[ 86%] Linking CXX executable broadcast_test\r\n[ 87%] Linking CXX executable scalar_tensor_test\r\nsrc/ATen/test/CMakeFiles/broadcast_test.dir/build.make:109: recipe for target \'src/ATen/test/broadcast_test\' failed\r\nCMakeFiles/Makefile2:247: recipe for target \'src/ATen/test/CMakeFiles/broadcast_test.dir/all\' failed\r\nsrc/ATen/test/CMakeFiles/scalar_tensor_test.dir/build.make:109: recipe for target \'src/ATen/test/scalar_tensor_test\' failed\r\nCMakeFiles/Makefile2:321: recipe for target \'src/ATen/test/CMakeFiles/scalar_tensor_test.dir/all\' failed\r\n[ 88%] Linking CXX executable scalar_test\r\nsrc/ATen/test/CMakeFiles/scalar_test.dir/build.make:109: recipe for target \'src/ATen/test/scalar_test\' failed\r\nCMakeFiles/Makefile2:210: recipe for target \'src/ATen/test/CMakeFiles/scalar_test.dir/all\' failed\r\n[ 89%] Linking CXX executable basic\r\nsrc/ATen/test/CMakeFiles/basic.dir/build.make:109: recipe for target \'src/ATen/test/basic\' failed\r\nCMakeFiles/Makefile2:284: recipe for target \'src/ATen/test/CMakeFiles/basic.dir/all\' failed\r\nMakefile:127: recipe for target \'all\' failed']","[""\r\nScanning dependencies of target THD\r\n[  4%] Building CXX object CMakeFiles/THD.dir/base/data_channels/DataChannelTCP.cpp.o\r\n[  8%] Building CXX object CMakeFiles/THD.dir/base/init_methods/InitMethodEnv.cpp.o\r\n[ 16%] Building CXX object CMakeFiles/THD.dir/base/TensorDescriptor.cpp.o\r\n[ 16%] Building CXX object CMakeFiles/THD.dir/base/data_channels/DataChannelMPI.cpp.o\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp: In member function ‚Äòat::Tensor thd::DataChannelMPI::_newLikeFlat(std::vector<at::Tensor>&) const‚Äô:\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:157:57: warning: narrowing conversion of ‚Äò((& t)->at::Tensor::is_cuda() ? (& t)->at::Tensor::get_device() : -1l)‚Äô from ‚Äòlong int‚Äô to ‚Äòint‚Äô inside { } [-Wnarrowing]\r\n   AutoGPU gpu_guard { t.is_cuda() ? t.get_device() : -1 };\r\n                                                         ^\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:158:45: warning: narrowing conversion of ‚Äò(& tensors)->std::vector<_Tp, _Alloc>::size<at::Tensor, std::allocator<at::Tensor> >()‚Äô from ‚Äòstd::vector<at::Tensor>::size_type {aka long unsigned int}‚Äô to ‚Äòlong int‚Äô inside { } [-Wnarrowing]\r\n   std::vector<int64_t> sizes { tensors.size() };  // sizes = [output.size()] + input.sizes()\r\n                                             ^\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:158:45: warning: narrowing conversion of ‚Äò(& tensors)->std::vector<_Tp, _Alloc>::size<at::Tensor, std::allocator<at::Tensor> >()‚Äô from ‚Äòstd::vector<at::Tensor>::size_type {aka long unsigned int}‚Äô to ‚Äòlong int‚Äô inside { } [-Wnarrowing]\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:160:10: error: ‚Äòinput‚Äô was not declared in this scope\r\n   return input.type().tensor(sizes);\r\n          ^\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp: In member function ‚Äòvirtual void thd::DataChannelMPI::scatter(std::vector<at::Tensor>&, at::Tensor&, thd::rank_type, THDGroup)‚Äô:\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:233:7: error: no match for ‚Äòoperator!‚Äô (operand type is ‚Äòat::Tensor‚Äô)\r\n   if (!output.contiguous())\r\n       ^\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:233:7: note: candidate is:\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:233:7: note: operator!(bool) <built-in>\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:233:7: note:   no known conversion for argument 1 from ‚Äòat::Tensor‚Äô to ‚Äòbool‚Äô\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp: In member function ‚Äòvirtual void thd::DataChannelMPI::reduce(at::Tensor&, THDReduceOp, thd::rank_type, THDGroup)‚Äô:\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:281:7: error: no match for ‚Äòoperator!‚Äô (operand type is ‚Äòat::Tensor‚Äô)\r\n   if (!data.contiguous())\r\n       ^\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:281:7: note: candidate is:\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:281:7: note: operator!(bool) <built-in>\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:281:7: note:   no known conversion for argument 1 from ‚Äòat::Tensor‚Äô to ‚Äòbool‚Äô\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp: In member function ‚Äòvirtual void thd::DataChannelMPI::broadcast(at::Tensor&, thd::rank_type, THDGroup)‚Äô:\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:299:7: error: no match for ‚Äòoperator!‚Äô (operand type is ‚Äòat::Tensor‚Äô)\r\n   if (!data.contiguous())\r\n       ^\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:299:7: note: candidate is:\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:299:7: note: operator!(bool) <built-in>\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:299:7: note:   no known conversion for argument 1 from ‚Äòat::Tensor‚Äô to ‚Äòbool‚Äô\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp: In member function ‚Äòvirtual void thd::DataChannelMPI::send(thd::Scalar&, thd::rank_type)‚Äô:\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:309:56: error: request for member ‚ÄòscalarType‚Äô in ‚Äò(& data)->thd::Scalar::type()‚Äô, which is of non-class type ‚Äòthd::RPCType‚Äô\r\n   MPI_Send(data.data(), 1, mpi_datatype.at(data.type().scalarType()),\r\n                                                        ^\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp: In member function ‚Äòvirtual void thd::DataChannelMPI::receive(thd::Scalar&, thd::rank_type)‚Äô:\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:324:17: error: ‚Äòstruct thd::Scalar‚Äô has no member named ‚Äòdata_ptr‚Äô\r\n   MPI_Recv(data.data_ptr(), 1, mpi_datatype.at(data.type().scalarType()),\r\n                 ^\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:324:60: error: request for member ‚ÄòscalarType‚Äô in ‚Äò(& data)->thd::Scalar::type()‚Äô, which is of non-class type ‚Äòthd::RPCType‚Äô\r\n   MPI_Recv(data.data_ptr(), 1, mpi_datatype.at(data.type().scalarType()),\r\n                                                            ^\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp: In member function ‚Äòvirtual void thd::DataChannelMPI::receive(at::Tensor&, thd::rank_type)‚Äô:\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:346:10: error: ‚Äòstatus‚Äô was not declared in this scope\r\n   return status.MPI_SOURCE;\r\n          ^\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:346:17: error: return-statement with a value, in function returning 'void' [-fpermissive]\r\n   return status.MPI_SOURCE;\r\n                 ^\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp: In member function ‚Äòvirtual thd::DataChannelMPI::RequestMPI* thd::DataChannelMPI::ireceive(at::Tensor&, thd::rank_type)‚Äô:\r\n/home/michael/pytorch/torch/lib/THD/base/data_channels/DataChannelMPI.cpp:384:10: error: cannot convert ‚Äòstd::unique_ptr<thd::DataChannelMPI::RequestMPI>‚Äô to ‚Äòthd::DataChannelMPI::RequestMPI*‚Äô in return\r\n   return request;\r\n          ^\r\n[ 20%] Building CXX object CMakeFiles/THD.dir/base/init_methods/InitMethodUtils.cpp.o\r\n[ 24%] Building CXX object CMakeFiles/THD.dir/base/init_methods/InitMethodFile.cpp.o\r\nCMakeFiles/THD.dir/build.make:62: recipe for target 'CMakeFiles/THD.dir/base/data_channels/DataChannelMPI.cpp.o' failed\r\nmake[2]: *** [CMakeFiles/THD.dir/base/data_channels/DataChannelMPI.cpp.o] Error 1\r\nmake[2]: *** Waiting for unfinished jobs....\r\nCMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/THD.dir/all' failed\r\nmake[1]: *** [CMakeFiles/THD.dir/all] Error 2\r\nMakefile:127: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\n""]",[],0,0
465,pytorch,19742,closed,"Serialization of tensors with pickle.dumps seems to be inconsistent, leading to inconsistent redis cache hit/miss","## üêõ Bug

<!-- A clear and concise description of what the bug is. -->

Scenario: 

Redis cache set up 
call set pickle.dumps(tensor) some_value
call get pickle.dumps(tensor)
call get pickle.dumps(tensor) (different call)

hit on call 1,
potential miss on call 2 

These inconsistent hits/misses occur over O(1000) calls to the the same tensors in objects repeated ~9 times


## To Reproduce

Steps to reproduce the behavior: (skeleton)
0. import redis
0. import time
0. value = torch.tensor(...) 
0. start = time.time()
1. redis_cache = redis.StrictRedis(host=""localhost"", port=6379, db=0)  # redis_cache
2. redis_cache.set(pickle.dumps(value), pickle.dumps((hidden, cell)))
3. cache_return = redis_cache.get(pickle.dumps(value))
            if cache_return is not None:
                logger.info(""Cache hit for {} took {} ms"".format(value, time.time()-start))
                return pickle.loads(cache_return)
4. cache_return = redis_cache.get(pickle.dumps(value))
            if cache_return is not None:
                logger.info(""Cache hit for {} took {} ms"".format(value, time.time()-start))
                return pickle.loads(cache_return)

(Cache miss logging would happen after .set() is called)

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

Example Log

2019-04-25 14:04:00,344 ID_1 [INFO] cache_encoder: Cache hit for tensor([[ 2, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,  3]]) took 0.00019598007202148438 ms
2019-04-25 14:04:00,347 ID_1 [INFO] cache_encoder: Cache miss for tensor([[ 2, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,  3]]) took 0.001859426498413086 ms

The weird thing is the hit happened before the miss, so it looks like something is unreliable with the serialisation

2019-04-25 14:04:00,303 ID_1 [INFO] cache_encoder: Cache hit for tensor([[ 2, 12,  3]]) took 0.0001957416534423828 ms
2019-04-25 14:04:00,312 ID_1 [INFO] cache_encoder: Cache miss for tensor([[ 2, 12,  3]]) took 0.0018868446350097656 ms

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

Consistent cache hits for the same tensor after: set pickle.dumps() cache_return

## Environment

 - PyTorch Version (e.g., 1.0): 1.01
 - OS (e.g., Linux): Ubuntu 16.04
 - How you installed PyTorch (, , source): pip install
 - Build command you used (if compiling from source):
 - Python version: 3.6
 - CUDA/cuDNN version: 8
 - GPU models and configuration: GeForce GTX 1050
 - Any other relevant information: 

## Additional context

<!-- Add any other context about the problem here. -->

When using str(value) instead of pickle.dumps(value), the cache hits are consistent (however, for large tensors ... get injected into the str() output, so this ultimately fails). This makes me think the problem is in how pickle.dumps() serializes the tensors. I saw a similar issue: https://github.com/pytorch/pytorch/issues/9168 for changes (improving speed) along these lines (moving to byteIO).

Because of the related issue I thought I would post the issue here.
",awaiting response (this tag is deprecated) module: pickle module: serialization triaged,"[""Can you provide a minimum example that reproduces this behavior and that indicate that this is a PyTorch problem?\r\n\r\nFrom your description, it might imply that `pickle.dumps(tensor)` is not always the same. But I couldn't reproduce this locally.\r\n\r\nIt could also be that the algorithm that Redis uses to cache large objects is not perfect and is subject to missing it a few times. Hard to say without a repro."", ""we spent some time (@fmassa did) trying to repro it, but couldn't. Hopefully you can carve out a smaller test case. If you end up doing so, please reopen the issue."", ""@soumith @BonShillings  I manage to reproduce this issue _consistently_ on my machine, with pytorch 1.3.1.\r\n\r\n```\r\nadd11 = lambda: torch.Tensor([1]) + torch.Tensor([1])\r\nadd11() == add11()  # return tensor([True])\r\nhash(add11()) == hash(add11())  # returns True\r\npickle.dumps(add11()) == pickle.dumps(add11())  # returns False\r\npickle.dumps(add11().data) == pickle.dumps(add11().data)  # returns False\r\npickle.dumps(torch.Tensor([1])) == pickle.dumps(torch.Tensor([1]))  # returns False\r\n```\r\nbut: `pickle.dumps(torch.Tensor([1])) == pickle.dumps(torch.Tensor([1]).data)` is True.\r\n\r\nSimilarly, this issue reflects on Python's LRU cache.\r\nExample code:\r\n```\r\n@functools.lru_cache(None)\r\ndef add(x, y):\r\n return x + y\r\n\r\nfor _ in range(10):\r\n tmp = add(torch.Tensor([1]), torch.Tensor([1]))\r\nadd.cache_info().hits  # returns 0\r\n```""]",[],"['conda', 'pip']",0,0
466,pytorch,4573,closed,"Documentation for ""char_rnn_classification_tutorial"" has wrong output cell calculation","@soumith Looks like [this tutorial](http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html) has a bug in this code:      

    

Looks like the output of the forward pass is done on the input and NOT on the processed data as computed by the RNN cell (ie: self.i2h(combined))   

So, this:    
   
Should become:    

  
If you guys agree, I'm happy to make that minor (but frustrating if you don't know what's going on) change to the docs.   ",,"[""Sorry, but what exactly is the problem? It doesn't matter if you use `hidden` or `combined` - you're composing linear operators, so you can alternatively use a single one, which is what the code does in here. Also, if you want to discuss that please open an issue in pytorch/tutorials repo (that's where they are hosted). Thanks!"", '@apaszke ok will open there. \r\n\r\nThe issue is that seld.i2o is intended to map the hidden into classification space (with the softmax).  \r\n\r\nHowever, as it‚Äôs written now, it is mapping the input directly without processing in the cell. \r\n\r\nThat is, current code:\r\n\r\nht = (xt * W_hx + b) + (ht_prev * W_hh + b)  \r\noutput_t = (xt * W_hx + b ) + (ht_prev * W_hh + b)  \r\n\r\nBut it really should be:\r\noutput_t = (ht * W_io + b)']","['python   \r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\nclass RNN(nn.Module):\r\n    def __init__(self, input_size, hidden_size, output_size):\r\n        super(RNN, self).__init__()\r\n\r\n        self.hidden_size = hidden_size\r\n\r\n        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\r\n        self.i2o = nn.Linear(input_size + hidden_size, output_size)\r\n        self.softmax = nn.LogSoftmax(dim=1)\r\n\r\n    def forward(self, input, hidden):\r\n        combined = torch.cat((input, hidden), 1)\r\n        hidden = self.i2h(combined)\r\n        output = self.i2o(combined)\r\n        output = self.softmax(output)\r\n        return output, hidden\r\n\r\n    def initHidden(self):\r\n        return Variable(torch.zeros(1, self.hidden_size))\r\n\r\nn_hidden = 128\r\nrnn = RNN(n_letters, n_hidden, n_categories)\r\n', 'python    \r\n    def forward(self, input, hidden):\r\n        combined = torch.cat((input, hidden), 1)\r\n        hidden = self.i2h(combined)\r\n        output = self.i2o(combined)\r\n        output = self.softmax(output)\r\n        return output, hidden\r\n', 'python    \r\n    def forward(self, input, hidden):\r\n        combined = torch.cat((input, hidden), 1)\r\n        hidden = self.i2h(combined)\r\n        output = self.i2o(hidden) # this line needs to be changed    \r\n        output = self.softmax(output)\r\n        return output, hidden\r\n']",[],0,0
467,pytorch,5198,closed,[feature request] support batch diag,Should be easy to implement. But we need to figure out a good name/API. We can't reuse  as it will be ambiguous when input is 2D.  sounds weird to me..,,"['how about `torch.diag(batch=True)`? We can have similar extensions to other operators...', ""Looks reasonable. I'd say numpy's API on this one is kinda bad. The 1D=>2D and 2D=>1D operations shouldn't have the same name. But people are already used to it.\r\n\r\nBut this way is inconsistent with `mm` and `bmm`, and other things.."", '@SsnL, any reason you closed this? This would be a nice feature.', ""@gwgundersen It's implemented now :)"", '@gwgundersen at https://github.com/pytorch/pytorch/pull/6718', 'Is the batch semantics (being able to address arbitrary dimension for expansion) also available now for the 1D=>2D case or just for the 2D=>1D?\r\n', ""Hello everyone, I'm not sure that requested feature addressed in other PRs (according to https://discuss.pytorch.org/t/batch-of-diagonal-matrix/13560/2). I need a `tensorflow.matrix_diag` method and new `torch.diagonal` works with multi dimensional cases, but doesn't do job in reverse (I don't mean AD reverse mode).\r\n\r\n@SsnL , @soumith ^^^\r\n\r\nThanks"", ""I'm not sure I understand it correctly, but maybe you are looking for `diagflat`?"", 'Why this issue is closed? the  #6718 does not implement this feature. @SsnL ', ""> I'm not sure I understand it correctly, but maybe you are looking for `diagflat`?\r\n\r\ndiagflat does not work in batch. "", ""Oh, I see (I hadn't checked the documentation of TF before, my bad :-) )\r\n\r\nIndeed, an equivalent to `tensorflow.matrix_diag` is currently not implemented in PyTorch, but can be implemented with a few lines:\r\n```python\r\ndef matrix_diag(diagonal):\r\n    N = diagonal.shape[-1]\r\n    shape = diagonal.shape[:-1] + (N, N)\r\n    device, dtype = diagonal.device, diagonal.dtype\r\n    result = torch.zeros(shape, dtype=dtype, device=device)\r\n    indices = torch.arange(result.numel(), device=device).reshape(shape)\r\n    indices = indices.diagonal(dim1=-2, dim2=-1)\r\n    result.view(-1)[indices] = diagonal\r\n    return result\r\n```\r\n\r\n---\r\nEDIT: there is probably a way of avoiding creating a full `indices` matrix as big as the result, and directly creating it using smarter operations, but I was too lazy to do it :-) )"", ""@awav @rudaoshi Ah I'm sorry. I opened the issue to request the n-D to (n-1)-D case. I opened a new issue on matrix diag at https://github.com/pytorch/pytorch/issues/12160 .""]",[],"['torch.diag', 'bdiag']",0,0
468,pytorch,7544,closed,How to make dropout 'not scaling' during training?,"Dropout always scale 1/(1-p) during training, but I want to get the original outputs. How to get it?",,"['You can set the modules to evaluation mode, even though you will still be training.']",[],[],0,0
469,pytorch,992,closed,loading GPU checkpoints on CPU with remapping is trying to load torch.cuda,"As @jzbontar reports:
serialization storage remapping is trying to load somewhere in the codepath.

> I get an error when loading a cuda tensor on a CPU only machine.  Here is the code I use: https://gist.github.com/jzbontar/b50f8c9dd22e49ff68c7c91dad63166a. The error is: AssertionError('\nFound no NVIDIA driver on your system... Both machines use pytorch version 0.1.10.",high priority,"['Fixed', 'Sorry, but I am still getting this error on 0.1.12_2 (installed from binary) on a CPU-only machine. Specifically, the error msg\r\n```\r\nLoading model file... Traceback (most recent call last):\r\n  File ""./translate.py"", line 193, in <module>\r\n    translator = onmt.Translator(opt)\r\n  File ""/home/user/chunk/onmt/Translator.py"", line 12, in __init__\r\n    checkpoint = torch.load(opt.model, map_location=lambda storage, loc: storage)\r\n  File ""/usr/local/lib/python2.7/dist-packages/torch/serialization.py"", line 229, in load\r\n    return _load(f, map_location, pickle_module)\r\n  File ""/usr/local/lib/python2.7/dist-packages/torch/serialization.py"", line 377, in _load\r\n    result = unpickler.load()\r\n  File ""/usr/local/lib/python2.7/dist-packages/torch/cuda/__init__.py"", line 272, in __new__\r\n    _lazy_init()\r\n  File ""/usr/local/lib/python2.7/dist-packages/torch/cuda/__init__.py"", line 84, in _lazy_init\r\n    _check_driver()\r\n  File ""/usr/local/lib/python2.7/dist-packages/torch/cuda/__init__.py"", line 58, in _check_driver\r\n    http://www.nvidia.com/Download/index.aspx"""""")\r\nAssertionError: (AssertionError(\'\\nFound no NVIDIA driver on your system. Please check that you\\nhave an NVIDIA GPU and installed a driver from\\nhttp://www.nvidia.com/Download/index.aspx\',), <class \'torch.cuda.FloatTensor\'>, ())\r\n```\r\n\r\nseemed like `unpickler.load()` somehow triggered cuda init?', 'Oops I found out that the model was trained using an older version of pytorch. Retrained it and all is fine. Thanks for all your hard work!', 'i also face this error, as I check version, it is code after ""Fix map_location in torch.load #1006 ""\r\ncould you plz help to check\r\nTraceback (most recent call last):\r\n  File ""test.py"", line 40, in <module>\r\n    model = torch.load(""model_epoch_x3_L1_500.pth"")\r\n  File "".local/lib/python2.7/site-packages/torch/serialization.py"", line 231, in load\r\n    return _load(f, map_location, pickle_module)\r\n  File ""/.local/lib/python2.7/site-packages/torch/serialization.py"", line 379, in _load\r\n    result = unpickler.load()\r\n  File ""/lib/python2.7/site-packages/torch/serialization.py"", line 350, in persistent_load\r\n    data_type(size), location)\r\n  File ""/.local/lib/python2.7/site-packages/torch/serialization.py"", line 85, in default_restore_location\r\n    result = fn(storage, location)\r\n  File ""/.local/lib/python2.7/site-packages/torch/serialization.py"", line 67, in _cuda_deserialize\r\n    return obj.cuda(device_id)\r\n  File ""/.local/lib/python2.7/site-packages/torch/_utils.py"", line 58, in _cuda\r\n    with torch.cuda.device(device):\r\n  File ""/.local/lib/python2.7/site-packages/torch/cuda/__init__.py"", line 125, in __enter__\r\n    _lazy_init()\r\n  File ""/.local/lib/python2.7/site-packages/torch/cuda/__init__.py"", line 84, in _lazy_init\r\n    _check_driver()\r\n  File ""/.local/lib/python2.7/site-packages/torch/cuda/__init__.py"", line 58, in _check_driver\r\n    http://www.nvidia.com/Download/index.aspx"""""")\r\nAssertionError: \r\nFound no NVIDIA driver on your system. Please check that you\r\nhave an NVIDIA GPU and installed a driver from']",[],[],0,0
470,pytorch,9990,closed,Build fails in caffe2 git master with OpenCL ON,"I'm getting the following compile error when trying to build caffe2 git master:


- Cmake command and output: [https://bpaste.net/show/f14fd917039e](https://bpaste.net/show/f14fd917039e)

- Caffe2 from pytorch stable release v0.4.1 builds fine with the same cmake options as above (OpenCL ON).

- Caffe2 git master builds fine when using the same cmake options as above and switching OpenCL to .

- The error seems be **not** related to the gcc version used. The same error occurs with gcc8, gcc7 and gcc5.4.

**System Information:**
**OS:** Arch Linux x86_64
**Compiler:** gcc 8.1.1 (as mentioned, the same error also occurs with gcc7 and gcc5.4)
**Caffe2:** git master
**OpenCL headers:** 2.2.20170516",caffe2 todo,"['I encountered the same issue...\r\nCurrently, have no solution.', 'I am facing the same problem. Have you solved it?', '> Have you solved it?\r\n\r\nThere is still no solution currently.', 'Is there a plan to make Tensor a template again? If not, is there a workaround for OpenCL/AMD support besides compiling from around the time that it was originally added (see commit cd86d4c5548c15e0bc9773565fa4fad73569f948)', '@maxgillett the answer is no to both questions. However Rocm / AMD support exists and is tested in continuous integration (though the builds for that are undocumented)', 'closing with wontfix tag', 'Just disable OpenCL? \r\n']","['\r\n[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/opencl/context.cc.o\r\nIn file included from /storage/linux/abs/caffe2-cpu-git/src/pytorch-git/caffe2/core/registry.h:21,\r\n                 from /storage/linux/abs/caffe2-cpu-git/src/pytorch-git/caffe2/core/flags.h:23,\r\n                 from /storage/linux/abs/caffe2-cpu-git/src/pytorch-git/caffe2/core/logging.h:10,\r\n                 from /storage/linux/abs/caffe2-cpu-git/src/pytorch-git/caffe2/core/allocator.h:6,\r\n                 from /storage/linux/abs/caffe2-cpu-git/src/pytorch-git/caffe2/core/context.h:9,\r\n                 from /storage/linux/abs/caffe2-cpu-git/src/pytorch-git/caffe2/contrib/opencl/context.h:4,\r\n                 from /storage/linux/abs/caffe2-cpu-git/src/pytorch-git/caffe2/contrib/opencl/context.cc:1:\r\n/storage/linux/abs/caffe2-cpu-git/src/pytorch-git/caffe2/contrib/opencl/context.cc:10:18: error: ‚Äòcaffe2::Tensor‚Äô is not a template\r\n CAFFE_KNOWN_TYPE(Tensor<OpenCLContext>);\r\n                  ^~~~~~\r\n/storage/linux/abs/caffe2-cpu-git/src/pytorch-git/caffe2/core/typeid.h:403:28: note: in definition of macro ‚ÄòCAFFE_KNOWN_TYPE‚Äô\r\n   CaffeTypeId TypeMeta::Id<T>() {                                                  \\\r\n                            ^\r\n/storage/linux/abs/caffe2-cpu-git/src/pytorch-git/caffe2/core/typeid.h:403:15: error: redefinition of ‚Äòstatic caffe2::CaffeTypeId caffe2::TypeMeta::Id() [with T = caffe2::Tensor]‚Äô\r\n   CaffeTypeId TypeMeta::Id<T>() {                                                  \\\r\n               ^~~~~~~~\r\n/storage/linux/abs/caffe2-cpu-git/src/pytorch-git/caffe2/contrib/opencl/context.cc:10:1: note: in expansion of macro ‚ÄòCAFFE_KNOWN_TYPE‚Äô\r\n CAFFE_KNOWN_TYPE(Tensor<OpenCLContext>);\r\n ^~~~~~~~~~~~~~~~\r\n/storage/linux/abs/caffe2-cpu-git/src/pytorch-git/caffe2/core/typeid.h:425:22: note: ‚Äòstatic caffe2::CaffeTypeId caffe2::TypeMeta::Id() [with T = caffe2::Tensor]‚Äô previously declared here\r\n   inline CaffeTypeId TypeMeta::Id<T>() {  \\\r\n                      ^~~~~~~~\r\n/storage/linux/abs/caffe2-cpu-git/src/pytorch-git/caffe2/core/typeid.h:446:1: note: in expansion of macro ‚ÄòCAFFE_DECLARE_KNOWN_TYPE‚Äô\r\n CAFFE_DECLARE_KNOWN_TYPE(1, Tensor);\r\n ^~~~~~~~~~~~~~~~~~~~~~~~\r\nmake[2]: *** [caffe2/CMakeFiles/caffe2.dir/build.make:362: caffe2/CMakeFiles/caffe2.dir/contrib/opencl/context.cc.o] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:1737: caffe2/CMakeFiles/caffe2.dir/all] Error 2\r\nmake: *** [Makefile:141: all] Error 2\r\n']","[""-DUSE_OPENCL:BOOL='OFF'""]",0,0
471,pytorch,2240,closed,Error building torch from source for python 2.7 on macOS,"Hello,

Here is the error.
I'm running macOS 10.12.6 (16G29), python 2.7 on anaconda 3, CUDA 8.0.90, Apple LLVM version 8.0.0 (clang-800.0.42.1). I installed LLVM 8.0.0 specifically since it's the one listed on cuda docs.
I have successfully built pyTorch from source for python 3.6 on this computer before, so I'm clueless how this is happening.
I will just use python 3 for now. Hope this information is helpful to you.


",,"['Any luck getting past this one?', 'going back to f1fd4ac7edeb2477404883b21c3c4fe8420c0757 makes the build work', 'I am getting the same error while trying to compile on MacOS 10.12.6. Any suggestions?', ""Succeeded building torch from source on the latest commit. Don't forget to `python3 setup.py clean` before build."", '@Fuchai I think [this PR](https://github.com/pytorch/pytorch/pull/2319) fixes your issue and has now been merged. Please try and build again from master and let us know :)', 'Yes the build from master was successful on both python 3 and python 2. Thank you!\r\n`python setup.py clean` could be helpful. Not sure whether I did it last time.']",[],"[""torch/csrc/autograd/functions/convolution.cpp:188:20: error: no viable\r\n      conversion from 'std::vector<long>' to 'IntList' (aka\r\n      'ArrayRef<long long>')\r\n    output.resize_(output_size(input, weight));\r\n                   ^~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include/ATen/ArrayRef.h:34:9: note: \r\n      candidate constructor (the implicit copy constructor) not viable: no known\r\n      conversion from 'std::vector<long>' to 'const at::ArrayRef<long long> &'\r\n      for 1st argument\r\n  class ArrayRef {\r\n        ^\r\n/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include/ATen/ArrayRef.h:34:9: note: \r\n      candidate constructor (the implicit move constructor) not viable: no known\r\n      conversion from 'std::vector<long>' to 'at::ArrayRef<long long> &&' for\r\n      1st argument\r\n/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include/ATen/ArrayRef.h:57:18: note: \r\n      candidate constructor not viable: no known conversion from\r\n      'std::vector<long>' to 'const long long &' for 1st argument\r\n    /*implicit*/ ArrayRef(const T &OneElt)\r\n                 ^\r\n/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include/ATen/ArrayRef.h:83:18: note: \r\n      candidate constructor not viable: no known conversion from\r\n      'std::vector<long>' to 'const std::initializer_list<long long> &' for 1st\r\n      argument\r\n    /*implicit*/ ArrayRef(const std::initializer_list<T> &Vec)\r\n                 ^\r\n/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include/ATen/ArrayRef.h:70:18: note: \r\n      candidate template ignored: could not match 'long long' against 'long'\r\n    /*implicit*/ ArrayRef(const std::vector<T, A> &Vec)\r\n                 ^\r\n/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include/ATen/ArrayRef.h:75:28: note: \r\n      candidate template ignored: could not match 'array' against 'vector'\r\n    /*implicit*/ constexpr ArrayRef(const std::array<T, N> &Arr)\r\n                           ^\r\n/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include/ATen/ArrayRef.h:80:28: note: \r\n      candidate template ignored: could not match 'const long long [N]' against\r\n      'std::vector<long>'\r\n    /*implicit*/ constexpr ArrayRef(const T (&Arr)[N]) : Data(Arr), Length(N) {}\r\n                           ^\r\n/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include/ATen/TensorMethods.h:13:41: note: \r\n      passing argument to parameter 'size' here\r\ninline Tensor & Tensor::resize_(IntList size) {\r\n                                        ^\r\nclang -fno-strict-aliasing -I/Users/JasonHu/anaconda3/envs/condapy2/include -arch x86_64 -DNDEBUG -g -fwrapv -O3 -Wall -I/Users/JasonHu/Downloads/pytorch-master -I/Users/JasonHu/Downloads/pytorch-master/torch/csrc -I/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include -I/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include/TH -I/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include/THPP -I/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include/THNN -I/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include/ATen -I/Users/JasonHu/anaconda3/envs/condapy2/lib/python2.7/site-packages/numpy/core/include -I/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include/THD -I/usr/local/cuda/include -I/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include/THCUNN -I/usr/local/cuda/include -I/Users/JasonHu/anaconda3/envs/condapy2/include/python2.7 -c /Users/JasonHu/Downloads/pytorch-master/torch/csrc/generated/TensorHalf.cpp -o build/temp.macosx-10.7-x86_64-2.7/Users/JasonHu/Downloads/pytorch-master/torch/csrc/generated/TensorHalf.o -D_THP_CORE -std=c++11 -Wno-write-strings -fno-strict-aliasing -DWITH_NUMPY -DWITH_DISTRIBUTED -DWITH_CUDA -DCUDA_LIB_PATH=/usr/local/cuda/lib -DWITH_CUDNN\r\nclang -fno-strict-aliasing -I/Users/JasonHu/anaconda3/envs/condapy2/include -arch x86_64 -DNDEBUG -g -fwrapv -O3 -Wall -I/Users/JasonHu/Downloads/pytorch-master -I/Users/JasonHu/Downloads/pytorch-master/torch/csrc -I/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include -I/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include/TH -I/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include/THPP -I/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include/THNN -I/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include/ATen -I/Users/JasonHu/anaconda3/envs/condapy2/lib/python2.7/site-packages/numpy/core/include -I/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include/THD -I/usr/local/cuda/include -I/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include/THCUNN -I/usr/local/cuda/include -I/Users/JasonHu/anaconda3/envs/condapy2/include/python2.7 -c /Users/JasonHu/Downloads/pytorch-master/torch/csrc/generated/TensorInt.cpp -o build/temp.macosx-10.7-x86_64-2.7/Users/JasonHu/Downloads/pytorch-master/torch/csrc/generated/TensorInt.o -D_THP_CORE -std=c++11 -Wno-write-strings -fno-strict-aliasing -DWITH_NUMPY -DWITH_DISTRIBUTED -DWITH_CUDA -DCUDA_LIB_PATH=/usr/local/cuda/lib -DWITH_CUDNN\r\n1 error generated.\r\nclang -fno-strict-aliasing -I/Users/JasonHu/anaconda3/envs/condapy2/include -arch x86_64 -DNDEBUG -g -fwrapv -O3 -Wall -I/Users/JasonHu/Downloads/pytorch-master -I/Users/JasonHu/Downloads/pytorch-master/torch/csrc -I/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include -I/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include/TH -I/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include/THPP -I/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include/THNN -I/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include/ATen -I/Users/JasonHu/anaconda3/envs/condapy2/lib/python2.7/site-packages/numpy/core/include -I/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include/THD -I/usr/local/cuda/include -I/Users/JasonHu/Downloads/pytorch-master/torch/lib/tmp_install/include/THCUNN -I/usr/local/cuda/include -I/Users/JasonHu/anaconda3/envs/condapy2/include/python2.7 -c /Users/JasonHu/Downloads/pytorch-master/torch/csrc/generated/TensorChar.cpp -o build/temp.macosx-10.7-x86_64-2.7/Users/JasonHu/Downloads/pytorch-master/torch/csrc/generated/TensorChar.o -D_THP_CORE -std=c++11 -Wno-write-strings -fno-strict-aliasing -DWITH_NUMPY -DWITH_DISTRIBUTED -DWITH_CUDA -DCUDA_LIB_PATH=/usr/local/cuda/lib -DWITH_CUDNN\r\nerror: command 'clang' failed with exit status 1""]",0,0
472,pytorch,12127,closed,how to use the checkpoint function,"I  try to use the checkpoint function,when I train,gpu keeps the memory but don't work  and the train progress don't work either(without any error print). How can I make the checkpoint function work.
",,"['hi @JamesKasperYu , please use https://discuss.pytorch.org for support and questions']",[],[],0,0
473,pytorch,20717,closed,Problem with DataLoader,"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce
I have training , validation and test dataset. I have created dataloaders for three of them with training shuffle = True and for valdiation and test shuffle = False . So in one of the code epoch loop contains iterator over train and validation and in another one , epoch loop contains iterator over train , validation and test.

Check the output of the code 1 and 2
1. https://www.kaggle.com/suchith0312/pytorch-dataloader-testing?scriptVersionId=14425205
2. https://www.kaggle.com/suchith0312/pytorch-dataloader-testing?scriptVersionId=14425182

## Expected behavior

The train id's from output of files must be same but they are different. 

## Environment
 - PyTorch Version : 1.0.1.post2
 - Python version: 3.6.6
 - CUDA version : 10.0
- Cudnn version 7.4.2 ",,"['Your iteration over test advances rng state.', '> Your iteration over test advances rng state.\r\n\r\nHow can I prevent that or there is no way to get the outputs of two codes same. ', 'A possible solution could be: https://github.com/pytorch/pytorch/pull/20749']",[],[],0,0
474,pytorch,6472,closed,Advanced indexing doesn't validate negative indexes (regression),"Minimum example:

    torch.zeros(10)[torch.LongTensor([-11])] # returns bogus result on 0.4, error on 0.3.1

These all fail as they should:

    torch.zeros(10)[10]
    torch.zeros(10)[-11]
    torch.zeros(10)[torch.LongTensor([10])]

Works correctly on 0.3.1.post2.
Doesn't work on 0.4.0a0+df039e2.

Compiled with:
",,"[""Thanks for reporting. This appears to be fixed on latest master. I can't reproduce. Try pull and rebuild."", 'Did so. Advanced indexing still doesn\'t raise an exception on negative out-of-bounds indexes, but returns an arbitrary result:\r\n```\r\n‚ü´ python\r\nPython 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 18:21:58) \r\n[GCC 7.2.0] on linux\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import torch\r\n>>> torch.__version__\r\n\'0.4.0a0+04c215b\'\r\n>>> torch.zeros(10)[-11]\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\nIndexError: index -11 is out of bounds for dimension 0 with size 10\r\n>>> torch.zeros(10)[torch.LongTensor([-11])]\r\n\r\n1.00000e-09 *\r\n  1.8028\r\n[torch.FloatTensor of size (1,)]\r\n\r\n>>> \r\n```\r\n', 'I can reproduce ', ""Cuda version works through. I'll add a check to TH.""]","[""\r\n‚ü´ gcc-5 -v\r\nUsing built-in specs.\r\nCOLLECT_GCC=gcc-5\r\nCOLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/5/lto-wrapper\r\nTarget: x86_64-linux-gnu\r\nConfigured with: ../src/configure -v --with-pkgversion='Ubuntu 5.5.0-8ubuntu1' --with-bugurl=file:///usr/share/doc/gcc-5/README.Bugs --enable-languages=c,ada,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-5 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --enable-default-pie --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-5-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-5-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-5-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu\r\nThread model: posix\r\ngcc version 5.5.0 20171010 (Ubuntu 5.5.0-8ubuntu1) \r\n\r\n""]",[],0,0
475,pytorch,23002,closed,Wrong input to Categorical makes Python quit unexpectedly,"I pass a zero tensor to Categorical, which I expect torch should throw me some exception. Instead, Python quits unexpectedly



",,"['![Screen Shot 2019-07-17 at 3 57 28 PM](https://user-images.githubusercontent.com/3657457/61417473-0ef09580-a8ac-11e9-81ce-ac8dd3c74a70.png)\r\n', 'this is expected, division by zero on integers generates a Hardware FPE.\r\n\r\nI think you meant to do `Categorical(torch.tensor([0.0]))`\r\n']","['\r\n(base) czxttkl-mbp:Horizon czxttkl$ python3\r\nPython 3.7.1 (v3.7.1:260ec2c36a, Oct 20 2018, 00:48:40) \r\n[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import torch\r\n>>> torch.__version__\r\n\'1.1.0.post2\'\r\n>>> from torch.distributions.categorical import Categorical\r\n>>> Categorical(torch.tensor([0]))\r\nFloating point exception: 8\r\n\r\n# Python completely quits\r\n']",[],0,0
476,pytorch,19305,closed,libtorch gpu efficiency,"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce
here is my code

auto out = faceDetect->forward({ tensor_image }).toTuple();
c10::cuda::CUDACachingAllocator::emptyCache();
...some tensor ops on gpu
boundingBoxesOfOne = torch::masked_select(boundingBoxesOfOne, mask);

faceDetect is a sfd net,after forward(),if i call emptyCache(),it will cost 170ms time to emptyCache;if i don't call emptyCache(),masked_select will cost 170ms;so i don't know how to optimize this situationÔºåthksÔºÅ


## Environment

libtorch Version 1.0 :
OS windows10:
CUDA/cuDNN version: 9.0
GPU models and configuration: GTX 1060

## Additional context

<!-- Add any other context about the problem here. -->
",,"['CUDA is asynchronous. The time duration you observed is the time needed to forward though the network.', '> CUDA is asynchronous. The time duration you observed is the time needed to forward though the network.\r\n\r\nforward() has cost time Ôºåthe result out has got,so is that cuda need to empty cached memory asynchronouslyÔºübut 170ms is too longÔºü\r\nor you mean the result out has not got,when i access the variableÔºåit will cost time to wait result outÔºü\r\nif it is the second situationÔºåhow to set CUDA operations synchronousÔºüthksÔºÅ ', '> > CUDA is asynchronous. The time duration you observed is the time needed to forward though the network.\r\n> \r\n> forward() has cost time Ôºåthe result out has got,so is that cuda need to empty cached memory asynchronouslyÔºübut 170ms is too longÔºü\r\n> or you mean the result out has not got,when i access the variableÔºåit will cost time to wait result outÔºü\r\n> if it is the second situationÔºåhow to set CUDA operations synchronousÔºüthksÔºÅ\r\n\r\none more question,one image convert to tensor to forward() cost 200ms,three same images cat to a tensor to forward() cost 590ms,that means a batch images to forwad() is serial not parallel?', '`CUDA_LAUNCH_BLOCKING=1` environment variable will make cuda calls synchronous (at the cost of some performance). You can also do `torch.cuda.synchronize()` or in C++ simply `cudaSynchronize` from the CUDA API.\r\n\r\n', 'if you have questions, ask on https://discuss.pytorch.org', 'CUDA is asynchronous. The time duration you observed is the time needed to forward though the network.\r\n\r\n\r\n\r\n\r\n------------------ ÂéüÂßãÈÇÆ‰ª∂ ------------------\r\nÂèë‰ª∂‰∫∫: ""yinxiaochuan""<notifications@github.com>;\r\nÂèëÈÄÅÊó∂Èó¥: 2019Âπ¥5Êúà22Êó•(ÊòüÊúü‰∏â) Êôö‰∏ä6:09\r\nÊî∂‰ª∂‰∫∫: ""pytorch/pytorch""<pytorch@noreply.github.com>;\r\nÊäÑÈÄÅ: ""È£éÂÆáÂêåÈÇπ""<634162865@qq.com>;""Author""<author@noreply.github.com>;\r\n‰∏ªÈ¢ò: Re: [pytorch/pytorch] libtorch gpu efficiency (#19305)\r\n\r\n\r\n\r\n\r\nI also think this is a bug. I convert a pytorch model to cpp w/ and w/o mask_seleced operator. The inference  time in cpp will increase about 60ms.\r\n \r\n‚Äî\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub, or mute the thread.']",[],[],0,0
477,pytorch,16045,closed,"Bug: fail to throw error when computing loss between tensors with shapes [n, 1] and [n]","## Issue description
When computing the loss between  and  with shapes such as  and , an error should be thrown.  Indeed, my colleagues tell me this was the case in version 0.4, but lost in version 1.0.

## Code example

If you leave y.reshape commented out, the correct estimates of this straightforward linear regression problem are not found and the loss fails to decrease into a region near the global optimum of this convex problem.  Yet, no error is raised.

If you uncomment y.reshape, the correct estimates are found and the loss decreases in the anticipated way.


## System Info
Collecting environment information...
PyTorch version: 1.0.0
Is debug build: No
CUDA used to build PyTorch: None

OS: Ubuntu 16.04.5 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CMake version: Could not collect

Python version: 3.5
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip] Could not collect
[conda] blas                      1.0                         mkl  
[conda] mkl                       2017.0.3                      0  
[conda] torch                     1.0.0                     <pip>
[conda] torchvision               0.2.1                     <pip>
",,"['Same behavior on \r\nCollecting environment information...\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.1.85\r\nGPU models and configuration:\r\nGPU 0: TITAN V\r\nGPU 1: TITAN V\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: TITAN Xp\r\n\r\nNvidia driver version: 390.42\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] mkl                       2017.0.3                      0\r\n[conda] torch                     1.0.0                     <pip>\r\n[conda] torchvision               0.2.1                     <pip>', 'MSELoss does broadcasting between tensors of shape `(n, 1)` and `(n)`, resulting in an output of shape `(n, n)` which is probably not what you wanted.', 'In my humble opinion, this behavior is sufficiently unexpected to warrant throwing an error or warning.', 'I have encountered this exact problem, and debugging it was very time consuming, I believe a warning at least would be very important. \r\n\r\nIf this mistake is made in the first iterations on a model and is not noticed, it might invalidate some ideas all together. ', ""Let's say a user writes their MSELoss without using the nn.Module, as \r\n```\r\ndef loss(x, t):\r\n   torch.sum((x - t) ** 2) / x.numel()\r\n```\r\nWould you expect this function to warn against inputs of size `n` and `n, 1`?"", ""> Let's say a user writes their MSELoss without using the nn.Module, as\r\n> \r\n> ```\r\n> def loss(x, t):\r\n>    torch.sum((x - t) ** 2) / x.numel()\r\n> ```\r\n> Would you expect this function to warn against inputs of size `n` and `n, 1`?\r\n\r\nIndeed, this is a case that cannot be caught, without a way of disabling broadcasting globally through a config setting, or through the use of a context manager, though I assume that would be hard to implement and quite error-prone. \r\n\r\nBut I would argue that the sole purpose of `MSELoss` (and other loss functions) is to compute a loss between 2 theoretical same-shape tensors, and so the cases where broadcasting would be useful is limited. I cannot think of a real-world, useful example for such scenario, as usually you want the data inside a batch to be as varied as possible.\r\n\r\nI would say that some warnings are better than no warnings.\r\nFor example, tensorflow is performing checks in such cases, for example: https://github.com/tensorflow/tensorflow/issues/1406\r\n\r\n"", 'cc @ailzhang @fmassa do you remember why we decided to let these losses broadcast?', 'are you sure we decided to let these losses broadcast as opposed to these modules just happened to be implemented via broadcastable ops?', 'Sure, it makes sense to disable broadcasting for these.', '+1 to throw a warning/error for a shape mismatch.\r\nWe had a few threads in the past about bad training when the tensors were silently broadcasted using `nn.MSELoss`.\r\n[Example1](https://discuss.pytorch.org/t/sklearn-model-to-pytorch-model/33532/3?u=ptrblck)\r\n[Example2](https://discuss.pytorch.org/t/pytorch-beginner-model-fit-problems/37292/2?u=ptrblck)', 'Having spent way too long finding this error, I would be happy to submit a pull request -- but I would appreciate your help/thoughts on a few considerations first.\r\n\r\n1. I can\'t think of any case in which broadcasting is useful for MSELoss(). Therefore, throwing an error is seemingly more appropriate to me. However, I\'ve often seen training functions like this:\r\n\r\n```python\r\ndef train_epoch(model:nn.Module, dl:DataLoader, opt:optim.Optimizer, loss_func:LossFunction)->None:\r\n    ""Simple training of `model` for 1 epoch of `dl` using optim `opt` and loss function `loss_func`.""\r\n    model.train()\r\n    for xb,yb in dl:\r\n        loss = loss_func(model(xb), yb)\r\n        loss.backward()\r\n        opt.step()\r\n        opt.zero_grad()\r\n```\r\n\r\nfrom fastai https://github.com/fastai/fastai/blob/5ac8dfe800dddb6ede6ce4178a1d68de7b087f6e/fastai/basic_train.py . `model(xb)` has shape (N,1) while `yb` has shape (N,). In this case, the DataLoader is returning the shapes that cause this function not to work (I am also submitting an issue to fastai). Therefore, returning an error here every time seems a bit harsh unless DataLoader is changed.\r\n\r\n2. Considering (1), I might propose something like the following fix in F.mse_loss() (adapted from the warnings/errors in F.binary_cross_entropy()). I am going off the assumption that anytime one inputs a tensor of dimension 1, they do not want to do any special broadcasting. This seems fair to me, but feel free to disagree -- I have never contributed here before and definitely would appreciate comments.\r\n\r\n```python\r\nif not (target.size() == input.size()):\r\n    # first handle the one-dimensional cases \r\n    if target.ndimension() == 1:\r\n        target = target.unsqueeze(1)\r\n    elif input.ndimension() == 1:\r\n        input = input.unsqueeze(1)\r\n    else:\r\n        warnings.warn(""Using a target size ({}) that is different to the input size ({}).""\r\n                      ""This will likely lead to incorrect results due to broadcasting. ""\r\n                      ""Please ensure they have the same size."".format(target.size(), input.size()))\r\n        \r\nif input.numel() != target.numel():\r\n    raise ValueError(""Target and input must have the same number of elements. target nelement ({}) ""\r\n                     ""!= input nelement ({})"".format(target.numel(), input.numel()))\r\n```\r\n\r\n', ""Hmmm I don't remember why we decided to broadcast for some loss functions, it might be when some of the loss functions were implemented in python, they got broadcast for free and people start seeing mismatched behaviors. \r\nI'm fine with throwing errors on this. \r\ncc: @gchanan  @fmassa your thoughts? "", 'we should throw a warning when broadcasted for nn.MSELoss i think.', ""i'm also supportive of a hard error, but a warning would be sufficient"", '@mc-robinson  Feel free to submit a PR if you are interested! ', 'Ok will do! Do we think my code was alright or do we just want to stick with a warning for now before broadcasting?', 'Hard error has the potential to impact fair amount of training code... For example, the following code would lose its generality unless we deal with DataLoader shapes automatically in the loss function\r\n\r\n```python\r\nmodel.train()\r\nfor xb,yb in dl:\r\n        loss = loss_func(model(xb), yb)\r\n        loss.backward()\r\n        opt.step()\r\n        opt.zero_grad()\r\n```', 'Yea I think we might prefer a big warning at this moment, and make it a hard error in a future release when users are used to this behavior. A hard BC breaking is bad I guess.', ""Ok, I'll at least get a warning out there for now"", 'warnings without a way to turn them off are pretty annoying.  You could do something like add two parameters:\r\n`warn_broadcasting=True, error_broadcasting=False`.\r\n\r\nwhich would allow us to support all the combinations pretty easily but still have backwards compatibility and still give the user control.', 'closing for now since the warning has been implemented.  If there is sufficient request for an error option, please open another issue.', 'Just wondering is there any reason this warning is not implemented for other (or all) losses?\r\nI just spend an awful lot of time on this for the L1Loss.']","['predicted', 'target', 'target.shape = [100]', 'predicted.shape = [100, 1]', '\r\n# Version 1.0\r\n# Set up straightforward linear regression problem.\r\nimport torch\r\nimport torch.nn as nn\r\nsample_size = 100\r\nx = torch.randn((sample_size,2))\r\neps = torch.randn(sample_size )\r\ny = 10 + 9*x[:,0] - 2*x[:, 1] +  eps\r\n#y = y.reshape(sample_size, 1)\r\nmodel = nn.Linear(2, 1)\r\n\r\noptimizer = torch.optim.SGD(model.parameters(), lr=.1)\r\nloss_func = nn.MSELoss()\r\nfor epoch in range(30):\r\n   optimizer.zero_grad()\r\n   yhat = model(x)\r\n   loss_val = loss_func(yhat, y)\r\n   loss_val.backward()\r\n   optimizer.step()\r\n   print(loss_val.item())\r\n\r\nprint(model.weight.data)\r\nprint(model.bias.data)\r\n\r\n']",[],0,0
478,pytorch,10303,closed,cuda runtime error(59): device-side assert when running torch.topk,"The code and dataset were the same as before, but I got the following error. However they could work well about a half mouth ago.
the pytorch's version is 0.3.1, BUT I find the version 0.4.0 also meeting with the same error
raceback (most recent call last):
File ""main.py"", line 341, in
main()
File ""main.py"", line 141, in main
train(train_loader, model, criterion, optimizer, epoch)
File ""main.py"", line 192, in train
prec1, prec5 = accuracy(output.data, target, topk=(1,5))
File ""main.py"", line 329, in accuracy
_, pred = output.topk(maxk, 1, True, True)
RuntimeError: invalid argument 5: k not in range for dimension at /pytorch/torch/lib/THC/generic/THCTensorTopK.cu:21
/pytorch/torch/lib/THCUNN/ClassNLLCriterion.cu:101: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [0,0,0] Assertion t >= 0 && t < n_classes failed.
/pytorch/torch/lib/THCUNN/ClassNLLCriterion.cu:101: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [1,0,0] Assertion t >= 0 && t < n_classes failed.
/pytorch/torch/lib/THCUNN/ClassNLLCriterion.cu:101: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [2,0,0] Assertion t >= 0 && t < n_classes failed.
/pytorch/torch/lib/THCUNN/ClassNLLCriterion.cu:101: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [3,0,0] Assertion t >= 0 && t < n_classes failed.
THCudaCheck FAIL file=/pytorch/torch/lib/THC/generic/THCStorage.c line=184 error=59 : device-side assert triggered
terminate called after throwing an instance of 'std::runtime_error'
what(): cuda runtime error (59) : device-side assert triggered at /pytorch/torch/lib/THC/generic/THCStorage.c:184
Aborted (core dumped)",,"[""Looks like topk is failing? Could you please post a minimal reproducible example as well as let us know what pytorch version you're using? We fixed a few topk bugs recently."", '@zou3519 Thanks! Looking forward to your any replies.\r\nThe code and dataset were the same as before, but I got the following error. However they could work well about a half mouth ago.\r\nthe pytorch\'s version is 0.3.1, BUT I find the version 0.4.0 also meeting with the same error\r\nraceback (most recent call last):\r\nFile ""main.py"", line 341, in \r\nmain()\r\nFile ""main.py"", line 141, in main\r\ntrain(train_loader, model, criterion, optimizer, epoch)\r\nFile ""main.py"", line 192, in train\r\nprec1, prec5 = accuracy(output.data, target, topk=(1,5))\r\nFile ""main.py"", line 329, in accuracy\r\n_, pred = output.topk(maxk, 1, True, True)\r\nRuntimeError: invalid argument 5: k not in range for dimension at /pytorch/torch/lib/THC/generic/THCTensorTopK.cu:21\r\n/pytorch/torch/lib/THCUNN/ClassNLLCriterion.cu:101: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [0,0,0] Assertion t >= 0 && t < n_classes failed.\r\n/pytorch/torch/lib/THCUNN/ClassNLLCriterion.cu:101: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [1,0,0] Assertion t >= 0 && t < n_classes failed.\r\n/pytorch/torch/lib/THCUNN/ClassNLLCriterion.cu:101: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [2,0,0] Assertion t >= 0 && t < n_classes failed.\r\n/pytorch/torch/lib/THCUNN/ClassNLLCriterion.cu:101: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [3,0,0] Assertion t >= 0 && t < n_classes failed.\r\nTHCudaCheck FAIL file=/pytorch/torch/lib/THC/generic/THCStorage.c line=184 error=59 : device-side assert triggered\r\nterminate called after throwing an instance of \'std::runtime_error\'\r\nwhat(): cuda runtime error (59) : device-side assert triggered at /pytorch/torch/lib/THC/generic/THCStorage.c:184\r\nAborted (core dumped)', 'Oh I see. The assert is actually in NLLLoss. The problem is that there exists some target value in your `target` tensor that is not in the range [0, n_classes). Could you assert this in your code and see if that assert is triggered?', 'Thanks for your advice.\r\nI will check it', 'closing via https://github.com/pytorch/pytorch/issues/10303#issuecomment-411853972', 'I kind of had the same problem. I was trying to modify the transfer learning tutorial (https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) with the hymenoptera dataset (bees and ants dataset) to make it fit to my project. The problem was that i had 3 classes and the tutorial has 2.  So i found that you have to define the number of outputs from the fully connected network (the output layer). So the only thing i had to do was changing ""model_ft.fc = nn.Linear(num_ftrs, 2)"" to ""model_ft.fc = nn.Linear(num_ftrs, 3)"". Hope it helps somebody :)', '@KasperJuunge that fixed my code! Thanks so much for your comment']",[],[],0,0
479,pytorch,24093,closed,pytorch forced to use cuda 10 after new update,"## üêõ Bug
When installing pytorch with conda in a new environment, it always installs compiled with the cuda version 10 and cudatoolkit=10.0. Whether I conda install pytorch 1.2, 1.1, 1.0.1, or 1.0 with cuda90 it always tries to install the cuda 10 version of the package as well as cudatoolkit=10.0. This is via the pytorch conda channel. My machine has an older driver version that can't easily be upgraded. I also tried pip installing from https://download.pytorch.org/whl/cu90/stable and it still insists on using cuda 10.

## To Reproduce
conda create --name test_env python=3.6
conda activate test_env
conda install pytorch=1.1 cuda90 -c pytorch
conda list

In python when you run torch.version.cuda it also lists cuda 10.

The following NEW packages will be INSTALLED:

    blas:           1.0-mkl
    cffi:           1.12.3-py36h2e261b9_0
    cuda90:         1.0-h6433d27_0                        pytorch
    cudatoolkit:    10.0.130-0
    intel-openmp:   2019.4-243
    libgfortran-ng: 7.3.0-hdf63c60_0
    mkl:            2019.4-243
    mkl_fft:        1.0.12-py36ha843d7b_0
    mkl_random:     1.0.2-py36hd81dba3_0
    ninja:          1.9.0-py36hfd86e86_0
    numpy:          1.16.4-py36h7e9f1db_0
    numpy-base:     1.16.4-py36hde5b4d6_0
    pycparser:      2.19-py36_0
    pytorch:        1.1.0-py3.6_cuda10.0.130_cudnn7.5.1_0 pytorch

Proceed ([y]/n)? n

## Expected behavior

Should be able to select the cuda version.

## Environment

Collecting environment information...
PyTorch version: N/A
Is debug build: N/A
CUDA used to build PyTorch: N/A

OS: Ubuntu 16.04.3 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.6
Is CUDA available: N/A
CUDA runtime version: Could not collect
GPU models and configuration:
GPU 0: TITAN Xp
GPU 1: TITAN Xp
GPU 2: TITAN Xp
GPU 3: TITAN Xp

Nvidia driver version: 390.67
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.3

Versions of relevant libraries:
[pip] Could not collect
[conda] Could not collect

",module: build module: docs triaged,"['the relevant command for pytorch 1.1 was `conda install pytorch=1.1 cudatoolkit=9.0 -c pytorch`', 'I need to update the ""Old Versions"" page.', 'closed via https://github.com/pytorch/pytorch.github.io/commit/98f41eab2df3bfa0fccbfedebccaa5d624f13767']",[],[],0,0
480,pytorch,15016,closed,torch.tril does not support 0-sized dims,"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:



<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior



## Environment

PyTorch version: 1.0.0a0+54b5dd9
Is debug build: No
CUDA used to build PyTorch: 9.2.88

OS: CentOS Linux 7 (Core)
GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)
CMake version: version 3.12.2

Python version: 3.5
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: Tesla M40
GPU 1: Tesla M40

Nvidia driver version: 396.26
cuDNN version: Probably one of the following:
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.0.5
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.3
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.6.0.20
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.6.0.21
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so.7.1.2
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so.7.4.1
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn_static.a
/usr/local/fbcode/gcc-5-glibc-2.23/lib/libcudnn.so.6
/usr/local/fbcode/gcc-5-glibc-2.23/lib/libcudnn.so.6.0.20
/usr/local/fbcode/gcc-5-glibc-2.23/lib/libcudnn.so.6.0.21
/usr/local/fbcode/gcc-5-glibc-2.23/lib/libcudnn.so.7.1.2
/usr/local/fbcode/gcc-5-glibc-2.23/lib/libcudnn.so.7.1.4
/usr/local/fbcode/platform007/lib/libcudnn.so.7.1.4

Versions of relevant libraries:
[pip] Could not collect
[conda] magma-cuda92              2.4.0                         1    pytorch
[conda] torch                     1.0.0a0+54b5dd9           <pip>

",todo,['Fixed by #15257'],"['python\r\n>>> import torch\r\n>>> torch.ones(0, 3)\r\ntensor([], size=(0, 3))\r\n>>> torch.ones(0, 3).tril(0)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\nRuntimeError: invalid argument 1: expected a matrix at ../aten/src/TH/generic/THTensorMoreMath.cpp:1255\r\n>>> torch.ones(1, 3).tril(0)\r\ntensor([[1., 0., 0.]])\r\n', 'python\r\n>>> import numpy as np\r\n>>> np.tril(np.ones((0, 3)))\r\narray([], shape=(0, 3), dtype=float64)\r\n']",[],0,0
481,pytorch,14653,closed,[JIT] Error reporting on imported modules highlights serialization code,"Repro:



Output:



This has the potential to be super confusing to end users. We should probably preserve the original source for highlighting",oncall: jit,['This is fixed on master'],"[""\r\nimport torch\r\n\r\ndef baz(x):\r\n    return torch.neg(x) + 3.14159\r\n\r\nbaz_traced = torch.jit.trace(baz, (torch.rand(5, 4, 3),))\r\n\r\n@torch.jit.script\r\ndef foo(x):\r\n    bar = torch.neg(torch.mm(x, torch.zeros(3, 4, 5)))\r\n    return baz_traced(bar)\r\n\r\nfoo.save('test.out')\r\n\r\nimported = torch.jit.load('test.out')\r\nimported(torch.rand(3, 4))\r\n"", '\r\nTraceback (most recent call last):\r\n  File ""error_report.py"", line 16, in <module>\r\n    imported(torch.rand(3, 4))\r\n  File ""/Users/jamesreed/onnx-fairseq/pytorch/torch/nn/modules/module.py"", line 489, in __call__\r\n    result = self.forward(*input, **kwargs)\r\nRuntimeError:\r\nmatrices expected, got 2D, 3D tensors at ../aten/src/TH/generic/THTensorMath.cpp:935:\r\noperation failed in interpreter:\r\nop_version_set = 0\r\ndef forward(self,\r\n    x: Tensor) -> Tensor:\r\n  _0 = torch.zeros([3, 4, 5], dtype=6, layout=0, device=[0, -1])\r\n  bar = torch.neg(torch.mm(x, _0))\r\n                  ~~~~~~~~ <--- HERE\r\n  _1 = torch.add(torch.neg(bar), CONSTANTS.c0, alpha=1)\r\n  return _1\r\n']",[],0,0
482,pytorch,29024,closed,Speed slows down after model quantization,"my model is based on E-inception,after quantized  , i found it forward slower,
the original model run on CPU cost 200ms and 20ms on GPU , but the quantized model cost 290ms,

is it caused by the operation of layers concatenate?? 


cc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a",oncall: quantization,"[""@jiacheng1gujiaxin I don't think there's enough information here to give an answer to why your model is slower. Please re-open the issue with sufficient code to reproduce the issue (for example, the model definition).\r\n\r\nMaybe @jerryzh168 (cc'd; or anyone else working on quantization) knows something generally about why models may be slower after quantization?"", 'have you ever solved this problem?', 'We need more information for this, can you post your original and annotated model?\r\n\r\nSent from my iPhone\r\n\r\nOn Nov 27, 2019, at 03:56, Du Yang <notifications@github.com> wrote:\r\n\r\n\ufeff\r\n\r\nhave you ever solved this problem?\r\n\r\n‚Äî\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/pytorch/pytorch/issues/29024?email_source=notifications&email_token=ABF2R2LCV3I5IK7NBLLDC43QVZN7TA5CNFSM4JHXO442YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEFJITRQ#issuecomment-559057350>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/ABF2R2PXVQATMAXD6KK7QNDQVZN7TANCNFSM4JHXO44Q>.\r\n']",[],[],0,0
483,pytorch,31483,closed,undefined symbol: _Py_ZeroStruct,"Although I have installed ""pytorch"" and ""text"" with python and python3 using


When I want to text_classification tutorial, I get this error


I don't know what is missing here. Any way to fix that?

cc @ezyang",module: binaries triaged,"['This looks like an error with the envoronment, can you try creating a clean virtual environment, install pytorch and torchtext according to instructions. If your problem persists, please also provide the output of environment collection script\r\n\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```', ""Conda is not installed and I don't have root access to install it.\r\nRight now, I see \r\n```\r\n$ python ../collect_env.py                        Collecting environment information...\r\nPyTorch version: 1.4.0a0+2488231\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-16ubuntu3) 7.3.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 2.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.168\r\nGPU models and configuration:\r\nGPU 0: GeForce RTX 2080 Ti\r\nGPU 1: TITAN V\r\n\r\nNvidia driver version: 418.56\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.5\r\n[pip] torch==1.4.0a0+2488231\r\n[pip] torchtext==0.4.0\r\n[pip] torchvision==0.5.0a0+5c03d59\r\n[conda] Could not collect\r\n```"", 'I have uploaded the log files here\r\n[pt.zip](https://github.com/pytorch/pytorch/files/3985199/pt.zip)\r\n.\r\nAs you can see, both PyTorch and Text have been successfully installed via python2.7 and python3.\r\n', 'You env log shows that you have functional torch install with python 2, can you try running the same env collection script with python3?', 'It says `collect` not found.\r\n\r\n```\r\n$ python3 collect_env.py\r\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-16ubuntu3) 7.3.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: N/A\r\nCUDA runtime version: 10.1.168\r\nGPU models and configuration:\r\nGPU 0: GeForce RTX 2080 Ti\r\nGPU 1: TITAN V\r\n\r\nNvidia driver version: 418.56\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.4\r\n[pip3] torch==1.4.0a0+2488231\r\n[pip3] torchtext==0.4.0\r\n[conda] Could not collect\r\n```\r\n\r\nBut I can not install collect via pip3\r\n```\r\n$ pip3 install --user collect\r\nCollecting collect\r\n  Downloading https://files.pythonhosted.org/packages/cf/5e/c0f0f51d081665374a2c219ea4ba23fb1e179b70dded96dc16606786d828/collect-0.1.1.tar.gz\r\nCollecting couchdbkit>=0.5.7 (from collect)\r\n  Downloading https://files.pythonhosted.org/packages/a1/13/9e9ff695a385c44f62b4766341b97f2bd8b596962df2a0beabf358468b70/couchdbkit-0.6.5.tar.gz (81kB)\r\n    100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81kB 1.8MB/s\r\n    Complete output from command python setup.py egg_info:\r\n    Traceback (most recent call last):\r\n      File ""<string>"", line 1, in <module>\r\n      File ""/tmp/pip-build-hg_juowy/couchdbkit/setup.py"", line 25, in <module>\r\n        long_description = file(\r\n    NameError: name \'file\' is not defined\r\n\r\n    ----------------------------------------\r\nCommand ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-build-hg_juowy/couchdbkit/\r\n```', ""It says there's no pytorch under python3:\r\n```\r\nPyTorch version: N/A\r\n```\r\nPlease create a clean python 3 environment and follow either binary install instructions from pytorch.org, or installation instructions from github. \r\n"", 'Thank you. Seems that the installation was broken although everything looks normal.', 'libtorch_python.so: undefined symbol: _Py_ZeroStruct #42450\r\n']","['\r\npython setup.py install --user\r\npython3 setup.py install --user\r\n', '\r\n$ python3 main.py\r\nTraceback (most recent call last):\r\n  File ""main.py"", line 1, in <module>\r\n    import torch\r\n  File ""/home/mnaderan/.local/lib/python3.6/site-packages/torch/__init__.py"", line 81, in <module>\r\n    from torch._C import *\r\nImportError: /home/mnaderan/.local/lib/python3.6/site-packages/torch/lib/libtorch_python.so: undefined symbol: _Py_ZeroStruct\r\ncorrupted size vs. prev_size\r\nAborted (core dumped)\r\n']",[],0,0
484,pytorch,20048,closed,Forking is not possible anymore when using any PyTorch version from about 2 months ago,"I have been building PyTorch from source in the past couple of months. Since around 1 (or maybe 2) month(s) ago I am unable to do forward passing to my models when using the  package of PyTorch. If I'm not mistaken, the last version of PyTorch that I built and still allows me to use ing is [this](https://github.com/pytorch/pytorch/tree/bad4442) one; or more precisely, the version of PyTorch that I am using now and allows me to do ing is .

Not being able to do ing is pretty annoying cause I have written all of my work flow in a way that it depends on using  with ing. In one part of my work flow I start tens of processes and do forward passing on a model that is on the CPU memory. This is pretty useful and super efficient since the weights of the model are not being copied every time I start a new process. However, if I  processes the weights of the model needs to get copied and it's not gonna be efficient anymore. 

In another part of my framework, I start a process while the model is still on CPU's memory and then do  within the process and the model is then copied to the GPU memory. This is a bit inefficient but still allows me to start a 4-5 processes (depending on the GPU memory) and do forward-pass my inputs. 

Here's a pseudocode of what I do:



With the current versions of PyTorch (since 1-2 months ago) I cannot do this anymore. I'm afraid this is happening due to some indirect affects of some commits which did not intend to disable ing. So I wonder if you guys can either revert the changes that have caused this or enable this feature again.

Also, when using some of the PyTorch versions from the master branch (since 1-2 months ago) I might get the following if I do  within an started process:



This might be somewhat relevant to [lazy initialization of CUDA](https://github.com/pytorch/pytorch/pull/2811).",high priority module: cuda module: regression triaged,"['It is surprising that your program ever worked because CUDA runtime does not support `fork`. See this note being added: https://github.com/pytorch/pytorch/pull/19904', ""@SsnL @soumith @ezyang I know CUDA was never `fork`-safe and that's why I didn't do `model.cuda()` before starting any of the processes: cause once CUDA is initialized within one environment/process you cannot use the model in a child environment/process. However, in the past I could initialize CUDA within each child process as long as I did not do `model.cuda()` before the child process was started. This was possible probably because starting a new process is like starting a new environment and CUDA wouldn't have an issue with that.\r\n\r\nFor instance, if my model weights take about 300MBs on GPU memory and I would run 5 processes and do `model.cuda()` within each of them I would see 5 different python processes running on a single GPU and the model weights would be copied 5 times. So I would need **5 x 300 + 5 x CUDA_overhead** MBs of memory on the GPU. This was not great of course since I would have love dto only have one copy of the model on the GPU but it served for my purposes. With the recent versions of PyTorch this is not possible anymore ... .\r\n\r\nWhat is the solution now? Why can't I even do `fork`ing when the model is on the CPU memory? How come my code always works up until that specific version of PyTorch but not now?"", 'there was a bug that was in the last 2 months, but was fixed on master. I am trying to find the relevant issue and fix.', ""I've been searching for the fix, but I think it was titled as something else -- so it's been hard to find. But I can confirm that it was definitely fixed."", ""@soumith Fixed meaning that I should be able to run my code, at least when on CPU? But that's not what's happening now. I cannot use my model in any process."", ""are you saying it doesn't work like that on master, or on a commit from 2 months ago? I thought master is fine (forking before initializing CUDA should work fine)"", ""I think he's saying that the code works 2 months ago, but not anymore on master."", ""@soumith Compiling PyTorch from master (or any commit from the past two month'ish) does not allow me to do `fork`ing. I was able to do `fork`ing on commits from 2 months ago and before that."", 'So, one interesting thing to note, is that a single call to `cudaGetDeviceCount` is sufficient to initialize CUDA and cause forking to fail. I made some relevant changes during a time when I was not aware of this distinction (#18445), and this might be the cause of the problem.\r\n\r\n```\r\n#include <cuda.h>\r\n#include <stdio.h>\r\n#include <unistd.h>\r\n\r\nint main() { \r\n  int c;\r\n  cudaGetDeviceCount(&c);\r\n  fork();\r\n  void* p;\r\n  cudaError_t err = cudaMalloc(&p, 100);\r\n  if (err != cudaSuccess) { \r\n    printf(""failed\\n"");\r\n  } \r\n}\r\n```', ""Update: The particular PR I linked looks blameless, as it doesn't seem to cause cudaGetDeviceCount to be called any more than it was previously. Still investigating."", 'This script passes on master:\r\n\r\n```\r\nimport torch\r\nimport os\r\n\r\nprint(torch.zeros(2))\r\n\r\nos.fork()\r\n\r\nprint(torch.zeros(2, device=\'cuda\'))\r\n```\r\n\r\nSo, at least the ""basic"" CPU functionality is not broken.', '@Amir-Arsalan I ""reconstituted"" your small script into a running reproducer:\r\n\r\n```\r\nimport torch\r\nfrom torch.multiprocessing import Process\r\nimport torchvision\r\n\r\nmodel = torchvision.models.resnet50()\r\ninputList = [torch.randn(1, 3, 224, 224), torch.randn(1, 3, 224, 224)]\r\n\r\ndef doForwardPass(input):\r\n    model.cuda()\r\n\r\nprocesses = []\r\nfor i in range(2):\r\n    processes.append(Process(target=doForwardPass, kwargs={\'input\': inputList[i]}))\r\n    processes[-1].start()\r\n\r\nfor i in range(2):\r\n    processes[i].join()\r\n```\r\n\r\nHowever, on master, this does not reproduce the problem. Can you dump your entire script here, by any chance? Otherwise, we\'ll need a more detailed reproducer.', ""@ezyang I'll try to provide a script on Monday."", ""@Amir-Arsalan I'm going to close this issue for now. Please feel free to reopen when you do have a reproducer."", ""@ezyang Sorry I forgot to work on the example as I got very busy with my stuff. I definitely know that this is an issue and will try to give you an example as soon as possible. Btw, I tried the old PyTorch version on GeForce 20 GPUs and that one also failed!! Forking does not work on those GPUs but I didn't dig into it further."", ""Thanks, looking forward to it.\n\nExcerpts from Amir Arsalan Soltani's message of 2019-06-04 07:06:50 -0700:\n> @ezyang Sorry I forgot to work on the example as I got very busy with my stuff. I definitely know that this is an issue and will try to give you an example as soon as possible.\n> \n""]","[""\r\nfrom torch.multiprocessing import Process\r\nmodel = loadModel() # somehow load a model (e.g. from torch vision)\r\ninputList = loadListOfInputs() # somehow get the list of input tensors\r\n\r\nprocesses = []\r\nfor i in range(100):\r\n    processes.append(Process(target=doForwardPass, kwargs={'input': inputList[i]}))\r\n    processes[-1].start()\r\n\r\nfor i in range(100):\r\n    processes[i].join()\r\n\r\ndef doForwardPass(input):\r\n    # model.cuda() # this, uncommented, used to work also\r\n    output = model(input)\r\n""]","['multiprocessing', 'fork', 'fork', '1.1.0a0+3a01a45', 'fork', 'multiprocessing.Process', 'fork', 'spawn', 'model.cuda()', 'fork', 'model.cuda()', 'THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=51 error=3 : initialization error']",0,0
485,pytorch,8652,closed,Batch Sampler and Dataset indexing too restricted,"I was trying to use a Dataloader with a sampler of strings to index my dataset. The default [BatchSampler](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/sampler.py#L104) threw me an error that essentially str cannot be casted to int. The cast in question happens on line 139 (currently):

I copied the source for the Batch Loader from master, removed the cast, and everything worked as expected.

[torch.utils.data](https://pytorch.org/docs/stable/data.html) DataLoader doesn't suggest that samplers should return integers, and the various Samplers just say list of indices. An index set doesn't have to be integers. Now Dataset *does* say __getitem__ should support **integer** indexing, but why?

Really, the choice of sampler (including choosing by default the default sampler) drives whether a Dataset must be indexable by integers. On the other hand, sometimes it's more sensible to ""key"" the dataset. For example, the sampler could ""request"" the a filename be loaded. Or, the actual data could be organized in a dict (like I have in my examples below).

Along the way, I noticed that BatchLoader is now asserting that sampler is a Sampler. (This isn't the case in 0.4.0, just master). I don't really understand why. Why not duck type that sampler is iterable? Just a try-except around the for-loop that catches TypeError and returns whatever type of error. It breaks the example if I'm not mistaken, and excludes a very useful way to unittest.

## MWE
",,[],"['\r\nbatch.append(int(idx))\r\n', '\r\n## M.should-be-W.E.:\r\n']","['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '\r\nfrom string import ascii_lowercase\r\nimport torch\r\nfrom torch._six import int_classes as _int_classes\r\nfrom torch.utils.data import sampler, DataLoader\r\n\r\n\r\n# CHANGES to BatchSampler in master: Removing type cast and removing sampler\r\n# explicit type-check, adding iterable duck-type\r\nclass BatchSampler(sampler.Sampler):\r\n    r""""""Wraps another sampler to yield a mini-batch of indices.\r\n    Args:\r\n        sampler (Sampler): Base sampler.\r\n        batch_size (int): Size of mini-batch.\r\n        drop_last (bool): If ', 'True', ', the sampler will drop the last batch if\r\n            its size would be less than ', 'batch_size', '\r\n    Example:\r\n        >>> list(BatchSampler(range(10), batch_size=3, drop_last=False))\r\n        [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\r\n        >>> list(BatchSampler(range(10), batch_size=3, drop_last=True))\r\n        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\r\n    """"""\r\n    def __init__(self, sampler, batch_size, drop_last):\r\n        if not isinstance(batch_size, _int_classes) or isinstance(batch_size, bool) or \\\r\n                batch_size <= 0:\r\n            raise ValueError(""batch_size should be a positive integeral value, ""\r\n                             ""but got batch_size={}"".format(batch_size))\r\n        if not isinstance(drop_last, bool):\r\n            raise ValueError(""drop_last should be a boolean value, but got ""\r\n                             ""drop_last={}"".format(drop_last))\r\n        self.sampler = sampler\r\n        self.batch_size = batch_size\r\n        self.drop_last = drop_last\r\n\r\n    def __iter__(self):\r\n        batch = []\r\n        try:\r\n            for idx in self.sampler:\r\n                batch.append(idx)\r\n                if len(batch) == self.batch_size:\r\n                    yield batch\r\n                    batch = []\r\n            if len(batch) > 0 and not self.drop_last:\r\n                yield batch\r\n        except TypeError:\r\n            raise ValueError(""sampler should be an iterable, ""\r\n                                       ""but got sampler={}"".format(sampler))\r\n\r\n    def __len__(self):\r\n        if self.drop_last:\r\n            return len(self.sampler) // self.batch_size\r\n        else:\r\n            return (len(self.sampler) + self.batch_size - 1) // self.batch_size\r\n\r\n\r\ndataset_stub = {ascii_lowercase[i]: i for i in range(20)}\r\nsampler = sampler.SubsetRandomSampler(list(dataset_stub.keys()))\r\nloader = DataLoader(dataset_stub, batch_sampler=BatchSampler(sampler, 1, True))\r\n\r\nfor elem in loader:\r\n    print(elem)\r\n\r\nfrom string import ascii_lowercase\r\nimport torch\r\nfrom torch.utils.data import sampler, DataLoader\r\n\r\ndataset_stub = {ascii_lowercase[i]: i for i in range(20)}\r\nsampler = sampler.SubsetRandomSampler(list(dataset_stub.keys()))\r\nloader = DataLoader(dataset_stub, sampler=sampler)\r\n\r\nfor elem in loader:\r\n    print(elem)\r\n', '']",0,0
486,pytorch,9383,closed,[docs] Make clear the format of torch.eig eigenvalues,"The second dimension must be for the complex imaginary part, but it is not clear in the docs.


Currently in https://pytorch.org/docs/master/torch.html?highlight=eig#torch.eig:
 without specified shapes",module: docs todo,"[""also eigenvectors shape isn't clear (are they returned as column or row vectors) and what is the sorted order of eigenvalues (descending or not)""]","['python\r\na = torch.Tensor([[1, 0], [0, 1]])\r\nprint(a.eig()[0].shape) # torch.Size([2, 2])\r\nprint(np.linalg.eig(a)[0].shape) # (2,)\r\n']",['e (Tensor): the right eigenvalues of a'],0,0
487,pytorch,2011,closed,"Build passing, but runtime failing (at commit ebdec9a837074a303fd5ffb6f319cd593955becc)","Running import torch gives the following error on the latest master:



# git log
commit ebdec9a837074a303fd5ffb6f319cd593955becc
Author: lynic <xuanlangjian@gmail.com>
Date:   Fri Jul 7 23:06:56 2017 +0800

    Skip distributed tests if not supported (#2004)
",,"['this is most definitely that you have an old install of pytorch somewhere that is conflicting with the freshly built source.\r\n\r\nTry running these commands in a terminal, and afterwards rebuild pytorch from source:\r\n\r\n```\r\nconda uninstall pytorch\r\npip uninstall torch\r\npip uninstall torch # yes twice\r\n```']","['\r\nTraceback (most recent call last):\r\n  File ""classifier.py"", line 1, in <module>\r\n    import torch\r\n  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/torch/__init__.py"", line 53, in <module>\r\n    from torch._C import *\r\nImportError: /opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/torch/lib/libTHC.so.1: undefined symbol: THLongStorage_inferSizeN\r\n']",[],0,0
488,pytorch,30717,closed,TestTorchDeviceTypeXLA: Could not start gRPC server,"Excerpt from [this CircleCI build](https://circleci.com/gh/pytorch/pytorch/3768299) on the  branch:



cc @ezyang @gchanan @zou3519 @ailzhang",module: flaky-tests module: xla triaged,"['Another instance observed here: https://circleci.com/gh/pytorch/pytorch/3785488', 'This ""address already in use"" error happens when port 40934 is in use and we want to start another gRPC server listening to it. IIRC we don\'t overlap builds on a single machine so this could probably caused by a incorrectly decommission machine  from CircleCI. \r\nWe can fix it by killing whatever is listening on 40934 before build - if we are certain builds are separated, or randomize the port. \r\nI\'ll lower the priority but fix it later. ']","['\r\nDec 04 01:51:20 E1204 01:51:20.635135781     187 server_chttp2.cc:40]        {""created"":""@1575424280.635041133"",""description"":""No address added out of total 1 resolved"",""file"":""external/grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc"",""file_line"":348,""referenced_errors"":[{""created"":""@1575424280.635039519"",""description"":""Failed to add any wildcard listeners"",""file"":""external/grpc/src/core/lib/iomgr/tcp_server_posix.cc"",""file_line"":324,""referenced_errors"":[{""created"":""@1575424280.635019167"",""description"":""Address family not supported by protocol"",""errno"":97,""file"":""external/grpc/src/core/lib/iomgr/socket_utils_common_posix.cc"",""file_line"":379,""os_error"":""Address family not supported by protocol"",""syscall"":""socket"",""target_address"":""[::]:40934""},{""created"":""@1575424280.635038986"",""description"":""Unable to configure socket"",""fd"":7,""file"":""external/grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":217,""referenced_errors"":[{""created"":""@1575424280.635035502"",""description"":""Address already in use"",""errno"":98,""file"":""external/grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":190,""os_error"":""Address already in use"",""syscall"":""bind""}]}]}]}\r\nDec 04 01:51:20 2019-12-04 01:51:20.635182: E tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:509] Unknown: Could not start gRPC server\r\nDec 04 01:51:20 2019-12-04 01:51:20.637020: E tensorflow/compiler/xla/xla_client/tf_logging.cc:11] Check failed: tensorflow::NewServer(server_def, &server_) == ::tensorflow::Status::OK() (Unknown: Could not start gRPC server vs. OK)\r\nDec 04 01:51:20 \r\nDec 04 01:51:20 Running tests...\r\nDec 04 01:51:20 ----------------------------------------------------------------------\r\nDec 04 01:51:20 \r\nDec 04 01:51:20 ======================================================================\r\nDec 04 01:51:20 ERROR [0.000s]: setUpClass (__main__.TestTorchDeviceTypeXLA)\r\nDec 04 01:51:20 ----------------------------------------------------------------------\r\nDec 04 01:51:20 Traceback (most recent call last):\r\nDec 04 01:51:20   File ""/var/lib/jenkins/workspace/test/common_device_type.py"", line 326, in setUpClass\r\nDec 04 01:51:20     cls.primary_device = str(xm.xla_device())\r\nDec 04 01:51:20   File ""/opt/conda/lib/python3.6/site-packages/torch_xla-0.8-py3.6-linux-x86_64.egg/torch_xla/core/xla_model.py"", line 142, in xla_device\r\nDec 04 01:51:20     devices = get_xla_supported_devices(devkind=devkind)\r\nDec 04 01:51:20   File ""/opt/conda/lib/python3.6/site-packages/torch_xla-0.8-py3.6-linux-x86_64.egg/torch_xla/core/xla_model.py"", line 42, in get_xla_supported_devices\r\nDec 04 01:51:20     xla_devices = torch_xla._XLAC._xla_get_devices()\r\nDec 04 01:51:20 RuntimeError: tensorflow/compiler/xla/xla_client/xrt_local_service.cc:56 : Check failed: tensorflow::NewServer(server_def, &server_) == ::tensorflow::Status::OK() (Unknown: Could not start gRPC server vs. OK)\r\n']",['master'],0,0
489,pytorch,15187,closed,complex extension does not work,"## üêõ Bug

I was trying to build the [pytorch-complex](https://github.com/Roger-luo/pytorch-complex) which I haven't work on for 3 month with the following on Mac OS X Mojave.

Then I got the following error. 


And a bunch of others:



Then I tried to build the one in test (), with the same build script in pytorch-complex. I still got this.

## To Reproduce

Steps to reproduce the behavior:

1. git clone pytorch 1.0.0 or later
2. git clone , and run 

## Expected behavior

You will get that error.

## Environment



## Additional context

I did some search, I guess this might relate to https://github.com/martinmoene/gsl-lite/issues/63

However, I found it includes  in  which is strange...

cc: @ezyang ",,['Fixed now! Not the functional error though.'],"[""\r\n/Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/c10/DeviceType.h:46:20: error:\r\n      explicit specialization of non-template struct 'hash'\r\ntemplate <> struct hash<c10::DeviceType> {\r\n                   ^   ~~~~~~~~~~~~~~~~~\r\n/Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/c10/DeviceType.h:48:21: error:\r\n      expected '(' for function-style cast or type construction\r\n    return std::hash<int>()(static_cast<int>(k));\r\n           ~~~~~~~~~^\r\n/Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/c10/DeviceType.h:48:25: error:\r\n      expected '(' for function-style cast or type construction\r\n    return std::hash<int>()(static_cast<int>(k));\r\n                     ~~~^\r\n/Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/c10/DeviceType.h:48:27: error:\r\n      expected expression\r\n    return std::hash<int>()(static_cast<int>(k));\r\n"", ""\r\n/Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/c10/util/string_utils.h:54:12: error:\r\n      no member named 'stod' in namespace 'std'\r\nusing std::stod;\r\n      ~~~~~^\r\n/Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/c10/util/string_utils.h:55:12: error:\r\n      no member named 'stoi' in namespace 'std'; did you mean 'atoi'?\r\nusing std::stoi;\r\n      ~~~~~^~~~\r\n           atoi\r\n/usr/include/c++/4.2.1/cstdlib:113:11: note: 'atoi' declared here\r\n  using ::atoi;\r\n          ^\r\nIn file included from src/module.cpp:1:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/torch/extension.h:4:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/torch/csrc/api/include/torch/all.h:4:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/torch/csrc/api/include/torch/data.h:3:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/torch/csrc/api/include/torch/data/dataloader.h:3:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/torch/csrc/api/include/torch/data/dataloader_options.h:4:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/torch/csrc/api/include/torch/types.h:3:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/ATen/ATen.h:3:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/ATen/Allocator.h:2:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/c10/core/Allocator.h:6:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/c10/Device.h:5:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/c10/util/Exception.h:5:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/c10/util/StringUtil.h:5:\r\n/Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/c10/util/string_utils.h:56:12: error:\r\n      no member named 'stoull' in namespace 'std'\r\nusing std::stoull;\r\n      ~~~~~^\r\n/Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/c10/util/string_utils.h:57:12: error:\r\n      no member named 'to_string' in namespace 'std'\r\nusing std::to_string;\r\n      ~~~~~^\r\nIn file included from src/module.cpp:1:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/torch/extension.h:4:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/torch/csrc/api/include/torch/all.h:4:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/torch/csrc/api/include/torch/data.h:3:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/torch/csrc/api/include/torch/data/dataloader.h:3:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/torch/csrc/api/include/torch/data/dataloader_options.h:4:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/torch/csrc/api/include/torch/types.h:3:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/ATen/ATen.h:3:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/ATen/Allocator.h:2:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/c10/core/Allocator.h:6:\r\n/Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/c10/Device.h:109:8: error:\r\n      explicit specialization of non-template struct 'hash'\r\nstruct hash<c10::Device> {\r\n       ^   ~~~~~~~~~~~~~\r\n/Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/c10/Device.h:109:8: error:\r\n      redefinition of 'hash'\r\n/Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/c10/DeviceType.h:46:20: note:\r\n      previous definition is here\r\ntemplate <> struct hash<c10::DeviceType> {\r\n                   ^\r\nIn file included from src/module.cpp:1:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/torch/extension.h:4:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/torch/csrc/api/include/torch/all.h:4:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/torch/csrc/api/include/torch/data.h:3:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/torch/csrc/api/include/torch/data/dataloader.h:3:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/torch/csrc/api/include/torch/data/dataloader_options.h:4:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/torch/csrc/api/include/torch/types.h:3:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/ATen/ATen.h:3:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/ATen/Allocator.h:2:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/c10/core/Allocator.h:7:\r\n/Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/c10/util/UniqueVoidPtr.h:42:8: error:\r\n      no template named 'unique_ptr' in namespace 'std'\r\n  std::unique_ptr<void, DeleterFnPtr> ctx_;\r\n  ~~~~~^\r\n/Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/c10/util/UniqueVoidPtr.h:66:8: error:\r\n      no template named 'unique_ptr' in namespace 'std'\r\n  std::unique_ptr<void, DeleterFnPtr>&& move_context() {\r\n  ~~~~~^\r\n/Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/c10/util/UniqueVoidPtr.h:67:17: error:\r\n      no member named 'move' in namespace 'std'\r\n    return std::move(ctx_);\r\n           ~~~~~^\r\n/Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/c10/util/UniqueVoidPtr.h:102:54: error:\r\n      no type named 'nullptr_t' in namespace 'std'\r\ninline bool operator==(const UniqueVoidPtr& sp, std::nullptr_t) noexcept {\r\n                                                ~~~~~^\r\n/Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/c10/util/UniqueVoidPtr.h:105:29: error:\r\n      no type named 'nullptr_t' in namespace 'std'\r\ninline bool operator==(std::nullptr_t, const UniqueVoidPtr& sp) noexcept {\r\n                       ~~~~~^\r\n/Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/c10/util/UniqueVoidPtr.h:108:54: error:\r\n      no type named 'nullptr_t' in namespace 'std'\r\ninline bool operator!=(const UniqueVoidPtr& sp, std::nullptr_t) noexcept {\r\n                                                ~~~~~^\r\n/Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/c10/util/UniqueVoidPtr.h:111:29: error:\r\n      no type named 'nullptr_t' in namespace 'std'\r\ninline bool operator!=(std::nullptr_t, const UniqueVoidPtr& sp) noexcept {\r\n                       ~~~~~^\r\nIn file included from src/module.cpp:1:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/torch/extension.h:4:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/torch/csrc/api/include/torch/all.h:4:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/torch/csrc/api/include/torch/data.h:3:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/torch/csrc/api/include/torch/data/dataloader.h:3:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/torch/csrc/api/include/torch/data/dataloader_options.h:4:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/torch/csrc/api/include/torch/types.h:3:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/ATen/ATen.h:3:\r\nIn file included from /Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/ATen/Allocator.h:2:\r\n/Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/c10/core/Allocator.h:46:8: error:\r\n      no template named 'unique_ptr' in namespace 'std'\r\n  std::unique_ptr<void, DeleterFnPtr>&& move_context() {\r\n  ~~~~~^\r\n/Users/roger/.conda/envs/complex/lib/python3.7/site-packages/torch/lib/include/c10/core/Allocator.h:67:48: error:\r\n      no type named 'nullptr_t' in namespace 'std'\r\ninline bool operator==(const DataPtr& dp, std::nullptr_t) noexcept {\r\n                                          ~~~~~^\r\nfatal error: too many errors emitted, stopping now [-ferror-limit=]\r\n20 errors generated.\r\nerror: command 'gcc' failed with exit status 1\r\n"", '\r\nPyTorch version: 1.0.0a0+db5d313\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.14.1\r\nGCC version: Could not collect\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.14.5)\r\n[conda] mkl                       2019.1                      144\r\n[conda] mkl-include               2019.1                      144\r\n[conda] torch                     1.0.0a0+db5d313           <pip>\r\n[conda] torch                     1.0.0a0+e77de07           <pip>\r\n']","['complex_registration_extension.cpp', 'pytorch-complex', 'python setup.py build', '<functional>', 'Devicetype.h']",0,0
490,pytorch,7428,closed,libtorch test_jit hangs if an error occurs,"Steps to reproduce:
1. Build this commit https://github.com/pytorch/pytorch/pull/7275/commits/8d10a9245e364920e7fdba3af03b3a14ff41f126 with CUDA in pytorch-linux-xenial-cuda9-cudnn7-py3-test (alternately, ezyang/test_jit_hangtest_jitcpp-build/libtorch/bin/test_jit`

Expected result: it terminates

Actual result: it hang:



when I ctrl-c it finally comes back:



CC @lantiga @goldsborough ",,"[""FWIW, the hang is not happening on my MBP (no CUDA):\r\n* `./test_jit` throws an unexpected exception (with message `CUDAFloatType is not enabled`) and quits (without hanging)\r\n* injecting an unexpected exception in a CPU test and running `./test_jit [cpu]` also quits without hanging\r\n\r\nI'll need to try on my linux box at work."", ""we don't use catch anymore, guessing this not a problem""]","['\r\njenkins@033788db9077:~/workspace$ ../cpp-build/libtorch/bin/test_jit \r\n\r\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntest_jit is a Catch v2.2.1 host application.\r\nRun with -? for options\r\n\r\n-------------------------------------------------------------------------------\r\njit test CUDA\r\n  graph executor\r\n-------------------------------------------------------------------------------\r\n/var/lib/jenkins/workspace/torch/csrc/jit/test_jit.cpp:914\r\n...............................................................................\r\n\r\n/var/lib/jenkins/workspace/torch/csrc/jit/test_jit.cpp:914: FAILED:\r\ndue to unexpected exception with message:\r\n  cannot initialize CUDA without ATen_cuda library (initCUDA at /var/lib/\r\n  jenkins/workspace/aten/src/ATen/detail/CUDAHooksInterface.h:42)\r\n  frame #0: at::Context::lazyInitCUDA()::{lambda()#1}::operator()() const +\r\n  0x32 (0x63ef90 in ../cpp-build/libtorch/bin/test_jit)\r\n  frame #1: void std::_Bind_simple<at::Context::lazyInitCUDA()::{lambda()#1} ()\r\n  >::_M_invoke<>(std::_Index_tuple<>) + 0x28 (0x690b38 in ../cpp-build/\r\n  libtorch/bin/test_jit)\r\n  frame #2: std::_Bind_simple<at::Context::lazyInitCUDA()::{lambda()#1} ()>::\r\n  operator()() + 0x2c (0x6808aa in ../cpp-build/libtorch/bin/test_jit)\r\n  frame #3: void std::__once_call_impl<std::_Bind_simple<at::Context::\r\n  lazyInitCUDA()::{lambda()#1} ()> >() + 0x17 (0x66bfbc in ../cpp-build/\r\n  libtorch/bin/test_jit)\r\n  frame #4: <unknown function> + 0xea99 (0x7fd2b2e22a99 in /lib/x86_64-linux-\r\n  gnu/libpthread.so.0)\r\n  frame #5: ../cpp-build/libtorch/bin/test_jit() [0x5e9e9d]\r\n  frame #6: void std::call_once<at::Context::lazyInitCUDA()::{lambda()#1}>(std:\r\n  :once_flag&, at::Context::lazyInitCUDA()::{lambda()#1}&&) + 0x7f (0x657c67 in\r\n  ../cpp-build/libtorch/bin/test_jit)\r\n  frame #7: at::Context::lazyInitCUDA() + 0x3d (0x63f071 in ../cpp-build/\r\n  libtorch/bin/test_jit)\r\n  frame #8: at::Context::initCUDAIfNeeded(at::Backend) + 0x21 (0x63f0bb in ../\r\n  cpp-build/libtorch/bin/test_jit)\r\n  frame #9: at::Context::getTypeOpt(at::Backend, at::ScalarType) + 0x23\r\n  (0x63ed79 in ../cpp-build/libtorch/bin/test_jit)\r\n  frame #10: at::Context::getType(at::Backend, at::ScalarType) + 0x4a (0x63ee58\r\n  in ../cpp-build/libtorch/bin/test_jit)\r\n  frame #11: ../cpp-build/libtorch/bin/test_jit() [0x60ebb6]\r\n  frame #12: ../cpp-build/libtorch/bin/test_jit() [0x60ebee]\r\n  frame #13: torch::jit::testGraphExecutor() + 0x79 (0x621eeb in ../cpp-build/\r\n  libtorch/bin/test_jit)\r\n  frame #14: ../cpp-build/libtorch/bin/test_jit() [0x6251a7]\r\n  frame #15: Catch::TestInvokerAsFunction::invoke() const + 0x16 (0x5fe09c in .\r\n  ./cpp-build/libtorch/bin/test_jit)\r\n  frame #16: Catch::TestCase::invoke() const + 0x29 (0x5fd755 in ../cpp-build/\r\n  libtorch/bin/test_jit)\r\n  frame #17: Catch::RunContext::invokeActiveTestCase() + 0x38 (0x5f7f1c in ../\r\n  cpp-build/libtorch/bin/test_jit)\r\n  frame #18: Catch::RunContext::runCurrentTest(std::__cxx11::basic_string<char,\r\n  std::char_traits<char>, std::allocator<char> >&, std::__cxx11::basic_string\r\n  <char, std::char_traits<char>, std::allocator<char> >&) + 0x2ce (0x5f7c30 in\r\n  ../cpp-build/libtorch/bin/test_jit)\r\n  frame #19: Catch::RunContext::runTest(Catch::TestCase const&) + 0x240\r\n  (0x5f67ba in ../cpp-build/libtorch/bin/test_jit)\r\n  frame #20: ../cpp-build/libtorch/bin/test_jit() [0x5f9574]\r\n  frame #21: Catch::Session::runInternal() + 0x13d (0x5fa931 in ../cpp-build/\r\n  libtorch/bin/test_jit)\r\n  frame #22: Catch::Session::run() + 0x57 (0x5fa6b5 in ../cpp-build/libtorch/\r\n  bin/test_jit)\r\n  frame #23: Catch::Session::run(int, char**) + 0x53 (0x5fa655 in ../cpp-build/\r\n  libtorch/bin/test_jit)\r\n  frame #24: main + 0x55 (0x60ea6b in ../cpp-build/libtorch/bin/test_jit)\r\n  frame #25: __libc_start_main + 0xf0 (0x7fd2b0947830 in /lib/x86_64-linux-gnu/\r\n  libc.so.6)\r\n  frame #26: _start + 0x29 (0x5e9d89 in ../cpp-build/libtorch/bin/test_jit)\r\n\r\n\r\n', '\r\n\r\n^C-------------------------------------------------------------------------------\r\njit test CUDA\r\n  fusion\r\n-------------------------------------------------------------------------------\r\n/var/lib/jenkins/workspace/torch/csrc/jit/test_jit.cpp:916\r\n...............................................................................\r\n\r\n/var/lib/jenkins/workspace/torch/csrc/jit/test_jit.cpp:916: FAILED:\r\ndue to a fatal error condition:\r\n  SIGINT - Terminal interrupt signal\r\n\r\n===============================================================================\r\ntest cases:   2 |   1 passed | 1 failed\r\nassertions: 137 | 135 passed | 2 failed\r\n\r\n']","['docker pull ', '; alternately, purposely introduce an unexpected exception in the ', ' testsuite)\r\n2. Run ']",0,0
491,pytorch,923,closed,Build broken on Mac OS X,"Building form source for Mac OS X fails. Turns out there is a missing dependency between THPP and THCS, adding this dependency to CMakeLists.txt was sufficient for a successful build.",,[],[],[],0,0
492,pytorch,20465,closed,Second order gradient cuda error,"## üêõ Bug
Convnet training on GPU: when penalizing gradient growth (backpropagating a gradient of a gradient) the following error happens:

indexValue >= 0 && indexValue < src.sizes[dim]

## Environment
PyTorch version: 1.1.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 18.04.1 LTS
GCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
CMake version: version 3.6.2

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.1.85
GPU models and configuration:
GPU 0: TITAN X (Pascal)
GPU 1: TITAN X (Pascal)
GPU 2: TITAN X (Pascal)
GPU 3: TITAN X (Pascal)

Nvidia driver version: 418.56
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.3
/usr/local/cuda-9.0/lib64/libcudnn.so.7.2.1

Versions of relevant libraries:
[pip3] numpy==1.15.0
[pip3] torch==1.1.0
[pip3] torchfile==0.1.0
[pip3] torchvision==0.2.2
[conda] blas                      1.0                         mkl
[conda] cuda92                    1.0                           0    pytorch
[conda] mkl                       2018.0.3                      1
[conda] mkl_fft                   1.0.4            py36h4414c95_1
[conda] mkl_random                1.0.1            py36h4414c95_1
[conda] pytorch                   1.1.0           py3.6_cuda10.0.130_cudnn7.5.1_0    pytorch
[conda] torchfile                 0.1.0                      py_0    conda-forge
[conda] torchvision               0.2.2                      py_3    pytorch
## Additional context

<!-- Add any other context about the problem here. -->
If I change the coefficient in  from 0.1 to 10 the error does NOT happen (at least not before ~40 epochs), same if I reduce the learning rate from 1 to 0.001. 
",high priority module: autograd module: cuda module: double backwards module: nn triaged,"['Adding back the autograd label since the conditions appears to be triggered there.\r\n\r\n@li-roy @zou3519 Could you take a look at this?', ""The device-side assert is triggered here:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/14ecf92d4212996937a9a1ceadd2202bd828636e/aten/src/THC/THCTensorScatterGather.cu#L100\r\n\r\nIn the reproducer (thanks!), `indexValue` is `-1` when things start to go awry.  This suggests that somewhere negative indices aren't adjusted; the question is whether `index.data` should **only contain indices that are in range or allow negative indices**.\r\n\r\n\r\nSome other kernels adjust indices, example:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/14ecf92d4212996937a9a1ceadd2202bd828636e/aten/src/ATen/native/cuda/IndexKernel.cu#L60\r\n\r\n\r\nIndeed the reproducer passes at least 40 epochs with this diff:\r\n\r\n```\r\ndiff --git a/aten/src/THC/THCTensorScatterGather.cu b/aten/src/THC/THCTensorScatterGather.cu\r\nindex baec8cb..d6d1fd7 100644\r\n--- a/aten/src/THC/THCTensorScatterGather.cu\r\n+++ b/aten/src/THC/THCTensorScatterGather.cu\r\n@@ -97,6 +97,9 @@ __global__ void THCudaTensor_gatherKernel(\r\n                                                           src, &srcOffset);\r\n \r\n     int64_t indexValue = index.data[indexOffset];\r\n+    if (indexValue < 0) {\r\n+        indexValue += src.sizes[dim];\r\n+    }\r\n     assert(indexValue >= 0 && indexValue < src.sizes[dim]);\r\n     srcOffset += indexValue * src.strides[dim];\r\n```\r\n\r\n"", ""I'm running into the same issue in a very similar setting. like @michaelklachko in my case it seems to be triggered (more often) when learning rate is high.\r\nI'm curious about what is calling that kernel to see if we can get around it."", ""@fps7806 The reproducer script produces `NaNs` in `grad_sum`. Here is an example run:\r\n\r\n```\r\ntensor(0.3012, device='cuda:0', grad_fn=<AddBackward0>) torch.float32\r\ntensor(0.0048, device='cuda:0', grad_fn=<AddBackward0>) torch.float32\r\ntensor(2.2294, device='cuda:0', grad_fn=<AddBackward0>) torch.float32\r\ntensor(241.6396, device='cuda:0', grad_fn=<AddBackward0>) torch.float32\r\ntensor(2.6122e+13, device='cuda:0', grad_fn=<AddBackward0>) torch.float32\r\ntensor(inf, device='cuda:0', grad_fn=<AddBackward0>) torch.float32\r\ntensor(nan, device='cuda:0', grad_fn=<AddBackward0>) torch.float32\r\n```\r\n\r\nActually the `autograd.detect_anomaly()` context manager detects those `NaNs` **before** the device assert happens.\r\n\r\nSo this issue seems to be about behaving gracefully once an anomaly has occurred. CPU and CUDA behave differently:  The CPU kernels continue to produce all `NaNs`, in CUDA the `indices` kernel arg is all `-1` for some reason, which I'm currently investigating.\r\n\r\nThe CPU behavior looks correct to me, unless all behavior is undefined once an anomaly has occurred.\r\n\r\n"", ""The cause was a bit difficult to locate.  Contrary to the previous message, I could also reproduce the issue on the CPU by inserting `torch.manual_seed(8017)` at the top of @michaelklachko's script.\r\n\r\nSo both CPU and CUDA kernels currently can produce negative indices. The issue is in the max_pool() kernels, which leave `maxindex==-1` if all input values are `-inf` or `-FLOAT_MAX`.\r\n\r\nIt has always been that way (before porting to ATen), so there's a chance that the behavior was intentional.  But I can't see the reason. :)\r\n\r\nSo #23161 changes the behavior to allow `-inf` in the output values and set the indices properly.\r\n\r\n "", '@skrah thank you! On a higher level, is there a way to enable a warning message when any of the values in the graph are NaNs or infs? Similar to how numpy warns when we try to calculate ""a mean of an empty slice"", etc?', '@michaelklachko Yes, in your example you can use this context manager:\r\n\r\n```python\r\nwith autograd.detect_anomaly():\r\n    grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\r\n\r\n    grad_sum = 0\r\n    for grad in grads:\r\n        grad_sum += 0.1 * grad.pow(2).sum()\r\n\r\n    grad_sum.backward(retain_graph=False)\r\n```\r\n\r\nThis detects `NaNs`:\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File ""adb.py"", line 48, in <module>\r\n    grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\r\n  File ""/home/stefan/rel2/lib/python3.7/site-packages/torch/autograd/__init__.py"", line 149, in grad\r\n    inputs, allow_unused)\r\nRuntimeError: Function \'LogSoftmaxBackward\' returned nan values in its 0th output.\r\n```\r\n\r\n', 'Oh wow! This is very useful! ']",['\r\n\r\n## To Reproduce\r\n\r\n'],"['', '\r\n/opt/conda/conda-bld/pytorch_1556653099582/work/aten/src/THC/THCTensorScatterGather.cu:100: void THCudaTensor_gatherKernel(TensorInfo<Real, IndexType>, TensorInfo<Real, IndexType>, TensorInfo<long, IndexType>, int, IndexType) [with IndexType = unsigned int, Real = float, Dims = 3]: block: [40,0,0], thread: [31,0,0] Assertion ', ' failed.\r\nTHCudaCheck FAIL file=/opt/conda/conda-bld/pytorch_1556653099582/work/aten/src/THC/generic/THCTensorScatterGather.cu line=71 error=59 : device-side assert triggered\r\nTraceback (most recent call last):\r\n  File ""cudafail.py"", line 47, in <module>\r\n    grad_sum.backward(retain_graph=False)\r\n  File ""/home/michael/miniconda2/envs/pt/lib/python3.6/site-packages/torch/tensor.py"", line 107, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File ""/home/michael/miniconda2/envs/pt/lib/python3.6/site-packages/torch/autograd/__init__.py"", line 93, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1556653099582/work/aten/src/THC/generic/THCTensorScatterGather.cu:71\r\n\r\nimport torch\r\nfrom torch import nn\r\nfrom torchvision import datasets, transforms\r\n\r\nclass Net(nn.Module):\r\n\tdef __init__(self):\r\n\t\tsuper(Net, self).__init__()\r\n\t\tself.conv1 = nn.Conv2d(3, 20, kernel_size=5, bias=False)\r\n\t\tself.conv2 = nn.Conv2d(20, 40, kernel_size=5, bias=False)\r\n\t\tself.linear1 = nn.Linear(40 * 5 * 5, 300, bias=False)\r\n\t\tself.linear2 = nn.Linear(300, 10, bias=False)\r\n\t\tself.pool = nn.MaxPool2d(2, 2)\r\n\t\tself.relu = nn.ReLU()\r\n\r\n\tdef forward(self, input):\r\n\t\tx = self.relu(self.pool(self.conv1(input)))\r\n\t\tx = self.relu(self.pool(self.conv2(x)))\r\n\t\tx = x.view(x.size(0), -1)\r\n\t\tx = self.relu(self.linear1(x))\r\n\t\treturn self.linear2(x)\r\n\r\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\r\ntrainset = datasets.CIFAR10(root=\'./data\', train=True, download=True, transform=transform)\r\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=8)\r\n\r\nmodel = Net().cuda()\r\noptimizer = torch.optim.SGD(model.parameters(), lr=1, momentum=0, nesterov=False)\r\n\r\nfor epoch in range(100):\r\n        print(epoch)\r\n\tmodel.train()\r\n\tfor i, (images, labels) in enumerate(trainloader, 0):\r\n\t\timages = images.cuda()\r\n\t\tlabels = labels.cuda()\r\n\t\toutputs = model(images)\r\n\t\tloss = nn.CrossEntropyLoss()(outputs, labels)\r\n\t\toptimizer.zero_grad()\r\n\t\tloss.backward(retain_graph=True)\r\n\r\n\t\tgrads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\r\n\t\tgrad_sum = 0\r\n\t\tfor grad in grads:\r\n\t\t\tgrad_sum += 0.1 * grad.pow(2).sum()\r\n\t\tgrad_sum.backward(retain_graph=False)\r\n\r\n\t\toptimizer.step()\r\n', '', 'grad_sum += 0.1 * grad.pow(2).sum()']",0,0
493,pytorch,12981,closed,torchvision install error,"Sorry to trouble you, but I get some error when I start pytorch
I set 
- PyTorch Build ‚Äî‚Äî Stable
- Your OS‚Äî‚ÄîWindows
- Package‚Äî‚ÄîConda
- Language‚Äî‚ÄîPython 3.6
- CUDA‚Äî‚Äî8.0

It shows 
Run this Command:
pytorchpip3

The first command is alright, shows  # All requested packages already installed. And I can import torch.
But the second command shows:

I have tried use , and shows the same message. 
I find 
 
 in the error message,But




",,"['Neither your pip nor pip3 point to the one that pytorch was installed in. Please consider using absolute path for certain pip.', '> Neither your pip nor pip3 point to the one that pytorch was installed in.\r\n\r\nMeans my pip or pip3 and pytorch install in different environmentÔºü „Ää= I am sure they are in the same environment. And my pip also use python3.6\r\n`E:\\mypro\\ESPCN-master>pip --version\r\npip 18.1 from c:\\program files (x86)\\python36-32\\lib\\site-packages\\pip (python 3.6)`\r\n> Please consider using absolute path for certain pip.\r\n\r\n', 'You can do the following in CMD to install torchvision very quickly. However, I\'d suggest that you clean your environment a little bit.\r\n```cmd\r\n:: Detect where your python is\r\nwhere python.exe\r\nwhere conda.exe\r\n:: The example output for the last two commands: (Only first line)\r\n:: C:\\Anaconda2\\python.exe\r\n:: C:\\Anaconda2\\Scripts\\conda.exe\r\n\r\n:: Use the pip that lives with conda\r\n""C:\\Anaconda2\\Scripts\\pip.exe"" install torchvision\r\n\r\n:: To clean your environment, adjust your environmental variable PATH and \r\n:: make sure the output (first line) of the two commands point to the same environment.\r\n\r\n```\r\n', 'thanks a lot']","['\r\nRequirement already satisfied: torchvision in c:\\program files (x86)\\python36-32\\lib\\site-packages (0.2.1)\r\nRequirement already satisfied: numpy in c:\\program files (x86)\\python36-32\\lib\\site-packages (from torchvision) (1.13.3)\r\nCollecting torch (from torchvision)\r\n  Using cached https://files.pythonhosted.org/packages/5f/e9/bac4204fe9cb1a002ec6140b47f51affda1655379fe302a1caef421f9846/torch-0.1.2.post1.tar.gz\r\n    Complete output from command python setup.py egg_info:\r\n    Traceback (most recent call last):\r\n      File ""<string>"", line 1, in <module>\r\n      File ""C:\\Users\\qiucy\\AppData\\Local\\Temp\\pip-install-xstv3999\\torch\\setup.py"", line 11, in <module>\r\n        raise RuntimeError(README)\r\n    RuntimeError: PyTorch does not currently provide packages for PyPI (see status at https://github.com/pytorch/pytorch/issues/566).\r\n\r\n    Please follow the instructions at http://pytorch.org/ to install with miniconda instead.\r\n\r\n\r\n    ----------------------------------------\r\nCommand ""python setup.py egg_info"" failed with error code 1 in C:\\Users\\qiucy\\AppData\\Local\\Temp\\pip-install-xstv3999\\torch\\\r\n', '\r\nimport torchvision\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\nModuleNotFoundError: No module named \'torchvision\'\r\n']","['conda install pytorch cuda80 -c ', '\r\n', ' install torchvision', 'pip install torchvision', 'Requirement already satisfied: torchvision in c:\\program files (x86)\\python36-32\\lib\\site-packages (0.2.1)']",0,0
494,pytorch,16277,closed,Inconsistency in inception model,"## üêõ Bug

Hi
I was going through the torchvision code to implement c++ API and I noticed an Inconsistency.

In line 298 of inception.py stddev of a class of type BasicConv2d is set to 0.01:



But in line 60 it checks if it's conv2d or linear which a BasicConv2d is none:



I put this code inside the previous if statement:



and it only printed:



so convs inside conv1 BasicConv2d of InceptionAux get initialized with stddev=0.1

is this an error or is it intentional?",,"['hi, please report / ask in https://github.com/pytorch/vision']","[' python\r\nself.conv1.stddev = 0.01\r\n', "" python\r\nif isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\r\n   import scipy.stats as stats\r\n   stddev = m.stddev if hasattr(m, 'stddev') else 0.1\r\n"", "" python\r\nif hasattr(m, 'stddev'):\r\n    print(m)\r\n"", '\r\nLinear(in_features=768, out_features=1000, bias=True)\r\n']",[],0,0
495,pytorch,18354,closed,Port SpatialReflectionPadding and TemporalReflectionPadding to ATen,Large,module: nn module: porting triaged,"['Porting SpatialReflectionPadding and TemporalReflectionPadding are completed in https://github.com/pytorch/pytorch/pull/15480 and https://github.com/pytorch/pytorch/pull/15525 .', '@ezyang , can you confirm that the issue can be closed?']",[],[],0,0
496,pytorch,16042,closed,grad_fn missing?,"## üêõ Bug

Hi, Sorry if this is not a bug I tried to post in forums but am getting no response :( 

## To Reproduce

import torch

x = torch.randn(1,1,1,1).requires_grad_(True)

y = torch.nn.Conv2d(1,1,1)(x).mean()**2
conv_grad,  = torch.autograd.grad(y, x, create_graph=True, retain_graph=True)
print(""Conv grad"",conv_grad)

y = torch.nn.Linear(1,1)(x).mean()**2
linear_grad,  = torch.autograd.grad(y, x, create_graph=True, retain_graph=True)
print(""Linear grad"",linear_grad)

## Expected behavior

I think both gradient variables should have an associated grad_fn, however only the linear version does. Isn't the math the same for both here, the 1x1 convolution vs the 1x1 linear layer? Or maybe I am missing something :(
",,"[""I think this was flagged as a bug, and a fix is in master. See https://github.com/pytorch/pytorch/issues/15353\r\n\r\nIt'll be part of the 1.0.1 release this week.""]",[],[],0,0
497,pytorch,23823,closed,pytorch cannot be installed under Windows 10 if Python 3.7 was installed from Microsoft Store,"## Issue description

pytorch 1.1.0 cannot be installed under Windows 10 if Python 3.7 was installed from Microsoft Store.

The reason is a combination of
a) long Python installation path used by Microsoft Store installer
b) long filename path used within pytorch for collect_and_distribute_fpn_rpn_proposals_op_test.test_collect_and_dist.zip
c) by default, Windows 10 still refuses paths longer than 256 characters

## Code example


fails with an error message:

> 

The error is caused by the 256 character limit on Windows paths. With a file and pathname so long, no matter how short <username> is the path is 257 characters or longer.

## System Info
PyTorch version: 1.1.0
OS: Microsoft Windows 10 Pro, Version 1903 
Python version: 3.7

Versions of relevant libraries:
[pip3] numpy==1.17.0
[pip3] torch==1.1.0
[pip3] torchvision==0.3.0
",module: windows triaged,"['I am not the first or only one to have this problem, see https://www.reddit.com/r/pytorch/comments/c6cllq/issue_installing_pytorch/ .\r\nA workaround is to tell Windows to accept long paths, [see here](https://www.reddit.com/r/pytorch/comments/c6cllq/issue_installing_pytorch/ew27hih/?utm_source=share&utm_medium=web2x). However, this requires people to search for the error message online and find this. A more convenient solution implementable in pytorch could be to simply rename the collect_and_distribute_fpn_rpn_proposals_op_test.py file to something shorter? I know this only reduces the problem a little, since users with long usernames can still run into such a problem. \r\n\r\nA more clean solution would be to have whichever tool fails to unpack that file throw a more meaningful error. Just some thoughts...', 'This problem also occurs in my Windows OS 18965.1005.\r\n\r\nAnd I fix this bug refers to upper link [reddit.com/r/pytorch/comments/c6cllq/issue_installing_pytorch/ew27hih/?utm_source=share&utm_medium=web2x](https://www.reddit.com/r/pytorch/comments/c6cllq/issue_installing_pytorch/ew27hih/?utm_source=share&utm_medium=web2x).\r\n\r\nThe problem is caused by the Long Path Name of `collect_and_distribute_fpn_rpn_proposals_op_test.test_collect_and_dist.zip`.', 'Upstream issue: https://bugs.python.org/issue18199\r\nThey rejected the idea to add the prefix ""\\\\?\\"" to the path to extend the MAX_PATH to 65536. So you\'ll need to get in the registry route.', 'Okay, so for anyone else looking for the workaround: In the Windows Registry, within HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem you have to set LongPathsEnabled to 1 and reboot.', ""> Okay, so for anyone else looking for the workaround: In the Windows Registry, within HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem you have to set LongPathsEnabled to 1 and reboot.\r\n\r\nActually I didn't even have to reboot (Version 1909 build 18363.1082)"", 'The registry works and a reboot is not necessary.\r\nBtw, I found an alternate solution: change your Windows user name to 1 single character, which makes the path length exactly 256 chars', 'Issue is still present']",[],"['pip3 install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp37-cp37m-win_amd64.whl', ""ERROR: Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\[username]\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python37\\\\site-packages\\\\caffe2\\\\python\\\\serialized_test\\\\data\\\\operator_test\\\\collect_and_distribute_fpn_rpn_proposals_op_test.test_collect_and_dist.zip'""]",0,0
498,pytorch,24537,closed,Migrate `asin` and `asin_` from the TH to Aten (CUDA),"Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review.",better-engineering module: porting triaged,[],[],[],0,0
499,pytorch,2870,closed,torch.max has extra dimensions on a slice with size 1,"
The size reported at the end should be (32, 1), but is (32, 1, 1). Using torch==0.2.0.post3 both on CPU and GPU.",,"['should be fixed by this: https://github.com/pytorch/pytorch/pull/2318', 'Thanks @gchanan , will re-open if I find any issues with this in master!']","['\r\n>>> x = torch.zeros((32, 10, 10))\r\n>>> y = x[:, 2:4, 3:5]\r\n>>> torch.max(y, dim=1, keepdim=False)[0].size()\r\ntorch.Size([32, 2])   # correct\r\n>>> y = x[:, 2:3, 3:4]\r\n>>> torch.max(y, dim=1, keepdim=False)[0].size()\r\ntorch.Size([32, 1, 1])  # wrong\r\n']","['', '', '', '', '', '']",0,0
500,pytorch,22269,closed,"Add user-facing Function, so we don't have to expose torch.jit._C.Function","Add user-facing Function, so we don't have to expose torch.jit._C.Function

More details:
https://github.com/pytorch/pytorch/pull/22090#discussion_r296443544",oncall: jit triaged,['Dup of #20017'],[],[],0,0
501,pytorch,7525,closed,tqdm install is failing with ValueError: bad marshal data (unknown type code),"Sample failing CI build: https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-trusty-pynightly-test/6167//console

I did some investigation, and the error only happens if you use  directly (e.g., if you use , and it goes away if you use a newer Python 3.7 nightly (e.g., from deadsnakes). So it might be a bug in Python 3.7 (slash setuptools) which was subsequently fixed.

Unfortunately, preinstall tqdm isn't sufficient to avoid the problem. But  on torchvision might be enough. I am giving it a try.",,['mooting this'],[],"['setuptools', 'python setup.py install', 'pip install -e .']",0,0
502,pytorch,3326,closed,nn.EmbeddingBag doesn't seem to support None offset for 2D tensors,"Example

Gives
RuntimeError: expected a Variable argument, but got torch.LongTensor",,"['Are you sure you are passing a `Variable` instance to `embed`? This works for me:\r\n\r\n```\r\nIn [11]: a = Variable(torch.LongTensor([[1, 0, 2], [2, 1, 0], [0, 0, 1]]))\r\n\r\nIn [12]: emb = nn.EmbeddingBag(3,4)\r\n\r\nIn [13]: emb(a)\r\nOut[13]:\r\nVariable containing:\r\n-0.7979  0.9842  0.4947 -0.8800\r\n-0.7979  0.9842  0.4947 -0.8800\r\n 0.0435  0.7139 -0.1079 -0.8040\r\n[torch.FloatTensor of size 3x4]\r\n```', ""@soumith, this doesn't seem to be an issue anymore with PyTorch 1.0.""]","['\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.autograd import Variable\r\na = Variable(torch.LongTensor([[1, 0, 2], [2, 1, 0], [0, 0, 1]]))\r\nembed = nn.EmbeddingBag(3, 4)\r\nembed(a)\r\n']",[],0,0
503,pytorch,8867,closed,[JIT] Normalize representation of traced and scripted functions/modules ,"Currently we have a mix of actual types when we script/trace a function/module:






This causes issues, for example, since we can only inline a  into a ScriptModule if it's an instance of GraphExecutor(https://github.com/pytorch/pytorch/blob/2b926aafb09575dd83ebf4fe2a76cbe239596f0b/torch/csrc/jit/script/init.cpp#L82). This causes problems in the case of calling Script/Traced modules from script functions -- they show up as s.

Let's make it so that all of {script,traced} {function,module} produce a Module, and re-work the inlining logic to merge modules in all cases. This will greatly simplify the logic and make things easier to understand and maintain",oncall: jit,[],"[""\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef script_fn(x):\r\n    return x\r\n\r\nprint('*****Script function*****')\r\nprint(type(script_fn))\r\nprint(isinstance(script_fn, torch._C.GraphExecutor))\r\n\r\n@torch.jit.trace(torch.zeros(3, 3))\r\ndef trace_fn(x):\r\n    return x\r\n\r\nprint('*****Traced function*****')\r\nprint(type(trace_fn))\r\nprint(isinstance(trace_fn, torch._C.GraphExecutor))\r\n\r\nclass ScriptModule(torch.jit.ScriptModule):\r\n    @torch.jit.script_method\r\n    def forward(self, x):\r\n        return x\r\nsm = ScriptModule()\r\n\r\nprint('*****Script module*****')\r\nprint(type(sm))\r\nprint(isinstance(sm, torch._C.GraphExecutor))\r\n\r\nclass TracedModule(torch.nn.Module):\r\n    def forward(self, x):\r\n        return x\r\ntm = torch.jit.trace(torch.zeros(3, 3))(TracedModule())\r\n\r\nprint('*****Traced module*****')\r\nprint(type(tm))\r\nprint(isinstance(tm, torch._C.GraphExecutor))\r\n\r\n"", ""\r\n*****Script function*****\r\n<class 'torch._C.GraphExecutor'>\r\nTrue\r\n*****Traced function*****\r\n<class 'torch._C.GraphExecutor'>\r\nTrue\r\n*****Script module*****\r\n<class '__main__.ScriptModule'>\r\nFalse\r\n*****Traced module*****\r\n<class 'torch.jit.TopLevelTracedModule'>\r\nFalse\r\n""]","['PythonValue', 'PythonOp']",0,0
504,pytorch,17335,closed,.circleci/regenerate.sh doesn't work when run from root directory,,,['Fixed in commit dd3acbc6d'],['\r\n(/Users/ezyang/Dev/pytorch-tmp-env) macbook-pro-116:pytorch-tmp ezyang$ .circleci/regenerate.sh \r\n+ ./generate-config-yml.py\r\n.circleci/regenerate.sh: line 3: ./generate-config-yml.py: No such file or directory\r\n'],[],0,0
505,pytorch,19162,closed,Denormalize option in torchvision.utils.save_image(),"## üöÄ Feature
<!-- A clear and concise description of the feature proposal -->
A function to denormalize an image based on mean and standard deviation.

## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->
When working with images on NN's trained on a specific dataset (for example ImageNet), an image is first normalized to the mean and standard deviation of that dataset. When we want to save such an image later in the process we can use the function *torchvision.utils.save_image()*. However the image is still normalized and will have a different mean and standard deviation compared to the original image. There is no option to *denormalize* such an image such that the initial normalization is undone and the saved image has the same mean and std.

## Pitch

<!-- A clear and concise description of what you want to happen. -->
A extra parameter to the *torchvision.utils.save_image()* function to denormalize an image based on a mean and standardization array.

## Alternatives

<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->
One way to tackle the problem currently is to use the *transforms.Normalize()* function. My current implementation is shown below. One flaw of this implementation is that the image has to be clipped to keep the values between 0 and 1. Thus some information is lost. I am not sure how to do this operation lossless.



",,"['Hi,\r\n\r\nThis issue belongs to `torchvision` https://github.com/pytorch/vision. Can you open an issue there instead?\r\nThanks!']","['python\r\ndef img_denorm(img, mean, std):\r\n    #for ImageNet the mean and std are:\r\n    #mean = np.asarray([ 0.485, 0.456, 0.406 ])\r\n    #std = np.asarray([ 0.229, 0.224, 0.225 ])\r\n\r\n    denormalize = transforms.Normalize((-1 * mean / std), (1.0 / std))\r\n\r\n    res = img.squeeze(0)\r\n    res = denormalize(res)\r\n\r\n    #Image needs to be clipped since the denormalize function will map some\r\n    #values below 0 and above 1\r\n    res = torch.clamp(res, 0, 1)\r\n    \r\nreturn(res)\r\n']",[],0,0
506,pytorch,5066,closed,Import Error after updating to 0.3,"I tried to update pytorch from 0.2 to 0.3.

The update itself is fine, but when I run the codes, it raises this exception:

CXXABI_1.3.8' not found (required by /home/sliu426/anaconda2/lib/python2.7/site-packages/rdkit/Chem/../../../../././libicui18n.so.58)

import torch
from rdkit import Chem
pip install pytorch`, I guess the installation changes some paths. So I simply switch these two lines of codes and it works well for now:


The solution is a little tricky, and I'm wondering if there's a better one?",,"[""Sorry for the lack of replies. PyTorch 0.3 is a pretty old version of PyTorch and we generally don't backport bug fixes. Please try a more recent version of PyTorch if you're still having this problem.""]","['\r\n\r\nAnd here is my code:\r\n', '\r\nfrom rdkit import Chem\r\nimport torch\r\n']","['', '\r\nImportError: /lib64/libstdc++.so.6: version ', '', ""\r\n\r\nConsidering I didn't change anything exception ""]",0,0
507,pytorch,30796,closed,How to Build pytorch with local protobuf rather than third_party/protobuf?,"## ‚ùì Questions and Help
I want to build pytorch with my own os built protobuf lib rather than third_part/protobuf, Which prefix to change, Can anyone help me?


### Please note that this issue tracker is not a help form and this issue will be closed.

We have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:

- [Discussion Forum](https://discuss.pytorch.org/)
",,['See message about the discussion forum.'],[],[],0,0
508,pytorch,123,closed,0 indexed random,"What about having torch.IntTensor(10).random_(4) returning integers between 0 and 3 instead of 1 and 4, to make it consistent with numpy.random.randint?
",todo,['Fixed in #161.\n'],[],[],0,0
509,pytorch,9987,closed,eigen submodule not found,"Could you fix this submodule not found problem of caffe2?

remote: Repository not found.
fatal: repository 'https://github.com/RLovelett/eigen.git/' not found
fatal: clone of 'https://github.com/RLovelett/eigen.git' into submodule path '/home/ly/workspace/git/pose/caffe2/third_party/eigen' failed
Failed to clone 'third_party/eigen' a second time, aborting",caffe2,['Try `git submodule sync`. You have an older checkout of pytorch and your submodules need to be synced to point to the most recent ones.'],[],[],0,0
510,pytorch,24918,closed,A lot of deprecation warnings in tests,"Several test suites, for example test_indexing, throw deprecation warnings like this: 

",better-engineering module: tests triaged,"['I am facing a similar issue after I upgraded to pytorch 1.2.0', '@anshulpaigwar, just FYI, this is intentional. You should update your code and get rid of these warnings as in the next release they will turn into errors once we drop support for uint8 masks and related.', 'it would be good if it will show where this error happen', 'I fixed this warning by changing objects of type `ByteTensor` to `BoolTensor` instead. This was the source of the warnings for me.']",[],"['UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.']",0,0
511,pytorch,27690,closed,Multi-GPU Gather is much slower than Scatter,"## üêõ Bug

Multi-GPU gathering is much slower than scattering

## To Reproduce
Can run the following script on a Multi-GPU machine which should replicate the issue. It creates a large tensor on the CPU and scatters it to multiple GPUs, then also creates large I have tested with both pytorch 1.1.0 and pytorch 1.2.0.


Output: 
Note that when batch size (args.sz) = 128, the tensor to be scattered/gathered is 1GiB.

| NGpus        | batch size           | Scatter Time (s)  |  Gather Time (s) |
| ------------- |:-------------:| -----:|---:|
| 2 | 64 |  0.07 | 0.35 |
| 2 | 128 | 0.14 | 0.7 |
| 2 | 256 | 0.27 | 1.4 |
| 4     | 64   | 0.12 | 0.35 |
| 4 | 128   |  0.16 | 0.7 |
| 4 | 256 | 0.29 | 1.6 |

## Expected behavior

I would expect the scatter and gather timings to be a lot closer than this, not a factor of 5 out.

## Environment

",module: cuda module: multi-gpu module: performance triaged,"['Hello @fbcotter, do you have some concrete reasoning as to why you expect these operations to be similar in runtime? GPUs may have characteristics that make this performance gap obvious.\r\n\r\n', 'Perhaps you are right @cpuhrsch. From past experience I thought copying host to device and device to host would take similar times. However, testing a simpler version of the above code with only 1 GPU gives the same 5:1 time difference.']","['python\r\n# coding: utf-8\r\nimport time\r\n# import py3nvml\r\nimport torch\r\nfrom torch.nn.parallel._functions import Scatter, Gather\r\nimport argparse\r\n\r\n\r\nN = 3\r\n\r\nparser = argparse.ArgumentParser(description=\'Test scatter gather\')\r\nparser.add_argument(\'--sz\', type=int, default=128, help=\'Batch size\')\r\nparser.add_argument(\'--Ngpus\', type=int, default=4, help=\'Batch size\')\r\n\r\nargs = parser.parse_args()\r\n# Set CUDA_VISIBLE_DEVICES \r\n# py3nvml.grab_gpus(args.Ngpus)\r\n\r\n# Create data so all gpus have been used once\r\ncpu = -1\r\ndevs = [torch.device(\'cuda:{}\'.format(i)) for i in range(args.Ngpus)]\r\nx_cpu = torch.randn(args.sz, 128, 128, 128)\r\nfor i in range(args.Ngpus):\r\n    y_gpu = torch.randn(args.sz, 128, 128, 128, device=devs[i])\r\n\r\ncpu_to_gpu = 0\r\nfor i in range(N):\r\n    x_cpu = torch.randn(args.sz, 128, 128, 128)\r\n\r\n    start = time.time()\r\n    out_list = Scatter.apply(devs, None, 0, x_cpu)\r\n    for i in range(args.Ngpus):\r\n        torch.cuda.synchronize(device=devs[i])\r\n    end = time.time()\r\n\r\n    cpu_to_gpu += (end-start)\r\n\r\nprint(""Scattering CPU to GPU takes {:.2f}s"".format(cpu_to_gpu/N))\r\ngpu_to_cpu = 0\r\nfor i in range(N):\r\n    y = [torch.randn(args.sz//args.Ngpus, 128, 128, 128, device=dev) for dev in devs]\r\n\r\n    start = time.time()\r\n    Gather.apply(cpu, 0, *y)\r\n    for i in range(args.Ngpus):\r\n        torch.cuda.synchronize(device=devs[i])\r\n    end = time.time()\r\n\r\n    gpu_to_cpu += (end-start)\r\n\r\nprint(""Gathering GPU to CPU takes {:.2f}s"".format(gpu_to_cpu/N))\r\n', '\r\nCollecting environment information...\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080\r\nGPU 1: GeForce GTX 1080\r\nGPU 2: GeForce GTX 1080\r\nGPU 3: GeForce GTX 1080\r\nGPU 4: GeForce GTX 1080\r\nGPU 5: GeForce GTX 1080\r\nGPU 6: GeForce GTX 1080\r\nGPU 7: GeForce GTX 1080\r\n\r\nNvidia driver version: 418.67\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.10\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.2\r\n[pip3] torch==1.2.0\r\n[conda] Could not collect\r\n']",[],0,0
512,pytorch,11232,closed,[sparse autograd] create get_indices/values; allow backward via ctor,,module: sparse,['fixed in #13001 '],[],[],0,0
513,pytorch,25320,closed,Compilation Error on Windows,"
#### ~~How to reproduce~~




## Configuration Log
<details><summary>Expand for detail</summary>
<p>



</p>
</details>

## First Error
<details><summary>Expand for detail</summary>
<p>



</p>
</details>

## Second Error
<details><summary>Expand for detail</summary>
<p>



</p>
</details>

## Third Error

The third one is same to the second one, but happens to target 

cc @peterjc123",module: build module: windows triaged,"['NOTE, I have solved the first one and walk around the second and third one and I have successfully built it. Maintainer can skip this issue for now before I making a PR to fix it.', 'thank you, looking forward to the PR!', '# the first compilation error\r\n\r\nManually compile the obj and find all included files, I find that in `C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\um\\combaseapi.h`, there is\r\n```\r\n#define PURE                    = 0\r\n```\r\non line 174. While in file https://github.com/pytorch/pytorch/blob/07fe66f25e3aac91ae29fdd8a43bb662cda6080f/aten/src/ATen/core/dispatch/OperatorOptions.h#L10-L16 , the `PURE` is expanded.\r\n\r\nthe full include path for `combaseapi.h` is\r\n```\r\nNote: including file: D:\\pytorch\\aten\\src\\ATen/Parallel.h\r\nNote: including file:  D:\\pytorch\\aten\\src\\ATen/ParallelNativeTBB.h\r\nNote: including file:   D:\\pytorch\\third_party\\tbb\\include\\tbb/tbb.h\r\nNote: including file:    D:\\pytorch\\third_party\\tbb\\include\\tbb\\combinable.h\r\nNote: including file:     D:\\pytorch\\third_party\\tbb\\include\\tbb\\enumerable_thread_specific.h\r\nNote: including file:      D:\\pytorch\\third_party\\tbb\\include\\tbb\\tbb_thread.h\r\nNote: including file:       D:\\pytorch\\third_party\\tbb\\include\\tbb\\machine/windows_api.h\r\nNote: including file:        C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\um\\windows.h\r\nNote: including file:         C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\um\\ole2.h\r\nNote: including file:          C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\um\\objbase.h\r\nNote: including file:           C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\um\\combaseapi.h\r\n```\r\n\r\n# the second and third error\r\n\r\nwith the `-keep` flag passed to nvcc, we can get the expanded code as following\r\n\r\n```cuda\r\n# file THCTensorRandom.cpp1.ii\r\n\r\n# ... skip 1070490 lines of code\r\ntemplate <typename T>\r\n__declspec(__global__) void\r\naliasMultinomialSetup(int64_t *J, T*q, int64_t inputsize, int64_t * smaller, int64_t *larger, int small_c, int large_c) {\r\n  T one = ScalarConvert<int64_t, T>::to(1);\r\n\r\n\r\n\r\n  int64_t large = 0;\r\n  int64_t char = 0;                                      // <----- this line is expanded exceptionally\r\n  while (small_c > 0 && large_c > 0) {\r\n    large = larger[large_c-1];\r\n    char = smaller[small_c-1];                           // <----- and this line\r\n    J[char] = large;                                     // <----- and this line\r\n    T q_sum = THCNumerics<T>::add(q[large], q[char]);    // <----- and this line\r\n    q[large] = THCNumerics<T>::sub(q_sum, one);\r\n    if (THCNumerics<T>::lt(q[large], one)) {\r\n      smaller[small_c-1] = large;\r\n      large_c -= 1;\r\n    } else {\r\n      larger[large_c-1] = large;\r\n      small_c -= 1;\r\n    }\r\n  }\r\n}\r\n# ... omit\r\n```\r\n\r\nManually compile this obj and find all included files, I find that in `C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\shared\\rpcndr.h`, there is\r\n```\r\n#define small char\r\n```\r\non line 190.\r\n\r\nthe full include path for `rpcndr.h` is\r\n```\r\nNote: including file: D:\\test\\pytorch\\aten\\src\\TH/THTensor.hpp\r\nNote: including file:  D:\\test\\pytorch\\aten\\src\\TH/THTensor.h\r\nNote: including file:   D:\\test\\pytorch\\aten\\src\\TH/THTensorApply.h\r\nNote: including file:    D:\\test\\pytorch\\aten\\src\\ATen/Parallel.h\r\nNote: including file:     D:\\test\\pytorch\\aten\\src\\ATen/ParallelNativeTBB.h\r\nNote: including file:      D:\\test\\pytorch\\third_party\\tbb\\include\\tbb/tbb.h\r\nNote: including file:       D:\\test\\pytorch\\third_party\\tbb\\include\\tbb\\combinable.h\r\nNote: including file:        D:\\test\\pytorch\\third_party\\tbb\\include\\tbb\\enumerable_thread_specific.h\r\nNote: including file:         D:\\test\\pytorch\\third_party\\tbb\\include\\tbb\\tbb_thread.h\r\nNote: including file:          D:\\test\\pytorch\\third_party\\tbb\\include\\tbb\\machine/windows_api.h\r\nNote: including file:           C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\um\\windows.h\r\nNote: including file:            C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\um\\winscard.h\r\nNote: including file:             C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\shared\\wtypes.h\r\nNote: including file:              C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\shared\\rpcndr.h\r\n```\r\n', 'OMG, these macros in the Windows SDK are so annoying.']","['powershell\r\n# conda env setup...\r\n# Visual Studio env setup\r\n# pytorch build env variable setup\r\n> python .\\setup.py build_ext\r\n', '\r\nBuilding wheel torch-1.3.0a0+6c9410f\r\n-- Building version 1.3.0a0+6c9410f\r\ncmake -GNinja -DATEN_THREADING=TBB -DBUILD_CAFFE2_OPS=0 -DBUILD_PYTHON=True -DBUILD_TEST=False -DCMAKE_BUILD_TYPE=RelWithDebInfo -DCMAKE_INSTALL_PREFIX=D:\\test\\pytorch\\torch -DCMAKE_PREFIX_PATH=C:\\Users\\cloud\\Miniconda3\\Lib\\site-packages -DNUMPY_INCLUDE_DIR=C:\\Users\\cloud\\Miniconda3\\lib\\site-packages\\numpy\\core\\include -DPYTHON_EXECUTABLE=C:\\Users\\cloud\\Miniconda3\\python.exe -DPYTHON_INCLUDE_DIR=C:\\Users\\cloud\\Miniconda3\\include -DPYTHON_LIBRARY=C:\\Users\\cloud\\Miniconda3/libs/python37.lib -DTORCH_BUILD_VERSION=1.3.0a0+6c9410f -DUSE_CUDA=True -DUSE_DISTRIBUTED=False -DUSE_FBGEMM=0 -DUSE_FFMPEG=0 -DUSE_LEVELDB=0 -DUSE_LMDB=0 -DUSE_NUMPY=True -DUSE_OPENCV=0 -DUSE_OPENMP=0 -DUSE_TBB=1 -DUSE_ZSTD=0 D:\\test\\pytorch  -- The CXX compiler identification is MSVC 19.22.27905.0\r\n-- The C compiler identification is MSVC 19.22.27905.0\r\n-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.22.27905/bin/Hostx64/x64/cl.exe\r\n-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.22.27905/bin/Hostx64/x64/cl.exe -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.22.27905/bin/Hostx64/x64/cl.exe\r\n-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.22.27905/bin/Hostx64/x64/cl.exe -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Not forcing any particular BLAS to be found\r\n-- Performing Test COMPILER_WORKS\r\n-- Performing Test COMPILER_WORKS - Success\r\n-- Performing Test SUPPORT_GLIBCXX_USE_C99\r\n-- Performing Test SUPPORT_GLIBCXX_USE_C99 - Success\r\n-- Performing Test CAFFE2_EXCEPTION_PTR_SUPPORTED\r\n-- Performing Test CAFFE2_EXCEPTION_PTR_SUPPORTED - Success\r\n-- std::exception_ptr is supported.\r\n-- NUMA is disabled\r\n-- Performing Test CAFFE2_NEED_TO_TURN_OFF_DEPRECATION_WARNING\r\n-- Performing Test CAFFE2_NEED_TO_TURN_OFF_DEPRECATION_WARNING - Failed\r\n-- Performing Test CAFFE2_COMPILER_SUPPORTS_AVX2_EXTENSIONS\r\n-- Performing Test CAFFE2_COMPILER_SUPPORTS_AVX2_EXTENSIONS - Success\r\n-- Current compiler supports avx2 extension. Will build perfkernels.\r\n-- Performing Test CAFFE2_COMPILER_SUPPORTS_AVX512_EXTENSIONS\r\n-- Performing Test CAFFE2_COMPILER_SUPPORTS_AVX512_EXTENSIONS - Success\r\n-- Current compiler supports avx512f extension. Will build fbgemm.\r\n-- Performing Test COMPILER_SUPPORTS_HIDDEN_VISIBILITY\r\n-- Performing Test COMPILER_SUPPORTS_HIDDEN_VISIBILITY - Failed\r\n-- Performing Test COMPILER_SUPPORTS_HIDDEN_INLINE_VISIBILITY\r\n-- Performing Test COMPILER_SUPPORTS_HIDDEN_INLINE_VISIBILITY - Failed\r\n-- Performing Test COMPILER_SUPPORTS_RDYNAMIC\r\n-- Performing Test COMPILER_SUPPORTS_RDYNAMIC - Failed\r\n-- Building using own protobuf under third_party per request.\r\n-- Use custom protobuf build.\r\n-- Looking for pthread.h\r\n-- Looking for pthread.h - not found\r\n-- Found Threads: TRUE\r\n-- Caffe2 protobuf include directory: $<BUILD_INTERFACE:D:/test/pytorch/third_party/protobuf/src>$<INSTALL_INTERFACE:include>\r\n-- Compiling TBB from source\r\n-- The ASM_MASM compiler identification is MSVC\r\n-- Found assembler: C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.22.27905/bin/Hostx64/x64/ml64.exe\r\n-- Performing Test SUPPORTS_VOLATILE_FLAG\r\n-- Performing Test SUPPORTS_VOLATILE_FLAG - Success\r\n-- Trying to find preferred BLAS backend of choice: MKL\r\n-- MKL_THREADING = OMP\r\n-- Looking for sys/types.h\r\n-- Looking for sys/types.h - found\r\n-- Looking for stdint.h\r\n-- Looking for stdint.h - found\r\n-- Looking for stddef.h\r\n-- Looking for stddef.h - found\r\n-- Check size of void*\r\n-- Check size of void* - done\r\n-- Looking for cblas_sgemm\r\n-- Looking for cblas_sgemm - found\r\n-- MKL libraries: C:/Program Files (x86)/IntelSWTools/compilers_and_libraries/windows/mkl/lib/intel64_win/mkl_intel_lp64.lib;C:/Program Files (x86)/IntelSWTools/compilers_and_libraries/windows/mkl/lib/intel64_win/mkl_intel_thread.lib;C:/Program Files (x86)/IntelSWTools/compilers_and_libraries/windows/mkl/lib/intel64_win/mkl_core.lib;C:/Program Files (x86)/IntelSWTools/compilers_and_libraries/windows/compiler/lib/intel64_win/libiomp5md.lib\r\n-- MKL include directory: C:/Program Files (x86)/IntelSWTools/compilers_and_libraries/windows/mkl/include\r\n-- MKL OpenMP type: Intel\r\n-- MKL OpenMP library: C:/Program Files (x86)/IntelSWTools/compilers_and_libraries/windows/compiler/lib/intel64_win/libiomp5md.lib\r\nCMake Warning at cmake/Dependencies.cmake:211 (message):\r\n  Target platform ""Windows"" is not supported in QNNPACK.  Supported platforms\r\n  are Android, iOS, Linux, and macOS.  Turn this warning off by\r\n  USE_QNNPACK=OFF.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:311 (include)\r\n\r\n\r\nCMake Warning at cmake/External/nnpack.cmake:21 (message):\r\n  NNPACK not supported on MSVC yet.  Turn this warning off by USE_NNPACK=OFF.\r\nCall Stack (most recent call first):\r\n  cmake/Dependencies.cmake:279 (include)\r\n  CMakeLists.txt:311 (include)\r\n\r\n\r\nCMake Warning at cmake/Dependencies.cmake:289 (message):\r\n  Not compiling with NNPACK.  Suppress this warning with -DUSE_NNPACK=OFF\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:311 (include)\r\n\r\n\r\n-- Downloading PSimd to D:/test/pytorch/build/confu-srcs/psimd (define PSIMD_SOURCE_DIR to avoid it)\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: D:/test/pytorch/build/confu-deps/psimd-download\r\n[1/9] Creating directories for \'psimd\'\r\n[2/9] Performing download step (git clone) for \'psimd\'\r\nCloning into \'psimd\'...\r\nYour branch is up to date with \'origin/master\'.\r\nAlready on \'master\'\r\n[3/9] No patch step for \'psimd\'\r\n[4/9] Performing update step for \'psimd\'\r\nCurrent branch master is up to date.\r\n[5/9] No configure step for \'psimd\'\r\n[6/9] No build step for \'psimd\'\r\n[7/9] No install step for \'psimd\'\r\n[8/9] No test step for \'psimd\'\r\n[9/9] Completed \'psimd\'\r\n-- Using third party subdirectory Eigen.\r\nPython 3.7.3\r\n-- Found PythonInterp: C:/Users/cloud/Miniconda3/python.exe (found suitable version ""3.7.3"", minimum required is ""2.7"")\r\n-- Found PythonLibs: C:/Users/cloud/Miniconda3/libs/python37.lib (found suitable version ""3.7.3"", minimum required is ""2.7"")\r\n-- Could NOT find pybind11 (missing: pybind11_DIR)\r\n-- Could NOT find pybind11 (missing: pybind11_INCLUDE_DIR)\r\n-- Using third_party/pybind11.\r\n-- Found CUDA: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1 (found version ""10.1"")\r\n-- Caffe2: CUDA detected: 10.1\r\n-- Caffe2: CUDA nvcc is: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/bin/nvcc.exe\r\n-- Caffe2: CUDA toolkit directory: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\r\n-- Caffe2: Header version is: 10.1\r\n-- Found CUDNN: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64/cudnn.lib\r\n-- Found cuDNN: v7.6.2  (include: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include, library: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64/cudnn.lib)\r\nCMake Warning at cmake/public/utils.cmake:172 (message):\r\n  In the future we will require one to explicitly pass TORCH_CUDA_ARCH_LIST\r\n  to cmake instead of implicitly setting it as an env variable.  This will\r\n  become a FATAL_ERROR in future version of pytorch.\r\nCall Stack (most recent call first):\r\n  cmake/public/cuda.cmake:369 (torch_cuda_get_nvcc_gencode_flag)\r\n  cmake/Dependencies.cmake:808 (include)\r\n  CMakeLists.txt:311 (include)\r\n\r\n\r\n-- Added CUDA NVCC flags for: -gencode;arch=compute_61,code=sm_61\r\n-- Could NOT find CUB (missing: CUB_INCLUDE_DIR)\r\nCMake Warning at cmake/Dependencies.cmake:1012 (message):\r\n  Metal is only used in ios builds.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:311 (include)\r\n\r\n\r\n--\r\n-- ******** Summary ********\r\n--   CMake version         : 3.14.4\r\n--   CMake command         : C:/Program Files/CMake/bin/cmake.exe\r\n--   System                : Windows\r\n--   C++ compiler          : C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.22.27905/bin/Hostx64/x64/cl.exe\r\n--   C++ compiler version  : 19.22.27905.0\r\n--   CXX flags             : /DWIN32 /D_WINDOWS /W3 /GR /EHsc /EHa\r\n--   Build type            : RelWithDebInfo\r\n--   Compile definitions   : TH_BLAS_MKL;_OPENMP_NOFORCE_MANIFEST;ONNX_ML=1\r\n--   CMAKE_PREFIX_PATH     : C:\\Users\\cloud\\Miniconda3\\Lib\\site-packages;C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\r\n--   CMAKE_INSTALL_PREFIX  : D:/test/pytorch/torch\r\n--   CMAKE_MODULE_PATH     : D:/test/pytorch/cmake/Modules;D:/test/pytorch/cmake/public/../Modules_CUDA_fix\r\n--\r\n--   ONNX version          : 1.5.0\r\n--   ONNX NAMESPACE        : onnx_torch\r\n--   ONNX_BUILD_TESTS      : OFF\r\n--   ONNX_BUILD_BENCHMARKS : OFF\r\n--   ONNX_USE_LITE_PROTO   : OFF\r\n--   ONNXIFI_DUMMY_BACKEND : OFF\r\n--   ONNXIFI_ENABLE_EXT    : OFF\r\n--\r\n--   Protobuf compiler     :\r\n--   Protobuf includes     :\r\n--   Protobuf libraries    :\r\n--   BUILD_ONNX_PYTHON     : OFF\r\n--\r\n-- ******** Summary ********\r\n--   CMake version         : 3.14.4\r\n--   CMake command         : C:/Program Files/CMake/bin/cmake.exe\r\n--   System                : Windows\r\n--   C++ compiler          : C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.22.27905/bin/Hostx64/x64/cl.exe\r\n--   C++ compiler version  : 19.22.27905.0\r\n--   CXX flags             : /DWIN32 /D_WINDOWS /W3 /GR /EHsc /EHa\r\n--   Build type            : RelWithDebInfo\r\n--   Compile definitions   : TH_BLAS_MKL;_OPENMP_NOFORCE_MANIFEST;ONNX_ML=1\r\n--   CMAKE_PREFIX_PATH     : C:\\Users\\cloud\\Miniconda3\\Lib\\site-packages;C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\r\n--   CMAKE_INSTALL_PREFIX  : D:/test/pytorch/torch\r\n--   CMAKE_MODULE_PATH     : D:/test/pytorch/cmake/Modules;D:/test/pytorch/cmake/public/../Modules_CUDA_fix\r\n--\r\n--   ONNX version          : 1.4.1\r\n--   ONNX NAMESPACE        : onnx_torch\r\n--   ONNX_BUILD_TESTS      : OFF\r\n--   ONNX_BUILD_BENCHMARKS : OFF\r\n--   ONNX_USE_LITE_PROTO   : OFF\r\n--   ONNXIFI_DUMMY_BACKEND : OFF\r\n--\r\n--   Protobuf compiler     :\r\n--   Protobuf includes     :\r\n--   Protobuf libraries    :\r\n--   BUILD_ONNX_PYTHON     : OFF\r\n-- Found CUDA with FP16 support, compiling with torch.cuda.HalfTensor\r\n-- Removing -DNDEBUG from compile flags\r\n-- MAGMA not found. Compiling without MAGMA support\r\n-- Could not find hardware support for NEON on this machine.\r\n-- No OMAP3 processor on this machine.\r\n-- No OMAP4 processor on this machine.\r\n-- Looking for cpuid.h\r\n-- Looking for cpuid.h - not found\r\n-- Performing Test NO_GCC_EBX_FPIC_BUG\r\n-- Performing Test NO_GCC_EBX_FPIC_BUG - Failed\r\n-- Performing Test C_HAS_AVX_1\r\n-- Performing Test C_HAS_AVX_1 - Success\r\n-- Performing Test C_HAS_AVX2_1\r\n-- Performing Test C_HAS_AVX2_1 - Success\r\n-- Performing Test CXX_HAS_AVX_1\r\n-- Performing Test CXX_HAS_AVX_1 - Success\r\n-- Performing Test CXX_HAS_AVX2_1\r\n-- Performing Test CXX_HAS_AVX2_1 - Success\r\n-- AVX compiler support found\r\n-- AVX2 compiler support found\r\n-- Performing Test BLAS_F2C_DOUBLE_WORKS\r\n-- Performing Test BLAS_F2C_DOUBLE_WORKS - Failed\r\n-- Performing Test BLAS_F2C_FLOAT_WORKS\r\n-- Performing Test BLAS_F2C_FLOAT_WORKS - Success\r\n-- Performing Test BLAS_USE_CBLAS_DOT\r\n-- Performing Test BLAS_USE_CBLAS_DOT - Success\r\n-- Found a library with BLAS API (mkl).\r\n-- Found a library with LAPACK API (mkl).\r\ndisabling ROCM because NOT USE_ROCM is set\r\n-- MIOpen not found. Compiling without MIOpen support\r\ndisabling MKLDNN because USE_MKLDNN is not set\r\n-- Performing Test C_HAS_THREAD\r\n-- Performing Test C_HAS_THREAD - Success\r\n-- don\'t use NUMA\r\n-- Performing Test COMPILER_SUPPORTS_NO_AVX256_SPLIT\r\n-- Performing Test COMPILER_SUPPORTS_NO_AVX256_SPLIT - Failed\r\nATen is compiled with TBB (D:/test/pytorch/third_party/tbb)\r\n-- NCCL operators skipped due to no CUDA support\r\n-- Excluding ideep operators as we are not using ideep\r\n-- Excluding image processing operators due to no opencv\r\n-- Excluding video processing operators due to no opencv\r\n-- MPI operators skipped due to no MPI support\r\n-- Include Observer library\r\n--   D:/test/pytorch/aten/src/ATen/native/cudnn/AffineGridGenerator.cpp\r\n--   D:/test/pytorch/aten/src/ATen/native/cudnn/BatchNorm.cpp\r\n--   D:/test/pytorch/aten/src/ATen/native/cudnn/Conv.cpp\r\n--   D:/test/pytorch/aten/src/ATen/native/cudnn/GridSampler.cpp\r\n--   D:/test/pytorch/aten/src/ATen/native/cudnn/LossCTC.cpp\r\n--   D:/test/pytorch/aten/src/ATen/native/cudnn/RNN.cpp\r\n--   D:/test/pytorch/aten/src/ATen/native/miopen/BatchNorm_miopen.cpp\r\n--   D:/test/pytorch/aten/src/ATen/native/miopen/Conv_miopen.cpp\r\n--   D:/test/pytorch/aten/src/ATen/native/miopen/RNN_miopen.cpp\r\n--   D:/test/pytorch/aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cpp\r\n--   D:/test/pytorch/aten/src/ATen/cuda/CUDABlas.cpp\r\n--   D:/test/pytorch/aten/src/ATen/cuda/CUDAContext.cpp\r\n--   D:/test/pytorch/aten/src/ATen/cuda/CUDAGenerator.cpp\r\n--   D:/test/pytorch/aten/src/ATen/cuda/PinnedMemoryAllocator.cpp\r\n--   D:/test/pytorch/aten/src/ATen/cuda/detail/CUDAHooks.cpp\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/CUDAUnaryOps.cpp\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp\r\n--   D:/test/pytorch/build/aten/src/ATen/CUDAType.cpp\r\n--   D:/test/pytorch/build/aten/src/ATen/CUDAType.h\r\n--   D:/test/pytorch/build/aten/src/ATen/LegacyTHFunctionsCUDA.cpp\r\n--   D:/test/pytorch/build/aten/src/ATen/LegacyTHFunctionsCUDA.h\r\n--   D:/test/pytorch/build/aten/src/ATen/SparseCUDAType.cpp\r\n--   D:/test/pytorch/build/aten/src/ATen/SparseCUDAType.h\r\n--   D:/test/pytorch/aten/src/THC/THCCachingHostAllocator.cpp\r\n--   D:/test/pytorch/aten/src/THC/THCGeneral.cpp\r\n--   D:/test/pytorch/aten/src/THC/THCStorageCopy.cpp\r\n--   D:/test/pytorch/aten/src/THC/THCTensor.cpp\r\n--   D:/test/pytorch/aten/src/THC/THCReduceApplyUtils.cu\r\n--   D:/test/pytorch/aten/src/THC/THCBlas.cu\r\n--   D:/test/pytorch/aten/src/THC/THCSleep.cu\r\n--   D:/test/pytorch/aten/src/THC/THCStorage.cu\r\n--   D:/test/pytorch/aten/src/THC/THCStorageCopy.cu\r\n--   D:/test/pytorch/aten/src/THC/THCTensor.cu\r\n--   D:/test/pytorch/aten/src/THC/THCTensorCopy.cu\r\n--   D:/test/pytorch/aten/src/THC/THCTensorMath.cu\r\n--   D:/test/pytorch/aten/src/THC/THCTensorMathBlas.cu\r\n--   D:/test/pytorch/aten/src/THC/THCTensorMathMagma.cu\r\n--   D:/test/pytorch/aten/src/THC/THCTensorMathPairwise.cu\r\n--   D:/test/pytorch/aten/src/THC/THCTensorMathReduce.cu\r\n--   D:/test/pytorch/aten/src/THC/THCTensorMathScan.cu\r\n--   D:/test/pytorch/aten/src/THC/THCTensorIndex.cu\r\n--   D:/test/pytorch/aten/src/THC/THCTensorRandom.cu\r\n--   D:/test/pytorch/aten/src/THC/THCTensorScatterGather.cu\r\n--   D:/test/pytorch/aten/src/THC/THCTensorTopK.cu\r\n--   D:/test/pytorch/aten/src/THC/THCTensorSort.cu\r\n--   D:/test/pytorch/aten/src/THC/THCSortUtils.cu\r\n--   D:/test/pytorch/aten/src/THC/THCTensorMode.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorSortByte.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathCompareTByte.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathPointwiseByte.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathCompareByte.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathReduceByte.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMaskedByte.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorSortChar.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathCompareTChar.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathPointwiseChar.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathCompareChar.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathReduceChar.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMaskedChar.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorSortShort.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathCompareTShort.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathPointwiseShort.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathCompareShort.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathReduceShort.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMaskedShort.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorSortInt.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathCompareTInt.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathPointwiseInt.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathCompareInt.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathReduceInt.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMaskedInt.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorSortLong.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathCompareTLong.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathPointwiseLong.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathCompareLong.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathReduceLong.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMaskedLong.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorSortHalf.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathCompareTHalf.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathPointwiseHalf.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathCompareHalf.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathReduceHalf.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMaskedHalf.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorSortFloat.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathCompareTFloat.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathPointwiseFloat.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathCompareFloat.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathReduceFloat.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMaskedFloat.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorSortDouble.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathCompareTDouble.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathPointwiseDouble.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathCompareDouble.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathReduceDouble.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMaskedDouble.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathCompareTBool.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathCompareBool.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathReduceBool.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMaskedBool.cu\r\n--   D:/test/pytorch/aten/src/THC/generated/THCTensorMathPointwiseBool.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/AbsCriterion.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/Abs.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/BCECriterion.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/DistKLDivCriterion.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/ELU.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/FeatureLPPooling.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/GatedLinearUnit.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/HardTanh.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/IndexLinear.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/L1Cost.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/LeakyReLU.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/LogSigmoid.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/LookupTableBag.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/LookupTable.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/MarginCriterion.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/MSECriterion.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/MultiLabelMarginCriterion.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/MultiMarginCriterion.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/RReLU.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/Sigmoid.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/SmoothL1Criterion.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/SoftMarginCriterion.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/SoftPlus.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/SoftShrink.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/SpatialClassNLLCriterion.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/SpatialConvolutionLocal.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/SpatialConvolutionMM.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/SpatialCrossMapLRN.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/SpatialDepthwiseConvolution.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/SpatialSubSampling.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/Sqrt.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/Square.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/Tanh.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/TemporalConvolution.cu\r\n--   D:/test/pytorch/aten/src/THCUNN/TemporalMaxPooling.cu\r\n--   D:/test/pytorch/aten/src/ATen/cuda/detail/IndexUtils.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/Activation.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/AveragePool2d.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/AveragePool3d.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/BatchLinearAlgebra.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/BinaryOpsKernel.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/CUDAScalar.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/Col2Im.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/Copy.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/CrossKernel.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/DilatedMaxPool2d.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/DilatedMaxPool3d.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/DistanceKernel.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/Distributions.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/Dropout.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/Embedding.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/EmbeddingBag.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/FillKernel.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/FractionalMaxPool2d.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/FractionalMaxPool3d.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/GridSampler.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/Im2Col.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/Indexing.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/Lerp.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/LinearAlgebra.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/Loss.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/LossCTC.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/MaxUnpooling.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/NaiveConvolutionTranspose3d.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/NaiveDilatedConvolution.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/Normalization.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/PointwiseOpsKernel.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/RNN.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/RangeFactories.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/Reduce.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/ReduceOpsKernel.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/ReflectionPad.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/Repeat.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/ReplicationPadding.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/Resize.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/SoftMax.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/SortingKthValue.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/SparseMM.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/SpectralOps.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/SummaryOps.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/TensorCompare.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/TensorFactories.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/TensorTransformations.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/UnaryOpsKernel.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/Unique.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/UpSampleBicubic2d.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/UpSampleBilinear2d.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/UpSampleLinear1d.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/UpSampleNearest1d.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/UpSampleNearest2d.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/UpSampleNearest3d.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/cuda/WeightNorm.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/sparse/cuda/SparseCUDABlas.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu\r\n--   D:/test/pytorch/aten/src/ATen/native/quantized/cuda/fake_quantize_per_tensor_affine.cu\r\n--   D:/test/pytorch/aten/src/ATen/cudnn/Descriptors.cpp\r\n--   D:/test/pytorch/aten/src/ATen/cudnn/Handle.cpp\r\n--   D:/test/pytorch/aten/src/ATen/cudnn/Types.cpp\r\n--   D:/test/pytorch/caffe2/core/common_cudnn.cc\r\n--   D:/test/pytorch/caffe2/core/blob_serialization_gpu.cc\r\n--   D:/test/pytorch/caffe2/core/common_gpu.cc\r\n--   D:/test/pytorch/caffe2/core/event_gpu.cc\r\n--   D:/test/pytorch/caffe2/core/context_gpu.cu\r\n--   utils/math/broadcast.cu\r\n--   utils/math/elementwise.cu\r\n--   utils/math/reduce.cu\r\n--   utils/math/transpose.cu\r\n--   utils/math_gpu.cu\r\n--   D:/test/pytorch/caffe2/db/create_db_op_gpu.cc\r\n--   D:/test/pytorch/caffe2/distributed/file_store_handler_op_gpu.cc\r\n--   D:/test/pytorch/caffe2/queue/queue_ops_gpu.cc\r\n--   D:/test/pytorch/caffe2/sgd/iter_op_gpu.cc\r\n--   D:/test/pytorch/caffe2/sgd/learning_rate_op_gpu.cc\r\n--   D:/test/pytorch/caffe2/sgd/adadelta_op_gpu.cu\r\n--   D:/test/pytorch/caffe2/sgd/adagrad_op_gpu.cu\r\n--   D:/test/pytorch/caffe2/sgd/adam_op_gpu.cu\r\n--   D:/test/pytorch/caffe2/sgd/fp16_momentum_sgd_op.cu\r\n--   D:/test/pytorch/caffe2/sgd/fp32_momentum_sgd_op.cu\r\n--   D:/test/pytorch/caffe2/sgd/lars_op_gpu.cu\r\n--   D:/test/pytorch/caffe2/sgd/momentum_sgd_op_gpu.cu\r\n--   D:/test/pytorch/caffe2/sgd/rmsprop_op_gpu.cu\r\n--   D:/test/pytorch/caffe2/sgd/yellowfin_op_gpu.cu\r\n--   D:/test/pytorch/caffe2/../torch/csrc/jit/fuser/cuda/fused_kernel.cpp\r\n--   D:/test/pytorch/caffe2/../torch/csrc/autograd/profiler_cuda.cpp\r\n--   D:/test/pytorch/caffe2/../torch/csrc/autograd/functions/comm.cpp\r\n--   D:/test/pytorch/caffe2/../torch/csrc/cuda/comm.cpp\r\n-- Using ATen parallel backend: TBB\r\n-- Using Lib/site-packages as python relative installation path\r\nCMake Warning at CMakeLists.txt:533 (message):\r\n  Generated cmake files are only fully tested if one builds with system glog,\r\n  gflags, and protobuf.  Other settings may generate files that are not well\r\n  tested.\r\n\r\n\r\n--\r\n-- ******** Summary ********\r\n-- General:\r\n--   CMake version         : 3.14.4\r\n--   CMake command         : C:/Program Files/CMake/bin/cmake.exe\r\n--   System                : Windows\r\n--   C++ compiler          : C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.22.27905/bin/Hostx64/x64/cl.exe\r\n--   C++ compiler id       : MSVC\r\n--   C++ compiler version  : 19.22.27905.0\r\n--   BLAS                  : MKL\r\n--   CXX flags             : /DWIN32 /D_WINDOWS /W3 /GR /EHsc /EHa /MP /bigobj\r\n--   Build type            : RelWithDebInfo\r\n--   Compile definitions   : TH_BLAS_MKL;_OPENMP_NOFORCE_MANIFEST;ONNX_ML=1;ONNX_NAMESPACE=onnx_torch;_CRT_SECURE_NO_DEPRECATE=1\r\n--   CMAKE_PREFIX_PATH     : C:\\Users\\cloud\\Miniconda3\\Lib\\site-packages;C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\r\n--   CMAKE_INSTALL_PREFIX  : D:/test/pytorch/torch\r\n--\r\n--   TORCH_VERSION         : 1.3.0\r\n--   CAFFE2_VERSION        : 1.3.0\r\n--   BUILD_CAFFE2_MOBILE   : ON\r\n--   USE_STATIC_DISPATCH   : OFF\r\n--   BUILD_ATEN_ONLY       : OFF\r\n--   BUILD_BINARY          : OFF\r\n--   BUILD_CUSTOM_PROTOBUF : ON\r\n--     Link local protobuf : ON\r\n--   BUILD_DOCS            : OFF\r\n--   BUILD_PYTHON          : True\r\n--     Python version      : 3.7.3\r\n--     Python executable   : C:/Users/cloud/Miniconda3/python.exe\r\n--     Pythonlibs version  : 3.7.3\r\n--     Python library      : C:/Users/cloud/Miniconda3/libs/python37.lib\r\n--     Python includes     : C:/Users/cloud/Miniconda3/include\r\n--     Python site-packages: Lib/site-packages\r\n--   BUILD_CAFFE2_OPS      : 0\r\n--   BUILD_SHARED_LIBS     : ON\r\n--   BUILD_TEST            : False\r\n--   INTERN_BUILD_MOBILE   :\r\n--   USE_ASAN              : OFF\r\n--   USE_CUDA              : True\r\n--     CUDA static link    : OFF\r\n--     USE_CUDNN           : ON\r\n--     CUDA version        : 10.1\r\n--     cuDNN version       : 7.6.2\r\n--     CUDA root directory : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\r\n--     CUDA library        : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64/cuda.lib\r\n--     cudart library      : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64/cudart_static.lib\r\n--     cublas library      : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64/cublas.lib\r\n--     cufft library       : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64/cufft.lib\r\n--     curand library      : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64/curand.lib\r\n--     cuDNN library       : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64/cudnn.lib\r\n--     nvrtc               : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64/nvrtc.lib\r\n--     CUDA include path   : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\r\n--     NVCC executable     : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/bin/nvcc.exe\r\n--     CUDA host compiler  : C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.22.27905/bin/Hostx64/x64/cl.exe\r\n--     USE_TENSORRT        : OFF\r\n--   USE_ROCM              : OFF\r\n--   USE_EIGEN_FOR_BLAS    :\r\n--   USE_FBGEMM            : OFF\r\n--   USE_FFMPEG            : 0\r\n--   USE_GFLAGS            : OFF\r\n--   USE_GLOG              : OFF\r\n--   USE_LEVELDB           : 0\r\n--   USE_LITE_PROTO        : OFF\r\n--   USE_LMDB              : 0\r\n--   USE_METAL             : OFF\r\n--   USE_MKL               : ON\r\n--   USE_MKLDNN            : OFF\r\n--   USE_NCCL              : OFF\r\n--   USE_NNPACK            : OFF\r\n--   USE_NUMPY             : ON\r\n--   USE_OBSERVERS         : ON\r\n--   USE_OPENCL            : OFF\r\n--   USE_OPENCV            : 0\r\n--   USE_OPENMP            : 0\r\n--   USE_TBB               : 1\r\n--   USE_PROF              : OFF\r\n--   USE_QNNPACK           : OFF\r\n--   USE_REDIS             : OFF\r\n--   USE_ROCKSDB           : OFF\r\n--   USE_ZMQ               : OFF\r\n--   USE_DISTRIBUTED       : OFF\r\n--   BUILD_NAMEDTENSOR   : OFF\r\n--   Public Dependencies  : Threads::Threads;caffe2::mkl\r\n--   Private Dependencies : cpuinfo;fp16;foxi_loader\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: D:/test/pytorch/build\r\ncmake --build . --target install --config RelWithDebInfo -- -j 6\r\n', ""\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorOptions.h(15): error C2143: syntax error: missing '}' before '='    D:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorOptions.h(15): error C2513: 'c10::AliasAnalysisKind': no variable declared before '='\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorOptions.h(16): error C2143: syntax error: missing ';' before '}'    D:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorOptions.h(18): error C2065: 'AliasAnalysisKind': undeclared identifier\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorOptions.h(18): error C2146: syntax error: missing ')' before identifier 'aliasAnalysisKind'\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorOptions.h(18): error C2143: syntax error: missing ';' before '{'    D:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorOptions.h(18): error C2447: '{': missing function header (old-style formal list?)\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorOptions.h(32): error C3646: 'aliasAnalysis': unknown override specifier\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorOptions.h(32): error C2059: syntax error: '('\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorOptions.h(32): error C2334: unexpected token(s) preceding '{'; skipping apparent function body\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorOptions.h(36): error C2061: syntax error: identifier 'AliasAnalysisKind'\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorOptions.h(49): error C3646: 'aliasAnalysisKind_': unknown override specifier\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorOptions.h(49): error C2059: syntax error: '='\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorOptions.h(49): error C2653: 'AliasAnalysisKind': is not a class or namespace name\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorOptions.h(49): error C2238: unexpected token(s) preceding ';'       D:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorOptions.h(37): error C2065: 'aliasAnalysisKind_': undeclared identifier\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorOptions.h(37): error C2065: 'v': undeclared identifier\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorOptions.h(41): error C2039: 'aliasAnalysisKind_': is not a member of 'OperatorOptions'\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorOptions.h(30): note: see declaration of 'OperatorOptions'\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorOptions.h(52): error C2059: syntax error: '}'\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorOptions.h(52): error C2143: syntax error: missing ';' before '}'    D:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/RegistrationHandleRAII.h(5): error C2143: syntax error: missing ';' before '{'\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/RegistrationHandleRAII.h(5): error C2447: '{': missing function header (old-style formal list?)\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorEntry.h(102): error C3646: 'registerKernel': unknown override specifier\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorEntry.h(102): error C2059: syntax error: '('\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorEntry.h(102): error C2238: unexpected token(s) preceding ';'        D:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorEntry.h(103): error C3646: 'registerCatchallKernel': unknown override specifier\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorEntry.h(103): error C2059: syntax error: '('\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorEntry.h(103): error C2238: unexpected token(s) preceding ';'        D:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorEntry.h(105): error C3646: 'registerUnboxedAutogradKernel': unknown override specifier\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorEntry.h(105): error C2059: syntax error: '('\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/OperatorEntry.h(105): error C2238: unexpected token(s) preceding ';'        D:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/Dispatcher.h(81): error C3646: 'registerKernel': unknown override specifier D:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/Dispatcher.h(81): error C2059: syntax error: '('\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/Dispatcher.h(81): error C2238: unexpected token(s) preceding ';'\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/Dispatcher.h(91): error C3646: 'registerCatchallKernel': unknown override specifier\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/Dispatcher.h(91): error C2059: syntax error: '('\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/Dispatcher.h(91): error C2238: unexpected token(s) preceding ';'\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/Dispatcher.h(93): error C3646: 'registerUnboxedAutogradKernel': unknown override specifier\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/Dispatcher.h(93): error C2059: syntax error: '('\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/Dispatcher.h(93): error C2238: unexpected token(s) preceding ';'\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/Dispatcher.h(169): error C2061: syntax error: identifier 'RegistrationHandleRAII'\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/Dispatcher.h(173): error C3646: 'registrationHandle_': unknown override specifier\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/Dispatcher.h(173): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int\r\nD:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/Dispatcher.h(170): error C2065: 'registrationHandle': undeclared identifier D:\\test\\pytorch\\aten\\src\\ATen/core/dispatch/Dispatcher.h(170): error C2614: 'c10::SchemaRegistrationHandleRAII': illegal member initialization: 'registrationHandle_' is not a base or member\r\n"", '\r\nD:\\test\\pytorch\\aten\\src\\THC/THCTensorRandom.cuh(379): error: invalid combination of type specifiers\r\n          detected during instantiation of ""void aliasMultinomialSetup(int64_t *, T *, int64_t, int64_t *, int64_t *, int, int) [with T=c10::Half]""\r\nD:\\test\\pytorch\\aten\\src\\THC/generic/THCTensorRandom.cu(262): here\r\n\r\nD:\\test\\pytorch\\aten\\src\\THC/THCTensorRandom.cuh(379): error: expected an identifier\r\n          detected during instantiation of ""void aliasMultinomialSetup(int64_t *, T *, int64_t, int64_t *, int64_t *, int, int) [with T=c10::Half]""\r\nD:\\test\\pytorch\\aten\\src\\THC/generic/THCTensorRandom.cu(262): here\r\n\r\nD:\\test\\pytorch\\aten\\src\\THC/THCTensorRandom.cuh(382): error: expected an identifier\r\n          detected during instantiation of ""void aliasMultinomialSetup(int64_t *, T *, int64_t, int64_t *, int64_t *, int, int) [with T=c10::Half]""\r\nD:\\test\\pytorch\\aten\\src\\THC/generic/THCTensorRandom.cu(262): here\r\n\r\nD:\\test\\pytorch\\aten\\src\\THC/THCTensorRandom.cuh(383): error: type name is not allowed\r\n          detected during instantiation of ""void aliasMultinomialSetup(int64_t *, T *, int64_t, int64_t *, int64_t *, int, int) [with T=c10::Half]""\r\nD:\\test\\pytorch\\aten\\src\\THC/generic/THCTensorRandom.cu(262): here\r\n\r\nD:\\test\\pytorch\\aten\\src\\THC/THCTensorRandom.cuh(384): error: type name is not allowed\r\n          detected during instantiation of ""void aliasMultinomialSetup(int64_t *, T *, int64_t, int64_t *, int64_t *, int, int) [with T=c10::Half]""\r\nD:\\test\\pytorch\\aten\\src\\THC/generic/THCTensorRandom.cu(262): here\r\n\r\nD:\\test\\pytorch\\aten\\src\\THC/THCTensorRandom.cuh(379): error: invalid combination of type specifiers\r\n          detected during instantiation of ""void aliasMultinomialSetup(int64_t *, T *, int64_t, int64_t *, int64_t *, int, int) [with T=float]""\r\nD:\\test\\pytorch\\aten\\src\\THC/generic/THCTensorRandom.cu(262): here\r\n\r\nD:\\test\\pytorch\\aten\\src\\THC/THCTensorRandom.cuh(379): error: expected an identifier\r\n          detected during instantiation of ""void aliasMultinomialSetup(int64_t *, T *, int64_t, int64_t *, int64_t *, int, int) [with T=float]""\r\nD:\\test\\pytorch\\aten\\src\\THC/generic/THCTensorRandom.cu(262): here\r\n\r\nD:\\test\\pytorch\\aten\\src\\THC/THCTensorRandom.cuh(382): error: expected an identifier\r\n          detected during instantiation of ""void aliasMultinomialSetup(int64_t *, T *, int64_t, int64_t *, int64_t *, int, int) [with T=float]""\r\nD:\\test\\pytorch\\aten\\src\\THC/generic/THCTensorRandom.cu(262): here\r\n\r\nD:\\test\\pytorch\\aten\\src\\THC/THCTensorRandom.cuh(383): error: type name is not allowed\r\n          detected during instantiation of ""void aliasMultinomialSetup(int64_t *, T *, int64_t, int64_t *, int64_t *, int, int) [with T=float]""\r\nD:\\test\\pytorch\\aten\\src\\THC/generic/THCTensorRandom.cu(262): here\r\n\r\nD:\\test\\pytorch\\aten\\src\\THC/THCTensorRandom.cuh(384): error: type name is not allowed\r\n          detected during instantiation of ""void aliasMultinomialSetup(int64_t *, T *, int64_t, int64_t *, int64_t *, int, int) [with T=float]""\r\nD:\\test\\pytorch\\aten\\src\\THC/generic/THCTensorRandom.cu(262): here\r\n\r\nD:\\test\\pytorch\\aten\\src\\THC/THCTensorRandom.cuh(379): error: invalid combination of type specifiers\r\n          detected during instantiation of ""void aliasMultinomialSetup(int64_t *, T *, int64_t, int64_t *, int64_t *, int, int) [with T=double]""\r\nD:\\test\\pytorch\\aten\\src\\THC/generic/THCTensorRandom.cu(262): here\r\n\r\nD:\\test\\pytorch\\aten\\src\\THC/THCTensorRandom.cuh(379): error: expected an identifier\r\n          detected during instantiation of ""void aliasMultinomialSetup(int64_t *, T *, int64_t, int64_t *, int64_t *, int, int) [with T=double]""\r\nD:\\test\\pytorch\\aten\\src\\THC/generic/THCTensorRandom.cu(262): here\r\n\r\nD:\\test\\pytorch\\aten\\src\\THC/THCTensorRandom.cuh(382): error: expected an identifier\r\n          detected during instantiation of ""void aliasMultinomialSetup(int64_t *, T *, int64_t, int64_t *, int64_t *, int, int) [with T=double]""\r\nD:\\test\\pytorch\\aten\\src\\THC/generic/THCTensorRandom.cu(262): here\r\n\r\nD:\\test\\pytorch\\aten\\src\\THC/THCTensorRandom.cuh(383): error: type name is not allowed\r\n          detected during instantiation of ""void aliasMultinomialSetup(int64_t *, T *, int64_t, int64_t *, int64_t *, int, int) [with T=double]""\r\nD:\\test\\pytorch\\aten\\src\\THC/generic/THCTensorRandom.cu(262): here\r\n\r\nD:\\test\\pytorch\\aten\\src\\THC/THCTensorRandom.cuh(384): error: type name is not allowed\r\n          detected during instantiation of ""void aliasMultinomialSetup(int64_t *, T *, int64_t, int64_t *, int64_t *, int, int) [with T=double]""\r\nD:\\test\\pytorch\\aten\\src\\THC/generic/THCTensorRandom.cu(262): here\r\n']",['caffe2\\CMakeFiles\\torch.dir\\__\\aten\\src\\TH\\THTensorRandom.cpp.obj'],0,0
514,pytorch,140,closed,Index / SetValue don't work for 1D Variables,"Snippet to reproduce.



The problem is in [here](https://github.com/pytorch/pytorch/blob/master/torch/autograd/functions/tensor.py#L27), [here](https://github.com/pytorch/pytorch/blob/master/torch/autograd/functions/tensor.py#L40) and [here](https://github.com/pytorch/pytorch/blob/master/torch/autograd/functions/tensor.py#L45), and is due to the fact that  return numbers when indexing a 1D tensor (which I think is the desired behaviour as we don't have broadcasting yet?).

Maybe we should check if the input is 1D (and that the index is of range 1) and to use  instead of  in this case?
Autograd  return a 1D tensor anyway if we index it with a 1D tensor in forward, but this solution could avoid the sync point with cuda tensors in  (as we don't return a number).

Or, another solution would be to have something like 0D tensors which behave almost like numbers? This seems to be what numpy does.
",high priority,"['One thing to have in mind, if we only add the narrow thing I mentioned, some more special treatment will need to be added if we want to do something like\n\n``` python\nimport torch\nfrom torch.autograd import Variable\n\nx = Variable(torch.Tensor([1, 2, 3]), requires_grad=True)\ny = Variable(torch.Tensor([2, 3, 4]), requires_grad=True)\nx[0] = y[1] # this would error out with narrows\nx[y[0]] = 1 # error as well, but less common\n```\n\nThe issue first issue would be that we pass a 1D tensor as the filling value to the `fill` function (instead of a number).\nThe second one would be that we would be passing a 1D tensor as argument of the index (or narrow) function instead of a number as well.\n\nGetting the number back from the tensor would be a sync point on the GPU (as is already the case btw), and might make the code pretty ugly with lots of checks here and there asserting for 1d tensors that are supposed to be numbers.\n\nI think this is a corner case though, as maybe not many people will be changing single elements of the tensors (would be slow!).\n', 'Resolution of this issue will depend on what we decide about #144.\n', 'Fixed in #176.\n']","[' python\nimport torch\nfrom torch.autograd import Variable\n\nx = Variable(torch.Tensor([1, 2, 3]), requires_grad=True)\ny = x[0]\n\ny.backward() # error\n\nx[1] = 5 # error\n']","['torch.Tensor', 'narrow', 'select', 'Variables', 'Index']",0,0
515,pytorch,23738,closed,Python 3.5 conda nightlies don't work,"Reproducer in a Python 3.5 conda environment:

",module: binaries triaged,"['This is apparently because we have never built conda binaries for python 3.5', 'They were disabled in https://github.com/pytorch/pytorch/pull/21380 without explanation. cc @kostmo', 'They were disabled because we hoped to drop support for 3.5. But 3.8 release is not official yet, so we need to retain 3.5', ""They've been turned back on!"", ""I am using conda 4.7.10 (after `conda update --all), but the problem persists for Python 3.7.3\r\n\r\n```\r\n(base) >conda install walrus\r\nCollecting package metadata (current_repodata.json): done\r\nSolving environment: failed with current_repodata.json, will retry with next repodata source.\r\nCollecting package metadata (repodata.json): done\r\nSolving environment: failed\r\n\r\nPackagesNotFoundError: The following packages are not available from current channels:\r\n\r\n  - walrus\r\n\r\nCurrent channels:\r\n\r\n  - https://repo.anaconda.com/pkgs/main/win-64\r\n  - https://repo.anaconda.com/pkgs/main/noarch\r\n  - https://repo.anaconda.com/pkgs/r/win-64\r\n  - https://repo.anaconda.com/pkgs/r/noarch\r\n  - https://repo.anaconda.com/pkgs/msys2/win-64\r\n  - https://repo.anaconda.com/pkgs/msys2/noarch\r\n\r\nTo search for alternate channels that may provide the conda package you're\r\nlooking for, navigate to\r\n\r\n    https://anaconda.org\r\n\r\nand use the search bar at the top of the page.\r\n```"", 'I enabled verbose, I noticed this issue:\r\n\r\n```\r\nDEBUG urllib3.connectionpool:_make_request(393): https://repo.anaconda.com:443 ""GET /pkgs/r/win-64/current_repodata.json HTTP/1.1"" 304 0\r\nDEBUG conda.core.subdir_data:fetch_repodata_remote_request(473):\r\n>>GET /pkgs/r/win-64/current_repodata.json HTTPS\r\n> User-Agent: conda/4.7.10 requests/2.22.0 CPython/3.7.3 Windows/10 Windows/10.0.18362\r\n> Accept: */*\r\n> Accept-Encoding: gzip, deflate, compress, identity\r\n> Connection: keep-alive\r\n> Content-Type: application/json\r\n> If-Modified-Since: Tue, 25 Jun 2019 19:50:04 GMT\r\n> If-None-Match: W/""9cdacec2edd24dc16ae8d85ef2f9b44c""\r\n\r\n<<HTTPS 304 Not Modified\r\n< Age: 1137332\r\n< Cache-Control: public, max-age=30\r\n< CF-Cache-Status: HIT\r\n< CF-RAY: 501cc49d1d10a9ca-SIN\r\n< Date: Tue, 06 Aug 2019 00:06:59 GMT\r\n< ETag: ""9cdacec2edd24dc16ae8d85ef2f9b44c""\r\n< Expect-CT: max-age=604800, report-uri=""https://report-uri.cloudflare.com/cdn-cgi/beacon/expect-ct""\r\n< Expires: Tue, 06 Aug 2019 00:07:29 GMT\r\n< Last-Modified: Tue, 25 Jun 2019 19:50:04 GMT\r\n< Server: cloudflare\r\n< Set-Cookie: __cfduid=d18d2c413b88cc96ff92eeb5378a430f31565050019; expires=Wed, 05-Aug-20 00:06:59 GMT; path=/; domain=.anaconda.com; HttpOnly; Secure\r\n< Vary: Accept-Encoding\r\n< x-amz-id-2: NXonvH8FUrJMkNisTyGODHlu1e2wPrcaXn7UNvdTWv3zVzbbY+DqfCdXsSCQR1nx+JfE8HHnoSQ=\r\n< x-amz-request-id: 968302B7283965DE\r\n< x-amz-version-id: nBF.V6Lk.eFofKCObPpzwdG1RgOYh9KC\r\n< Connection: keep-alive\r\n< Elapsed: 00:00.131019\r\n```\r\n\r\nHowever, I could manually download `https://repo.anaconda.com/pkgs/r/win-64/current_repodata.json` using my browser.  It seems that many websites are now protected with Cloudflare.  \r\n']","[""\r\nconda install -c pytorch pytorch-nightly                      \r\nCollecting package metadata (current_repodata.json): done\r\nSolving environment: failed with current_repodata.json, will retry with next repodata source.\r\nInitial quick solve with frozen env failed.  Unfreezing env and trying again.                                                    \r\nSolving environment: failed with current_repodata.json, will retry with next repodata source.\r\nCollecting package metadata (repodata.json): done\r\nSolving environment: failed\r\nInitial quick solve with frozen env failed.  Unfreezing env and trying again.                                                    \r\nSolving environment: failed\r\n\r\nUnsatisfiableError: The following specifications were found\r\nto be incompatible with the existing python installation in your environment:                                                    \r\n\r\n  - pytorch-nightly -> python[version='>=2.7,<2.8.0a0,>=3.6,<3.7.0a0,>=3.7,<3.8.0a0']                                            \r\n\r\nIf python is on the left-most side of the chain, that's the version you've asked for.                                            \r\nWhen python appears to the right, that indicates that the thing on the left is somehow                                           \r\nnot available for the python version you are constrained to.  Your current python version                                        \r\nis (python=3.5).  Note that conda will not change your python version to a different minor version                               \r\nunless you explicitly specify that.\r\n\r\nThe following specifications were found to be incompatible with each other:                                                      \r\n\r\n\r\n\r\nPackage certifi conflicts for:\r\npytorch-nightly -> numpy[version='>=1.11'] -> mkl_random[version='>=1.0.2,<2.0a0'] -> numpy-base[version='>=1.0.2,<2.0a0'] -> python[version='>=2.7,<2.8.0a0,>=3.6,<3.7.0a0,>=3.7,<3.8.0a0'] -> pip -> wheel -> setuptools -> certifi[version='>=2016.09']         \r\npython=3.5 -> pip -> wheel -> setuptools -> certifi[version='>=2016.09']                                                         \r\nPackage pip conflicts for:\r\npython=3.5 -> pip\r\npytorch-nightly -> numpy[version='>=1.11'] -> mkl_random[version='>=1.0.2,<2.0a0'] -> numpy-base[version='>=1.0.2,<2.0a0'] -> python[version='>=2.7,<2.8.0a0,>=3.6,<3.7.0a0,>=3.7,<3.8.0a0'] -> pip\r\nPackage wheel conflicts for:\r\npython=3.5 -> pip -> wheel\r\npytorch-nightly -> numpy[version='>=1.11'] -> mkl_random[version='>=1.0.2,<2.0a0'] -> numpy-base[version='>=1.0.2,<2.0a0'] -> python[version='>=2.7,<2.8.0a0,>=3.6,<3.7.0a0,>=3.7,<3.8.0a0'] -> pip -> wheel                                                       \r\nPackage setuptools conflicts for:\r\npython=3.5 -> pip -> wheel -> setuptools\r\npytorch-nightly -> numpy[version='>=1.11'] -> mkl_random[version='>=1.0.2,<2.0a0'] -> numpy-base[version='>=1.0.2,<2.0a0'] -> python[version='>=2.7,<2.8.0a0,>=3.6,<3.7.0a0,>=3.7,<3.8.0a0'] -> pip -> wheel -> setuptools     \r\n""]",[],0,0
516,pytorch,14967,closed,ONNX - Pytorch-1.0 with Cuda-10.0. ONNX Export core dumps on GPU machine,"## üêõ Bug

Pytorch-1.0-Cuda-10.0 with ONNX-1.2.1 and 1.3.0. Export onnx model core dumps on Ubuntu on GPU instance

## To Reproduce

 Steps to reproduce the behavior:
 
Spin up an Ubuntu with GPU instance like EC2 p2 or g3
 
1. install Cuda-10, cudnn-7.4.1, NCCL-2.3.7
2. anaconda
2. conda create -n pytorch_p27 python=2.7
3. conda install pytorch=1.0.0=py2.7_cuda10.0.130_cudnn7.4.1_1 torchvision=0.2.1 cuda100=1.0 -c pytorch
4. pip install -U onnx==1.2.1 or 1.3.0
 
example used to replicate:
https://github.com/onnx/tutorials/blob/master/tutorials/PytorchOnnxExport.ipynb
 
Error:
Illegal instruction (core dumped)



## Expected behavior

It should have exported a .onnx file with the model

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
PyTorch version: 1.0.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130
 
OS: Ubuntu 16.04.5 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CMake version: version 3.5.1
 
Python version: 2.7
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration:
GPU 0: Tesla M60
GPU 1: Tesla M60
GPU 2: Tesla M60
GPU 3: Tesla M60
 
Nvidia driver version: 410.79
cuDNN version: Probably one of the following:
/usr/local/cuda-10.0/lib64/libcudnn.so.7.4.1
/usr/local/cuda-10.0/lib64/libcudnn_static.a
/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21
/usr/local/cuda-8.0/lib64/libcudnn_static.a
/usr/local/cuda-9.0/lib64/libcudnn.so.7.3.1
/usr/local/cuda-9.0/lib64/libcudnn_static.a
/usr/local/cuda-9.2/lib64/libcudnn.so.7.3.1
/usr/local/cuda-9.2/lib64/libcudnn_static.a
 
Versions of relevant libraries:
[pip] Could not collect
[conda] blas                      1.0                         mkl 
[conda] cuda100                   1.0                           0    pytorch
[conda] mkl                       2018.0.3                      1 
[conda] mkl_fft                   1.0.6            py27h7dd41cf_0 
[conda] mkl_random                1.0.1            py27h4414c95_1 
[conda] pytorch                   1.0.0           py2.7_cuda10.0.130_cudnn7.4.1_1  [cuda100]  pytorch
[conda] torchvision               0.2.1                      py_2    pytorch

## Additional context

 complete list of packages
packages in environment:
 
Name                    Version                   Build  Channel
blas                      1.0                         mkl 
ca-certificates           2018.03.07                    0 
certifi                   2018.10.15               py27_0 
cffi                      1.11.5           py27he75722e_1 
cuda100                   1.0                           0    pytorch
freetype                  2.9.1                h8a8886c_1 
intel-openmp              2019.1                      144 
jpeg                      9b                   h024ee3a_2 
libedit                   3.1.20170329         h6b74fdf_2 
libffi                    3.2.1                hd88cf55_4 
libgcc-ng                 8.2.0                hdf63c60_1 
libgfortran-ng            7.3.0                hdf63c60_0 
libpng                    1.6.35               hbc83047_0 
libstdcxx-ng              8.2.0                hdf63c60_1 
libtiff                   4.0.9                he85c1e1_2 
mkl                       2018.0.3                      1 
mkl_fft                   1.0.6            py27h7dd41cf_0 
mkl_random                1.0.1            py27h4414c95_1 
ncurses                   6.1                  he6710b0_1 
ninja                     1.8.2            py27h6bb024c_1 
numpy                     1.15.4           py27h1d66e8a_0 
numpy-base                1.15.4           py27h81de0dd_0 
olefile                   0.46                     py27_0 
onnx                      1.3.0                     <pip>
openssl                   1.1.1a               h7b6447c_0 
pillow                    5.3.0            py27h34e0f95_0 
pip                       18.1                     py27_0 
protobuf                  3.6.1                     <pip>
pycparser                 2.19                     py27_0 
python                    2.7.15               h9bab390_4 
pytorch                   1.0.0           py2.7_cuda10.0.130_cudnn7.4.1_1  [cuda100]  pytorch
readline                  7.0                  h7b6447c_5 
setuptools                40.6.2                   py27_0 
six                       1.11.0                   py27_1 
sqlite                    3.25.3               h7b6447c_0 
tk                        8.6.8                hbc83047_0  
torchvision               0.2.1                      py_2    pytorch
typing                    3.6.6                     <pip>
typing-extensions         3.6.6                     <pip>
wheel                     0.32.3                   py27_0 
xz                        5.2.4                h14c3975_4 
zlib                      1.2.11               h7b6447c_3 

(opening this problem on behalf of a team member: @surajkota)
",module: onnx,"['@surajkota can provide further details on this problem.', 'This is no longer an issue. We updated LD_LIBRARY_PATH to use the MKLDNN library from PyTorch and it helped solve the problem.']",[],[],0,0
517,pytorch,8388,closed,Running simultaneous DataParallels can potentially result in locked models,"I observed that running simultaneous DataParallels might result in at least one of the models being unable to progress at all.



System configuration:
- PyTorch 0.4.0 stable release, installed using 
- EC2 P2.8xlarge (8 x K80s)
- CUDA 8
- Python 3.6



On a separate machine with 8 x Titan X, the following happened after I tried running multiple parallel models:


There's RAM that's somehow unaccounted for.",,"['This is expected, and is caused by a NCCL library (which we use for multiGPU communication). Unfortunately there are no workarounds, but in general running multiple networks on the same GPUs is not a good idea.', ""To elaborate on Adam's answer. At the NVIDIA driver level, deadlocks are expected with NCCL -- there are no known workarounds, and no expected solutions. Sharing GPUs for multiple workloads in the distributed setting is not ideal for this reason.\r\n\r\nOne alternative for you to consider is to move to DistributedDataParallel (even for a single machine) and use the `gloo` backend: https://pytorch.org/docs/stable/distributed.html#launch-utility"", 'Ah okay, thanks.']","['\r\nubuntu@ip-XXX:~/vrex2$ ./run.sh\r\nLoading data file...\r\nLoaded!\r\n^CTraceback (most recent call last):\r\n  File ""/home/ubuntu/anaconda3/lib/python3.6/runpy.py"", line 193, in _run_module_as_main\r\n    ""__main__"", mod_spec)\r\n  File ""/home/ubuntu/anaconda3/lib/python3.6/runpy.py"", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File ""/home/ubuntu/vrex2/utils/train.py"", line 121, in <module>\r\n    main()\r\n  File ""/home/ubuntu/vrex2/utils/train.py"", line 116, in main\r\n    train(config)\r\n  File ""/home/ubuntu/vrex2/utils/train.py"", line 69, in train\r\n    scores = model(model_in)\r\n  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 491, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py"", line 114, in forward\r\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\r\n  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py"", line 124, in parallel_apply\r\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\r\n  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py"", line 57, in parallel_apply\r\n    thread.join()\r\n  File ""/home/ubuntu/anaconda3/lib/python3.6/threading.py"", line 1056, in join\r\n    self._wait_for_tstate_lock()\r\n  File ""/home/ubuntu/anaconda3/lib/python3.6/threading.py"", line 1072, in _wait_for_tstate_lock\r\n    elif lock.acquire(block, timeout):\r\nKeyboardInterrupt\r\n^CException ignored in: <module \'threading\' from \'/home/ubuntu/anaconda3/lib/python3.6/threading.py\'>\r\nTraceback (most recent call last):\r\n  File ""/home/ubuntu/anaconda3/lib/python3.6/threading.py"", line 1294, in _shutdown\r\n    t.join()\r\n  File ""/home/ubuntu/anaconda3/lib/python3.6/threading.py"", line 1056, in join\r\n    self._wait_for_tstate_lock()\r\n  File ""/home/ubuntu/anaconda3/lib/python3.6/threading.py"", line 1072, in _wait_for_tstate_lock\r\n    elif lock.acquire(block, timeout):\r\nKeyboardInterrupt\r\n^C^C^C^C^C^C\r\n', '\r\nubuntu@ip-96-115-215-139:~/vrex2$ nvidia-smi\r\nTue Jun 12 18:35:36 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.111                Driver Version: 384.111                   |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           On   | 00000000:00:17.0 Off |                    0 |\r\n| N/A   83C    P0   127W / 149W |   1998MiB / 11439MiB |     67%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla K80           On   | 00000000:00:18.0 Off |                    0 |\r\n| N/A   67C    P0   130W / 149W |   1878MiB / 11439MiB |     61%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Tesla K80           On   | 00000000:00:19.0 Off |                    0 |\r\n| N/A   77C    P0   124W / 149W |   1878MiB / 11439MiB |     61%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  Tesla K80           On   | 00000000:00:1A.0 Off |                    0 |\r\n| N/A   63C    P0   128W / 149W |   1878MiB / 11439MiB |     62%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   4  Tesla K80           On   | 00000000:00:1B.0 Off |                    0 |\r\n| N/A   79C    P0   123W / 149W |   1878MiB / 11439MiB |     60%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   5  Tesla K80           On   | 00000000:00:1C.0 Off |                    0 |\r\n| N/A   68C    P0   127W / 149W |   1878MiB / 11439MiB |     59%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   6  Tesla K80           On   | 00000000:00:1D.0 Off |                    0 |\r\n| N/A   75C    P0   112W / 149W |   1878MiB / 11439MiB |     60%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   7  Tesla K80           On   | 00000000:00:1E.0 Off |                    0 |\r\n| N/A   62C    P0   135W / 149W |   1878MiB / 11439MiB |     61%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0     76810      C   python                                      1985MiB |\r\n|    1     76810      C   python                                      1865MiB |\r\n|    2     76810      C   python                                      1865MiB |\r\n|    3     76810      C   python                                      1865MiB |\r\n|    4     76810      C   python                                      1865MiB |\r\n|    5     76810      C   python                                      1865MiB |\r\n|    6     76810      C   python                                      1865MiB |\r\n|    7     76810      C   python                                      1865MiB |\r\n+-----------------------------------------------------------------------------+\r\n', '\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX TIT...  Off  | 0000:04:00.0     Off |                  N/A |\r\n| 22%   50C    P2    84W / 250W |   2550MiB / 12207MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX TIT...  Off  | 0000:05:00.0     Off |                  N/A |\r\n| 22%   41C    P2    67W / 250W |   2394MiB / 12207MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce GTX TIT...  Off  | 0000:08:00.0     Off |                  N/A |\r\n| 22%   40C    P2    67W / 250W |   2276MiB / 12207MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce GTX TIT...  Off  | 0000:09:00.0     Off |                  N/A |\r\n| 22%   42C    P2    67W / 250W |   2157MiB / 12207MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   4  GeForce GTX TIT...  Off  | 0000:85:00.0     Off |                  N/A |\r\n| 22%   24C    P8    15W / 250W |   1973MiB / 12207MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   5  GeForce GTX TIT...  Off  | 0000:86:00.0     Off |                  N/A |\r\n| 22%   23C    P8    15W / 250W |   1975MiB / 12207MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   6  GeForce GTX TIT...  Off  | 0000:89:00.0     Off |                  N/A |\r\n| 22%   23C    P8    15W / 250W |   1973MiB / 12207MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   7  GeForce GTX TIT...  Off  | 0000:8A:00.0     Off |                  N/A |\r\n| 22%   24C    P8    15W / 250W |   1973MiB / 12207MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0     20159    C   python                                         400MiB |\r\n|    1     31020    C   python                                         419MiB |\r\n|    2     31020    C   python                                         301MiB |\r\n|    3     31020    C   python                                         182MiB |\r\n+-----------------------------------------------------------------------------+\r\n']",['conda install -c pytorch pytorch torchvision'],0,0
518,pytorch,17315,closed,Autograd inside hooks is disabled by default?,"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->

I was playing around  with the   and ""intermediate gradients"". And discovered strange behaviour (as for me).  Looks like by default  is disabled inside hooks (using  to extract intermediate gradients). But why? To avoid a kind of ""backward inside backward""?

## To Reproduce

### Plain hook works fine.


Output:


### Autograd inside hook fails.


Output:

Ok, it fails because  there is no  for . Looks like we need to turn on .

### Autograd inside hook with  seems to work fine.


Output:

Now it woks fine. In addition, outside of hooks with gradients it works as usual.

### Autograd outside of hooks works as expected.


Output:

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behaviour
I expected   to work as usual inside hooks ().
I am not sure is it a bug or smth. I'm just worrying, why does it behave in such way?
<!-- A clear and concise description of what you expected to happen. -->

## Environment
",,"['autograd in general is disabled on the backward pass by default.\r\n\r\nIf you do want to have it enabled, use `torch.autograd.grad(..., create_graph=True)`. See: https://pytorch.org/docs/stable/autograd.html?highlight=grad#torch.autograd.grad', 'Now I see. Thank you!']","[""python\r\nimport torch\r\n\r\n\r\ndef extract(V):\r\n    def hook(grad):\r\n        V.grad = grad\r\n    return hook\r\n\r\nX = torch.ones(5, requires_grad=True)\r\nV = X.pow(3)\r\nY = V.sum()\r\n\r\nV.register_hook(extract(V))\r\nY.backward()\r\n\r\nprint('X.grad:', X.grad)\r\nprint('V.grad:', V.grad)\r\n"", '\r\nX.grad: tensor([3., 3., 3., 3., 3.])\r\nV.grad: tensor([1., 1., 1., 1., 1.])\r\n', ""python\r\nimport torch\r\n\r\n\r\ndef extract_and_grad(V):\r\n    def hook(grad):\r\n        V.grad = grad\r\n        V.grad.requires_grad_(True)\r\n        print('V.grad:', V.grad)\r\n        U = V.grad.pow(2).sum()\r\n        print('U:', U)\r\n        try:\r\n            dU_dV_grad = torch.autograd.grad(U, V.grad)\r\n            print('V.grad.grad:', dU_dV_grad)\r\n        except RuntimeError as err:\r\n            print(err)\r\n    return hook\r\n\r\nX = torch.ones(5, requires_grad=True)\r\nV = X.pow(3)\r\nY = V.sum()\r\n\r\nV.register_hook(extract_and_grad(V))\r\nY.backward()\r\n"", '\r\nV.grad: tensor([1., 1., 1., 1., 1.], requires_grad=True)\r\nU: tensor(5.)\r\nelement 0 of tensors does not require grad and does not have a grad_fn\r\n', ""python\r\nimport torch\r\n\r\n\r\ndef extract_and_grad_v2(V):\r\n    def hook(grad):\r\n        V.grad = grad\r\n        V.grad.requires_grad_(True)\r\n        print('V.grad:', V.grad)\r\n        \r\n        with torch.enable_grad():\r\n            U = V.grad.pow(2).sum()\r\n            print('U:', U)\r\n        try:\r\n            dU_dV_grad, = torch.autograd.grad(U, V.grad)\r\n            print('V.grad.grad:', dU_dV_grad)\r\n        except RuntimeError as err:\r\n            print(err)\r\n    return hook\r\n\r\nX = torch.ones(5, requires_grad=True)\r\nV = X.pow(3)\r\nY = V.sum()\r\n\r\nV.register_hook(extract_and_grad_v2(V))\r\nY.backward()\r\n"", '\r\nV.grad: tensor([1., 1., 1., 1., 1.], requires_grad=True)\r\nU: tensor(5., grad_fn=<SumBackward0>)\r\nV.grad.grad: tensor([2., 2., 2., 2., 2.])\r\n', ""python\r\nimport torch\r\n\r\n\r\ndef extract(V):\r\n    def hook(grad):\r\n        V.grad = grad\r\n    return hook\r\n\r\nX = torch.ones(5, requires_grad=True)\r\nV = X.pow(3)\r\nY = V.sum()\r\n\r\nV.register_hook(extract(V))\r\nY.backward()\r\n\r\nV.grad.requires_grad_(True)\r\nprint('X.grad:', X.grad)\r\nprint('V.grad:', V.grad)\r\nU = V.grad.pow(2).sum()\r\nprint('U:', U)\r\ntry:\r\n    dU_dV_grad, = torch.autograd.grad(U, V.grad)\r\n    print('V.grad.grad:', dU_dV_grad)\r\nexcept RuntimeError as err:\r\n    print(err)\r\n"", '\r\nX.grad: tensor([3., 3., 3., 3., 3.])\r\nV.grad: tensor([1., 1., 1., 1., 1.], requires_grad=True)\r\nU: tensor(5., grad_fn=<SumBackward0>)\r\nV.grad.grad: tensor([2., 2., 2., 2., 2.])\r\n', '\r\nPyTorch version: 1.0.1.post2\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Linux Mint 19.1 Tessa\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: GeForce GTX 970\r\nNvidia driver version: 410.78\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.2\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.15.4\r\n[pip] torch==1.0.1.post2\r\n[pip] torchvision==0.2.1\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.1                      144  \r\n[conda] mkl_fft                   1.0.10           py36ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py36hd81dba3_0  \r\n[conda] pytorch                   1.0.1           py3.6_cuda10.0.130_cudnn7.4.2_2    pytorch\r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n']","['autograd', 'autograd', 'register_hook', 'grad_fn', 'U', 'autograd', 'torch.enable_grad()', 'autograd', 'registered_hook']",0,0
519,pytorch,371,closed,"nn.Embedding CUDA ""too many resources requested for launch""","Looks like nn.Embedding backwards doesn't work when the batch size is >1024. I think it switches sorting algorithms to something that runs out of registers.

Here's a repro:


Running it on K40:


cc @wickedfoo",medium priority (this tag is deprecated) module: dependency bug,"['cannot repro with master and K40 with CUDA 7.5 and NVIDIA driver: 367.48.\r\nWill try CUDA 8.0 now', 'reproduced the issue with CUDA 8.0, i think I know what the fix is. Working on it.', 'Thanks @soumith !']","['\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\nlut = nn.Embedding(10000, 10).cuda()\r\nout = lut(Variable(torch.range(1,1024).long().cuda())).sum()\r\nout.backward()\r\nprint(""1024 worked"")\r\nout = lut(Variable(torch.range(1,1025).long().cuda())).sum()\r\nout.backward()\r\nprint(""1025 worked"")\r\n', '\r\n1024 worked\r\nTHCudaCheck FAIL file=/home/alerer/local/pytorch/pytorch/torch/lib/THC/generated/../generic/THCTensorSort.cu line=153 error=7 : too many resources requested for launch\r\nTraceback (most recent call last):\r\n  File ""memtest.py"", line 10, in <module>\r\n    out.backward()\r\n  File ""/data/users/alerer/pytorch/pytorch/torch/autograd/variable.py"", line 99, in backward\r\n    self._execution_engine.run_backward((self,), (gradient,), retain_variables)\r\n  File ""/data/users/alerer/pytorch/pytorch/torch/nn/functions/thnn/sparse.py"", line 86, in backward\r\n    1\r\nRuntimeError: cuda runtime error (7) : too many resources requested for launch at /home/alerer/local/pytorch/pytorch/torch/lib/THC/generated/../generic/THCTensorSort.cu:153\r\n']",[],0,0
520,pytorch,8522,closed,[JIT] Unsupported op descriptor scatter,"Don't support scatter in dispatch. Example failure:



Affected tests:
- 
- 
- 
- 
- 
- 
",,[],"['\r\n======================================================================\r\nERROR: test_scatter_dim0 (__main__.TestAutogradGenerated)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""test/test_autograd.py"", line 3665, in do_test\r\n    check(name)\r\n  File ""test/test_autograd.py"", line 3587, in check\r\n    check_against_reference(self, create_traced_fn(fn), fn, (self_variable,) + args_variable)\r\n  File ""test/test_autograd.py"", line 3498, in check_against_reference\r\n    outputs_test = func(*nograd_inputs)\r\n  File ""test/test_autograd.py"", line 3446, in traced_fn\r\n    return traced(*inputs_tensors)\r\nRuntimeError: Unsupported op descriptor: scatter-3-dim_i. File a bug report.\r\n']","['test_scatter_dim0', 'test_scatter_dim0_neg0', 'test_scatter_dim1', 'test_scatter_dim1_neg0', 'test_scatter_scalar_all_dim0', 'test_scatter_scalar_all_dim0_neg0']",0,0
521,pytorch,28653,closed,QAT with CUDA,"Try post train with my own network and dataset but accuracy is not good enough. 
Try QAT with too slow. 
Any way to quickly run QAT on NVIDIA GPU?

cc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a",oncall: quantization triaged,"['QAT supports CUDA through [`FakeQuantize`](https://pytorch.org/docs/stable/quantization.html#torch.quantization.FakeQuantize). You can follow the [tutorial](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#quantization-aware-training), but you will need to adapt it to use CUDA.', '@dtcrong Closing this. If you have any questions about quantization, you can direct it to the [Forum](https://discuss.pytorch.org/c/quantization)']",[],[],0,0
522,pytorch,23222,closed,Maybe there is a problem when using random.shuffle(),"## üêõ Bug

When I applied the random.shuffle to a 1-D tensor, the num in the tensor was changed.

## To Reproduce

Steps to reproduce the behavior:
1.Do as follows:
->a = torch.arange(10)
->random.shuffle(a)

## Expected behavior
Expected: 
  tensor([1, 8, 5, 3, 9, 0, 2, 6, 4, 7])
but I got:
  tensor([0, 1, 2, 1, 4, 5, 1, 5, 7, 4])

I am not sure whether it's a bug.

## Environment

Python 3.7.0
torch 1.1.0
",module: advanced indexing,"['This is pertinent to this issue https://github.com/pytorch/pytorch/issues/17008.\r\n\r\nThe implementation of random.shuffle performs pythonic element swapping, which causes the issue.', '@vishwakftw Thanks.', 'closing in favor of #17008']",[],[],0,0
523,pytorch,10807,closed,[caffe2] libcaffe2.soÔºöonnx_c2::GetEmptyStringAlreadyInited[abi:cxx11]  undefined reference,"I build caffe2 with origin caffe2 + ubuntu16.04 + anaconda3, the cmake summary is : 

-- ******** Summary ********
-- General:
--   CMake version         : 3.6.3
--   CMake command         : /home/hengshan/.conda/envs/caffe2_py2/bin/cmake
--   Git version           : v0.8.1-1502-g3c9081d-dirty
--   System                : Linux
--   C++ compiler          : /usr/bin/c++
--   C++ compiler version  : 5.4.0
--   BLAS                  : Eigen
--   CXX flags             :  -fvisibility-inlines-hidden -DONNX_NAMESPACE=onnx_c2 -O2 -fPIC -Wno-narrowing -Wno-invalid-partial-specialization
--   Build type            : Release
--   Compile definitions   : 
--   BUILD_BINARY          : ON
--   BUILD_CUSTOM_PROTOBUF : OFF
--     Protobuf compiler   : 
--     Protobuf includes   : 
--     Protobuf libraries  : 
--   BUILD_DOCS            : OFF
--   BUILD_PYTHON          : ON
--     Python version      : 2.7.13
--     Python includes     : /home/hengshan/.conda/envs/caffe2_py2/include/python2.7
--   BUILD_SHARED_LIBS     : ON
--   BUILD_TEST            : ON
--   USE_ATEN              : OFF
--   USE_ASAN              : OFF
--   USE_CUDA              : ON
--     CUDA version        : 8.0
--     CuDNN version       : 6.0.21
--     CUDA root directory : /usr/local/cuda-8.0
--     CUDA library        : /usr/lib/x86_64-linux-gnu/libcuda.so
--     CUDA NVRTC library  : /usr/local/lib/libnvrtc.so
--     CUDA runtime library: /usr/local/cuda-8.0/lib64/libcudart.so
--     CUDA include path   : /usr/local/cuda-8.0/include
--     NVCC executable     : /usr/local/cuda-8.0/bin/nvcc
--     CUDA host compiler  : /usr/bin/cc
--   USE_EIGEN_FOR_BLAS    : 1
--   USE_FFMPEG            : ON
--   USE_GFLAGS            : ON
--   USE_GLOG              : ON
--   USE_GLOO              : ON
--   USE_LEVELDB           : ON
--     LevelDB version     : 1.18
--     Snappy version      : 1.1.3
--   USE_LITE_PROTO        : OFF
--   USE_LMDB              : ON
--     LMDB version        : 0.9.17
--   USE_METAL             : OFF
--   USE_MKL               : 
--   USE_MOBILE_OPENGL     : OFF
--   USE_MPI               : ON
--   USE_NCCL              : ON
--   USE_NERVANA_GPU       : OFF
--   USE_NNPACK            : ON
--   USE_OBSERVERS         : ON
--   USE_OPENCV            : ON
--     OpenCV version      : 3.3.0
--   USE_OPENMP            : OFF
--   USE_PROF              : OFF
--   USE_REDIS             : OFF
--   USE_ROCKSDB           : OFF
--   USE_ZMQ               : OFF
-- Configuring done
-- Generating done

the error is : 
../lib/libcaffe2.soÔºöÂØπ‚Äòonnx_c2::GetEmptyStringAlreadyInited[abi:cxx11]()‚ÄôÊú™ÂÆö‰πâÁöÑÂºïÁî®
collect2: error: ld returned 1 exit status
caffe2/CMakeFiles/logging_test.dir/build.make:117: recipe for target 'bin/logging_test' failed
make[2]: *** [bin/logging_test] Error 1
CMakeFiles/Makefile2:1346: recipe for target 'caffe2/CMakeFiles/logging_test.dir/all' failed
make[1]: *** [caffe2/CMakeFiles/logging_test.dir/all] Error 2
../lib/libcaffe2.soÔºöÂØπ‚Äòonnx_c2::GetEmptyStringAlreadyInited[abi:cxx11]()‚ÄôÊú™ÂÆö‰πâÁöÑÂºïÁî®
collect2: error: ld returned 1 exit status

",caffe2,"['I think we have fixed this issue, please checkout our master and try again, thanks.']",[],[],0,0
524,pytorch,19697,closed,torch.transpose doesn't shares it‚Äôs underlying storage with the original tensor,"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->
This concerns the method:
torch.transpose(input, dim0, dim1) ‚Üí Tensor

According to the documentation, https://pytorch.org/docs/stable/torch.html :
> The resulting out tensor shares it‚Äôs underlying storage with the input tensor, so changing the content of one would change the content of the other.

However, transposed versions of the original tensor are not changed when the original tensor's data is changed via ""set_"" method (specifically).

## To Reproduce


>  t is: 
> tensor([[-1.,  2.]])
>  ttr is: 
> tensor([[-1.],
>         [ 2.]])
> --------------------
>  t is: 
> tensor([[5., 2.]])
>  ttr is: 
> tensor([[5.],
>         [2.]])
> The transposed versions were updated, as expected
> --------------------
>  t is: 
> tensor([[10., 20.]])
>  ttr is: 
> tensor([[10.],
>         [20.]])
> The transposed versions were updated, as expected
> --------------------
>  t is: 
> tensor([[12., 22.]])
>  ttr is: 
> tensor([[12.],
>         [22.]])
> The transposed versions were updated, as expected
> --------------------
>  t is: 
> tensor([[-1., -2.]])
>  ttr is: 
> tensor([[12.],
>         [22.]])

## Expected behavior
I expect that ""changing the content of one (via set_) would change the content of the other."", as per the documentation.

## Environment

PyTorch version: 1.0.1.post2
Is debug build: No
CUDA used to build PyTorch: None

OS: Mac OSX 10.14.3
GCC version: Could not collect
CMake version: version 3.13.4

Python version: 3.7
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip3] numpy==1.14.5
[pip3] torch==1.0.1.post2
[pip3] torchvision==0.2.2.post3
[conda] Could not collect


## Additional context

<!-- Add any other context about the problem here. -->
",,"[""I'd say this is expected. `set_` does not modify the storage content of a tensor, but changes the storage object altogether.... The old storage is shared, but not the new one."", 'This is not what I understand from this documentation: https://pytorch.org/docs/stable/tensors.html\r\n\r\n> Methods which mutate a tensor are marked with an underscore suffix. For example, torch.FloatTensor.abs_() computes the absolute value in-place and returns the modified tensor, while torch.FloatTensor.abs() computes the result in a new tensor.', ""@BruceK4t1qbit \r\n\r\n> Methods which mutate a tensor are marked with an underscore suffix.\r\n\r\nYes, `set_` mutates the Tensor -- by changing it's storage in-place. That's why it has a postfix `_`.\r\n\r\nI think what you are looking for is `copy_` instead of `set_`"", '@soumith thanks']","['\r\nimport torch\r\nfrom torch import Tensor\r\nt = Tensor([[-1,2]])\r\nttr = t.transpose(0,1)\r\nprint(f"" t is: \\n{t}"")\r\nprint(f"" ttr is: \\n{ttr}"")\r\nprint(""-""*20)\r\n\r\nt[0,0] = 5\r\nprint(f"" t is: \\n{t}"")\r\nprint(f"" ttr is: \\n{ttr}"")\r\nprint(f""The transposed version was updated, as expected"")\r\nprint(""-""*20)\r\n\r\nt[:,:] = Tensor([[10,20]])\r\nprint(f"" t is: \\n{t}"")\r\nprint(f"" ttr is: \\n{ttr}"")\r\nprint(f""The transposed version was updated, as expected"")\r\nprint(""-""*20)\r\n\r\nt.add_(Tensor([[2,2]]))\r\nprint(f"" t is: \\n{t}"")\r\nprint(f"" ttr is: \\n{ttr}"")\r\nprint(f""The transposed version was updated, as expected"")\r\nprint(""-""*20)\r\n\r\nt.set_(Tensor([[-1,-2]]))\r\nprint(f"" t is: \\n{t}"")\r\nprint(f"" ttr is: \\n{ttr}"")\r\nprint(f""Note that the transposed version was not updated!"")\r\n']",[],0,0
525,pytorch,15557,closed,"main.exe ,which was made  from main.py by pyinstaller , faced on error  ","## ‚ùì Questions and Help
I made main.exe ,which is made from main.py by the pyinstaller.
But I did main.exe,then it faced  ""main.exe: error: unrecognized arguments: --multiprocessing-fork
parent_pid=7056 pipe_handle=1188"" erroe and stopped.

How do I change main.py code to operate main.exe without this error?

I'd really appreciate it if anyone will give me the advice as soon as possile

„ÉªCUDA9, Python3.6 ,Pytorch Windows for CUDA9,
„ÉªOS„ÄÄWindows10 Pro 64bit
„ÉªNvidia GTX980
        
",,"[""Please read this [post](https://github.com/pyinstaller/pyinstaller/wiki/Recipe-Multiprocessing). The issue should actually be sent to pyinstaller. Because it isn't a pytorch problem."", ""Thank you for your advice ,peterjc123.\r\nI understood this error isn't pytorch problem , it is pyinstaller problem.""]",[],[],0,0
526,pytorch,28639,closed,Set CUDA device is expensive operation,"## üêõ Bug

Calling cudart::cudaApiSetDevice appears to be a very expensive operation. However, as my system has only a single GPU, it is unnecessary to call it at all.

Tested on Windows.

![image](https://user-images.githubusercontent.com/61218/67546817-99188780-f740-11e9-9566-792c5eab40e4.png)


## To Reproduce

Steps to reproduce the behavior:

1. Run models with benchmarking turned on
2. Profile the application

## Expected behavior

Set device should not even be called on a single GPU device because device 0 will always be used anyway.

## Environment

 - PyTorch Version (e.g., 1.0): 1.3.0
 - OS (e.g., Linux): Windows x64 MSVC 2019
 - How you installed PyTorch (, , source): prebuilt off website
 - Build command you used (if compiling from source): N/A
 - Python version: N/A
 - CUDA/cuDNN version: 10.1/7.6
 - GPU models and configuration: GTX 2060
 - Any other relevant information:


cc @VitalyFedyunin @ngimel @mruberry",module: cuda module: performance triaged,"[""This has always been shown to be a profiling artifact, see e.g. https://github.com/pytorch/pytorch/issues/18048.\r\n\r\nWhat I think we'd need to see is actual, runnable benchmarking that demonstrates it's a real issue (you can see how to make the calls no-ops in the linked issues). "", 'Thanks,\r\nI believe it is not a profiling artifact though. It appears to sync at this stage which is why the profiler shows high CPU at this point. If you are running a single model you then see no difference in performance because it would have had to sync anyway.\r\nIn real life however, I run many models at the same time and this syncing does slow down performance.\r\nI removed the call in my own code and now have better utilisation.\r\n\r\nIt may be hard to produce an example.']",[],"['conda', 'pip']",0,0
527,pytorch,8594,closed,PyTorch 0.4 hangs with nn.DataParallel ,"I have an issue that matches [https://github.com/pytorch/pytorch/issues/7019](url) but the suggested solution does not work for me.

This code never returns a value to y:

    import torch
    import torch.nn as nn
    from torch.autograd import Variable

    class NET(nn.Module):
        def __init__(self):
            super(NET, self).__init__()
            self.dense = nn.Linear(256, 512)

        def forward(self, input):
            return self.dense(input)

    if __name__ == '__main__':
        model = NET()
        model = nn.DataParallel(model).cuda()
        x = Variable(torch.rand(128, 256))
        y = model(x) ##### <<<<--- GETS STUCK HERE FOREVER

the solution was to delete  NCCL_SHM_DISABLE=1 and NCCL_P2P_DISABLE=1 from /etc/nccl.conf

I do not have a /etc/nccl.conf in Ubuntu 18.04 but I did unset those environment variables and I get this:

NCCL version 2.1.15+cuda9.0
rig:2637:2637 [0] INFO NET : Using interface enp0s31f6:192.168.85.32<0>
rig:2637:2637 [0] INFO NET/Socket : 1 interfaces found
rig:2637:2637 [3] INFO Using 256 threads
rig:2637:2637 [3] INFO Min Comp Cap 6
rig:2637:2637 [3] INFO NCCL_SINGLE_RING_THRESHOLD=131072
rig:2637:2637 [3] INFO Ring 00 : 0 1 2 3
rig:2637:2637 [0] INFO Ring 00 : 0[0] -> 1[1] via P2P/direct pointer
rig:2637:2637 [1] INFO Ring 00 : 1[1] -> 2[2] via P2P/direct pointer
rig:2637:2637 [2] INFO Ring 00 : 2[2] -> 3[3] via P2P/direct pointer
rig:2637:2637 [3] INFO Ring 00 : 3[3] -> 0[0] via P2P/direct pointer
rig:2637:2637 [0] INFO Launch mode Group/CGMD
^C gives:
File ""/home/minimumnz/anaconda3/envs/tacotron/lib/python3.6/threading.py"", line 1072, in _wait_for_tstate_lock
elif lock.acquire(block, timeout):

it seems stuck in some thread.

    Collecting environment information...
    PyTorch version: 0.4.0
    Is debug build: No 
    CUDA used to build PyTorch: 9.0.176

    OS: Ubuntu 18.04 LTS 
    GCC version: (Ubuntu 7.3.0-16ubuntu3) 7.3.0 
    CMake version: Could not collect

    Python version: 3.6
    Is CUDA available: Yes 
    CUDA runtime version: Could not collect
    GPU models and configuration:
    GPU 0: GeForce GTX 1080 Ti
    GPU 1: GeForce GTX 1080 Ti
    GPU 2: GeForce GTX 1080 Ti
    GPU 3: GeForce GTX 1080 Ti

    Nvidia driver version: 390.67
    cuDNN version: Probably one of the following:
    /usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.3
    /usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a

    Versions of relevant libraries:
    [pip] numpy (1.14.5)
    [pip] torch (0.4.0)
    [pip] torchvision (0.2.1)
    [conda] cuda90 1.0 h6433d27_0 pytorch
    [conda] pytorch 0.4.0 py36_cuda9.0.176_cudnn7.1.2_1 [cuda90] pytorch
    [conda] torch 0.4.0 
    [conda] torchvision 0.2.1 py36_1 pytorch",awaiting response (this tag is deprecated),"['Hi @minimumnz I was not able to repro this issue on my local build. Would you mind try git clone the master branch and build from source there?\r\nIf it persists, could you provide a docker image for us to work on a fix? Thanks! ', '@minimumnz Do you have any updates on this? Thanks!', 'I have tried this multiple times and I think this is not an issue any more. Closing for now due to no followup response. Feel free to reopen if it still happens @minimumnz ', 'I am facing the same issue (running the same example code).\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 396.26                 Driver Version: 396.26                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           On   | 00000000:86:00.0 Off |                    0 |\r\n| N/A   37C    P0    76W / 149W |    352MiB / 11441MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla K80           On   | 00000000:87:00.0 Off |                    0 |\r\n| N/A   45C    P0    86W / 149W |    350MiB / 11441MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1863      C   /home/USER/anaconda3/bin/python             341MiB |\r\n|    1      1863      C   /home/USER/anaconda3/bin/python             339MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\nI also have no `/etc/nccl.conf` on my machine.\r\n\r\n> ```\r\n> cuda90                    1.0                  h6433d27_0    pytorch\r\n> pytorch                   0.4.0           py36_cuda9.0.176_cudnn7.1.2_1  [cuda90]  pytorch\r\n> torchvision               0.2.1                    py36_1    pytorch\r\n> ```']",[],[],0,0
528,pytorch,31419,closed,[android] initHybrid missing/broken in pytorch 1.4.0 nightly,"## üêõ Bug

Trying to load a tensor from a blob leads to an error:
    java.lang.UnsatisfiedLinkError: No implementation found for com.facebook.jni.HybridData org.pytorch.Tensor.initHybrid() (tried Java_org_pytorch_Tensor_initHybrid and Java_org_pytorch_Tensor_initHybrid__)
        at org.pytorch.Tensor.initHybrid(Native Method)
        at org.pytorch.Tensor.initHybrid(Tensor.java:337)
        at org.pytorch.Tensor.fromBlob(Tensor.java:289)

This is a very recently introduced bug, as the nightly snapshot last week worked fine.

## To Reproduce

Steps to reproduce the behavior:
1. Create Android Project and add pytorch 1.4.0 nighly snapshot support (e.g. as shown here https://github.com/pytorch/pytorch/issues/29806) 
2. add following lines
        val inputHeight = 299
        val inputWidth = 299
        val inputTensorBuffer = Tensor.allocateFloatBuffer(3 * inputWidth * inputHeight)
        val inputTensor = Tensor.fromBlob( inputTensorBuffer, longArrayOf( 1, 3, inputHeight.toLong(), inputWidth.toLong()))
3. build & run app
4. App crashes with error above

## Expected behavior

Loading a tensor into memory without error (worked with nightly build last week)

## Environment

 - PyTorch Version (e.g., 1.0): 1.4.0 nightly snapshot (18 Dec 2019)
 - OS (e.g., Linux): Windows 10
 - Android studio 3.5.3, OpenJDK JRE 1.8.0_202
 - CUDA/cuDNN version: None (CPU-only)
 - GPU models and configuration: None 

## Additional context

Using a stable version is not an option, as 1.3.0 has a breaking bug in loading the inception v3 model (https://github.com/pytorch/pytorch/issues/29806) ",oncall: mobile,"['Complete repo to reproduce the issue: https://github.com/phillies/pytorch_debug_android', '@phillies \r\nThanks for reporting it.\r\nThe problem happens as native libraries at the moment are loaded only during `org.pytorch.Module.load`, as a hotfix to unblock you while we are working on the fix - if you move `Tensor.fromBlob` after `Module.load` that error should not happen\r\n\r\nI am working on the fix.', '@phillies ,\r\nThe fix was merged into master as https://github.com/pytorch/pytorch/commit/3a19980b78b6638235edef367784ecc3dc37e364\r\nI have republished android nightlies, the error should not happen if you update to [the nightlies with version >= 78](https://oss.sonatype.org/service/local/repositories/snapshots/content/org/pytorch/pytorch_android/1.4.0-SNAPSHOT/pytorch_android-1.4.0-20191220.225919-78.aar) (gradle key `--refresh-dependencies`) \r\n\r\n', 'Thanks @IvanKobzarev this error is fixed, however, now I get a segfault when I call Module.load().\r\n`A/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0 in tid 9362 (es.pytorchdebug), pid 9362 (es.pytorchdebug)`\r\nI guess this is not related to this issue?']",[],[],0,0
529,pytorch,19104,closed,Is there 0.5.0version of pytorch?,"when I tried to use Dataparallel, I came across the problem"" Arguments are located on different GPUs"",
I found that I should use the 0.5.0 version to solve the problem on the forum.
However ,I have been trying to find the correct version for a long time with no result.Can anyone help me ?
(ps, I tried to run the code on pytorch1.0, but error came in the dataloader part ‚ÄúRandomSampler‚Äô object has no attribute 'replacement‚Äù.and I found no solution.
Thank you !!",,['there is no 0.5.0.\r\n1.0 is the 0.5.0'],[],[],0,0
530,pytorch,17531,closed,JIT graph to ONNX Loop cond variable should be tensor(bool),"## üêõ Bug

ONNX Loop produced by  and  does not conform to ONNX loop spec: The second input to ONNX loop--the conditional variable --should be , but I got . 

## To Reproduce

Run the following script (environment details below):



produces the following ONNX program:



 is tensor of type  (see the program print out). However,  is the second input to , which is required to be  by the [ONNX spec](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Loop). 

## Environment
PyTorch version: 1.0.1.post2
Is debug build: No
CUDA used to build PyTorch: None

OS: Mac OSX 10.14.1
GCC version: Could not collect
CMake version: version 3.13.4

Python version: 3.7
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip3] numpy==1.16.1
[pip3] torch==1.0.1.post2
[conda] Could not collect

",module: onnx oncall: jit,"['This seems to be a problem in ONNX conversion itself, script is emitting the correct type for the condition:\r\n\r\n```\r\nimport torch\r\n\r\nclass SimpleModel(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super(SimpleModel, self).__init__()\r\n        #self.xx = torch.zeros(2, 2)\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, num_iter : int):\r\n        x = torch.ones([2, 2], dtype=torch.float32)\r\n        y = torch.ones(2, 2, dtype=torch.float32)\r\n        v = torch.ones(2, 1, dtype=torch.float32)\r\n        for i in range(num_iter):\r\n            v = x * v\r\n        return x, v\r\n\r\nprint(SimpleModel().graph)\r\n```\r\n\r\n```\r\ngraph(%num_iter : int) {\r\n  %20 : bool = prim::Constant[value=1]()\r\n  %5 : Device = prim::Constant[value=""cpu""]()\r\n  %4 : int = prim::Constant[value=0]()\r\n  %3 : int = prim::Constant[value=6]()\r\n  %1 : int = prim::Constant[value=2]()\r\n  %13 : int = prim::Constant[value=1]()\r\n  %2 : int[] = prim::ListConstruct(%1, %1)\r\n  %x : Tensor = aten::ones(%2, %3, %4, %5)\r\n  %16 : int[] = prim::ListConstruct(%1, %13)\r\n  %v.1 : Tensor = aten::ones(%16, %3, %4, %5)\r\n  %v : Tensor = prim::Loop(%num_iter, %20, %v.1)\r\n    block0(%i : int, %23 : Tensor) {\r\n      %v.2 : Tensor = aten::mul(%x, %23)\r\n      -> (%20, %v.2)\r\n    }\r\n  %27 : (Tensor, Tensor) = prim::TupleConstruct(%x, %v)\r\n  return (%27);\r\n}\r\n```\r\n\r\n@houseroad can you take a look?', 'Sure, I put it to my list.', 'hi @houseroad , while waiting for the fix, is there any workaround to prevent this problem from happening?']","['\r\nfrom __future__ import print_function\r\nimport torch\r\nimport onnx\r\nfrom onnx.onnx_pb2 import TensorProto\r\n\r\nclass SimpleModel(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super(SimpleModel, self).__init__()\r\n        #self.xx = torch.zeros(2, 2)\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, num_iter : int):\r\n        x = torch.ones([2, 2], dtype=torch.float32)\r\n        y = torch.ones(2, 2, dtype=torch.float32)\r\n        v = torch.ones(2, 1, dtype=torch.float32)\r\n        for i in range(num_iter):\r\n            v = x * v\r\n        return x, v\r\n\r\nmodel = SimpleModel()\r\n\r\nmodel_onnx = torch.onnx._export(model, torch.tensor(5), ""simple_loop.onnx"",\r\n        verbose=True,\r\n        operator_export_type=torch.onnx.OperatorExportTypes.ONNX,\r\n        example_outputs=(torch.zeros(2,2), torch.zeros(2,1)))\r\n\r\nprog = onnx.load(""simple_loop.onnx"")\r\nprint(""%1 tensor type is int64? "", prog.graph.node[0].attribute[0].t.data_type\r\n        == TensorProto.INT64)\r\n', '\r\ngraph(%num_iter : Long()) {\r\n  %1 : Long() = onnx::Constant[value={1}]()\r\n  %2 : Float(2, 2) = onnx::Constant[value= 1  1  1  1 [ CPUFloatType{2,2} ]]()\r\n  %3 : Float(2, 1) = onnx::Constant[value= 1  1 [ CPUFloatType{2,1} ]]()\r\n  %4 : Tensor = onnx::Loop(%num_iter, %1, %3)\r\n    block0(%i : int, %cond : Tensor, %7 : Tensor) {\r\n      %8 : Tensor = onnx::Mul(%2, %7)\r\n      -> (%1, %8)\r\n    }\r\n  return (%2, %4);\r\n}\r\n']","['ScriptModule', 'onnx._export', 'cond', 'tensor(bool)', 'tensor(int64)', '%1', 'TensorProto.INT64', '%1', 'block0', 'tensor(bool)']",0,0
531,pytorch,27680,closed,1>E:\software\libtorch\include\torch/csrc/utils/variadic.h(195): error C2951: Ê®°Êùø Â£∞ÊòéÂè™ËÉΩÂú®ÂÖ®Â±Ä„ÄÅÂëΩÂêçÁ©∫Èó¥ÊàñÁ±ªËåÉÂõ¥ÂÜÖ‰ΩøÁî®,"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:

1.
1.
1.

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


 - PyTorch Version (e.g., 1.0):
 - OS (e.g., Linux):
 - How you installed PyTorch (, , source):
 - Build command you used (if compiling from source):
 - Python version:
 - CUDA/cuDNN version:
 - GPU models and configuration:
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->
",,"['torch::nn::Sequential model(torch::nn::Linear(2, 3)); ', 'os  windows10 \r\nlibtorch 1.2\r\n', ""There are a couple of questions about your issue:\r\n1. Why do you copy Chinese text to the topic? It's fine for me because I know Chinese, but it's not true for all the developers here. You need to translate it into English.\r\n2. Why don't you fill in the form? Is there any trouble there?\r\n3. Where is your code? What do you mean by simply copying `torch::nn::Sequential model(torch::nn::Linear(2, 3));` here?\r\n4. Moreover, could you please tell me what compiler you are using and how you are constructing your project?\r\n5. There is an similar issue [here](https://github.com/pytorch/pytorch/issues/21768). Would you please have a try first?"", 'I\'m sorry .\r\nthis is my base problem\r\nOS &nbsp; &nbsp; &nbsp; &nbsp;: Windows 10\r\nCompiler:Visual Studio 2015\r\n\r\n\r\nwhen my code include:\r\nnn::Sequential moduel(nn::Linear(2,3));\r\n\r\n\r\n\r\nor&nbsp;\r\n\r\n\r\nnn::Sequential moduel(....)\r\n\r\n\r\n\r\n\r\nthis programe will compile fail.\r\n\r\n\r\nthe error is:\r\ntorch/csrc/utils/variadic.h(195): error C2059:&nbsp;grammatical&nbsp;mistake:‚Äútemplate‚Äù\r\nE:\\software\\libtorch\\include\\torch/csrc/utils/variadic.h(195): error C2951: &nbsp;Template declarations can only be used in the global, namespace, or class scope\r\n\r\n\r\n\r\n\r\nmeanwhile:\r\nthe same code . I use Ubuntu OS can\'t meet this problem\r\n\r\n\r\n\r\n\r\n------------------&nbsp;ÂéüÂßãÈÇÆ‰ª∂&nbsp;------------------\r\nÂèë‰ª∂‰∫∫:&nbsp;""peterjc123""<notifications@github.com&gt;;\r\nÂèëÈÄÅÊó∂Èó¥:&nbsp;2019Âπ¥10Êúà10Êó•(ÊòüÊúüÂõõ) ‰∏ãÂçà3:14\r\nÊî∂‰ª∂‰∫∫:&nbsp;""pytorch/pytorch""<pytorch@noreply.github.com&gt;;\r\nÊäÑÈÄÅ:&nbsp;"".""<547691062@qq.com&gt;;""Author""<author@noreply.github.com&gt;;\r\n‰∏ªÈ¢ò:&nbsp;Re: [pytorch/pytorch] 1&gt;E:\\software\\libtorch\\include\\torch/csrc/utils/variadic.h(195): error C2951: Ê®°Êùø Â£∞ÊòéÂè™ËÉΩÂú®ÂÖ®Â±Ä„ÄÅÂëΩÂêçÁ©∫Èó¥ÊàñÁ±ªËåÉÂõ¥ÂÜÖ‰ΩøÁî® (#27680)\r\n\r\n\r\n\r\n\r\nThere are a couple of questions about your issue:\r\n  \r\nWhy do you copy Chinese text to the topic? It\'s fine for me because I know Chinese, but it\'s not true for all the developers here. You need to translate it into English.\r\n \r\nWhy don\'t you fill in the form? Is there any trouble there?\r\n \r\nWhere is your code? What do you mean by simply copying torch::nn::Sequential model(torch::nn::Linear(2, 3)); here?\r\n \r\nMoreover, could you please tell me what compiler you are using and how you are constructing your project?\r\n  \r\n‚Äî\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub, or unsubscribe.', 'Please use VS 2017 instead. ', 'when I use Visual Studio 2017\r\n\r\nwhen the code include :\r\n#include <torch/torch.h>\r\nthis programe will compile fail.\r\nerror is:\r\n1>e:\\software\\libtorch\\include\\torch\\csrc\\api\\include\\torch\\data\\detail\\queue.h(78): error C2872: ‚Äústd‚Äù: Ambiguous notation\r\n\r\n\r\n', 'when I use Visual Studio 2017\r\n\r\nwhen the code include :\r\n#include <torch/torch.h&gt;\r\nthis programe will compile fail.\r\nerror is:\r\n1&gt;e:\\software\\libtorch\\include\\torch\\csrc\\api\\include\\torch\\data\\detail\\queue.h(78): error C2872: ‚Äústd‚Äù: Ambiguous notation\r\n\r\n\r\n\r\n------------------&nbsp;ÂéüÂßãÈÇÆ‰ª∂&nbsp;------------------\r\nÂèë‰ª∂‰∫∫:&nbsp;""peterjc123""<notifications@github.com&gt;;\r\nÂèëÈÄÅÊó∂Èó¥:&nbsp;2019Âπ¥10Êúà10Êó•(ÊòüÊúüÂõõ) ‰∏ãÂçà3:14\r\nÊî∂‰ª∂‰∫∫:&nbsp;""pytorch/pytorch""<pytorch@noreply.github.com&gt;;\r\nÊäÑÈÄÅ:&nbsp;"".""<547691062@qq.com&gt;;""Author""<author@noreply.github.com&gt;;\r\n‰∏ªÈ¢ò:&nbsp;Re: [pytorch/pytorch] 1&gt;E:\\software\\libtorch\\include\\torch/csrc/utils/variadic.h(195): error C2951: Ê®°Êùø Â£∞ÊòéÂè™ËÉΩÂú®ÂÖ®Â±Ä„ÄÅÂëΩÂêçÁ©∫Èó¥ÊàñÁ±ªËåÉÂõ¥ÂÜÖ‰ΩøÁî® (#27680)\r\n\r\n\r\n\r\n\r\nThere are a couple of questions about your issue:\r\n  \r\nWhy do you copy Chinese text to the topic? It\'s fine for me because I know Chinese, but it\'s not true for all the developers here. You need to translate it into English.\r\n \r\nWhy don\'t you fill in the form? Is there any trouble there?\r\n \r\nWhere is your code? What do you mean by simply copying torch::nn::Sequential model(torch::nn::Linear(2, 3)); here?\r\n \r\nMoreover, could you please tell me what compiler you are using and how you are constructing your project?\r\n  \r\n‚Äî\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub, or unsubscribe.', 'Please add `::` before the usage of `std` in the corresponding line.', 'when i use this code will error.\r\n------------------------------------------------\r\n#include <torch/torch.h&gt;\r\n#include<iostream&gt;\r\nint main() {\r\n\tsystem(""pause"");\r\n\treturn 0;\r\n}\r\n\r\n\r\n\r\n------------------------------------------------\r\nand this code is normal\r\n---------\r\n#include<iostream&gt;\r\nint main() {\r\n\tsystem(""pause"");\r\n\treturn 0;\r\n}\r\n\r\n---------\r\n\r\n\r\n\r\n\r\n------------------&nbsp;ÂéüÂßãÈÇÆ‰ª∂&nbsp;------------------\r\nÂèë‰ª∂‰∫∫:&nbsp;""peterjc123""<notifications@github.com&gt;;\r\nÂèëÈÄÅÊó∂Èó¥:&nbsp;2019Âπ¥10Êúà10Êó•(ÊòüÊúüÂõõ) ‰∏ãÂçà4:49\r\nÊî∂‰ª∂‰∫∫:&nbsp;""pytorch/pytorch""<pytorch@noreply.github.com&gt;;\r\nÊäÑÈÄÅ:&nbsp;"".""<547691062@qq.com&gt;;""Author""<author@noreply.github.com&gt;;\r\n‰∏ªÈ¢ò:&nbsp;Re: [pytorch/pytorch] 1&gt;E:\\software\\libtorch\\include\\torch/csrc/utils/variadic.h(195): error C2951: Ê®°Êùø Â£∞ÊòéÂè™ËÉΩÂú®ÂÖ®Â±Ä„ÄÅÂëΩÂêçÁ©∫Èó¥ÊàñÁ±ªËåÉÂõ¥ÂÜÖ‰ΩøÁî® (#27680)\r\n\r\n\r\n\r\n\r\nPlease add :: before the usage of std in the corresponding line.\r\n \r\n‚Äî\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub, or unsubscribe.', 'I know, have you tried my suggestion? That is to open `e:\\software\\libtorch\\include\\torch\\csrc\\api\\include\\torch\\data\\detail\\queue.h` and replace `std` with `::std` in line 78.', 'There are many similar mistakes', '@liushuan Just fix them one by one.', 'Have you used this method before?', ""@liushuan Isn't this one simply the duplicate of https://github.com/pytorch/pytorch/issues/18607 and https://github.com/pytorch/pytorch/pull/17863? "", 'thanks, I will try again.', ""this problem be solve with this method.\r\nI tested on different configurations and found that problem can be solved by change the conformance mode property to No. You can find that option in C/C++->Language. That's a new feature in vs2017 and 2019.\r\n"", '@liushuan Where can I found ""C/C++->Language""? Do you use ""Visual Studio 2017"", not ""Visual Studio 2017 Build tool""?', '@sh0416 If you use ""Visual Studio 2017 Build tool"", please remove \'/permissive-\' from the argument list in the compiler command line.', '@peterjc123 Thanks for fast reply, I tried to extend my C++ code to python code using `torch.utils.cpp_extension`. So, I just use ""python setup.py install"". What do I do in this case?', 'Could you please open a new issue with the full log and the commands you entered?', '@peterjc123 For now, I use ""Visual Studio 2015 Build tool"". I tried it with ""Visual Studio 2017 Build tool"" and open a new issue if the problem remains.', '@peterjc123 I reinstall all compilation tools. I install ""Visual Studio 2019"" and ""CUDA 10.1"". Now, that error doesn\'t occur, but other errors emerge. After searching the solution, I found https://github.com/facebookresearch/detectron2/issues/9#issuecomment-549013090 and it works for me.\r\nThe problem was about `constexpr`, which needs to be fixed in the compile time. I just changed it to `const`, which has weaker constraint, and it works. :)\r\nIf you think it cause some bugs, please let me know. Thanks.']","['\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n']","['conda', 'pip']",0,0
532,pytorch,9634,closed,"Model only has gradients when ""x = x + bias"" and not when ""x += bias""","## Issue description

I've built a simple model with a single bias layer (adding a set of constants to the inputs) as follows:


However, when I try training with this with an Adam optimizer, the weights of the bias layer do not change at all. Checking  shows that it is indeed in the optimizer, and checking  after running  returns a vector of all zeros.

As soon as I change the  to  in the forward() function, the model trains perfectly. The same issue also occurs with other operators such as . This model works:


It seems there is some difference with how the gradients of these are calculated which means that the gradients are all-zero when += is used and so training the model does not work. Is this intended behaviour?

## System Info
",,"[""Yes, this is intended behaviour.\r\nThe `=+, =*, ...` ops are inplace operations (like add_) and you cannot keep backward through them.\r\n\r\n*Edit*:  As @SsnL points out, the problem is not the in place op itself, but that the original (and then overwritten) value is needed somewhere in the backward. Whether or not values are needed can appear to be subtle.\r\n\r\nWhen using `x = x + y` the old `x` stays around (but doesn't have a name anymore) and will be visited in the backward."", 'Thanks, I will watch out for this in the future.', 'The above explanation is misleading. One should be able to backprop through inplace ops. The following script works correctly on a master build:\r\n```py\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nclass Bias(nn.Module):\r\n    def __init__(self, dim):\r\n        super(Bias, self).__init__()\r\n        self.bias = nn.Parameter(torch.zeros(dim))\r\n\r\n    def forward(self, x):\r\n        x += self.bias\r\n        return x\r\n\r\na = torch.randn(1)\r\nb = Bias(1)\r\n\r\nl = b(a)\r\nl.backward()\r\nprint(b.bias.grad)  # [1.]\r\n```']","['python\r\nclass Bias(nn.Module):\r\n    def __init__(self, dim):\r\n        super(Bias, self).__init__()\r\n        self.bias = nn.Parameter(torch.zeros(dim))\r\n\r\n    def forward(self, x):\r\n        x += self.bias\r\n        return x\r\n', 'python\r\nclass Bias(nn.Module):\r\n    def __init__(self, dim):\r\n        super(Bias, self).__init__()\r\n        self.bias = nn.Parameter(torch.zeros(dim))\r\n\r\n    def forward(self, x):\r\n        x = x + self.bias\r\n        return x\r\n', '\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.61\r\n\r\nOS: Ubuntu 16.04.2 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.61\r\nGPU models and configuration: \r\nGPU 0: TITAN X (Pascal)\r\n\r\nNvidia driver version: 384.81\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.14.5)\r\n[pip] numpydoc (0.8.0)\r\n[pip] torch (0.4.0)\r\n[pip] torchfile (0.1.0)\r\n[pip] torchsummary (1.3)\r\n[pip] torchvision (0.2.1)\r\n[conda] pytorch                   0.4.0           py36_cuda8.0.61_cudnn7.1.2_1    pytorch\r\n[conda] torchfile                 0.1.0                     <pip>\r\n[conda] torchsummary              1.3                       <pip>\r\n[conda] torchvision               0.2.1                    py36_1    pytorch\r\n']","['list(model.parameters())', 'list(model.parameters()[0].grad)', 'loss.backward()', 'x += self.bias', 'x = x + self.bias', '*=']",0,0
533,pytorch,1984,closed,Different results for batch size 1 Variables,"pytorch version: '0.1.12_2'

Run this snippet (The required serialised tensor has been included in the attachment.



The output

[issue.zip](https://github.com/pytorch/pytorch/files/1124350/issue.zip)
",,"['My fault, I need to call `eval` on net to use evaluation mode.']","[""python\r\nimport torch\r\nfrom torch.autograd import Variable\r\nfrom torch import nn\r\n\r\ntorch.manual_seed(0)\r\n\r\nx_restored = Variable(torch.load('x.pth'), requires_grad=False)\r\nx_res_repeated = x_restored.repeat(10, 1)\r\n\r\nclass Network(nn.Module):\r\n    def __init__(self, dim_in, dim_out):\r\n        super(Network, self).__init__()\r\n        self.m = nn.Sequential(\r\n            nn.Linear(dim_in, 100),\r\n            nn.BatchNorm1d(100),\r\n            nn.ReLU(),\r\n            nn.Linear(100, 100),\r\n            nn.BatchNorm1d(100),\r\n            nn.ReLU(),\r\n            nn.Linear(100, dim_out*2)\r\n        )\r\n\r\n    def forward(self, X):\r\n        return self.m(X)\r\n\r\nnet = Network(x_restored.size()[1], 2)\r\nprint(net(x_restored))\r\nprint(net(x_res_repeated))\r\n"", '\r\nVariable containing:\r\n1.00000e-02 *\r\n -0.5704 -3.7342 -3.6541  7.4486\r\n[torch.FloatTensor of size 1x4]\r\n\r\nVariable containing:\r\n1.00000e-02 *\r\n -0.5703 -3.7480 -3.6610  7.4439\r\n -0.5703 -3.7480 -3.6610  7.4439\r\n -0.5703 -3.7480 -3.6610  7.4439\r\n -0.5703 -3.7480 -3.6610  7.4439\r\n -0.5703 -3.7480 -3.6610  7.4439\r\n -0.5703 -3.7480 -3.6610  7.4439\r\n -0.5703 -3.7480 -3.6610  7.4439\r\n -0.5703 -3.7480 -3.6610  7.4439\r\n -0.4867 -3.7301 -3.6103  7.4878\r\n -0.4867 -3.7301 -3.6103  7.4878\r\n[torch.FloatTensor of size 10x4]\r\n']",[],0,0
534,pytorch,12562,closed,Declarations.yaml cleanup,"Based on notes from @ezyang, @zdevito and @gchanan.

We'd like Declarations.yaml to be the single, externally visible API that all consumers, both internal and external, go through. Up until now, it has been designed under the assumption that only internal consumers make use of it, and that we can refactor these consumers when we wish to change Declarations.yaml. However, we have quickly gotten into a state where even our internal consumers are unmaintainable. This issue tracks a list of changes we'd like to apply to Declarations.yaml.

Meta-principle: the metadata for a type primarily consists of the type string (e.g., ). However, there may be some other metadata.

- [x] Stop hard-coding the list of arguments that  expands into. Instead, the list of kwargs a TensorOptions expands into should be specified only once and used everywhere.
- [ ] De-C++-ify our type syntax. This means no more putting  in an input type (so basically, drop  and use  only). Our type system should be C++/Python-agnostic. To determine if something has a reference or not, should be determined if the parameter in question is an input or output parameter, which should be part of the type string.
- [x] Add optional to the type syntax. @wanchaol is working on this
- [ ] Replace  with a simple list of  and/or .
- [x] Eliminate  entirely; once optional is supported, any arguments which have ""complex"" default values can simply specify the argument as optional, and then let the kernel compute the default argument itself
- [x] Unify  and . We think we have two only because TH and native had different paths, accidentally.
- [x] Augment type strings with alias sets, perhaps with exclamation marks. This could be used to remove  annotations. This will be handled by the JIT team.
- [x] Eliminate . @gchanan is working on this
- [ ] Eliminate . These would be replaced as outputs of ""inner"" native functions, which are the actual differentiable functions. Blocked on adding named outputs.
- [x] Add named outputs, ala . This helps the clarity of defining derivatives for multi-return functions.
- [ ] Eliminate , since we have eliminated most broadcasting from Type
- [ ] Eliminate 
- [ ] Eliminate , instead inferring it from the name of a function. Put this inference function in one place and make everyone use it.
- [ ] Eliminate , no one actually needs it anymore",,"[""> Replace method_of with a simple list of method and/or function.\r\nThis means eventually all of THNN ends up in torch, as it's ported to native?\r\n\r\nAlso, is there something to be done about operators? Declarations.yaml doesn't seem to contain anything on them (e.g. `__mul__` in Python).\r\n"", ""Yeah. Actually, a good first step is to just codegen out all of the wrapping code from native to THNN, but still use the native mechanism to generate the operators.\r\n\r\nRe `__mul__`, I don't actually know exactly how this works. But I guess if it's not in Declarations.yaml, we must be binding it by hand Python-side. I don't think we have plans to change this, but maybe we should?"", ""I just now updated this ticket with all the refactorings that have been carried out, based eyeballing Declarations.yaml. So we're in pretty good shape. The biggest one is expunging C++ from `type` and co. cc @cpuhrsch is that happening at some point?"", ""@ezyang that's not planned to happen.  The latest view of this (based on discussions between myself and @zdevito) is that the cost/benefit of doing that kind of conversion is too high.  Instead, we should view `Declarations.yaml` as being what drives generation of `VariableType` and everything else should ideally be driven by schemas."", 'Closing due to age']",[],"['f(Tensor x, Tensor y) -> Tensor', 'TensorOptions', 'Tensor &', 'type', 'dynamic_type', 'method_of', 'method', 'function', 'python_default_init', 'default_init', 'init', 'output', 'is_type_dispatched', 'buffers', 'f(Tensor x) -> (Tensor out, Tensor buf)', 'method_prefix_derived', 'mode', 'inplace', 'device_guard']",0,0
535,pytorch,30400,closed,miniz is missing in .gitmodule,"## üêõ Bug

miniz is missing in .gitmodule but there is miniz in the third_party

Steps to reproduce the behavior:

1.sync the lastest PyTorch code;
1.run build
1.report 'caffe2/CMakeList.txt missing miniz'

It seems that .gitmodule does NOT match third_party completely?
",,['Maybe it is located in such repo'],[],[],0,0
536,pytorch,1301,closed,Test failure when running pytorch without cuDNN,,high priority,"['It seems to be broken because of the fused RNN kernels commit. If I add `False and` to [this condition](https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/rnn.py#L23), then the test passes (with `torch.backends.cudnn.enabled = False` set before). @csarofeen ', 'Working on this. I understand the problem, and am working on a fix.', '@apaszke @adamlerer Thanks for finding this and bringing it to my attention.']","['\r\n# compile pytorch without cudnn\r\n$ python test_nn.py TestNN.test_variable_sequence_cuda\r\n/data/users/alerer/pytorch/pytorch/torch/backends/cudnn/__init__.py:40: UserWarning: PyTorch was compiled without cuDNN support. To use cuDNN, rebuild PyTorch making sure the library is visible to the build system.\r\n======================================================================\r\nFAIL: test_variable_sequence_cuda (__main__.TestNN)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""test_nn.py"", line 1501, in test_variable_sequence_cuda\r\n    self._test_variable_sequence(True)\r\n  File ""test_nn.py"", line 1492, in _test_variable_sequence\r\n    self.assertEqual(x_leaf.grad.data, grad_x)\r\n  File ""/data/users/alerer/pytorch/pytorch/test/common.py"", line 158, in assertEqual\r\n    assertTensorsEqual(x, y)\r\n  File ""/data/users/alerer/pytorch/pytorch/test/common.py"", line 150, in assertTensorsEqual\r\n    self.assertLessEqual(max_err, prec, message)\r\nAssertionError: 0.2056929971275647 not less than or equal to 1e-05 :\r\n']",[],0,0
537,pytorch,675,closed,[build/nccl] failed to build libnccl on Debian unstable,"Failed to build CUDA version of pytorch (without CUDNN) with the latest source.

OS: debian unstable/experimental
Compiler: gcc-5, g++-5
CUDA: 8.0.44 (package provided by Debian)

buildlog: http://debomatic-amd64.debian.net/distribution#experimental/pytorch-contrib/0.1.7~1/buildlog



I have no idea about this ...",,"[""If you look [here](https://github.com/pytorch/pytorch/blob/master/torch/lib/nccl/Makefile#L58-L60) you'll see that it fails, because `ls` can't find `libcudart.so`, and that's why `CUDA_MAJOR` and `CUDA_MINOR` end up being empty strings. I don't know how a debian CUDA package looks, but you probably don't have `libcudart.so` with a version extension."", 'Maybe `CUDA_VERSION=8.0 pp setup.py build` helps?', '@apaszke Thanks, the fix is to to export the two environment variables:\r\n```\r\nexport CUDA_HOME=/usr\r\nexport CUDA_LIB=/usr/lib/$(shell dpkg-architecture -qDEB_HOST_MULTIARCH)\r\n```']","['\r\n-- The C compiler identification is GNU 5.4.1\r\n-- The CXX compiler identification is GNU 5.4.1\r\n-- Check for working C compiler: /usr/bin/gcc-5\r\n-- Check for working C compiler: /usr/bin/gcc-5 -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /usr/bin/g++-5\r\n-- Check for working CXX compiler: /usr/bin/g++-5 -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Found CUDA: /usr (found suitable version ""8.0"", minimum required is ""7.0"") \r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /<<PKGBUILDDIR>>/torch/lib/build/nccl\r\nmake[2]: Entering directory \'/<<PKGBUILDDIR>>/torch/lib/build/nccl\'\r\nmake[3]: Entering directory \'/<<PKGBUILDDIR>>/torch/lib/build/nccl\'\r\nmake[4]: Entering directory \'/<<PKGBUILDDIR>>/torch/lib/build/nccl\'\r\nScanning dependencies of target nccl\r\nmake[4]: Leaving directory \'/<<PKGBUILDDIR>>/torch/lib/build/nccl\'\r\nmake[4]: Entering directory \'/<<PKGBUILDDIR>>/torch/lib/build/nccl\'\r\n[100%] Generating lib/libnccl.so\r\nmake[5]: Entering directory \'/<<PKGBUILDDIR>>/torch/lib/nccl\'\r\nls: cannot access \'/usr/lib64/libcudart.so.*\': No such file or directory\r\nls: cannot access \'/usr/lib64/libcudart.so.*\': No such file or directory\r\nGrabbing  src/nccl.h                > /<<PKGBUILDDIR>>/torch/lib/build/nccl/include/nccl.h\r\nCompiling src/libwrap.cu            > /<<PKGBUILDDIR>>/torch/lib/build/nccl/obj/libwrap.o\r\nCompiling src/core.cu               > /<<PKGBUILDDIR>>/torch/lib/build/nccl/obj/core.o\r\nCompiling src/all_gather.cu         > /<<PKGBUILDDIR>>/torch/lib/build/nccl/obj/all_gather.o\r\nCompiling src/all_reduce.cu         > /<<PKGBUILDDIR>>/torch/lib/build/nccl/obj/all_reduce.o\r\nCompiling src/broadcast.cu          > /<<PKGBUILDDIR>>/torch/lib/build/nccl/obj/broadcast.o\r\nCompiling src/reduce.cu             > /<<PKGBUILDDIR>>/torch/lib/build/nccl/obj/reduce.o\r\nCompiling src/reduce_scatter.cu     > /<<PKGBUILDDIR>>/torch/lib/build/nccl/obj/reduce_scatter.o\r\nsrc/core.cu(724): error: expected an expression\r\n\r\nsrc/core.cu(724): error: expected an expression\r\n\r\n2 errors detected in the compilation of ""/tmp/tmpxft_00002c02_00000000-13_core.compute_52.cpp1.ii"".\r\nMakefile:98: recipe for target \'/<<PKGBUILDDIR>>/torch/lib/build/nccl/obj/core.o\' failed\r\nmake[5]: *** [/<<PKGBUILDDIR>>/torch/lib/build/nccl/obj/core.o] Error 2\r\n']",[],0,0
538,pytorch,9167,closed,Redundant code in distributed,"It seems like the  is not used at all. Could someone tell me why don't directly  to ?

https://github.com/pytorch/pytorch/blob/4b2b6907929093634c1e452aa2a5a85e4dd9a793/torch/csrc/distributed/Module.cpp#L786",,"[""It's a legacy thing, we can remove it."", ""I'm willing to create a PR for this.\n\nAdam Paszke <notifications@github.com> ‰∫é2018Âπ¥7Êúà6Êó•Âë®‰∫î ‰∏ãÂçà8:17ÂÜôÈÅìÔºö\n\n> It's a legacy thing, we can remove it.\n>\n> ‚Äî\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/9167#issuecomment-403017113>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEWFDFGfpufgIWtjArsb4KXTTZnQwcj3ks5uD1VqgaJpZM4VCgev>\n> .\n>\n"", 'Please go ahead']",[],"['descriptors', 'push_back', 'raw_descriptors']",0,0
539,pytorch,7072,closed,JIT fails on GPU !=0,"## Issue description

Run the test case code below on a system with 2 or more GPUs and you will get this stack trace:

Traceback (most recent call last):
  File ""testnet.py"", line 51, in <module>
    r = net(V)
RuntimeError: arguments are located on different GPUs at /home/tester/pytorch/aten/src/THC/generic/THCTensorMathPairwise.cu:250

## Code example

",oncall: jit,"['Seems like https://github.com/pytorch/pytorch/issues/6010 fixed this. Closing for now, please reopen if you have this problem. ']","['python\r\nimport torch\r\nfrom torch import jit\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\nimport torch.optim as optim........\r\n\r\ntorch.backends.cudnn.benchmark = True\r\n\r\n@torch.jit.compile(nderivs=1)\r\nclass TestNet(nn.Module):\r\n    def __init__(self):\r\n        super(TestNet, self).__init__()\r\n        self.net1 = nn.Linear(100, 200)\r\n        self.net2 = nn.Linear(200, 1)\r\n        self.bn = nn.BatchNorm1d(200)\r\n        self.sigmoid = nn.Sigmoid()\r\n        self.ReLU = nn.ReLU(inplace=False)\r\n        self.drop = nn.Dropout(0.5)\r\n...............\r\n    def forward(self, V):\r\n#        return self.sigmoid(self.net2(self.drop(self.ReLU(self.bn(self.net1(V)))))).squeeze().\r\n        return self.sigmoid(self.net2(self.ReLU(self.bn(self.net1(V))))).squeeze().\r\n\r\n\r\nuse_cuda = True\r\nGPU = 1\r\nnet = TestNet()\r\ncriterion = nn.BCELoss()\r\nif use_cuda:\r\n    net.cuda(GPU)\r\n    criterion.cuda(GPU)\r\n    V = Variable(torch.randn(100, 100)).cuda(GPU)\r\n    label = Variable(torch.randn(100)).cuda(GPU)\r\nelse:\r\n    V = Variable(torch.randn(100, 100))\r\n    label = Variable(torch.randn(100))\r\n\r\n\r\noptim_betas = (0.9, 0.999)\r\noptimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.01, betas=optim_betas)\r\ncriterion = nn.BCELoss()\r\n\r\nnet.train()\r\nfor i in range(0,1000000):\r\n    net.zero_grad()\r\n    r = net(V)\r\n    err = criterion(r, label)\r\n    err.backward()...\r\n\r\n']",[],0,0
540,pytorch,30331,closed,[v1.4.0] Release Tracker,"The branch v1.4.0 has been cut.

If you need any particular patches onto this branch, please comment below and send a PR to v1.4.0 branch (instead of master).

---

[Current PRs open against the v1.4.0 branch](https://github.com/pytorch/pytorch/pulls?utf8=%E2%9C%93&q=is%3Apr+is%3Aopen+base%3Av1.4.0)
",releng triaged,"['was trying to get https://github.com/pytorch/pytorch/pull/30288 ""updates to quantization doc"" under the wire. It should be low risk but it does change both doc rst files and pytorch docstrings. I currently have three failures in CircleCI .. two of them seem to be pre-existing false positives but one might be a flaky test. Will flag when I get this landed and also create a PR against the release branch. ', 'ok .. the doc changes landed on master. The new PR on the v1.4.0 is posted and is #30372 ', 'Request for patch: https://github.com/pytorch/pytorch/pull/30545\r\nStatus: merged into master at 5c6705e after the 1.4 branch cut\r\nUpdate: merged into 1.4.0\r\n', 'https://github.com/pytorch/pytorch/pull/30575 adds notes about ordering on singular values and eigenvalues output by torch.svd and torch.symeig respectively.\r\n\r\n---\r\nStatus: Merged into 1.4.0', 'Request for patch: https://github.com/pytorch/pytorch/pull/30629\r\n\r\nBackwards-compatibility fix for a change landed shortly before the branch cut', 'UPDATE: this has been merged into v1.4.0 branch.\r\n\r\n--------\r\n\r\nRequest for patch: https://github.com/pytorch/pytorch/pull/30668\r\n\r\nIt fixes high-pri issues https://github.com/pytorch/pytorch/issues/30508 and https://github.com/pytorch/pytorch/issues/30462 which prevent users from doing any of the following:\r\n1. Moving `torch::nn` modules with undefined parameters/buffers to a different device/dtype\r\n2. Optimizing `torch::nn` modules with undefined parameters', ""UPDATE: this has been merged into v1.4.0 branch.\r\n\r\n--------\r\n\r\nRequest for patch: https://github.com/pytorch/pytorch/pull/30698\r\n\r\nIt is a 3-line fix that removes `namespace F = torch::nn::functional` from `torch/nn/modules/batchhnorm.h`, so that people don't have to define `torch::nn::functional` as `F` if they don't want to. Fixes issue https://github.com/pytorch/pytorch/issues/30682."", ""https://github.com/pytorch/pytorch/pull/30729 cherry-picks changes to torch.where that were made on the 1.3.1 branch but hadn't been ported to master at the time of the 1.4.0 branch cut.\r\n"", 'Request for patch: [#30648 ](https://github.com/pytorch/pytorch/pull/30648)\r\n\r\nUpdate docs persons of interest to add back the MSFT folks for ONNX Governance\r\n\r\n', ""Request for patch: #30747 \r\n\r\nIt's to export the operator names from Python API for mobile custom build. "", 'https://github.com/pytorch/pytorch/pull/30750\r\n\r\nThis turns off the new profiling graph executor for 1.4', '#30776 Fixes an error message.', 'Request for patch: https://github.com/pytorch/pytorch/pull/30752\r\n\r\nStatus: merged into master at 1350b99\r\n\r\nUpdate: Merged into 1.4.0', 'Request for patch: [#30648](https://github.com/pytorch/pytorch/pull/30648)\r\n\r\nRestructure docs organization and naming\r\n', ""Request for patch: ~~#24947~~  #30901 (sorry, didn't know we need to target the v1.4.0 branch ourselves)\r\n\r\nThis is a bug fix (for example, see cupy/cupy#2589) for interoperating with other Python GPU libraries like CuPy and Numba that support the `__cuda_array_interface__` protocol. This patch ensures PyTorch's compliance with the protocol. \r\n\r\ncc: @awthomp @beam2d @jakirkham"", 'Request for patch: https://github.com/pytorch/pytorch/pull/30939\r\n\r\nSupport exporting aten::copy_ and aten::index_put to ONNX opset 11', 'UPDATE: this has been merged into v1.4.0 branch.\r\n\r\n------\r\n\r\nRequest for patch: #31166\r\n\r\nThis PR removes the `transposed` argument from `Conv{1,2,3}dOptions`, so that users are encouraged to use `ConvTranspose{1,2,3}d` instead if they want to do deconvolution. ', 'Request for patch: https://github.com/pytorch/pytorch/pull/31170\r\n\r\nAdd Support for ONNX Interpolate Add Scales Params', 'Should we include this? https://github.com/pytorch/pytorch/pull/31279. I can cherry-pick it if needed.', 'We will start building v1.4 RC release. No more patches after this comment.\r\n\r\ncc: @peterjc123 ', 'Is it possible to still include #31390?', ""I'll let @gchanan  or @soumith  to make the call"", '~~Request for patch from MSFT: https://github.com/pytorch/pytorch/pull/31397~~\r\n\r\n~~Add ONNX Export Support to floor_divide~~\r\n\r\n~~This PR is required to unblock onnx export for torchvision models, fixing the breakage introduced recently from torchvision side.~~\r\n\r\ncc @spandantiwari @lara-hdr @fmassa @houseroad \r\n\r\nTurns out this is not required for 1.4 since floor_divide itself is not included.', '@mingbowan what can we do to land #30750 ? it is a critical patch. ', 'What is the story on https://github.com/pytorch/pytorch/pull/30372 .. can we get this incorporated please?', 'Is it possible to include #31432 as well? This fixes a segmentation fault in `unflatten` for named tensors.\r\n\r\n---\r\nStatus: Merged into 1.4.0', 'Folks -- we are trying to get the tests in a stable state before we start cherry-picking more PRs.  Hopefully that will be done tomorrow and we can resume.\r\n\r\nThanks!', 'Will 1.4.0 support Python 3.8?\r\n\r\n<hr>\r\n\r\nFrom https://github.com/pytorch/pytorch/issues/21741#issuecomment-541242504 it looks like it will.', 'https://github.com/pytorch/pytorch/pull/31637', 'Request for cherry-picking: https://github.com/pytorch/pytorch/pull/31716', 'Request for cherry-picking: #31816', 'https://github.com/pytorch/pytorch/pull/31850 - a 1.4.0 cherry-pick of #28175 ', 'Request for cherry-pick https://github.com/pytorch/pytorch/pull/31890 small PR for full ONNX support for Torchvision models (including KeypointRCNN).\r\n\r\ncc @fmassa ', 'Fix dll load logic for Python 3.8 on Windows: #32339 ', 'release is closed']",[],[],0,0
541,pytorch,24583,closed,Migrate `leaky_relu` and `leaky_relu_` from the TH to Aten (CUDA),"Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review.",better-engineering module: porting triaged,[],[],[],0,0
542,pytorch,30321,closed,dyld: Library not loaded: /usr/local/opt/openssl/lib/libssl.1.0.0.dylib Referenced from: /usr/local/bin/sccache,"All OS X builds are failing due to this https://app.circleci.com/jobs/github/pytorch/pytorch/3698524

CircleCI ticket: https://support.circleci.com/hc/en-us/requests/63093",,"['From CircleCI:\r\n\r\n> From an initial look, usually the error message you mentioned is due to openssl not being installed and can usually be rectified by installing from homebrew: brew install openssl. Try giving this a go as a temporary workaround while we investigate this issue further.', 'The test machines did not had libssl1.0.0 anymore for some reason.\r\nRecompiled sccache to statically package libssl to avoid this issue in the future.', 'Another useful thing to do is to copy paste the commands you ran to bbuild for future reference :>', 'For future reference, the binary is build from the master branch of https://github.com/yf225/sccache (to support nvcc).\r\nYou can get the statically linked version with:\r\n```\r\nexport OPENSSL_STATIC=yes\r\ncargo build --features=s3 --release\r\n```\r\n\r\nThis flag is not advertised in the old version of the repo we compile but it does work.\r\n', 'For posterity this is what CircleCI responded with:\r\n\r\n----\r\n\r\nWe have looked into this issue and have found that this is being caused by a dependency conflict.\r\n\r\nTo install libomp requires the following dependencies (taken from the build log):\r\n\r\n==> Installing dependencies for libomp: pkg-config, gdbm, openssl@1.1, readline, sqlite, xz, python, sphinx-doc and cmake\r\nThis package is specifically installing OpenSSL 1.1 and higher. When homebrew installs OpenSSL, it will change the symlinks in \r\n/usr/local/opt/openssl/ to point to the latest version (which truly resides in \r\n/usr/local/opt/openssl@1.1). This in turns means the file \r\n/usr/local/opt/openssl/lib/libssl.1.0.0.dylib is no longer in that directory as it has been replaced by \r\nlibssl.1.1.dylib.\r\n\r\nThe previous version of OpenSSL (1.0) can be found in \r\n/usr/local/opt/openssl@1.0/ which is where the previous version of the dylib, which is trying to be used, can be found (libssl.1.0.0.dylib).\r\n\r\nYour script seems to be using something that is trying to specifically use this version of the dylib. Best practise would be to instead use \r\n/usr/local/opt/openssl/lib/libssl.dylib which will always point to the latest version of openssl as it is not requesting a specific version. You may need to check the dependencies of the packages you are using and make sure to pin specific versions of these packages to prevent this kind of conflict.\r\n\r\nDo you know if any packages you use have been updated recently?']",[],[],0,0
543,pytorch,18682,closed,Time-consuming gradient computation and backward for realizing meta network,"## ‚ùì Questions and Help
I am trying to implement the [paper](https://arxiv.org/pdf/1803.09050.pdf) for noisy label task. We found the training time is extremely time consuming, which troubled us for a long time.

The algorithm which i am tryinng to implement is the following:
![image](https://user-images.githubusercontent.com/30048368/55319105-c94bc080-54a6-11e9-923f-81b819c96154.png)

And the code is the following:
 

We find time_backward in meta-val stage takes up 99% of the meta-val training time. Moreover, the meta-val training time takes up 93% of the all training time. 

Could anyone give some suggestions on how to resolve this issue? Thank you very much!",,['please use https://discuss.pytorch.org for questions'],"['\r\nmeta_train_preds = meta_model(train_images)\r\n            meta_train_loss = ((-1* torch.log(meta_train_preds + 1e-8).sum(dim=1)\r\n                ).view(config.minibatch_size, config.group + 100 - 1).mean(dim=1))\r\n\r\n            example_weights = torch.zeros(\r\n                config.minibatch_size, requires_grad=True, dtype=torch.float32\r\n            ).cuda()\r\n\r\n            meta_train_weighted_loss = (example_weights * meta_train_loss).sum()\r\n\r\n            meta_model.zero_grad()\r\n            grads = torch.autograd.grad(\r\n                meta_train_weighted_loss, (meta_model.params()), create_graph=True\r\n            )\r\n            meta_model.update_params(lr, source_params=grads)\r\n            time_meta_train = time.time() - time_proc_data - time_iter_start\r\n\r\n            time_start_val = time.time()\r\n            meta_val_preds = meta_model(val_images)\r\n            meta_val_loss = criterion(meta_val_preds, val_labels)\r\n            time_val_forward = time.time() - time_start_val\r\n\r\n            val_grad_to_examples_weight = torch.autograd.grad(\r\n                meta_val_loss, example_weights, only_inputs=True\r\n            )[0]\r\n\r\n            time_val_backward = time.time() - time_start_val - time_val_forward\r\n\r\n            clip_example_weights = torch.clamp(-val_grad_to_examples_weight, min=0)\r\n            sum_weights = torch.sum(clip_example_weights)\r\n\r\n            if sum_weights != 0:\r\n                norm_example_weights = clip_example_weights / sum_weights\r\n            else:\r\n                norm_example_weights = clip_example_weights\r\n            time_post_operation = (\r\n                time.time() - time_start_val - time_val_forward - time_val_backward\r\n            )\r\n            time_meta_val = (\r\n                time.time() - time_meta_train - time_proc_data - time_iter_start\r\n            )\r\n']",[],0,0
544,pytorch,3999,closed,Save big file is slow,"It spent a lot of time for I to save file by torch.save().

The file size is about 3.5G, and it costs 2hr 49m totally. Is it make sense?
",,"['I replied to your post on the [forums](https://discuss.pytorch.org/t/using-torch-save-to-save-big-file-is-very-slow/10749/2), feel free to reply to my comments there or here. \r\n\r\nAs an addenum, I tried running the following:\r\n```\r\nx = torch.zeros(3500000000//4)\r\ntorch.save(x, ""tmp.txt"")\r\n```\r\nand it runs in around 2 seconds for me (I am on an ssd, though).', 'lets keep the discusson on forums for this one.', 'I have replied the discussion on the forums']",[],[],0,0
545,pytorch,3610,closed,High GPU memory usage on master,"Running cyclegan from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix on master takes huge amount of memory (>11G with default options). It only takes 3.5G on 0.3 and 0.2 branches. 

The memory usage increases as I run more iteration. It seems some tensors are not freed properly.

I remember running on a pre-ATen-merge version without such issue.

Repro command:


By the way, this doesn't happen on the other model in the repo, pix2pix",high priority,"['We observed the same issues with many other kinds of CNNs, both in our own code and the pytorch examples.\r\n\r\nFor example, launching a resnet101 training on ImageNet with https://github.com/pytorch/examples/blob/master/imagenet/main.py on four Titan Xp works with v0.2 but immediately goes out of memory with master.', '@ducksoup try out the `v0.3.0` branch, it should be stable and have most of the latest fixes.', '@SsnL can you confirm that master is fixed on this issue? if not follow up with @colesbury ', '@soumith It is indeed fixed. However, GPU memory consumption on master is about 4G rather than 3.5G on 0.2 and 0.3 branches.', 'This seems fixed.']",['\r\npython train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan --no_dropout --niter 1 --niter_decay 0 --no_html --max_dataset_size 10 --batchSize 1\r\n'],[],0,0
546,pytorch,27123,closed,RuntimeError: CUDA error: device-side assert triggered,"Hello, I implemented DNC codes with PyTorch Version 1.2.
but I got runtime errors when I trained my DNC model. 
I attach a file which contains error messages, because it is too long to write here. 
----------------------
I solve this problem.

",module: cuda triaged,[],[],[],0,0
547,pytorch,22171,closed,[jit] Compile only the relevant branch for `isinstance`,"Similar to our thing where we only compile if a submodule is not None (#14533), we could skip compiling the non-taken branch for  checks (since their result is constant) to support use cases like this:




cc @suo",jit-backlog oncall: jit triaged,[],"['python\r\nclass M(torch.jit.ScriptModule):\r\n    @torch.jit.script_method\r\n    def forward(self, x):\r\n        return x + 2\r\n\r\nclass N(torch.jit.ScriptModule):\r\n    @torch.jit.script_method\r\n    def forward(self, x):\r\n        return x + 10, x + 100\r\n\r\nclass Mod(torch.jit.ScriptModule):\r\n    def __init__(self, x):\r\n        super(Mod, self).__init__()\r\n        self.submodule = x\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, x):\r\n        res = self.submodule(x)\r\n\r\n        if isinstance(res, tuple):\r\n            b = res[1]\r\n        else:\r\n            b = res\r\n\r\n        return b\r\n']",['isinstance'],0,0
548,pytorch,15287,closed,Please make Slerp function available for generative model sampling,"## üöÄ Feature
I see that [](https://pytorch.org/docs/stable/torch.html?highlight=lerp#torch.lerp) function is available but  does not exist in PyTorch 1.0.

## Motivation
This feature request is motivated from the current research requirements and the way generative models are being developed (GANs specifically) i.e. the manipulation of output samples is made from latent input space. 
The paper [DCGAN](https://arxiv.org/abs/1511.06434) performed latent interpolation linearly ([](https://pytorch.org/docs/stable/torch.html?highlight=lerp#torch.lerp) would suffice) but [Sampling Generative Networks](https://arxiv.org/abs/1609.04468) paper suggests to use [Slerp function](https://en.wikipedia.org/wiki/Slerp) for interpolation. Moreover the following recent works go for Slerp too, for instance recently introduced [GAN 2.0](https://arxiv.org/abs/1812.04948), etc.

## Pitch
Please make [Slerp](https://en.wikipedia.org/wiki/Slerp) function available so that we can sample generative networks more effectively. 

## Alternatives
As an alternative please provide a PyTorch -based code snippet to compute [Slerp function](https://en.wikipedia.org/wiki/Slerp) effectively and easily.

## Additional context

Here is a Wikipedia's article showing example of [C++ and Python-NumPy code snippet](https://en.wikipedia.org/wiki/Slerp#Source_code) for [Slerp](https://en.wikipedia.org/wiki/Slerp). ",todo,"['Hi Rahul,\r\n\r\nThe NumPy code snippet given for slerp can be translated 1-to-1 into a PyTorch snippet.\r\nSlerp is not common enough that I think has value to add it as a standard function into PyTorch.', ""> The NumPy code snippet given for slerp can be translated 1-to-1 into a PyTorch snippet.\r\n\r\nOk. Can someone help me in writing one in PyTorch? I don't understand how to from that Wikipedia's NumPy snippet. \r\nBut @soumith I will still want you to implement it as it helps in better interpolations as reported in paper I mentioned above. "", 'you can post on https://discuss.pytorch.org to see if anyone would help, though it looks like a literal translation with minor changes needed']",[],"['torch.lerp', 'torch.slerp', 'torch.lerp', 'Tensor']",0,0
549,pytorch,6194,closed,Import order matters: segmentation fault if torch is imported first,"I have a couple of packages that I import in my project. Obviously, one of them is . I noticed that if I import  after only one of my other packages things are fine. However, if I import  first I get a segmentation fault. I'm not picky about importing the other module first and then do  but for some reason, I need to import torch first in my work and this has made things a bit complicated for me. I wonder, is this a Python issue or could it be  issue or maybe the other package (bpy) issue? Does anyone know how I can figure out what is causing the segmentation fault?",,"['What are the other packages?', '@colesbury So as I implied (sorry for not being very clear), none of the other packages matter. If I import `bpy` first and then `torch` everything seems to be fine. But importing `torch` and then `bpy` gives me seg fault.\r\n\r\n![1](https://user-images.githubusercontent.com/6061721/38215014-5409962a-3694-11e8-9ec4-11061f1a6d07.png)\r\n\r\nReverse order:\r\n\r\n![1](https://user-images.githubusercontent.com/6061721/38215097-a5b9277e-3694-11e8-9d4b-521db0b6550d.png)\r\n', 'Can you do this inside `gdb python`, then `run`, then `bt` when the segfault occurs, and please share the backtrace.', '@goldsborough Can you write the steps a bit more specific? I have never run Python debugger. Should I do `gdb python` first. Then import the modules and then exit python and do `run` and `bt`?', 'Yeah it should look like this\r\n\r\n```\r\n# This will start gdb with the python process\r\n$ gdb python\r\n<...>\r\n# here you write `run` to start the process\r\n(gdb) run\r\nPython 3.6.3 |Anaconda, Inc.| (default, Nov 20 2017, 20:41:42)\r\n[GCC 7.2.0] on linux\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import torch\r\n>>> import bpy\r\n# here it will segfault, then\r\n(gdb) bt\r\n```\r\n\r\nwe need the output of that', 'Also, what version of PyTorch are you running? (`import torch; print(torch.__version__)`). How did you install it? (e.g. from source, conda, pip)', '@colesbury I have had this issue for a long time but I eventually decided to say something about it on GitHub. I started compiling PyTorch from source from 2 months ago or so. I never tired with `pip` or `conda` so I\'m not sure if the same problem persists if I had installed PyTorch that way. Doing `torch.__version__` shows me `0.4.0a0+2e156f3`.\r\n\r\n I just did what @goldsborough asked me to do. Here\'s the output:\r\n\r\n```\r\nimport torch\r\n[New Thread 0x2aaaaf6aa700 (LWP 9233)]\r\n[New Thread 0x2aaaaf8ab700 (LWP 9234)]\r\n[New Thread 0x2aaab3aac700 (LWP 9235)]\r\n[New Thread 0x2aaab5cad700 (LWP 9236)]\r\n[New Thread 0x2aaab7eae700 (LWP 9237)]\r\n[New Thread 0x2aaaba0af700 (LWP 9238)]\r\n[New Thread 0x2aaabc2b0700 (LWP 9239)]\r\n[New Thread 0x2aaabe4b1700 (LWP 9240)]\r\n[New Thread 0x2aaac06b2700 (LWP 9241)]\r\n[New Thread 0x2aaac28b3700 (LWP 9242)]\r\n[New Thread 0x2aaac4ab4700 (LWP 9243)]\r\n[New Thread 0x2aaac6cb5700 (LWP 9244)]\r\n[New Thread 0x2aaac6eb6700 (LWP 9245)]\r\n[New Thread 0x2aaacb0b7700 (LWP 9246)]\r\n[New Thread 0x2aaacd2b8700 (LWP 9247)]\r\n[New Thread 0x2aaacf4b9700 (LWP 9248)]\r\n[New Thread 0x2aaad16ba700 (LWP 9249)]\r\n[New Thread 0x2aaad38bb700 (LWP 9250)]\r\n[New Thread 0x2aaad5abc700 (LWP 9251)]\r\n[New Thread 0x2aaad7cbd700 (LWP 9252)]\r\n[New Thread 0x2aaad9ebe700 (LWP 9253)]\r\n[New Thread 0x2aaadc0bf700 (LWP 9254)]\r\n[New Thread 0x2aaade2c0700 (LWP 9255)]\r\n[New Thread 0x2aaae04c1700 (LWP 9257)]\r\n[New Thread 0x2aaae26c2700 (LWP 9258)]\r\n[New Thread 0x2aaae48c3700 (LWP 9259)]\r\n[New Thread 0x2aaae6ac4700 (LWP 9260)]\r\n[New Thread 0x2aaae6cc5700 (LWP 9261)]\r\n[New Thread 0x2aaaeaec6700 (LWP 9262)]\r\n[New Thread 0x2aaaed0c7700 (LWP 9263)]\r\n[New Thread 0x2aaaef2c8700 (LWP 9264)]\r\n[New Thread 0x2aaaf14c9700 (LWP 9265)]\r\n[New Thread 0x2aaaf36ca700 (LWP 9266)]\r\n[New Thread 0x2aaaf38cb700 (LWP 9267)]\r\n[New Thread 0x2aaaf7acc700 (LWP 9268)]\r\n[New Thread 0x2aaaf9ccd700 (LWP 9269)]\r\n[New Thread 0x2aaafbece700 (LWP 9270)]\r\n[New Thread 0x2aaafe0cf700 (LWP 9272)]\r\n[New Thread 0x2aab002d0700 (LWP 9273)]\r\n[New Thread 0x2aab024d1700 (LWP 9274)]\r\n[New Thread 0x2aab046d2700 (LWP 9275)]\r\n[New Thread 0x2aab068d3700 (LWP 9277)]\r\n[New Thread 0x2aab08ad4700 (LWP 9278)]\r\n[New Thread 0x2aab0acd5700 (LWP 9281)]\r\n[New Thread 0x2aab0ced6700 (LWP 9282)]\r\n[New Thread 0x2aab0f0d7700 (LWP 9283)]\r\n[New Thread 0x2aab112d8700 (LWP 9284)]\r\n[Thread 0x2aaaf38cb700 (LWP 9267) exited]\r\n[Thread 0x2aab0f0d7700 (LWP 9283) exited]\r\n[Thread 0x2aab0ced6700 (LWP 9282) exited]\r\n[Thread 0x2aab112d8700 (LWP 9284) exited]\r\n[Thread 0x2aab0acd5700 (LWP 9281) exited]\r\n[Thread 0x2aab08ad4700 (LWP 9278) exited]\r\n[Thread 0x2aab068d3700 (LWP 9277) exited]\r\n[Thread 0x2aab046d2700 (LWP 9275) exited]\r\n[Thread 0x2aab024d1700 (LWP 9274) exited]\r\n[Thread 0x2aab002d0700 (LWP 9273) exited]\r\n[Thread 0x2aaafe0cf700 (LWP 9272) exited]\r\n[Thread 0x2aaafbece700 (LWP 9270) exited]\r\n[Thread 0x2aaaf9ccd700 (LWP 9269) exited]\r\n[Thread 0x2aaaf7acc700 (LWP 9268) exited]\r\n[Thread 0x2aaaf36ca700 (LWP 9266) exited]\r\n[Thread 0x2aaaf14c9700 (LWP 9265) exited]\r\n[Thread 0x2aaaef2c8700 (LWP 9264) exited]\r\n[Thread 0x2aaaed0c7700 (LWP 9263) exited]\r\n[Thread 0x2aaaeaec6700 (LWP 9262) exited]\r\n[Thread 0x2aaae6cc5700 (LWP 9261) exited]\r\n[Thread 0x2aaae6ac4700 (LWP 9260) exited]\r\n[Thread 0x2aaae48c3700 (LWP 9259) exited]\r\n[Thread 0x2aaae26c2700 (LWP 9258) exited]\r\n[Thread 0x2aaae04c1700 (LWP 9257) exited]\r\n[Thread 0x2aaade2c0700 (LWP 9255) exited]\r\n[Thread 0x2aaadc0bf700 (LWP 9254) exited]\r\n[Thread 0x2aaad9ebe700 (LWP 9253) exited]\r\n[Thread 0x2aaad7cbd700 (LWP 9252) exited]\r\n[Thread 0x2aaad5abc700 (LWP 9251) exited]\r\n[Thread 0x2aaad38bb700 (LWP 9250) exited]\r\n[Thread 0x2aaad16ba700 (LWP 9249) exited]\r\n[Thread 0x2aaacf4b9700 (LWP 9248) exited]\r\n[Thread 0x2aaacd2b8700 (LWP 9247) exited]\r\n[Thread 0x2aaacb0b7700 (LWP 9246) exited]\r\n[Thread 0x2aaac6eb6700 (LWP 9245) exited]\r\n[Thread 0x2aaac6cb5700 (LWP 9244) exited]\r\n[Thread 0x2aaac4ab4700 (LWP 9243) exited]\r\n[Thread 0x2aaac28b3700 (LWP 9242) exited]\r\n[Thread 0x2aaac06b2700 (LWP 9241) exited]\r\n[Thread 0x2aaabe4b1700 (LWP 9240) exited]\r\n[Thread 0x2aaabc2b0700 (LWP 9239) exited]\r\n[Thread 0x2aaaba0af700 (LWP 9238) exited]\r\n[Thread 0x2aaab7eae700 (LWP 9237) exited]\r\n[Thread 0x2aaab5cad700 (LWP 9236) exited]\r\n[Thread 0x2aaab3aac700 (LWP 9235) exited]\r\n[Thread 0x2aaaaf8ab700 (LWP 9234) exited]\r\n[Thread 0x2aaaaf6aa700 (LWP 9233) exited]\r\n>>> import bpy\r\n[New Thread 0x2aab112d8700 (LWP 9300)]\r\n[New Thread 0x2aab0f0d7700 (LWP 9301)]\r\n[New Thread 0x2aab0ced6700 (LWP 9302)]\r\n[New Thread 0x2aab0acd5700 (LWP 9303)]\r\n[New Thread 0x2aab08ad4700 (LWP 9304)]\r\n[New Thread 0x2aab068d3700 (LWP 9305)]\r\n[New Thread 0x2aab046d2700 (LWP 9306)]\r\n[New Thread 0x2aab024d1700 (LWP 9307)]\r\n[New Thread 0x2aab002d0700 (LWP 9308)]\r\n[New Thread 0x2aaafe0cf700 (LWP 9309)]\r\n[New Thread 0x2aaafbece700 (LWP 9310)]\r\n[New Thread 0x2aaaf9ccd700 (LWP 9311)]\r\n[New Thread 0x2aaaf7acc700 (LWP 9312)]\r\n[New Thread 0x2aaaf38cb700 (LWP 9313)]\r\n[New Thread 0x2aaaf36ca700 (LWP 9314)]\r\n[New Thread 0x2aaaf14c9700 (LWP 9315)]\r\n[New Thread 0x2aaaef2c8700 (LWP 9316)]\r\n[New Thread 0x2aaaed0c7700 (LWP 9317)]\r\n[New Thread 0x2aaaeaec6700 (LWP 9318)]\r\n[New Thread 0x2aaab7eae700 (LWP 9319)]\r\n[New Thread 0x2aaaba0af700 (LWP 9320)]\r\n[New Thread 0x2aaabc2b0700 (LWP 9321)]\r\n[New Thread 0x2aaabe4b1700 (LWP 9322)]\r\n[New Thread 0x2aaac06b2700 (LWP 9323)]\r\n[New Thread 0x2aaac28b3700 (LWP 9324)]\r\n[New Thread 0x2aaac4ab4700 (LWP 9325)]\r\n[New Thread 0x2aaacb0b7700 (LWP 9326)]\r\n[New Thread 0x2aaacd2b8700 (LWP 9327)]\r\n[New Thread 0x2aaacf4b9700 (LWP 9328)]\r\n[New Thread 0x2aaad16ba700 (LWP 9329)]\r\n[New Thread 0x2aaad38bb700 (LWP 9330)]\r\n[New Thread 0x2aaad5abc700 (LWP 9331)]\r\n[New Thread 0x2aaad7cbd700 (LWP 9332)]\r\n[New Thread 0x2aaad9ebe700 (LWP 9333)]\r\n[New Thread 0x2aaadc0bf700 (LWP 9334)]\r\n[New Thread 0x2aaade2c0700 (LWP 9335)]\r\n[New Thread 0x2aaae04c1700 (LWP 9336)]\r\n[New Thread 0x2aaae26c2700 (LWP 9337)]\r\n[New Thread 0x2aaae48c3700 (LWP 9338)]\r\n[New Thread 0x2aabbf200700 (LWP 9339)]\r\n[New Thread 0x2aabbf401700 (LWP 9340)]\r\n[New Thread 0x2aabbf602700 (LWP 9341)]\r\n[New Thread 0x2aabbf803700 (LWP 9342)]\r\n[New Thread 0x2aabbfa04700 (LWP 9343)]\r\n[New Thread 0x2aabbfc05700 (LWP 9344)]\r\n[New Thread 0x2aabbfe06700 (LWP 9345)]\r\n[New Thread 0x2aabc0007700 (LWP 9346)]\r\n[New Thread 0x2aabc0208700 (LWP 9347)]\r\n\r\nThread 1 ""python3"" received signal SIGSEGV, Segmentation fault.\r\n0x00002aaba30a100e in cuewInit () from /usr/lib/python3/dist-packages/bpy.so\r\n(gdb) bt\r\n#0  0x00002aaba30a100e in cuewInit () from /usr/lib/python3/dist-packages/bpy.so\r\n#1  0x00002aaba22f8cd4 in ccl::device_cuda_init() () from /usr/lib/python3/dist-packages/bpy.so\r\n#2  0x00002aaba22eb34c in ccl::Device::available_devices() () from /usr/lib/python3/dist-packages/bpy.so\r\n#3  0x00002aaba21e0bc9 in ccl::get_device_types_func(_object*, _object*) () from /usr/lib/python3/dist-packages/bpy.so\r\n#4  0x00000000004e9b7f in PyCFunction_Call ()\r\n#5  0x00000000005372f4 in PyEval_EvalFrameEx ()\r\n#6  0x0000000000540f9b in PyEval_EvalCodeEx ()\r\n#7  0x00000000004ebd23 in ?? ()\r\n#8  0x00000000005c1797 in PyObject_Call ()\r\n#9  0x0000000000534d90 in PyEval_CallObjectWithKeywords ()\r\n#10 0x00002aaba19865cf in bpy_prop_enum_itemf_cb () from /usr/lib/python3/dist-packages/bpy.so\r\n#11 0x00002aaba1daf54a in RNA_property_enum_items_ex () from /usr/lib/python3/dist-packages/bpy.so\r\n#12 0x00002aaba1daf5f3 in RNA_property_enum_items () from /usr/lib/python3/dist-packages/bpy.so\r\n#13 0x00002aaba1daf992 in RNA_property_enum_value () from /usr/lib/python3/dist-packages/bpy.so\r\n#14 0x00002aaba1976b1f in pyrna_py_to_prop () from /usr/lib/python3/dist-packages/bpy.so\r\n#15 0x00002aaba1977739 in pyrna_struct_setattro () from /usr/lib/python3/dist-packages/bpy.so\r\n#16 0x0000000000593e39 in PyObject_SetAttr ()\r\n#17 0x0000000000537b71 in PyEval_EvalFrameEx ()\r\n#18 0x0000000000540f9b in PyEval_EvalCodeEx ()\r\n#19 0x00000000004ebd23 in ?? ()\r\n#20 0x00000000005c1797 in PyObject_Call ()\r\n#21 0x00002aaba1981e8c in bpy_app_generic_callback () from /usr/lib/python3/dist-packages/bpy.so\r\n#22 0x00002aaba1f0130d in BLI_callback_exec () from /usr/lib/python3/dist-packages/bpy.so\r\n#23 0x00002aaba157c77e in WM_init () from /usr/lib/python3/dist-packages/bpy.so\r\n#24 0x00002aaba156ef15 in main_python_enter () from /usr/lib/python3/dist-packages/bpy.so\r\n#25 0x00002aaba197028d in bpy_module_delay_init () from /usr/lib/python3/dist-packages/bpy.so\r\n#26 0x00002aaba19702ed in dealloc_obj_dealloc () from /usr/lib/python3/dist-packages/bpy.so\r\n#27 0x00000000005a0189 in PyDict_SetItem ()\r\n#28 0x00000000005a114c in PyDict_SetItemString ()\r\n#29 0x000000000051ffb6 in PyModule_AddObject ()\r\n#30 0x00000000006100db in _PyImport_LoadDynamicModuleWithSpec ()\r\n#31 0x0000000000610538 in ?? ()\r\n#32 0x00000000004e9c36 in PyCFunction_Call ()\r\n#33 0x000000000053dbbb in PyEval_EvalFrameEx ()\r\n#34 0x0000000000540199 in ?? ()\r\n#35 0x000000000053c1d0 in PyEval_EvalFrameEx ()\r\n#36 0x000000000053b7e4 in PyEval_EvalFrameEx ()\r\n#37 0x000000000053b7e4 in PyEval_EvalFrameEx ()\r\n#38 0x000000000053b7e4 in PyEval_EvalFrameEx ()\r\n#39 0x000000000053b7e4 in PyEval_EvalFrameEx ()\r\n#40 0x0000000000540f9b in PyEval_EvalCodeEx ()\r\n#41 0x00000000004ebd23 in ?? ()\r\n#42 0x00000000005c1797 in PyObject_Call ()\r\n#43 0x00000000005c257a in _PyObject_CallMethodIdObjArgs ()\r\n#44 0x00000000005260c8 in PyImport_ImportModuleLevelObject ()\r\n#45 0x0000000000549e78 in ?? ()\r\n#46 0x00000000004e9ba7 in PyCFunction_Call ()\r\n#47 0x00000000005c1797 in PyObject_Call ()\r\n#48 0x0000000000534d90 in PyEval_CallObjectWithKeywords ()\r\n#49 0x000000000053a1c7 in PyEval_EvalFrameEx ()\r\n#50 0x0000000000540199 in ?? ()\r\n#51 0x0000000000540e4f in PyEval_EvalCode ()\r\n#52 0x000000000060c272 in ?? ()\r\n#53 0x000000000046b89f in PyRun_InteractiveOneObject ()\r\n#54 0x000000000046ba48 in PyRun_InteractiveLoopFlags ()\r\n#55 0x000000000046cfa0 in ?? ()\r\n#56 0x00000000004cf2bd in ?? ()\r\n#57 0x00000000004cfeb1 in main ()\r\n```\r\n\r\nIf I import in reverse order `gdb bt` does not show anything and here\'s the output:\r\n\r\n```\r\nimport bpy\r\n[New Thread 0x2aaacbe00700 (LWP 9835)]\r\n[New Thread 0x2aaacc001700 (LWP 9836)]\r\n[New Thread 0x2aaacc202700 (LWP 9837)]\r\n[New Thread 0x2aaacc403700 (LWP 9838)]\r\n[New Thread 0x2aaacc604700 (LWP 9839)]\r\n[New Thread 0x2aaacc805700 (LWP 9840)]\r\n[New Thread 0x2aaacca06700 (LWP 9841)]\r\n[New Thread 0x2aaaccc07700 (LWP 9842)]\r\n[New Thread 0x2aaacce08700 (LWP 9843)]\r\n[New Thread 0x2aaacd009700 (LWP 9844)]\r\n[New Thread 0x2aaacd20a700 (LWP 9845)]\r\n[New Thread 0x2aaacd40b700 (LWP 9846)]\r\n[New Thread 0x2aaacd60c700 (LWP 9847)]\r\n[New Thread 0x2aaacd80d700 (LWP 9848)]\r\n[New Thread 0x2aaacda0e700 (LWP 9849)]\r\n[New Thread 0x2aaacdc0f700 (LWP 9850)]\r\n[New Thread 0x2aaacde10700 (LWP 9851)]\r\n[New Thread 0x2aaace011700 (LWP 9852)]\r\n[New Thread 0x2aaace212700 (LWP 9853)]\r\n[New Thread 0x2aaace413700 (LWP 9854)]\r\n[New Thread 0x2aaace614700 (LWP 9855)]\r\n[New Thread 0x2aaace815700 (LWP 9856)]\r\n[New Thread 0x2aaacea16700 (LWP 9857)]\r\n[New Thread 0x2aaacec17700 (LWP 9858)]\r\n[New Thread 0x2aaacee18700 (LWP 9859)]\r\n[New Thread 0x2aaacf019700 (LWP 9860)]\r\n[New Thread 0x2aaacf21a700 (LWP 9861)]\r\n[New Thread 0x2aaacf41b700 (LWP 9862)]\r\n[New Thread 0x2aaacf61c700 (LWP 9863)]\r\n[New Thread 0x2aaacf81d700 (LWP 9864)]\r\n[New Thread 0x2aaacfa1e700 (LWP 9865)]\r\n[New Thread 0x2aaacfc1f700 (LWP 9866)]\r\n[New Thread 0x2aaacfe20700 (LWP 9867)]\r\n[New Thread 0x2aaad0021700 (LWP 9868)]\r\n[New Thread 0x2aaad0222700 (LWP 9869)]\r\n[New Thread 0x2aaad0423700 (LWP 9870)]\r\n[New Thread 0x2aaad0624700 (LWP 9871)]\r\n[New Thread 0x2aaad0825700 (LWP 9872)]\r\n[New Thread 0x2aaad0a26700 (LWP 9873)]\r\n[New Thread 0x2aaad0c27700 (LWP 9874)]\r\n[New Thread 0x2aaad0e28700 (LWP 9875)]\r\n[New Thread 0x2aaad1029700 (LWP 9876)]\r\n[New Thread 0x2aaad122a700 (LWP 9877)]\r\n[New Thread 0x2aaad142b700 (LWP 9878)]\r\n[New Thread 0x2aaad162c700 (LWP 9879)]\r\n[New Thread 0x2aaad182d700 (LWP 9880)]\r\n[New Thread 0x2aaad1a2e700 (LWP 9881)]\r\n[New Thread 0x2aaad1c2f700 (LWP 9882)]\r\n[New Thread 0x2aaadb1ce700 (LWP 9906)]\r\n>>> import torch\r\n[New Thread 0x2aaade103700 (LWP 9908)]\r\n[New Thread 0x2aaade304700 (LWP 9909)]\r\n[New Thread 0x2aaae2505700 (LWP 9910)]\r\n[New Thread 0x2aaae4706700 (LWP 9911)]\r\n[New Thread 0x2aaae6907700 (LWP 9912)]\r\n[New Thread 0x2aaae8b08700 (LWP 9913)]\r\n[New Thread 0x2aaaead09700 (LWP 9914)]\r\n[New Thread 0x2aaaecf0a700 (LWP 9915)]\r\n[New Thread 0x2aaaef10b700 (LWP 9916)]\r\n[New Thread 0x2aaaef30c700 (LWP 9917)]\r\n[New Thread 0x2aaaf150d700 (LWP 9918)]\r\n[New Thread 0x2aaaf570e700 (LWP 9919)]\r\n[New Thread 0x2aaaf790f700 (LWP 9920)]\r\n[New Thread 0x2aaaf9b10700 (LWP 9921)]\r\n[New Thread 0x2aaafbd11700 (LWP 9922)]\r\n[New Thread 0x2aaafdf12700 (LWP 9923)]\r\n[New Thread 0x2aab00113700 (LWP 9924)]\r\n[New Thread 0x2aab02314700 (LWP 9925)]\r\n[New Thread 0x2aab04515700 (LWP 9926)]\r\n[New Thread 0x2aab06716700 (LWP 9927)]\r\n[New Thread 0x2aab08917700 (LWP 9928)]\r\n[New Thread 0x2aab0ab18700 (LWP 9929)]\r\n[New Thread 0x2aab0cd19700 (LWP 9930)]\r\n[New Thread 0x2aab0ef1a700 (LWP 9931)]\r\n[New Thread 0x2aab1111b700 (LWP 9932)]\r\n[New Thread 0x2aab1331c700 (LWP 9933)]\r\n[New Thread 0x2aab1551d700 (LWP 9934)]\r\n[New Thread 0x2aab1771e700 (LWP 9935)]\r\n[New Thread 0x2aab1991f700 (LWP 9936)]\r\n[New Thread 0x2aab1bb20700 (LWP 9937)]\r\n[New Thread 0x2aab1dd21700 (LWP 9938)]\r\n[New Thread 0x2aab1ff22700 (LWP 9939)]\r\n[New Thread 0x2aab22123700 (LWP 9940)]\r\n[New Thread 0x2aab24324700 (LWP 9941)]\r\n[New Thread 0x2aab26525700 (LWP 9942)]\r\n[New Thread 0x2aab28726700 (LWP 9943)]\r\n[New Thread 0x2aab2a927700 (LWP 9944)]\r\n[New Thread 0x2aab2ab28700 (LWP 9945)]\r\n[New Thread 0x2aab2ed29700 (LWP 9946)]\r\n[New Thread 0x2aab30f2a700 (LWP 9947)]\r\n[New Thread 0x2aab3312b700 (LWP 9948)]\r\n[New Thread 0x2aab3532c700 (LWP 9949)]\r\n[New Thread 0x2aab3752d700 (LWP 9950)]\r\n[New Thread 0x2aab3972e700 (LWP 9951)]\r\n[New Thread 0x2aab3b92f700 (LWP 9952)]\r\n[New Thread 0x2aab3db30700 (LWP 9953)]\r\n[New Thread 0x2aab3fd31700 (LWP 9954)]\r\n[Thread 0x2aab1111b700 (LWP 9932) exited]\r\n[Thread 0x2aab3fd31700 (LWP 9954) exited]\r\n[Thread 0x2aab3752d700 (LWP 9950) exited]\r\n[Thread 0x2aab3532c700 (LWP 9949) exited]\r\n[Thread 0x2aab3312b700 (LWP 9948) exited]\r\n[Thread 0x2aab3b92f700 (LWP 9952) exited]\r\n[Thread 0x2aab3db30700 (LWP 9953) exited]\r\n[Thread 0x2aab3972e700 (LWP 9951) exited]\r\n[Thread 0x2aab30f2a700 (LWP 9947) exited]\r\n[Thread 0x2aab2ed29700 (LWP 9946) exited]\r\n[Thread 0x2aab2ab28700 (LWP 9945) exited]\r\n[Thread 0x2aab2a927700 (LWP 9944) exited]\r\n[Thread 0x2aab28726700 (LWP 9943) exited]\r\n[Thread 0x2aab26525700 (LWP 9942) exited]\r\n[Thread 0x2aab24324700 (LWP 9941) exited]\r\n[Thread 0x2aab22123700 (LWP 9940) exited]\r\n[Thread 0x2aab1ff22700 (LWP 9939) exited]\r\n[Thread 0x2aab1dd21700 (LWP 9938) exited]\r\n[Thread 0x2aab1bb20700 (LWP 9937) exited]\r\n[Thread 0x2aab1991f700 (LWP 9936) exited]\r\n[Thread 0x2aab1771e700 (LWP 9935) exited]\r\n[Thread 0x2aab1551d700 (LWP 9934) exited]\r\n[Thread 0x2aab1331c700 (LWP 9933) exited]\r\n[Thread 0x2aab0ef1a700 (LWP 9931) exited]\r\n[Thread 0x2aab0cd19700 (LWP 9930) exited]\r\n[Thread 0x2aab0ab18700 (LWP 9929) exited]\r\n[Thread 0x2aab08917700 (LWP 9928) exited]\r\n[Thread 0x2aab06716700 (LWP 9927) exited]\r\n[Thread 0x2aab04515700 (LWP 9926) exited]\r\n[Thread 0x2aab02314700 (LWP 9925) exited]\r\n[Thread 0x2aab00113700 (LWP 9924) exited]\r\n[Thread 0x2aaafdf12700 (LWP 9923) exited]\r\n[Thread 0x2aaafbd11700 (LWP 9922) exited]\r\n[Thread 0x2aaaf9b10700 (LWP 9921) exited]\r\n[Thread 0x2aaaf790f700 (LWP 9920) exited]\r\n[Thread 0x2aaaf570e700 (LWP 9919) exited]\r\n[Thread 0x2aaaf150d700 (LWP 9918) exited]\r\n[Thread 0x2aaaef30c700 (LWP 9917) exited]\r\n[Thread 0x2aaaef10b700 (LWP 9916) exited]\r\n[Thread 0x2aaaecf0a700 (LWP 9915) exited]\r\n[Thread 0x2aaaead09700 (LWP 9914) exited]\r\n[Thread 0x2aaae8b08700 (LWP 9913) exited]\r\n[Thread 0x2aaae6907700 (LWP 9912) exited]\r\n[Thread 0x2aaae4706700 (LWP 9911) exited]\r\n[Thread 0x2aaae2505700 (LWP 9910) exited]\r\n[Thread 0x2aaade304700 (LWP 9909) exited]\r\n[Thread 0x2aaade103700 (LWP 9908) exited]\r\n```', '@colesbury @goldsborough I wonder, did you investigate this more? At the moment, for me, it would be even helpful for me if you can tell me if you think this is something caused by `bpy` or `torch`.', '@Amir-Arsalan the segfault is happening on `bpy`, when trying to initialize cuda. ', '@fmassa @soumith @colesbury @goldsborough I posted a bug report for Blender: [here](https://developer.blender.org/T54561)', 'It turns out this happens because the rendering engine in `bpy` calls `cuInit()` --> #13883 ']",[],"['torch', 'torch', 'torch', 'import torch', 'torch']",0,0
550,pytorch,3914,closed,Broken ByteTensor indexing of Variables (still works for Tensors),"In PyTorch master 0.4 709fcfd, variables cannot be indexed.



This used to work in PyTorch 0.2 f964105:
",,"[""This is breaking some of Pyro's inference algorithms on PyTorch master: https://github.com/uber/pyro/blob/6e2e12b70b94e6fde2450fd9e94fddd32aa181f5/pyro/infer/trace_elbo.py#L119""]","['py\r\nimport torch\r\nfrom torch.autograd import Variable\r\nprint(torch.__version__)\r\n# 0.4.0a0+709fcfd\r\n\r\nx = Variable(torch.Tensor([0]))\r\nx.data[x.data == 1]\r\n# [torch.DoubleTensor with no dimension]\r\nx[x == 1]\r\n# *** RuntimeError: dimension out of range (expected to be in range of [-1, 0], but got 1)\r\n', 'py\r\nimport torch\r\nfrom torch.autograd import Variable\r\nprint(torch.__version__)\r\n# 0.2.0+f964105\r\n\r\nx = Variable(torch.Tensor([0]))\r\nx.data[x.data == 1]\r\n# [torch.FloatTensor with no dimension]\r\nx[x == 1]\r\n# Variable containing:[torch.FloatTensor with no dimension]\r\n']",[],0,0
551,pytorch,17971,closed,CuDNN error: CUDNN_STATUS_EXECUTION_FAILED,"## üêõ Bug
![2019-03-13 20-11-29](https://user-images.githubusercontent.com/12708080/54278734-2d791400-45ce-11e9-8926-64ffb9c5c2ed.png)
![2019-03-13 20-12-06](https://user-images.githubusercontent.com/12708080/54278743-34078b80-45ce-11e9-90ef-f2291ef9b95e.png)


## To Reproduce

Steps to reproduce the behavior:
Implementing such a network like this (or change from https://github.com/DrSleep/light-weight-refinenet  +  https://github.com/DrSleep/multi-task-refinenet)

![2019-03-13 20-39-56](https://user-images.githubusercontent.com/12708080/54279505-310d9a80-45d0-11e9-803a-0c7b5eeffb01.png)
![2019-03-13 20-39-35](https://user-images.githubusercontent.com/12708080/54279506-310d9a80-45d0-11e9-8429-bb7dfc595c70.png)
![2019-03-13 20-38-45](https://user-images.githubusercontent.com/12708080/54279508-31a63100-45d0-11e9-9463-71bd9e3ea21b.png)
![2019-03-13 20-38-34](https://user-images.githubusercontent.com/12708080/54279509-31a63100-45d0-11e9-87ee-bdd67041d22d.png)


and replace datasets.py to:
class Pad(object):
    """"""Pad image and mask to the desired size

    Args:
      size (int) : minimum length/width
      img_val (array) : image padding value
      msk_val (int) : mask padding value

    """"""
    def __init__(self, size, img_val, msk_val, dpt_val=None):
        self.size = size
        self.img_val = img_val
        self.msk_val = msk_val
        if dpt_val is None :
            self.dpt_val = msk_val
        else:
            self.dpt_val = dpt_val

    def __call__(self, sample):
        image, mask, depth = sample['image'], sample['mask'], sample['depth']
        h, w = image.shape[:2]
        h_pad = int(np.clip(((self.size - h) + 1)// 2, 0, 1e6))
        w_pad = int(np.clip(((self.size - w) + 1)// 2, 0, 1e6))
        pad = ((h_pad, h_pad), (w_pad, w_pad))
        image = np.stack([np.pad(image[:,:,c], pad,
                         mode='constant',
                         constant_values=self.img_val[c]) for c in range(3)], axis=2)
        mask = np.pad(mask, pad, mode='constant', constant_values=self.msk_val)
        depth = np.pad(depth, pad, mode='constant', constant_values=self.dpt_val)
        return {'image': image, 'mask': mask, 'depth': depth}

class RandomCrop(object):
    """"""Crop randomly the image in a sample.

    Args:
        output_size (tuple or int): Desired output size. If int, square crop
            is made.
    """"""

    def __init__(self, crop_size):
        assert isinstance(crop_size, int)
        self.crop_size = crop_size
        if self.crop_size % 2 != 0:
            self.crop_size -= 1

    def __call__(self, sample):
        image, mask, depth = sample['image'], sample['mask'], sample['depth']
        h, w = image.shape[:2]
        new_h = min(h, self.crop_size)
        new_w = min(w, self.crop_size)
        top = np.random.randint(0, h - new_h + 1)
        left = np.random.randint(0, w - new_w + 1)
        image = image[top: top + new_h,
                        left: left + new_w]
        mask = mask[top: top + new_h,
                    left: left + new_w]
        depth = depth[top: top + new_h,
                    left: left + new_w]
        return {'image': image, 'mask': mask, 'depth': depth}

class ResizeShorterScale(object):
    """"""Resize shorter side to a given value and randomly scale.""""""

    def __init__(self, shorter_side, low_scale, high_scale):
        assert isinstance(shorter_side, int)
        self.shorter_side = shorter_side
        self.low_scale = low_scale
        self.high_scale = high_scale

    def __call__(self, sample):
        image, mask, depth = sample['image'], sample['mask'], sample['depth']
        min_side = min(image.shape[:2])
        scale = np.random.uniform(self.low_scale, self.high_scale)
        if min_side * scale < self.shorter_side:
            scale = (self.shorter_side * 1. / min_side)
        image = cv2.resize(image, None, fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)
        mask = cv2.resize(mask, None, fx=scale, fy=scale, interpolation=cv2.INTER_NEAREST)
        depth = cv2.resize(depth, None, fx=scale, fy=scale, interpolation=cv2.INTER_NEAREST)
        return {'image': image, 'mask': mask, 'depth': depth}

class RandomMirror(object):
    """"""Randomly flip the image and the mask""""""

    def __init__(self):
        pass

    def __call__(self, sample):
        image, mask, depth = sample['image'], sample['mask'], sample['depth']
        do_mirror = np.random.randint(2)
        if do_mirror:
            image = cv2.flip(image, 1)
            mask = cv2.flip(mask, 1)
            depth = cv2.flip(depth, 1)
        return {'image': image, 'mask' : mask, 'depth': depth}

class Normalise(object):
    """"""Normalise a tensor image with mean and standard deviation.
    Given mean: (R, G, B) and std: (R, G, B),
    will normalise each channel of the torch.*Tensor, i.e.
    channel = (channel - mean) / std

    Args:
        mean (sequence): Sequence of means for R, G, B channels respecitvely.
        std (sequence): Sequence of standard deviations for R, G, B channels
            respecitvely.
    """"""

    def __init__(self, scale, mean, std):
        self.scale = scale
        self.mean = mean
        self.std = std

    def __call__(self, sample):
        image = sample['image']
        return {'image': (self.scale * image - self.mean) / self.std, 'mask' : sample['mask'], 'depth': sample['depth']}

class ToTensor(object):
    """"""Convert ndarrays in sample to Tensors.""""""

    def __call__(self, sample):
        image, mask, depth = sample['image'], sample['mask'], sample['depth']
        # swap color axis because
        # numpy image: H x W x C
        # torch image: C X H X W
        image = image.transpose((2, 0, 1))
        return {'image': torch.from_numpy(image),
                'mask': torch.from_numpy(mask),
                'depth': torch.from_numpy(depth)}

class NYUDataset(Dataset):
    """"""NYUv2-40""""""

    def __init__(
        self, data_file, data_dir, transform_trn=None, transform_val=None
        ):
        """"""
        Args:
            data_file (string): Path to the data file with annotations.
            data_dir (string): Directory with all the images.
            transform_{trn, val} (callable, optional): Optional transform to be applied
                on a sample.
        """"""
        with open(data_file, 'rb') as f:
            datalist = f.readlines()
        self.datalist = [(i,l,m) for i,l,m in map(lambda x: x.decode('utf-8').strip('\n').split('\t'), datalist)]
        self.root_dir = data_dir
        self.transform_trn = transform_trn
        self.transform_val = transform_val
        self.stage = 'train'

    def set_stage(self, stage):
        self.stage = stage

    def __len__(self):
        return len(self.datalist)

    def __getitem__(self, idx):
        #img_name = os.path.join(self.root_dir, self.datalist[idx][0])
        #msk_name = os.path.join(self.root_dir, self.datalist[idx][1])
        img_name = self.datalist[idx][0]
        msk_name = self.datalist[idx][1]
        dpt_name = self.datalist[idx][2]
        def read_image(x):
            img_arr = np.array(Image.open(x))
            if len(img_arr.shape) == 2: # grayscale
                img_arr = np.tile(img_arr, [3, 1, 1]).transpose(1, 2, 0)
            return img_arr
        image = read_image(img_name)
        mask = np.array(Image.open(msk_name))
        depth = np.array(Image.open(dpt_name))
        if img_name != msk_name:
            assert len(mask.shape) == 2, 'Masks must be encoded without colourmap'
        sample = {'image': image, 'mask': mask, 'depth': depth}
        if self.stage == 'train':
            if self.transform_trn:
                sample = self.transform_trn(sample)
        elif self.stage == 'val':
            if self.transform_val:
                sample = self.transform_val(sample)
        return sample



## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

conda install --yes --file requirements.txt

# This file may be used to create an environment using:
# $ conda create --name <env> --file <this file>
# platform: linux-64
atomicwrites=1.2.1=py36_0
attrs=18.2.0=py36h28b3542_0
backcall=0.1.0=py36_0
blas=1.0=mkl
ca-certificates=2018.03.07=0
certifi=2018.10.15=py36_0
cffi=1.11.5=py36he75722e_1
cloudpickle=0.6.1=py36_0
cudatoolkit=9.0=h13b8566_0
cudnn=7.1.2=cuda9.0_0
cycler=0.10.0=py36h93f1223_0
cython=0.29=py36he6710b0_0
cytoolz=0.9.0.1=py36h14c3975_1
dask-core=0.20.0=py36_0
dbus=1.13.2=h714fa37_1
decorator=4.3.0=py36_0
expat=2.2.6=he6710b0_0
fontconfig=2.13.0=h9420a91_0
freetype=2.9.1=h8a8886c_1
glib=2.56.2=hd408876_0
gst-plugins-base=1.14.0=hbbd80ab_1
gstreamer=1.14.0=hb453b48_1
icu=58.2=h9c2bf20_1
imageio=2.4.1=py36_0
intel-openmp=2019.0=118
ipython=7.1.1=py36h39e3cac_0
ipython_genutils=0.2.0=py36hb52b0d5_0
jedi=0.13.1=py36_0
jpeg=9b=h024ee3a_2
kiwisolver=1.0.1=py36hf484d3e_0
libedit=3.1.20170329=h6b74fdf_2
libffi=3.2.1=hd88cf55_4
libgcc-ng=8.2.0=hdf63c60_1
libgfortran-ng=7.3.0=hdf63c60_0
libpng=1.6.35=hbc83047_0
libstdcxx-ng=8.2.0=hdf63c60_1
libtiff=4.0.9=he85c1e1_2
libuuid=1.0.3=h1bed415_2
libxcb=1.13=h1bed415_1
libxml2=2.9.8=h26e45fe_1
markdown=2.6.8=py36_0
matplotlib=3.0.1=py36h5429711_0
mkl=2018.0.3=1
mkl_fft=1.0.6=py36h7dd41cf_0
mkl_random=1.0.1=py36h4414c95_1
more-itertools=4.3.0=py36_0
nccl=1.3.5=cuda9.0_0
ncurses=6.1=hf484d3e_0
networkx=2.2=py36_1
ninja=1.8.2=py36h6bb024c_1
numpy=1.15.3=py36h1d66e8a_0
numpy-base=1.15.3=py36h81de0dd_0
olefile=0.46=py36_0
openssl=1.0.2p=h14c3975_0
parso=0.3.1=py36_0
pcre=8.42=h439df22_0
pexpect=4.6.0=py36_0
pickleshare=0.7.5=py36_0
pillow=5.3.0=py36h34e0f95_0
pip=10.0.1=py36_0
pluggy=0.8.0=py36_0
prompt_toolkit=2.0.7=py36_0
ptyprocess=0.6.0=py36_0
py=1.7.0=py36_0
pycparser=2.19=py36_0
pydot=1.2.4=py36_0
pygments=2.2.0=py36h0d3125c_0
pyparsing=2.2.2=py36_0
pyqt=5.9.2=py36h05f1152_2
pytest=3.9.3=py36_0
pytest-runner=4.2=py36_0
python=3.6.6=h6e4f718_2
python-dateutil=2.7.5=py36_0
pytorch=0.4.1=py36_py35_py27__9.0.176_7.1.2_2
pytz=2018.7=py36_0
pywavelets=1.0.1=py36hdd07704_0
qt=5.9.6=h8703b6f_2
readline=7.0=h7b6447c_5
scikit-image=0.14.1=py36he6710b0_0
scipy=1.1.0=py36hfa4b5c9_1
setuptools=40.4.3=py36_0
sip=4.19.8=py36hf484d3e_0
six=1.11.0=py36_1
sqlite=3.25.2=h7b6447c_0
tk=8.6.8=hbc83047_0
toolz=0.9.0=py36_0
torchvision=0.2.1=py_2
tornado=5.1.1=py36h7b6447c_0
traitlets=4.3.2=py36_0
wcwidth=0.1.7=py36_0
wheel=0.32.2=py36_0
xz=5.2.4=h14c3975_4
zlib=1.2.11=ha838bed_2

condapip`, source): conda
 - Build command you used (if compiling from source): conda install --yes --file requirements.txt 
 - Python version: 3.6
 - CUDA/cuDNN version: 9.0/7.1.2
 - GPU models and configuration: GTX1070Ti
 - Any other relevant information:
",module: cudnn triaged,"['It seems the bug or problem of Pytorch 1.1. But why do you have such a problem in 0.4?', 'However, I fixed the bug, but not from codes. I change the python to Conda python, and the problem is solved. So the codes still not compatible with the original python3. When installing from Conda, the Cuda toolkit will be installed, and it might be the solution.\r\n\r\n\r\n@jeffreyksmithjr @vishwakftw ', 'if you runing pytorch in docker,  you shuld know that: https://github.com/NVIDIA/tacotron2/issues/109']",[],"['', '\r\n\r\n - PyTorch Version (e.g., 1.0):  0.4.1\r\n - OS (e.g., Linux): ubuntu 16.04.5\r\n - How you installed PyTorch (', ', ']",0,0
552,pytorch,15202,closed,The font size of equations in tutorial is too small,"The tutorial at: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#gradients
![image](https://user-images.githubusercontent.com/1032377/49982017-a95cb500-ff28-11e8-9465-7b0da9ce39c4.png)
",module: docs,"['Looks relatively OK on my machine, but I agree that your font looks really bad:\r\n![image](https://user-images.githubusercontent.com/5652049/49982250-a57d6280-ff29-11e8-9947-a3df825ff940.png)\r\nwhat browser and OS are you running?', '@zou3519\r\nBrowser is: Chromium Version 70.0.3538.110 (Official Build) Arch Linux (64-bit)\r\nOperating System: Arch Linux \r\nKDE Plasma Version: 5.14.4\r\nQt Version: 5.11.2\r\nKDE Frameworks Version: 5.52.0\r\nKernel Version: 4.19.4-arch1-1-ARCH\r\nOS Type: 64-bit\r\nProcessors: 20 √ó Intel¬Æ Core‚Ñ¢ i7-6950X CPU @ 3.00GHz\r\nMemory: 94.3 GiB of RAM\r\n', ""I don't know what causes the problem, but in the below screenshot, there is a circled CSS saying\r\n```CSS\r\ndisplay: inline-block; position: relative; width: 6.737em; height: 0px; font-size: 52%\r\n```\r\n\r\nManually changing it to `font-size: 100%;` would restore the font size to normal...\r\n\r\n![image](https://user-images.githubusercontent.com/1032377/49983855-2725be00-ff33-11e8-9ec7-1de8df1902b8.png)\r\n"", ""https://5c1420f1f7907c558e51f188--shiftlab-pytorch-tutorials.netlify.com/beginner/blitz/autograd_tutorial.html#gradients\r\n\r\nAre you still seeing the issue on that version? The `minScaleAdjust` of Mathjax's [HTML-CSS output processor](http://docs.mathjax.org/en/latest/options/output-processors/HTML-CSS.html#the-html-css-output-processor) has been bumped from `50` to `100`. Mathjax will try to fit the size of the equation to the surrounding text so I'm unsure why it would be rendering so small for you but this setting might keep it from doing so automatically."", ""@brsoff It's much better now:\r\n![image](https://user-images.githubusercontent.com/1032377/50029350-78c85a00-ffc0-11e8-9476-6431abf918a0.png)\r\n"", ""@brsoff \r\n> Are you still seeing the issue on that version? The `minScaleAdjust` of Mathjax's [HTML-CSS output processor](http://docs.mathjax.org/en/latest/options/output-processors/HTML-CSS.html#the-html-css-output-processor) has been bumped from `50` to `100`. \r\n\r\nThe link to HTML-CSS part is broken maybe [this](https://docs.mathjax.org/en/latest/output/html.html#html-output) you were pointing to\r\n\r\n""]",[],[],0,0
553,pytorch,15066,closed,[docs] Delete ffi documentation,there's still a page on it: https://pytorch.org/docs/master/ffi.html?highlight=ffi,module: docs,"['pytorch 1.0 was realsed in  Dec. 2018.    torch.utils.ffi is deprecated. this link show pytorch1.0 API, \r\nand you can see ffi API in pytoch0.41\r\nhttps://pytorch.org/docs/0.4.1/cpp_extension.html?highlight=torch%20utils%20cpp_extension', 'resolved by #15220 ', 'But the docs in https://pytorch.org/docs/master/notes/extending.html still mentions: ""WRITING CUSTOM C EXTENSIONS Example available at this GitHub repository.""']",[],[],0,0
554,pytorch,8987,closed,undefined reference to `onnx::GetEmptyStringAlreadyInited[abi:cxx11]()',"Hi, I met the following issue when build pytorch from source. I can build caffe2 successfully by using , but can not build pytorch by using . Does anyone know how to solve it? Thanks!

## Issue description

Failed to run 'bash tools/build_pytorch_libs.sh --use-nnpack caffe2 nanopb libshm gloo THD'

## Code example



## System Info
- PyTorch or Caffe2: PyTorch
- How you installed PyTorch (conda, pip, source): source
- Build command you used (if compiling from source): 
- OS: Ubuntu 16.04
- PyTorch version: source code cloned from the master branch
- Python version: 3.5.2
- CUDA/cuDNN version: N/A
- GPU models and configuration: N/A
- GCC version (if compiling from source): 5.4
- CMake version: 3.11.0
- Versions of any other relevant libraries:

-Wall -Wno-unused -Wno-attributes -Wno-unused-result -Wno-psabi -ffp-contract=off -fno-math-errno -fno-trapping-mathonnx::GetEmptyStringAlreadyInited[abi:cxx11]()'
> /home/xxx/Project/svn-store/PyTorch/pytorch-git/build/lib/libcaffe2.so: undefined reference to onnx::GetEmptyStringAlreadyInited[abi:cxx11]()'
> /home/xxx/Project/svn-store/PyTorch/pytorch-git/build/lib/libcaffe2.so: undefined reference to ",,"['I had the same issue.', 'Did you update the submodules? Also, could you use `python setup.py install` instead of running the bash script directly?', 'me too, during installation on CentOS release 6.7\r\nI used,  python setup.py install  as in installation instruction\r\nAnaconda2\r\n', 'I had the same issue. I run python setup.py install and get the same error.', ""@jamesarambam  @universewill same error as in `undefined reference to onnx::GetEmptyStringAlreadyInited[abi:cxx11]()' /home/xxx/Project/svn-store/PyTorch/pytorch-git/build/lib/libcaffe2.so: undefined reference toonnx::GetEmptyStringAlreadyInitedabi:cxx11'`? can you make sure that your onnx submodule is up to date?"", 'My Error : \r\n\r\n```\r\n+++++++++\r\nrunning install\r\nrunning build_deps\r\n+ USE_CUDA=0\r\n+ USE_ROCM=0\r\n+ USE_NNPACK=0\r\n+ USE_MKLDNN=0\r\n+ USE_GLOO_IBVERBS=0\r\n+ USE_DISTRIBUTED_MW=0\r\n+ FULL_CAFFE2=0\r\n+ [[ 7 -gt 0 ]]\r\n+ case ""$1"" in\r\n+ USE_NNPACK=1\r\n+ shift\r\n+ [[ 6 -gt 0 ]]\r\n+ case ""$1"" in\r\n+ USE_MKLDNN=1\r\n+ shift\r\n+ [[ 5 -gt 0 ]]\r\n+ case ""$1"" in\r\n+ break\r\n+ CMAKE_INSTALL=\'make install\'\r\n+ USER_CFLAGS=\r\n+ USER_LDFLAGS=\r\n+ [[ -n \'\' ]]\r\n+ [[ -n \'\' ]]\r\n+ [[ -n \'\' ]]\r\n++ dirname tools/build_pytorch_libs.sh\r\n+ cd tools/..\r\n+++ pwd\r\n++ printf \'%q\\n\' /home/jamess/Downloads/pytorch\r\n+ PWD=/home/jamess/Downloads/pytorch\r\n+ BASE_DIR=/home/jamess/Downloads/pytorch\r\n+ TORCH_LIB_DIR=/home/jamess/Downloads/pytorch/torch/lib\r\n+ INSTALL_DIR=/home/jamess/Downloads/pytorch/torch/lib/tmp_install\r\n+ THIRD_PARTY_DIR=/home/jamess/Downloads/pytorch/third_party\r\n+ CMAKE_VERSION=cmake\r\n+ C_FLAGS=\' -I""/home/jamess/Downloads/pytorch/torch/lib/tmp_install/include""   -I""/home/jamess/Downloads/pytorch/torch/lib/tmp_install/include/TH"" -I""/home/jamess/Downloads/pytorch/torch/lib/tmp_install/include/THC""   -I""/home/jamess/Downloads/pytorch/torch/lib/tmp_install/include/THS"" -I""/home/jamess/Downloads/pytorch/torch/lib/tmp_install/include/THCS""   -I""/home/jamess/Downloads/pytorch/torch/lib/tmp_install/include/THNN"" -I""/home/jamess/Downloads/pytorch/torch/lib/tmp_install/include/THCUNN""\'\r\n+ C_FLAGS=\' -I""/home/jamess/Downloads/pytorch/torch/lib/tmp_install/include""   -I""/home/jamess/Downloads/pytorch/torch/lib/tmp_install/include/TH"" -I""/home/jamess/Downloads/pytorch/torch/lib/tmp_install/include/THC""   -I""/home/jamess/Downloads/pytorch/torch/lib/tmp_install/include/THS"" -I""/home/jamess/Downloads/pytorch/torch/lib/tmp_install/include/THCS""   -I""/home/jamess/Downloads/pytorch/torch/lib/tmp_install/include/THNN"" -I""/home/jamess/Downloads/pytorch/torch/lib/tmp_install/include/THCUNN"" -DOMPI_SKIP_MPICXX=1\'\r\n+ LDFLAGS=\'-L""/home/jamess/Downloads/pytorch/torch/lib/tmp_install/lib"" \'\r\n+ LD_POSTFIX=.so\r\n++ uname\r\n+ [[ Linux == \\D\\a\\r\\w\\i\\n ]]\r\n+ LDFLAGS=\'-L""/home/jamess/Downloads/pytorch/torch/lib/tmp_install/lib""  -Wl,-rpath,$ORIGIN\'\r\n+ CPP_FLAGS=\' -std=c++11 \'\r\n+ GLOO_FLAGS=\r\n+ THD_FLAGS=\r\n+ NCCL_ROOT_DIR=/home/jamess/Downloads/pytorch/torch/lib/tmp_install\r\n+ [[ 0 -eq 1 ]]\r\n+ [[ 0 -eq 1 ]]\r\n+ [[ 0 -eq 1 ]]\r\n+ CWRAP_FILES=\'/home/jamess/Downloads/pytorch/torch/lib/ATen/Declarations.cwrap;/home/jamess/Downloads/pytorch/torch/lib/THNN/generic/THNN.h;/home/jamess/Downloads/pytorch/torch/lib/THCUNN/generic/THCUNN.h;/home/jamess/Downloads/pytorch/torch/lib/ATen/nn.yaml\'\r\n+ CUDA_NVCC_FLAGS=\' -I""/home/jamess/Downloads/pytorch/torch/lib/tmp_install/include""   -I""/home/jamess/Downloads/pytorch/torch/lib/tmp_install/include/TH"" -I""/home/jamess/Downloads/pytorch/torch/lib/tmp_install/include/THC""   -I""/home/jamess/Downloads/pytorch/torch/lib/tmp_install/include/THS"" -I""/home/jamess/Downloads/pytorch/torch/lib/tmp_install/include/THCS""   -I""/home/jamess/Downloads/pytorch/torch/lib/tmp_install/include/THNN"" -I""/home/jamess/Downloads/pytorch/torch/lib/tmp_install/include/THCUNN"" -DOMPI_SKIP_MPICXX=1\'\r\n+ [[ -z \'\' ]]\r\n+ CUDA_DEVICE_DEBUG=0\r\n+ \'[\' -z 32 \']\'\r\n+ BUILD_TYPE=Release\r\n+ [[ -n \'\' ]]\r\n+ [[ -n \'\' ]]\r\n+ echo \'Building in Release mode\'\r\nBuilding in Release mode\r\n+ mkdir -p torch/lib/tmp_install\r\n+ for arg in \'""$@""\'\r\n+ [[ caffe2 == \\n\\c\\c\\l ]]\r\n+ [[ caffe2 == \\g\\l\\o\\o ]]\r\n+ [[ caffe2 == \\c\\a\\f\\f\\e\\2 ]]\r\n+ pushd /home/jamess/Downloads/pytorch\r\n~/Downloads/pytorch ~/Downloads/pytorch\r\n+ build_caffe2\r\n+ mkdir -p build\r\n+ pushd build\r\n~/Downloads/pytorch/build ~/Downloads/pytorch ~/Downloads/pytorch\r\n+ cmake .. -DCMAKE_BUILD_TYPE=Release -DBUILD_CAFFE2=0 -DBUILD_ATEN=ON -DBUILD_PYTHON=0 -DBUILD_BINARY=OFF -DBUILD_SHARED_LIBS=ON -DONNX_NAMESPACE=onnx_torch -DUSE_CUDA=0 -DUSE_ROCM=0 -DUSE_NNPACK=1 -DCUDNN_INCLUDE_DIR= -DCUDNN_LIB_DIR= -DCUDNN_LIBRARY= -DUSE_MKLDNN=1 -DMKLDNN_INCLUDE_DIR=/home/jamess/anaconda2/bin/../include -DMKLDNN_LIB_DIR=/home/jamess/anaconda2/bin/../lib -DMKLDNN_LIBRARY=/home/jamess/anaconda2/bin/../lib/libmkldnn.so -DCMAKE_INSTALL_PREFIX=/home/jamess/Downloads/pytorch/torch/lib/tmp_install -DCMAKE_EXPORT_COMPILE_COMMANDS=1 -DCMAKE_C_FLAGS= -DCMAKE_CXX_FLAGS= -DCMAKE_EXE_LINKER_FLAGS= -DCMAKE_SHARED_LINKER_FLAGS=\r\n-- Need to define long as a separate typeid.\r\n-- std::exception_ptr is NOT supported.\r\n-- NUMA is not available\r\n-- Turning off deprecation warning due to glog.\r\n-- Building using own protobuf under third_party per request.\r\n-- Use custom protobuf build.\r\n-- Caffe2 protobuf include directory: $<BUILD_INTERFACE:/home/jamess/Downloads/pytorch/third_party/protobuf/src>$<INSTALL_INTERFACE:include>\r\n-- The BLAS backend of choice:MKL\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel_lp64: /home/jamess/anaconda2/lib/libmkl_intel_lp64.so\r\n--   Library mkl_gnu_thread: /home/jamess/anaconda2/lib/libmkl_gnu_thread.so\r\n--   Library mkl_core: /home/jamess/anaconda2/lib/libmkl_core.so\r\n--   Library gomp: -fopenmp\r\n--   Library pthread: /usr/lib64/libpthread.so\r\n--   Library m: /usr/lib64/libm.so\r\n--   Library dl: /usr/lib64/libdl.so\r\n-- Brace yourself, we are building NNPACK\r\nCMake Warning at cmake/Dependencies.cmake:310 (find_package):\r\n  By not providing ""FindEigen3.cmake"" in CMAKE_MODULE_PATH this project has\r\n  asked CMake to find a package configuration file provided by ""Eigen3"", but\r\n  CMake did not find one.\r\n\r\n  Could not find a package configuration file provided by ""Eigen3"" with any\r\n  of the following names:\r\n\r\n    Eigen3Config.cmake\r\n    eigen3-config.cmake\r\n\r\n  Add the installation prefix of ""Eigen3"" to CMAKE_PREFIX_PATH or set\r\n  ""Eigen3_DIR"" to a directory containing one of the above files.  If ""Eigen3""\r\n  provides a separate development package or SDK, be sure it has been\r\n  installed.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:181 (include)\r\n\r\n\r\n-- Did not find system Eigen. Using third party subdirectory.\r\n-- Could NOT find pybind11 (missing: pybind11_INCLUDE_DIR)\r\nCMake Error at cmake/Dependencies.cmake:803 (message):\r\n  The C++ compiler does not support required functions.  This is very likely\r\n  due to a known bug in GCC 5 (and maybe other versions) on Ubuntu 17.10 and\r\n  newer.  For more information, see:\r\n  https://github.com/pytorch/pytorch/issues/5229\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:181 (include)\r\n\r\n\r\n-- Configuring incomplete, errors occurred!\r\nSee also ""/home/jamess/Downloads/pytorch/build/CMakeFiles/CMakeOutput.log"".\r\nSee also ""/home/jamess/Downloads/pytorch/build/CMakeFiles/CMakeError.log"".\r\nFailed to run \'bash tools/build_pytorch_libs.sh --use-nnpack --use-mkldnn caffe2 nanopb libshm gloo THD\'\r\n\r\n++++++++++\r\n```', 'I am trying to install pytorch from source because of this (glibc version) issue https://github.com/pytorch/pytorch/issues/6607', 'FWIW: same issue here.', 'I run python setup.py install  and had the same issue', 'I run python setup.py install and had the same issue', 'I run python setup.py install and had the same issue', ""@jamesarambam your error message is different, and is exactly what it says on the tin. You'll need to use a different version of gcc."", ""@cycleuser @cenaliu @Rahul-Venugopal @jdsalmonson Could you please paste the error message which is causing build to fail? There are two distinct error messages in this issue already, and I don't know which one you are referring to."", ""@StatML Can you run the following commands:\r\n\r\n```\r\npython setup.py clean\r\npip uninstall onnx\r\npip uninstall onnx # (yes, twice please)\r\n```\r\n\r\nand the try rebuilding? If that doesn't work, can you make a fresh conda environment and rebuild?"", 'Hi  @SsnL @ezyang Thanks for your help! uninstalling ONNX then reinstalling does work. :)']",[],"['python3 setup_caffe2.py install', 'python3 setup.py install', 'python3 setup.py install', 'python3 setup.py install', '', '\r\n> xxx@yyy:~/Project/svn-store/PyTorch/pytorch-git$ sudo tools/build_pytorch_libs.sh  --use-nnpack caffe2 nanopb libshm gloo THD\r\n> + USE_CUDA=0\r\n> + USE_ROCM=0\r\n> + USE_NNPACK=0\r\n> + USE_MKLDNN=0\r\n> + USE_GLOO_IBVERBS=0\r\n> + USE_DISTRIBUTED_MW=0\r\n> + FULL_CAFFE2=0\r\n> + [[ 6 -gt 0 ]]\r\n> + case ""$1"" in\r\n> + USE_NNPACK=1\r\n> + shift\r\n> + [[ 5 -gt 0 ]]\r\n> + case ""$1"" in\r\n> + break\r\n> + CMAKE_INSTALL=\'make install\'\r\n> + USER_CFLAGS=\r\n> + USER_LDFLAGS=\r\n> + [[ -n \'\' ]]\r\n> + [[ -n \'\' ]]\r\n> + [[ -n \'\' ]]\r\n> ++ dirname tools/build_pytorch_libs.sh\r\n> + cd tools/..\r\n> +++ pwd\r\n> ++ printf \'%q\\n\' /home/xxx/Project/svn-store/PyTorch/pytorch-git\r\n> + PWD=/home/xxx/Project/svn-store/PyTorch/pytorch-git\r\n> + BASE_DIR=/home/xxx/Project/svn-store/PyTorch/pytorch-git\r\n> + TORCH_LIB_DIR=/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib\r\n> + INSTALL_DIR=/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/tmp_install\r\n> + THIRD_PARTY_DIR=/home/xxx/Project/svn-store/PyTorch/pytorch-git/third_party\r\n> + CMAKE_VERSION=cmake\r\n> + C_FLAGS=\' -I""/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/tmp_install/include""   -I""/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/tmp_install/include/TH"" -I""/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/tmp_install/include/THC""   -I""/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/tmp_install/include/THS"" -I""/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/tmp_install/include/THCS""   -I""/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/tmp_install/include/THNN"" -I""/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/tmp_install/include/THCUNN""\'\r\n> + C_FLAGS=\' -I""/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/tmp_install/include""   -I""/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/tmp_install/include/TH"" -I""/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/tmp_install/include/THC""   -I""/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/tmp_install/include/THS"" -I""/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/tmp_install/include/THCS""   -I""/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/tmp_install/include/THNN"" -I""/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/tmp_install/include/THCUNN"" -DOMPI_SKIP_MPICXX=1\'\r\n> + LDFLAGS=\'-L""/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/tmp_install/lib"" \'\r\n> + LD_POSTFIX=.so\r\n> ++ uname\r\n> + [[ Linux == \\D\\a\\r\\w\\i\\n ]]\r\n> + LDFLAGS=\'-L""/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/tmp_install/lib""  -Wl,-rpath,$ORIGIN\'\r\n> + CPP_FLAGS=\' -std=c++11 \'\r\n> + GLOO_FLAGS=\r\n> + THD_FLAGS=\r\n> + NCCL_ROOT_DIR=/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/tmp_install\r\n> + [[ 0 -eq 1 ]]\r\n> + [[ 0 -eq 1 ]]\r\n> + [[ 0 -eq 1 ]]\r\n> + CWRAP_FILES=\'/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/ATen/Declarations.cwrap;/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/THNN/generic/THNN.h;/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/THCUNN/generic/THCUNN.h;/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/ATen/nn.yaml\'\r\n> + CUDA_NVCC_FLAGS=\' -I""/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/tmp_install/include""   -I""/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/tmp_install/include/TH"" -I""/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/tmp_install/include/THC""   -I""/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/tmp_install/include/THS"" -I""/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/tmp_install/include/THCS""   -I""/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/tmp_install/include/THNN"" -I""/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/tmp_install/include/THCUNN"" -DOMPI_SKIP_MPICXX=1\'\r\n> + [[ -z \'\' ]]\r\n> + CUDA_DEVICE_DEBUG=0\r\n> + \'[\' -z \'\' \']\'\r\n> ++ getconf _NPROCESSORS_ONLN\r\n> + NUM_JOBS=4\r\n> + BUILD_TYPE=Release\r\n> + [[ -n \'\' ]]\r\n> + [[ -n \'\' ]]\r\n> + echo \'Building in Release mode\'\r\n> Building in Release mode\r\n> + mkdir -p torch/lib/tmp_install\r\n> + for arg in \'""$@""\'\r\n> + [[ caffe2 == \\n\\c\\c\\l ]]\r\n> + [[ caffe2 == \\g\\l\\o\\o ]]\r\n> + [[ caffe2 == \\c\\a\\f\\f\\e\\2 ]]\r\n> + pushd /home/xxx/Project/svn-store/PyTorch/pytorch-git\r\n> ~/Project/svn-store/PyTorch/pytorch-git ~/Project/svn-store/PyTorch/pytorch-git\r\n> + build_caffe2\r\n> + mkdir -p build\r\n> + pushd build\r\n> ~/Project/svn-store/PyTorch/pytorch-git/build ~/Project/svn-store/PyTorch/pytorch-git ~/Project/svn-store/PyTorch/pytorch-git\r\n> + cmake .. -DCMAKE_BUILD_TYPE=Release -DBUILD_CAFFE2=0 -DBUILD_ATEN=ON -DBUILD_PYTHON=0 -DBUILD_BINARY=OFF -DBUILD_SHARED_LIBS=ON -DONNX_NAMESPACE= -DUSE_CUDA=0 -DUSE_ROCM=0 -DUSE_NNPACK=1 -DCUDNN_INCLUDE_DIR= -DCUDNN_LIB_DIR= -DCUDNN_LIBRARY= -DUSE_MKLDNN=0 -DMKLDNN_INCLUDE_DIR= -DMKLDNN_LIB_DIR= -DMKLDNN_LIBRARY= -DCMAKE_INSTALL_PREFIX=/home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/tmp_install -DCMAKE_EXPORT_COMPILE_COMMANDS=1 -DCMAKE_C_FLAGS= -DCMAKE_CXX_FLAGS= -DCMAKE_EXE_LINKER_FLAGS= -DCMAKE_SHARED_LINKER_FLAGS=\r\n> -- Does not need to define long separately.\r\n> -- std::exception_ptr is supported.\r\n> -- NUMA is available\r\n> -- Current compiler supports avx2 extention. Will build perfkernels.\r\n> -- Building using own protobuf under third_party per request.\r\n> -- Use custom protobuf build.\r\n> -- Caffe2 protobuf include directory: $<BUILD_INTERFACE:/home/xxx/Project/svn-store/PyTorch/pytorch-git/third_party/protobuf/src>$<INSTALL_INTERFACE:include>\r\n> -- The BLAS backend of choice:Eigen\r\n> -- Brace yourself, we are building NNPACK\r\n> -- Found Numa  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libnuma.so)\r\n> -- Found system Eigen at /usr/local/include/eigen3\r\n> -- Found gcc >=5 and CUDA <= 7.5, adding workaround C++ flags\r\n> -- Could not find CUDA with FP16 support, compiling without torch.CudaHalfTensor\r\n> -- Removing -DNDEBUG from compile flags\r\n> -- Compiling with OpenMP support\r\n> -- MAGMA not found. Compiling without MAGMA support\r\n> -- Could not find hardware support for NEON on this machine.\r\n> -- No OMAP3 processor on this machine.\r\n> -- No OMAP4 processor on this machine.\r\n> -- SSE2 Found\r\n> -- SSE3 Found\r\n> -- AVX Found\r\n> -- AVX2 Found\r\n> -- Atomics: using GCC intrinsics\r\n> -- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_intel - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_gf - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_intel - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_gf - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_intel - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_gf - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_intel - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_gf - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_intel - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_gf - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_intel - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_gf - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl - guide - pthread - m]\r\n> --   Library mkl: not found\r\n> -- MKL library not found\r\n> -- Checking for [openblas]\r\n> --   Library openblas: /usr/lib/libopenblas.so\r\n> -- Found a library with BLAS API (open).\r\n> -- Found a library with LAPACK API. (open)\r\n> disabling CUDA because NOT USE_CUDA is set\r\n> -- CuDNN not found. Compiling without CuDNN support\r\n> -- Could NOT find MKLDNN (missing: MKLDNN_INCLUDE_DIR MKLDNN_LIBRARY) \r\n> -- MKLDNN not found. Compiling without MKLDNN support\r\n> -- GCC 5.4.0: Adding gcc and gcc_s libs to link line\r\n> disabling CUDA because USE_CUDA is set false\r\n> -- Configuring build for SLEEF-v3.2\r\n>    Target system: Linux-4.8.0-52-generic\r\n>    Target processor: x86_64\r\n>    Host system: Linux-4.8.0-52-generic\r\n>    Host processor: x86_64\r\n>    Detected C compiler: GNU @ /usr/bin/cc\r\n> -- Using option ', ' to compile libsleef\r\n> -- Building shared libs : OFF\r\n> -- MPFR : /usr/lib/x86_64-linux-gnu/libmpfr.so\r\n> -- MPFR header file in /usr/include\r\n> -- GMP : /usr/lib/x86_64-linux-gnu/libgmp.so\r\n> -- RUNNING_ON_TRAVIS : 0\r\n> -- COMPILER_SUPPORTS_OPENMP : 1\r\n> CMake Warning at CMakeLists.txt:341 (message):\r\n>   Generated cmake files are only fully tested if one builds with system glog,\r\n>   gflags, and protobuf.  Other settings may generate files that are not well\r\n>   tested.\r\n> \r\n> \r\n> -- \r\n> -- ******** Summary ********\r\n> -- General:\r\n> --   CMake version         : 3.11.0-rc3\r\n> --   CMake command         : /home/xxx/Downloads/software-tools/cmake-3.11.0-rc3-Linux-x86_64/bin/cmake\r\n> --   Git version           : \r\n> --   System                : Linux\r\n> --   C++ compiler          : /usr/bin/c++\r\n> --   C++ compiler version  : 5.4.0\r\n> --   BLAS                  : Eigen\r\n> --   CXX flags             : -msse3 -msse4.1 -msse4.2   --std=c++11  -fvisibility-inlines-hidden -DONNX_NAMESPACE= -D_FORCE_INLINES -D_MWAITXINTRIN_H_INCLUDED -D__STRICT_ANSI__ -fopenmp -O2 -fPIC -Wno-narrowing -Wall -Wextra -Wno-missing-field-initializers -Wno-type-limits -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-error=deprecated-declarations\r\n> --   Build type            : Release\r\n> --   Compile definitions   : USE_GCC_ATOMICS=1;HAVE_MMAP=1;_FILE_OFFSET_BITS=64;HAVE_SHM_OPEN=1;HAVE_SHM_UNLINK=1;HAVE_MALLOC_USABLE_SIZE=1\r\n> --   CMAKE_PREFIX_PATH     : \r\n> --   CMAKE_INSTALL_PREFIX  : /home/xxx/Project/svn-store/PyTorch/pytorch-git/torch/lib/tmp_install\r\n> -- \r\n> --   BUILD_CAFFE2          : 0\r\n> --   BUILD_ATEN            : ON\r\n> --   BUILD_BINARY          : OFF\r\n> --   BUILD_CUSTOM_PROTOBUF : ON\r\n> --     Link local protobuf : ON\r\n> --   BUILD_PYTHON          : 0\r\n> --   BUILD_SHARED_LIBS     : ON\r\n> --   BUILD_TEST            : OFF\r\n> --   USE_ASAN              : OFF\r\n> --   USE_ATEN              : OFF\r\n> --   USE_CUDA              : 0\r\n> --   USE_ROCM              : OFF\r\n> --   USE_EIGEN_FOR_BLAS    : ON\r\n> --   USE_FFMPEG            : OFF\r\n> --   USE_GFLAGS            : OFF\r\n> --   USE_GLOG              : OFF\r\n> --   USE_GLOO              : OFF\r\n> --   USE_LEVELDB           : OFF\r\n> --   USE_LITE_PROTO        : OFF\r\n> --   USE_LMDB              : OFF\r\n> --   USE_METAL             : OFF\r\n> --   USE_MKL               : \r\n> --   USE_MOBILE_OPENGL     : OFF\r\n> --   USE_MPI               : OFF\r\n> --   USE_NCCL              : OFF\r\n> --   USE_NERVANA_GPU       : OFF\r\n> --   USE_NNPACK            : 1\r\n> --   USE_OBSERVERS         : ON\r\n> --   USE_OPENCL            : OFF\r\n> --   USE_OPENCV            : OFF\r\n> --   USE_OPENMP            : OFF\r\n> --   USE_PROF              : OFF\r\n> --   USE_REDIS             : OFF\r\n> --   USE_ROCKSDB           : OFF\r\n> --   USE_ZMQ               : OFF\r\n> --   USE_DISTRIBUTED       : OFF\r\n> --   Public Dependencies  : \r\n> --   Private Dependencies : nnpack;cpuinfo;/usr/lib/x86_64-linux-gnu/libnuma.so;rt;gcc_s;gcc;dl\r\n> -- Configuring done\r\n> -- Generating done\r\n> -- Build files have been written to: /home/xxx/Project/svn-store/PyTorch/pytorch-git/build\r\n> + make install -j4\r\n> [  1%] Built target js_embed\r\n> [  5%] Built target cpuinfo\r\n> [  6%] Built target pthreadpool\r\n> [ 12%] Built target libprotobuf-lite\r\n> [ 12%] Built target onnxifi_loader\r\n> [ 12%] Built target c10\r\n> [ 12%] Built target common\r\n> [ 13%] Built target mkrename\r\n> [ 13%] Built target mkdisp\r\n> [ 13%] Built target mkmasked_gnuabi\r\n> [ 13%] Built target mkrename_gnuabi\r\n> [ 13%] Built target ATEN_CPU_FILES_GEN_TARGET\r\n> [ 13%] Built target arraymap\r\n> [ 14%] Built target mkalias\r\n> [ 19%] Built target nnpack\r\n> [ 19%] Built target renameAVX.h_generated\r\n> [ 20%] Built target nnpack_reference_layers\r\n> [ 20%] Built target renamedsp128.h_generated\r\n> [ 21%] Built target headers\r\n> [ 21%] Built target dispsse.c_generated\r\n> [ 21%] Built target renamedsp256.h_generated\r\n> [ 36%] Built target libprotobuf\r\n> [ 36%] Built target dispavx.c_generated\r\n> [ 36%] Built target renameSSE2.h_generated\r\n> [ 37%] Built target renameFMA4.h_generated\r\n> [ 37%] Built target renameAVX2.h_generated\r\n> [ 37%] Built target renameAVX2128.h_generated\r\n> [ 37%] Built target renameSSE4.h_generated\r\n> [ 37%] Built target caffe2_protos\r\n> [ 38%] Built target sleefavx\r\n> [ 38%] Built target dispsse_obj\r\n> [ 39%] Built target dispavx_obj\r\n> [ 40%] Built target sleefsse2\r\n> [ 40%] Built target sleeffma4\r\n> [ 40%] Built target sleefavx2\r\n> [ 41%] Built target sleefavx2128\r\n> [ 42%] Built target sleefsse4\r\n> [ 43%] Built target sleef\r\n> [ 61%] Built target libprotoc\r\n> [ 61%] Built target protoc\r\n> [ 62%] Built target gen_onnx_proto\r\n> [ 64%] Built target onnx_proto\r\n> [ 69%] Built target onnx\r\n> [ 93%] Built target caffe2\r\n> [ 94%] Linking CXX executable ../bin/wrapdim_test\r\n> [ 94%] Linking CXX executable ../bin/apply_utils_test\r\n> [ 94%] Linking CXX executable ../bin/dlconvertor_test\r\n> [ 94%] Linking CXX executable ../bin/scalar_test\r\n> /home/xxx/Project/svn-store/PyTorch/pytorch-git/build/lib/libcaffe2.so: undefined reference to ', ""onnx::GetEmptyStringAlreadyInited[abi:cxx11]()'\r\n> collect2: error: ld returned 1 exit status\r\n> collect2: error: ld returned 1 exit status\r\n> caffe2/CMakeFiles/wrapdim_test.dir/build.make:84: recipe for target 'bin/wrapdim_test' failed\r\n> make[2]: *** [bin/wrapdim_test] Error 1\r\n> CMakeFiles/Makefile2:1086: recipe for target 'caffe2/CMakeFiles/wrapdim_test.dir/all' failed\r\n> make[1]: *** [caffe2/CMakeFiles/wrapdim_test.dir/all] Error 2\r\n> make[1]: *** Waiting for unfinished jobs....\r\n> caffe2/CMakeFiles/dlconvertor_test.dir/build.make:84: recipe for target 'bin/dlconvertor_test' failed\r\n> make[2]: *** [bin/dlconvertor_test] Error 1\r\n> CMakeFiles/Makefile2:1011: recipe for target 'caffe2/CMakeFiles/dlconvertor_test.dir/all' failed\r\n> make[1]: *** [caffe2/CMakeFiles/dlconvertor_test.dir/all] Error 2\r\n> /home/xxx/Project/svn-store/PyTorch/pytorch-git/build/lib/libcaffe2.so: undefined reference to "", ""onnx::GetEmptyStringAlreadyInited[abi:cxx11]()'\r\n> collect2: error: ld returned 1 exit status\r\n> collect2: error: ld returned 1 exit status\r\n> caffe2/CMakeFiles/apply_utils_test.dir/build.make:84: recipe for target 'bin/apply_utils_test' failed\r\n> make[2]: *** [bin/apply_utils_test] Error 1\r\n> caffe2/CMakeFiles/scalar_test.dir/build.make:84: recipe for target 'bin/scalar_test' failed\r\n> CMakeFiles/Makefile2:897: recipe for target 'caffe2/CMakeFiles/apply_utils_test.dir/all' failed\r\n> make[1]: *** [caffe2/CMakeFiles/apply_utils_test.dir/all] Error 2\r\n> make[2]: *** [bin/scalar_test] Error 1\r\n> CMakeFiles/Makefile2:1124: recipe for target 'caffe2/CMakeFiles/scalar_test.dir/all' failed\r\n> make[1]: *** [caffe2/CMakeFiles/scalar_test.dir/all] Error 2\r\n> Makefile:140: recipe for target 'all' failed\r\n> make: *** [all] Error 2\r\n> \r\n"", '']",0,0
555,pytorch,828,closed,backward() in Autograd Index Function is broken when indexing with LongTensor,"In :


I think the index-copy statement doesn't work as intended.


I guess the statement should be something like
",high priority,"[""you're right. looks like a bug. will fix."", 'Fixed in #852.']","[""python\r\n    def backward(self, grad_output):\r\n        # TODO: this won't have to be zeroed\r\n        grad_input = grad_output.new(self.input_size).zero_()\r\n        grad_input.index(self.index).copy_(grad_output)\r\n        return grad_input\r\n"", '\r\nIn [15]: a = torch.zeros((3, 5))\r\n\r\nIn [16]: a\r\nOut[16]: \r\n\r\n 0  0  0  0  0\r\n 0  0  0  0  0\r\n 0  0  0  0  0\r\n[torch.FloatTensor of size 3x5]\r\n\r\nIn [17]: b = torch.ones((2, 5))\r\n\r\nIn [18]: a.index(torch.LongTensor([0, 2])).copy_(b)\r\nOut[18]: \r\n\r\n 1  1  1  1  1\r\n 1  1  1  1  1\r\n[torch.FloatTensor of size 2x5]\r\n\r\nIn [19]: a\r\nOut[19]: \r\n\r\n 0  0  0  0  0\r\n 0  0  0  0  0\r\n 0  0  0  0  0\r\n[torch.FloatTensor of size 3x5]\r\n', 'python\r\n        grad_input.index_copy_(0, self.index, grad_output)\r\n']",['torch/autograd/_functions/tensor.py:20'],0,0
556,pytorch,4858,closed,CUDA multinomial with replacement can select zero-probability events,"I'm running Ubuntu 16.04.3 on an AWS P3.2xlarge, with the NVIDIA libcuda1-384 package, version 384.111-0ubuntu0.16.04.1, and pytorch version 0.3.0.post4 and cuda90, installed via conda.

In some circumstances, the CUDA multinomial sampler with  can sample an event with zero probability.

The script below reproduces this. I haven't tried to reduce the distribution to a minimal case.  The output is below the script.

I've attached [the RNG state](https://github.com/pytorch/pytorch/files/1665704/failed_state.pt.gz) just prior to the failure. Reproduction from this state is demonstrated in the script.

Bests regards,
Alex

{SAVE_PATH}failed_state.pt",high priority,"[""Thanks for the detailed report. The bug is not reproducible on master with the provided check point. It's likely fixed with the many changes done to multinomial."", 'Hold on. I reproduced it.', 'I did not know this issue was open and also created a case [here](https://discuss.pytorch.org/t/bad-behavior-of-multinomial-function/10232/3). If it is any help, I found that it depends on the range of the values as well, if I change logits_range in the script below to 1 or 100 it does not happen.\r\n\r\n```\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\nimport torch.nn.functional as F\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\n\r\n\r\ndef test(n, hot=False, logits_range=10):\r\n    torch.manual_seed(1234)\r\n\r\n    logits = Variable(torch.Tensor(128, 50).uniform_(-logits_range, logits_range).cuda())\r\n\r\n    # Set randomly 40 elements per row to 0\r\n    mask = torch.zeros_like(logits).byte()\r\n    _, idx_mask = Variable(torch.Tensor(128, 50).uniform_(0, 1).cuda()).topk(40, 1)\r\n    mask.scatter_(1, idx_mask, True)\r\n\r\n    logits[mask] = -np.inf\r\n\r\n    probs = F.softmax(logits, dim=1)\r\n\r\n    assert (probs[mask] == 0).all()\r\n    assert (torch.abs(probs.sum(1) - 1) < 1e-6).all()\r\n\r\n    if hot:\r\n        with open(\'rng_state.pt\', \'rb\') as f:\r\n            rng_state = torch.load(f)\r\n        torch.cuda.set_rng_state(rng_state)\r\n\r\n    for j in tqdm(range(n)):\r\n\r\n        rng_state = torch.cuda.get_rng_state()\r\n\r\n        sample = probs.multinomial(1).squeeze(-1)\r\n        mask_sample = mask.gather(1, sample.unsqueeze(-1)).squeeze(-1)\r\n\r\n        if mask_sample.any():\r\n            print(""Sampled value that was masked and had probability 0 in iteration {}"".format(j))\r\n            wrong = torch.nonzero(mask_sample).squeeze(-1)\r\n            print(""Wrong samples: indices {}, sampled {}, probs {}"".format(\r\n                wrong.data.cpu().numpy().tolist(),\r\n                sample[wrong].data.cpu().numpy().tolist(),\r\n                probs[wrong, sample[wrong]].data.cpu().numpy().tolist()\r\n            ))\r\n\r\n            if hot:\r\n                break\r\n\r\n            with open(\'rng_state.pt\', \'wb\') as f:\r\n                torch.save(rng_state, f)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    with torch.cuda.device(0):\r\n        test(100000, hot=False)\r\n```', 'Hi,\r\n\r\nthank you @coventry for the test case. I think I found the bug, your code was invaluable in finding and testing a fix.\r\n\r\nBest regards\r\n\r\nThomas\r\n\r\nP.S.: Please ignore the suboptimal workarounds before the pull-request.', '@soumith this can be closed', ""I have seen this happen with PyTorch 0.4.0.\r\nInterestingly, the zero-probability event that was selected happened to be index 0 out of 255, even though there were many other zero-probability indices - so the fact that index zero was chosen may be a clue?\r\nIn my case the number of samples (n_samples) was 1, which seems to map to the optimized flavor  'sampleMultinomialOnce()'.\r\nI have not been able to consistently reproduce this so far."", 'I\'m still kind of proud for debugging and fixing this (one of my early deep into CUDA patches and I did most of the searching while on the train with a sketchy connection to the GPU computer), but I only improved the binary search based multi-item case, not the sampleMultinomialOnce.\r\n\r\nThis reproduces the problem for me:\r\n```\r\nimport torch\r\na = torch.zeros(3421,2, device=""cuda"")\r\na[:,1] = 1\r\ntorch.cuda.manual_seed(5214)\r\nb = torch.multinomial(a, 1)\r\nassert b.min().item()>0\r\n```\r\nI can take a look, but I herd whispers that the random functions might be in for a rework anyway.\r\n(I just took a large vector of 0-1 probs and looped over the manual seeds to find this.)', ""Yes, I can reproduce the issue with your code. I was also able to create my own reproducible test case, but yours is simpler so let's stick with that.\r\n\r\n@soumith @apaszke can we reopen this bug? \r\n"", 'It looks like the problem is broader than we initially thought. Zero probability events can get selected anywhere in the distribution, not only just if they are the first or last bin. The script below reproduces the problem in bins in the middle of the distribution the in just a few seconds. \r\n\r\n... \r\n\r\n```\r\nimport sys\r\nimport torch\r\n\r\nbatch_size = 1024\r\ndist_size = 2048\r\ntorch.manual_seed(1)\r\ntorch.cuda.manual_seed(1)\r\nweights = torch.zeros(batch_size, dist_size).cuda()\r\nwith torch.no_grad():\r\n    k = 0\r\n    while True:\r\n        if k % 10 == 0:\r\n            print(""iteration %d"" % k)\r\n        # create a different unnormalized PDF each time.\r\n        weights.uniform_(0.4, 0.6) # use rather large weights\r\n\r\n        # zero out about half of the probabilities\r\n        cond = torch.rand(batch_size, dist_size).ge(0.5)\r\n        weights[cond] = 0\r\n        assert(weights.sum(dim=1).gt(0).all().item()) # make sure we didn\'t accidentally zero out all the weights in a distribution\r\n\r\n        # Sample\r\n        s = weights.multinomial(1).squeeze(1)\r\n\r\n        # Check if any selected samples had zero probability\r\n        selected_probs = weights[torch.arange(batch_size, dtype=torch.long), s]\r\n        if not selected_probs.eq(0).sum() == 0:\r\n            # Bug detected. Print out some debug info.\r\n            print(""\\nError: Zero-probability event sampled!"")\r\n            for i in range(batch_size):\r\n                if selected_probs[i] == 0:\r\n                        print(""bin %d out of %d in distribution %d was chosen even though its probability is %.9f"" %(s[i], dist_size, i, selected_probs[i]))\r\n            sys.exit(1)\r\n        k = k + 1\r\n```\r\n**Output:**\r\n```\r\niteration 0\r\niteration 10\r\niteration 20\r\niteration 30\r\niteration 40\r\niteration 50\r\niteration 60\r\niteration 70\r\niteration 80\r\n\r\nError: Zero-probability event sampled!\r\nbin 1679 out of 2048 in distribution 461 was chosen even though its probability is 0.000000000\r\n```', 'Another finding from the thread for @t-vi\'s pull request  (https://github.com/pytorch/pytorch/pull/9960) is that there appears to be a problem with some of the floating point calculations in the code.\r\n\r\nFor example, I added a this printout:\r\n```printf(""Zero probability selected! cat=%d, probability=%.9g, sample=%.9g, prevBucket=%.9g, curBucket=%.9g\\n"", cat, prob, sample, prevBucket, curBucket);```\r\nat this line:\r\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/THC/THCTensorRandom.cuh#L263\r\n\r\nand, when running the above script, got the output:\r\n```Zero probability selected! cat=1679, probability=0, sample=0.811896265, prevBucket=0.811896205, curBucket=0.811896265```\r\n\r\nNote how ```prevBucket``` and ```curBucket```, which should be equal since this is a zero-probability bin, are in fact slightly different. This leads to the bin getting selected even though it has zero probability because this condition evaluates as true:\r\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/THC/THCTensorRandom.cuh#L257\r\n\r\nAssuming that such small drifts in floating point calculations are hard to avoid, maybe the condition should be made to work with a tolerance rather than a hard ```>``` check? For example, we could select the bin only if curBucket > prevBucket + eps.\r\n\r\n@t-vi @apaszke @soumith - Thoughts?\r\n\r\nEdit: updated the printout to also include the uniform sample value.', 'oh, and just wanted to thank @t-vi for the fix candidates and great discussion!', 'Specifically, we could do something like this:\r\nChange\r\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/THC/THCTensorRandom.cuh#L257\r\nto \r\n```\r\nbool inBucket =\r\n        (cat < categories) &&\r\n        (!THCNumerics<T>::gt(sample, curBucket)) &&\r\n        (THCNumerics<T>::gt(sample, prevBucket)) &&\r\n        (THCNumerics<T>::gt(THCNumerics<T>::sub(curBucket, prevBucket), ScalarConvert<float, T>::to(1e-07)));\r\n```\r\n... except that the threshold (1e-7 in this example) would need to depend on the data type T, and there is still some question how exactly to choose it.\r\n\r\nStill, I tried this and was no longer able to produce zero-probability events, at least with preliminary (short) testing.\r\n', ""I'd still think we should be able to take it apart to know why it fails here. I'll give it another shot with debugging - hopefully also finding out why the same kernel seems to be behaving differently in the extension and in THC."", '@t-vi  Any progress on this? ', 'The PR fixes it.']",['\r\n\r\n'],"['replacement=True', '', ""python\r\nimport torch as T\r\nimport os\r\n\r\nfreqs = T.cuda.FloatTensor([\r\n    0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\r\n    0.03178183361887932, 0.027680952101945877, 0.033176131546497345,\r\n    0.046052902936935425, 0.07742464542388916, 0.11543981730937958,\r\n    0.14148041605949402, 0.15784293413162231, 0.13180233538150787,\r\n    0.08271478116512299, 0.049702685326337814, 0.027557924389839172,\r\n    0.018125897273421288, 0.011851548217236996, 0.010252203792333603,\r\n    0.007422595750540495, 0.005372154992073774, 0.0045109698548913,\r\n    0.0036087757907807827, 0.0035267581697553396, 0.0018864056328311563,\r\n    0.0024605290964245796, 0.0022964938543736935, 0.0018453967059031129,\r\n    0.0010662291897460818, 0.0009842115687206388, 0.00045109697384759784,\r\n    0.0007791675161570311, 0.00020504408166743815, 0.00020504408166743815,\r\n    0.00020504408166743815, 0.00012302644609007984, 0.0,\r\n    0.00012302644609007984, 4.100881778867915e-05, 0.0, 0.0, 0.0, 0.0,\r\n    0.0, 0.0])\r\n\r\nfor i in range(1_000_000_000):\r\n    state = T.cuda.get_rng_state()\r\n    sample = T.multinomial(freqs, 1000, True)\r\n    if freqs[sample].min() == 0:\r\n        break\r\n\r\nprint(72*'-')\r\nprint(f'failure after {i} iterations')\r\nsample_idx = (freqs[sample] == 0).nonzero()[0][0]\r\nsampled = sample[sample_idx]\r\nprint(f'{sample_idx}th element of last sample was {sampled}, '\r\n      f'which has probability {freqs[sampled]}')\r\n\r\nprint(72*'-')\r\nSAVE_PATH = 'failed_state.pt'\r\nT.save(state, open(SAVE_PATH, 'wb'))\r\nprint(f'\\nRNG state saved to "", ""')\r\n\r\nT.cuda.set_rng_state(T.load(open(SAVE_PATH, 'rb')))\r\nassert freqs[T.multinomial(freqs, 1000, True)].min() == 0\r\n\r\nprint(72*'-')\r\nprint(f'\\npytorch version: {T.__version__}\\n')\r\nprint(72*'-')\r\nprint('Operating system:')\r\nos.system('lsb_release -a')\r\nprint(72*'-')\r\nprint('\\nGPU information:')\r\nos.system('nvidia-smi')\r\nprint(72*'-')\r\nprint('\\nCUDA library information:')\r\nos.system('dpkg -l | grep -i cuda')\r\n\r\nubuntu@ip-172-30-1-60:~/workdir$ python -i fail.py \r\n------------------------------------------------------------------------\r\nfailure after 4309 iterations\r\n631th element of last sample was 0, which has probability 0.0\r\n------------------------------------------------------------------------\r\n\r\nRNG state saved to "", '\r\n------------------------------------------------------------------------\r\n\r\npytorch version: 0.3.0.post4\r\n\r\n------------------------------------------------------------------------\r\nOperating system:\r\nNo LSB modules are available.\r\nDistributor ID: Ubuntu\r\nDescription:    Ubuntu 16.04.3 LTS\r\nRelease:        16.04\r\nCodename:       xenial\r\n------------------------------------------------------------------------\r\n\r\nGPU information:\r\nThu Jan 25 21:17:03 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.111                Driver Version: 384.111                   |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla V100-SXM2...  Off  | 00000000:00:1E.0 Off |                    0 |\r\n| N/A   38C    P0    46W / 300W |   4558MiB / 16152MiB |     31%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0       988      C   python                                       738MiB |\r\n|    0     15193      C   python                                      3810MiB |\r\n+-----------------------------------------------------------------------------+\r\n------------------------------------------------------------------------\r\n\r\nCUDA library information:\r\nii  libcuda1-384                                    384.111-0ubuntu0.16.04.1     amd64                        NVIDIA CUDA runtime library\r\n', '']",0,0
557,pytorch,27820,closed,IterableDataset should be added into dataset.pyi,"## üêõ Bug
trying to import  in  creates an error in PyCharm. look at the following picture:
![ÂõæÁâá](https://user-images.githubusercontent.com/44257865/66712058-9c6b5500-edc9-11e9-9492-64c416939e7b.png)

It says that , where in , we can see that:
![ÂõæÁâá](https://user-images.githubusercontent.com/44257865/66712088-f9670b00-edc9-11e9-9ad4-417bc791c207.png)

It's obviously that there is no  but only its super class  exists.
I believe it should be added.


## To Reproduce
Steps to reproduce the behavior:

1. Open PyCharm
2. Write 
3. Error

## Expected behavior

No unresolved reference hinted.

## Environment


## Additional context


cc @SsnL @ezyang",module: dataloader module: typing triaged,"['Thanks for reporting! Would you mind submitting a PR to fix this?', 'Pull request submitted! Would you mind a look at #27966?\r\n@SsnL ', 'Still have this problem.\r\n`from .dataset import IterableDataset as IterableDataset` also need to be added into `torch\\utils\\data\\__init__.pyi`', ""ACK. I'll try opening up another pull request to solve this."", 'Pull request opened. Awaiting to be merged.\r\ncc @SsnL @ezyang', 'Wait a minute...\r\nThere still are some similar problems: `ChainDataset`, `get_worker_info` and `_DatasetKind`.\r\nJust now, I have checked the difference between `torch\\utils\\data\\__init__.pyi` and `torch\\utils\\data\\__init__.py`. These three:  `ChainDataset`, `get_worker_info` and `_DatasetKind` are imported in `__init__.py`, but not in `__init__.pyi`.  \r\nShould they also be added into  `__init__.pyi`?\r\n@DuckSoft ', 'Maybe we need an official answer, then. I am confused.', '> Wait a minute...\r\n> There still are some similar problems: `ChainDataset`, `get_worker_info` and `_DatasetKind`.\r\n> Just now, I have checked the difference between `torch\\utils\\data\\__init__.pyi` and `torch\\utils\\data\\__init__.py`. These three: `ChainDataset`, `get_worker_info` and `_DatasetKind` are imported in `__init__.py`, but not in `__init__.pyi`.\r\n> Should they also be added into `__init__.pyi`?\r\n> @DuckSoft\r\n\r\nWe got the official answers! I will put `ChainDataset`, `get_worker_info` in!', 'Hi, I am using the pytorch-nightly in our requirements.txt, but still have same issue `IterableDataset should be added into dataset.pyi`, can I know what is correct pytorch version I need to have this PR?']",['\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: Could not collect\r\n\r\nOS: Arch Linux\r\nGCC version: (GCC) 9.2.0\r\nCMake version: version 3.15.4\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: GPU 0: GeForce RTX 2060\r\nNvidia driver version: 435.21\r\ncuDNN version: /usr/lib/libcudnn.so.7.6.4\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.2\r\n[pip3] torch==1.2.0\r\n[pip3] torchvision==0.4.0a0\r\n[conda] Could not collect\r\n'],"['IterableDataset', 'torch.utils.data.dataset', 'cannot find reference in dataset.pyi', 'dataset.pyi', 'IterableDataset', 'Dataset', 'from torch.utils.data.dataset import IterableDataset']",0,0
558,pytorch,2733,closed,Pytorch in multi-cpu cluster,"Hi, 
I would like to use the distributed module to train a convolution net in a CPU cluster. Investigating your code, the function torch.cuda.device_count() is called in several places, and is used to populate the device_ids list. Since I don't have any GPU devices in my cluster, the method device_count() will always return 0 and any subsequent attempt to access device_ids[0] will result in an index exception.
Taking a naive path and changing device_count so that it always returns the number of nodes I intend to use then I get a different error: 

if not all(input.is_cuda for input in inputs):
   raise TypeError('Broadcast function not implemented for CPU tensors')

So I would like to ask you whether you have any plans to implement the distributed module to train networks in a multi-cpu cluster.

Many thanks",oncall: distributed triaged,"[""Just to confirm - when you say multi-cpu cluster you don't mean having 2 CPUs within a single computer (NUMA), but an actual cluster with multiple machines, right?\r\n\r\nThe problem is that `DistributedDataParallel` doesn't support non-CUDA networks right now. We should implement it at some point."", ""Yes I meant a cluster with multiple machines. \r\n\r\nNo problem, I'll keep an eye for new updates.\r\n\r\nThanks"", 'Hi\r\nI am trying to run distributeddataparallelCPU module for a simple model,\r\nHere is the file(test.py) I wrote:\r\n\r\n```\r\nimport torch.distributed as dist\r\nimport os\r\nimport argparse\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\'--local_rank\', default=0, type=int)\r\nargs = parser.parse_args()\r\ndist.init_process_group(backend=""mpi"", init_method=""env://"",\r\n                       world_size = int(os.environ[""WORLD_SIZE""]), rank=args.local_rank)\r\n## Model.\r\nclass Model(nn.Module):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.conv1 = nn.Conv2d(1, 20, 5)\r\n        self.conv2 = nn.Conv2d(20, 20, 5)\r\n\r\n    def forward(self, x):\r\n       x = F.relu(self.conv1(x))\r\n       return F.relu(self.conv2(x))\r\n\r\nmodel = Model() \r\nnet = torch.nn.parallel.DistributedDataParallelCPU(model)\r\n```\r\nand I call the file as below:\r\n```\r\npython -m torch.distributed.launch  --nnodes=1 --node_rank=0 --master_addr=127.0.0.1 --master_port=6006  test.py \r\n```\r\nI got the error that \r\nRuntimeError: the MPI backend is not available; try to recompile the THD package with MPI support at /opt/conda/conda-bld/pytorch_1532581333611/work/torch/lib/THD/process_group/General.cpp:17\r\n\r\nThanks for your assistance \r\n', 'Excuse me, does `DistributedDataParallel` support non-CUDA networks right now?', '@sth1997 @marcsv87 DDP on CPU devices should be available now (see [doc](https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/distributed.py#L174)). I am closing this one,  but feel free to reopen this issue if it fails to work for you. \r\n\r\n@rabeeh that seems to be a different issue than the original published one. If you still need assistance on that, could you please post create a new issue for it?\r\n\r\n']",[],[],0,0
559,pytorch,16428,closed,'swap' type in torch.nn.TripletMarginLoss,"## üìö Documentation

is it  instead of ?

https://pytorch.org/docs/stable/nn.html?highlight=tripletmarginloss#torch.nn.TripletMarginLoss


<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->
",module: docs,[],[],"['bool', 'float']",0,0
560,pytorch,6512,closed,Scalar operations are traced incorrectly,"This code:

prints

because it's not using the tensor overload. We should change the dispatch code to never use the scalar overloads when they really are tensors.",oncall: jit,"['This seems to be fixed:\r\n\r\n```\r\ngraph(%0 : Float(5, 2)\r\n      %1 : Float()) {\r\n  %2 : Float(5, 2) = aten::expand[size=[5, 2], implicit=1](%0)\r\n  %3 : Float(5, 2!) = aten::expand[size=[5, 2], implicit=1](%1)\r\n  %4 : Float(5, 2) = aten::add[alpha={1}](%2, %3)\r\n  return (%4);\r\n}\r\n```']","['python\r\nimport torch                                    \r\n                                                \r\nx = torch.randn(5, 2)                           \r\ny = torch.tensor(2.)                                                          \r\ndef fn(x, y):                                   \r\n    return x + y                                \r\ntrace, _ = torch.jit.get_trace_graph(fn, (x, y))\r\nprint(trace.graph())                            \r\n', '\r\ngraph(%0 : Float(5, 2)\r\n      %1 : Float()) {\r\n  %2 : Float(5, 2) = aten::add[other={2}, alpha={1}](%0)\r\n  return (%2);\r\n}\r\n']",[],0,0
561,pytorch,24639,closed,Migrate `std` from the TH to Aten (CUDA),"Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review.",better-engineering module: porting triaged,"[""Ok, so picture is (same for `std` and `var`) aten's `var` calls to `_var` when dims are not specified. Then `_var` calls to the th code `(var_single)`. \r\n\r\nSo - we have aten version for per-dim `std` and `var`, we have no optimized version when dims is not specified ( and doing `torch.var(x.reshape(-1),0)` is 5 times slower ).""]",[],[],0,0
562,pytorch,566,closed,rebuild pip wheels with manylinux,"For install instructions please go to http://pytorch.org

This is needed to work across many different linux distros, new and old.

manylinux will build the wheel on a CentOS5 (yes!) Docker machine.",todo,"[""this is really dumb, but after reading [PEP-513](https://www.python.org/dev/peps/pep-0513/) and looking at what TF and others do, all the user-failures came down to the name of the file.\r\n\r\nI fixed the filenames on the website pip install commands.\r\n\r\nI'm still going to setup manylinux wheels, but de-prioritizing this down to low."", ""Running `pip install pytorch` prints `RuntimeError: PyTorch does not currently provide packages for PyPI (see status at https://github.com/pytorch/pytorch/issues/566).` That doesn't seem related to this issue at all, maybe #939 was meant, or #707?"", ""If you don't support pypi, why send users here and not to http://pytorch.org/ ?\r\nOr even tell them directly to \r\n```\r\npip install http://download.pytorch.org/whl/torch-0.1.10.post1-cp35-cp35m-macosx_10_6_x86_64.whl\r\npip install torchvision \r\n```\r\netc, depending on the platform"", 'hi guys, I got an error when I execute the second command ""pip install torchvision"", and the error report is\r\n\r\n> Collecting torchvision\r\n> Using cached torchvision-0.1.8-py2.py3-none-any.whl\r\n> Collecting torch (from torchvision)\r\n> Using cached torch-0.1.2.post1.tar.gz\r\n>    Complete output from command python setup.py egg_info:\r\n>    Traceback (most recent call last):\r\n>      File ""\\<string\\>"", line 1, in \\<module\\>\r\n>      File ""/tmp/pip-build-hho2Nf/torch/setup.py"", line 11, in \\<module\\>\r\n>        raise RuntimeError(README)\r\n>    RuntimeError: PyTorch does not currently provide packages for PyPI (see status at https://github.com/pytorch/pytorch/issues/566).\r\n>  Please follow the instructions at http://pytorch.org/ to install with miniconda instead.\r\n>      ----------------------------------------\r\n> Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-build-hho2Nf/torch/\r\n\r\nThe problem that confused me is that ""/tmp/pip-build-hho2Nf/torch/setup.py"", where the error happens, seems to be a temporary file and I don\'t know what\'s in its line 11, so I can\'t start to debug the problem. Can any one tell me what is this setup.py file?\r\n\r\nMy computer needs proxy, but I don\'t think this is relevant because pip command works fine with other packages on my computer. Meanwhile, when I downloaded the torchvision whl file and try to install it locally, the same error happens.\r\n\r\nMy operating system is Linux mint 18, a derivative of Ubuntu 16 LTS. It strange that these two pip command in my laptop(also a mint 18 system) works fine.\r\n\r\nThank you for you help and time. \r\n\r\n\r\n', '@Lowpassfilter first install pytorch, and then install torchvision', '@soumith do you mean the following two commands:\r\n>pip install http://download.pytorch.org/whl/cu75/torch-0.1.12.post2-cp27-none-linux_x86_64.whl \r\n>pip install torchvision\r\n\r\neverything goes fine with the first command, however, the second command will produce the error I mentioned above.', 'problem is that it is using a cached file `Using cached torch-0.1.2.post1.tar.gz`. You can try to remove the pip cache: `rm -rf ~/.cache/pip`', 'I came across the same problem as Lowpossfilter, and this did not work', ""so uuuh. what's the status here? Those instructions don't work for me for me with the same exact error on the torchvision step:\r\n\r\nRuntimeError: PyTorch does not currently provide packages for PyPI (see status at https://github.com/pytorch/pytorch/issues/566)\r\n\r\n<strike>If this is explicitly not supported why are there instructions on the pytorch website for pip?</strike>\r\nSeems like things work fine for 2.7 but not 3.5 oddly enough. (speaking of which those instructions are off anyways since it's pip3 for python 3)\r\n"", ""@soumith I have done it, and this didn't work for me."", 'clueless, what are we supposed to do ?', '> problem is that it is using a cached file Using cached torch-0.1.2.post1.tar.gz. You can try to remove the pip cache: rm -rf ~/.cache/pip\r\n\r\nThat\'s not the problem here.\r\n\r\nThe ``torch-0.1.2`` tarball at https://pypi.python.org/pypi/torch is broken (the ``raise RuntimeError(README)`` error reported above). ``pip install torchvision`` tries to install ``torch`` as its dependency, and that\'s where things currently fail. To avoid that, one can try:\r\n\r\n    $ sudo apt-get install cmake   # if not already installed\r\n    $ pip install git+https://github.com/pytorch/pytorch\r\n\r\nThat will at least get past the issue in the ``setup.py`` file in the PyPI tarball, but gives me a compile error in the end:\r\n\r\n    In file included from /tmp/pip-D56ElD-build/torch/lib/TH/THVector.c:3:0:\r\n    /tmp/pip-D56ElD-build/torch/lib/TH/generic/THVectorDispatch.c: In function ‚ÄòTHFloatVector_vectorDispatchInit‚Äô:\r\n    /tmp/pip-D56ElD-build/torch/lib/TH/generic/simd/simd.h:114:3: error: inconsistent operand constraints in an ‚Äòasm‚Äô\r\n       asm volatile ( ""cpuid\\n\\t""\r\n       ^\r\n    /tmp/pip-D56ElD-build/torch/lib/TH/generic/simd/simd.h:114:3: error: inconsistent operand constraints in an ‚Äòasm‚Äô\r\n       asm volatile ( ""cpuid\\n\\t""\r\n       ^\r\n    make[2]: *** [CMakeFiles/TH.dir/THVector.c.o] Error 1\r\n    make[2]: *** Waiting for unfinished jobs....\r\n    make[1]: *** [CMakeFiles/TH.dir/all] Error 2\r\n    make: *** [all] Error 2\r\n\r\nI\'m on a quite standard Ubuntu install with gcc 4.8.4, so maybe current master is also not in an installable state right now.\r\n', 'looks like the error is back for me. What is the root of this problem? How can we find it?\r\n', 'same here!', 'yeah well, just go to:\r\n\r\nhttp://pytorch.org/\r\n\r\nand install it using the given address, in my case (osx 2.7 no cuda) was:\r\n\r\npip install http://download.pytorch.org/whl/torch-0.1.12.post2-cp27-none-macosx_10_7_x86_64.whl \r\npip install torchvision\r\n\r\nhave fun!', 'Those install instructions do not work for everybody. I\'ve just come across two more machines (of colleagues of mine) that have this error. It seems to occur at random and the error message is entirely unhelpful. \r\n\r\nIt seems odd that this bug is tagged ""low priority"" considering how many people use pip to install things. Difficulty of installation is the number one thing I\'ve seen turn away students and amateurs from choosing a first tool to learn. Perhaps I\'m not experienced enough to say, but this seems like something that affects the level to which pytorch will be adopted in the future.\r\n\r\nHow can we help to resolve this?', ""Agree with @ThaHypnotoad. I've been waiting for this fix for more than a month now. During the same month, I've gotten more comfortable with Tensorflow, which makes my switch to pytorch doubtful. \r\n\r\nWill opening a new 'issue' move things?"", 'I had the same problem and I\'ve found the solution. Basically, pip is trying to run ""pip install torch"" because torch is listed in the dependencies and it does not detect the previously build version with wheel. So just run ""pip install --no-deps torchvision"" and it should work.\r\n\r\nAnd this could be permanently fixed by updating the setup.py file in torchvision repository.', ""I can confirm that @DjAntaki s solution works on every machine I've seen have this issue on."", '@DjAntaki Thanks!', 'Still now I am not able to install pytorch\r\nI ran this command:\r\n```shell\r\n~ pip install http://download.pytorch.org/whl/cu75/torch-0.2.0.post1-cp36-cp36m-manylinux1_x86_64.whl \r\npip install torchvision\r\n```\r\nAnd the output which I am getting is:\r\n> torch-0.2.0.post1-cp36-cp36m-manylinux1_x86_64.whl is not a supported wheel on this platform.\r\nCollecting torchvision\r\n  Using cached torchvision-0.1.9-py2.py3-none-any.whl\r\nCollecting torch (from torchvision)\r\n  Using cached torch-0.1.2.post1.tar.gz\r\n    Complete output from command python setup.py egg_info:\r\n    Traceback (most recent call last):\r\n      File ""<string>"", line 1, in <module>\r\n      File ""/tmp/pip-build-Qy2Ipn/torch/setup.py"", line 11, in <module>\r\n        raise RuntimeError(README)\r\n    RuntimeError: PyTorch does not currently provide packages for PyPI (see status at https://github.com/pytorch/pytorch/issues/566).\r\n    \r\n    Please follow the instructions at http://pytorch.org/ to install with miniconda instead.\r\n    \r\n    \r\n    ----------------------------------------\r\nCommand ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-build-Qy2Ipn/torch/\r\n\r\nI am using Ubuntu 16.04 LTS. Is there any other way out ?', 'Check the version of ""pip"". If you have multiple pythons and pips, It could be either pip2 or pip3. I feel like the instructions should be more explicit om the website.', ""Observation:\r\n`$pip install torchvision` works\r\n`$sudo pip install torchvision` doesn't work. Not sure why."", 'Updates on this?', 'So this is a blocker why pytorch is not on PyPI?', 'currently, we are waiting on the PyPI team to increase our binary size limit, so that we can upload pytorch to PyPI. the wheels are built as manylinux1.', '@soumith any clue or issue when PyPi are going to increase the size?\r\n', 'I do not know. I have pinged them again. I hope they reply / increase the size soon.', 'So, anyone? one command to end this line! :beers:', 'CUT AND PASTE FROM ABOVE  FROM @DjAntaki:\r\nSo just run ""pip install --no-deps torchvision"" and it should work.\r\n\r\nThis worked for me on Ubuntu 16.04 LTS', ""@DjAntaki 's solution works.\r\n\r\nAnd i find another one : \r\nusing `virtualenv`\r\nnothing goes wrong on Ubuntu 16.04 LTS"", '@soumith How about contacting @dstufft? See https://github.com/pypa/packaging-problems/issues/86#issuecomment-263464389', 'it got approved recently. we are planning to upload the next version of pytorch on pypi', '@Lowpassfilter I have the same problem. And I found the reason is that the pip version is too old.\r\nSo I use ""pip install --upgrade pip"" to upgrade the pip. Then the problem disappeared.', '@thuwyq I work behind a proxy, and unfortunately,  the newest version of pip will fail behind a proxy, so I have to use the old version of pip. Lucky, the answer of @jlquinn  helps me. It works with --no-deps.', 'Please pin the comment from @DjAntaki to the top of this issue so people don¬¥t have to read all to the end. Simple solution for an annoying issue. ', '@soumith Looking forward to your upload in PyPI.', 'Is there still a plan to release PyTorch 0.3.0 on PyPI?', ""there is. i am trying to fix a segfault folks are seeing on the pip package (related to libstdc++ static linkage), once that's done I'll upload the packages on PyPI"", '@soumith Hey, any updates?', '@prajjwal1\r\nI had a similar error. Sudo solved it.', 'I had a similar error too. Sudo solved it.', ""I confirm that DjAntaki's solution worked for me on Ubuntu 17.10 with the system's builtin Python 3.6.3. The provided directions on the pytorch.org website are still out of date. The directions should be updated."", 'I can confirm that upgrading pip and using `sudo` works, though it feels a bit dirty. Any idea why `sudo` is needed after running `pip3 install --upgrade pip`?\r\n\r\nJust in case someone is confused, what I did to get this work (on Ubuntu 17.10):\r\n\r\n- `pip3 install http://download.pytorch.org/whl/cu90/torch-0.3.1-cp36-cp36m-linux_x86_64.whl`\r\n- `pip3 install --upgrade pip`\r\n- `sudo pip3 install torchvision`\r\n\r\nLooks like the last step changes permissions on pip and forces the use of sudo? Any idea how to make it work without sudo as well?', 'i have the same problem. i sloved it in this way:\r\npip3 install --upgrade pip\r\nsudo pip3 install torchvision\r\n\r\nmine is ubuntu16.04,cpu,python3.5', 'So, **PyTorch does not currently provide packages for PyPI** (at least for the moment).\r\nThe interpreter is trying to run `pip install torch` or `pip install torchvision`.\r\nThe solution that worked for me (In order to add pytorch module in working PyCharm project):\r\n\r\n1. Install **pytorch** from the website (pytorch.org) using pip or conda (miniconda in my case)\r\n2. Go to Settings -> Project Interpreter \r\n3. Go to **Add Local Python Interpreter** and choose `Existing environment`\r\n4. Choose your python environment where pytorch was installed with conda or pip.\r\n\r\n_My setup: Ubuntu 16.04, Python 2.7, miniconda_\r\n\r\nHope this helps!', 'What is the status on this?', 'this can be closed. we have pip wheels built and uploaded to PyPI', '`RuntimeError: PyTorch does not currently provide packages for PyPI (see status at https://github.com/pytorch/pytorch/issues/566).\r\n\r\nPlease follow the instructions at http://pytorch.org/ to install with miniconda instead.\r\n`\r\nI was using pip to install pytorch with the following command: \r\n`pip3 install torch torchvision`\r\nwhich I found in pytorch.org. Is this issue really resolved? \r\nOn my mac os everything works fine (python 3.6), however, when I tried to install it on a linux gpu server (python 3.4), I got the above error.', ""@salavi pytorch doesn't provide packages for 3.4. It only provides packages for 3.5, 3.6, 2.7 at the moment (and 3.7 from next release)"", 'win10\r\npip3 install torchvision  does not work\r\npip install torchvision works', 'import torch works on terminal,but unwork on pycharm(win7)', 'Where can I find a list for wheels???!! Older versions for ubuntu', 'hi ', 'hi ,I am in Win10 .Python version is 3.6.5. pip version is 19.0.3\r\nI can run pip install --no-deps torchvision and can install it successfully.\r\nBut, I try to install torch,It fail.\r\nI try to run these:\r\npip3 install torch\r\npip install torch\r\npip3 install --no-deps torch\r\npip install --no-deps torch\r\nAll fail.Get the same error :\r\n\r\nc:\\>pip install --no-deps torch -i  https://pypi.mirrors.ustc.edu.cn/simple/\r\nLooking in indexes: https://pypi.mirrors.ustc.edu.cn/simple/\r\nCollecting torch\r\n  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/5f/e9/bac4204fe9cb1a002ec6140b47f51affda1655379fe302a1caef421f9846/torch-0.1.2.post1.tar.gz\r\n    Complete output from command python setup.py egg_info:\r\n    Traceback (most recent call last):\r\n      File ""<string>"", line 1, in <module>\r\n      File ""C:\\Users\\65140\\AppData\\Local\\Temp\\pip-install-417u5z60\\torch\\setup.py"", line 11, in <module>\r\n        raise RuntimeError(README)\r\n    RuntimeError: PyTorch does not currently provide packages for PyPI (see status at https://github.com/pytorch/pytorch/issues/566).\r\n\r\n    Please follow the instructions at http://pytorch.org/ to install with miniconda instead.\r\n\r\n\r\n    ----------------------------------------\r\nCommand ""python setup.py egg_info"" failed with error code 1 in C:\\Users\\65140\\AppData\\Local\\Temp\\pip-install-417u5z60\\torch\\', 'I have solved this problem.\r\nYou should download whl file.\r\nI try this command:\r\n\r\npip3 install http://download.pytorch.org/whl/cpu/torch-0.4.1-cp36-cp36m-win_amd64.whl\r\npip3 install torchvision\r\n', 'This problem, I mean -> RuntimeError: PyTorch does not currently provide packages for PyPI (see status at #566). -> in my case (on Windows) was solved by installing the 64-bit version of Python and rewriting path variables to Python and its libraries. After updating the path try rebooting. And after that I could freely install PyTorch without any errors. ', '> ÊàëÊúâÂêåÊ†∑ÁöÑÈóÆÈ¢òÔºåÊàëÊâæÂà∞‰∫ÜËß£ÂÜ≥ÊñπÊ°à„ÄÇÂü∫Êú¨‰∏äÔºåpipÊ≠£Âú®Â∞ùËØïËøêË°å‚Äúpip install torch‚ÄùÔºåÂõ†‰∏∫ÁÅ´ÁÇ¨ÂàóÂú®‰æùËµñÈ°π‰∏≠Âπ∂‰∏îÂÆÉ‰∏ç‰ºöÊ£ÄÊµã‰ª•Ââç‰ΩøÁî®wheelÁöÑÊûÑÂª∫ÁâàÊú¨„ÄÇÊâÄ‰ª•ÔºåÂè™ÈúÄËøêË°å‚Äúpip install --no-deps torchvision‚ÄùÂç≥ÂèØ„ÄÇ\r\n> \r\n> ËøôÂèØ‰ª•ÈÄöËøáÊõ¥Êñ∞torchvisionÂ≠òÂÇ®Â∫ì‰∏≠ÁöÑsetup.pyÊñá‰ª∂Êù•Ê∞∏‰πÖ‰øÆÂ§ç„ÄÇ\r\n\r\nÊ≠£Âú®Â∞ùËØïËØ•ÊñπÊ≥ïÔºå‰∏çÊòØÊòØÂê¶ÂèØ‰ª•Ëß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢ò„ÄÇ', '(pytorch) C:\\Windows\\system32>pip install torch\r\nCollecting torch\r\n  Using cached https://files.pythonhosted.org/packages/5f/e9/bac4204fe9cb1a002ec6140b47f51affda1655379fe302a1caef421f9846/torch-0.1.2.post1.tar.gz\r\n    ERROR: Complete output from command python setup.py egg_info:\r\n    ERROR: Traceback (most recent call last):\r\n      File ""<string>"", line 1, in <module>\r\n      File ""C:\\Users\\PanPan\\AppData\\Local\\Temp\\pip-install-a6nog5b5\\torch\\setup.py"", line 11, in <module>\r\n        raise RuntimeError(README)\r\n    RuntimeError: PyTorch does not currently provide packages for PyPI (see status at https://github.com/pytorch/pytorch/issues/566).\r\n\r\n    Please follow the instructions at http://pytorch.org/ to install with miniconda instead.\r\n\r\n    ----------------------------------------\r\nERROR: Command ""python setup.py egg_info"" failed with error code 1 in C:\\Users\\PanPan\\AppData\\Local\\Temp\\pip-install-a6nog5b5\\torch\\', '@Man1029, you can get latest **torch** or **torchvision** package through command: \r\n`conda install pytorch-cpu torchvision-cpu -c pytorch` **(cpu only)**\r\nBasically, it uses conda packages available at [Anaconda cloud](https://anaconda.org/pytorch/repo?type=conda&label=main)', '> I had the same problem and I\'ve found the solution. Basically, pip is trying to run ""pip install torch"" because torch is listed in the dependencies and it does not detect the previously build version with wheel. So just run ""pip install --no-deps torchvision"" and it should work.\r\n> \r\n> And this could be permanently fixed by updating the setup.py file in torchvision repository.\r\n\r\nThanks buddy!']",[],[],0,0
563,pytorch,6140,closed,[jit][script] Support `dim` wrapping,"The following does not compile:

The expected behavior should be that the dim wraps around (to match torch semantics).

cc @zdevito ",oncall: jit,"['Nowadays I get this error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""dim.py"", line 6, in <module>\r\n    unsqueezel(x)\r\nRuntimeError: torch/csrc/jit/autodiff.cpp:199: createZerosLike: can\'t allocate zero gradient for a value without a type\r\n```', 'Though it works if i remove `requires_grad=True`. So closing this one and making a new issue']","['\r\nimport torch\r\n@torch.jit.script\r\ndef unsqueezel(x):\r\n    return x.unsqueeze(dim=-1)\r\nx = torch.randn((2, 2), requires_grad=True)\r\nunsqueezel(x)\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-8-6b2f4059fc46> in <module>()\r\n      1 import torch\r\n----> 2 @torch.jit.script\r\n      3 def unsqueezel(x):\r\n      4     return x.unsqueeze(dim=-1)\r\n      5 x = torch.randn((2, 2), requires_grad=True)\r\n\r\n~/pytorch/pytorch/torch/jit/__init__.py in script(fn)\r\n    528     rcb = createResolutionCallback()\r\n    529     ast = get_jit_ast(fn)\r\n--> 530     graph = _jit_script_compile(ast, rcb)\r\n    531     return torch._C.GraphExecutor(graph, True)\r\n    532\r\n\r\nRuntimeError:\r\nUnexpected kind of attribute value: 45:\r\n@torch.jit.script\r\ndef unsqueezel(x):\r\n    return x.unsqueeze(dim=-1)\r\n                           ~~ <--- HERE\r\n']",[],0,0
564,pytorch,956,closed,"load_lua yields ""object has no attribute 'running_var'"" for BatchNormalization","When I try to load OpenFace model (https://storage.cmusatyalab.org/openface-models/nn4.small2.v1.t7) as:

    model = load_lua('nn4.small2.v1.t7',unknown_classes=True),

then, to convert unknown classes. 
But, I got the errors at the line below:

torch/utils/serialization/read_lua_file.py"", line 269, in BatchNorm_reader
    obj.running_var = obj.running_var.pow(-2).add(-obj.eps)

Thank you.
",,"['This is a really old torch model.\r\nA fix would be to load it in LuaTorch and save it back. Then the issue should be fixed.\r\nSo:\r\n```\r\n$ th -lnn\r\n> m = torch.load(""nn4.small2.v1.t7"")\r\n> torch.save(""nn4.small2.v1.t7"", m)\r\n```', 'thank you, so much!!!\n\nBest Regards,\nTae-hoon Kim\n\nOn Fri, Mar 10, 2017 at 5:56 AM, Soumith Chintala <notifications@github.com>\nwrote:\n\n> This is a really old torch model.\n> A fix would be to load it in LuaTorch and save it back. Then the issue\n> should be fixed.\n> So:\n>\n> $ th -lnn\n> > m = torch.load(""nn4.small2.v1.t7"")\n> > torch.save(""nn4.small2.v1.t7"", m)\n>\n> ‚Äî\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/956#issuecomment-285480480>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AG2zCwnQmOO4e5F2hAj_hxgYJGFiSMIZks5rkGeUgaJpZM4MWa-v>\n> .\n>\n', 'I load and save the t7 model with the latest lua torch.\r\n\r\nBut when I try to load it in PyTorch, I get this error:\r\n\r\nFile ""/usr/local/lib/python3.5/site-packages/torch/utils/serialization/read_lua_file.py"", line 357, in storage_to_size\r\n    value = getattr(obj, attr)\r\nAttributeError: \'DepthConcat\' object has no attribute \'outputSize\'\r\n\r\nAnybody has the same problem?\r\n\r\n========\r\n\r\nOK, problem found.\r\nDepthConcat only creates \'outputSize\' field during forward:\r\n```lua\r\nfunction DepthConcat:updateOutput(input)\r\n   self.outputSize = self.outputSize or torch.LongStorage()\r\n```\r\nSo, the solution is\r\n1) load .t7 with latest lua\r\n2) forward a random tensor through it\r\n3) save to another .t7\r\n', ""The whole code looks like this:\r\n\r\nTorch:\r\n```\r\nrequire 'nn'\r\nrequire 'dpnn'\r\nrequire 'cunn'\r\nmodel = torch.load('n4.small2.v1.t7')\r\n\r\n-- Forward pass a random tensor\r\nx = torch.FloatTensor()\r\nmodel:forward(x:resize(1,3,96,96))\r\n\r\ntorch.save('nn4.small2.v1.resaved.t7',model)\r\n```\r\n\r\nThen in pytorch:\r\n```\r\nfrom torch.utils.serialization import load_lua\r\nmodel = load_lua('nn4.small2.v1.resaved.t7',unknown_classes=True)\r\n```\r\n"", ""@liorshk Is there a reason why the new `nn4.small2.v1.resaved.t7` produces a different output than the original one? I haven't changed anything else.""]",[],[],0,0
565,pytorch,4119,closed,CI with DEBUG=1,"Turning on DEBUG=1 turns off inlining. However, if you declare a function as , it won't get object code generated for it, which means there will be undefined symbol errors. You might not notice this with a release build, since everything gets inlined away.

We can catch errors like this by doing CI with DEBUG=1. Probably just pick one platform and run that only.",,[],[],['inline foo() { ... }'],0,0
566,pytorch,23826,closed,[quantization] Enable quantization OSS tests,These weren't run before. In progress: https://github.com/pytorch/pytorch/pull/23718,oncall: quantization quantization_release_1.3 triaged,[],[],[],0,0
567,pytorch,25971,closed,"Outdated exception message, ""Call init_rpc(name) first.""","## üêõ Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Call


got


cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera",module: rpc triaged,['Closing this issue since it is resolved:\r\n\r\n```\r\n>>> from torch.distributed import rpc\r\n>>> rpc.rpc_sync()\r\n>>> RuntimeError: RPC has not been initialized. Call torch.distributed.rpc.init_rpc first.'],"['\r\ndist.rpc(...)\r\n', '\r\n    raise RuntimeError(""RPC has not been initialized. ""\r\nRuntimeError: RPC has not been initialized. Call init_rpc(name) first.\r\n']",[],0,0
568,pytorch,24958,closed,"In Sparse Adam, initialize state['exp_avg']  and state['exp_avg_sq'] during optimizer initialization, not after step","## üöÄ Feature
In the Sparse Adam optimizer, [ https://github.com/pytorch/pytorch/blob/master/torch/optim/sparse_adam.py ]

The 'exp_avg' and 'exp_avg_sq' are only initialized after the first step is taken. And then after each step, there is a check of if the state hasn't been initialized. Compare this to adagrad https://github.com/pytorch/pytorch/blob/master/torch/optim/adagrad.py ], where 'sum' is initialized when the optimizer is initialized. Is there any reason for this? 

## Motivation

For those doing manipulations of 'exp_avg' and 'exp_avg_sq', the code would be clearer if they didn't have to run the first step before coding the manipulators. 

Also, unless I am missing something, the code would seem more cleaner if the state was initialized during the optimizer init, instead of checking if it's been initiated already during each step, though there's probably no performance reduction in the way it's currently coded. 

## Pitch

Move 


to the  __init__ method. 


## Additional context

Here's a full code sample




If there is agreement on this, I already created a pull request, here 

https://github.com/pytorch/pytorch/pull/24960

cc @vincentqb",module: optimizer triaged,"['@SsnL How do you feel the proposed?', ""This is BC breaking in the way that currently you can construct the optimizer before moving the optimized tensors returned by the iterable (e.g., `net.parameters()` and `net.to('cuda')`), and the patch breaks it.\r\n\r\nI know that we explicitly mentioned in doc that optimizers should always be constructed after moving parameters to different device/dtype. But I don't think that we should introduce BC breaking behavior just for a minor inconsistency with some other optimizers."", ""@SsnL should it be the adagrad optimizer's code that should be changed then? ""]","[""\r\n\t                    state['step'] = 0\r\n                    # Exponential moving average of gradient values\r\n                    state['exp_avg'] = torch.zeros_like(p.data)\r\n                    # Exponential moving average of squared gradient values\r\n                    state['exp_avg_sq'] = torch.zeros_like(p.data)\r\n"", '\r\ndef __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8):\r\n        if not 0.0 < lr:\r\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\r\n        if not 0.0 < eps:\r\n            raise ValueError(""Invalid epsilon value: {}"".format(eps))\r\n        if not 0.0 <= betas[0] < 1.0:\r\n            raise ValueError(""Invalid beta parameter at index 0: {}"".format(betas[0]))\r\n        if not 0.0 <= betas[1] < 1.0:\r\n            raise ValueError(""Invalid beta parameter at index 1: {}"".format(betas[1]))\r\n        defaults = dict(lr=lr, betas=betas, eps=eps)\r\n        super(SparseAdam, self).__init__(params, defaults)\r\n\r\n\tfor group in self.param_groups:\r\n            for p in group[\'params\']:\r\n                state = self.state[p]\r\n                state[\'exp_avg\'] = torch.zeros_like(p.data)\r\n                    # Exponential moving average of squared gradient values\r\n                state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\r\n\r\n    def step(self, closure=None):\r\n        """"""Performs a single optimization step.\r\n        Arguments:\r\n            closure (callable, optional): A closure that reevaluates the model\r\n                and returns the loss.\r\n        """"""\r\n        loss = None\r\n        if closure is not None:\r\n            loss = closure()\r\n\r\n        for group in self.param_groups:\r\n            for p in group[\'params\']:\r\n                if p.grad is None:\r\n                    continue\r\n                grad = p.grad.data\r\n                if not grad.is_sparse:\r\n                    raise RuntimeError(\'SparseAdam does not support dense gradients, please consider Adam instead\')\r\n\r\n                state[\'step\'] += 1\r\n\r\n                grad = grad.coalesce()  # the update is non-linear so indices must be unique\r\n                grad_indices = grad._indices()\r\n                grad_values = grad._values()\r\n                size = grad.size()\r\n\r\n']",[],0,0
569,pytorch,16574,closed,Make it possible to use mypy to typecheck code that uses PyTorch,"We generated a type stub for  module in , which is good enough to get autocomplete working in PyCharm. However, the type stub is not that well tested for actually, you know, accurately reflecting the types of our functions (we have light testing, but it's not enough) and it's unlikely that it's actually good enough to actually use mypy to typecheck PyTorch-using code. It seems like this is something that people might be interested in, so I'm creating this issue to track. Please emoji if this is something that you would use.

cc @ezyang @gchanan @zou3519",feature high priority module: typing triaged,"['We probably want to wait for python 2 to go away.', 'Is there a reason why we didn\'t mark this issue as ""triaged"" after removing ""triage review""?', ""Hiya @ezyang! I want to contribute towards this feature.\r\n\r\nI've read [CONTRIBUTING.md](https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md) and [contribution_guide.rst](https://github.com/pytorch/pytorch/blob/master/docs/source/community/contribution_guide.rst).\r\n\r\nSome questions for context on this particular issue ‚Äì my read of the progress made in https://github.com/pytorch/pytorch/pull/19089 is that while we've considered autogenerating stubs using tools like `gen_pyi.py`, we ultimately settled on simply writing `.pyi` files and keeping them up to date. Is that correct?\r\n\r\nSecond question ‚Äì if the above is true, do we have any means of enforcing that the `.pyi` files stay up-to-date with the `.py` files (say, using tests in CI), or would a PR that fixes this issue also need to enforce that property going forwards (by adding more tests to the CI suite)?"", ""It also seems that we're deprioritizing this until Python 2 goes away, but if it's all the same to folks, I'd be interested in doing the work anyways (even if that means adding annotations to Python 2-related code that is soon-to-be-removed.)"", ""> Second question ‚Äì if the above is true, do we have any means of enforcing that the .pyi files stay up-to-date with the .py files (say, using tests in CI), or would a PR that fixes this issue also need to enforce that property going forwards (by adding more tests to the CI suite)?\r\n\r\nI found [`pytorch/test_type_hints.py`](https://github.com/pytorch/pytorch/blob/59b14a762094825613ab061e4ff96586555d15ee/test/test_type_hints.py), I'll see where that leads me."", ""Hey @csvoss, nice to see a familiar face :)\r\n\r\nHere's my personal thinking on the matter. I've worked on a few (smaller than PyTorch) codebases with Python type checking, and the primary way we keep the type annotations correct in these settings is to actually typecheck all of the code in our codebase that uses the types, so you can check both inside and outside the function are consistent with your annotation. So, in the long term, that's the approach I'd want to take for typing PyTorch's external API--eating your own dogfood and all that.\r\n\r\nI'm not a mypy expert, so I don't know how doable it is to do this plan with stub files. I do know if I wrote all of the type annotations inline, I could just run mypy and have it take care of the rest, and so the act of refining all of our types would also be the same project as making PyTorch codebase statically typed. Switching to Python 3 only helps a lot, because we can slowly start moving annotations inline to Python code (since annotating Python code also causes it to be typechecked by mypy). In principle we could do this prior to Python 3, using Python 2 annotation syntax, but this would cause a lot of churn in the two months we have left before Python 2 dies... doesn't seem worth it.\r\n\r\nBut I agree with you that there is some useful legwork we can do before hand, which is to try to make our stubs more useful. The key is to have a corpus of program to typecheck against the stubs. The way `test_type_hints.py` works is it slurps up all of the example programs in our docstrings and then runs mypy on it (against our stubs). That's fine, but there's also a lot of missing coverage. We could add more programs, but I'm not too sure how to structure this. Maybe a good start would be to typecheck all of our tutorials and make sure they typecheck. Getting this all setup in our CI may be quite involved though.\r\n\r\nI'm always happy to accept patches that make types better, even if there is no testing for it. So you could push for that. But what would give us more leverage in the long term is figuring out a good testing strategy, e.g., laying the groundwork for typechecking all of pytorch with mypy. But I have no illusions about how much work this would be, so maybe patches to make types better is the right thing for now."", ""There are some functions and methods in Pytorch where the return type depends on the value of an argument. An example is `nonzero` with its `as_tuple` argument. Contrary to this comment in `__init__.pyi.in`, these functions can actually be typed correctly:\r\n```\r\n# The return value of this function depends on the value of `as_tuple`,\r\n# (similar to `unique`, `lu`, etc.); as such, it is not\r\n# possible to type correctly\r\ndef nonzero(input: Tensor, *, out: Optional[Tensor]=None, as_tuple: Optional[_bool]=None): ...\r\n```\r\nThis requires a combination of overloads and the `Literal` type. Unfortunately it doesn't exist in the stdlib until 3.8, but it is included in the `typing-extensions` package. Would including a dependency on `typing-extensions` in addition to `typing` be acceptable?"", ""I personally think `typing-extensions` is OK, but we'll have to update the packaging if/when we add it."", 'now that Python 2 has gone away, shall we start to prioritize this? :) we would highly benefit from this', 'Yes, though this is blocked on killing some FB internal users of Python 2 before we can move this repository onwards.', '@ezyang @rgommers glad seeing a PR! and saw in the news today that pytorch 1.5 will drop python 2 support, so very excited about the great progress here.\r\n\r\nwondering is there a plan for having the mypy support, what happens after the PR merges above?', ""Once we land Ralf's PR, we then will begin the process of inlining the pyi definitions into the py files and making the PyTorch codebase itself typecheckable. There will be a lot to do, and we're probably going to run some fixathon for it once we work out the basic playbook."", '@ezyang, thoughts on a PyTorch equivalent of [this](https://github.com/tensorflow/community/blob/master/rfcs/20200211-tf-types.md)?  Even a basic impl such as the one found in the [tf-addons repo](https://github.com/tensorflow/addons/blob/master/tensorflow_addons/utils/types.py) may make for a smoother transition to inlined type hints, especially with the ever-present `Union[str, torch.device]`, etc.', ""Well, I have zero objections to a `torch.types` module, in principle (in other projects I've done with types, having a types module has been handy). And it is true that generally you want to reduce the dependency a types modules has, so that it can be included anywhere.\r\n\r\nThat being said, I'm less clear about how you would go about putting the `Tensor` type in `torch.types`, since it has a lot of methods that we generate automatically via a type hinting script. The TF proposal proposes doing this using abstract base classes, but I think there would be technical difficulties with actually doing this in PyTorch, since our classes are bound from Python C API. Someone will have to look into this. BTW, the `tf-addons` script is a lot more practical, and we should definitely consider just putting these definitions directly in the relevant modules (to avoid having to deal with circular dependencies.)"", ""gh-37594 (for review, input very welcome) contains documentation on where we want to end up and how to start improving the type annotations for a module. \r\n\r\nWhen that PR is merged I plan to add a large checklist here as a tracking issue that people can assign to themselves and check off when done. That seems better than one issue per module, that'd be too noisy and a lot of overhead (some modules can be fixed very quickly)."", ""> a PyTorch equivalent of [this](https://github.com/tensorflow/community/blob/master/rfcs/20200211-tf-types.md)? Even a basic impl such as the one found in the [tf-addons repo](https://github.com/tensorflow/addons/blob/master/tensorflow_addons/utils/types.py) may make for a smoother transition to inlined type hints, especially with the ever-present `Union[str, torch.device]`, etc.\r\n\r\nThe basic implementation seems useful - those are all unions that are probably needed in other places. The proposal goes a little overboard it looks like to me - things that are objects in the API should mostly be defined inline (like `Tensor`) I'd think. We do need a `TensorLike` like that add-on has.\r\n\r\nAnd minor suggestion: call the module `annotations` rather than `types`. `types` is a stdlib module, and shadowing that isn't the best style usually.\r\n\r\n"", 'Fully onboard with the ""simpler"" solution. I find myself repeatedly creating `DeviceLike`, etc. aliases across projects, so having a single, definitive source of truth would make life easier, especially with 3rd party packages.\r\n\r\nHappy to provide feedback and/or devote cycles if that would be helpful.', 'Possibly related: Discussion in NumPy about type hints (PEP 484):\r\nhttps://github.com/numpy/numpy/issues/7370\r\n\r\nFrom that thread + related issues:\r\n- https://github.com/numpy/numpy/labels/static%20typing\r\n- https://github.com/ramonhagenaars/nptyping\r\n- https://github.com/AndreaCensi/contracts', ""Here's a summary of the current status:\r\n- mypy currently runs on ~1150 files\r\n- there's 59 `ignore_errors = True` left for modules. Most of those are for `caffe2`, for private modules, or have open PRs to remove the ignores. The main exception is some `nn.modules` submodules. Should be feasible to get through all of those (except `caffe2`, we decided not to bother with those) for the 1.8 release.\r\n- The main issue after that is how to make sure that the annotations are actually correct. Mypy won't catch errors unless the code is exercised. Probably the best way to do that is to make mypy run on test files. Those also contain lots of errors that need fixing, but after that there should be decent coverage, which will prevent a continuous drip of little issues being opened."", ""> The main issue after that is how to make sure that the annotations are actually correct. Mypy won't catch errors unless the code is exercised. Probably the best way to do that is to make mypy run on test files. Those also contain lots of errors that need fixing, but after that there should be decent coverage, which will prevent a continuous drip of little issues being opened.\r\n\r\n@guilhermeleobas suggested using [Typeguard](https://typeguard.readthedocs.io/en/latest/index.html); Numba is apparently using it successfully to verify their annotations. This would use the import hook method and run only as an optional test dependency in some CI jobs - see https://github.com/numba/numba/pull/6458.\r\n\r\nIf it can be made to work, that's probably more effective than running mypy over the test files, and a lot less work (there's a ton of mypy errors in test files that take time to resolve)."", ""There's another testing approach that should be cleaned up. Currently there are a few dedicated typing tests in `test/type_hint_tests/`, but they're not very useful (they're untyped Python code that won't catch all types of problems) and the one-snippet-per-python-file makes little sense.\r\n\r\nInstead, we can adopt what NumPy does. It has a way to write a lot of typing tests very compactly in this format:\r\n```\r\nreveal_type(np.ravel(A))  # E: numpy.ndarray[Any, Any]\r\nreveal_type(np.ravel(B))  # E: numpy.ndarray[Any, Any]\r\nreveal_type(np.nonzero(a))  # E: tuple[numpy.ndarray[Any, Any]]\r\n```\r\nMachinery for it to recognize the `# E:` expected mypy output format is at https://github.com/numpy/numpy/blob/master/numpy/typing/tests/test_typing.py\r\n\r\nThis will be especially useful for adding good tests for APIs that have more static typing complexity, e.g. multiple overloads, or the `nn.Module.forward` machinery."", ""> Instead, we can adopt what NumPy does. It has a way to write a lot of typing tests very compactly in this format:\r\n> \r\n> ```\r\n> reveal_type(np.ravel(A))  # E: numpy.ndarray[Any, Any]\r\n> reveal_type(np.ravel(B))  # E: numpy.ndarray[Any, Any]\r\n> reveal_type(np.nonzero(a))  # E: tuple[numpy.ndarray[Any, Any]]\r\n> ```\r\n> \r\n> Machinery for it to recognize the `# E:` expected mypy output format is at https://github.com/numpy/numpy/blob/master/numpy/typing/tests/test_typing.py\r\n> \r\n> This will be especially useful for adding good tests for APIs that have more static typing complexity, e.g. multiple overloads, or the `nn.Module.forward` machinery.\r\n\r\nI've send https://github.com/pytorch/pytorch/pull/52408 porting NumPy `test_typing.py` to PyTorch."", ""> The main issue after that is how to make sure that the annotations are actually correct.\r\n\r\nAfter gh-52408 and gh-54234 we now have a nice compact way to add tests for type annotations. And anecdotally, the number of type annotation errors slipping through or breaking in CI has reduced over time.\r\n\r\nMypy is upgraded to the latest version (0.812) and runs on almost all files now, there's only a handful of `ignore_errors` for files under `torch/` and (with 2 exceptions for which PRs have been open for some time). So we're in pretty decent shape here.\r\n\r\nThere are some open issues left with `module: typing`, but those are all smaller things and can be handled like any other issue. I think we can declary victory here and close this tracking issue.""]",[],['torch'],0,0
570,pytorch,17569,closed,Is it possible to integrate jax into pytorch ?,"# What is jax (just in case for other reader)
[jax](https://github.com/google/jax) is full numpy acceleration and autodiff with functional neural networks, and is essentially autograd 2.0. (from [Italian Association for Machine Learning)](https://iaml.it/blog/jax-intro-english)

# Feature Request
Is it possible to incorporate jax into implementation of Pytorch? It looks so promising to speed up machine learning.  
I would like to know if there is any plan about jax. Thanks.",feature triage review,"['What are some examples of things that jax enables you to do that you cannot easily do today with PyTorch?', 'Sorry I don\'t know much about Pytorch implementation, but I think JAX has some good features.\r\n# 1. What JAX can do \r\n(excerpts from [You don\'t know JAX](https://colinraffel.com/blog/you-don-t-know-jax.html) by Colin Raffel )\r\n## 1.1.  jax.jit can dramatically speed-up your code\r\nJAX provides a JIT (just-in-time) compiler which takes a standard Python/numpy function and compiles it to  via [XLA](https://www.tensorflow.org/xla/) \r\n- **run efficiently on an accelerator (GPU/TPU)**\r\n- **avoids the overhead of the Python interpreter, whether you are on CPU/GPU/TPU**. \r\n## 1.2. JAX can parallelly process a minibatch\r\nJAX provides jax.vmap, which is a transformation which **automatically ‚Äúvectorizes‚Äù a function**. What this means is that it allows you to compute the output of a function in parallel over some axis of the input. \r\n\r\n# 2. Examples\r\nHaven\'t found a concrete example yet. But according to [this tweet](https://twitter.com/xenophar/status/1099862725050523648), he said ""he is getting something like a 100x boost compared to autograd"".\r\nAnd founder of Keras is thinking of implementing Keras API on it. ([tweet](https://twitter.com/fchollet/status/1095774910096367616))\r\n\r\n# 3. What I would like to see/know\r\n- Maybe Pytorch can collaborate with JAX team?\r\n- Is there any plan about JAX?  / Will there any plan about JAX?', '1.1, we already have with torch.jit\r\n1.2 seems interesting\r\n2. probably not relevant\r\n\r\n1.2 is tracked in https://github.com/pytorch/pytorch/issues/1642\r\n\r\nOverall, there isn\'t really the concept of ""jax in pytorch"" or ""pytorch in jax"" in the same sense that they\'re both frontends ', 'some additional description on JAX: https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/\r\n\r\n> Enter Jax. Jax is built by the same people who built the original Autograd, and features both forward- and reverse-mode auto-differentiation. This allows computation of higher order derivatives orders of magnitude faster than what PyTorch/TensorFlow can offer.\r\n\r\n> Jax offers more than just higher order derivatives, however. The Jax developers view Jax as a framework for composing arbitrary functional transformations, including vmap (for automatic batching) or pmap (for automatic parallelization).\r\n\r\n> The original autograd had its devoted followers (11 papers at ICML used it despite having no GPU support), and it‚Äôs likely that Jax will soon pick up a similar devoted community, using it for all sorts of n-th order derivatives.\r\n\r\n']",[],[],0,0
571,pytorch,2369,closed,Calculate the derivative of a node with respect to an input image,"
I would like to calculate the derivative of any node in respect of image input to implement visualization methods using PyTorch.

Any suggestions ? ",,"['If you make the input image a `Variable` with `requires_grad=True` you will get that derivative in `image.grad` after calling `.backward(grad_output)` on the node.', 'Thanks for your response, I will test it. How I can calculate this derivative of internal node or class node and not the cost function ?', 'please use https://discuss.pytorch.org for such questions.']",[],[],0,0
572,pytorch,15138,closed,Build simple c++ example-cpp using Libtorch fails on arm with undefined reference to c10::Error::Error,"## üêõ Bug

<!-- After building PyTorch from source, to generate libs required to run https://pytorch.org/cppdocs/installing.html on arm, building example-cpp fails with following error. -->

## To Reproduce

Steps to reproduce the behavior:

1.Build PyTorch from source.

2.Replace libs folder in Libtorch (https://download.pytorch.org/libtorch/nightly/cu90/libtorch-shared-with-deps-latest.zip) with the pytorch/build/lib (lib folder created on arm after building pytorch).

3.build the example-cpp 

## Error Message

>  nvidia@tegra-ubuntu:~/uvaidya/LibTorch/example-app/build$ make
> [ 50%] Linking CXX executable example-app
> CMakeFiles/example-app.dir/example-app.cpp.o: In function c10::Error::Error(c10::SourceLocation, std::string const&)'
> example-app.cpp:(.text._ZN3c106DeviceC2ENS_10DeviceTypeEs[_ZN3c106DeviceC5ENS_10DeviceTypeEs]+0x1bc): undefined reference to at::TensorOptions::device() const':
> example-app.cpp:(.text._ZNK2at13TensorOptions6deviceEv[_ZNK2at13TensorOptions6deviceEv]+0x30): undefined reference to at::TensorOptions::requires_grad() const':
> example-app.cpp:(.text._ZNK2at13TensorOptions13requires_gradEv[_ZNK2at13TensorOptions13requires_gradEv]+0x38): undefined reference to c10::impl::getDeviceGuardImpl(c10::DeviceType)':
> example-app.cpp:(.text._ZN3c104impl18getDeviceGuardImplENS_10DeviceTypeE[_ZN3c104impl18getDeviceGuardImplENS_10DeviceTypeE]+0x11c): undefined reference to torch::autograd::make_variable(at::Tensor, bool)':
> example-app.cpp:(.text._ZN5torch8autograd13make_variableEN2at6TensorEb[_ZN5torch8autograd13make_variableEN2at6TensorEb]+0xbc): undefined reference to torch::jit::SourceRange::highlight(std::ostream&) const':
> example-app.cpp:(.text._ZNK5torch3jit11SourceRange9highlightERSo[_ZNK5torch3jit11SourceRange9highlightERSo]+0x1c0): undefined reference to c10::Error::Error(c10::SourceLocation, std::string const&)'
> example-app.cpp:(.text._ZNK5torch3jit11SourceRange9highlightERSo[_ZNK5torch3jit11SourceRange9highlightERSo]+0x41c): undefined reference to c10::Error::Error(c10::SourceLocation, std::string const&)' follow
> CMakeFiles/example-app.dir/example-app.cpp.o: In function c10::Symbol::fromQualString(std::string const&)'
> CMakeFiles/example-app.dir/example-app.cpp.o: In function c10::Error::Error(c10::SourceLocation, std::string const&)'
> collect2: error: ld returned 1 exit status
> CMakeFiles/example-app.dir/build.make:99: recipe for target 'example-app' failed
> make[2]: *** [example-app] Error 1
> CMakeFiles/Makefile2:72: recipe for target 'CMakeFiles/example-app.dir/all' failed
> make[1]: *** [CMakeFiles/example-app.dir/all] Error 2
> Makefile:83: recipe for target 'all' failed
> make: *** [all] Error 2


## Environment

Collecting environment information...
PyTorch version: 1.0.0a0+db5d313
Is debug build: No
CUDA used to build PyTorch: 9.2.78

OS: Ubuntu 16.04 LTS
GCC version: (Ubuntu/Linaro 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CMake version: version 3.12.3

Python version: 3.5
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Probably one of the following:
/usr/lib/aarch64-linux-gnu/libcudnn.so.7.1.2
/usr/lib/aarch64-linux-gnu/libcudnn_static_v7.a

Versions of relevant libraries:
[pip] Could not collect
[conda] Could not collect



",module: cpp,"['cc: @goldsborough looks like example-cpp is not linking to `libc10.so`', 'I managed to successfully compile it on Jetson TX2 following these steps:\r\n\r\n1.Build PyTorch from source. `python3 setup.py build`\r\n\r\n2.No need to download LibTorch from the official website, which is the x86_64 build.\r\n\r\n3.build the example-cpp using compiled tmp_install directory: `cmake -DCMAKE_PREFIX_PATH=/absolute/path/to/pytorch/torch/lib/tmp_install ..`', '@uvaidya i ran into a related problem while building torch-android.\r\n\r\nYou might be building `libtorch` with a compiler that is incompatible with the compiler building your final app.\r\n\r\nFor example, you built libtorch with gcc 4.9.2 and your final app with gcc 5.1, and the C++ ABI between both of them is not the same, so you are seeing linker errors like these', 'Thanks @soumith .\r\nAfter making the C++ ABI same, i.e. setting flag properly we found that example cpp is getting compiled successfully.\r\nThanks @huangyuyao  .\r\nSteps mentioned by you are to the point, in  _/absolute/path/to/pytorch/torch/lib/tmp_install_ , actual libtorch libraries gets created.\r\n\r\nClosing this bug. Thanks ']",[],"[""c10::Device::Device(c10::DeviceType, short)':\r\n> example-app.cpp:(.text._ZN3c106DeviceC2ENS_10DeviceTypeEs[_ZN3c106DeviceC5ENS_10DeviceTypeEs]+0xdc): undefined reference to "", ""c10::Error::Error(c10::SourceLocation, std::string const&)'\r\n> CMakeFiles/example-app.dir/example-app.cpp.o: In function "", ""at::getDefaultTensorOptions()'\r\n> CMakeFiles/example-app.dir/example-app.cpp.o: In function "", ""at::getDefaultTensorOptions()'\r\n> CMakeFiles/example-app.dir/example-app.cpp.o: In function "", ""c10::Error::Error(c10::SourceLocation, std::string const&)'\r\n> CMakeFiles/example-app.dir/example-app.cpp.o: In function "", ""c10::Error::Error(c10::SourceLocation, std::string const&)'\r\n> CMakeFiles/example-app.dir/example-app.cpp.o: In function "", ""c10::Error::Error(c10::SourceLocation, std::string const&)'\r\n> example-app.cpp:(.text._ZNK5torch3jit11SourceRange9highlightERSo[_ZNK5torch3jit11SourceRange9highlightERSo]+0x2b8): undefined reference to "", ""c10::Error::Error(c10::SourceLocation, std::string const&)'\r\n> CMakeFiles/example-app.dir/example-app.cpp.o:example-app.cpp:(.text._ZNK5torch3jit11SourceRange9highlightERSo[_ZNK5torch3jit11SourceRange9highlightERSo]+0x594): more undefined references to "", ""torch::rand(c10::ArrayRef<long>, at::TensorOptions const&)':\r\n> example-app.cpp:(.text._ZN5torch4randEN3c108ArrayRefIlEERKN2at13TensorOptionsE[_ZN5torch4randEN3c108ArrayRefIlEERKN2at13TensorOptionsE]+0x98): undefined reference to "", ""c10::intrusive_ptr<at::TensorImpl, at::UndefinedTensorImpl>::retain_()':\r\n> example-app.cpp:(.text._ZN3c1013intrusive_ptrIN2at10TensorImplENS1_19UndefinedTensorImplEE7retain_Ev[_ZN3c1013intrusive_ptrIN2at10TensorImplENS1_19UndefinedTensorImplEE7retain_Ev]+0x114): undefined reference to ""]",0,0
573,pytorch,4408,closed,"torch.onnx.export fails on Linear when bias=False, torch.onnx.symbolic.matmul does not exist","Using  on a model that contains a linear layer with no bias, I receive an error: 



I've constructed a minimal working example that reproduces this


Environment Info:
",,"['I notice in `symbolic.py` there is a function: \r\n\r\n```\r\ndef bmm(g, self, other):\r\n    return g.op(""MatMul"", self, other)\r\n```\r\n\r\nis it sufficient to simply add another function: \r\n\r\n```\r\ndef matmul(g, self, other):\r\n    return g.op(""MatMul"", self, other)\r\n```\r\n\r\nIf I hack this into my installed pytorch, then the model ""seems"" to work correctly. I can save it, reload and then it checks correctly. \r\n\r\nIs this the correct fix? I\'m not really sure what `bmm` stnads for, nor is it obvious from the docs what sort of object `g` is (although I assume it might stand for graph?). ', ""`bmm` is batched matrix multiply, `g` is a graph. I'm not 100% sure that this is enough, but it looks good. Can you please send a PR, and we'll have someone review that? Thank you!"", 'fixed via #4452 ']","['python\r\n~/venv3/lib/python3.5/site-packages/torch/onnx/__init__.py:312: UserWarning: ONNX export failed on matmul because torch.onnx.symbolic.matmul does not exist\r\n  File ""~/venv3/lib/python3.5/site-packages/torch/onnx/__init__.py"", line 151, in _export\r\n    proto = trace.export([], _onnx_opset_version)\r\nRuntimeError: ONNX export failed: Couldn\'t export operator matmul\r\n\r\nGraph we tried to export:\r\ngraph(%0 : Float(1, 128)\r\n      %1 : Float(2, 128)) {\r\n  %2 : Float(128!, 2!) = Transpose[perm=[1, 0]](%1), scope: Linear\r\n  %3 : Float(1, 2) = matmul(%0, %2), scope: Linear\r\n  return (%3);\r\n}\r\n', ""python\r\ndef mwe_bias():\r\n    linear = torch.nn.Linear(128, 2, bias=1)\r\n    args = tuple([torch.autograd.Variable(torch.randn(1, 128), requires_grad=True)])\r\n    torch.onnx.export(linear, args, 'foo.onnx', export_params=False)\r\n\r\n\r\ndef mwe_nobias():\r\n    linear = torch.nn.Linear(128, 2, bias=0)\r\n    args = tuple([torch.autograd.Variable(torch.randn(1, 128), requires_grad=True)])\r\n    torch.onnx.export(linear, args, 'foo.onnx', export_params=False)\r\n\r\n# does not cause an error\r\nmwe_bias()\r\n# Causes an error\r\nmwe_nobias()\r\n"", ""python\r\nsys.platform = linux\r\nsys.version = 3.5.2 (default, Nov 17 2016, 17:05:23) \\n[GCC 5.4.0 20160609]\r\ntorch.version.__version__ = '0.4.0a0+af3bffb'\r\ntorch.version.cuda = 8.0.61\r\n""]",['torch.onnx.export'],0,0
574,pytorch,19057,closed,building blocked when executing `python setup.py build` during COMPILER_SUPPORTS_LONG_DOUBLE,"## building blocked when executing python setup.py build

### Here is my environment info:

- OS: Ubuntu 16.04
- GCC: 5.3.0
- CUDA: 9.0
- GPU: GeForce GTX 1080 * 2
- PyTorch to be installed: 1.0.0

### I try to build PyTorch=1.0.0 from source but it seems to be blocked here (). I had waited for a long time but it didn't go on. Why did this happen?




",module: build triaged,"['Is `/mnt/lustre` a remote filesystem by any chance? If you rerun the command as `strace -f python setup.py build` what syscall does strace show you are hanging on?', '@ezyang \r\nHi, `/mnt/lustre` is a remote filesystem on the server of my company.\r\nWhen I typed in the command `strace -f python setup.py build`, the last several lines of the output are shown below:\r\n```\r\n[pid 41042] access(""/mnt/lustre/share/gcc/gcc-5.3.0/lib/gcc/x86_64-unknown-linux-gnu/5.3.0/"", X_OK) = 0\r\n[pid 41042] stat(""/mnt/lustre/share/gcc/gcc-5.3.0/lib/gcc/x86_64-unknown-linux-gnu/5.3.0/include"", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0\r\n[pid 41042] stat(""/usr/local/include"", {st_mode=S_IFDIR|0755, st_size=6, ...}) = 0\r\n[pid 41042] stat(""/mnt/lustre/share/gcc/gcc-5.3.0/include"", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0\r\n[pid 41042] stat(""/mnt/lustre/share/gcc/gcc-5.3.0/lib/gcc/x86_64-unknown-linux-gnu/5.3.0/include-fixed"", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0\r\n[pid 41042] stat(""/mnt/lustre/share/gcc/gcc-5.3.0/lib/gcc/x86_64-unknown-linux-gnu/5.3.0/../../../../x86_64-unknown-linux-gnu/include"", 0x7ffde8e37e50) = -1 ENOENT (No such file or directory)\r\n[pid 41042] stat(""/usr/include"", {st_mode=S_IFDIR|0755, st_size=12288, ...}) = 0\r\n[pid 41042] stat(""src.c.gch"", 0x7ffde8e37ea0) = -1 ENOENT (No such file or directory)\r\n[pid 41042] open(""src.c"", O_RDONLY|O_NOCTTY) = 3\r\n[pid 41042] fstat(3, {st_mode=S_IFREG|0664, st_size=215, ...}) = 0\r\n[pid 41042] read(3, ""\\n  typedef long double vlongdoub""..., 215) = 215\r\n[pid 41042] close(3)                    = 0\r\n[pid 41042] mmap(NULL, 16384, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7fb8d7966000\r\n[pid 41042] mmap(NULL, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7fb8d7964000\r\n[pid 41042] lstat(""/tmp"", {st_mode=S_IFDIR|S_ISVTX|0777, st_size=167936, ...}) = 0\r\n[pid 41042] lstat(""/tmp/ccTKY12a.s"", {st_mode=S_IFREG|0600, st_size=0, ...}) = 0\r\n[pid 41042] getcwd(""/mnt/lustre/chenxiaokang/pytorch-git/build/CMakeFiles/CMakeTmp"", 4096) = 63\r\n[pid 41042] lstat(""/mnt/lustre/chenxiaokang/pytorch-git/build/CMakeFiles/CMakeTmp/src.c"", {st_mode=S_IFREG|0664, st_size=215, ...}) = 0\r\n[pid 41042] open(""/tmp/ccTKY12a.s"", O_WRONLY|O_CREAT|O_TRUNC, 0666) = 3\r\n[pid 41042] fstat(3, {st_mode=S_IFREG|0600, st_size=0, ...}) = 0\r\n[pid 41042] mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7fb8d7963000\r\n[pid 41042] stat(""/mnt/lustre/chenxiaokang/pytorch-git/build/CMakeFiles/CMakeTmp"", {st_mode=S_IFDIR|0775, st_size=4096, ...}) = 0\r\n[pid 41042] stat(""."", {st_mode=S_IFDIR|0775, st_size=4096, ...}) = 0\r\n[pid 41042] lstat(""/mnt"", {st_mode=S_IFDIR|0755, st_size=35, ...}) = 0\r\n[pid 41042] lstat(""/mnt/lustre"", {st_mode=S_IFDIR|0755, st_size=49152, ...}) = 0\r\n[pid 41042] lstat(""/mnt/lustre/share"", {st_mode=S_IFDIR|S_ISVTX|0777, st_size=28672, ...}) = 0\r\n[pid 41042] lstat(""/mnt/lustre/share/gcc"", {st_mode=S_IFDIR|0777, st_size=4096, ...}) = 0\r\n[pid 41042] lstat(""/mnt/lustre/share/gcc/gcc-5.3.0"", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0\r\n[pid 41042] lstat(""/mnt/lustre/share/gcc/gcc-5.3.0/lib"", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0\r\n[pid 41042] lstat(""/mnt/lustre/share/gcc/gcc-5.3.0/lib/gcc"", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0\r\n[pid 41042] lstat(""/mnt/lustre/share/gcc/gcc-5.3.0/lib/gcc/x86_64-unknown-linux-gnu"", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0\r\n[pid 41042] lstat(""/mnt/lustre/share/gcc/gcc-5.3.0/lib/gcc/x86_64-unknown-linux-gnu/5.3.0"", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0\r\n[pid 41042] lstat(""/mnt/lustre/share/gcc/gcc-5.3.0/lib/gcc/x86_64-unknown-linux-gnu/5.3.0/include"", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0\r\n[pid 41042] lstat(""/mnt/lustre/share/gcc/gcc-5.3.0/lib/gcc/x86_64-unknown-linux-gnu/5.3.0/include/stdc-predef.h"", 0x7ffde8e36d20) = -1 ENOENT (No such file or directory)\r\n[pid 41042] stat(""/mnt/lustre/share/gcc/gcc-5.3.0/lib/gcc/x86_64-unknown-linux-gnu/5.3.0/include/stdc-predef.h.gch"", 0x7ffde8e37e50) = -1 ENOENT (No such file or directory)\r\n[pid 41042] open(""/mnt/lustre/share/gcc/gcc-5.3.0/lib/gcc/x86_64-unknown-linux-gnu/5.3.0/include/stdc-predef.h"", O_RDONLY|O_NOCTTY) = -1 ENOENT (No such file or directory)\r\n[pid 41042] lstat(""/usr"", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0\r\n[pid 41042] lstat(""/usr/local"", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0\r\n[pid 41042] lstat(""/usr/local/include"", {st_mode=S_IFDIR|0755, st_size=6, ...}) = 0\r\n[pid 41042] lstat(""/usr/local/include/stdc-predef.h"", 0x7ffde8e36d20) = -1 ENOENT (No such file or directory)\r\n[pid 41042] stat(""/usr/local/include/stdc-predef.h.gch"", 0x7ffde8e37e50) = -1 ENOENT (No such file or directory)\r\n[pid 41042] open(""/usr/local/include/stdc-predef.h"", O_RDONLY|O_NOCTTY) = -1 ENOENT (No such file or directory)\r\n[pid 41042] lstat(""/mnt"", {st_mode=S_IFDIR|0755, st_size=35, ...}) = 0\r\n[pid 41042] lstat(""/mnt/lustre"", {st_mode=S_IFDIR|0755, st_size=49152, ...}) = 0\r\n[pid 41042] lstat(""/mnt/lustre/share"", {st_mode=S_IFDIR|S_ISVTX|0777, st_size=28672, ...}) = 0\r\n[pid 41042] lstat(""/mnt/lustre/share/gcc"", {st_mode=S_IFDIR|0777, st_size=4096, ...}) = 0\r\n[pid 41042] lstat(""/mnt/lustre/share/gcc/gcc-5.3.0"", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0\r\n[pid 41042] lstat(""/mnt/lustre/share/gcc/gcc-5.3.0/include"", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0\r\n[pid 41042] lstat(""/mnt/lustre/share/gcc/gcc-5.3.0/include/stdc-predef.h"", 0x7ffde8e36d20) = -1 ENOENT (No such file or directory)\r\n[pid 41042] stat(""/mnt/lustre/share/gcc/gcc-5.3.0/include/stdc-predef.h.gch"", 0x7ffde8e37e50) = -1 ENOENT (No such file or directory)\r\n[pid 41042] open(""/mnt/lustre/share/gcc/gcc-5.3.0/include/stdc-predef.h"", O_RDONLY|O_NOCTTY) = -1 ENOENT (No such file or directory)\r\n[pid 41042] lstat(""/mnt"", {st_mode=S_IFDIR|0755, st_size=35, ...}) = 0\r\n[pid 41042] lstat(""/mnt/lustre"", {st_mode=S_IFDIR|0755, st_size=49152, ...}) = 0\r\n[pid 41042] lstat(""/mnt/lustre/share"", {st_mode=S_IFDIR|S_ISVTX|0777, st_size=28672, ...}) = 0\r\n[pid 41042] lstat(""/mnt/lustre/share/gcc"", {st_mode=S_IFDIR|0777, st_size=4096, ...}) = 0\r\n[pid 41042] lstat(""/mnt/lustre/share/gcc/gcc-5.3.0"", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0\r\n[pid 41042] lstat(""/mnt/lustre/share/gcc/gcc-5.3.0/lib"", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0\r\n[pid 41042] lstat(""/mnt/lustre/share/gcc/gcc-5.3.0/lib/gcc"", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0\r\n[pid 41042] lstat(""/mnt/lustre/share/gcc/gcc-5.3.0/lib/gcc/x86_64-unknown-linux-gnu"", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0\r\n[pid 41042] lstat(""/mnt/lustre/share/gcc/gcc-5.3.0/lib/gcc/x86_64-unknown-linux-gnu/5.3.0"", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0\r\n[pid 41042] lstat(""/mnt/lustre/share/gcc/gcc-5.3.0/lib/gcc/x86_64-unknown-linux-gnu/5.3.0/include-fixed"", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0\r\n[pid 41042] lstat(""/mnt/lustre/share/gcc/gcc-5.3.0/lib/gcc/x86_64-unknown-linux-gnu/5.3.0/include-fixed/stdc-predef.h"", 0x7ffde8e36d20) = -1 ENOENT (No such file or directory)\r\n[pid 41042] stat(""/mnt/lustre/share/gcc/gcc-5.3.0/lib/gcc/x86_64-unknown-linux-gnu/5.3.0/include-fixed/stdc-predef.h.gch"", 0x7ffde8e37e50) = -1 ENOENT (No such file or directory)\r\n[pid 41042] open(""/mnt/lustre/share/gcc/gcc-5.3.0/lib/gcc/x86_64-unknown-linux-gnu/5.3.0/include-fixed/stdc-predef.h"", O_RDONLY|O_NOCTTY) = -1 ENOENT (No such file or directory)\r\n[pid 41042] lstat(""/usr"", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0\r\n[pid 41042] lstat(""/usr/include"", {st_mode=S_IFDIR|0755, st_size=12288, ...}) = 0\r\n[pid 41042] lstat(""/usr/include/stdc-predef.h"", {st_mode=S_IFREG|0644, st_size=1629, ...}) = 0\r\n[pid 41042] stat(""/usr/include/stdc-predef.h.gch"", 0x7ffde8e37e50) = -1 ENOENT (No such file or directory)\r\n[pid 41042] open(""/usr/include/stdc-predef.h"", O_RDONLY|O_NOCTTY) = 4\r\n[pid 41042] fstat(4, {st_mode=S_IFREG|0644, st_size=1629, ...}) = 0\r\n[pid 41042] read(4, ""/* Copyright (C) 1991-2012 Free ""..., 1629) = 1629\r\n[pid 41042] close(4)                    = 0\r\n[pid 40811] close(4)                    = 0\r\n[pid 40811] read(3, """", 50000)          = 0\r\n[pid 40811] close(3)                    = 0\r\n[pid 40811] wait4(40991, \r\n```\r\nAnd the output **hangs on** here.\r\n', 'Can you paste the whole log to a gist?', ""@ezyang \r\nI am sorry that I fell asleep last night. I have redirected the output of the terminal to 'log.txt', but the file is too large (> 50M) and can't be uploaded to github gist. Can I share you a BaiDuYun link?\r\n**link**: [https://pan.baidu.com/s/1mD3CLDdMwOmDnxhTIWxJ-w](https://pan.baidu.com/s/1mD3CLDdMwOmDnxhTIWxJ-w) \r\n**passwd**: 3krw"", ""@charlesCXK Sorry about the late response. I don't know how to download from that link from the Baidu link; it seems to require a desktop client."", 'But essentially, what I am curious, is what `open` call resulted in fd 40991. So just search for that number from the bottom of the file up.', ""I'm closing this issue because it is unlikely to be a PyTorch issue; it is probably network filesystem related. As a workaround, consider compiling from local disk."", 'Reopened because I saw another user report #16206 of the exact same thing.', '@ezyang \r\nThank you for your reply! I had downloaded PyTorch=1.0.0 from pip3, and that satisfied my needs.', '@ezyang @charlesCXK I encounter the same problem when compiling torch 1.2.0 with gcc 5.3 or gcc 4.9. However, when compiling with gcc 4.8, everything is ok: the COMPILER_SUPPORTS_LONG_DOUBLE check does not block, but just fails and skips to the next check.', 'Very interesting that when I try gcc-5.4, the COMPILER_SUPPORTS_LONG_DOUBLE check neither blocks nor fails.']","['\r\n-- ******** Summary ********\r\n--   CMake version         : 3.11.0\r\n--   CMake command         : /mnt/lustre/share/cmake-3.11.0-Linux-x86_64/bin/cmake\r\n--   System                : Linux\r\n--   C++ compiler          : /mnt/lustre/share/gcc/gcc-5.3.0/bin/c++\r\n--   C++ compiler version  : 5.3.0\r\n--   CXX flags             :  -Wno-deprecated -fvisibility-inlines-hidden -Wnon-virtual-dtor\r\n--   Build type            : Release\r\n--   Compile definitions   : \r\n--   CMAKE_PREFIX_PATH     : /mnt/lustre/chenxiaokang/anaconda3/envs/pytorch1.0/lib/python3.7/site-packages\r\n--   CMAKE_INSTALL_PREFIX  : /mnt/lustre/chenxiaokang/pytorch-git/torch/lib/tmp_install\r\n--   CMAKE_MODULE_PATH     : /mnt/lustre/chenxiaokang/pytorch-git/cmake/Modules;/mnt/lustre/chenxiaokang/pytorch-git/cmake/public/../Modules_CUDA_fix\r\n-- \r\n--   ONNX version          : 1.4.1\r\n--   ONNX NAMESPACE        : onnx_torch\r\n--   ONNX_BUILD_TESTS      : OFF\r\n--   ONNX_BUILD_BENCHMARKS : OFF\r\n--   ONNX_USE_LITE_PROTO   : OFF\r\n--   ONNXIFI_DUMMY_BACKEND : OFF\r\n--   ONNXIFI_ENABLE_EXT    : OFF\r\n-- \r\n--   Protobuf compiler     : \r\n--   Protobuf includes     : \r\n--   Protobuf libraries    : \r\n--   BUILD_ONNX_PYTHON     : OFF\r\n-- Found CUDA with FP16 support, compiling with torch.cuda.HalfTensor\r\n-- Removing -DNDEBUG from compile flags\r\n-- MAGMA not found. Compiling without MAGMA support\r\n-- Could not find hardware support for NEON on this machine.\r\n-- No OMAP3 processor on this machine.\r\n-- No OMAP4 processor on this machine.\r\n-- Looking for cpuid.h\r\n-- Looking for cpuid.h - found\r\n-- Performing Test HAVE_GCC_GET_CPUID\r\n-- Performing Test HAVE_GCC_GET_CPUID - Success\r\n-- Performing Test NO_GCC_EBX_FPIC_BUG\r\n-- Performing Test NO_GCC_EBX_FPIC_BUG - Success\r\n-- Performing Test C_HAS_AVX_1\r\n-- Performing Test C_HAS_AVX_1 - Failed\r\n-- Performing Test C_HAS_AVX_2\r\n-- Performing Test C_HAS_AVX_2 - Success\r\n-- Performing Test C_HAS_AVX2_1\r\n-- Performing Test C_HAS_AVX2_1 - Failed\r\n-- Performing Test C_HAS_AVX2_2\r\n-- Performing Test C_HAS_AVX2_2 - Success\r\n-- Performing Test CXX_HAS_AVX_1\r\n-- Performing Test CXX_HAS_AVX_1 - Failed\r\n-- Performing Test CXX_HAS_AVX_2\r\n-- Performing Test CXX_HAS_AVX_2 - Success\r\n-- Performing Test CXX_HAS_AVX2_1\r\n-- Performing Test CXX_HAS_AVX2_1 - Failed\r\n-- Performing Test CXX_HAS_AVX2_2\r\n-- Performing Test CXX_HAS_AVX2_2 - Success\r\n-- AVX compiler support found\r\n-- AVX2 compiler support found\r\n-- Performing Test HAS_C11_ATOMICS\r\n-- Performing Test HAS_C11_ATOMICS - Success\r\n-- Atomics: using C11 intrinsics\r\n-- Looking for cheev_\r\n-- Looking for cheev_ - found\r\n-- Found a library with LAPACK API. (open)\r\n-- Found CUDA: /mnt/lustre/share/cuda-9.0 (found suitable version ""9.0"", minimum required is ""5.5"") \r\ndisabling ROCM because NOT USE_ROCM is set\r\n-- MIOpen not found. Compiling without MIOpen support\r\n-- Could NOT find MKLDNN (missing: MKLDNN_LIBRARIES) \r\n-- MKLDNN source files not found!\r\n-- MKLDNN source files not found!\r\nCMake Warning at cmake/Dependencies.cmake:1318 (MESSAGE):\r\n  MKLDNN could not be found.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:201 (include)\r\n\r\n\r\n-- Looking for clock_gettime in rt\r\n-- Looking for clock_gettime in rt - found\r\n-- Looking for mmap\r\n-- Looking for mmap - found\r\n-- Looking for shm_open\r\n-- Looking for shm_open - found\r\n-- Looking for shm_unlink\r\n-- Looking for shm_unlink - found\r\n-- Looking for malloc_usable_size\r\n-- Looking for malloc_usable_size - found\r\n-- Performing Test C_HAS_THREAD\r\n-- Performing Test C_HAS_THREAD - Success\r\n-- GCC 5.3.0: Adding gcc and gcc_s libs to link line\r\n-- Using python found in /mnt/lustre/chenxiaokang/anaconda3/envs/pytorch1.0/bin/python\r\n-- Check size of long double\r\n-- Check size of long double - done\r\n-- Performing Test COMPILER_SUPPORTS_LONG_DOUBLE\r\n']",['Performing Test COMPILER_SUPPORTS_LONG_DOUBLE'],0,0
575,pytorch,6462,closed,How to set batch_size when using DataParallel mode in pytorch?,"I can't find document about the batch_size setting and DataParallel mode.
so How to set batch_size when using DataParallel mode in pytorch? Is it similiar with Keras?
Thanks

In Keras, the parallel_model( like DataParallel mode in pytorch), which document is very clear I think:
-------------------------------------------Keras document beginging------------------------------------------
E.g. if your batch_size is 64 and you use gpus=2, then we will divide the input into 2 sub-batches of 32 samples, process each sub-batch on one GPU, then return the full batch of 64 processed samples

This  call will be distributed on 8 GPUs.
 Since the batch size is 256, each GPU will process 32 samples.

parallel_model.fit(x, y, epochs=20, batch_size=256)
-------------------------------------------Keras document ended------------------------------------------",,"[""You don't set the batch size in any way when the module is constructed. It will automatically handle all possible sizes you'll pass at run time.""]",[],['fit'],0,0
576,pytorch,22586,closed,got nan when gumbel_softmax calculated in GPU,"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce


got results: 
> GPU: nan 0.004% probability happen, tot 38
> CPU: nan 0.000% probability happen, tot 0

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

no nan happen

## Environment

 - PyTorch Version (1.1.0):
 - OS (Linux):
 - How you installed PyTorch ():
 - Python version: 2.7
 - CUDA/cuDNN version: 9.0
",high priority module: NaNs and Infs module: distributions triaged,"['According to https://github.com/pytorch/pytorch/issues/22557 this has been fixed on master. Please let us know if this is not true!\r\n', ""(Sorry, it's actually #22442)""]","['\r\nimport torch\r\nimport torch.nn as nn\r\nimport math\r\n\r\nif __name__ == ""__main__"":\r\n    batch_size = 128\r\n    temperature = 5.0\r\n    theta = torch.FloatTensor([1.753356814384460449,1.898535370826721191,0.6992630958557128906,\r\n                                0.2227068245410919189,0.6384450793266296387,1.431323885917663574,\r\n                                -0.05012089386582374573, -0.06672633439302444458])\r\n    device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\r\n    t_gpu = theta.repeat(batch_size, 1).to(device)\r\n    max_num = 1000000\r\n    nan_num = 0\r\n    for i in range(max_num):\r\n        weight = nn.functional.gumbel_softmax(t_gpu, temperature)\r\n        if math.isnan(torch.sum(weight)):\r\n            nan_num+=1\r\n    print(""GPU: nan {:.3f}% probability happen, tot {}"".format(100.0 * nan_num / max_num, nan_num))\r\n    nan_num = 0\r\n    t_cpu = theta.repeat(batch_size, 1)\r\n    for i in range(max_num):\r\n        weight = nn.functional.gumbel_softmax(t_cpu, temperature)\r\n        if math.isnan(torch.sum(weight)):\r\n            nan_num+=1\r\n    print(""CPU: nan {:.3f}% probability happen, tot {}"".format(100.0 * nan_num / max_num, nan_num))\r\n']",['pip'],0,0
577,pytorch,4053,closed,grad_fn() crashed in pytorch 0.3.0,"@soumith 
Environment:
unbuntu 16.04+torch-0.3.0.post4-cp35-cp35m-linux_x86_64.whl (from pytorch.org)
windows 10 + pytorch(master) compiled with msvc


According to internal logicÔºåit should be a failure but not a crash. when I tested the code  in pytorch (0.2.0), it did not crash.",,"['Hi,\r\n\r\nWhat are you trying to use `grad_fn` for? You should never be calling it yourself.\r\nAlso since x is a leaf variable, `x.grad_fn` is `None`. So the first print should fail.', '@albanD \r\nThe code is odd, but maybe it helps to catch errors. \r\n```python\r\nimport torch\r\nfrom torch.autograd import Variable\r\nx = Variable(torch.ones(2, 2), requires_grad=True)\r\ny = x + 2\r\ny.grad_fn() # crashed here!\r\n```\r\n', ""The issue is that the generated backwards functions don't do any validation of their input.\r\n\r\nI think we should just disable calling grad_fn's manually from Python. Ensuring proper argument parsing everywhere is going to be a pain and I don't see a good use-case for it. "", '@Giszy as the above comment state, this does crash, but these functions should never be called.\r\nIf you have a valid use case that requires it that we did not though of, please present it.\r\n\r\n@colesbury How can we make these objects not callable from python since they are `THPCppFunction` objects that are also used for the forward conv and all cpp implemented `Function`s? ', 'The old cpp function path will be removed soon, so we can just remove their call methods.', 'FYI, I got the same segfault running a python interpreter following this [tutorial](https://hsaghir.github.io/data_science/pytorch_starter/). The tutorial suggests we call `.creator`, which has been [replaced](https://github.com/pytorch/tutorials/pull/91/files) with `.grad_fn` .  I called `.grad_fn()` out of curiousity, and -> segfault.', '@apaszke I think this can be closed too.', 'closed via #8774 ']","['python\r\nimport torch\r\nfrom torch.autograd import Variable\r\nx = Variable(torch.ones(2, 2), requires_grad=True)\r\ny = x + 2\r\n#print(x.grad_fn()) #succeed to catch errors\r\nprint(y.grad_fn()) # crashed here!\r\n']",[],0,0
578,pytorch,1623,closed,incompatibility between Pytorch 0.1.12 and cuda 8.0.61/cudnn 5.1.10 ?,"Hi,

I have two linux servers. My code works in one machine but not the other. In the one that it doesn't work, the error message is:  occured during batch normalization( convolution is fine). Also, the same code works in CPU but not in GPU. 

I guess this problem is mainly related to GPU libraries like cuda or cudnn. *The one that doesn't work has the following configurations*:
- Nvidia driver 375.26
- cuda 8.0.61
- cudnn 5.1.10

*While in the one it does work*, I have the following configurations:
- Nvidia driver 367.48
- cuda 8.0.44
- cudnn 5.1.5

TensorFlow can run in the above environment flawless though. So I am wondering if this is caused by the incompatibility between PyTorch 0.1.12 and cuda 8.0.61/cudnn 5.1.10 ?
",,"[""1. if you use the pytorch binaries, we ship cudnn6 in them, use it it's great.\r\n2. cudnn 5.1.10 maybe has a bug. cc: @ngimel . @gongbudaizhe do you have a small test case to reproduce this?\r\n3. TensorFlow does not use CuDNN batchnorm by default, so maybe that's why? "", 'Small test case would help, thanks. ', ""After some experimentation, I have pin pointed where exactly the error comes from. It seems batch norm in cudnn 5.1.10 doesn't support eps as small as 1e-6. \r\n\r\nA small test case would be:\r\n```python\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.autograd import Variable\r\n\r\nclass TestModel(nn.Module):\r\n  def __init__(self):\r\n    super(TestModel, self).__init__()\r\n    self.bn = nn.BatchNorm2d(96, eps=1e-6) # Bug here\r\n    # self.bn = nn.BatchNorm2d(96, eps=1e-5) # No Bug version\r\n\r\n  def forward(self, x):\r\n    out = self.bn(x)\r\n    return out\r\n\r\nif __name__ == '__main__':\r\n  feat = Variable(torch.rand((8, 96, 59, 59))).cuda()\r\n  model = TestModel().cuda()\r\n  out = model(feat)\r\n```"", ""Yes, cudnn.h for 5.1.10 defines CUDNN_BN_EPSILON to be 1e-5 (minimum allowable value). I can't check header for 5.1.5. Even with 1e-5 epsilon for many input data distributions the results become pretty unstable. Is there a case for this restriction to be lifted, does using very small epsilon provide benefits?"", ""I see. Not sure if small epsilon is necessary. I am using 1e-6 just to replicate others' configurations. \r\n\r\nShould PyTorch provide more informative error message about this restriction? "", ""Ideally, CUDNN should provide more informative error messages (pytorch can not know why cudnn is returning an error code in each particular case) , but that's a long standing discussion."", 'It just hits on me. As mentioned by @soumith , TensorFlow does not use CuDNN batchnorm by default. So is this possible in Pytorch, to not use default CuDNN batchnorm?', 'Never mind. I found the switch in `torch._C._functions.BatchNorm`.']",[],['RuntimeError: CUDNN_STATUS_BAD_PARAM'],0,0
579,pytorch,17048,closed,Can not subclass `subclass torch.multiprocessing.Pool()`,"## üêõ Bug

Trying to subclass:

Ends up with:


This does makes sense, as it is not possible to subclass python's  as well - but only .  
 is **not** a type, but rather a context method:


The main problem is that  doesn't expose the  module - so it impossible to subclass  


This might be related to #16954 - but I don't think it's a dupe, as the main problem here is exposing the  module.

## To Reproduce

Steps to reproduce the behavior:

1. 

## Expected behavior
It should be possible to subclass torch's multiprocessing pool.

## Environment

PyTorch version: 1.0.0
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 16.04 LTS,
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CMake version: Could not collect

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: Tesla P100-PCIE-12GB
Nvidia driver version: 390.30
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.14.3
[pip3] torch==1.0.0
[pip3] torchvision==0.2.1
[conda] Could not collect",,"[""Doesn't the following work?\r\n```\r\nfrom torch.multiprocessing.pool import Pool\r\nclass MyPool(Pool):\r\n    pass\r\n```\r\nIt does for me..."", 'No, it doesn\'t. At least not on python3.6.\r\n\r\nThis is what happens when you try to instantiate a `MyPool()` object:\r\n```python\r\nfrom torch.multiprocessing.pool import Pool\r\nclass MyPool(Pool):\r\n    pass\r\n p = MyPool()\r\n```\r\noutput:\r\n```\r\nTraceback (most recent call last):\r\n  File ""test_subclass_pool.py"", line 5, in <module>\r\n    p = MyPool()\r\n  File ""/Users/ilaigiloh/anaconda2/envs/gal3/lib/python3.6/multiprocessing/pool.py"", line 157, in __init__\r\n    self._setup_queues()\r\n  File ""/Users/ilaigiloh/anaconda2/envs/gal3/lib/python3.6/site-packages/torch/multiprocessing/pool.py"", line 23, in _setup_queues\r\n    self._inqueue = SimpleQueue()\r\nTypeError: __init__() missing 1 required keyword-only argument: \'ctx\'\r\n```', ""Oh wait - this is a different problem. It seems like now I'm hitting #16954. I guess it does work... üòÑ  \r\nThanks!"", 'Closing this in favor of #16954 ']","['python\r\nclass MyPool(torch.multiprocessing.Pool):\r\n   pass\r\n', 'bash\r\nTypeError: method expected 2 arguments, got 3\r\n', 'python\r\nprint ( torch.multiprocessing.Pool)\r\n#output:\r\n<bound method BaseContext.Pool of <multiprocessing.context.DefaultContext object at 0x10f4bd550>>\r\n', ""bash\r\nAttributeError: module 'torch.multiprocessing' has no attribute 'pool'\r\n""]","['multiprocessing.Pool', 'multiprocessing.pool.Pool', 'multiprocessing.Pool', 'torch', 'torch.multiprocessing.pool', 'torch.multiprocessing.pool.Pool()', 'torch.pool', 'class MyPool(torch.multiprocessing.Pool):']",0,0
580,pytorch,18873,closed,Run shellcheck on OSS builds,,module: ci triaged,[],[],[],0,0
581,pytorch,9226,closed,warn if batchsize < #gpu in data parallel,,,"[""I'm not sure this should be a warning. It can happen at the end of an epoch where the left-over batch is much smaller."", ""@colesbury solid point... uhh, I really want to have an actual code warning if people try to use DP with suboptimal batch size because it seems that many users don't really understand DP and just treat it as a blackbox.""]",[],[],0,0
582,pytorch,8393,closed,[jit][script] Can't allocate zero gradients for a value without a type,"See also https://github.com/pytorch/pytorch/issues/8410

Example:


output:



Operator level tests that fail with this:

- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
",oncall: jit,"[""Shouldn't all types default to `cpu/float`?"", ""I made this pretty hastily cleaning up tasks. The underlying problem here is that the example specifies requires_grad but the gradient for the output in undefined. We try to create a zero gradient tensor, but we don't have the type (including shape) of the output for some reason"", '@zdevito was this closed by https://github.com/pytorch/pytorch/pull/8641?']","['\r\nimport torch\r\n@torch.jit.script\r\ndef unsqueezel(x):\r\n    return x.unsqueeze(dim=-1)\r\nx = torch.randn((2, 2), requires_grad=True)\r\nunsqueezel(x)\r\n', '\r\nTraceback (most recent call last):\r\n  File ""dim.py"", line 6, in <module>\r\n    unsqueezel(x)\r\nRuntimeError: torch/csrc/jit/autodiff.cpp:199: createZerosLike: can\'t allocate zero gradient for a value without a type\r\n']","['test_add', 'test_add_broadcast_all', 'test_add_broadcast_lhs', 'test_add_broadcast_rhs', 'test_add_scalar', 'test_add_scalar_broadcast_lhs', 'test_add_scalar_broadcast_rhs', 'test_sub', 'test_sub_broadcast_all', 'test_sub_broadcast_lhs', 'test_sub_broadcast_rhs', 'test_sub_scalar_broadcast_lhs', 'test_sub_scalar_broadcast_rhs']",0,0
583,pytorch,947,closed,"thpp Tensor templatized over Device (and maybe Density, e.g sparse vs dense)","Right now, THPP exposes four templatized classes:



I propose to consolidate these into a single class, like so:

We can then do the following:

and so on.

In this fashion, nothing about the current API would change (and so all current code that uses THPP would all work the same), but users of THPP would be able to choose what level of templatization they want. If there are some current methods that are only in some subset of the classes, we can either do template class specialization, or simply make those methods part of all the classes.

I am interested in this because I am using THPP in a separate project, in which it would be cleaner to have the Tensor class to be templatized over at least the Device (in addition to the Type), as it is in most other frameworks (e.g MXNet).

I am happy to do this and submit a PR if the developers are interested, otherwise I will just make those changes locally.",triage review,"[""I'm not sure merging over density makes sense, as the function signatures themselves might want to be different (we want to retain this flexibility)"", ""fair enough -- I didn't really look at that, I'm only really interested in the Dense tensors anyways.\r\n\r\nWe can do something like:\r\n```\r\nTorchTensor<T,D>\r\nand\r\nTorchSparseTensor<T,D>\r\n```\r\n\r\nOr even just the `TorchTensor<T,D>`.\r\n\r\nWould that be good with you guys?"", 'will wait for @colesbury and @apaszke to respond, they are the heavy users of THPP.', 'I think we could add that, but can you first tell me how would you like to implement that?', ""I would probably do it very similar to how it's done now.\r\n\r\nDifferent files in generic/ for the different devices, the explicit template instantiation over <real,device> instead of just <real>. That would allow for the real differences in implementation between cuda and cpu, thought i would bet there's a way to reconcile it into one implementation w/ different macros, that just doesn't seem worth it though.\r\n\r\nThen, unify the interface, the only thing different atm in the interface as far as I can tell is that THCTensor has a THCState* state whereas the THTensor doesn't. I think I would change that to void* state in both, anyways I think it would be good to give the cpu Tensor a void* state (or at least, maybe, change state to be a virtual function returning a void* which is NULL for THTensor), because the THNN functions now take a THNNState* anyways which as far as I can tell is just void*, and expected to be NULL. This would help unify the code that would bring THNN and THCUNN into c++ land as well (what I am working on involves this too)."", ""So you mean to remove `THTensor`, etc.? Wouldn't it be simpler to just implement some kind of a trait struct?\r\n```cpp\r\ntemplate<typename real, thd::Device device, thd::Kind kind>\r\nstruct tensor_traits {};\r\n\r\ntemplate<>\r\nstruct tensor_traits<float, thd::Device::GPU, thd::Kind::Dense> {\r\n    using tensor_type = THCTensor<float>;\r\n}\r\n```\r\n\r\nIn your code you could do this then:\r\n\r\n```cpp\r\ntemplate <typename T>\r\nusing MyTensor = tensor_traits<T, thd::Device::CPU, thd::Kind::Dense>;\r\n```"", ""yeah, that's possible, and that's what I've done right now.\r\n\r\nit's really just a convenience/cleanliness thing to me. pretty much everything is templatized over <T,D> when you are writing code in c++, and currently I have something like this everywhere:\r\n\r\ntemplate <typename T, typename D>\r\nsome_templatized_thing\r\n{\r\nusing TTensor = TDTensorTraits<T,D>::TensorType;\r\n}\r\n\r\nand then I use TTensor as my type. It just feels a little weird/unclean in c++ world to me, because most things are templatized over <T,D>, it's odd that the fundamental class (Tensor) isn't.\r\n\r\nbut you're right it's not a functional difference"", 'THPP is being/has been completely phased out in favor of the auto-generated library https://github.com/zdevito/aten which to an extent does what you want.']","['\r\nTorchTensor<Type,Device,Density> (or at least, <Type,Device>).\r\n', '\r\ntemplate <typename T>\r\nusing THTensor = TorchTensor<T,CPU,Dense>\r\n\r\ntemplate <typename T>\r\nusing THCTensor = TorchTensor<T,GPU,Dense>\r\n']","['THTensor<T>, THCTensor<T>, THCSTensor<T>, and THSTensor<T>.']",0,0
584,pytorch,3567,closed,undefined symbol: cudnnSetConvolutionGroupCount while running with cuDNN 7.0.3 and CUDA 9,"Installed from source, tried various branches _(master, v0.3.0, soumith-patch-1)_, but still getting this error.
Full uninstall and reinstall didn't help either. 
Calling  (form torchvision module) works fine, creating variables on the device doesn't seem to provoke this error either.
I've also just ran  and got this [log](https://gist.github.com/VladislavZavadskyy/8b1a5abffe6298fe2908fe0e3bd6cbf9), which suggests that it searches for cudnn 6, but I have no idea why or how to fix it.",,"['There turned out to be sneaky conda package.\r\nFixed by doing `conda uninstall cudnn` and recompiling.', 'Thanks VladislavZavadskyy, this fixed the issue for me too!']",[],"['inception.cuda()', ""LD_DEBUG=libs python -c 'import torch'""]",0,0
585,pytorch,17465,closed,The source code for conv2d,"Hi, I wanna to do some work about the conv2d, but the release did not make the source code of con2d available.  What should I do to approach the conv2d?

## ‚ùì Questions and Help

### Please note that this issue tracker is not a help form and this issue will be closed.

We have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:

- [Discussion Forum](https://discuss.pytorch.org/)
",,"['Convolution base code is present in following file and directory\r\n\r\n- aten/src/ATen/native/Convolution.cpp\r\n- aten/src/THNN/generic/\r\n\r\nwhat do you want to work on?\r\n\r\nWe have a discussion forum here, https://discuss.pytorch.org ']",[],[],0,0
586,pytorch,27296,closed,document: torch.quantize_per_tensor and torch.quantize_per_channel,"add quantized tensor creation functions on torch.tensor( ) documentation, add the torch.quantize_per_tensor() and torch.quantize_per_channel(). 


@raghuramank100 for visibility",enhancement module: docs quantization_release_1.3 triaged,['this is done see the various functions in https://pytorch.org/docs/stable/tensors.html'],[],[],0,0
587,pytorch,15589,closed,unable to load python-trained model with libtorch c++ api in Windows,"version: stable 1.0.0
get from the url
https://download.pytorch.org/libtorch/cu90/libtorch-win-shared-with-deps-latest.zip

test code

#include <torch/torch.h>
#include <torch/script.h>

#include<fstream>
#include <iostream>
using namespace std;

int main(int argc, const char* argv[]) {

  at::Tensor tensor = torch::rand({2, 3}).to(at::kCUDA);
  std::cout << tensor << std::endl;


  /**/
  string model_name = ""model.pt"";
  try {
	  
	  std::ifstream in(model_name, std::ios_base::binary);

	  if (in.fail()) {
		  cout << ""failed to open model"" << endl;
	  }
	  else {
		  cout << ""successed to open model"" << endl;
	  }

	  AT_CHECK(!in.fail(), ""load: could not open file "", model_name);

	  cout << ""parsed checking"" << endl;
	   

	  std::shared_ptr<torch::jit::script::Module> module = torch::jit::load(model_name);
	  module->to(at::kCUDA);

	  assert(module != nullptr);
	  std::cout << ""ok\n"";

	  std::vector<torch::jit::IValue> inputs;
	  inputs.push_back(torch::ones({ 1, 3, 224, 224 }).to(at::kCUDA));

	  // Execute the model and turn its output into a tensor.
	  at::Tensor output = module->forward(inputs).toTensor();

	  std::cout << output.slice(/*dim=*/1, /*start=*/0, /*end=*/5) << '\n';
	 
  }
  catch (exception & err) {
	  cout << err.what() << endl;
  }
   
  

  cout << ""finished"" << endl;



}


output:

D:\learn\cv\lab\example-app\build6\Debug>example-app.exe  model.pt
 0.6020  0.1421  0.9155
 0.6821  0.7416  0.7934
[ Variable[CUDAFloatType]{2,3} ]
load: could not open file  (load at ..\torch\csrc\jit\import.cpp:250)
(no backtrace available)
finished

the model is the python trained,with the following code
import torch
import torchvision

# An instance of your model.
model = torchvision.models.resnet18()

# An example input you would normally provide to your model's forward() method.
example = torch.rand(1, 3, 224, 224)

# Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.
traced_script_module = torch.jit.trace(model, example)

# save
traced_script_module.save(""model.pt"")


it should be in the right position, as the check code shows.",module: windows,"['I can reproduce this. Workground: Use the `load` function that takes the `ifstream` object instead of the `std::string` object. That is,\r\n```cpp\r\n// Change this\r\nstd::shared_ptr<torch::jit::script::Module> module = torch::jit::load(model_name);\r\n// to\r\nstd::shared_ptr<torch::jit::script::Module> module = torch::jit::load(in);\r\n```', 'This is not a bug. For MSVC, `std::string` is not defined the same for the configurations Debug and Release. Since we built libtorch with the __Release__ configuration, if you try to use the __Debug__ configuration when compiling the executable, the `std::string` object could not be passed into the torch library, and hence the error. I tried building it with Release using the following command and it worked fine. \r\n```cmd\r\nset CMAKE_BUILD_TYPE=Release\r\nmsbuild INSTALL.vcxproj /p:Configuration=%CMAKE_BUILD_TYPE%\r\n```', ""@peterjc123 I changed the string to ifstream,  it can find the model but still aborts with the following error\r\n\r\n**std::_Hash<std::_Umap_traits<std::basic_string<char,std::char_traits<char>,std::allocator<char> >,unsigned __int64,std::_Uhash_compare<std::basic_string<char,std::char_traits<char>,std::allocator<char> >,std::hash<std::basic_string<char,std::char_traits<char>,std::allocator<char> > >,std::equal_to<std::basic_string<char,std::char_traits<char>,std::allocator<char> > > >,std::allocator<std::pair<std::basic_string<char,std::char_traits<char>,std::allocator<char> > const ,unsigned __int64> >,0> >::_Vec_lo**(...) ËøîÂõû 0x6„ÄÇ\r\n\r\nhowever, when i change the mode from Debug to Release in Visual Studio 2017, still with the string object with input, it works and get the desired output, with no error\r\n\r\nI test the latest load function in ubuntu 16.04, with gcc 4.9.3,the same code\r\nthrows the error\r\nterminte called after throwing an instance of 'std::bad_alloc'\r\nwait(): std::bad_alloc\r\nAborted(core dumped)\r\nwhen I comment out the line\r\n#the code\r\nauto output = module->forward(inputs).toTensor();\r\nthe programme processed without this error tip\r\n\r\n"", 'also, in Release mode, with the ifstream with input, it get the desired output without exception', '@xinheblue Yes, that is expected. We should not mix up the build configurations. So this is not a bug. Would you please close this issue?', 'Thanks @peterjc123 , but if one wants to develop in Debug mode with the prebuilt library, the current issue may still exists.', ""Then you'll need the debug version. Unluckily, we don't currently provide this."", 'thanks, close the issue now', 'thanks, @peterjc123 @xinheblue  according you advices, I solved!']",[],[],0,0
588,pytorch,27507,closed,[ONNX] Export torch.meshgrid,"## üöÄ Feature
Export meshgrid to ONNX

## Motivation

Instead of an error:



Can use  and 

## Pitch

Use  and  to export meshgrid

## Alternatives

Use  and  in the code

",module: onnx triaged,['There is an open PR adding support for meshgrid in https://github.com/pytorch/pytorch/pull/26037'],"[""python\r\n90     global _registry\r\n---> 91     return _registry[(domain, version)][opname]\r\nKeyError: 'meshgrid'\r\n""]","['view', 'expand', 'view', 'expand', 'view', 'expand']",0,0
589,pytorch,2062,closed,Installation error ,"I am installing pytorch from source using python 2.7. I am getting the following error:

 File ""/scratch0/Softwares/pytorch_new/torch/lib/ATen/gen.py"", line 6, in <module>
    import preprocess_declarations
  File ""/scratch0/Softwares/pytorch_new/torch/lib/ATen/preprocess_declarations.py"", line 3, in <module>
    from function_wrapper import TYPE_FORMAL_GENERIC
  File ""/scratch0/Softwares/pytorch_new/torch/lib/ATen/function_wrapper.py"", line 2, in <module>
    from code_template import CodeTemplate
  File ""/scratch0/Softwares/pytorch_new/torch/lib/ATen/code_template.py"", line 13, in <module>
    class CodeTemplate(object):
  File ""/scratch0/Softwares/pytorch_new/torch/lib/ATen/code_template.py"", line 15, in CodeTemplate
    '(^[^\n\S]*)?\$([^\d\W]\w*|\{,?[^\d\W]\w*\,?})', re.MULTILINE)
  File ""/scratch0/Softwares/virtualpython/env_new/lib64/python2.7/re.py"", line 190, in compile
    return _compile(pattern, flags)
  File ""/scratch0/Softwares/virtualpython/env_new/lib64/python2.7/re.py"", line 242, in _compile
    raise error, v # invalid expression
sre_constants.error: nothing to repeat

CMake Error at CMakeLists.txt:126 (message):
  Failed to get generated_cpp list


-- Configuring incomplete, errors occurred!



",module: dependency bug,"['Might be related to recently merge pr https://github.com/pytorch/pytorch/pull/2033 I guess.', '@yogeshbalaji what is your cmake version? `cmake --version`', '@soumith  cmake version 3.8.1', '@yogeshbalaji what is your Python version? `python --version`', 'maybe @yogeshbalaji has a debug build of python? Alban sent this PR to fix something that looks similar; https://github.com/pytorch/pytorch/pull/2065', 'I\'m thinking it may be related to: https://stackoverflow.com/questions/3675144/regex-error-nothing-to-repeat given that the line here:\r\n\r\n```\r\nFile ""/scratch0/Softwares/pytorch_new/torch/lib/ATen/code_template.py"", line 15, in CodeTemplate\r\n\'(^[^\\n\\S])?$([^\\d\\W]\\w|{,?[^\\d\\W]\\w*,?})\', re.MULTILINE)\r\n```\r\n\r\nHas a regex string that matches this error case, which apparently was fixed in newer versions of Python.', 'Yeah, I cannot repro on both `3.6.1` and `2.7.13`', ""@killeent The version I am using is Python 2.7.5, however I'm running it in virtualenv"", ""@yogeshbalaji maybe that's the issue, prob fixed in `2.7.12` / `2.7.13`"", 'it does say: `This bug seems to have been fixed between 2.7.5 and 2.7.6.` on the stackoverflow page', 'The python version was indeed the issue. Worked when I used Python 2.7.12.\r\nThanks everyone for the help.']",[],[],0,0
590,pytorch,10617,closed,"Command ""python setup.py egg_info"" failed with error code 1","Trying to install pytorch on my pyenv python 2.7



Installed other libraries like numpy and pandas which worked fine.
pip list gives:

I am on macOS 10.13.5.",,"[""i'm curious, what was the issue?"", 'I had python 2.7.0 installed, when I switched to 2.7.15 it worked fine. ']","['\r\npip install torch\r\nCollecting torch\r\n  Using cached https://files.pythonhosted.org/packages/5f/e9/bac4204fe9cb1a002ec6140b47f51affda1655379fe302a1caef421f9846/torch-0.1.2.post1.tar.gz\r\n    Complete output from command python setup.py egg_info:\r\n    Traceback (most recent call last):\r\n      File ""<string>"", line 1, in <module>\r\n      File ""/private/var/folders/32/4_yd4dkj6xj469sw24cmj8_w0000gn/T/pip-install-yfJzlA/torch/setup.py"", line 11, in <module>\r\n        raise RuntimeError(README)\r\n    RuntimeError: PyTorch does not currently provide packages for PyPI (see status at https://github.com/pytorch/pytorch/issues/566).\r\n\r\n    Please follow the instructions at http://pytorch.org/ to install with miniconda instead.\r\n\r\n\r\n    ----------------------------------------\r\nCommand ""python setup.py egg_info"" failed with error code 1 in /private/var/folders/32/4_yd4dkj6xj469sw24cmj8_w0000gn/T/pip-install-yfJzlA/torch/\r\n', '\r\nPackage         Version\r\n--------------- -------\r\ndill            0.2.8.2\r\nez-setup        0.9\r\nnumpy           1.15.0\r\npandas          0.23.4\r\npip             18.0\r\npython-dateutil 2.7.3\r\npytz            2018.5\r\nsetuptools      40.0.0\r\nsix             1.11.0\r\nwheel           0.31.1\r\n']",[],0,0
591,pytorch,11757,closed,Unable to build PyTorch from source without NumPy support,"## Issue description

So far, all the builds in the CI have had NumPy support, meaning that this issue has not been exposed in its entirety yet.

I tried to build PyTorch without NumPy support, and it wasn't possible at first. Editing  made sense to accomplish this, but then it seems that Caffe2 requires NumPy support.

This is the error I got after (almost) building PyTorch.


## Code example

This is what I did to the  file:

## System Info
- PyTorch or Caffe2: PyTorch
- How you installed PyTorch (conda, pip, source): source
- Build command you used (if compiling from source): 
- OS: Ubuntu 18.04.1
- Python version: 3.7
- GCC version (if compiling from source): 7.3
- CMake version: 3.11.3
- Versions of any other relevant libraries: None
",,"[""I'll take this up.""]","['bash\r\nIn file included from /media/vishwak/Official/Projects/pytorch/caffe2/python/pybind_state.cc:1:0:\r\n/media/vishwak/Official/Projects/pytorch/caffe2/python/pybind_state.h:24:10: fatal error: numpy/arrayobject.h: No such file or directory\r\n #include <numpy/arrayobject.h>\r\n          ^~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\n', 'diff\r\ndiff --git a/cmake/Dependencies.cmake b/cmake/Dependencies.cmake\r\nindex dbec0d07e..c8dafb23e 100644\r\n--- a/cmake/Dependencies.cmake\r\n+++ b/cmake/Dependencies.cmake\r\n@@ -397,9 +397,12 @@ if(BUILD_PYTHON)\r\n   set(Python_ADDITIONAL_VERSIONS 3.7 3.6 3.5 2.8 2.7 2.6)\r\n   find_package(PythonInterp 2.7)\r\n   find_package(PythonLibs 2.7)\r\n-  find_package(NumPy REQUIRED)\r\n-  if(PYTHONINTERP_FOUND AND PYTHONLIBS_FOUND AND NUMPY_FOUND)\r\n-    include_directories(SYSTEM ${PYTHON_INCLUDE_DIR} ${NUMPY_INCLUDE_DIR})\r\n+  find_package(NumPy)\r\n+  if(PYTHONINTERP_FOUND AND PYTHONLIBS_FOUND)\r\n+    include_directories(SYSTEM ${PYTHON_INCLUDE_DIR})\r\n+    if(NUMPY_FOUND)\r\n+      include_directories(SYSTEM ${NUMPY_INCLUDE_DIR})\r\n+    endif()\r\n     # Observers are required in the python build\r\n     caffe2_update_option(USE_OBSERVERS ON)\r\n   else()\r\n']","['Dependencies.cmake', 'Dependencies.cmake', 'NO_TEST=1 NO_CAFFE2_OPS=1 MAX_JOBS=3 NO_CUDA=1 python setup.py build']",0,0
592,pytorch,2754,closed,ppc64le test_cuda.py failures,"I am using ppc64 Ubuntu 16.04. 
I compiled pytorch version 0.2.0 and am running into some problems with test_cuda.py.
To isolate the failures I commented out some of the types and float_types like so:

types = [
\#    torch.FloatTensor,
\#    torch.DoubleTensor,
\#    torch.LongTensor,
\#    torch.IntTensor,
\#    torch.ShortTensor,
    torch.CharTensor,
\#    torch.ByteTensor,
]

float_types = [
   torch.FloatTensor,
\#   torch.DoubleTensor
]  # TODO: add half...

Then I run ""python test_cuda.py""
I get a bunch of these errors:
/opt/pytorch/torch/lib/THC/THCTensorTopK.cuh:431: void gatherTopK(TensorInfo<T, IndexType>, IndexType, IndexType, IndexType, IndexType, TensorInfo<T, IndexType>, IndexType, IndexType, TensorInfo<long, IndexType>, IndexType) [with T = char, IndexType = unsigned int, Dim = 3, Order = true]: block: [64,0,0], thread: [9,0,0] Assertion  failed.

the only difference in each line are the block and thread data 

I also get a bunch of these for different tests:

  ERROR: test_CharTensor_tril (__main__.TestCuda)

----------------------------------------------------------------------
Traceback (most recent call last):
  File ""mytest_cuda.py"", line 358, in tmp
    gpu_tensor = to_gpu(cpu_tensor)
  File ""/opt/pytorch/test/common.py"", line 89, in to_gpu
    return obj.clone().type(t)
  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/torch/_utils.py"", line 35, in _type
    return new_type(self.size()).copy_(self, async)
RuntimeError: cuda runtime error (59) : device-side assert triggered at /opt/pytorch/torch/lib/THC/generic/THCTensorCopy.c:18

all failing at the same THCTensorCopy.c:18

All these failures only happen with type = torch.CharTensor

here is output of nvidia-smi
\# nvidia-smi
Fri Sep 15 21:54:54 2017
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.66                 Driver Version: 384.66                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 00000000:03:00.0 Off |                    0 |
| N/A   45C    P0    59W / 149W |    426MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla K80           Off  | 00000000:04:00.0 Off |                    0 |
| N/A   30C    P8    30W / 149W |      1MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla K80           Off  | 00000002:03:00.0 Off |                    0 |
| N/A   32C    P8    26W / 149W |      1MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla K80           Off  | 00000002:04:00.0 Off |                    0 |
| N/A   25C    P8    29W / 149W |      1MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+

",,"[""hi, if anyone from the community can give me some pointers on where to keep looking to fix this problem, I would appreciate it.  I've narrowed the failures to 3 operations in the test_cuda.py\r\n   ('topk', small_3d_unique, lambda t: [2, 1, False, True], 'dim_sort'),\r\n   ('topk', small_3d_unique, lambda t: [2, -1, False, True], 'neg_dim_sort'),\r\n   ('topk', small_3d_unique, lambda t: [2, 1, True, True], 'dim_desc_sort'),\r\n\r\nAnd this only happens when using  torch.CharTensor  (but in cuda)\r\n\r\nAnd in the file torch/lib/THC/THCTensorTopK.cuh,  the failure is happening in \r\nfunction gatherTopK  , line 431   \r\nassert(writeIndex < outputSliceSize);\r\n\r\nI print out the writeIndex and outputSliceSize,  the writeIndex starts at 0 and goes up to a max of  but the outputSliceSize remains at 2.  \r\n\r\nAlso, I do not know if this has any bearing but on ppc64 char is unsigned while on x86 char is signed.\r\nI appreciate any pointers on which part of the code I can be looking into.\r\n\r\n\r\n\r\n"", 'This seemed to have been fixed by #2440', 'issue #2440 changed the use of char in /torch/lib/THC/THCNumerics.cuh to int_8 and uint_8 and therefore fixed the problem in ppc64le as well']",[],['writeIndex < outputSliceSize'],0,0
593,pytorch,25801,closed,[jit] Missing source highlight error,"



cc @suo",good first issue jit-backlog oncall: jit,"['I would like to work on this issue. Can someone give me some context or let me know how to start?', ""RIght now the output is like\r\n```\r\nRuntimeError:\r\nNumber of type annotations (2) did not match the number of function parameters (1):\r\n# type: (List[int], int) -> bool\r\n```\r\nwhich doesn't highlight the source of the error like our other error messages, e.g.:\r\n```python\r\n@torch.jit.script\r\ndef fn(x):\r\n    # type: (List[int]) -> bool\r\n    return max(x)\r\n```\r\n\r\noutputs\r\n\r\n```\r\nRuntimeError:\r\nArguments for call are not valid.\r\nThe following operator variants are available:\r\n\r\n  prim::max(int a, int b) -> (int):\r\n  Expected a value of type 'int' for argument 'a' but instead found type 'List[int]'.\r\n\r\n  prim::max(float a, float b) -> (float):\r\n  Expected a value of type 'float' for argument 'a' but instead found type 'List[int]'.\r\n\r\n  prim::max(int a, float b) -> (float):\r\n  Expected a value of type 'int' for argument 'a' but instead found type 'List[int]'.\r\n\r\n  prim::max(float a, int b) -> (float):\r\n  Expected a value of type 'float' for argument 'a' but instead found type 'List[int]'.\r\n\r\nThe original call is:\r\nat ../test.py:18:11\r\n@torch.jit.script\r\ndef fn(x):\r\n    # type: (List[int]) -> bool\r\n    return max(x)\r\n           ~~~ <--- HERE\r\n```\r\n\r\nOur parser (`torch/csrc/jit/script/parser.cpp`) should be including it, but the source range might be getting lost somewhere."", 'Hi @driazati , I am looking forward to contribute to PyTorch. Is it still an open issue?', 'Edit: I think my comment below were unnecessarily long. Simply put, my question is:\r\n\r\nFor this example code,\r\n\r\n```\r\nimport torch\r\n@torch.jit.script\r\ndef f1(x):\r\n    # type: (int, int) -> None\r\n    pass\r\n```\r\n\r\nWhich error report should be implemented?\r\n\r\n1. ""HERE"" under python function declaration\r\n\r\n```\r\nRuntimeError:\r\nNumber of type annotations (2) did not match the number of function parameters (1):\r\nat __scratch__/example.py:2:0\r\ndef f1(x):\r\n~~~~~~~~ <--- HERE\r\n    # type: (int, int) -> None\r\n    pass\r\n```\r\n\r\nor\r\n\r\n2. ""HERE"" under type comment annotation\r\n\r\n```\r\nRuntimeError:\r\nNumber of type annotations (2) did not match the number of function parameters (1):\r\nat __scratch__/example.py:2:0\r\ndef f1(x):\r\n    # type: (int, int) -> None\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n    pass\r\n```\r\n\r\n---\r\n\r\nOriginal comment:\r\n\r\nHi @driazati, I was looking  around the code of pytorch jit mechanism.\r\nI found that this ""type annotation comment"" parsing is currently done in a separate parser independent from whole source as in:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/6a4ca9abec1c18184635881c08628737c8ed2497/torch/csrc/jit/script/init.cpp#L878-L881\r\n\r\nThat\'s why `SourceRange` (`type_annotation_decl.range()`) passed to `ErrorReport` here\r\n\r\nhttps://github.com/pytorch/pytorch/blob/6a4ca9abec1c18184635881c08628737c8ed2497/torch/csrc/jit/script/parser.cpp#L12-L28\r\n\r\ndoesn\'t include source location information as a part of whole source.\r\n\r\nI can think of two ways to approach this problem though.\r\n\r\nOne quick way to ""fix"" this is to use `throw ErrorReport(decl.range())` instead of `throw ErrorReport(type_annotation_decl.range())`, which would work like this:\r\n\r\n```\r\n=== Example code ( __scratch__/example.py) ==\r\n\r\nimport torch\r\ndef f1(x):\r\n    # type: (int, int) -> None\r\n    pass\r\ntorch.jit.script(f1)\r\n\r\n\r\n\r\n=== Error message ==\r\n\r\nRuntimeError: \r\nNumber of type annotations (2) did not match the number of function parameters (1):\r\nat __scratch__/example.py:2:0\r\ndef f1(x):\r\n~~~~~~~~ <--- HERE\r\n    # type: (int, int) -> None\r\n    pass\r\n```\r\n\r\nSecond way is to properly handle `SourceRange` for `type_annotation_decl`, which, I think, requires to implement a better version of `parse_type_comment` or some way to inject proper `SourceRange` into `Decl` afterward. A bit complicated part is that ""type annotation comment"" extraction is currently handled in python code (`torch.jit.annotations.get_type_line`)\r\n\r\nhttps://github.com/pytorch/pytorch/blob/6a4ca9abec1c18184635881c08628737c8ed2497/torch/jit/annotations.py#L137\r\n\r\nwhich doesn\'t concern with SourceRange concept used in cpp-part of parser/lexer.\r\n\r\n\r\nAnyways, if you guys are fine with the first way of fixing this, then I can make PR. But if more proper fix is desired, I might not be able to work on this.\r\nPlease let me know how you think. Thanks.\r\n\r\nBtw, reading pytorch source is so interesting, thank you very much for such a great code base.', ""Thanks for digging into this!\r\n\r\nI think 1 and 2 are pretty similar, but I'm slightly partial to 2 since it's saying what was expected (the function parameters) vs what is actually there (the type annotations), and highlighting the actual part. But since they're so similar I think it's fine to do 1, especially since that change is so simple.\r\n\r\nGo ahead and make the PR (and add a test for it! see `test_jit.py`)""]","['\r\n@torch.jit.script\r\ndef fn(x):\r\n    # type: (List[int], int) -> bool\r\n    return max(x)\r\n']",[],0,0
594,pytorch,3946,closed,"Any Pytorch function can work as Keras's ""TimeDistributed""?","In keras,there is a timedistributed function (https://github.com/fchollet/keras/blob/master/keras/layers/wrappers.py)  which can apply a layer to each temporal slice, I hope the author can develop a function like that.",,"['you can do `TimeDistributed` in pytorch with a regular `for` loop.', 'Indeed, this works with the default ""eager mode"". However, I cannot make it to work with a for loop in TorchScript mode...', '@julioasotodv for loops are supported in TorchScript mode, as documented', '@soumith indeed, but only looping over range() or a tuple; not over a tensor', ""@julioasotodv we do support looping over a Tensor's size.\r\n\r\n```\r\nIn [3]: @torch.jit.script\r\n   ...: def foo(x):\r\n   ...:     for i in range(x.size(0)):\r\n   ...:         print(i)\r\n   ...:\r\n\r\nIn [4]: foo(torch.randn(10))\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n```\r\n\r\nYou can also loop on the value of a Tensor:\r\n\r\n```\r\nIn [5]: @torch.jit.script\r\n   ...: def foo(x):\r\n   ...:     for i in range(int(x[0])):\r\n   ...:         print(i)\r\n   ...:\r\n\r\nIn [6]: foo(torch.full((5,), fill_value=10.0))\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n```""]",[],[],0,0
595,pytorch,16693,closed,[build error] Build caffe error,"## ‚ùìError while building latest pytorch



I did the installing steps presented in the official website. This is on macOS.

UI.:
Interesting that there is string.h in the anaconda directory:
",module: build triaged,"['similar to #16602', '> similar to #16602\r\n\r\nI do not see how this is similar to #16602', ""It seems like there is something wrong with your system headers. I'm not sure how we can help here. You could try blowing away your Anaconda environment and trying again.""]","['\r\n[ 70%] Building CXX object caffe2/torch/lib/libshm/CMakeFiles/torch_shm_manager.dir/manager.cpp.o\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/local/cuda/include/cuda_runtime.h:In file included from <built-in>:1:\r\nIn file included from /usr/local/cuda/include/cuda_runtime.hIn file included from <built-in>:1:\r\nIn file included from /usr/local/cuda/include/cuda_runtime.h:120:\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/local/cuda/include/cuda_runtime.hIn file included from <built-in>:1:\r\nIn file included from /usr/local/cuda/include/cuda_runtime.h:120:\r\nIn file included from /usr/local/cuda/include/crt/common_functions.h:77:\r\n/anaconda2/bin/../include/c++/v1/string.h:61:15: fatal error: \'string.h\' file not found\r\n#include_next <string.h>\r\n              ^~~~~~~~~~\r\nIn file included from :120:\r\nIn file included from /usr/local/cuda/include/crt/common_functions.h:77:\r\n/anaconda2/bin/../include/c++/v1/string.h:61:15: fatal error: \'string.h\' file not found\r\n#include_next <string.h>\r\n              ^~~~~~~~~~\r\n120:\r\nIn file included from /usr/local/cuda/include/crt/common_functions.h:77:\r\n/anaconda2/bin/../include/c++/v1/string.h:61:15: fatal error: \'string.h\' file not found\r\n#include_next <string.h>\r\n              ^~~~~~~~~~\r\n:120:\r\nIn file included from /usr/local/cuda/include/crt/common_functions.h:77:\r\n/anaconda2/bin/../include/c++/v1/string.h:61:15: fatal error: \'string.h\' file not found\r\n#include_next <string.h>\r\n              ^~~~~~~~~~\r\n<built-in>:1:\r\nIn file included from /usr/local/cuda/include/cuda_runtime.h:120:\r\nIn file included from /usr/local/cuda/include/crt/common_functions.h:77:\r\n/anaconda2/bin/../include/c++/v1/string.h:61:15: fatal error: \'string.h\' file not found\r\n#include_next <string.h>\r\n              ^~~~~~~~~~\r\nIn file included from /usr/local/cuda/include/crt/common_functions.h:77:\r\n/anaconda2/bin/../include/c++/v1/string.h:61:15: fatal error: \'string.h\' file not found\r\n#include_next <string.h>\r\n              ^~~~~~~~~~\r\n[ 70%] Built target caffe2_pybind11_state\r\n[ 70%] Building NVCC (Device) object caffe2/CMakeFiles/caffe2_gpu.dir/__/aten/src/THC/caffe2_gpu_generated_THCTensorCopy.cu.o\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/local/cuda/include/cuda_runtime.h:120:\r\nIn file included from /usr/local/cuda/include/crt/common_functions.h:77:\r\n/anaconda2/bin/../include/c++/v1/string.h:61:15: fatal error: \'string.h\' file not found\r\n#include_next <string.h>\r\n              ^~~~~~~~~~\r\n1 error generated.\r\nCMake Error at caffe2_gpu_generated_THCSleep.cu.o.Release.cmake:219 (message):\r\n  Error generating\r\n  /Users/Dani/pytorch/build/caffe2/CMakeFiles/caffe2_gpu.dir/__/aten/src/THC/./caffe2_gpu_generated_THCSleep.cu.o\r\n\r\n\r\nmake[2]: *** [caffe2/CMakeFiles/caffe2_gpu.dir/__/aten/src/THC/caffe2_gpu_generated_THCSleep.cu.o] Error 1\r\nmake[2]: *** Waiting for unfinished jobs....\r\n1 error generated.\r\nCMake Error at caffe2_gpu_generated_THCBlas.cu.o.Release.cmake:219 (message):\r\n  Error generating\r\n  /Users/Dani/pytorch/build/caffe2/CMakeFiles/caffe2_gpu.dir/__/aten/src/THC/./caffe2_gpu_generated_THCBlas.cu.o\r\n\r\n\r\nmake[2]: *** [caffe2/CMakeFiles/caffe2_gpu.dir/__/aten/src/THC/caffe2_gpu_generated_THCBlas.cu.o] Error 1\r\n1 error generated.\r\nCMake Error at caffe2_gpu_generated_THCReduceApplyUtils.cu.o.Release.cmake:219 (message):\r\n  Error generating\r\n  /Users/Dani/pytorch/build/caffe2/CMakeFiles/caffe2_gpu.dir/__/aten/src/THC/./caffe2_gpu_generated_THCReduceApplyUtils.cu.o\r\n\r\n\r\nmake[2]: *** [caffe2/CMakeFiles/caffe2_gpu.dir/__/aten/src/THC/caffe2_gpu_generated_THCReduceApplyUtils.cu.o] Error 1\r\n1 error generated.\r\n1 error generated.\r\nCMake Error at caffe2_gpu_generated_THCTensor.cu.o.Release.cmake:219 (message):\r\n  Error generating\r\n  /Users/Dani/pytorch/build/caffe2/CMakeFiles/caffe2_gpu.dir/__/aten/src/THC/./caffe2_gpu_generated_THCTensor.cu.o\r\n\r\n\r\nmake[2]: *** [caffe2/CMakeFiles/caffe2_gpu.dir/__/aten/src/THC/caffe2_gpu_generated_THCTensor.cu.o] Error 1\r\nCMake Error at caffe2_gpu_generated_THCStorageCopy.cu.o.Release.cmake:219 (message):\r\n  Error generating\r\n  /Users/Dani/pytorch/build/caffe2/CMakeFiles/caffe2_gpu.dir/__/aten/src/THC/./caffe2_gpu_generated_THCStorageCopy.cu.o\r\n\r\n\r\nmake[2]: *** [caffe2/CMakeFiles/caffe2_gpu.dir/__/aten/src/THC/caffe2_gpu_generated_THCStorageCopy.cu.o] Error 1\r\n1 error generated.\r\nCMake Error at caffe2_gpu_generated_THCTensorCopy.cu.o.Release.cmake:219 (message):\r\n  Error generating\r\n  /Users/Dani/pytorch/build/caffe2/CMakeFiles/caffe2_gpu.dir/__/aten/src/THC/./caffe2_gpu_generated_THCTensorCopy.cu.o\r\n\r\n\r\nmake[2]: *** [caffe2/CMakeFiles/caffe2_gpu.dir/__/aten/src/THC/caffe2_gpu_generated_THCTensorCopy.cu.o] Error 1\r\n1 error generated.\r\nCMake Error at caffe2_gpu_generated_THCStorage.cu.o.Release.cmake:219 (message):\r\n  Error generating\r\n  /Users/Dani/pytorch/build/caffe2/CMakeFiles/caffe2_gpu.dir/__/aten/src/THC/./caffe2_gpu_generated_THCStorage.cu.o\r\n\r\n\r\nmake[2]: *** [caffe2/CMakeFiles/caffe2_gpu.dir/__/aten/src/THC/caffe2_gpu_generated_THCStorage.cu.o] Error 1\r\nmake[1]: *** [caffe2/CMakeFiles/caffe2_gpu.dir/all] Error 2\r\nmake[1]: *** Waiting for unfinished jobs....\r\n[ 70%] Linking CXX executable ../../../../bin/torch_shm_manager\r\n[ 70%] Built target torch_shm_manager\r\nmake: *** [all] Error 2\r\nTraceback (most recent call last):\r\n  File ""setup.py"", line 720, in <module>\r\n    build_deps()\r\n  File ""setup.py"", line 279, in build_deps\r\n    build_dir=\'build\')\r\n  File ""/Users/Dani/pytorch/tools/build_pytorch_libs.py"", line 248, in build_caffe2\r\n    check_call([\'make\', \'-j\', str(max_jobs), \'install\'], cwd=build_dir, env=my_env)\r\n  File ""/anaconda2/envs/pytorch/lib/python3.7/subprocess.py"", line 347, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command \'[\'make\', \'-j\', \'8\', \'install\']\' returned non-zero exit status 2.\r\n', '\r\nls /anaconda2/bin/../include/c++/v1/\r\n__bit_reference\t\t\tcfenv\t\t\t\tfloat.h\t\t\t\tset\r\n__bsd_locale_defaults.h\t\tcfloat\t\t\t\tforward_list\t\t\tsetjmp.h\r\n__bsd_locale_fallbacks.h\tchrono\t\t\t\tfstream\t\t\t\tshared_mutex\r\n__config\t\t\tcinttypes\t\t\tfunctional\t\t\tsstream\r\n__cxxabi_config.h\t\tciso646\t\t\t\tfuture\t\t\t\tstack\r\n__debug\t\t\t\tclimits\t\t\t\tinitializer_list\t\tstdbool.h\r\n__functional_03\t\t\tclocale\t\t\t\tinttypes.h\t\t\tstddef.h\r\n__functional_base\t\tcmath\t\t\t\tiomanip\t\t\t\tstdexcept\r\n__functional_base_03\t\tcodecvt\t\t\t\tios\t\t\t\tstdint.h\r\n__hash_table\t\t\tcomplex\t\t\t\tiosfwd\t\t\t\tstdio.h\r\n__libcpp_version\t\tcomplex.h\t\t\tiostream\t\t\tstdlib.h\r\n__locale\t\t\tcondition_variable\t\tistream\t\t\t\tstreambuf\r\n__mutex_base\t\t\tcsetjmp\t\t\t\titerator\t\t\tstring\r\n__nullptr\t\t\tcsignal\t\t\t\tlimits\t\t\t\tstring.h\r\n__refstring\t\t\tcstdarg\t\t\t\tlimits.h\t\t\tstring_view\r\n__split_buffer\t\t\tcstdbool\t\t\tlist\t\t\t\tstrstream\r\n__sso_allocator\t\t\tcstddef\t\t\t\tlocale\t\t\t\tsupport\r\n__std_stream\t\t\tcstdint\t\t\t\tlocale.h\t\t\tsystem_error\r\n__string\t\t\tcstdio\t\t\t\tmap\t\t\t\ttgmath.h\r\n__threading_support\t\tcstdlib\t\t\t\tmath.h\t\t\t\tthread\r\n__tree\t\t\t\tcstring\t\t\t\tmemory\t\t\t\ttuple\r\n__tuple\t\t\t\tctgmath\t\t\t\tmodule.modulemap\t\ttype_traits\r\n__undef_min_max\t\t\tctime\t\t\t\tmutex\t\t\t\ttypeindex\r\nalgorithm\t\t\tctype.h\t\t\t\tnew\t\t\t\ttypeinfo\r\nany\t\t\t\tcwchar\t\t\t\tnumeric\t\t\t\tunordered_map\r\narray\t\t\t\tcwctype\t\t\t\toptional\t\t\tunordered_set\r\natomic\t\t\t\tcxxabi.h\t\t\tostream\t\t\t\tutility\r\nbitset\t\t\t\tdeque\t\t\t\tqueue\t\t\t\tvalarray\r\ncassert\t\t\t\terrno.h\t\t\t\trandom\t\t\t\tvariant\r\nccomplex\t\t\texception\t\t\tratio\t\t\t\tvector\r\ncctype\t\t\t\texperimental\t\t\tregex\t\t\t\twchar.h\r\ncerrno\t\t\t\text\t\t\t\tscoped_allocator\t\twctype.h\r\n\r\n']",[],0,0
596,pytorch,17537,closed,Cannot initial from_blob from std::vector.data(),"## üêõ Bug


## To Reproduce




I end up getting giberrish tensor returned from this function. Once the function returns to Python
",,"[""This is [documented behaviour](https://pytorch.org/cppdocs/api/function_namespacetorch_1aff6f8e6185457b2b67a1a9f292effe6b.html?#_CPPv4N5torch9from_blobEPvN2at11IntArrayRefERKN2at13TensorOptionsE):\r\n> Exposes the given data as a Tensor without taking ownership of the original data. \r\n\r\nI'm not an expert, but I take that [it's not possible to acquire the data ownership from a vector](https://stackoverflow.com/questions/22576858/is-there-a-way-to-move-data-from-a-vector-to-a-dynamic-array).\r\n"", 'Question 1:\r\n\r\nIs the only solution to do a .clone()\r\n\r\nQuestion 2:\r\n\r\nIs there a way to build push_back to a tensor, just like a vector\r\n```\r\nstd::vector<int> overall_indices;\r\n\r\n// push back into the std::vector unknown number of times\r\noverall_indices.push_back(0);\r\ntorch::from_blob(overall_indices.data(), {n, 3}, opts);\r\n```\r\n\r\nFor example\r\n```\r\ntorch::Tensor overall_indices_tensor;\r\n\r\n// push back into the the tensor unknown number of times\r\noverall_indices_tensor.push_back(0); \r\n```', '1. Yes.\n2. No. If you know the (maximal) size, get an empty tensor. What vector does internally is to realloc and copy as needed, so you could do that if you have to.\n\nAm 27. Februar 2019 16:29:02 MEZ schrieb sidazhang <notifications@github.com>:\n>Question 1:\n>\n>Is the only solution to do a .clone()\n>\n>Question 2:\n>\n>Is there a way to build push_back to a tensor, just like a vector\n>```\n>std::vector<int> overall_indices;\n>\n>// push back into the std::vector unknown number of times\n>overall_indices.push_back(0);\n>torch::from_blob(overall_indices.data(), {n, 3}, opts);\n>```\n>\n>For example\n>```\n>torch::Tensor overall_indices_tensor;\n>\n>// push back into the the tensor unknown number of times\n>overall_indices_tensor.push_back(0); \n>```\n>\n>-- \n>You are receiving this because you commented.\n>Reply to this email directly or view it on GitHub:\n>https://github.com/pytorch/pytorch/issues/17537#issuecomment-467906313\n', 'If you absolutey wanted some fancy thing, you could allocate an object (with ""new"") that takes the container and uses dlpack with a deleter to free itself. Then you can unpack the dlpack into a Tensor because the dlpack-created Tensor takes ownership of the memory. That said, `.clone` surely is the right solution except very special cases.']","['\r\ntorch::Tensor some_function()\r\n{\r\n    std::vector<int> overall_indices;\r\n    // .... fill the vector\r\n    torch::TensorOptions opts(torch::kInt);\r\n    auto t = torch::from_blob(overall_indices.data(), {n, 3}, opts);\r\n    return t\r\n}\r\n', '\r\nsome_function()\r\n-> tensor([[         5,          0,          0],\r\n        [         0, 1040267192,          0],\r\n        [-966454432,      32681,          1],\r\n        [         4,          0,          0]]']",[],0,0
597,pytorch,27626,closed,Cross entropy with sequence log_softmax not (almost) equal to 1,"## ‚ùì Is this a feature or a bug

When using the Cross entropy with K-target class in a sequence application, is it expected to squeeze before calling the loss or we are suppose to just past a vector of minibatch_size, sequence_length, K-tag ?

Because, in this line of code   the  step is giving me prob vector greater than 1 or not (almost) equal to 1. 
> After the exponantial application for sure.

Here an example of my situation, when 
",triage review triaged,"[""I don't know the expected behavior of cross entropy. It would be good to get someone who knows the behavior to chime in."", ""It's seem odd to me the output of the `log_softmax`. But as I was trying to work my head around it i'm not sure of the expected behavior when using sequence."", ""I'm going to close this, because it's really a question for the forums (Please use https://discuss.pytorch.org (https://discuss.pytorch.org/) for questions).  If you find it's a bug, please reopen this.""]","['\r\ny_target = [0, 1, 1, 1, 1, 1, 1, 7, 3, 3, 4, 5]\r\n\r\ny_predict = tensor([[[-3.2910, -3.7040, -3.9663, -4.1059, -4.1012, -4.1368, -4.1158,\r\n              -4.2506, -4.3760, -4.1655, -3.7205, -2.8966],\r\n             [8.8342, 9.8005, 9.9371, 9.5386, 9.1664, 8.5789, 7.4153,\r\n              3.6922, 1.4415, -4.2969, -9.6262, -9.6677],\r\n             [4.8800, 3.9539, 3.4548, 3.2552, 3.2441, 3.1305, 3.0244,\r\n              3.4114, 2.4158, 0.8764, -1.0089, -1.9860],\r\n             [0.9770, 0.4299, 0.5240, 0.6457, 0.3984, 0.4953, 0.4214,\r\n              0.4627, 3.8482, 5.2386, 3.0858, 1.8424],\r\n             [-7.9146, -7.7125, -7.8830, -7.8319, -7.5219, -7.4129, -6.9861,\r\n              -5.2630, -5.4942, 1.5527, 8.9790, 4.6239],\r\n             [-11.0455, -10.9685, -10.4619, -9.8926, -9.7860, -9.2574, -8.6142,\r\n              -6.8966, -3.1693, -1.9010, 2.4475, 10.5265],\r\n             [3.2678, 3.2880, 3.1630, 3.0535, 3.2340, 3.1786, 3.6118,\r\n              3.5331, 1.1905, 0.1634, -0.8760, -2.4547],\r\n             [1.0813, 1.1683, 1.2462, 1.3132, 1.4225, 1.4196, 1.5218,\r\n              1.9667, 1.2652, 0.6920, -0.1019, -0.8134],\r\n             [0.9259, 1.0986, 1.2023, 1.2958, 1.3188, 1.3190, 1.3048,\r\n              1.1060, 1.0091, 0.1047, -0.7875, -0.9094]]])\r\n\r\nlog_softmax(y_predict, 1) # vector prob not (almost) equal to 1\r\nlog_softmax(y_predict, 2) # vector (almost) equal to 1\r\n']","['nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)', 'log_softmax']",0,0
598,pytorch,768,closed,Feature Request: Bitwise Operations,"it seems that there is SOME support for bitwise operations in NumPy:
https://docs.scipy.org/doc/numpy/reference/generated/numpy.bitwise_xor.html
Bitwise operations are also supported by CUDA:
https://docs.nvidia.com/cuda/cuda-c-programming-guide/#warp-shuffle-functions
yet, the implementation of binary operation  in  is 

which takes two ByteTensor. My undertsanding is that this is inefficient, because only one Bit within every Byte of this tensor is actually encoding something. Please correct me if I'm wrong!!

for the record neither TensorFlow has bitwise operations
",medium priority (this tag is deprecated),"['There are some pending PRs implementing bitwise operations in the C backend https://github.com/torch/torch7/pull/933 and https://github.com/torch/cutorch/pull/699', ""You can't optimize it out to work on a single bit, because loads from any memory (both CUDA and RAM) have a certain granularity, that is never smaller than a single byte anyway. Still, merging these two ops into a single kernel would make it faster, but I don't think it should be a bottleneck. Is it too slow for your use case? Or is it just a suggestion?"", '@apaszke I think the PRs that I mentioned do what @avloss meant, as it uses the underlying C bitwise operators.', ""@fmass I think so too. I'm just trying to decide on how to prioritize the task."", 'Thanks for quick response guys! @fmassa is right, I meant bitwise on bits within a byte - `11^21=30` (as is vanilla python). I imagine porting that from torch (given that PR is merged) is not a straight forward thing.', '@avloss I believe once the PRs in torch/cutorch are merged, integrating the changes in pytorch should not be difficult.', 'fixed via #1556 ', '@soumith could you please link to how to use these bitwise operations?\r\n\r\nSearching on the docs, they do not seem to be there:\r\nhttps://pytorch.org/docs/master/search.html?q=bitwise&check_keywords=yes&area=default\r\n\r\nThanks!']",[],"['xor', 'PyTorch', 'return (self + other).eq(1)']",0,0
599,pytorch,12946,closed,[jit][trace] non-argument tensors with requires_grad=True are traced as constants,"## üêõ Bug

non-argument tensors with requires_grad=True are traced as constants

## To Reproduce

## Expected behavior

The output should have requires_grad=True so that one can backprop through it

## Environment
PT master

Reported by @SsnL ",,"['Thanks :)', ""I don't think we want you to allow gradient to flow through constant nodes, it would break the autodiff.cpp badly. A warning or an error would be appropriate here. It should say that all requires_grad=True tensors need to be inputs to the trace."", 'the conclusion was that this is hard to support and we should throw a warning or error when this occurs to alert our users about the change in semantics', 'Errors now.\r\n\r\n```\r\n>>> tf = torch.jit.trace(f, (torch.randn(1),))\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/data/users/ezyang/pytorch-tmp2/torch/jit/_trace.py"", line 780, in trace\r\n    traced = torch._C._create_function_from_trace(\r\n  File ""<stdin>"", line 2, in f\r\nRuntimeError: Cannot insert a Tensor that requires grad as a constant. Consider making it a parameter or input, or detaching the gradient\r\nTensor:\r\n-0.6102\r\n-0.7205\r\n 0.1871\r\n[ torch.FloatTensor{3} ]\r\n```']","['\r\nimport torch\r\nx = torch.randn(3, requires_grad=True)\r\ndef f(inp):\r\n    return (x + inp).sum()\r\n\r\ntf = torch.jit.trace(f, (torch.randn(1),))\r\nf(torch.randn(1))\r\n# expected output: tensor(-1.2899, grad_fn=<SumBackward0>) result requires grad\r\n\r\ntf(torch.randn(1))\r\n# got output: tensor(-3.6019), result does not require grad!\r\n']",[],0,0
600,pytorch,12901,closed,torch.nn.L1Loss works incorrectly in certain situations,"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->
torch.nn.L1Loss()'s parameter *reduction* doesn't work if b = net(a).



## To Reproduce

Steps to reproduce the behavior:




<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Outputs



<!-- A clear and concise description of what you expected to happen. -->

## Environment

PyTorch version: 0.4.1
Is debug build: No
CUDA used to build PyTorch: 9.2.148

OS: Debian GNU/Linux 9.5 (stretch)
GCC version: (Debian 6.3.0-18+deb9u1) 6.3.0 20170516
CMake version: Could not collect

Python version: 3.5
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: Tesla K80
Nvidia driver version: 396.44
cuDNN version: Probably one of the following:
/usr/local/cuda-9.2/lib64/libcudnn.so.7.2.1
/usr/local/cuda-9.2/lib64/libcudnn_static.a

Versions of relevant libraries:
[pip] intel-numpy (1.15.1)
[pip] numpy (1.15.1)
[pip] torch (0.4.1)
[pip] torchvision (0.2.1)
[conda] Could not collect


## Additional context
I have checked the source. In  F.l1_loss(), it use 

to convert reduction to an integer. However, it calls _pointwise_loss(), 
which assumes that reduction is still a str.

That's the reason of this bug.
<!-- Add any other context about the problem here. -->
",,"['Hey, can you please give a better explanation of this bug? I know it\'s legitimate because you showed it to me and Chen, but your explanation isn\'t one the maintainers are going to be able to look at and say ""alright, I know what I need to fix."" I\'ll give you extra credit for finding this, remind me during office hours.', 'I just sat down and looked at the code, and this only impacts an old version of pytorch man', ""> I just sat down and looked at the code, and this only impacts an old version of pytorch man\r\n\r\nWhat do you mean by old version? I'm using v0.4.1 (latest stable release)."", '> Hey, can you please give a better explanation of this bug? I know it\'s legitimate because you showed it to me and Chen, but your explanation isn\'t one the maintainers are going to be able to look at and say ""alright, I know what I need to fix."" I\'ll give you extra credit for finding this, remind me during office hours.\r\n\r\nI\'ve posted an explanation.', ""> > I just sat down and looked at the code, and this only impacts an old version of pytorch man\r\n> \r\n> What do you mean by old version? I'm using v0.4.1 (latest stable release).\r\n\r\nIt has been fixed via https://github.com/pytorch/pytorch/pull/10018."", 'This bug has been fixed on master.']","[""\r\nb = net(a)\r\nl = torch.nn.L1Loss(reduction='elementwise_mean')\r\nl(a, b) # wrong, it's summation \r\n\r\n"", ""\r\nimport torch\r\nimport torch.nn as nn\r\n\r\ndatadim = 33\r\n\r\n\r\nclass BugNet(nn.Module):\r\n    def __init__(self):\r\n        super(BugNet, self).__init__()\r\n        self.output = nn.Linear(datadim, datadim)\r\n\r\n    def forward(self, x):\r\n        return self.output(x)\r\n\r\n\r\na = torch.ones((4, 33))\r\nbug = BugNet()\r\nb = bug(a)\r\n\r\nloss_fn1 = nn.L1Loss(reduction='sum')\r\nloss_fn2 = nn.L1Loss(reduction='elementwise_mean')\r\nelementwise_mean_loss = lambda x, x_out: torch.mean(torch.abs(x-x_out.view_as(x)))\r\n\r\nprint(loss_fn1(a, b))  # 127.6617, True\r\nprint(loss_fn2(a, b))  # 127.6617, False\r\nprint(loss_fn2(b, a))  # 0.9671, True\r\nprint(elementwise_mean_loss(a, b))  # 0.9671, True\r\n"", '\r\ntensor(127.6617, grad_fn=<SumBackward0>)\r\ntensor(127.6617, grad_fn=<SumBackward0>)\r\ntensor(0.9671, grad_fn=<L1LossBackward>)\r\ntensor(0.9671, grad_fn=<MeanBackward1>)\r\n']","['reduction = _Reduction.get_enum(reduction)', ""return torch.mean(d) if reduction == 'elementwise_mean' else torch.sum(d)  # In _pointwise_loss()""]",0,0
601,pytorch,1151,closed,Does pytorch implement SpatialUpSamplingBilinear layer?,"Hi, I want to load a model trained by torch, which contains the layer ""nn.SpatialUpSamplingBilinear"", the convert code is simple as blow:



it will generate the exception:


And I find the  in pytorch,  does it function like the layer ? 
",,['http://pytorch.org/docs/nn.html#upsamplingbilinear2d'],"[""\r\nimport torch\r\nfrom torch.utils.serialization import load_lua\r\nmodel = load_lua('model.net')\r\n"", ""\r\nT7ReaderException: don't know how to deserialize Lua class nn.SpatialUpSamplingBilinear. If you want to ignore this error and load this object as a dict, specify unknown_classes=True in reader's constructor\r\n"", 'class torch.nn.UpsamplingBilinear2d', 'nn.SpatialUpSamplingBilinear']",[],0,0
602,pytorch,16558,closed,Pytorch Import:  failed to map segment Error ,"I installed Pytorch .3.1 using pip3 for python3.5, and the installation was successful.
However, I am unable to load the package in python

import torch
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""user/.local/lib/python3.5/site-packages/torch/__init__.py"", line 56, in <module>
    from torch._C import *
ImportError: libgcc_s.so.1: failed to map segment from shared object


Can anybody help, how to resolve this?

Best",,"[""It seems you don't have gcc installed, either try to install pytorch from conda and it will install gcc for you, or get it from your package manager (apt, if it's ubuntu)"", 'Closing due to lack of activity. Please reopen if you see this error again.']",[],[],0,0
603,pytorch,23589,closed,ValueError: only one element tensors can be converted to Python scalars,"So I have tried the script in the issue **ValueError: only one element tensors can be converted to Python scalars #22674** with the newest Pytorch Version, but it seems doesn't work. Do you have any idea about this

The configuration im using:
Pytorch version: 1.1.0
Python version:3.7.3
torchvision: 0.3.0",,"['Please install the nightly build, and not the latest release.', 'Closing as answered. Feel free to reopen if you have any followup.']",[],[],0,0
604,pytorch,2281,closed,Error installing branch v0.2.0 from source ,"I'm on ubuntu 16.04 with titan X. I am trying to install v0.2.0 branch from source.

I created new conda env  and followed the instructions on Readme.md.

python setup.py install fails with error which look like this:


I'm attaching complete log file: [log_file.txt](https://github.com/pytorch/pytorch/files/1196589/log_file.txt)


Sasank.",,"['Maybe you should clean your directory and try again.\r\n\r\nRun `git clean -xdf` or `python3 setup.py clean` and try again?', 'Wow, how did I miss this! \r\n\r\nThanks a lot @CDLuminate !']","['\r\n... /pytorch/torch/lib/THCUNN/generic/SpatialFullConvolution.cu(18): error: identifier ""THNN_CudaHalfSpatialFullDilatedConvolution_updateOutput"" is undefined\r\n\r\n...\r\n\r\n ... /pytorch/torch/lib/THCUNN/generic/SpatialFullConvolution.cu(55): error: identifier ""THNN_CudaHalfSpatialFullDilatedConvolution_accGradParameters"" is undefined\r\n']",['pytorch'],0,0
605,pytorch,28795,closed,Getting Build Error,"I am building PyTorch and I am getting the following error. Would be great to know how to fix it.

Thank you

",module: build triaged,['Please follow the template to open an issue. We need more information.'],"['\r\n[10/699] Building CXX object caffe2/CMakeFiles/torch.dir/operators/spatial_batch_norm_gradient_op.cc.o\r\nninja: build stopped: subcommand failed.\r\nTraceback (most recent call last):\r\n  File ""setup.py"", line 751, in <module>\r\n    build_deps()\r\n  File ""setup.py"", line 310, in build_deps\r\n    cmake=cmake)\r\n']",[],0,0
606,pytorch,5486,closed,Tensors don't gracefully compare to NoneType,"
at ",,"['@li-roy will look into this', '```python\r\n[torch.Tensor([1, 2]), 1, 2].count(1)\r\n#File ""/home/mscho/vadim/.wigwam/prefix/python/lib/python2.7/site-packages/torch/tensor.py"", line # 168, in __bool__    "" containing more than one value is ambiguous"")\r\n\r\n# what should be the semantics here? \r\n[torch.Tensor([1]), 1, 2].count(1)\r\n#2\r\n```\r\na special case for None will be useful, but can it be handled in general?', ""Note that this doesn't work with NumPy either. The `==` operator does element-wise comparisons for PyTorch and NumPy. I'm not sure changing it just for None is a good idea."", ""Agree. A mid-ground may be returning False for all non-admissible types (not just None), but it won't cover the general scenario fully, and probably won't be discoverable/intuitive."", ""What is the suggested workaround in this circumstance?\r\n`import torch`\r\n`x = [torch.rand((3,2)), None] ` \r\n`if None in x:\r\n    print('None found in list')`"", ""@srinivr Perhaps just write a small function that returns False if it's a Tensor, otherwise does the check for None?"", ""@li-roy  I'm currently iterating through the list and check with `is None` but I wanted to know if there was a better way around it. But thanks for the tip! I can make it a function."", ""I'm using `type(my_tensor) == type(None)`. It works for my case, but I'm wondering if it could be a general solution.""]","['python\r\nimport torch\r\n[torch.Tensor(), None, None].count(None)\r\n\r\n#Traceback (most recent call last):\r\n#  File ""<stdin>"", line 1, in <module>\r\n#  File "".../lib/python2.7/site-packages/torch/tensor.py"", line 312, in __eq__\r\n#    return self.eq(other)\r\n#TypeError: eq received an invalid combination of arguments - got (NoneType), but expected one of:\r\n# * (float value)\r\n#      didn\'t match because some of the arguments have invalid types: (NoneType)\r\n# * (torch.FloatTensor other)\r\n#      didn\'t match because some of the arguments have invalid types: (NoneType)\r\n']",['0.4.0a0+1fdb392'],0,0
607,pytorch,1228,closed,Import torch causes error,"This might be relevant to #691. I have installed pytorch from source on Mac OS successfully, but  causes error:
> 
>
> ImportError                               Traceback (most recent call last)
> <ipython-input-1-c031d3dd82fc> in <module>()
> ----> 1 import torch
> 
> /Users/dqwang/Study/tools/anaconda2/lib/python2.7/site-packages/torch/__init__.pyc in <module>()
>      51 sys.setdlopenflags(_dl_flags.RTLD_GLOBAL | _dl_flags.RTLD_NOW)
>      52 
> ---> 53 from torch._C import *
>      54 
>      55 __all__ += [name for name in dir(_C)
> 
> ImportError: dlopen(/Users/dqwang/Study/tools/anaconda2/lib/python2.7/site-packages/torch/_C.so, 10): Symbol not found: __ZNSs4_Rep20_S_empty_rep_storageE
>   Referenced from: /Users/dqwang/Study/tools/anaconda2/lib/python2.7/site-packages/torch/_C.so
>   Expected in: flat namespace
>  in /Users/dqwang/Study/tools/anaconda2/lib/python2.7/site-packages/torch/_C.so
> 
The author of that thread got away with the issue by installing from conda package rather than source, which solved my case as well. But I have to install from the source because of I need CUDA.",,"['The problem comes from the installer picks up the wrong compiler (somehow it picks up the anaconda gcc other than AppleClang). \r\n\r\nFixed by reinstalling pytorch with `CC=clang CXX=clang++ python setup.py install`. Hints takes from #532.  This should also fix #691.', 'Thanks, I\'ve updated the ""From source"" instructions to reflect this here: https://github.com/pytorch/pytorch/blob/master/README.md#from-source']",[],['import torch'],0,0
608,pytorch,1752,closed,dockerfile: directory '.' is not installable. file 'setup.py' not found,"I am using the dockerfile, but the there is an error: directory '.' is not installable. file 'setup.py' not found.

I believe it's located at the line ""pip install -v ."" Anyone know how to fix it? Thanks!",,['You have to clone the repo first. '],[],[],0,0
609,pytorch,24587,closed,Migrate `log10` and `log10_` from the TH to Aten (CUDA),"Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review.",better-engineering module: porting triaged,[],[],[],0,0
610,pytorch,7660,closed,Canonicalize aten operators in IR,"Currently many operators in the aten IR have multiple forms.

Any operator that takes a non-tensor argument (e.g. IntList or int64_t) will have two forms: one with the non-tensor arguments encoded as tensor inputs and one with the arguments as attributes on the graph.

This makes every pass that we write in the IR harder because it needs to handle both cases.

Proposed solution:

1. Canonicalize on the tensor input format. Graphs are optimized in this format. This format is more general because inputs are not required to be constants.

2. expose the  function in compiler for general usage. Optimizations then check for const-ness of these inputs to work.

3. Before interpretation, lower back to the attribute-only format when thing are constants.  already includes this function, it just has to be written into a pass.",oncall: jit,['We got rid of attributes for any aten operator so i believe this has been fixed.'],[],"['at::optional<T> constant_as<T>(Value*)', 'compiler.cpp']",0,0
611,pytorch,1930,closed,Different behavior of sum() in-place on CPU vs GPU,,high priority module: cpu module: cuda module: numerical-stability todo triaged,"[""passing the input as the output for a function that does resizes should be an error; these functions usually resize the outputs at the start of the call and it's likely they don't even do the calculations correctly since that also modifies the input.\r\n\r\nOr maybe we just check that output tensors aren't resized?  (this is what numpy does).  I believe @zdevito had a use case where he wanted to pass in a big buffer, or let it grow dynamically, and just wanted to use that for the output without worrying about the sizes himself.  Do I have that right, Zach?"", ""The cases I had in mind was when you had done some optimization pass and\ncome up with pre-allocated Tensor buffers rather than relying on a caching\nallocator. I know that two Tensors are not live at the same time, and want\nto use the same buffer for two different ops, but I don't want to have to\nmanage their sizes manually because the Op should know what size the output\nwould need to be. In this case, I would make two tensors and back them with\nthe same storage, allowing the op itself to resize each individual tensor\nto the right size for that op.\n\nOn Wed, Jun 28, 2017 at 11:40 AM, gchanan <notifications@github.com> wrote:\n\n> passing the input as the output for a function that does resizes should be\n> an error; these functions usually resize the outputs at the start of the\n> call and it's likely they don't even do the calculations correctly since\n> that also modifies the input.\n>\n> Or maybe we just check that output tensors aren't resized? (this is what\n> numpy does). I believe @zdevito <https://github.com/zdevito> had a use\n> case where he wanted to pass in a big buffer, or let it grow dynamically,\n> and just wanted to use that for the output without worrying about the sizes\n> himself. Do I have that right, Zach?\n>\n> ‚Äî\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/1930#issuecomment-311749984>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAWmGnc6QBxt-FUPHD96rdZD0Uke8f9mks5sIp4agaJpZM4OIWFK>\n> .\n>\n"", 'With recent changes in how reductions are implemented this issue might not be current anymore @melange396 ', 'covered by https://github.com/pytorch/pytorch/issues/17935.']","['\r\nx = torch.randperm(16).resize_(4,4) # create tensor\r\ny = x.cuda() # copy to GPU\r\n# perform same operation on both:\r\nx.sum(dim=0,out=x)\r\ny.sum(dim=0,out=y)\r\nprint(y) # see expected behavior\r\nprint(x) # see zeros instead\r\n']",[],0,0
612,pytorch,10660,closed,Inconsistent output of torch.mm when called in two mathematically equivalent way,"Hi,

 I tired to use torch.mm to get matrix multiplication, but found it returned slightly different results when I called it in two equivalent way.

Assume we have two matrix  matrix_a and matrix_b and want to get the matrix multiplication matrix_a@matrix_b
1. First, I use matrix_a and matrix_b as input directly, torch.mm(matrix_a, matrix_b)
2. On the other hand, I call torch.mm with each row of matrix_a together with matrix_b as inputs, and then concatenate the results.

The two results are expected to be exactly the same, but they are still slightly diferent. So can somebody explain the reason?

Thanks,


Here is my test code.



Here is the output.

>  2018-08-18 21:32:11,294 INFO 87997 [test.py:81] difference between two values: tensor(1.00000e-06 *
>        1.3113)
> 2018-08-18 21:32:11,295 INFO 87997 [test.py:83] batch id: 0
> 2018-08-18 21:32:11,295 INFO 87997 [test.py:84] [-0.89934576  0.11556479 -0.48109883 -0.3961889 ]
> 2018-08-18 21:32:11,295 INFO 87997 [test.py:85] [-0.89934576  0.11556476 -0.4810989  -0.39618894]
> 2018-08-18 21:32:11,296 INFO 87997 [test.py:83] batch id: 1
> 2018-08-18 21:32:11,296 INFO 87997 [test.py:84] [ 2.6500921   0.5928108  -2.213797    0.39320117]
> 2018-08-18 21:32:11,296 INFO 87997 [test.py:85] [ 2.6500921   0.59281075 -2.2137969   0.3932012 ]
> 2018-08-18 21:32:11,296 INFO 87997 [test.py:83] batch id: 2
> 2018-08-18 21:32:11,296 INFO 87997 [test.py:84] [ 1.3148142  3.5158207 -2.2286756 -2.1263554]
> 2018-08-18 21:32:11,297 INFO 87997 [test.py:85] [ 1.3148141  3.515821  -2.2286754 -2.1263554]
> 2018-08-18 21:32:11,297 INFO 87997 [test.py:83] batch id: 3
> 2018-08-18 21:32:11,297 INFO 87997 [test.py:84] [-0.25188264  1.0855039   1.4054772  -0.47265998]
> 2018-08-18 21:32:11,297 INFO 87997 [test.py:85] [-0.25188267  1.085504    1.4054773  -0.47265998]
> 2018-08-18 21:32:11,298 INFO 87997 [test.py:83] batch id: 4
> 2018-08-18 21:32:11,298 INFO 87997 [test.py:84] [ 1.250929   3.790292  -1.7040166 -2.443583 ]
> 2018-08-18 21:32:11,298 INFO 87997 [test.py:85] [ 1.250929   3.790292  -1.7040166 -2.443583 ]",,"['Here\'s a thread where I go through why you dont get the ""exact"" same results, and why this is expected, due to floating point numbers: https://github.com/pytorch/pytorch/issues/9146#issuecomment-409331986']","['\r\ntorch.manual_seed(0)\r\n\r\nmatrix_a = torch.randn([5, 4])\r\nmatrix_b = torch.randn([4, 4])\r\nvalues = torch.randn([5, 4])\r\n\r\n# 1. call torch.mm directly on matrix_a and matrix_b.\r\ntorch.mm(matrix_a, matrix_b, out=values)\r\n\r\n# save the result in buffer.\r\nvalues_1 = values.clone()\r\n\r\n# 2. call torch.mm on each row of matrix_a and matrix_b.\r\nbatch_size = matrix_a.size(0)\r\nfor b in range(batch_size):\r\n    torch.mm(matrix_a[b:b+1], matrix_b, out=values[b:b+1])\r\n\r\n# value and value_1 are supposed to be equal. But they are not.\r\nlogger.info(""difference between two values: %s"" % (torch.sum(torch.abs(values - values_1))))\r\nfor b in range(batch_size):\r\n    logger.info(""batch id: %s"" % b)\r\n    logger.info(values_1[b].data.numpy())\r\n    logger.info(values[b].data.numpy())\r\n']",[],0,0
613,pytorch,1528,closed,LongTensor indexing with duplication not propagating gradients,"It seems like if we use LongTensor indexing to duplicate entries of a Variable, this breaks autograd. Specifically, if any entry in a vector is duplicated, backward() only maintains gradients for the last copy of the element. Simple example:



However:


This is version under version 0.1.11+fc48d2c, tested on 0.1.11+9f3119a as well.  This happens for both cuda and cpu tensors.
",high priority,"['FYI, realized I had been testing the double backprop branch, but this also happens with 0.1.12_2.', 'Less critical than I had originally though (still worth fixing, but there\'s a viable alternative), as it looks like `torch.index_select` has the right behavior, even if it\'s a little odd that it requires a Variable third argument:\r\n\r\n```python\r\ny = Variable(torch.ones(1), requires_grad=True)\r\ny_dup = torch.index_select(y, 0, Variable(torch.LongTensor([0,0])))\r\ny_dup[0].backward() # backprop on first \r\nprint(y.grad)\r\n""""""\r\nIn [42]: Variable containing:\r\n 1\r\n[torch.FloatTensor of size 1]""""""\r\n```', ""Yes, it's because backward of assignments does `grad[index] = grad_output`, and this uses `index_copy_` instead of `index_add_` internally. `index_select` should be correct. I'll fix that today"", 'Additionally, I have another example for this bug.\r\n```python\r\nva = Variable(torch.Tensor([1,1]), requires_grad=True)\r\nidx = torch.LongTensor([0,0,0])\r\nloss = va[idx].sum()\r\nloss.backward()\r\nprint(va.grad)\r\n```\r\noutput:\r\n```\r\nVariable containing:\r\n 1  # supposed to be 3\r\n 0\r\n[torch.FloatTensor of size 2]\r\n```\r\nIt costs me 2 weeks to locate this bug in my script.\r\nPlz fix ASAP.', ""I'll push a fix today. Sorry for the trouble""]","['python\r\ny = Variable(torch.ones(1), requires_grad=True)\r\ny_dup = y[torch.LongTensor([0,0])]\r\ny_dup[1].backward() # backprop on second element\r\nprint(y.grad)\r\n""""""\r\nVariable containing:\r\n 1\r\n[torch.FloatTensor of size 1]""""""\r\n', 'python\r\ny = Variable(torch.ones(1), requires_grad=True)\r\ny_dup = y[torch.LongTensor([0,0])]\r\ny_dup[0].backward()  # backprop on first element, same exact value as second\r\nprint(y.grad)\r\n""""""\r\nVariable containing:\r\n 0\r\n[torch.FloatTensor of size 1]""""""\r\n']",[],0,0
614,pytorch,7276,closed,[feature request] Caffe2 model to PyTorch model,"Do you think about converting Caffe2 model to PyTorch model for continue changing model, e.g fine-tuning, transfer-learning?

According to new feature of PyTorch 1.0, it seems possible, isn't it?",,"[""at the moment this is not planned as such (to create a convertor for caffe2 model to pytorch model). it's not listed in the 1.0 roadmap either.\r\nHowever, you can import both caffe2 and pytorch into the same python, and get the output of caffe2 and send it to your pytorch model, and similarly for the gradients (get them from pytorch backward and send them to caffe2)""]",[],[],0,0
615,pytorch,871,closed,GPU torch.multinomial produces an out-of-bounds index," on the GPU can produce indices that are out of bounds.

Consider the following code:



Here's output from an example run:



Here, the sampled indices should be between 0 to 5, but one of the values is  .

The iteration at which this happens is random.

The following code uses probabilities closer to actual probabilities



but it has the same problem (the problem occurs much later on average though). Example output snippet:

",high priority,"['@ankitkv There are known problems with multinomial. See https://github.com/torch/cutorch/issues/636, https://github.com/torch/cutorch/issues/700 etc. Your code illustrates how this may occur (albeit very infrequently).\r\n\r\ncc @pavanky ', 'In case this is helpful to anyone, a possible temporary workaround is to use\r\n```\r\n_, sample = torch.max(log_dist - torch.log(-torch.log(torch.rand(*log_dist.size()).cuda())), 1)\r\n```\r\nwhere `log_dist` is batchwise log probabilities (e.g. output of `F.log_softmax`).', '@killeent I am not too familiar with why it is breaking :) I just noticed that the tests for MultiNomial were failing frequently and sent in a fix that decreased the frequency. But it looks like there are other issues with it.', ""Sure @pavanky - I just thought the fix you added (iirc by making sure the values weren't out of bounds) might have been related to this problem initially. @soumith I'll try and diagnose what the problem is on this one"", '@ankitkv See my associated PR that fixes the first issue. I have not been able to repro the second issue (ran 20M iterations). Though I have a suspicion of what the problem is.', ""@killeent I was able to reproduce the second issue a bunch of times, and I tried dumping the probability matrix whenever that happens. unfortunately, just loading it and calling `multinomial` on it doesn't seem to reproduce it immediately. based on my outputs in the bug report, this code usually runs into the problem at some point (longest I've seen is 30M iters):\r\n\r\n```\r\nfrom __future__ import print_function\r\n\r\nimport torch\r\n\r\nprob = torch.cuda.FloatTensor(\r\n    [[0.0017,  0.8274,  0.1657,  0.0004,  0.0017,  0.0031],\r\n     [0.0123,  0.0157,  0.0001,  0.0020,  0.0015,  0.9683],\r\n     [0.0067,  0.1369,  0.8092,  0.0422,  0.0030,  0.0021],\r\n     [0.1306,  0.0887,  0.7521,  0.0024,  0.0005,  0.0258],\r\n     [0.0425,  0.5981,  0.0140,  0.0565,  0.1005,  0.1885],\r\n     [0.0006,  0.0017,  0.9819,  0.0114,  0.0038,  0.0007]])\r\n\r\ni = 0\r\nwhile True:\r\n    nprob = prob + torch.randn(prob.size()).cuda()*1e-3\r\n    sampled = torch.multinomial(nprob, 1)\r\n    if ((sampled > 5).sum()):\r\n        print('\\niter', i)\r\n        print('\\nprob:', nprob)\r\n        print('sampled:', sampled)\r\n        break\r\n    if i % 10000 == 0:\r\n        print('iter', i)\r\n    i += 1\r\n```"", 'I have ran for a pretty long time and still see no errors. @ankitkv see my PR https://github.com/torch/cutorch/issues/714. If you want, try cloning CuTorch and patching in that PR. Then you can copy over the THC directory to your PyTorch repo:\r\n\r\n```\r\ncp -r <path/to/cutorch/>lib/THC <path/to/pytorch>/torch/lib/THC\r\n```\r\n\r\nand rebuild PyTorch. If you can repro the issue with that patch in place that would be tremendously useful, or if it fixes your error that is useful as well.', '@killeent I get:\r\n```\r\niter 0\r\n/home/ankit/devel/pytorch/torch/lib/THC/THCTensorRandom.cuh:135: void sampleMultinomialOnce(long *, long, int, T *, T *) [with T = float, AccT = float]: block: [2,0,0], thread: [0,0,0] Assertion `!isinf(sum)` failed.\r\nTHCudaCheck FAIL file=/home/ankit/devel/pytorch/torch/lib/THC/generated/../THCTensorMathCompare.cuh line=84 error=59 : device-side assert triggered\r\nTraceback (most recent call last):\r\n  File ""test.py"", line 11, in <module>\r\n    if ((sampled > 5).sum()):\r\n  File ""/usr/local/lib/python2.7/dist-packages/torch/tensor.py"", line 398, in __gt__\r\n    return self.gt(other)\r\nRuntimeError: cuda runtime error (59) : device-side assert triggered at /home/ankit/devel/pytorch/torch/lib/THC/generated/../THCTensorMathCompare.cuh:84\r\n```\r\non running the first test code from the bug report.', 'the first test code has sums that go to infinity, and hence this device side assert. this is expected. https://github.com/pytorch/pytorch/blob/master/torch/lib/THC/THCTensorRandom.cuh#L134\r\nInstead of silently doing something wrong, it explicitly errors out.', ""I'm running the 2nd test code and 3rd test code overnight. Will report back any failures."", ""I ran it for a while, didn't see any problems. I guess if @soumith doesn't run into a failure, things are good."", ""@killeent @ankitkv It did fail later in the night. So some work to do. I'll compile pytorch with Trevor's open PR ( https://github.com/torch/cutorch/pull/714 ) and see if that fixes things:\r\n\r\nscript 3\r\n```\r\niter 12765047\r\n\r\nprob:\r\n-0.0016  0.8267  0.1659  0.0018  0.0038  0.0020\r\n 0.0124  0.0171  0.0008  0.0031  0.0006  0.9670\r\n 0.0066  0.1375  0.8100  0.0417  0.0018  0.0021\r\n 0.1305  0.0887  0.7545  0.0039 -0.0003  0.0249\r\n 0.0437  0.5982  0.0130  0.0565  0.1013  0.1882\r\n 0.0013  0.0021  0.9804  0.0143  0.0029  0.0003\r\n[torch.cuda.FloatTensor of size 6x6 (GPU 0)]\r\n\r\nsampled:\r\n 1.0000e+00\r\n 5.0000e+00\r\n 4.2805e+18\r\n 2.0000e+00\r\n 4.0000e+00\r\n 2.0000e+00\r\n[torch.cuda.LongTensor of size 6x1 (GPU 0)]\r\n```\r\n\r\nscript 2\r\n```\r\niter 59848171\r\n\r\nprob:\r\n 0.0165  0.0230  0.8879  0.0002  0.0002  0.0723\r\n 0.0013  0.2584  0.0149  0.6633  0.0609  0.0012\r\n 0.7082  0.1997  0.0446  0.0071  0.0002  0.0402\r\n 0.0088  0.1094  0.0770  0.0418  0.0002  0.7628\r\n 0.0021  0.9485  0.0098  0.0276  0.0009  0.0111\r\n 0.8859  0.1016  0.0060  0.0021  0.0039  0.0003\r\n[torch.cuda.FloatTensor of size 6x6 (GPU 0)]\r\n\r\nsampled:\r\n 2.0000e+00\r\n 4.0000e+00\r\n 0.0000e+00\r\n 5.0000e+00\r\n 4.3265e+18\r\n 0.0000e+00\r\n[torch.cuda.LongTensor of size 6x1 (GPU 0)]\r\n```"", ""Ok Trevor's latest PR fixes it. I'll merge it into pytorch now."", 'fixed in master now via: https://github.com/pytorch/pytorch/commit/dfca8dfdc5988813ed5673589ffa4fdd1c4f3d2d', 'For researchers who met this kind of error also.\r\n\r\nCould you check whether none-image file is in your dataset directory?\r\nIn my case, the error was exactly same and the cause was that.\r\n\r\n(Additionally, in fb.resnet.torch, this is exactly same.)']","[""\r\nfrom __future__ import print_function\r\nimport torch\r\n\r\ni = 0\r\nwhile True:\r\n    logdist = torch.zeros(6, 6).cuda()  # no problem without .cuda()\r\n    # the following is not logically correct, but is the fastest way I've found to reproduce the bug\r\n    logdist.log_normal_(mean=0.0, std=2.5)\r\n    prob = torch.exp(logdist)\r\n    sampled = torch.multinomial(prob, 1)  # multinomial is supposed to reweight to get probabilities\r\n    if ((sampled > 5).sum()):\r\n        print('\\niter', i)\r\n        print('\\nprob:', prob)\r\n        print('sampled:', sampled)\r\n        break\r\n    if i % 5000 == 0:\r\n        print('iter', i)\r\n    i += 1\r\n"", '\r\niter 0\r\niter 5000\r\n\r\niter 6491\r\n\r\nprob: \r\n 3.0595e+38  1.2861e+00  5.2613e+37  1.3855e+00  2.9041e+00  1.2441e+01\r\n 1.0531e+01  1.0103e+00  2.0693e+00  1.8121e+00  1.6328e+00  6.0454e+10\r\n 1.5679e+03  1.5869e+00  1.5553e+03  5.1932e+03  5.0801e+01  1.3416e+00\r\n 2.2532e+00  1.5512e+00  3.1946e+01  1.5208e+00  8.8690e+00  1.5255e+00\r\n 1.6197e+02  1.0395e+00  1.1355e+38  3.9969e+00  8.2150e+00  1.5104e+05\r\n 1.1240e+00  1.1315e+00  7.9896e+00  6.8996e+00  2.1447e+00  1.3858e+00\r\n[torch.cuda.FloatTensor of size 6x6 (GPU 0)]\r\n\r\nsampled: \r\n 9.0078e+18\r\n 5.0000e+00\r\n 3.0000e+00\r\n 3.0000e+00\r\n 2.0000e+00\r\n 1.0000e+00\r\n[torch.cuda.LongTensor of size 6x1 (GPU 0)]\r\n', ""\r\nfrom __future__ import print_function\r\n\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport torch.nn.functional as F\r\n\r\ni = 0\r\nwhile True:\r\n    data = torch.zeros(6, 6).cuda()  # no problem without .cuda()\r\n    data.normal_(mean=0.0, std=2.5)\r\n    logdist = F.log_softmax(Variable(data)).data\r\n    prob = torch.exp(logdist)  # prob.sum(1) are 1s.\r\n    sampled = torch.multinomial(prob, 1)\r\n    if ((sampled > 5).sum()):\r\n        print('\\niter', i)\r\n        print('\\nprob:', prob)\r\n        print('sampled:', sampled)\r\n        break\r\n    if i % 10000 == 0:\r\n        print('iter', i)\r\n    i += 1\r\n"", '\r\n...\r\niter 27310000\r\niter 27320000\r\niter 27330000\r\niter 27340000\r\n\r\niter 27346838\r\n\r\nprob: \r\n 0.0017  0.8274  0.1657  0.0004  0.0017  0.0031\r\n 0.0123  0.0157  0.0001  0.0020  0.0015  0.9683\r\n 0.0067  0.1369  0.8092  0.0422  0.0030  0.0021\r\n 0.1306  0.0887  0.7521  0.0024  0.0005  0.0258\r\n 0.0425  0.5981  0.0140  0.0565  0.1005  0.1885\r\n 0.0006  0.0017  0.9819  0.0114  0.0038  0.0007\r\n[torch.cuda.FloatTensor of size 6x6 (GPU 0)]\r\n\r\nsampled: \r\n 1.0000e+00\r\n 4.1752e+18\r\n 1.0000e+00\r\n 2.0000e+00\r\n 1.0000e+00\r\n 2.0000e+00\r\n[torch.cuda.LongTensor of size 6x1 (GPU 0)]\r\n']","['torch.multinomial', '9.0078e+18']",0,0
616,pytorch,13787,closed,Dataloader Segmentation Fault when using MPI backend & single process per gpu,"## üêõ Bug

When using DistributedDataParallel with mpi backend and assigning each gpu a single process on the host, program crashes at the end of an epoch. Failure is not always consistent.




## To Reproduce
[https://github.com/pytorch/examples/blob/master/imagenet/main.py](https://github.com/pytorch/examples/blob/master/imagenet/main.py)

run


Modifications:



## Environment

## Additional context

Crash is less likely to occur if the  is decreased,  or the  is increased.

core dump generated


[pytorchsegfault](https://github.com/pytorch/pytorch/files/2572658/pytorchsegfault.txt)

",oncall: distributed,"[""Distributed Data Parallel doesn't support MPI backend, please use NCCL backend for multiprocess Distributed Data Parallel Training"", 'We should document this better.  ', '@ygfeng See: https://pytorch.org/docs/master/nn.html#distributeddataparallel', '@teng-li Are there plans or a roadmap on when MPI would be supported for c10d?', '@ygfeng MPI is already supported in c10d, but DDP has limited testing/support.  Why do you need to use MPI for GPU-based DDP btw?  If you are using GPU with one process per GPU, you should just use NCCL, which is significantly faster than CUDA-aware MPI.', 'We were interested in testing our own custom MPI implementation and thought it would work. Is there any workarounds for the DataLoader segmentation faults?', 'Dataloader is using fork() by default. But MPI is not fork safe and I am sure you will see this warning in OpenMPI\r\n\r\n```\r\nA process has executed an operation involving a call to the\r\n""fork()"" system call to create a child process.  Open MPI is currently\r\noperating in a condition that could result in memory corruption or\r\nother system errors; your job may hang, crash, or produce silent\r\ndata corruption.  The use of fork() (or system() or other calls that\r\ncreate child processes) is strongly discouraged.\r\n\r\nThe process that invoked fork was:\r\n\r\n  Local host:          [[22789,1],7] (PID 31147)\r\n\r\nIf you are *absolutely sure* that your application will successfully\r\nand correctly survive a call to fork(), you may disable this warning\r\nby setting the mpi_warn_on_fork MCA parameter to 0.\r\n```\r\n\r\nSo, the only thing I would suggest on trying is to set python\'s multiprocessing start method to be either `spawn` or `fork_server`.\r\n\r\nFor now, we don\'t plan on supporting MPI for GPU-based Distributed Data Parallel (DDP). The user should use NCCL for multiprocessing DDP for GPU training.  ']","['\r\n[82362] *** Process received signal ***\r\n[82362] Signal: Segmentation fault (11)\r\n[82362] Signal code: Address not mapped (1)\r\n[82362] Failing at address: 0x1d3fca368\r\n[82362] [ 0] [0x7fffa5f504d8]\r\n[82362] [ 1] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0x11c258)[0x7fffa5d8c258]\r\n[82362] [ 2] /lib64/libc.so.6(__libc_malloc+0x8c)[0x7fffa597945c]\r\n[82362] [ 3] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0xdabe0)[0x7fffa5d4abe0]\r\n[82362] [ 4] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(_PyObject_GenericSetAttrWithDict+0x11c)[0x7fffa5d5915c]\r\n[82362] [ 5] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyObject_GenericSetAttr+0x1c)[0x7fffa5d5937c]\r\n[82362] [ 6] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyObject_SetAttr+0xb0)[0x7fffa5d588a0]\r\n[82362] [ 7] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x698c)[0x7fffa5df989c]\r\n[82362] [ 8] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0xd18)[0x7fffa5dfea78]\r\n[82362] [ 9] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x8150)[0x7fffa5dfb060]\r\n[82362] [10] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0xa68c)[0x7fffa5dfd59c]\r\n[82362] [11] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0xd18)[0x7fffa5dfea78]\r\n[82362] [12] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0xba5b4)[0x7fffa5d2a5b4]\r\n[82362] [13] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyObject_Call+0x74)[0x7fffa5cd9754]\r\n[82362] [14] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyObject_CallMethod+0xd0)[0x7fffa5cd9cc0]\r\n[82362] [15] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_ReInitThreads+0xa0)[0x7fffa5df17d0]\r\n[82362] [16] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyOS_AfterFork+0x68)[0x7fffa5e5fa08]\r\n[82362] [17] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0x1f63b4)[0x7fffa5e663b4]\r\n[82362] [18] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x823c)[0x7fffa5dfb14c]\r\n[82362] [19] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0xd18)[0x7fffa5dfea78]\r\n[82362] [20] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0xba5b4)[0x7fffa5d2a5b4]\r\n[82362] [21] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyObject_Call+0x74)[0x7fffa5cd9754]\r\n[82362] [22] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0x812e0)[0x7fffa5cf12e0]\r\n[82362] [23] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyObject_Call+0x74)[0x7fffa5cd9754]\r\n[82362] [24] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0x11c220)[0x7fffa5d8c220]\r\n[82362] [25] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(+0x1179a8)[0x7fffa5d879a8]\r\n[82362] [26] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyObject_Call+0x74)[0x7fffa5cd9754]\r\n[82362] [27] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0x1788)[0x7fffa5df4698]\r\n[82362] [28] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalFrameEx+0xa68c)[0x7fffa5dfd59c]\r\n[82362] [29] /mnt/pai/home/yfeng/anaconda2/bin/../lib/libpython2.7.so.1.0(PyEval_EvalCodeEx+0xd18)[0x7fffa5dfea78]\r\n[82362] *** End of error message ***\r\nTraceback (most recent call last):\r\nTraceback (most recent call last):\r\n  File ""pytorch-examples/imagenet/main.py"", line 415, in <module>\r\n    main()\r\n  File ""pytorch-examples/imagenet/main.py"", line 242, in main\r\n    prec1 = validate(val_loader, model, criterion)\r\n  File ""pytorch-examples/imagenet/main.py"", line 332, in validate\r\n    for i, (input, target) in enumerate(val_loader):\r\n  File ""/opt/pytorch/lib/python2.7/site-packages/torch/utils/data/dataloader.py"", line 819, in __iter__\r\n    return _DataLoaderIter(self)\r\n  File ""/opt/pytorch/lib/python2.7/site-packages/torch/utils/data/dataloader.py"", line 584, in __init__\r\n    self._put_indices()\r\n  File ""/opt/pytorch/lib/python2.7/site-packages/torch/utils/data/dataloader.py"", line 646, in _put_indices\r\n    indices = next(self.sample_iter, None)\r\n  File ""/opt/pytorch/lib/python2.7/site-packages/torch/utils/data/sampler.py"", line 161, in __iter__\r\n    batch.append(idx)\r\n  File ""/opt/pytorch/lib/python2.7/site-packages/torch/utils/data/dataloader.py"", line 274, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 82360) is killed by signal: Segmentation fault. \r\n', ""\r\n        local_rank = int(os.getenv('OMPI_COMM_WORLD_LOCAL_RANK'))\r\n        world_size = int(os.getenv('OMPI_COMM_WORLD_SIZE'))\r\n...\r\n        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\r\n                                world_size=world_size)\r\n...\r\n        torch.cuda.set_device(local_rank)\r\n        model = torch.nn.parallel.DistributedDataParallel(model,\r\n                                                          device_ids=[local_rank],\r\n                                                          output_device=local_rank)\r\n"", '\r\nCollecting environment information...\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Red Hat Enterprise Linux Server 7.5 (Maipo)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)\r\nCMake version: Could not collect\r\n\r\nPython version: 2.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: \r\nGPU 0: Tesla V100-SXM2-32GB\r\nGPU 1: Tesla V100-SXM2-32GB\r\nGPU 2: Tesla V100-SXM2-32GB\r\nGPU 3: Tesla V100-SXM2-32GB\r\n\r\nNvidia driver version: 410.72\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-10.0/targets/ppc64le-linux/lib/libcudnn.so.7.3.0\r\n/usr/local/cuda-10.0/targets/ppc64le-linux/lib/libcudnn_static.a\r\n', '\r\n(gdb) bt\r\n#0  0x00007fffa24065e0 in _int_malloc () from /lib64/libc.so.6\r\n#1  0x00007fffa240945c in malloc () from /lib64/libc.so.6\r\n#2  0x00007fffa27c7490 in PyList_New () from /opt/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#3  0x00007fffa27c7948 in list_concat () from /opt/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#4  0x00007fffa275ebf0 in PyNumber_Add () from /opt/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n#5  0x00007fffa2886e90 in PyEval_EvalFrameEx () from /opt/anaconda2/bin/../lib/libpython2.7.so.1.0\r\n']","['mpirun -n 4 python pytorch-examples/imagenet/main.py --dist-backend mpi --batch-size 64 -a resnet50 --epochs 2 /mnt/imagenetPyTorch/', 'num_workers', 'bucket_cap_mb']",0,0
617,pytorch,125,closed,make tests deterministic,"by default the tests should be deterministic, individually.
randomized tests can be run nightly or something, but not by default.
",todo,['Fixed in #161.\n'],[],[],0,0
618,pytorch,30986,closed,"Wrong result for CPU implementation (m,).addmv((m, 0), (0,)) when BLAS is not used","## üêõ Bug



gives



To be fixed in https://github.com/pytorch/pytorch/pull/30898",triaged,"['cc: @VitalyFedyunin @csarofeen @ptrblck ', 'This is supposed to be tested in `test_blas_alpha_beta_empty`, but this test has poor coverage that only applies to the code path where BLAS is enabled.', 'fixed']","[""python\r\nimport torch\r\n\r\ndef run(dtype):\r\n    a = torch.ones((5,), device='cpu', dtype=dtype)\r\n    b = torch.empty((5, 0), device='cpu', dtype=dtype)\r\n    c = torch.empty((0,), device='cpu', dtype=dtype)\r\n    print(a.addmv(b, c, alpha=1, beta=3))\r\n\r\nrun(torch.float)\r\nrun(torch.int)\r\n"", '\r\ntensor([3., 3., 3., 3., 3.])\r\ntensor([9, 9, 9, 9, 9], dtype=torch.int32)\r\n']",[],0,0
619,pytorch,11901,closed,[feature request] ability to nest ParameterList,"In some cases, it is more convenient to arrange some parameters as 2d or 3d arrays rather than 1D list. One could solve it by computing a Nd to 1D offset array, but this could be avoided if the  objects could be nested

Example:


",,"[""I really don't see how this can be useful... Can you give an example?"", 'you can get normal nested lists by putting a bunch of ParameterLists into a ModuleList', '@CharlesJQuarra Hi, have you figured it out? I want to use 2-D Modulelist, can we do it like this:\r\n\r\nIn the `__init__() `of a module class, I want to pack and register some conv layers like this:\r\n\r\n```\r\nself.outs = nn.ModuleList(\r\n            [nn.ModuleList([Conv(inp_dim + j * increase, oup_dim, 1, relu=False, bn=False) for j in range(5)]) for i in\r\n             range(nstack)])\r\n```\r\nthen in the forward() method, I want to use the conv layers in the nested module list.\r\n`preds_instack.append(self.outs[i][j](features_instack[j]))`']","['\r\nclass someModule(nn.Module):\r\n  def __init__(self, x, y):\r\n     params = []\r\n     for ix in range(x):\r\n       param_row = []\r\n       for iy in range(y):\r\n         param_row.append( nn.Parameter(torch.randn(1)))\r\n       params.append( nn.ParameterList(param_row))\r\n    param_array = nn.ParameterList(params)\r\n']",['nn.ParameterList'],0,0
620,pytorch,4031,closed,NVIDIA memory not deallocated after interupt,"When I interrupt my Pytorch script using Ctrl-C occasionally GPU memory is not deallocated. Also threads related to my script may or may not still be running. If they are running I kill them using ""kill -9 PID"". However this does not deallocate memory on the GPU. 

> Tue Dec  5 14:12:11 2017       
> +-----------------------------------------------------------------------------+
> | NVIDIA-SMI 384.98                 Driver Version: 384.98                    |
> |-------------------------------+----------------------+----------------------+
> | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
> | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
> |===============================+======================+======================|
> |   0  TITAN X (Pascal)    Off  | 00000000:02:00.0  On |                  N/A |
> | 31%   54C    P2    58W / 250W |   7829MiB / 12189MiB |      3%      Default |
> +-------------------------------+----------------------+----------------------+
>                                                                                
> +-----------------------------------------------------------------------------+
> | Processes:                                                       GPU Memory |
> |  GPU       PID   Type   Process name                             Usage      |
> |=============================================================================|
> |    0      1213      G   /usr/lib/xorg/Xorg                           166MiB |
> |    0      2355      G   compiz                                       249MiB |
> |    0     32412      G   ...-token=00DF6CCAB2487BD6F9AE70914F3A9358     8MiB |
> +-----------------------------------------------------------------------------+",,"['try: `killall python`. Some processes dont show up in `nvidia-smi`', 'I do:\r\n\r\n> ls -Al | grep python\r\n\r\nI then kill any python instance that is still running using:\r\n\r\n> kill -9 PID\r\n\r\nYet the memory is still allocated on the GPU.', 'Hi,\r\n\r\nYou can run `lsof /dev/nvidia0` to list all processes using the GPU. One of them is taking some memory, kill it.', ""I've had this issue a couple of times as well. I added a function to a small repo of GPU utilities that I have to kill processes that are using GPUs as shown by `lsof /dev/nvidia*` but which aren't listed by `nvidia-smi`. Take a look [here](https://github.com/neighthan/gpu-utils/blob/c6329f21fa3780ee89985b7974194cb808643f5c/gpu_utils/utils.py#L114) if you're interested."", ""Closing due to age and proposed solution. Please reopen if you're still experiencing this issue. ""]",[],[],0,0
621,pytorch,23236,closed,AttributeError: module 'torchvision.models' has no attribute 'detection',"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->

Detection module is not present in torchvision.models

## To Reproduce

Steps to reproduce the behavior:

1. import detection from torchvision,models in a python script

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


 - PyTorch Version (e.g., 1.0):
 - OS (e.g., Linux):
 - How you installed PyTorch (, , source):
 - Build command you used (if compiling from source):
 - Python version:
 - CUDA/cuDNN version:
 - GPU models and configuration:
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->
",module: vision triaged,"['If this is an issue related to torchvision, you should bring it up here.\r\nhttps://github.com/pytorch/vision/issues', ""Faced the same, realized it is a brand new feature only available in latest version 1.1.0 not before that... \r\n\r\nFunny part is I had migrated from 0.4.1 just a day back . \r\n\r\nEven though the homepage gives you the command to install pt 1.1.0 but, \r\nTurns out default `conda install pytorch torchvision -c pytorch `gives you the following : \r\n\r\n> pytorch            pytorch/linux-64::pytorch-1.0.0-py3.6_cuda9.0.176_cudnn7.4.1_1\r\n> torchvision        pytorch/noarch::torchvision-0.2.2-py_3\r\n\r\ndo instead a force install :\r\n`conda install pytorch=1.1.0 torchvision -c pytorch` \r\ngives you the following : \r\n\r\n> pytorch-1.1.0              |py3.6_cuda10.0.130_cudnn7.5.1_0       455.0 MB  pytorch\r\n> torchvision-0.3.0          |py36_cu10.0.130_1         3.7 MB  pytorch\r\n\r\n\r\nNow you do get the torchvision.models.detection module .. \r\nBut torch.cuda.is_available()  is giving me false now. \r\nIt downgraded\r\n> cudatoolkit                                    10.1.168-0 --> 10.0.130-0 \r\n\r\nI have cuda version 10.1 installed. \r\nLet's try : \r\n`conda install pytorch=1.1.0 torchvision cudatoolkit=10.1 -c pytorch`\r\n\r\nugh - \r\n\r\n> UnsatisfiableError: The following specifications were found to be in conflict:\r\n>   - cudatoolkit=10.1\r\n>   - pytorch=1.1.0 -> cudatoolkit[version='>=10.0,<10.1']\r\n\r\n\r\n@soumith  is there any plans to add support for Cuda 10.1 in near future ? \r\na pre built binary would really help. ""]","['\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n']","['conda', 'pip']",0,0
622,pytorch,7528,closed,[caffe2] C2 ONNX build is flaky due to lack of ordering dep,"When it fails it looks like this:



This indicates we're trying to run the proto compiler before the proto file is created. This probably means there should be some dep declared which we didn't. It's intermittent because sometimes we're lucky and the build gets ordered the right way.

Maybe this should get reported to onnx repo.

CC @houseroad @bddppq @anderspapitto 

Cases of this failing:
- https://github.com/pytorch/pytorch/pull/7465
- https://ci.pytorch.org/jenkins/job/caffe2-builds/job/py2-mkl-ubuntu16.04-build/4286//console",caffe2 module: onnx,"['This is being fixed https://github.com/onnx/onnx/pull/934', 'OK closing as an upstream issue.']","['\r\n02:41:21 [ 53%] Running C++ protocol buffer compiler on /var/lib/jenkins/workspace/build/third_party/onnx/onnx/onnx-operators_ONNX_NAMESPACE_FOR_C2_CI.proto\r\n02:41:21 onnx_ONNX_NAMESPACE_FOR_C2_CI.proto: File not found.\r\n02:41:21 onnx-operators_ONNX_NAMESPACE_FOR_C2_CI.proto: Import ""onnx_ONNX_NAMESPACE_FOR_C2_CI.proto"" was not found or had errors.\r\n02:41:21 onnx-operators_ONNX_NAMESPACE_FOR_C2_CI.proto:64:12: ""NodeProto"" is not defined.\r\n02:41:21 third_party/onnx/CMakeFiles/onnx_proto.dir/build.make:68: recipe for target \'third_party/onnx/onnx/onnx-operators_ONNX_NAMESPACE_FOR_C2_CI.pb.cc\' failed\r\n']",[],0,0
623,pytorch,24856,closed,[jit] schema matching incorrectly types a call to append with an argument of type Scalar,"Minimal Repro:



cc @suo",jit-backlog oncall: jit,"['@bhosmer', 'this is fixed on master']","[""\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef foo(x):\r\n    a = torch.jit.annotate(List[float], [])\r\n    a.append(x.item())\r\n    # this works\r\n    # a.append(float(x.item()))\r\n    return a\r\n\r\n\r\nprint(foo.graph)\r\n# note: it derives the type Scalar[] for the result of append\r\n# when it should but 'a' is a float[]\r\n\r\nfoo(torch.rand([]))\r\n\r\n""]",[],0,0
624,pytorch,25176,closed,Tensor slicing with boolean numpy mask wrong,"## üêõ Bug
Numpy arrays of dtype  should be interpreted as masks when slicing torch arrays, just like tensors of dtype  are. Insead, such arrays are interpreted as indices.

## To Reproduce
Steps to reproduce the behavior:


Output

## Expected behavior
The expected behaviour is that the result is the same as for slicing with torch tensors with the same dtype, where for the input

we get the expected output


## Environment
 - PyTorch Version: 1.2.0a0+0885dd2
 - OS: Ubuntu 16.04.6 LTS
 - How you installed PyTorch: Nvidia docker image ()
 - Python version: 3.6
 - CUDA/cuDNN version: Nvidia driver version: 384.183, 
 - GPU models and configuration: Tesla V100
",module: boolean tensor module: numpy triaged,"['cc @izdeby', 'Already fixed. \r\nRepro code: \r\n```\r\nimport torch\r\nimport numpy as np\r\nt = torch.tensor(range(5))\r\nmask = np.array([False, True, True, False, True])\r\n\r\nprint(t[mask.astype(np.bool)])\r\nprint(t[mask.astype(np.uint8)])\r\n```\r\n\r\noutput: \r\n_tensor([1, 2, 4])\r\n../aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.\r\ntensor([1, 2, 4])_']","['python\r\nt = torch.tensor(range(5))\r\nmask = np.array([False, True, True, False, True])\r\n\r\nprint(t[mask.astype(np.bool)])\r\nprint(t[mask.astype(np.uint8)])\r\n', 'bash\r\ntensor([0, 1, 1, 0, 1])\r\ntensor([1, 2, 4])\r\n', 'python\r\nt = torch.tensor(range(5))\r\nmask = torch.tensor([False, True, True, False, True])\r\n\r\nprint(t[mask.to(torch.bool)])\r\nprint(t[mask.to(torch.uint8)])\r\n', 'bash\r\ntensor([1, 2, 4])\r\ntensor([1, 2, 4])\r\n']","['np.bool', 'torch.bool', 'FROM nvcr.io/nvidia/pytorch:19.06-py3', ' /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.0']",0,0
625,pytorch,987,closed,Implement nn.Module.__dir__,"If I create a module class and add a layer to it, I can't see this new layer when I try to use IPython's autocomplete. For example, with the the following class:

    class Example(nn.Module):
        def __init__(self):
            super().__init__()
            self.fully_connected = nn.Linear(20, 1)

    ex = Example()

When I type , the only options are 'forward' and 'float'. And when I type , I don't get the completion option for the  attribute.

From my limited understanding, I think that this has to do with the strings that are returned with the  method:

    [attr for attr in ex.__dir__() if attr.startswith('f')]
    => ['forward', 'float']
    
So for the first issue, if I modify the class to have a method

    def __dir__(self):
        return super().__dir__() + list(self._modules)
        
then I get the  as a possible completion.
    
I'm not sure if this is because of an intentional design decision, but it would be nice if all of the relevant possible attributes were available with the auto complete, as I'm still becoming familiar with this library.

",medium priority (this tag is deprecated),"[""I'm not sure why, but 'size' also isn't in the Variable attr list when I try completion for\r\n\r\n    v = Variable(torch.randn(2, 3))\r\n    v.size()"", ""@d10genes it's because we dont implement `__dir__` for this (we have a custom setattr). We will fix this soon."", '@soumith Mind if I pick this up?', '@mrmiywj go on!', 'fixed by #1142 ']",[],"['ex.f<tab>', 'ex.fully_connected.b', 'bias', '__dir__', 'fully_connected']",0,0
626,pytorch,2656,closed,AttributeError: 'module' object has no attribute 'data' ,"import os                                                                                                                                                                                                   
import numpy as np
import cPickle
import torch
import torchvision
import gzip
class YT8M(torch.utils.data.Dataset):
  def __init__(self, data_dir):
    self.data_dir = data_dir
    self.file_list = os.listdir(data_dir)

AttributeError: 'module' object has no attribute 'data' 

Why is there no attribute 'data'?
",,[],[],[],0,0
627,pytorch,5917,closed,type object 'Embedding' has no attribute 'from_pretrained',"- OS: Ubuntu 14.04 LTS
- PyTorch version: 0.4.0
- How you installed PyTorch (conda, pip, source): pip
- Python version: 2.7.6
- CUDA/cuDNN version: 8
- GPU models and configuration: Tesla K20
- GCC version (if compiling from source): 4.8.4

",,"['emb.weight.data[i] = xxx works. Please fix the documentation ...', 'The documentation is right, but you are checking the documentation from master, while your version should be older than that.\r\nAlso check https://github.com/pytorch/pytorch/pull/5350 and https://github.com/pytorch/pytorch/issues/5833']",[],[],0,0
628,pytorch,20697,closed,Indexing by 1-dimension `int64` tensor returns incorrect shape,"## üêõ Bug

Sometimes indexing by a 1-dimension  tensor with a single element returns a tensor with reduced rank.

## To Reproduce

Steps to reproduce the behavior:

<img width=""498"" alt=""Screenshot 2019-05-19 21 41 52"" src=""https://user-images.githubusercontent.com/630490/57996908-ec462a00-7a7e-11e9-8ba2-1a7bf5f71bca.png"">

This bug gets even better:

<img width=""228"" alt=""Screenshot 2019-05-19 21 44 09"" src=""https://user-images.githubusercontent.com/630490/57996986-4515c280-7a7f-11e9-962d-75f60589a4a2.png"">




## Environment

Collecting environment information...
PyTorch version: 1.1.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 18.04.1 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04) 7.4.0
CMake version: version 3.10.2

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration:
GPU 0: Quadro GP100
GPU 1: Quadro GP100

Nvidia driver version: 410.79
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] numpy==1.16.2
[pip] torch==1.1.0
[pip] torchvision==0.2.2
[conda] mkl                       2019.3                      199
[conda] pytorch                   1.1.0           py3.7_cuda10.0.130_cudnn7.5.1_0    pytorch
[conda] pytorch-nightly           1.1.0.dev20190411 py3.7_cuda10.0.130_cudnn7.4.2_0    pytorch
[conda] torchvision               0.2.2                      py_3    pytorch",,"['This looks like the expected behavior? ', 'Also please update the ""steps to reproduce"" so that it\'s actually reproducible. For example, don\'t use images of code and include the full definition of variables (objs, a, etc.). The ""steps to reproduce"" should preferably be a self-contained script that someone else can run. It also helps to point out where the behavior is different than expected.', ""I couldn't reproduce this, i.e.\r\n\r\n```\r\nIn [31]: torch.zeros((54, 1, 64, 64))[torch.tensor([3])].shape\r\nOut[31]: torch.Size([1, 1, 64, 64])\r\n\r\nIn [33]: np.zeros((54, 1, 64, 64))[np.array([3])].shape\r\nOut[33]: (1, 1, 64, 64)\r\n```\r\n\r\nso I'm going to close, please reopen if I've done something incorrect."", 'I couldn‚Äôt reproduce this on a Mac either. This only occurs during run time\non learnfair, with CUDA loaded.\n\nThe problem is persistent and the screenshot shows the problem does exist.\n\nOn Mon, May 20, 2019 at 11:54 AM gchanan <notifications@github.com> wrote:\n\n> Closed #20697 <https://github.com/pytorch/pytorch/issues/20697>.\n>\n> ‚Äî\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/20697?email_source=notifications&email_token=AAEZ5WURQ6YXVV5R6FHYBQDPWLXU3A5CNFSM4HN6NME2YY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGORRHV7RY#event-2354012103>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAEZ5WXWWM72TNANPFBDS4LPWLXU3ANCNFSM4HN6NMEQ>\n> .\n>\n', '@episodeyang what exactly is the problem?', 'Hey Sam, I tried pretty hard to reproduce this, but it seems that it only happens with an actual run. This is why I attached the screenshot to show the problem.\r\n\r\nThe issue is that somehow, if you have a 1-dimensional tensor with only 1 element, and you use it to index another tensor, the returned tensor is short on one dimension. But casting the tensor to list works:\r\n\r\n<img width=""228"" alt=""Screenshot 2019-05-19 21 44 09"" src=""https://user-images.githubusercontent.com/630490/57996986-4515c280-7a7f-11e9-962d-75f60589a4a2.png"">\r\nThe shape of the tensor looks like below:\r\n\r\n<img width=""498"" alt=""Screenshot 2019-05-19 21 41 52"" src=""https://user-images.githubusercontent.com/630490/57996908-ec462a00-7a7e-11e9-8ba2-1a7bf5f71bca.png"">\r\n\r\nSo far I tried to reproduce this issue \r\n\r\n1. on my mac \r\n2. in an iPython console on `learnfair` with `cuda` enabled.\r\n\r\nBut neither demonstrates the reported issue.']","[""python\r\nobs.dtype\r\nOut[28]: dtype('float32')\r\nobs.shape\r\nOut[29]: (54, 1, 64, 64)\r\na.shape\r\nOut[30]: torch.Size([2])\r\nobs.shape\r\nOut[31]: (54, 1, 64, 64)\r\nb = torch.tensor([3])\r\nobs[b].shape\r\nOut[33]: (1, 64, 64)\r\na = torch.tensor([3, 3])\r\nobs[a].shape\r\nOut[35]: (2, 1, 64, 64)\r\n""]",['int64'],0,0
629,pytorch,14563,closed,Address multiple process groups from torch.distributed,"In  you can initialize a global process group using the  function. This function takes a backend argument and will initialize **either** the Gloo, NCCL, or MPI backend. You're at a loss if you want to use both Gloo and NCCL through the  frontend. The only way to do this today is by manually creating ProcessGroup instances and using them directly.

For the case where you're dealing with both CPU and CUDA tensors and want to use Gloo for the CPU ones and NCCL2 for the CUDA ones, we should consider supporting a mode where we don't initialize a single backend, but multiple at the same time. We then use a process group dispatcher class that forwards the calls to the appropriate process group depending on the device type of the tensor arguments. This class should be a subclass of the ProcessGroup base class in C++ such that we can use it both from the Python side as well as from the C++ side.

Have to figure out what is the right way to expose this to the  function.

cc @teng-li ",feature oncall: distributed triaged,"['This is possible as of #18595.', ""@pietern Can you provide a piece of code to demonstrate how to implement this feature which you described?\r\nI meet the some situation as you described, but still don't know how to create two process groups""]",[],"['torch.distributed', 'init_process_group', 'torch.distributed', 'init_process_group']",0,0
630,pytorch,18083,closed,"A common class in Linear, Conv, LSTM, ... ","Hi,

Is there any class that all  inherit from that? 
I want to create a general , which has , so that later I can fill  with any type of layer that I want, e.g.,  Is there such a capability in the current API? This can be done easily in python, though here we need declaration and this hinders the python's easiness. 

Thanks,
Afshin
",module: cpp,['This question is answered in https://discuss.pytorch.org/t/common-class-of-linear-conv-etc/39987.'],[],"['torch::nn::Linear, torch::nn::Conv1d, torch::nn::Conv2d, ... torch::nn::GRU, ....', 'class model', 'std::vector<the common class> layers', 'layers', 'Linear, Conv2d, etc.']",0,0
631,pytorch,2627,closed,Conv3D on CPU segfaults,"Running the code


Results in a segfault. Running the same code on the GPU works flawlessly. 
[Output from gdb](https://gist.github.com/dchansen/6c616b73e65a68c027efa29082e777a1)


The issue was originally raised [here](https://discuss.pytorch.org/t/segmentation-fault-for-sequential-and-conv3d-on-cpu/6353), but I've encountered the same bug independently. ",high priority,"[""i'm looking into this."", ""i fixed this in master. we're cutting a new release this week and the fix will be in the release. thanks a lot for reporting."", 'That was fast. Thank you!']","['import torch\r\nfrom torch import nn\r\nfrom torch.autograd import Variable\r\nv = Variable(torch.randn(1, 16, 128, 128, 128), volatile=True)\r\nmodel = nn.Conv3d(16,16,5)\r\nmodel(v)\r\n']","[""torch.__version__ = '0.2.0_1'""]",0,0
632,pytorch,17108,closed,[Running on windows 10] cuda runtime error (30) : unknown error at ..\aten\src\THC\THCGeneral.cpp:87,"## ‚ùì Questions and Help

### Please note that this issue tracker is not a help form and this issue will be closed.

We have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:

- [Discussion Forum](https://discuss.pytorch.org/)

While trying to run my test.py file on my anaconda prompt I got these messages below:

CUDA‚Ñ¢ is AVAILABLE
Please assign a gpu core (int, <1): 0
THCudaCheck FAIL file=..\aten\src\THC\THCGeneral.cpp line=87 error=30 : unknown error
Traceback (most recent call last):
  File ""VSLcore.py"", line 202, in <module>
    DQNAgent()
  File ""VSLcore.py"", line 87, in DQNAgent
    torch.set_default_tensor_type('torch.cuda.FloatTensor')
  File ""D:\Softwares\Anaconda3\lib\site-packages\torch\__init__.py"", line 158, in set_default_tensor_type
    _C._set_default_tensor_type(t)
  File ""D:\Softwares\Anaconda3\lib\site-packages\torch\cuda\__init__.py"", line 162, in _lazy_init
    torch._C._cuda_init()
RuntimeError: cuda runtime error (30) : unknown error at ..\aten\src\THC\THCGeneral.cpp:87

What should I do?",module: windows needs reproduction triaged,"['And also my CUDA version:\r\n\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2018 NVIDIA Corporation\r\nBuilt on Sat_Aug_25_21:08:04_Central_Daylight_Time_2018\r\nCuda compilation tools, release 10.0, V10.0.130', ""I think that this statement `torch.set_default_tensor_type('torch.cuda.FloatTensor')` should be replaced by `torch.set_default_tensor_type(torch.cuda.FloatTensor)`."", 'I am having the same issue here. My system:\r\n\r\n- Windows 10\r\n- NVIDIA GeForce GTX 1060\r\n- Python 3.7.1 (Anaconda)\r\n- PyTorch 1.0.1\r\n- CUDA 10\r\n\r\nAnd here is a sample code that reproduces the error:\r\n\r\n```\r\n>ipython\r\nPython 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)]\r\nType \'copyright\', \'credits\' or \'license\' for more information\r\nIPython 7.2.0 -- An enhanced Interactive Python. Type \'?\' for help.\r\n\r\nIn [1]: import torch\r\n\r\nIn [2]: torch.cuda.is_available()\r\nOut[2]: True\r\n\r\nIn [3]: torch.cuda.device_count()\r\nOut[3]: 1\r\n\r\nIn [4]: torch.cuda.current_device()\r\nTHCudaCheck FAIL file=..\\aten\\src\\THC\\THCGeneral.cpp line=87 error=30 : unknown error\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-4-3380d2c12118> in <module>\r\n----> 1 torch.cuda.current_device()\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py in current_device()\r\n    339 def current_device():\r\n    340     r""""""Returns the index of a currently selected device.""""""\r\n--> 341     _lazy_init()\r\n    342     return torch._C._cuda_getDevice()\r\n    343\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py in _lazy_init()\r\n    160             ""Cannot re-initialize CUDA in forked subprocess. "" + msg)\r\n    161     _check_driver()\r\n--> 162     torch._C._cuda_init()\r\n    163     _cudart = _load_cudart()\r\n    164     _cudart.cudaGetErrorName.restype = ctypes.c_char_p\r\n\r\nRuntimeError: cuda runtime error (30) : unknown error at ..\\aten\\src\\THC\\THCGeneral.cpp:87\r\n\r\nIn [5]:\r\n```\r\n\r\nCould this be a bug?\r\n', ""I don't think cuda error 30 is an error on our side. Please try these things first.\r\n1. Re-install latest GPU driver\r\n2. Reboot\r\n3. Ensure you have admin access"", 'OK, I did some extra tests, and it seems that it is some weird behavior **only when running on an interactive shell**. Here\'s what I have done (step-by-step)\r\n\r\n1. Prepare a simple file with the example:\r\n```\r\n> type torch_test.ipy\r\nimport torch\r\nprint(""torch.cuda.is_available()   ="", torch.cuda.is_available())\r\nprint(""torch.cuda.device_count()   ="", torch.cuda.device_count())\r\nprint(""torch.cuda.device(\'cuda\')   ="", torch.cuda.device(\'cuda\'))\r\nprint(""torch.cuda.current_device() ="", torch.cuda.current_device())\r\n```\r\n\r\nI can run this file with either `Python `or `iPython`, and it all works fine:\r\n```\r\n> python torch_test.ipy\r\ntorch.cuda.is_available()   = True\r\ntorch.cuda.device_count()   = 1\r\ntorch.cuda.device(\'cuda\')   = <torch.cuda.device object at 0x0000021B331A0160>\r\ntorch.cuda.current_device() = 0\r\n\r\n> ipython torch_test.ipy\r\ntorch.cuda.is_available()   = True\r\ntorch.cuda.device_count()   = 1\r\ntorch.cuda.device(\'cuda\')   = <torch.cuda.device object at 0x000002B39C1FD390>\r\ntorch.cuda.current_device() = 0\r\n```\r\n\r\nNow, if I try to use _**exactly the same**_ commands in an _**interactive**_  shell, I get the error:\r\n\r\nWith python:\r\n```\r\n>python\r\nPython 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import torch\r\n>>> print(""torch.cuda.is_available()   ="", torch.cuda.is_available())\r\ntorch.cuda.is_available()   = True\r\n>>> print(""torch.cuda.device_count()   ="", torch.cuda.device_count())\r\ntorch.cuda.device_count()   = 1\r\n>>> print(""torch.cuda.device(\'cuda\')   ="", torch.cuda.device(\'cuda\'))\r\ntorch.cuda.device(\'cuda\')   = <torch.cuda.device object at 0x0000028CBD034198>\r\n>>> print(""torch.cuda.current_device() ="", torch.cuda.current_device())\r\nTHCudaCheck FAIL file=..\\aten\\src\\THC\\THCGeneral.cpp line=87 error=30 : unknown error\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""C:\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py"", line 341, in current_device\r\n    _lazy_init()\r\n  File ""C:\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py"", line 162, in _lazy_init\r\n    torch._C._cuda_init()\r\nRuntimeError: cuda runtime error (30) : unknown error at ..\\aten\\src\\THC\\THCGeneral.cpp:87\r\n>>> ^Z\r\n```\r\n\r\nor with ipython:\r\n```\r\n>ipython\r\nPython 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)]\r\nType \'copyright\', \'credits\' or \'license\' for more information\r\nIPython 7.2.0 -- An enhanced Interactive Python. Type \'?\' for help.\r\n\r\nIn [1]: import torch\r\n\r\nIn [2]: print(""torch.cuda.is_available()   ="", torch.cuda.is_available())\r\ntorch.cuda.is_available()   = True\r\n\r\nIn [3]: print(""torch.cuda.device_count()   ="", torch.cuda.device_count())\r\ntorch.cuda.device_count()   = 1\r\n\r\nIn [4]: print(""torch.cuda.device(\'cuda\')   ="", torch.cuda.device(\'cuda\'))\r\ntorch.cuda.device(\'cuda\')   = <torch.cuda.device object at 0x0000018A068007F0>\r\n\r\nIn [5]: print(""torch.cuda.current_device() ="", torch.cuda.current_device())\r\nTHCudaCheck FAIL file=..\\aten\\src\\THC\\THCGeneral.cpp line=87 error=30 : unknown error\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-5-f8c552eb6277> in <module>\r\n----> 1 print(""torch.cuda.current_device() ="", torch.cuda.current_device())\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py in current_device()\r\n    339 def current_device():\r\n    340     r""""""Returns the index of a currently selected device.""""""\r\n--> 341     _lazy_init()\r\n    342     return torch._C._cuda_getDevice()\r\n    343\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py in _lazy_init()\r\n    160             ""Cannot re-initialize CUDA in forked subprocess. "" + msg)\r\n    161     _check_driver()\r\n--> 162     torch._C._cuda_init()\r\n    163     _cudart = _load_cudart()\r\n    164     _cudart.cudaGetErrorName.restype = ctypes.c_char_p\r\n\r\nRuntimeError: cuda runtime error (30) : unknown error at ..\\aten\\src\\THC\\THCGeneral.cpp:87\r\n\r\nIn [6]:\r\n```\r\n\r\nAny hints?\r\n', ""@gmseabra\r\nThanks for your post.\r\nI tested it as your described and surprisingly I got the result which was a completely opposite of yours. It turns out it run well on interactive she'll but got bugged on the other"", ""\r\n@ChocolateDave ,\r\n> @gmseabra\r\n> Thanks for your post.\r\n> I tested it as your described and surprisingly I got the result which was a completely opposite of yours. It turns out it run well on interactive she'll but got bugged on the other\r\n\r\nHow does it work in a Jupyter notebook?"", '@gmseabra @ChocolateDave \r\nI have the same problem with you. After a reboot, the problem was gone.  ', '> @gmseabra @ChocolateDave\r\n> I have the same problem with you. After a reboot, the problem was gone.\r\n\r\nCan you tell us what is your configuration? Thanks!', '@ChocolateDave , @kuretru:\r\nWhat are the versions of python, CUDA and PyTorch that you are using?\r\n\r\nI am using:\r\n\r\n- Windows 10 v1809\r\n- Anaconda 3\r\n- Python 3.7.1\r\n- CUDA 10.0 (V10.0.130)\r\n- PyTorch 1.0.1 (py3.7_cuda100_cudnn7_1)\r\n- cudatoolkit 10.0.130\r\n\r\nI have already tried rebooting, removing and reinstalling CUDA, torch, Anaconda, etc., and the error persists. There must be something else going on here...', ""@gmseabra \nThanks for all your advice. Pardon me for replying so late due to my busy trip schedule. And without my laptop, I couldn't test my program on Jupyter.\nIf I remember it correctly, I'm currently using the same system configuration as yours."", "" \r\n@peterjc123 \r\n> I don't think cuda error 30 is an error on our side. Please try these things first.\r\n> \r\n> Re-install latest GPU driver\r\n> Reboot\r\n> Ensure you have admin access\r\n\r\nI have tried all that, and the error is still there. DId you try and reproduce the error?\r\n"", '@gmseabra \r\nAll environments are brand new, I reinstalled the OS on February 14th.  \r\nAnd I am using:  \r\n* Nvidia GTX 860M\r\n* Windows 10 1809 x64\r\n* Python 3.7.2 x64\r\n* CUDA V10.0.130\r\n* PyTorch 1.0.1 (torch-1.0.1-cp37-cp37m-win_amd64.whl)\r\n\r\n```\r\nPython\r\n>>> import torch\r\n>>> torch.cuda.current_device()\r\n>>> RuntimeError: cuda runtime error (30) : unknown error at ..\\aten\\src\\THC\\THCGeneral.cpp:87\r\n```\r\nAfter a reboot\r\n```\r\nPython\r\n>>> import torch\r\n>>> torch.cuda.current_device()\r\n>>> 0\r\n```', ""Thanks. I tried it all - reinstalled the whole Windows, then installed Visual Studio and CUDA Toolkit, installed Miniconda, installed PyTorch in a new environment, and still the same. The commands work from a file, but not interactively.\r\n\r\nNote: I'm using Python 3.7.1. If I update the packages in miniconda, I fall into the error described here: https://github.com/pytorch/pytorch/issues/17233\r\n"", ""I'm sorry but the issues are not reproducible at my side. Could you please try these things to help me locate the problem?\r\n1. Install the GPU driver that ships with the CUDA installation\r\n2. Install the wheels package instead of the conda package\r\n\r\nUsually, the results should stay consistent regardless of the interactive mode is on or not So it's actually very weird. Maybe you should check whether they are using the exact same DLLs by using sth. like Process Explorer."", ""> Install the GPU driver that ships with the CUDA installation\r\n\r\nI'll try that\r\n \r\n> Usually, the results should stay consistent regardless of the interactive mode is on or not So it's actually very weird. Maybe you should check whether they are using the exact same DLLs by using sth. like Process Explorer.\r\n\r\nOK, what should I look for here?\r\n\r\nThanks for looking into the issue!\r\n"", 'Hi, I tried reverting to the CUDA drivers that come with the CUDA Development Kit, but I can\'t install them because I keep getting an error: ""Windows cannot verify the driver signature... (Code 52)"", so I have to stick with the most recent driver.\r\n\r\nMy system is an Acer laptop with:\r\n\r\n- Windows 10 Home Single Language v 1809\r\n- GeForce GTX 1060, Driver version 25.21.14.1891 (In the GeForce Experience it shows as 418.91)\r\n- Miniconda with Python 3.7.1\r\n\r\nMy exact procedure was:\r\n\r\n1. Install Miniconda. Do **not** update anything.\r\n1. Clone base into a new env: `(base) > conda create --name torch --clone base`\r\n1. Activate the new env: `(base) > conda activate torch`\r\n1. Install pytorch: `(torch) > conda install pytorch torchvision cudatoolkit=10.0 -c pytorch`\r\n1. Deactivte / reactivate the env, just to be sure\r\n1. Try to run the simple example torch_test.py by:` (torch) > python torch_test.py`\r\n1. Try to run the same sequence of commands using the python interactive interpreter, see results below.\r\n\r\nHere are the results I get. In the end I also add details about my environment and the output of the deviceQuery app from the CUDA tests:\r\n\r\nOutput of running the small program:\r\n```\r\n(torch) >python torch_test.py\r\ntorch.cuda.is_available()   = True\r\ntorch.cuda.device_count()   = 1\r\ntorch.cuda.device(\'cuda\')   = <torch.cuda.device object at 0x000001FCD3A61F28>\r\ntorch.cuda.current_device() = 0\r\n```\r\n\r\nOutput of interactive python interpreter:\r\n```\r\n(torch) > python\r\nPython 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import torch\r\n>>> torch.cuda.is_available()\r\nTrue\r\n>>> torch.cuda.device_count()\r\n1\r\n>>> torch.cuda.device(\'cuda\')\r\n<torch.cuda.device object at 0x000001E18C72D208>\r\n>>> torch.cuda.current_device()\r\nTHCudaCheck FAIL file=..\\aten\\src\\THC\\THCGeneral.cpp line=87 error=30 : unknown error\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""C:\\Miniconda3\\envs\\torch\\lib\\site-packages\\torch\\cuda\\__init__.py"", line 341, in current_device\r\n    _lazy_init()\r\n  File ""C:\\Miniconda3\\envs\\torch\\lib\\site-packages\\torch\\cuda\\__init__.py"", line 162, in _lazy_init\r\n    torch._C._cuda_init()\r\nRuntimeError: cuda runtime error (30) : unknown error at ..\\aten\\src\\THC\\THCGeneral.cpp:87\r\n>>>\r\n```\r\n\r\nFinally, here are the information about my conda environment:\r\n```\r\n(torch) >type torch_env.txt\r\n# packages in environment at C:\\Miniconda3\\envs\\torch:\r\n#\r\n# Name                    Version                   Build  Channel\r\nasn1crypto                0.24.0                   py37_0\r\nblas                      1.0                         mkl\r\nca-certificates           2018.03.07                    0\r\ncertifi                   2018.11.29               py37_0\r\ncffi                      1.11.5           py37h74b6da3_1\r\nchardet                   3.0.4                    py37_1\r\nconsole_shortcut          0.1.1                         3\r\ncryptography              2.4.2            py37h7a1dbc1_0\r\ncudatoolkit               10.0.130                      0\r\nfreetype                  2.9.1                ha9979f8_1\r\nicc_rt                    2019.0.0             h0cc432a_1\r\nidna                      2.8                      py37_0\r\nintel-openmp              2019.1                      144\r\njpeg                      9b                   hb83a4c4_2\r\nlibpng                    1.6.36               h2a8f88b_0\r\nlibtiff                   4.0.10               hb898794_2\r\nmenuinst                  1.4.14           py37hfa6e2cd_0\r\nmkl                       2019.1                      144\r\nmkl_fft                   1.0.10           py37h14836fe_0\r\nmkl_random                1.0.2            py37h343c172_0\r\nninja                     1.8.2            py37he980bc4_1\r\nnumpy                     1.15.4           py37h19fb1c0_0\r\nnumpy-base                1.15.4           py37hc3f5095_0\r\nolefile                   0.46                     py37_0\r\nopenssl                   1.1.1a               he774522_0\r\npillow                    5.4.1            py37hdc69c19_0\r\npip                       18.1                     py37_0\r\npycosat                   0.6.3            py37hfa6e2cd_0\r\npycparser                 2.19                     py37_0\r\npyopenssl                 18.0.0                   py37_0\r\npysocks                   1.6.8                    py37_0\r\npython                    3.7.1                h8c8aaf0_6\r\npytorch                   1.0.1           py3.7_cuda100_cudnn7_1    pytorch\r\npywin32                   223              py37hfa6e2cd_1\r\nrequests                  2.21.0                   py37_0\r\nruamel_yaml               0.15.46          py37hfa6e2cd_0\r\nsetuptools                40.6.3                   py37_0\r\nsix                       1.12.0                   py37_0\r\nsqlite                    3.26.0               he774522_0\r\ntk                        8.6.8                hfa6e2cd_0\r\ntorchvision               0.2.1                      py_2    pytorch\r\nurllib3                   1.24.1                   py37_0\r\nvc                        14.1                 h0510ff6_4\r\nvs2015_runtime            14.15.26706          h3a45250_0\r\nwheel                     0.32.3                   py37_0\r\nwin_inet_pton             1.0.1                    py37_1\r\nwincertstore              0.2                      py37_0\r\nxz                        5.2.4                h2fa13f4_4\r\nyaml                      0.1.7                hc54c509_2\r\nzlib                      1.2.11               h62dcd97_3\r\nzstd                      1.3.7                h508b16e_0\r\n```\r\n\r\nAnd the output of the deviceQuery, from CUDA tests suite:\r\n```\r\n(torch) >type deviceQuery.out\r\ndeviceQuery.exe Starting...\r\n\r\n CUDA Device Query (Runtime API) version (CUDART static linking)\r\n\r\nDetected 1 CUDA Capable device(s)\r\n\r\nDevice 0: ""GeForce GTX 1060""\r\n  CUDA Driver Version / Runtime Version          10.1 / 10.0\r\n  CUDA Capability Major/Minor version number:    6.1\r\n  Total amount of global memory:                 6144 MBytes (6442450944 bytes)\r\n  (10) Multiprocessors, (128) CUDA Cores/MP:     1280 CUDA Cores\r\n  GPU Max Clock rate:                            1733 MHz (1.73 GHz)\r\n  Memory Clock rate:                             4004 Mhz\r\n  Memory Bus Width:                              192-bit\r\n  L2 Cache Size:                                 1572864 bytes\r\n  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\r\n  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\r\n  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\r\n  Total amount of constant memory:               65536 bytes\r\n  Total amount of shared memory per block:       49152 bytes\r\n  Total number of registers available per block: 65536\r\n  Warp size:                                     32\r\n  Maximum number of threads per multiprocessor:  2048\r\n  Maximum number of threads per block:           1024\r\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\r\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\r\n  Maximum memory pitch:                          2147483647 bytes\r\n  Texture alignment:                             512 bytes\r\n  Concurrent copy and kernel execution:          Yes with 5 copy engine(s)\r\n  Run time limit on kernels:                     Yes\r\n  Integrated GPU sharing Host Memory:            No\r\n  Support host page-locked memory mapping:       Yes\r\n  Alignment requirement for Surfaces:            Yes\r\n  Device has ECC support:                        Disabled\r\n  CUDA Device Driver Mode (TCC or WDDM):         WDDM (Windows Display Driver Model)\r\n  Device supports Unified Addressing (UVA):      Yes\r\n  Device supports Compute Preemption:            Yes\r\n  Supports Cooperative Kernel Launch:            No\r\n  Supports MultiDevice Co-op Kernel Launch:      No\r\n  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\r\n  Compute Mode:\r\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\r\n\r\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.1, CUDA Runtime Version = 10.0, NumDevs = 1\r\nResult = PASS\r\n```\r\n\r\nI\'ve already tried reinstalling the system, uninstalling and reinstalling Anaconda and Miniconda, and nothing changes. \r\n\r\nShould I open a bug report?\r\n\r\nThanks!', 'Hi all, \r\n\r\nI just wanted to mention that I have just tried with the nightly build of pytorch, and the problem disappears. Using the nightly build available today (02/20/2019), I get the following:\r\n\r\n```\r\n(torch_nightly) >python\r\nPython 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import torch\r\n>>> torch.cuda.is_available()\r\nTrue\r\n>>> torch.cuda.current_device()\r\n0\r\n>>> quit()\r\n```\r\n\r\nSo it seems that, at some point between stable and today\'s build, the issue has been resolved.', ""@gmseabra I'm glad that it's solved. But I'm not sure which one is related to this."", ""Thank you guys all for all your support.üòÑ\nespecially @gmseabra\n\nI couldn't fix the problem so I decided to downgrade my python version to 3.6.8 and it somehow worked.\nThe bugs may still exist on a newer version of of python but to those who are currently stuck at this problem, downgrading your python version might be a good solution."", ""> Thank you guys all for all your support.üòÑ\r\n> especially @gmseabra\r\n> I couldn't fix the problem so I decided to downgrade my python version to 3.6.8 and it somehow worked.\r\n> The bugs may still exist on a newer version of of python but to those who are currently stuck at this problem, downgrading your python version might be a good solution.\r\n\r\nHave you tried using the nightly-build? That did work fine for me (as of 02/20/2019)."", '> @gmseabra I\'m glad that it\'s solved. But I\'m not sure which one is related to this.\r\n\r\n@peterjc123 Thanks. Is there any idea about when the ""nightly build"" becomes part of the ""stable"" distribution?', ""@gmseabra It won't be too soon. Our release cycle is ~90 days. BTW, would you please try if removing `nvcuda.dll` and `nvfatbinaryloader.dll` from `[Anaconda Root]\\Lib\\site-packages\\torch\\lib` helps?"", ""> @gmseabra It won't be too soon. Our release cycle is ~90 days.\r\n\r\nThanks.\r\n\r\n> BTW, would you please try if removing nvcuda.dll and nvfatbinaryloader.dll from [Anaconda Root]\\Lib\\site-packages\\torch\\lib helps?\r\n\r\nTried removing from \r\n```[Miniconda3]\\envs\\torch\\Lib\\site-packages\\torch\\lib```\r\n\r\nI also tried copying those DLLs from my torch-nightly env to the torch env, but there was no difference either way. "", 'I am getting the same error as this with PyTorch 1.0.1 and CUDA 10. Indeed, updating to one of the nightly builds solved the issue, yet I stumbled upon a ""classical nightly issue"": some random Assertion Failure which prompted me to message PyTorch developers about it. This is getting really frustrating since I\'ve been losing considerable time in reconfiguring my environment. I think I will have to downgrade some components now...\r\n\r\nEDIT: Downgrading to PyTorch 1.0.0 solved the issue for me as well. Clearly, there\'s a problem with 1.0.1.', 'I am getting the same error.\r\n\r\n**My setup:**\r\n* Nvidia GTX 1050Ti\r\n* Windows 10 Pro\r\n* Conda 4.6.7\r\n* Python 3.7.1 \r\n* CUDA V10.0.130\r\n* PyTorch 1.0.1 \r\n\r\n**My Jupyter Notebook Test:**\r\n\r\n**torch.cuda.is_available()** \r\nTrue\r\n\r\n**torch.backends.cudnn.enabled** \r\nTrue\r\n\r\n**torch.cuda.current_device()** \r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-4-3380d2c12118> in <module>\r\n----> 1 torch.cuda.current_device()\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py in current_device()\r\n    339 def current_device():\r\n    340     r""""""Returns the index of a currently selected device.""""""\r\n--> 341     _lazy_init()\r\n    342     return torch._C._cuda_getDevice()\r\n    343 \r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py in _lazy_init()\r\n    160             ""Cannot re-initialize CUDA in forked subprocess. "" + msg)\r\n    161     _check_driver()\r\n--> 162     torch._C._cuda_init()\r\n    163     _cudart = _load_cudart()\r\n    164     _cudart.cudaGetErrorName.restype = ctypes.c_char_p\r\n\r\nRuntimeError: cuda runtime error (30) : unknown error at ..\\aten\\src\\THC\\THCGeneral.cpp:87\r\n\r\n**torch.cuda.device(0)**\r\n<torch.cuda.device at 0x21f81413fd0>\r\n\r\n**torch.cuda.device_count()**\r\n1\r\n\r\n**torch.cuda.get_device_name(0)**\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-7-f9c260299c38> in <module>\r\n----> 1 torch.cuda.get_device_name(0)\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py in get_device_name(device)\r\n    274             if :attr:`device` is ``None`` (default).\r\n    275     """"""\r\n--> 276     return get_device_properties(device).name\r\n    277 \r\n    278 \r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py in get_device_properties(device)\r\n    296 def get_device_properties(device):\r\n    297     if not _initialized:\r\n--> 298         init()  # will define _get_device_properties and _CudaDeviceProperties\r\n    299     device = _get_device_index(device, optional=True)\r\n    300     if device < 0 or device >= device_count():\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py in init()\r\n    142     Does nothing if the CUDA state is already initialized.\r\n    143     """"""\r\n--> 144     _lazy_init()\r\n    145 \r\n    146 \r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py in _lazy_init()\r\n    160             ""Cannot re-initialize CUDA in forked subprocess. "" + msg)\r\n    161     _check_driver()\r\n--> 162     torch._C._cuda_init()\r\n    163     _cudart = _load_cudart()\r\n    164     _cudart.cudaGetErrorName.restype = ctypes.c_char_p\r\n\r\nRuntimeError: cuda runtime error (30) : unknown error at ..\\aten\\src\\THC\\THCGeneral.cpp:87\r\n\r\nThe THCGeneral.cpp code can be found at:\r\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/THC/THCGeneral.cpp\r\n\r\nThe code block in THCGeneral where the error is thrown is:\r\n\r\n  for (int i = 0; i < numDevices; ++i) {\r\n    THCCudaResourcesPerDevice* res = THCState_getDeviceResourcePtr(state, i);\r\n    THCudaCheck(cudaSetDevice(i));\r\n\r\n    /* The scratch space that we want to have available per each device is\r\n       based on the number of SMs available per device. We guarantee a\r\n       minimum of 128kb of space per device, but to future-proof against\r\n       future architectures that may have huge #s of SMs, we guarantee that\r\n       we have at least 16 bytes for each SM. */\r\n    int numSM = at::cuda::getDeviceProperties(i)->multiProcessorCount;\r\n    size_t sizePerStream =\r\n      MIN_GLOBAL_SCRATCH_SPACE_PER_DEVICE >= numSM * MIN_GLOBAL_SCRATCH_SPACE_PER_SM_STREAM ?\r\n      MIN_GLOBAL_SCRATCH_SPACE_PER_DEVICE :\r\n      numSM * MIN_GLOBAL_SCRATCH_SPACE_PER_SM_STREAM;\r\n    res->scratchSpacePerStream = sizePerStream;\r\n  }\r\n\r\nLine 87 of this code is:\r\nint numSM = at::cuda::getDeviceProperties(i)->multiProcessorCount;\r\n\r\nAny ideas why I and so many others are experiencing this exact same error?', ""Looks like the callback was accidentally triggered here. https://github.com/pytorch/pytorch/blame/master/torch/cuda/__init__.py#L188. Usually it won't happen. Anyway, I'll try to add a protection clause here for Windows."", '@peterjc123 any idea on how to fix this? The error came after I updated by GeForce driver using the NVIDIA GeForce Experience panel. \r\n\r\nMy models were running till now, and now they are not. \r\n\r\nHere is the full error message:\r\n\r\n```\r\nTHCudaCheck FAIL file=..\\aten\\src\\THC\\THCGeneral.cpp line=87 error=30 : unknown error\r\nTraceback (most recent call last):\r\n  File ""experiment.py"", line 249, in <module>\r\n    trainer = Trainer(model=model.cuda(device=device),\r\n  File ""F:\\ProgramData\\Anaconda3\\envs\\AllenNLP\\lib\\site-packages\\torch\\nn\\modules\\module.py"", line 260, in cuda\r\n    return self._apply(lambda t: t.cuda(device))\r\n  File ""F:\\ProgramData\\Anaconda3\\envs\\AllenNLP\\lib\\site-packages\\torch\\nn\\modules\\module.py"", line 187, in _apply\r\n    module._apply(fn)\r\n  File ""F:\\ProgramData\\Anaconda3\\envs\\AllenNLP\\lib\\site-packages\\torch\\nn\\modules\\module.py"", line 187, in _apply\r\n    module._apply(fn)\r\n  File ""F:\\ProgramData\\Anaconda3\\envs\\AllenNLP\\lib\\site-packages\\torch\\nn\\modules\\module.py"", line 187, in _apply\r\n    module._apply(fn)\r\n  [Previous line repeated 3 more times]\r\n  File ""F:\\ProgramData\\Anaconda3\\envs\\AllenNLP\\lib\\site-packages\\torch\\nn\\modules\\module.py"", line 193, in _apply\r\n    param.data = fn(param.data)\r\n  File ""F:\\ProgramData\\Anaconda3\\envs\\AllenNLP\\lib\\site-packages\\torch\\nn\\modules\\module.py"", line 260, in <lambda>\r\n    return self._apply(lambda t: t.cuda(device))\r\n  File ""F:\\ProgramData\\Anaconda3\\envs\\AllenNLP\\lib\\site-packages\\torch\\cuda\\__init__.py"", line 162, in _lazy_init\r\n    torch._C._cuda_init()\r\nRuntimeError: cuda runtime error (30) : unknown error at ..\\aten\\src\\THC\\THCGeneral.cpp:87\r\n```', 'Have you tried changing PyTorch to the nightly build? It worked for me:\r\n\r\nhttps://github.com/pytorch/pytorch/issues/17108#issuecomment-465793918\r\n', 'I tried the nighly build but was getting the same error. One update I would like to give, is that when I run the code in iPython then I am not getting this error. Only when I run the file as `python experiment.py`\r\n\r\nUPDATE: After I closed my iPython session, even the ipython thing is not working anymore. \r\nUPDATE2: Sometimes, the code works with iPython but not using python command line. In iPython I run the code as `%run -i experiment.py`', 'Updating to pytorch 1.0.0 fixed the issue. ', 'Does commenting out the clause https://github.com/pytorch/pytorch/blame/master/torch/cuda/__init__.py#L188 help? Does the error occur when you do cuda init for the first time?', 'Previously , I also had cuda runtime error, not 30 -mine is unknown error but after I lunch the anaconda as administrator, it solved the error.', 'I was able to get the below torch methods to return without any errors a few times when I launched a jupyter notebook from a administrative command prompt within the ""c:/users/system32"" folder:\r\n\r\nimport torch\r\ntorch.cuda.is_available()\r\ntorch.backends.cudnn.enabled\r\ntorch.cuda.current_device()\r\ntorch.cuda.device(0)\r\ntorch.cuda.device_count()\r\ntorch.cuda.get_device_name(0)\r\n\r\nHowever I haven\'t been able to get this to happen recently and it wasn\'t consistent even when it did work. What is consistent is that it always fails when ""torch.cuda.current_device()"" is run and also when ""torch.cuda.get_device_name(0)"" is run. In both cases it fails in the ""_lazy_init()"" method when ""torch._C._cuda_init()"" line is executed. The current error reported is ""RuntimeError: CUDA error: unknown error"". I updated pytorch yesterday to the nightly build.\r\n\r\nMy setup, as I mentioned in a previous comment, is:\r\n\r\nNvidia GTX 1050Ti\r\nWindows 10 Pro\r\nConda 4.6.7\r\nPython 3.7.1\r\nCUDA V10.0.130\r\nPytorch 1.0.0.dev20190310', 'Does anyone have any ideas on how I should move forward with troubleshooting why ""torch._C._cuda_init()"" errors out with ""RuntimeError: CUDA error: unknown error""?\r\n\r\nMy setup is:\r\n\r\nNvidia GTX 1050Ti\r\nWindows 10 Pro\r\nConda 4.6.7\r\nPython 3.7.1\r\nCUDA V10.0.130\r\nPytorch 1.0.0.dev20190310', ""@jsmith8888 Try updating your gpu driver. And you can try my suggestion too if it doesn't work. https://github.com/pytorch/pytorch/issues/17108#issuecomment-469494690."", '**Resolved the issue:**\r\n\r\nI was able to resolve the problem - although I don\'t know why the change I made avoided the error. \r\n\r\nAs I mentioned previously to test that the GPU was working I would run the below test one command at a time:\r\n\r\nimport torch\r\ntorch.cuda.is_available()\r\ntorch.backends.cudnn.enabled\r\ntorch.cuda.current_device()\r\ntorch.cuda.device(0)\r\ntorch.cuda.device_count()\r\ntorch.cuda.get_device_name(0)\r\n\r\nThis test would always produce an error on the steps ""torch.cuda.current_device()"" and ""torch.cuda.get_device_name(0)"".\r\n\r\nThe change that allowed me to use the GPU without the error was to not run the command ""torch.cuda.is_available()"". As long as I do not explicitly run this command all the other commands run successfully without an error. \r\n\r\nI then tested this with an actual CNN that I have been using on the CIFAR10 dataset and the GPU was successfully used. Previously I would get the same error that occurred during the test shown above. The CNN also had a step at the very beginning where the ""torch.cuda.is_available()"" command was issued and returned ""True"". When I commented out this command and explicitly set the variable tied to this command to ""True"" the entire CNN ran all the way through without error and fully utilized the GPU.\r\n\r\nThe question is why does the command ""torch.cuda.is_available()"" create the errors that prevented the  use of the GPU.\r\n\r\nAs I mentioned previously my set up is:\r\n\r\nNvidia GTX 1050Ti\r\nWindows 10 Pro\r\nConda 4.6.7\r\nPython 3.7.1\r\nCUDA V10.0.130\r\nPytorch 1.0.0.dev20190310\r\n', 'cc @ezyang', '@jsmith8888 What about my suggestion? Does it help?', 'I already had the latest Nvidia GPU driver so that was not a factor in the solution. I have been trying to resolve this issue for a few weeks now and as part of the troubleshooting process I always make sure that I have the latest GPU driver.', 'I found something similar with what  @jsmith8888 mentioned.\r\nI tried to run `torch.backends.cudnn.enabled`, `torch.cuda.current_device()`, `torch.cuda.device(0)`, \r\n`torch.cuda.device_count()`, `torch.cuda.get_device_name(0)` before `torch.cuda.is_available()` one by one. It turns out that if either `torch.cuda.current_device()` or `torch.cuda.get_device_name(0)` was executed before `torch.cuda.is_available()`, no error would occur. Hopefully this will be helpful.\r\n\r\nmy set up is:\r\nNvidia GTX 950M with the latest driver\r\nWindows 10 Pro\r\nConda   4.6.8\r\nPython  3.6.7\r\nCUDA    V9.2\r\nPytorch 1.0.1', 'what about the code modificationÔºü', ""Well, since the code ran well now, I didn't try to modify the source code."", 'I tried the code modification you mentioned previously, namely:\r\n\r\n_Does commenting out the clause https://github.com/pytorch/pytorch/blame/master/torch/cuda/__init__.py#L188 help? Does the error occur when you do cuda init for the first time?_\r\n\r\nI commented out #L188 but that did not correct the error. Also the error did occur when I would call ""torch._C._cuda_init()"" by itself. \r\n\r\nWith it working successfully now, as long as the command ""torch.cuda.is_available()"" is not used, I am using the standard code base with no changes made by me.\r\n', '@jsmith8888 ok, got that. Thanks for your report.', '> I think that this statement `torch.set_default_tensor_type(\'torch.FloatTensor\')` should be replaced by `torch.set_default_tensor_type(torch.cuda.FloatTensor)`.\r\n\r\nMy issue was resolved by doing the suggested above, which you can also force explicitly eg:\r\ntarget = target.type(\'torch.cuda.FloatTensor\').\r\nWhere before I was naively using:\r\ntarget = target.float()\r\n\r\nAlso don\'t forget to send data,target and model to cuda:\r\ndevice = torch.device(""cuda:0"" if use_cuda else ""cpu"")\r\nmodel = model.to(device)\r\ndata = data.to(device)\r\ntarget = target.to(device)', ""FYI - as per https://github.com/pytorch/pytorch/issues/17233#issuecomment-464962469 downgrading from python 3.7.3 to 3.7.1 (`conda install python=3.7.1`) fixed it for me. This might not necessarily be related to python version, per se - it might be magic conda does when updating python. I didn't try the nightly build."", ""Hey @peterjc123 , I didn't see your at from a while back.\r\n\r\nI changed the behavior of `available()` in #18445 to be a bit more robust to driver failures. Can people try the nightly and see if that makes things better?"", 'For me the solution was to uninstall the latest pytorch and install the 1.0.0 version.. Hope this helps!', ""I also have the same problem, the configuration is similar to yours, but a very old Nvidia graphics.\r\nWhen I run python test files with pycharm and notepad++, I get the same problem, and rebooting is not ok.\r\nBut when I run the test code in the command line, it works fine. When I return to pycharm or notepad++ again, I can run it.\r\nDon't turn off CMD when you run with pycharm or notepad++, otherwise there will still be problems.\r\nIt seems that this bug is kinda confusing.\r\n\r\nRun code in CMDÔºåIt worked.\r\n>>> import torch\r\n>>> a = torch.randn(5, 6)\r\n>>> b = torch.randn(6, 5)\r\n>>> device = torch.device('cuda')\r\n>>> a.to(device)        # bug before\r\n>>> b.to(device)\r\n>>> c = torch.matmul(a, b)\r\n>>> c.shape\r\ntorch.Size([5, 5])"", '@peterjc123  Regarding situation from https://forums.fast.ai/t/cuda-runtime-error-30-resnet-not-loading/38556/2, I think there is some inner errors between jupyter and pytorch 1.0.1, as a result of downgrading pytorch 1.0.0 could solve the problem.\r\n\r\nI noticed several issues raised about the same problem and this might be the best answer till now.', 'My system:\r\nWindows 10\r\nCuda 10.1\r\nPython 3.7.2\r\nPyTorch 1.0.1\r\nNVIDIA GeForce GTX 1050 Ti\r\n\r\nThe following always works:\r\n```\r\nimport torch\r\ntorch.cuda.current_device()\r\n```\r\n\r\nThe following always fails for me:\r\n```\r\nimport torch\r\ntorch.cuda.is_available()\r\ntorch.cuda.current_device()  # fails here\r\n```\r\n\r\nMy solution was to add to my scripts the call to `torch.cuda.current_device()` before any other cuda calls.\r\nHope this gives a hint as to where to look for the issue :)', ""I ran into the same problem (GTX 1050, anaconda environment, Win10, latest pytorch installed with anaconda (both pip and conda).\r\nI uninstalled, reinstalled pytorch in different environments several times without success until now. \r\n\r\nBefore that issue came up, pytorch worked as usual. I didn't change anything in the settings nor did I install packages, it just came up."", 'Quote from @andrei-rusu \r\n> I am getting the same error as this with PyTorch 1.0.1 and CUDA 10. Indeed, updating to one of the nightly builds solved the issue, yet I stumbled upon a ""classical nightly issue"": some random Assertion Failure which prompted me to message PyTorch developers about it. This is getting really frustrating since I\'ve been losing considerable time in reconfiguring my environment. I think I will have to downgrade some components now...\r\n> \r\n> EDIT: Downgrading to PyTorch 1.0.0 solved the issue for me as well. Clearly, there\'s a problem with 1.0.1.\r\n\r\nDowngrading PyTorch to 1.0.0 solved mine, also I need to ensure script ran in an administrative command. Thanks!', 'Downgrading to version 1.0.0 also worked for me. Also, I changed some nvidia graphics card settings (maximum performance=yes) which may also have contributed to get it working.', 'Same issue here:\r\n    Windows 10\r\n    NVIDIA GeForce GTX 940mx\r\n    Python 3.6.8\r\n    PyTorch 1.0.1\r\n    CUDA 10.1\r\n    cudnn 7.5\r\n\r\nDowngrading to pytorch 1.0.0 solved the issue\r\n', ""Well, would you guys please check whether this error persists in the nightlies? Downgrading is a workaround here, but it does little help to locate the actual cause of this issue. Let me conclude all the known possible reasons that may cause this issue:\r\n1. From 1.0.0 to 1.0.1, we switched to use the cuda libraries provided by the conda-forge channel in the conda package. Previously, we copied these libraries in the build machine into the binaries. We can ignore this factor if we use the pip package.\r\n2. The dll loading process of python in conda has changed. It started to use `AllDllDirectory`, which does not ensure the loading sequence. We can ignore this factor if we downgrade python to 3.6.7 or 3.7.1.\r\n3. The fix/issue mentioned by @ezyang https://github.com/pytorch/pytorch/pull/18445. We can ignore this factor if we use the nightlies or build from source.\r\n\r\nI'd be grateful if you could help me locate the issue. It's currently hard to fix because I cannot reproduce it on my side."", 'Nightly builds for windows are available here: https://download.pytorch.org/whl/nightly/cu100/torch_nightly.html but only for version 1.0.0', '@Jonas1312 You mean CUDA 10? If you are talking about the version of PyTorch, it will always build for the latest source every day.', 'https://download.pytorch.org/whl/nightly/cu100/torch_nightly.html shows the following packages: https://pastebin.com/yYxdEqU5\r\n\r\nI tried with the last windows build:\r\n\r\n```\r\nc:\\Users\\Jonas\\Desktop>python36 -m pip install torch_nightly-1.0.0.dev20190421-cp36-cp36m-win_amd64.whl\r\nProcessing c:\\users\\jonas\\desktop\\torch_nightly-1.0.0.dev20190421-cp36-cp36m-win_amd64.whl\r\nInstalling collected packages: torch-nightly\r\nSuccessfully installed torch-nightly-1.0.0.dev20190421\r\n\r\nc:\\Users\\Jonas\\Desktop>python36\r\nPython 3.6.8 (tags/v3.6.8:3c6b436a57, Dec 24 2018, 00:16:47) [MSC v.1916 64 bit (AMD64)] on win32\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import torch\r\n>>> print(""torch.cuda.is_available()   ="", torch.cuda.is_available())\r\ntorch.cuda.is_available()   = True\r\n>>> print(""torch.cuda.device_count()   ="", torch.cuda.device_count())\r\ntorch.cuda.device_count()   = 1\r\n>>> print(""torch.cuda.device(\'cuda\')   ="", torch.cuda.device(\'cuda\'))\r\ntorch.cuda.device(\'cuda\')   = <torch.cuda.device object at 0x00000251657A7518>\r\n>>> print(""torch.cuda.current_device() ="", torch.cuda.current_device())\r\ntorch.cuda.current_device() = 0\r\n>>> torch.__version__\r\n\'1.0.0.dev20190421\'\r\n>>>\r\n```\r\n\r\nIt\'s working but I don\'t understand why is it showing version 1.0.0 even if it\'s built with the latest source?', '@JohnRambo Oh, I see. I will update the build scripts.', '@JohnRambo Should be fixed now. Looks like I forgot to sync with upstream after I sent these changes about the version change.', '@peterjc123 I\'ve just installed _torch_nightly-1.1.0.dev20190424-cp36-cp36m-win_amd64.whl_ and it seems that it fixed the issue:\r\n\r\n```\r\nC:\\Users\\Jonas>nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2019 NVIDIA Corporation\r\nBuilt on Fri_Feb__8_19:08:26_Pacific_Standard_Time_2019\r\nCuda compilation tools, release 10.1, V10.1.105\r\n\r\nC:\\Users\\Jonas>python36\r\nPython 3.6.8 (tags/v3.6.8:3c6b436a57, Dec 24 2018, 00:16:47) [MSC v.1916 64 bit (AMD64)] on win32\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import torch\r\n>>> print(""torch.cuda.is_available()   ="", torch.cuda.is_available())\r\ntorch.cuda.is_available()   = True\r\n>>> print(""torch.cuda.device_count()   ="", torch.cuda.device_count())\r\ntorch.cuda.device_count()   = 1\r\n>>> print(""torch.cuda.device(\'cuda\')   ="", torch.cuda.device(\'cuda\'))\r\ntorch.cuda.device(\'cuda\')   = <torch.cuda.device object at 0x00000262DB837EB8>\r\n>>> print(""torch.cuda.current_device() ="", torch.cuda.current_device())\r\ntorch.cuda.current_device() = 0\r\n>>> torch.cuda.get_device_name(0)\r\n\'GeForce 940MX\'\r\n>>> torch.__version__\r\n\'1.1.0.dev20190424\'\r\n>>> a = torch.ones((1,1,1)).cuda()\r\n>>> a\r\ntensor([[[1.]]], device=\'cuda:0\')\r\n>>>\r\n```\r\n\r\nWorks with cuda 10.0 also!', 'I just got this error for the first time today, after running PyTorch 1.0.1 (CUDA 10.0) on Windows 10 for months and months with no problems.\r\n\r\nIn my case, the error only started happening when I updated my Nvidia graphics driver to 430.53 from 417.35. Luckily, simply reverting to driver version 417.35 caused the error to go away and everything works fine again. I did not need to touch my CUDA or Python environment to fix it, just roll back the graphics driver. Very odd, looks like Nvidia changed something in the driver code which is causing this.\r\n\r\nMy setup:\r\n\r\nWindows 10 1607 64-bit\r\nPython 3.6.8\r\nPyTorch 1.0.1\r\nCUDA 10.0\r\n\r\nPyTorch installed via pip', 'I got the similar issue with an error as ""RuntimeError: cuda runtime error (30) : unknown error at ..\\aten\\src\\THC\\THCGeneral.cpp:51"" on my new machine (Windows 10, Nvidia RTX2070). (Also referred to https://discuss.pytorch.org/t/a-error-when-using-gpu/32761).\r\nI tried a lot of methods suggested, such as cuda downgrading, Anaconda downgrading and upgrading, etc. However not successful.\r\nFor my case, the error suddenly gone seems only after I installed the latest Nvidia gaming driver.\r\nHope it is helpful.', ""We are building this time with the NVIDIA driver 418.96. But according to your test results, I don't know whether I should downgrade or upgrade it. However, if the problem is caused by the driver, we can actually do some tests on this. Also, if you have time, you can try whether building from source solves it."", ""Actually, from the CUDA document, I can only find that there is a lower limit of the driver version for each CUDA version: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#major-components. But it didn't mention what will happen if we compile binaries using newer versions of GPU drivers, or the driver version mismatches with the one on the user's PC."", '> I got the similar issue with an error as ""RuntimeError: cuda runtime error (30) : unknown error at ..\\aten\\src\\THC\\THCGeneral.cpp:51"" on my new machine (Windows 10, Nvidia RTX2070). (Also referred to https://discuss.pytorch.org/t/a-error-when-using-gpu/32761).\r\n> I tried a lot of methods suggested, such as cuda downgrading, Anaconda downgrading and upgrading, etc. However not successful.\r\n> For my case, the error suddenly gone seems only after I installed the latest Nvidia gaming driver.\r\n> Hope it is helpful.\r\n\r\nUpdating Nivida driver (to 430.39) also worked for me. ', '> \r\n> \r\n> My system:\r\n> Windows 10\r\n> Cuda 10.1\r\n> Python 3.7.2\r\n> PyTorch 1.0.1\r\n> NVIDIA GeForce GTX 1050 Ti\r\n> \r\n> The following always works:\r\n> \r\n> ```\r\n> import torch\r\n> torch.cuda.current_device()\r\n> ```\r\n> \r\n> The following always fails for me:\r\n> \r\n> ```\r\n> import torch\r\n> torch.cuda.is_available()\r\n> torch.cuda.current_device()  # fails here\r\n> ```\r\n> \r\n> My solution was to add to my scripts the call to `torch.cuda.current_device()` before any other cuda calls.\r\n> Hope this gives a hint as to where to look for the issue :)\r\n\r\nThanks! This is the exactly same thing that happens with me on Windows 10.\r\n\r\nIf I use torch.cuda.current_device() before anything cuda-related, it works like a charm.', ""> If I use torch.cuda.current_device() before anything cuda-related, it works like a charm.\r\n\r\nFor the record, this isn't supposed to be necessary, but it's possible this is broken."", 'Guys, I seem to find the root cause of this issue with the help of @Jonas1312 in https://github.com/pytorch/pytorch/issues/20635, it is caused by the fact that we changed the way we link against our libraries against `cudart`. I have made the PR  https://github.com/pytorch/pytorch/pull/21062. You can try whether it fixes your problem.', '> \r\n> \r\n> My system:\r\n> Windows 10\r\n> Cuda 10.1\r\n> Python 3.7.2\r\n> PyTorch 1.0.1\r\n> NVIDIA GeForce GTX 1050 Ti\r\n> \r\n> The following always works:\r\n> \r\n> ```\r\n> import torch\r\n> torch.cuda.current_device()\r\n> ```\r\n> \r\n> The following always fails for me:\r\n> \r\n> ```\r\n> import torch\r\n> torch.cuda.is_available()\r\n> torch.cuda.current_device()  # fails here\r\n> ```\r\n> \r\n> My solution was to add to my scripts the call to `torch.cuda.current_device()` before any other cuda calls.\r\n> Hope this gives a hint as to where to look for the issue :)\r\n\r\nThank you! I have the same problem, and I need to reboot the Python every time. According to what you said, I add torch.cuda.current_device() after import torch. It works.', 'fix was merged', '> My system:\r\n> Windows 10\r\n> Cuda 10.1\r\n> Python 3.7.2\r\n> PyTorch 1.0.1\r\n> NVIDIA GeForce GTX 1050 Ti\r\n> \r\n> The following always works:\r\n> \r\n> ```\r\n> import torch\r\n> torch.cuda.current_device()\r\n> ```\r\n> \r\n> The following always fails for me:\r\n> \r\n> ```\r\n> import torch\r\n> torch.cuda.is_available()\r\n> torch.cuda.current_device()  # fails here\r\n> ```\r\n> \r\n> My solution was to add to my scripts the call to `torch.cuda.current_device()` before any other cuda calls.\r\n> Hope this gives a hint as to where to look for the issue :)\r\n\r\nit worked for me as well \r\n\r\nimport torch \r\ntorch.cuda.current_device()', ""> I don't think cuda error 30 is an error on our side. Please try these things first.\r\n> \r\n> 1. Re-install latest GPU driver\r\n> 2. Reboot\r\n> 3. Ensure you have admin access\r\n\r\nYes,,after reboot my computer ,this error has been gone ""]",[],[],0,0
633,pytorch,30076,closed,LibTorch(C++) with Cuda is raising an exception,"I am trying to create NN with LibTorch 1.3 and C++ using Cuda 10.1 and Windows 10. For the build I am using Visual Studio 2019.

So far I tried basic examples and [MNIST example][1] with CPU which is working. However I cannot run it with CUDA. I tried to move model to GPU as it is described [here][2], but it is not working. 

> To move your model to GPU memory, you can write model.to(at::kCUDA);. Make sure the inputs to a model are also living in CUDA memory by calling tensor.to(at::kCUDA), which will return a new tensor in CUDA memory.

So I tried just simple

    int main(){
        auto net = std::make_shared<Net>();
        net->to(torch::kCUDA); //crashes here
    }

Then I tried to move simple Tensors to gpu memory but it crashes as well.

    #include <torch/torch.h>

    int main() 
    {
	    torch::Tensor a = torch::ones({ 2, 2 }, torch::requires_grad());
	    torch::Tensor b = torch::randn({ 2, 2 });
	    a.to(torch::kCUDA);    //Here it crashes
	    b.to(torch::kCUDA);    //
	    auto c = a + b;
    }

and I got:

    Exception thrown at 0x00007FFB8263A839 in Resnet50.exe: Microsoft C++ exception: c10::Error at memory location 0x000000E574979F30.
    Unhandled exception at 0x00007FFB8263A839 in Resnet50.exe: Microsoft C++ exception: c10::Error at memory location 0x000000E574979F30.

In debug mode it is pointing to  to

    auto operator()(Parameters... args) -> decltype(std::declval<FuncType>()(std::forward<Parameters>(args)...)) {
      return kernel_func_(std::forward<Parameters>(args)...);
    }

Using  shows it can find cuda device. 

Therefore I decided to build it from source but I it's not working either. It's related to [this](https://github.com/pytorch/pytorch/issues/30075#issue-525005329) post.

  [1]: https://pytorch.org/cppdocs/frontend.html#end-to-end-example
  [2]: https://pytorch.org/tutorials/advanced/cpp_export.html#step-4-executing-the-script-module-in-c",,"['We fixed a similar bug https://github.com/pytorch/pytorch/issues/22681. Please use the nightly build or build from source.', 'Current nightly version causing same error again.', '@Andrej-sens Maybe related to https://github.com/pytorch/pytorch/issues/31611.', ""Yes, that's the case. Thank you.""]",[],"['KernelBase.dll', 'torch::cuda::is_available()']",0,0
634,pytorch,15839,closed,MyPy Type Hinting Stub Files?,"## üìö Documentation
### (should this be a feature request?)
<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->

Having  type annotated stub files ([PEP561](https://www.python.org/dev/peps/pep-0561/)) and type hinted source ([PEP484](https://www.python.org/dev/peps/pep-0484/)) would be beneficial for maintainability, new users, and making contributions easier. ",,"['Would the stubs or annotating the current code base be a better first step?', '#7318 is a bug report asking for it.\r\n\r\nThe open PR #12500 implements this, but there might be work to do.']",[],['.pyi'],0,0
635,pytorch,16501,closed,[sparse sum] Sparse sum over dimmension gives unexpected results.,"## üêõ Bug

When summing over dimension 0 of tensor of 2 dimensions I'm getting a scalar, whereas summing over dimension -2 gives the correct answer. Is that expected?

## To Reproduce



## Expected behavior




## Environment


",,"['Thanks for the report, https://github.com/pytorch/pytorch/pull/16517 should fix it.']","[""\r\n    x = norm_adj = torch.tensor([[1., 0., 0., 1.],\r\n                                 [0., 1., 0., 0.],\r\n                                 [0., 1., 1., 0.],\r\n                                 [0., 1., 0., 2.]]).to_sparse()\r\n    print('RIGHT')\r\n    print(torch.sparse.sum(x,dim=-2))\r\n    print('WRONG')\r\n    print(torch.sparse.sum(x,dim=0))\r\n"", '\r\nRIGHT\r\ntensor(indices=tensor([[0, 1, 2, 3]]),\r\n       values=tensor([1., 3., 1., 3.]),\r\n       size=(4,), nnz=4, layout=torch.sparse_coo)\r\nWRONG\r\ntensor(8.)\r\n', '\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.14.2\r\nGCC version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.15.4)\r\n[pip] torch (1.0.0)\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2018.0.3                      1  \r\n[conda] mkl_fft                   1.0.6            py37hb8a8100_0  \r\n[conda] mkl_random                1.0.1            py37h5d10147_1  \r\n[conda] pytorch                   1.0.0                   py3.7_1    pytorch\r\n']",[],0,0
636,pytorch,28346,closed,Error for quantize_dynamic(resnet18) + torch.jit.trace + ios-demo-app/PyTorchDemo,"## üêõ Bug

I'm getting an error while loading quantized resnet18 on iphone with [ios-demo-app/PyTorchDemo](https://github.com/pytorch/ios-demo-app/tree/master/PyTorchDemo).



## To Reproduce
Convert the model with:


Run the [ios-demo-app/PyTorchDemo](https://github.com/pytorch/ios-demo-app/tree/master/PyTorchDemo) with . Get the stacktrace above.
Here is the forked demo with coverted models: [mirth/ios-demo-app](https://github.com/mirth/ios-demo-app)

## Expected behavior
For default image demo with  i'm getting ~50ms per frame on iPhone 6s.
For  on same iPhone i'm getting ~250ms per frame.
For  i'm getting an error with stacktrace above.
I'm expecting no error.

## Environment

PyTorch version: 1.3.0
Is debug build: No
CUDA used to build PyTorch: None

OS: Mac OSX 10.15
GCC version: Could not collect
CMake version: version 3.11.1

Python version: 3.6
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip] Could not collect
[conda] mkl                       2018.0.0             hc285769_4
[conda] mkl-service               1.1.2            py36h7ea6df4_4
[conda] torch                     1.3.0                    pypi_0    pypi
[conda] torchfile                 0.1.0                    pypi_0    pypi
[conda] torchnet                  0.0.4                    pypi_0    pypi
[conda] torchsummary              1.5.1                    pypi_0    pypi
[conda] torchvision               0.4.0                    pypi_0    pypi


cc @suo @jerryzh168 @jianyuh @dzhulgakov @raghuramank100",oncall: jit oncall: quantization triaged,"['Could you take a look? @jamesr66a ', 'quantize_dynamic is now supported on mobile, so this should work now.']","['\r\n2019-10-20 16:33:20.005883+0300 PyTorchDemo[592:88998] false CHECK FAILED at /Users/distiller/project/aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp\r\nThe above operation failed in interpreter, with the following stack trace:\r\nat code/__torch__/torchvision/models/resnet/___torch_mangle_59.py:288:8\r\n    input49 = torch._convolution(input48, weight35, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True)\r\n    input50 = torch.batch_norm(input49, weight36, bias17, running_mean17, running_var17, False, 0.10000000000000001, 1.0000000000000001e-05, True)\r\n    input51 = torch.relu_(input50)\r\n    input52 = torch._convolution(input51, weight37, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True)\r\n    out6 = torch.batch_norm(input52, weight38, bias18, running_mean18, running_var18, False, 0.10000000000000001, 1.0000000000000001e-05, True)\r\n    input53 = torch.add_(out6, input48, alpha=1)\r\n    input54 = torch.relu_(input53)\r\n    x = torch.adaptive_avg_pool2d(input54, [1, 1])\r\n    x0 = torch.flatten(x, 1, -1)\r\n    Y = ops.quantized.linear_dynamic(x0, _35)\r\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n    return torch.to(Y, 6, False, False)\r\nCompiled from code /Users/tolik/anaconda3/lib/python3.6/site-packages/torch/nn/quantized/dynamic/modules/linear.py(42): forward\r\n/Users/tolik/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py(525): _slow_forward\r\n/Users/tolik/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py(539): __call__\r\n/Users/tolik/anaconda3/lib/python3.6/site-packages/torchvision/models/resnet.py(208): forward\r\n/Users/tolik/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py(525): _slow_forward\r\n/Users/tolik/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py(539): __call__\r\n/Users/tolik/anaconda3/lib/python3.6/site-packages/torch/jit/__init__.py(997): trace_module\r\n/Users/tolik/anaconda3/lib/python3.6/site-packages/torch/jit/__init__.py(858): trace\r\nrepro.py(17): <module>\r\n', ""\r\nimport torch\r\nfrom torchvision.models import resnet18\r\n\r\nmodel = resnet18()\r\nmodel.eval()\r\nexample = torch.rand(1, 3, 224, 224)\r\n\r\ntraced_script_module = torch.jit.trace(model, example)\r\ntraced_script_module.save('/tmp/resnet18_float32.pt')\r\n\r\n\r\nmodel = resnet18()\r\nmodel.eval()\r\nexample = torch.rand(1, 3, 224, 224)\r\n\r\nmodel = torch.quantization.quantize_dynamic(model, dtype=torch.qint8)\r\ntraced_script_module = torch.jit.trace(model, example)\r\ntraced_script_module.save('/tmp/resnet18_qint8.pt')\r\n""]","['resnet18_qint8.pt', 'mobilenet_quantized.pt', 'resnet18_float32.pt', 'resnet18_qint8.pt']",0,0
637,pytorch,24452,closed,model deployment ÔºöMultiple serial modelsÔºåusing 3DconvÔºüÔºü,"## ‚ùì Questions and Help

I have about 4 to 10 models to execute seriallyÔºåevery model is 3D-convolutional modelÔºåI have tried TensorRT,but TensorRT only support 2 D conv,
what else can I try?

The needs are: stable and fast

",,"['Please use https://discuss.pytorch.org (https://discuss.pytorch.org/) for questions. Please kindly feel encouraged to reopen this issue, if this does not apply.']",[],[],0,0
638,pytorch,671,closed,Conv3d needs a no-bias option,"Right now, VolumetricConvolution in THNN doesn't mark or handle bias=NULL cases",medium priority (this tag is deprecated),"['Places that need to be guarded with: `if (bias != NULL) {...}`\r\n\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/lib/THNN/generic/VolumetricConvolutionMM.c#L303-L310 (if bias == NULL, fill the output with zeros)\r\n\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/lib/THNN/generic/VolumetricConvolutionMM.c#L554-L563\r\n\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/lib/THCUNN/generic/VolumetricConvolution.cu#L173-L196\r\n\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/lib/THCUNN/generic/VolumetricConvolution.cu#L456-L491\r\n\r\nWhen i doubt, look at SpatialConvolution*\r\n\r\ncc: @elistevens \r\n\r\n', ""Is there any significance to `VolumetricConvolution.cu` not having the `MM` suffix? I'm assuming that it corresponds to `SpatialConvolutionMM.cu`."", ""there's no significance. it's just unfortunate organic historical naming artifacts."", 'Gotta love those.\r\n\r\nAny tests I should be duplicating?', ""probs  need to replicate some of these with bias=False in constructor args:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/master/test/test_nn.py#L1562-L1581\r\n\r\nI realized that we dont test for bias=False in any of the conv* modules, no wonder we didn't catch the `bias=False` break"", ""Is the comment:\r\n```\r\nfor test_params in module_tests + new_module_tests:\r\n    # TODO: CUDA is not implemented yet\r\n```\r\nStill accurate?  I see `.cuda()` etc. in `NewModuleTest._do_test`. I'll remove if needed.\r\n\r\nAlso, should I approach the `bias=False` cases via copy/paste, or would something like `if 'Conv' in test_params['module_name']: kwargs['bias'] = False; NewModuleTest(**test_params)` be acceptable?"", 'Note that there is a pending PR adding no_bias support in THNN, but a CUDA PR is missing https://github.com/torch/nn/pull/1107', ""Is there a code migration path from `torch/nn` to `pytorch/pytorch` for the files that I'm presuming are shared between the two projects? I haven't dug in, but it looks like, for the files it touches, torch/nn#1107 is more complete than what I have.\r\n\r\nShould I manually copy the files?"", ""It's best if you can submit a PR with the C code to torch/nn, but we're accepting PRs either way I think. We're using git subtrees to merge the history of the repos."", 'PR here: https://github.com/pytorch/pytorch/pull/688']",[],[],0,0
639,pytorch,17161,closed,index operation is not supported in torch.HalfTensor ,"## üöÄ Feature
Currently, an indexing of torch.HalfTensor like half_tensor[indices] gives this runtime error.
RuntimeError: ""index"" not implemented for 'torch.HalfTensor'. So, requesting for an indexing method. 

## Motivation
It would be great to be able to index half tensors. My current use case for this is while doing large scale similarity metric computations on embeddings on the GPU where I figured I could use float16 to reduce memory usage.

## Pitch
Indexing method added to HalfTensor objects.

## Alternatives
For my use-case the alternative is to chunk up tensors appropriately to do similarity computations. While its works just as well, I can imagine other use cases where indexing on HalfTensor objects are needed.
",feature,"['Can you give a script to reproduce the error, and the version of PyTorch that you are using?', ""FTR, many indexing operations do work on half tensor:\r\n\r\n```\r\n>>> x = torch.empty(2,2,dtype=torch.half)\r\n>>> x = torch.empty(2,2,dtype=torch.half)[0]\r\n>>> torch.empty(2,2,dtype=torch.half)[0]\r\ntensor([-1.2450e+02,  1.6546e-03], dtype=torch.float16)\r\n>>> torch.empty(2,2,dtype=torch.half).cuda()[0]\r\ntensor([-1.2450e+02,  1.6546e-03], device='cuda:0', dtype=torch.float16)\r\n>>> torch.empty(2,2,dtype=torch.half)[(0,1)]\r\ntensor(0.0017, dtype=torch.float16)\r\n>>> torch.empty(2,2,dtype=torch.half)[1,:]\r\ntensor([nan, 0.], dtype=torch.float16)\r\n>>> torch.empty(2,2,dtype=torch.half)[:,1]\r\ntensor([0.0017, 0.0000], dtype=torch.float16)\r\n```"", '```python\r\nx = torch.empty(5, 5, dtype=torch.half)\r\ny = x[[0, 4, 2]]\r\n```\r\nI get `RuntimeError: ""index"" not implemented for \'torch.HalfTensor`. `1.0.1.post2` was the version I\'m running.']",[],[],0,0
640,pytorch,2404,closed,Advanced Indexing issues,"There seem to be issues with corner cases where  or  fails if  is 2 or 1 dimensional resp. In other words, an unnecessary trailing  causes failures, while it doesn't in numpy.

MWE is here: https://gist.github.com/arunmallya/23457765e37faf8932da9276e2b449a8",,"['Someone assign this to me and I can fix this.', '```data[:, mask, :]``` fails for me regardless of the dimensionality when ```mask``` has type ```torch.LongTensor```. Or am the only one with this problem?', 'addressed by https://github.com/pytorch/pytorch/pull/2589']",[],"['data[:, mask, ...]', 'data[mask, ...]', 'data', '...']",0,0
641,pytorch,27844,closed,"torch.hub does not handle tags and branches with path separator (e.g. ""/"") in them","## üêõ Bug

 treats tag or branch names as file paths; if the tag or branch has a path separator (e.g. ""/"" on Linux) in it e.g. branch  as in

    torch.hub.list(""moabitcoin/ig65m-pytorch:issues/4"")

then torch hub integration fails e.g. see




## To Reproduce

Steps to reproduce the behavior:

1. add a simple hubconf.py to your repository
1. create a tag or branch with a path separator in its name
1. use torch hub functionality with that tag or branch

## Expected behavior

 works with arbitrary git tags and branches

## Environment

<details>
<summary>env</summary>



</details>


## Additional context

Workaround: create a tag or branch alias without the path separator in it.

cc @ailzhang",module: hub triaged,"[""Thanks @daniel-j-h for reporting! I'll push a fix! ""]","['\r\n>>> torch.hub.list(""moabitcoin/ig65m-pytorch:issues/4"")\r\nDownloading: ""https://github.com/moabitcoin/ig65m-pytorch/archive/issues/4.zip"" to /root/.cache/torch/hub/issues/4.zip\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/usr/local/lib/python3.6/dist-packages/torch/hub.py"", line 272, in list\r\n    repo_dir = _get_cache_or_reload(github, force_reload, True)\r\n  File ""/usr/local/lib/python3.6/dist-packages/torch/hub.py"", line 153, in _get_cache_or_reload\r\n    download_url_to_file(url, cached_file, progress=False)\r\n  File ""/usr/local/lib/python3.6/dist-packages/torch/hub.py"", line 398, in download_url_to_file\r\n    f = tempfile.NamedTemporaryFile(delete=False, dir=dst_dir)\r\n  File ""/usr/lib/python3.6/tempfile.py"", line 690, in NamedTemporaryFile\r\n    (fd, name) = _mkstemp_inner(dir, prefix, suffix, flags, output_type)\r\n  File ""/usr/lib/python3.6/tempfile.py"", line 401, in _mkstemp_inner\r\n    fd = _os.open(file, flags, 0o600)\r\nFileNotFoundError: [Errno 2] No such file or directory: \'/root/.cache/torch/hub/issues/tmpq24habt0\'\r\n', '\r\nCollecting environment information...\r\nPyTorch version: 1.3.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.2\r\n[pip3] torch==1.3.0\r\n[pip3] torchvision==0.4.1\r\n[conda] Could not collect\r\n']","['torch.hub', '√¨ssues/4', 'torch.hub']",0,0
642,pytorch,8109,closed,Sometimes the torch.save method does not work properly.,"## Issue description
When indexing a part of the tensor, the entire original tensor is saved.

## Code example



I expect that divided tensors will be stored. (It will be about 100KB)
However, it is actually stored in the original matrix size. (Approximately 12MB)

## System Info

PyTorch
conda
Windows 10
0.4.0
Python 3.6
CUDA 9.0
Nvidia Geforce 1070",,"[""A simple workaround would be to do `.clone()` before saving, to make sure the view doesn't share memory with the original tensor (which is what causes `torch.save` to dump the whole block)."", ""this is expected behavior, so I'm closing the issue. (Adam's workaround in the comment above is sufficient for you to workaround)\r\n\r\nIf you are saving multiple views onto a larger Tensor, we want to make sure that the sharing is preserved when loading back, so the Tensor's backing storage is saved.""]","[""\r\nimport torch\r\norigin = torch.FloatTensor(128, 512, 7, 7) # original tensor (shape: [128, 512, 7, 7])\r\nindexed = origin[0] # part of tensor (shape: [512, 7, 7])\r\ntorch.save(indexed, 'tensor.pth') # save tensor\r\n""]",[],0,0
643,pytorch,5553,closed,NCCL Error 1 when using torch.nn.DataParallel,"I have ran into this error on the parameter broadcast step of .
It sounds like this, to be precise: .
#2332 is possibly related.
",,"['Steps I took to fix this issue:\r\n\r\n0.  Determine if CUDA version you\'ve installed is compatible with your GPU compute capability ([e.g. here](https://stackoverflow.com/questions/28932864/cuda-compute-capability-requirements))\r\n1.  Build NCCL from [source](https://github.com/NVIDIA/nccl) by running `NVCC_GENCODE=""-gencode=arch=compute_<compute_v>,code=sm_<compute_v> make CUDA_HOME=<cuda_path> test""` from source directory, where `<compute_v>` is compute capability version of your GPUs without dot (e.g. 3.0 would be 30). If you have GPUs with different compute capabilities duplicate the `-gencode=arch=compute_<compute_v>,code=sm_<compute_v> ` part as many times you as you need.\r\n2. Copy the contents of build folder into some permanent place (e.g. `/usr/local/nccl`).\r\n3. Run export `LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/nccl/lib`.\r\n4. Run tests from `test` folder to see if everything is fine (e.g. `./usr/local/nccl/test/single/all_reduce_test 10000`).\r\n4.5 If something is wrong in any of those tests, running the test with prefix `NCCL_DEBUG=WARN` or even `NCCL_DEBUG=INFO` will provide you information for further debugging, else proceed to the step 5.\r\n5. Set environmental variables `NCCL_ROOT_DIR`, `NCCL_LIB_DIR` and `NCCL_INCLUDE_DIR` to corresponding paths (e.g. `export NCCL_LIB_DIR=/usr/local/nccl/lib/`).\r\n6. Build pytorch from source.', '> Steps I took to fix this issue:\r\n> \r\n> 1. Determine if CUDA version you\'ve installed is compatible with your GPU compute capability ([e.g. here](https://stackoverflow.com/questions/28932864/cuda-compute-capability-requirements))\r\n> 2. Build NCCL from [source](https://github.com/NVIDIA/nccl) by running `NVCC_GENCODE=""-gencode=arch=compute_<compute_v>,code=sm_<compute_v> make CUDA_HOME=<cuda_path> test""` from source directory, where `<compute_v>` is compute capability version of your GPUs without dot (e.g. 3.0 would be 30). If you have GPUs with different compute capabilities duplicate the `-gencode=arch=compute_<compute_v>,code=sm_<compute_v> ` part as many times you as you need.\r\n> 3. Copy the contents of build folder into some permanent place (e.g. `/usr/local/nccl`).\r\n> 4. Run export `LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/nccl/lib`.\r\n> 5. Run tests from `test` folder to see if everything is fine (e.g. `./usr/local/nccl/test/single/all_reduce_test 10000`).\r\n>    4.5 If something is wrong in any of those tests, running the test with prefix `NCCL_DEBUG=WARN` or even `NCCL_DEBUG=INFO` will provide you information for further debugging, else proceed to the step 5.\r\n> 6. Set environmental variables `NCCL_ROOT_DIR`, `NCCL_LIB_DIR` and `NCCL_INCLUDE_DIR` to corresponding paths (e.g. `export NCCL_LIB_DIR=/usr/local/nccl/lib/`).\r\n> 7. Build pytorch from source.\r\n\r\nI am tripping on step 4: `bash: ./usr/local/nccl/test/single/all_reduce_test: No such file or directory`. I also tried `/usr/local/nccl/test/single/all_reduce_test 10000` (i.e. without the leading dot in the path) with the same result, `bash: /usr/local/nccl/test/single/all_reduce_test: No such file or directory`. Perhaps I am not executing step 1 correctly? In my case, I run the following:\r\n```bash\r\n# NVCC_GENCODE=""-gencode=arch=compute_30,code=sm_30 make CUDA_HOME=/usr/local/cuda-9.0/ test""\r\n# make -j src.build\r\n```\r\n\r\nI run that as opposed to `make -j src.build NVCC_GENCODE=""-gencode=arch=compute_30,code=sm_30 make CUDA_HOME=/usr/local/cuda-9.0/ test""`, as the latter command gives me \r\n\r\n```bash\r\n# make -j src.build NVCC_GENCODE=""-gencode=arch=compute_30,code=sm_30 make CUDA_HOME=/usr/local/cuda-9.0/ test""\r\nmake -C src build BUILDDIR=/root/nccl/build\r\nmake[1]: Entering directory \'/root/nccl/src\'\r\nGenerating nccl.h.in                           > /root/nccl/build/include/nccl.h\r\nCompiling  init.cu                             > /root/nccl/build/obj/init.o\r\nCompiling  ring.cu                             > /root/nccl/build/obj/ring.o\r\nCompiling  bootstrap.cu                        > /root/nccl/build/obj/bootstrap.o\r\nnvcc fatal   : A single input file is required for a non-link phase when an outputfile is specified\r\nMakefile:84: recipe for target \'/root/nccl/build/obj/init.o\' failed\r\nmake[1]: *** [/root/nccl/build/obj/init.o] Error 1\r\nmake[1]: *** Waiting for unfinished jobs....\r\nnvcc fatal   : A single input file is required for a non-link phase when an outputfile is specified\r\nMakefile:84: recipe for target \'/root/nccl/build/obj/ring.o\' failed\r\nmake[1]: *** [/root/nccl/build/obj/ring.o] Error 1\r\nnvcc fatal   : A single input file is required for a non-link phase when an outputfile is specified\r\nMakefile:84: recipe for target \'/root/nccl/build/obj/bootstrap.o\' failed\r\nmake[1]: *** [/root/nccl/build/obj/bootstrap.o] Error 1\r\nmake[1]: *** wait: No child processes.  Stop.\r\nMakefile:25: recipe for target \'src.build\' failed\r\nmake: *** [src.build] Error 2\r\n```', 'I can help -  @aleksod \r\nLooking at their official build guide: https://github.com/NVIDIA/nccl\r\n\r\nyou need to clone a different repo for the tests:\r\n`git clone https://github.com/NVIDIA/nccl-tests.git`\r\nThen, in order to build the tests, enter that repo and run:\r\nCUDA_HOME=[path to your cuda main install dir] NCCL_HOME=[path to your nccl build dir] make\r\n\r\nfor example\r\nCUDA_HOME=/usr/local/cuda-10.0 NCCL_HOME=/path/to/nccl/build make\r\n\r\nthen, an example test from their official doc is:\r\n`./build/all_reduce_perf -b 8 -e 256M -f 2 -g <ngpus>`\r\n\r\nif you get missing .so libraries errors, you can use LD_LIBRARY_PATH to run the test. for example:\r\n\r\n`LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64:/some/path/to/nccl/nccl/build/lib:$LD_LIBRARY_PATH ./build/all_reduce_perf -b 8 -e 256M -f 2 -g 2`\r\n\r\n\r\n\r\n\r\n']",[],"['torch.nn.parallel.replicate', 'NCCL Error 1: unhandled cuda error']",0,0
644,pytorch,10857,closed,"got error ""ninja: error: loading 'build/build.global.ninja': No such file or directory""","If you have a question or would like help and support, please ask at our
[forums](https://discuss.pytorch.org/).

If you are submitting a feature request, please preface the title with [feature request].
If you are submitting a bug report, please fill in the following details.

## Issue description

## Code example

## System Info
Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


- PyTorch or Caffe2: PyTorch
- How you installed PyTorch (conda, pip, source): source
- Build command you used (if compiling from source): python setup.py build
- OS: CentOS7
- PyTorch version: master branch (ca567862)
- Python version: 3.7.0
- CUDA/cuDNN version: 9.2/7.2.1
- GPU models and configuration: NVIDIA GTX1070
- GCC version (if compiling from source): 4.8.5
- CMake version: 3.12.0
- Versions of any other relevant libraries:
",,"['Could you try to clean `python setup.py clean` and rebuild?', 'Every time I cleaned and build, but it shows the same result.', 'Try uninstalling ninja and building without reinstalling it?', ""Oh, it works! Don't we use ninja to build pytorch any more? Thank you so much @zou3519 !""]","[""\r\nBuilding from source, an error reported related to ninja and stopped the building process\r\nrunning build_ext\r\n-- Building with NumPy bindings\r\n-- Detected cuDNN at /usr/local/cudnn/lib64/libcudnn.so.7, /usr/local/cudnn/include\r\n-- Not using MIOpen\r\n-- Detected CUDA at /usr/local/cuda\r\n-- Detected MKLDNN at /usr/local/lib/libmkldnn.so, /usr/local/include\r\n-- Using system provided NCCL library at /usr/local/nccl_2.2.13-1+cuda_9.2/lib/libnccl.so.2.2.13, /usr/local/nccl/include\r\n-- Building with distributed package \r\nninja: error: loading 'build/build.global.ninja': No such file or directory\r\nCommand '['/home/jbaik/.pyenv/versions/3.7.0/lib/python3.7/site-packages/ninja/data/bin/ninja', '-f', 'build/build.global.ninja']' returned non-zero exit status 1.\r\n"", '\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n']",[],0,0
645,pytorch,11482,closed,Error -3 while decompressing data: invalid block type,"## Issue description
I got the problem when I tried to download MNIST dataset. Tested twice: with pytorch installed using pip and pytorch compiled from source.
## Code example




## System Info

PyTorch version: 0.3.1
Is debug build: No
CUDA used to build PyTorch: None

OS: Mac OSX 10.13.6
GCC version: Could not collect
CMake version: version 3.12.1

Python version: 3.6
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip] Could not collect
[conda] Could not collect
",,['Closing due to age'],"[""\r\ndataset = datasets.MNIST(\r\n        './data',\r\n        train=True,\r\n        download=True,\r\n        transform=transforms.Compose([\r\n            transforms.ToTensor(),\r\n            transforms.Normalize((0.1307, ), (0.3081, ))\r\n        ]))\r\n""]",[],0,0
646,pytorch,9264,closed,Dilated Conv3d segfaults (cpu),"I have encountered a segfault when running  with dilation under pytorch 0.4.0 on the cpu for some specific tensor sizes. This is the code to reproduce the behavior:



And this is the stack trace from gdb:


And this is the environment information:


Thanks for your help!",,"[""repro'ed on master!"", 'oh it overflowed int range.... fix incoming', 'Well done, Tongzhou, that was really brisk!!']","['\r\nimport torch\r\nimport torch.nn as nn\r\nmod = nn.Conv3d(96, 256, 4, 1, 0, 2)     # does not crash with dilation = 1\r\nmod(torch.zeros([1, 96, 111, 63, 111]))\r\n', '\r\n#0  0x00007fffbfb0c3d7 in THNN_Floatvol2col(float const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float*)\r\n    () from /home/martin/miniconda3/envs/BAI/lib/python3.6/site-packages/torch/lib/libATen.so\r\n#1  0x00007fffbfc240cb in THNN_FloatVolumetricDilatedConvolution_updateOutput ()\r\n   from /home/martin/miniconda3/envs/BAI/lib/python3.6/site-packages/torch/lib/libATen.so\r\n#2  0x00007fffbf42ae43 in at::CPUFloatType::thnn_conv_dilated3d_forward(at::Tensor const&, at::Tensor const&, at::ArrayRef<long>, at::Tensor const&, at::ArrayRef<long>, at::ArrayRef<long>, at::ArrayRef<long>) const () from /home/martin/miniconda3/envs/BAI/lib/python3.6/site-packages/torch/lib/libATen.so\r\n#3  0x00007fffdeda145d in torch::autograd::VariableType::thnn_conv_dilated3d_forward (this=0x555556570a40, self=..., weight=..., kernel_size=..., bias=..., \r\n    stride=..., padding=..., dilation=...) at torch/csrc/autograd/generated/VariableType.cpp:17592\r\n#4  0x00007fffbf625595 in at::Type::thnn_conv_dilated3d(at::Tensor const&, at::Tensor const&, at::ArrayRef<long>, at::Tensor const&, at::ArrayRef<long>, at::ArrayRef<long>, at::ArrayRef<long>) const () from /home/martin/miniconda3/envs/BAI/lib/python3.6/site-packages/torch/lib/libATen.so\r\n#5  0x00007fffded3ec54 in torch::autograd::VariableType::thnn_conv_dilated3d (this=0x555556570a40, self=..., weight=..., kernel_size=..., bias=..., stride=..., \r\n    padding=..., dilation=...) at torch/csrc/autograd/generated/VariableType.cpp:17528\r\n#6  0x00007fffbf313a2b in at::native::_convolution_nogroup(at::Tensor const&, at::Tensor const&, at::Tensor const&, at::ArrayRef<long>, at::ArrayRef<long>, at::ArrayRef<long>, bool, at::ArrayRef<long>) () from /home/martin/miniconda3/envs/BAI/lib/python3.6/site-packages/torch/lib/libATen.so\r\n#7  0x00007fffbf620bae in at::Type::_convolution_nogroup(at::Tensor const&, at::Tensor const&, at::Tensor const&, at::ArrayRef<long>, at::ArrayRef<long>, at::ArrayRef<long>, bool, at::ArrayRef<long>) const () from /home/martin/miniconda3/envs/BAI/lib/python3.6/site-packages/torch/lib/libATen.so\r\n#8  0x00007fffded40db7 in torch::autograd::VariableType::_convolution_nogroup (this=0x555556570a40, input=..., weight=..., bias=..., stride=..., padding=..., \r\n    dilation=..., transposed=false, output_padding=...) at torch/csrc/autograd/generated/VariableType.cpp:18403\r\n#9  0x00007fffbf318fe8 in at::native::_convolution(at::Tensor const&, at::Tensor const&, at::Tensor const&, at::ArrayRef<long>, at::ArrayRef<long>, at::ArrayRef<long>, bool, at::ArrayRef<long>, long, bool, bool, bool) () from /home/martin/miniconda3/envs/BAI/lib/python3.6/site-packages/torch/lib/libATen.so\r\n#10 0x00007fffbf620b64 in at::Type::_convolution(at::Tensor const&, at::Tensor const&, at::Tensor const&, at::ArrayRef<long>, at::ArrayRef<long>, at::ArrayRef<long>, bool, at::ArrayRef<long>, long, bool, bool, bool) const () from /home/martin/miniconda3/envs/BAI/lib/python3.6/site-packages/torch/lib/libATen.so\r\n#11 0x00007fffded40a6e in torch::autograd::VariableType::_convolution (this=0x555556570a40, input=..., weight=..., bias=..., stride=..., padding=..., \r\n    dilation=..., transposed=false, output_padding=..., groups=1, benchmark=false, deterministic=false, cudnn_enabled=true)\r\n    at torch/csrc/autograd/generated/VariableType.cpp:18386\r\n#12 0x00007fffbf313e0e in at::native::convolution(at::Tensor const&, at::Tensor const&, at::Tensor const&, at::ArrayRef<long>, at::ArrayRef<long>, at::ArrayRef<long>, bool, at::ArrayRef<long>, long) () from /home/martin/miniconda3/envs/BAI/lib/python3.6/site-packages/torch/lib/libATen.so\r\n#13 0x00007fffbf620afe in at::Type::convolution(at::Tensor const&, at::Tensor const&, at::Tensor const&, at::ArrayRef<long>, at::ArrayRef<long>, at::ArrayRef<long>, bool, at::ArrayRef<long>, long) const () from /home/martin/miniconda3/envs/BAI/lib/python3.6/site-packages/torch/lib/libATen.so\r\n#14 0x00007fffdece124c in torch::autograd::VariableType::convolution (this=0x555556570a40, input=..., weight=..., bias=..., stride=..., padding=..., \r\n    dilation=..., transposed=<optimised out>, output_padding=..., groups=1) at torch/csrc/autograd/generated/VariableType.cpp:18368\r\n#15 0x00007fffbf3141aa in at::native::conv3d(at::Tensor const&, at::Tensor const&, at::Tensor const&, at::ArrayRef<long>, at::ArrayRef<long>, at::ArrayRef<long>, long) () from /home/martin/miniconda3/envs/BAI/lib/python3.6/site-packages/torch/lib/libATen.so\r\n#16 0x00007fffbf620d22 in at::Type::conv3d(at::Tensor const&, at::Tensor const&, at::Tensor const&, at::ArrayRef<long>, at::ArrayRef<long>, at::ArrayRef<long>, long) const () from /home/martin/miniconda3/envs/BAI/lib/python3.6/site-packages/torch/lib/libATen.so\r\n#17 0x00007fffdece147e in torch::autograd::VariableType::conv3d (this=0x555556570a40, input=..., weight=..., bias=..., stride=..., padding=..., dilation=..., \r\n    groups=1) at torch/csrc/autograd/generated/VariableType.cpp:18446\r\n#18 0x00007fffdee94387 in at::conv3d (groups=1, dilation=..., padding=..., stride=..., bias=..., weight=..., input=...)\r\n    at /opt/conda/conda-bld/pytorch_1524584710464/work/torch/lib/tmp_install/include/ATen/Functions.h:3044\r\n#19 torch::autograd::dispatch_conv3d (groups=1, dilation=..., padding=..., stride=..., bias=..., weight=..., input=...)\r\n    at torch/csrc/autograd/generated/python_torch_functions_dispatch.h:1073\r\n#20 torch::autograd::THPVariable_conv3d (self=<optimised out>, args=<optimised out>, kwargs=<optimised out>)\r\n    at torch/csrc/autograd/generated/python_torch_functions.cpp:1662\r\n', '\r\nCollecting environment information...\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.61\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.14.5)\r\n[pip] torch (0.4.0)\r\n[pip] torchvision (0.2.1)\r\n[conda] pytorch                   0.4.0           py36_cuda8.0.61_cudnn7.1.2_1    pytorch\r\n[conda] torchvision               0.2.1                    py36_1    pytorch\r\n']",['Conv3d'],0,0
647,pytorch,367,closed,Support dilation in Conv1d and Conv3d,,medium priority (this tag is deprecated),[],[],[],0,0
648,pytorch,10435,closed,[Caffe2 Bug] Error ISO C++ forbids declaration of DISABLE_COPY_AND_ASSIGN with no type,"## Issue description

Compile error when using 

https://github.com/pytorch/pytorch/blob/ab6afc2b238deb9e3b731399a367385518b788e5/modules/rocksdb/rocksdb.cc#L70

## System Info

- PyTorch or Caffe2: Caffe2
- How you installed PyTorch (conda, pip, source): source
- Build command you used (if compiling from source): cmake+ninja
- OS: CentOS
- PyTorch version: master
- GCC version (if compiling from source): 6.3.1
- CMake version: 3.12
",,['Merged'],[],['-DUSE_ROCKSDB=ON'],0,0
649,pytorch,7989,closed,[documentation request] 'clone' needs better documentation,"[The current documentation for  is](https://pytorch.org/docs/stable/tensors.html?highlight=clone#torch.Tensor.clone):

> Returns a copy of the  tensor. The copy has the same size and data type as .

This documentation fails to elucidate the fact that any gradient propagating through the cloned tensor will propagate to the original tensor. This is critical to the functionality of clone (and is why the method isn't called ""copy""), and is just begging for newcomers to make hard-to-find mistakes. ",todo,"['Thanks for the report, @rsokl. Please feel free to send a pr!']",[],"['Tensor.clone', 'self', 'self']",0,0
650,pytorch,17029,closed,Floating Point Exception on PyTorch nightlies,"## üêõ Bug
We are seeing a floating point exception in the nightly containers but not in v1.0.1 container. I believe there is a mkl-dnn patch that needs to be on master?

@soumith ",,"[""This is fixed in the latest mkl-dnn https://github.com/intel/mkl-dnn/blob/b88908909240bdab10f1cb7c256caa264fc15067/src/cpu/xbyak/xbyak_util.h#L216, but not in the mkl-dnn hash pytorch is using https://github.com/intel/mkl-dnn/blob/830a10059a018cd2634d94195140cf2d8790a75a/src/cpu/xbyak/xbyak_util.h#L186.\r\nmkl-dnn comes as ideep submodule, and ideep is not updated to point to the latest mkl-dnn, so it's a bit of a mess. "", '#16183 is related', '@mingfeima can you help get this patch into a ideep/mkl-dnn release that pytorch uses. For v1.0.1, I manually patched the source before building binaries.', '@soumith will do.', ' @Soumith  We has done the internal validation and submit a patch for this. https://github.com/pytorch/pytorch/pull/17107 ', 'Thank you.\r\nFixed via https://github.com/pytorch/pytorch/pull/17107']",[],[],0,0
651,pytorch,28201,closed,Optimize GroupNorm in PyTorch,"## üöÄ Feature
<!-- A clear and concise description of the feature proposal -->
Improve the performance of GroupNorm operator.

## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->

Similar as https://github.com/pytorch/pytorch/issues/27633, the current GroupNorm implementation is reshaping the input and doing BatchNorm to get the moments of input, then using addcmul for affine. This implementation is inefficient for both CPU and CUDA. 

The performance benchmark for input shape = [128, 256, 28, 28], num_groups = 32 is shown below.



And the performance benchmark for input shape = [256, 512, 56, 56], num_groups = 32 is shown below.



We can see that for both CPU and GPU version of GroupNorm, using BatchNorm with addcmul make things slow especially for backward pass. Actually on CPU side, since BatchNorm for inference is a affine function and can be fused with Conv, it makes the GroupNorm very slow when using BatchNorm on CPU for inference.

## Pitch

<!-- A clear and concise description of what you want to happen. -->

To implement a optimized version of GroupNorm which fused everything together.

## Alternatives

<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->

## Additional context

<!-- Add any other context or screenshots about the feature request here. -->
cc @dzhulgakov @ngimel @ppwwyyxx 
",triaged,[],"['\r\nGroupNorm forward: 210.97913278000487ms\r\n--------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  \r\nName                        Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     CUDA total %     CUDA total       CUDA time avg    Number of Calls  \r\n--------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  \r\nnative_batch_norm           68.00%           135.920s         68.00%           135.920s         135.920ms        NaN              0.000us          0.000us          1000             \r\naddcmul                     31.94%           63.841s          31.94%           63.841s          63.841ms         NaN              0.000us          0.000us          1000             \r\nview                        0.03%            69.572ms         0.03%            69.572ms         17.393us         NaN              0.000us          0.000us          4000             \r\ngroup_norm                  0.01%            29.095ms         100.00%          199.870s         199.870ms        NaN              0.000us          0.000us          1000             \r\n_batch_norm_impl_index      0.00%            5.179ms          68.01%           135.925s         135.925ms        NaN              0.000us          0.000us          1000             \r\nbatch_norm                  0.00%            4.594ms          68.01%           135.929s         135.929ms        NaN              0.000us          0.000us          1000             \r\ncontiguous                  0.00%            1.815ms          0.00%            1.815ms          1.815us          NaN              0.000us          0.000us          1000             \r\n--------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  \r\nSelf CPU time total: 199.870s\r\nCUDA time total: 0.000us\r\n\r\nGroupNorm backward: 498.55486265799846ms\r\n-----------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  \r\nName                                 Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     CUDA total %     CUDA total       CUDA time avg    Number of Calls  \r\n-----------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  \r\nmul                                  46.02%           224.071s         46.02%           224.071s         56.018ms         NaN              0.000us          0.000us          4000             \r\nnative_batch_norm_backward           43.86%           213.559s         43.86%           213.559s         213.559ms        NaN              0.000us          0.000us          1000             \r\nsum                                  3.91%            19.040s          3.91%            19.040s          9.520ms          NaN              0.000us          0.000us          2000             \r\nadd_                                 3.28%            15.951s          3.28%            15.951s          5.317ms          NaN              0.000us          0.000us          3000             \r\nAddcmulBackward                      1.95%            9.501s           47.97%           233.571s         233.571ms        NaN              0.000us          0.000us          1000             \r\ntorch::autograd::AccumulateGrad      0.97%            4.737s           4.25%            20.687s          6.896ms          NaN              0.000us          0.000us          3000             \r\nas_strided                           0.00%            20.049ms         0.00%            20.049ms         5.012us          NaN              0.000us          0.000us          4000             \r\nNativeBatchNormBackward              0.00%            13.951ms         43.86%           213.573s         213.573ms        NaN              0.000us          0.000us          1000             \r\nreshape                              0.00%            13.034ms         0.01%            33.083ms         8.271us          NaN              0.000us          0.000us          4000             \r\nViewBackward                         0.00%            9.419ms          0.01%            42.502ms         10.626us         NaN              0.000us          0.000us          4000             \r\ntorch::autograd::GraphRoot           0.00%            1.189ms          0.00%            1.189ms          1.189us          NaN              0.000us          0.000us          1000             \r\n-----------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  \r\nSelf CPU time total: 486.916s\r\nCUDA time total: 0.000us\r\n', '\r\nGroupNorm forward: 11.333400868010358ms\r\n--------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  \r\nName                        Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     CUDA total %     CUDA total       CUDA time avg    Number of Calls  \r\n--------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  \r\ngroup_norm                  23.06%           56.891ms         100.00%          246.741ms        246.741us        31.20%           11.369s          11.369ms         1000             \r\nbatch_norm                  8.59%            21.194ms         35.82%           88.382ms         88.382us         18.89%           6.884s           6.884ms          1000             \r\n_batch_norm_impl_index      6.50%            16.045ms         27.23%           67.189ms         67.189us         18.86%           6.872s           6.872ms          1000             \r\nnative_batch_norm           20.73%           51.143ms         20.73%           51.143ms         51.143us         18.83%           6.861s           6.861ms          1000             \r\naddcmul                     15.17%           37.434ms         15.17%           37.434ms         37.434us         12.13%           4.419s           4.419ms          1000             \r\nview                        21.76%           53.695ms         21.76%           53.695ms         13.424us         0.06%            21.595ms         5.399us          4000             \r\ncontiguous                  4.19%            10.339ms         4.19%            10.339ms         10.339us         0.03%            9.650ms          9.650us          1000             \r\n--------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  \r\nSelf CPU time total: 246.741ms\r\nCUDA time total: 36.436s\r\n\r\nGroupNorm backward: 42.1425356430118ms\r\n-----------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  \r\nName                                 Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     CUDA total %     CUDA total       CUDA time avg    Number of Calls  \r\n-----------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  \r\nAddcmulBackward                      10.08%           69.875ms         36.22%           251.046ms        251.046us        24.64%           19.492s          19.492ms         1000             \r\nmul                                  26.14%           181.171ms        26.14%           181.171ms        45.293us         24.60%           19.460s          4.865ms          4000             \r\nNativeBatchNormBackward              3.44%            23.832ms         9.82%            68.072ms         68.072us         14.23%           11.261s          11.261ms         1000             \r\nnative_batch_norm_backward           6.38%            44.240ms         6.38%            44.240ms         44.240us         14.23%           11.255s          11.255ms         1000             \r\ntorch::autograd::AccumulateGrad      7.14%            49.495ms         16.52%           114.485ms        38.162us         8.02%            6.343s           2.114ms          3000             \r\nadd_                                 9.38%            64.990ms         9.38%            64.990ms         21.663us         8.00%            6.326s           2.109ms          3000             \r\nsum                                  11.71%           81.163ms         11.71%           81.163ms         40.581us         6.15%            4.863s           2.431ms          2000             \r\nViewBackward                         9.72%            67.398ms         23.92%           165.801ms        41.450us         0.07%            57.930ms         14.482us         4000             \r\nreshape                              8.35%            57.903ms         14.20%           98.403ms         24.601us         0.04%            35.134ms         8.783us          4000             \r\nas_strided                           5.84%            40.500ms         5.84%            40.500ms         10.125us         0.01%            11.111ms         2.778us          4000             \r\ntorch::autograd::GraphRoot           1.81%            12.526ms         1.81%            12.526ms         12.526us         0.01%            8.698ms          8.698us          1000             \r\n-----------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  \r\nSelf CPU time total: 693.093ms\r\nCUDA time total: 79.113s\r\n\r\n']",[],0,0
652,pytorch,4336,closed,Linking Error: ../libATen.so.1: undefined reference to functions from NNPACK and some other libs...,"Hi, when I build the CPU version from source, I met the following issue. I am using Python 3.5, Ubuntu 16.04. Does anyone know how to fix it? Thanks a lot!

nnp_convolution_kernel_gradient'
> ../libATen.so.1: undefined reference to pthreadpool_create'
> ../libATen.so.1: undefined reference to nnp_convolution_inference'
> ../libATen.so.1: undefined reference to nnp_convolution_kernel_gradient'
> ../libATen.so.1: undefined reference to pthreadpool_create'
> ../libATen.so.1: undefined reference to nnp_convolution_inference'
> ../libATen.so.1: undefined reference to nnp_convolution_kernel_gradient'
> ../libATen.so.1: undefined reference to pthreadpool_create'
> ../libATen.so.1: undefined reference to nnp_convolution_inference'
> ../libATen.so.1: undefined reference to nnp_convolution_kernel_gradient'
> ../libATen.so.1: undefined reference to pthreadpool_create'
> ../libATen.so.1: undefined reference to nnp_convolution_inference'
> ../libATen.so.1: undefined reference to ",,"['Can you post the full build log? Do you have NNPACK installed somewhere?', 'Hi @apaszke Thanks for your response! The full build log is as follows. It seems that NNPACK had been installed. ` Found NNPACK    (include: /usr/local/include, library: /usr/local/lib/libnnpack.so)`\r\n\r\n```\r\n> running install\r\n> running build_deps\r\n> CUDA_TOOLKIT_ROOT_DIR not found or specified\r\n> -- Could NOT find CUDA (missing:  CUDA_TOOLKIT_ROOT_DIR CUDA_NVCC_EXECUTABLE CUDA_INCLUDE_DIRS CUDA_CUDART_LIBRARY) (Required is at least version ""5.5"")\r\n> -- Found gcc >=5 and CUDA <= 7.5, adding workaround C++ flags\r\n> -- Automatic GPU detection failed. Building for common architectures.\r\n> -- Autodetected CUDA architecture(s): 3.0;3.5;5.0;5.2+PTX\r\n> -- Could not find CUDA with FP16 support, compiling without torch.CudaHalfTensor\r\n> -- Removing -DNDEBUG from compile flags\r\n> -- Compiling with OpenMP support\r\n> -- MAGMA not found. Compiling without MAGMA support\r\n> -- Could not find hardware support for NEON on this machine.\r\n> -- No OMAP3 processor on this machine.\r\n> -- No OMAP4 processor on this machine.\r\n> -- SSE2 Found\r\n> -- SSE3 Found\r\n> -- AVX Found\r\n> -- AVX2 Found\r\n> -- Atomics: using GCC intrinsics\r\n> -- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_gf - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_intel - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_gf - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_intel - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_gf - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_intel - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf - mkl_sequential - mkl_core - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel - mkl_sequential - mkl_core - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_gf - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_intel - mkl_intel_thread - mkl_core - gomp - pthread - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_gf - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_intel - mkl_intel_thread - mkl_core - iomp5 - pthread - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_gf_lp64: not found\r\n> -- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_gf - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_gf: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_intel_lp64: not found\r\n> -- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl_intel - mkl_intel_thread - mkl_core - pthread - m - dl]\r\n> --   Library mkl_intel: not found\r\n> -- Checking for [mkl - guide - pthread - m]\r\n> --   Library mkl: not found\r\n> -- MKL library not found\r\n> -- Checking for [openblas]\r\n> --   Library openblas: /usr/lib/libopenblas.so\r\n> -- Found a library with BLAS API (open).\r\n> -- Found a library with LAPACK API. (open)\r\n> disabling CUDA because NO_CUDA is set\r\n> -- Could NOT find CUDNN (missing:  CUDNN_INCLUDE_DIR CUDNN_LIBRARY) \r\n> -- CuDNN not found. Compiling without CuDNN support\r\n> -- Found NNPACK    (include: /usr/local/include, library: /usr/local/lib/libnnpack.so)\r\n> -- Found PTHREADPOOL (library: /usr/local/lib/libpthreadpool.so)\r\n> -- Using python found in /usr/bin/python3\r\n> disable contrib because ATEN_NO_CONTRIB is set\r\n> -- Configuring done\r\n> -- Generating done\r\n> -- Build files have been written to: /home/pytorch-git/torch/lib/build/aten\r\n> [  1%] Built target aten_files_are_generated\r\n> [ 84%] Built target ATen\r\n> [ 85%] Linking CXX executable atest\r\n> [ 85%] Linking CXX executable basic\r\n> [ 86%] Linking CXX executable native_test\r\n> [ 87%] Linking CXX executable broadcast_test\r\n> .../libATen.so.1: undefined reference to .`nnp_convolution_kernel_gradient\'\r\n> ..//libATen.so.1: undefinedlibATen.so.1 reference to `nnp_convolution_input_gradient\': \r\n> ../libATen.so.1undefined : undefined reference to `pthreadpool_create\'\r\n> ../libATen.so.1: undefined reference to `nnp_convolution_output\'\r\n> ../libATen.so.1: undefined reference to `nnp_convolution_inference\'\r\n> ../libATen.so.1: undefined reference toreference ` nnp_initializeto\'\r\n>  collect2: error: ld returned 1 exit status\r\n> `nnp_convolution_kernel_gradient\'\r\n> ../libATen.so.1: undefined reference to `nnp_convolution_input_gradient\'\r\n> ../libATen.so.1: undefined reference to `pthreadpool_create\'\r\n> ../libATen.so.1: undefined reference to `nnp_convolution_output\'\r\n> ../src/ATen/test/CMakeFiles/atest.dir/build.make:96: recipe for target \'src/ATen/test/atest\' failed\r\n> make[2]: *** [src/ATen/test/atest] Error 1\r\n> libATen.so.1: undefined reference to `nnp_convolution_inference\'\r\n> CMakeFiles/Makefile2:243: recipe for target \'src/ATen/test/CMakeFiles/atest.dir/all\' failed\r\n> make[1]: *** [src/ATen/test/CMakeFiles/atest.dir/all] Error 2\r\n> .make[1]: *** Waiting for unfinished jobs....\r\n> ./libATen.so.1: undefined reference to `nnp_initialize\'\r\n> collect2: error: ld returned 1 exit status\r\n> src/ATen/test/CMakeFiles/broadcast_test.dir/build.make:96: recipe for target \'src/ATen/test/broadcast_test\' failed\r\n> make[2]: *** [src/ATen/test/broadcast_test] Error 1\r\n> CMakeFiles/Makefile2:354: recipe for target \'src/ATen/test/CMakeFiles/broadcast_test.dir/all\' failed\r\n> make[1]: *** [src/ATen/test/CMakeFiles/broadcast_test.dir/all] Error 2\r\n> ../libATen.so.1: undefined reference to `nnp_convolution_kernel_gradient\'\r\n> ../libATen.so.1: undefined reference to `nnp_convolution_input_gradient\'\r\n> ../libATen.so.1: undefined reference to `pthreadpool_create\'\r\n> ../libATen.so.1: undefined reference to `nnp_convolution_output\'\r\n> ../libATen.so.1: undefined reference to `nnp_convolution_inference\'\r\n> ../libATen.so.1: undefined reference to `nnp_initialize\'\r\n> collect2: error: ld returned 1 exit status\r\n> ../libATen.so.1: undefined reference to `nnp_convolution_kernel_gradient\'\r\n> ../libATen.so.1: undefined reference to `nnp_convolution_input_gradient\'\r\n> ../libATen.so.1: undefined reference to `pthreadpool_create\'\r\n> ../libATen.so.1: undefined reference to `nnp_convolution_output\'\r\n> ../libATen.so.1: undefined reference to `nnp_convolution_inference\'\r\n> ../libATen.so.1: undefined reference to `nnp_initialize\'\r\n> collect2: error: ld returned 1 exit status\r\n> src/ATen/test/CMakeFiles/native_test.dir/build.make:96: recipe for target \'src/ATen/test/native_test\' failed\r\n> make[2]: *** [src/ATen/test/native_test] Error 1\r\n> CMakeFiles/Makefile2:280: recipe for target \'src/ATen/test/CMakeFiles/native_test.dir/all\' failed\r\n> make[1]: *** [src/ATen/test/CMakeFiles/native_test.dir/all] Error 2\r\n> src/ATen/test/CMakeFiles/basic.dir/build.make:96: recipe for target \'src/ATen/test/basic\' failed\r\n> make[2]: *** [src/ATen/test/basic] Error 1\r\n> CMakeFiles/Makefile2:317: recipe for target \'src/ATen/test/CMakeFiles/basic.dir/all\' failed\r\n> make[1]: *** [src/ATen/test/CMakeFiles/basic.dir/all] Error 2\r\n> Makefile:127: recipe for target \'all\' failed\r\n> make: *** [all] Error 2\r\n```', 'I met the same problem. I think it caused by NNPACK. Is there any solution for this problem?', ""I recently changed how NNPACK links with ATen so this is probably my fault.\r\n\r\nFor now, if you apply  https://github.com/pytorch/pytorch/pull/4401 you can disable nnpack with `NO_NNPACK=1`. I'll look more into why there are linker errors after the new year."", '#4401 has been merged. so now with master you can do `NO_NNPACK=1 python setup.py install`', 'Reopening since there is still an underlying nnpack build bug.', ""The root-cause of this error is that you probably built and installed nnpack with `--inference-only`, but PyTorch requires both inference and backwards functions to be compiled. I'll beef up the CMake to test when this situation has occurred."", ""I pulled the code from the main branch today and still got this error:\r\n\r\n[ 91%] Linking CXX executable wrapdim_test\r\n../libATen.so.1: undefined reference to `nnp_convolution_kernel_gradient'\r\n\r\nI checked that the nnp_convolution_kernel_gradient is present in the libnnpack.so in /usr/local/lib on my system.\r\n\r\nWhen I pulled the change in 4439, I got a little bit further in compilation (wrapdim_test compiled ok) , but still got this:\r\n\r\n[ 93%] Linking CXX executable undefined_tensor_test\r\n../libATen.so.1: undefined reference to `nnp_convolution_kernel_gradient'\r\n\r\n  ""]",[],"['', '\r\n> [ 89%] Linking CXX executable atest\r\n> ../libATen.so.1: undefined reference to ', ""nnp_convolution_input_gradient'\r\n> ../libATen.so.1: undefined reference to "", ""nnp_convolution_output'\r\n> ../libATen.so.1: undefined reference to "", ""nnp_initialize'\r\n> collect2: error: ld returned 1 exit status\r\n> src/ATen/test/CMakeFiles/atest.dir/build.make:96: recipe for target 'src/ATen/test/atest' failed\r\n> make[2]: *** [src/ATen/test/atest] Error 1\r\n> CMakeFiles/Makefile2:243: recipe for target 'src/ATen/test/CMakeFiles/atest.dir/all' failed\r\n> make[1]: *** [src/ATen/test/CMakeFiles/atest.dir/all] Error 2\r\n> make[1]: *** Waiting for unfinished jobs....\r\n> [ 90%] Linking CXX executable broadcast_test\r\n> ../libATen.so.1: undefined reference to "", ""nnp_convolution_input_gradient'\r\n> ../libATen.so.1: undefined reference to "", ""nnp_convolution_output'\r\n> ../libATen.so.1: undefined reference to "", ""nnp_initialize'\r\n> collect2: error: ld returned 1 exit status\r\n> src/ATen/test/CMakeFiles/broadcast_test.dir/build.make:96: recipe for target 'src/ATen/test/broadcast_test' failed\r\n> make[2]: *** [src/ATen/test/broadcast_test] Error 1\r\n> CMakeFiles/Makefile2:354: recipe for target 'src/ATen/test/CMakeFiles/broadcast_test.dir/all' failed\r\n> make[1]: *** [src/ATen/test/CMakeFiles/broadcast_test.dir/all] Error 2\r\n> [ 90%] Linking CXX executable basic\r\n> ../libATen.so.1: undefined reference to "", ""nnp_convolution_input_gradient'\r\n> ../libATen.so.1: undefined reference to "", ""nnp_convolution_output'\r\n> ../libATen.so.1: undefined reference to "", ""nnp_initialize'\r\n> collect2: error: ld returned 1 exit status\r\n> src/ATen/test/CMakeFiles/basic.dir/build.make:96: recipe for target 'src/ATen/test/basic' failed\r\n> make[2]: *** [src/ATen/test/basic] Error 1\r\n> CMakeFiles/Makefile2:317: recipe for target 'src/ATen/test/CMakeFiles/basic.dir/all' failed\r\n> make[1]: *** [src/ATen/test/CMakeFiles/basic.dir/all] Error 2\r\n> [ 91%] Linking CXX executable native_test\r\n> ../libATen.so.1: undefined reference to "", ""nnp_convolution_input_gradient'\r\n> ../libATen.so.1: undefined reference to "", ""nnp_convolution_output'\r\n> ../libATen.so.1: undefined reference to "", ""nnp_initialize'\r\n> collect2: error: ld returned 1 exit status\r\n> src/ATen/test/CMakeFiles/native_test.dir/build.make:96: recipe for target 'src/ATen/test/native_test' failed\r\n> make[2]: *** [src/ATen/test/native_test] Error 1\r\n> CMakeFiles/Makefile2:280: recipe for target 'src/ATen/test/CMakeFiles/native_test.dir/all' failed\r\n> make[1]: *** [src/ATen/test/CMakeFiles/native_test.dir/all] Error 2\r\n> Makefile:127: recipe for target 'all' failed\r\n> make: *** [all] Error 2\r\n"", '']",0,0
653,pytorch,16990,closed,Unsafe .data in fbgemm integration,"https://github.com/pytorch/pytorch/blob/4292d13240e23a4a343b4ccb153214ab11c8d255/aten/src/ATen/native/QuantizedLinear.cpp#L39-L40

Here, e.g., if  isn't contiguous, the temporary tensor  can be freed and making the pointers invalid.

cc @jamesr66a ",,[],[],"['input', 'input.contiguous()']",0,0
654,pytorch,6287,closed,No .toDevice for ATen Tensors,"As title says, might be nice to be able to put ATen Tensors on other GPUs easily in C++.",,['There is now `to(Device)`: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/templates/Tensor.h#L93'],[],[],0,0
655,pytorch,3214,closed,Cannot directly use lr_scheduler in current pip-install package,"When I install pytorch using pip, I found lr_scheduler.py is in , but this line is missing:
https://github.com/pytorch/pytorch/blob/d89d9d74bdd0b64c119980003aa29195a61f93a9/torch/optim/__init__.py#L18
Thus user should manually add  to use torch.optim.lr_scheduler.
I hope it will be updated in the next version.",,"[""I also installed PyTorch 0.2.0 form Anaconda and `torch.optim.lr_scheduler` isn't available (Though according to release notes and documentation it should be there).\r\n\r\nI typed `torch.__version__` and the result was `0.2.0_0`.\r\n\r\nThank You."", ""It's because this code was added after the package was released. It will be there if you update."", 'Hi,\r\nUpdate how?\r\n\r\nThey only updated it today as answer for a request:\r\n\r\nhttps://github.com/ContinuumIO/anaconda-issues/issues/7410', ""Don't use the default conda channel. It has very old version of PyTorch only. Install with `conda install -c soumith pytorch`, or even better see the instructions in README."", 'They updated the default one today to 0.2.0_0 (Using `torch.__version__` as I wrote above) as an answer to the issue I opened.\r\nOn other computer I used `soumith` channel I got the exact same version.\r\n\r\nThe [Release page of GitHub talks about version 0.2.0](https://github.com/pytorch/pytorch/releases/tag/v0.2.0) as well.', 'I think 0.2.0_4 is the newest one. Also, 0.3 is coming out this week.', 'OK, I will wait 0.3 is updated.\r\nBut you see the problem in having Release Notes for version 0.2.0 stating on features while it is actually implemented only in 0.2.0_4.', ""Hi, I am using pytorch 0.2.0_4, but I cannot use lr_scheduler still.\r\n```python\r\nPython 3.5.4 |Anaconda, Inc.| (default, Oct  5 2017, 08:00:22) \r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 6.1.0 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: import torch\r\n\r\nIn [2]: torch.__version__\r\nOut[2]: '0.2.0_4'\r\n\r\nIn [3]: torch.optim.lr_scheduler\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-3-a472c70f0769> in <module>()\r\n----> 1 torch.optim.lr_scheduler\r\n\r\nAttributeError: module 'torch.optim' has no attribute 'lr_scheduler'\r\n```"", 'Please update to 0.3']",[],"['optim/', 'from . import lr_scheduler']",0,0
656,pytorch,8330,closed,x.copy_(y) doesn't work with sparse x,"

There isn't really any reason this shouldn't work; internally we have all the pieces implemented already.",module: sparse,[],"['\r\n>>> i = torch.LongTensor([[0, 2], [1, 0], [1, 2]])\r\n>>> v = torch.FloatTensor([3,      4,      5    ])\r\n>>> x = torch.sparse.FloatTensor(i.t(), v, torch.Size([2,3]))\r\n>>> x2 = torch.sparse.FloatTensor(i.t(), v, torch.Size([2,3]))\r\n>>> x.copy_(x2)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\nRuntimeError: copy does not support torch.sparse.FloatTensor to torch.sparse.FloatTensor copy.\r\n']",[],0,0
657,pytorch,17489,closed,Windows CPU debug build fails at linking stage,"## üêõ Bug
Hi, 
I can't build Windows Debug CPU version, the build fails during linking. 
I am using the latest source code. At the end of post there are exact commands I execute.
I am really after Libtorch build, I do not care about Python, but because I do not understand the whole thing, I feel obliged to build all.

Thank you,
Slawek

1. First of all an observation, CMakeLists.txt contains this:
set (CMAKE_CXX_FLAGS_DEBUG ""${CMAKE_CXX_FLAGS_DEBUG} -fno-omit-frame-pointer -O0"")
set (CMAKE_LINKER_FLAGS_DEBUG ""${CMAKE_STATIC_LINKER_FLAGS_DEBUG} -fno-omit-frame-pointer -O0"")
which causes endless warnings of unknown options.

Compilation appears successful  (2000 files or so) but then failures happen during linking
2.  

3. And then, another failure

Now, I downloaded and copied python37_d.lib into C:\local\Anaconda3\envs\PytorchDebug\libs and python37_d.dll into C:\local\Anaconda3\envs\PytorchDebug, but it does not help.


## Environment
 - PyTorch Version: 1, latest code
 - OS: Windows 10
 - Build commands:
call c:\local\Anaconda3\scripts\activate PytorchDebug
rem Done before: conda install numpy pyyaml mkl mkl-include setuptools cmake cffi typing
rem Done before, in f:\progs2\PytorchDebug: git clone --recursive https://github.com/pytorch/pytorch
F:
cd progs2\PytorchDebug\pytorch
set ""VS150COMNTOOLS=C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Auxiliary\Build""
set CMAKE_GENERATOR=Visual Studio 15 2017 Win64
set DISTUTILS_USE_SDK=1
set NO_CUDA=1
set DEBUG=1
call ""%VS150COMNTOOLS%\vcvarsall.bat"" x64 -vcvars_ver=14.11
python setup.py install

 - Python version: 3.7
",module: windows,"['If you build for libtorch, then please do this before build.\r\n```cmd\r\n: replace \r\npython setup.py install\r\n: with\r\npython tools\\build_libtorch.py\r\n```', 'Thank you. \r\nUnfortunately (this time after compiling 1319 files) I still fail at the same place:\r\n```\r\n  Creating library lib\\test_api.lib and object lib\\test_api.exp\r\ndataloader.cpp.obj : error LNK2019: unresolved external symbol ""__declspec(dllimport) public: void __cdecl torch::data::samplers::DistributedSampler<class std::vector<unsigned __int64,class std::allocator<unsigned __int64> > >::set_epoch(unsigned __int64)"" (__imp_?set_epoch@?$DistributedSampler@V?$vector@_KV?$allocator@_K@std@@@std@@@samplers@data@torch@@QEAAX_K@Z) referenced in function ""private: virtual void __cdecl DataTest_CanSaveAndLoadDistributedRandomSampler_Test::TestBody(void)"" (?TestBody@DataTest_CanSaveAndLoadDistributedRandomSampler_Test@@EEAAXXZ)\r\ndataloader.cpp.obj : error LNK2019: unresolved external symbol ""__declspec(dllimport) public: unsigned __int64 __cdecl torch::data::samplers::DistributedSampler<class std::vector<unsigned __int64,class std::allocator<unsigned __int64> > >::epoch(void)const "" (__imp_?epoch@?$DistributedSampler@V?$vector@_KV?$allocator@_K@std@@@std@@@samplers@data@torch@@QEBA_KXZ) referenced in function ""private: virtual void __cdecl DataTest_CanSaveAndLoadDistributedRandomSampler_Test::TestBody(void)"" (?TestBody@DataTest_CanSaveAndLoadDistributedRandomSampler_Test@@EEAAXXZ)\r\nbin\\test_api.exe : fatal error LNK1120: 2 unresolved externals\r\nninja: build stopped: subcommand failed.\r\nTraceback (most recent call last):\r\n  File ""tools\\build_libtorch.py"", line 25, in <module>\r\n    build_python=False, rerun_cmake=True, build_dir=\'.\')\r\n  File ""F:\\progs2\\pytorchDebug\\pytorch\\tools\\build_pytorch_libs.py"", line 243, in build_caffe2\r\n    cwd=build_dir, env=my_env)\r\n  File ""c:\\local\\Anaconda3\\envs\\PytorchDebug\\lib\\subprocess.py"", line 347, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command \'[\'cmake\', \'--build\', \'.\', \'--target\', \'install\', \'--config\', \'Debug\', \'--\', \'-j\', \'11\']\' returned non-zero exit status 1.\r\n```\r\n', ""The workground here is to use `set BUILD_TEST=OFF`. I'll look into it when I have time."", 'Thank you! I managed to built it.', 'Congratulations! If you have time, would you please try if #17494 fixes your problem?', 'Hi,\r\nYes. With #17494 I can build without set BUILD_TEST=OFF\r\n\r\nThanks!']","['Creating library lib\\test_api.lib and object lib\\test_api.exp\r\ndataloader.cpp.obj : error LNK2019: unresolved external symbol ""__declspec(dllimport) public: void __cdecl torch::data::samplers::DistributedSampler<class std::vector<unsigned __int64,class std::allocator<unsigned __int64> > >::set_epoch(unsigned __int64)"" (__imp_?set_epoch@?$DistributedSampler@V?$vector@_KV?$allocator@_K@std@@@std@@@samplers@data@torch@@QEAAX_K@Z) referenced in function ""private: virtual void __cdecl DataTest_CanSaveAndLoadDistributedRandomSampler_Test::TestBody(void)"" (?TestBody@DataTest_CanSaveAndLoadDistributedRandomSampler_Test@@EEAAXXZ)\r\ndataloader.cpp.obj : error LNK2019: unresolved external symbol ""__declspec(dllimport) public: unsigned __int64 __cdecl torch::data::samplers::DistributedSampler<class std::vector<unsigned __int64,class std::allocator<unsigned __int64> > >::epoch(void)const "" (__imp_?epoch@?$DistributedSampler@V?$vector@_KV?$allocator@_K@std@@@std@@@samplers@data@torch@@QEBA_KXZ) referenced in function ""private: virtual void __cdecl DataTest_CanSaveAndLoadDistributedRandomSampler_Test::TestBody(void)"" (?TestBody@DataTest_CanSaveAndLoadDistributedRandomSampler_Test@@EEAAXXZ)\r\nbin\\test_api.exe : fatal error LNK1120: 2 unresolved externals\r\n[2/3] Linking CXX shared library bin\\torch_python.dll\r\nFAILED: bin/torch_python.dll lib/torch_python.lib\r\n', 'LINK: command ""C:\\PROGRA~2\\MIB055~1\\2017\\COMMUN~1\\VC\\Tools\\MSVC\\1411~1.255\\bin\\HostX64\\x64\\link.exe /nologo caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\DataLoader.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\Device.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\Dtype.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\DynamicTypes.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\Exceptions.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\TypeInfo.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\Generator.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\Layout.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\Module.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\PtrWrapper.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\Size.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\Storage.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\api\\src\\python\\init.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\autograd\\functions\\init.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\autograd\\generated\\python_functions.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\autograd\\generated\\python_nn_functions.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\autograd\\generated\\python_torch_functions.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\autograd\\generated\\python_variable_methods.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\autograd\\init.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\autograd\\python_anomaly_mode.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\autograd\\python_cpp_function.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\autograd\\python_engine.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\autograd\\python_function.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\autograd\\python_hook.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\autograd\\python_legacy_variable.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\autograd\\python_variable.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\autograd\\python_variable_indexing.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\byte_order.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\jit\\batched\\BatchTensor.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\jit\\init.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\jit\\passes\\onnx.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\jit\\passes\\onnx\\fixup_onnx_loop.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\jit\\passes\\onnx\\prepare_division_for_onnx.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\jit\\passes\\onnx\\peephole.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\jit\\passes\\to_batch.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\jit\\python_arg_flatten.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\jit\\python_interpreter.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\jit\\python_ir.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\jit\\python_tracer.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\jit\\script\\init.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\jit\\script\\lexer.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\jit\\script\\module.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\jit\\script\\python_tree_views.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\multiprocessing\\init.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\nn\\THNN.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\onnx\\init.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\serialization.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\tensor\\python_tensor.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\utils.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\utils\\cuda_lazy_init.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\utils\\invalid_arguments.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\utils\\object_ptr.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\utils\\python_arg_parser.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\utils\\tensor_apply.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\utils\\tensor_dtypes.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\utils\\tensor_flatten.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\utils\\tensor_layouts.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\utils\\tensor_list.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\utils\\tensor_new.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\utils\\tensor_numpy.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\utils\\tensor_types.cpp.obj caffe2\\torch\\CMakeFiles\\torch_python.dir\\csrc\\utils\\tuple_parser.cpp.obj /out:bin\\torch_python.dll /implib:lib\\torch_python.lib /pdb:bin\\torch_python.pdb /dll /version:0.0 /debug /INCREMENTAL:NO /NODEFAULTLIB:LIBCMT.LIB /DEBUG:FULL lib\\torch.lib lib\\shm.lib C:\\local\\Anaconda3\\envs\\PytorchDebug\\libs\\python37.lib lib\\onnx.lib lib\\onnx_proto.lib -WHOLEARCHIVE:F:/progs2/pytorchDebug/pytorch/build/lib/onnx.lib lib\\caffe2.lib lib\\libprotobufd.lib C:\\Program Files (x86)\\IntelSWTools\\compilers_and_libraries\\windows\\mkl\\lib\\intel64_win\\mkl_intel_lp64.lib C:\\Program Files (x86)\\IntelSWTools\\compilers_and_libraries\\windows\\mkl\\lib\\intel64_win\\mkl_intel_thread.lib C:\\Program Files (x86)\\IntelSWTools\\compilers_and_libraries\\windows\\mkl\\lib\\intel64_win\\mkl_core.lib C:\\Program Files (x86)\\IntelSWTools\\compilers_and_libraries\\windows\\compiler\\lib\\intel64_win\\libiomp5md.lib lib\\c10.lib kernel32.lib user32.lib gdi32.lib winspool.lib shell32.lib ole32.lib oleaut32.lib uuid.lib comdlg32.lib advapi32.lib /MANIFEST /MANIFESTFILE:bin\\torch_python.dll.manifest"" failed (exit code 1104) with the following output:\r\nLINK : fatal error LNK1104: cannot open file \'python37_d.lib\'\r\nninja: build stopped: subcommand failed.\r\nTraceback (most recent call last):\r\n  File ""setup.py"", line 709, in <module>\r\n    build_deps()\r\n  File ""setup.py"", line 281, in build_deps\r\n    build_dir=\'build\')\r\n  File ""F:\\progs2\\pytorchDebug\\pytorch\\tools\\build_pytorch_libs.py"", line 243, in build_caffe2\r\n    cwd=build_dir, env=my_env)\r\n  File ""c:\\local\\Anaconda3\\envs\\PytorchDebug\\lib\\subprocess.py"", line 347, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command \'[\'cmake\', \'--build\', \'.\', \'--target\', \'install\', \'--config\', \'Debug\', \'--\', \'-j\', \'11\']\' returned non-zero exit status 1.\r\n']",[],0,0
658,pytorch,28347,closed,arange sometimes changes dimensionality of output tensor,"## üêõ Bug

Using  with an  target Tensor object, it will sometimes change the dimensionality of the target Tensor.  I'm creating a (1,N) tensor, and sometimes calling  changes it to a (N+1) vector.  Same on cuda or cpu.


## To Reproduce



This will output:



## Expected behavior

 should not change the shape of the target Tensor.

## Environment

PyTorch 1.3, Ubuntu 18.04LTS, py 3.6.9 under Anaconda.  In detail (from ):



## Additional context

I started doing it this way (out= Tensor) because I wanted to make sure the whole thing to happen on GPU, and that was the first thing I figured out.  I see another way to do this now --  which works.  But this seems like a bug.


cc @ezyang @gchanan @zou3519 @jerryzh168",high priority module: safe resize triaged,"['Thanks for the report. Does this always happen at `N=50`?', 'This also happens at `N=98, 103, 107, 196`. I can work on a fix.', 'The description of the issue isn\'t quite right.\r\n\r\nThis happens when the computed number of elements of the output doesn\'t match the number of elements of the input.  In the cases you found, that happens because floating point math isn\'t exact.\r\n\r\nTry it in python3:\r\n```\r\n>>> math.ceil((1 - -1)/(2/49))\r\n50\r\n```\r\n\r\nThe behavior for `arange(..., out=)` is to resize the input to the computed shape (which is always 1-dimensional) when the number of elements doesn\'t match.\r\n\r\nThis behaves differently from most torch functions in that is ""friendlier"" in that it will accept a shape that gets the number of elements correct instead of the exact shape.\r\n\r\nBut when you  combine these things, the behavior is surprising.\r\n\r\nThere are a number of things we can do:\r\n1) Always return a 1-d shape\r\n2) Try to retain the dimensionality of the input (note: this is not possible in general, but we could do things like support shapes of the form `(1, ..., 1, x, 1, ..., 1)` where `x != 1`.\r\n3) Give a hard error on resize.', ""Also, at a minimum, we should document the behavior of arange -- it behaves differently than other functions but isn't documented as such."", 'A short term improvement would be to warn if the output tensor has more than 0 elements but not the correct number of elements.', ""Yeah, the curse of being friendly is the surprises when the system is helpful in unexpected ways.  It's always a trade-off.\r\n\r\nA warning certainly would have saved me a chunk of debugging.  But IMHO a hard error on re-size makes sense, because when I supply something to out= I expect the same thing to come back more or less.  But y'all know much better than I do how to make it consistent with the rest of the system, which is probably the most important thing.\r\n\r\nThanks!\r\n"", ""(Sorry didn't mean to close - mouse slipped.)"", 'I am working on a PR.', '@prasunanand I laid out a number of options -- which are you taking?', 'I would like to go with ""Give a hard error on resize.""', 'high priority to at least warn.']","['python\r\nimport torch\r\n  \r\nfor N in range(45,100):\r\n    print(f""Trying {N}..."")\r\n    line = torch.zeros(size=(1,N))\r\n    print(f""  Before arange shape is {line.shape}"")\r\n    assert len(line.shape) == 2\r\n    torch.arange(-1, 1, 2/N, dtype=torch.float32, out=line)\r\n    print(f""  After arange shape is {line.shape}"")\r\n    assert len(line.shape) == 2\r\n', '\r\nTrying 45...\r\n  Before arange shape is torch.Size([1, 45])\r\n  After arange shape is torch.Size([1, 45])\r\nTrying 46...\r\n  Before arange shape is torch.Size([1, 46])\r\n  After arange shape is torch.Size([1, 46])\r\nTrying 47...\r\n  Before arange shape is torch.Size([1, 47])\r\n  After arange shape is torch.Size([1, 47])\r\nTrying 48...\r\n  Before arange shape is torch.Size([1, 48])\r\n  After arange shape is torch.Size([1, 48])\r\nTrying 49...\r\n  Before arange shape is torch.Size([1, 49])\r\n  After arange shape is torch.Size([50])\r\nTraceback (most recent call last):\r\n  File ""shapebug.py"", line 10, in <module>\r\n    assert len(line.shape) == 2\r\nAssertionError\r\n', '\r\nCollecting environment information...\r\nPyTorch version: 1.3.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.1.85\r\nGPU models and configuration: GPU 0: GeForce GTX 1070 with Max-Q Design\r\nNvidia driver version: 418.56\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] facenet-pytorch==0.3.1\r\n[pip] numpy==1.17.2\r\n[pip] torch==1.3.0\r\n[pip] torchvision==0.4.1a0+d94043a\r\n[conda] blas                      1.0                         mkl  \r\n[conda] facenet-pytorch           0.3.1                    pypi_0    pypi\r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-service               2.3.0            py36he904b0f_0  \r\n[conda] mkl_fft                   1.0.14           py36ha843d7b_0  \r\n[conda] mkl_random                1.1.0            py36hd6b4f25_0  \r\n[conda] pytorch                   1.3.0           py3.6_cuda10.0.130_cudnn7.6.3_0    pytorch\r\n[conda] torchvision               0.4.1                py36_cu100    pytorch\r\n']","['torch.arange', 'out=', 'arange', 'torch.arange', 'collect_env.py', ""line=torch.arange(device='cuda')""]",0,0
659,pytorch,2344,closed,Does CosineEmbeddingLoss support CUDA tensors?,"Noticed this as I tried to use the CosineEmbeddingLoss with a model copied to the GPU.



My default assumption is that I'm using cuda() wrong in some way. Thoughts?",,"['try this instead:\r\n```\r\ny = Variable(torch.cuda.FloatTensor([1.0] * input1.size()[0]))\r\n```\r\n\r\nBTW, questions like this are better discussed on https://discuss.pytorch.org/.']","['\r\nimport torch\r\nfrom torch.autograd import Variable\r\nfrom torch.nn._functions.loss import CosineEmbeddingLoss\r\n\r\ninput1 = Variable(torch.rand(5,10))\r\ninput1 = input1.cuda()\r\n\r\ninput2 = Variable(torch.rand(5,10))\r\ninput2 = input2.cuda()\r\n\r\ny = Variable(torch.FloatTensor([1.0] * input1.size()[0]))\r\n\r\nloss = CosineEmbeddingLoss()\r\nloss(input1, input2, y)\r\n\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/usr/local/lib/python3.5/dist-packages/torch/nn/_functions/loss.py"", line 41, in forward\r\n    torch.eq(y, -1, out=_idx)\r\nTypeError: torch.eq received an invalid combination of arguments - got (torch.FloatTensor, int, out=torch.cuda.ByteTensor), but expected one of:\r\n * (torch.FloatTensor tensor, float value, *, torch.FloatTensor out)\r\n      didn\'t match because some of the arguments have invalid types: (torch.FloatTensor, int, out=torch.cuda.ByteTensor)\r\n * (torch.FloatTensor tensor, torch.FloatTensor other, *, torch.FloatTensor out)\r\n      didn\'t match because some of the arguments have invalid types: (torch.FloatTensor, int, out=torch.cuda.ByteTensor)\r\n * (torch.FloatTensor tensor, float value, *, torch.ByteTensor out)\r\n      didn\'t match because some of the arguments have invalid types: (torch.FloatTensor, int, out=torch.cuda.ByteTensor)\r\n * (torch.FloatTensor tensor, torch.FloatTensor other, *, torch.ByteTensor out)\r\n      didn\'t match because some of the arguments have invalid types: (torch.FloatTensor, int, out=torch.cuda.ByteTensor)\r\n']",[],0,0
660,pytorch,3491,closed,RuntimeError: CUDA error (3): initialization error ,"
I run these codes using pytorch 0.2.0, python 3.5 with error


But using cpu( ) woks fine",,"['as mentioned in http://pytorch.org/docs/master/notes/multiprocessing.html#sharing-cuda-tensors\r\n\r\ninsert this to the top of your script\r\n\r\n```\r\nimport torch\r\ntorch.multiprocessing.set_start_method(""spawn"")\r\n```', 'Thanks\r\nBut it also returned errors when I Inserted it \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""<string>"", line 1, in <module>\r\n  File ""/usr/lib/python3.5/multiprocessing/spawn.py"", line 106, in spawn_main\r\n    exitcode = _main(fd)\r\n  File ""/usr/lib/python3.5/multiprocessing/spawn.py"", line 115, in _main\r\n    prepare(preparation_data)\r\n  File ""/usr/lib/python3.5/multiprocessing/spawn.py"", line 226, in prepare\r\n    _fixup_main_from_path(data[\'init_main_from_path\'])\r\n  File ""/usr/lib/python3.5/multiprocessing/spawn.py"", line 278, in _fixup_main_from_path\r\n    run_name=""__mp_main__"")\r\n  File ""/usr/lib/python3.5/runpy.py"", line 254, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File ""/usr/lib/python3.5/runpy.py"", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File ""/usr/lib/python3.5/runpy.py"", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File ""/home/mingyi/Ê°åÈù¢/test/test.py"", line 2, in <module>\r\n    torch.multiprocessing.set_start_method(""spawn"")\r\n  File ""/usr/lib/python3.5/multiprocessing/context.py"", line 231, in set_start_method\r\n    raise RuntimeError(\'context has already been set\')\r\nRuntimeError: context has already been set\r\nTraceback (most recent call last):\r\n  File ""<string>"", line 1, in <module>\r\nTraceback (most recent call last):\r\n  File ""<string>"", line 1, in <module>\r\n  File ""/usr/lib/python3.5/multiprocessing/spawn.py"", line 106, in spawn_main\r\n  File ""/usr/lib/python3.5/multiprocessing/spawn.py"", line 106, in spawn_main\r\n    exitcode = _main(fd)\r\n    exitcode = _main(fd)\r\n  File ""/usr/lib/python3.5/multiprocessing/spawn.py"", line 115, in _main\r\n  File ""/usr/lib/python3.5/multiprocessing/spawn.py"", line 115, in _main\r\n    prepare(preparation_data)\r\n    prepare(preparation_data)\r\n  File ""/usr/lib/python3.5/multiprocessing/spawn.py"", line 226, in prepare\r\n  File ""/usr/lib/python3.5/multiprocessing/spawn.py"", line 226, in prepare\r\n    _fixup_main_from_path(data[\'init_main_from_path\'])\r\n  File ""/usr/lib/python3.5/multiprocessing/spawn.py"", line 278, in _fixup_main_from_path\r\n    _fixup_main_from_path(data[\'init_main_from_path\'])\r\n  File ""/usr/lib/python3.5/multiprocessing/spawn.py"", line 278, in _fixup_main_from_path\r\n    run_name=""__mp_main__"")\r\n  File ""/usr/lib/python3.5/runpy.py"", line 254, in run_path\r\n    run_name=""__mp_main__"")\r\n  File ""/usr/lib/python3.5/runpy.py"", line 254, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File ""/usr/lib/python3.5/runpy.py"", line 96, in _run_module_code\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File ""/usr/lib/python3.5/runpy.py"", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File ""/usr/lib/python3.5/runpy.py"", line 85, in _run_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File ""/usr/lib/python3.5/runpy.py"", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File ""/home/mingyi/Ê°åÈù¢/test/test.py"", line 2, in <module>\r\n    exec(code, run_globals)\r\n  File ""/home/mingyi/Ê°åÈù¢/test/test.py"", line 2, in <module>\r\n    torch.multiprocessing.set_start_method(""spawn"")\r\n  File ""/usr/lib/python3.5/multiprocessing/context.py"", line 231, in set_start_method\r\n    torch.multiprocessing.set_start_method(""spawn"")\r\n  File ""/usr/lib/python3.5/multiprocessing/context.py"", line 231, in set_start_method\r\n    raise RuntimeError(\'context has already been set\')\r\n    raise RuntimeError(\'context has already been set\')\r\nRuntimeError: context has already been set\r\nRuntimeError: context has already been set\r\n\r\n    0     0     0\r\n    0     0     0\r\n    0     0     0\r\n    0     0     0\r\n    0     0     0\r\n    0     0     0\r\n    0     0     0\r\n    0     0     0\r\n    0     0     0\r\n[torch.FloatTensor of size 9x3]\r\n\r\n\r\nProcess finished with exit code 0\r\n\r\n```', ""@soumith My work has a great deal of dependency on 'fork'ing stuff. I wonder if there is a way to get around this and use `fork` instead of `spawn`? ""]","['\r\nimport torch\r\nimport torch.multiprocessing as mp\r\n\r\ndef subprocesses(A, B, D, i, j, size):\r\n    D[(j * size):((j + 1) * size), i] = torch.mul(B[:, i], A[j, i])\r\n\r\ndef task(A, B):\r\n\r\n    size1 = A.shape\r\n    size2 = B.shape\r\n    D = torch.zeros([size1[0]*size2[0], size1[1]]).cuda()\r\n    D.share_memory_()\r\n    \r\n    for i in range(1):\r\n        processes = []\r\n        for j in range(size1[0]):  \r\n            p = mp.Process(target=subprocesses, args=(A, B, D, i, j, size2[0]))\r\n            p.start()\r\n            processes.append(p)\r\n        for p in processes:\r\n            p.join()\r\n\r\n    return D\r\n\r\nA = torch.rand(3, 3).cuda()\r\nB = torch.rand(3, 3).cuda()\r\nC = task(A,B)\r\nprint(C)\r\n', '\r\nProcess Process-1:\r\nTraceback (most recent call last):\r\nProcess Process-2:\r\n  File ""/usr/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap\r\n    self.run()\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python3.5/multiprocessing/process.py"", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File ""/usr/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap\r\n    self.run()\r\n  File ""<ipython-input-2-cbb040477c37>"", line 3, in subprocesses\r\n    D[(j * size):((j + 1) * size), i] = torch.mul(B[:, i], A[j, i])\r\n  File ""/usr/lib/python3.5/multiprocessing/process.py"", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\nRuntimeError: CUDA error (3): initialization error\r\n  File ""<ipython-input-2-cbb040477c37>"", line 3, in subprocesses\r\n    D[(j * size):((j + 1) * size), i] = torch.mul(B[:, i], A[j, i])\r\nRuntimeError: CUDA error (3): initialization error\r\nProcess Process-3:\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap\r\n    self.run()\r\n  File ""/usr/lib/python3.5/multiprocessing/process.py"", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File ""<ipython-input-2-cbb040477c37>"", line 3, in subprocesses\r\n    D[(j * size):((j + 1) * size), i] = torch.mul(B[:, i], A[j, i])\r\nRuntimeError: CUDA error (3): initialization error\r\n']",[],0,0
661,pytorch,24612,closed,Migrate `polygamma` and `polygamma_` from the TH to Aten (CUDA),"Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review.",better-engineering module: porting triaged,[],[],[],0,0
662,pytorch,13,closed,Infinite recursion when indexing Variable,"


",,['never mind\n'],"[' python\nimport torch\nfrom torch.autograd import Variable\nv = Variable(torch.ones(5, 5))\nv[0]\n', '\n...\nRecursionError: maximum recursion depth exceeded\n']",[],0,0
663,pytorch,26834,closed,FakeQuantize params not rendering correctly,"the parameters for the FakeQuantize function aren't showing up right. 
<img width=""897"" alt=""Screen Shot 2019-09-25 at 1 12 06 PM"" src=""https://user-images.githubusercontent.com/45861273/65636145-99353400-df96-11e9-9e0f-4c6b79b7ac91.png"">
",module: docs quantization_release_1.3 triaged,"['I think I have a fix for this. Will put in a PR.  Here is the current status in my working repo. \r\n\r\n<img width=""1725"" alt=""fake_quantize_working"" src=""https://user-images.githubusercontent.com/45861273/65708591-c4c22800-e043-11e9-88ed-aa8dc22ed1c9.png"">\r\n', 'confirming this in the stable doc. ']",[],[],0,0
664,pytorch,6131,closed,Build picks up wrong copy of mkl when multiple available,"@aosokin reports from https://github.com/pytorch/pytorch/issues/6068#issuecomment-377226963

There might be another related issue with dependencies.
When doing import torch from an anaconda env pytorch uses mkl (libmkl_gf_lp64.so) installed in anaconda root.
Consequently, if the root and the env have different versions (in my case the root had 2018.0.1 and the env had 2018.0.2-1) I was getting this error:



Updating mkl in anaconda root to 2018.0.2-1 solved the problem.",,"[""I'm having this problem also.  I'm unable to upgrade my anaconda mkl because the newest is incompatible with several other conda packages I need.  Is there another workaround?"", 'I found that setting the `MKLROOT` environment variable to point to the local conda environment directory fixes the issue.\r\n\r\n```\r\nMKLROOT=""${HOME}/.conda/envs/${ENV_NAME}/"" \\\r\nMAX_JOBS=12 \\\r\npython setup.py install\r\n```', ""@davidbau I'm having the same issue on Ubuntu 16.04. Where did you right that MKLROOT line? In file? \r\n\r\nThanks"", ""I'm also running into this issue even after specifying `MKLROOT` as my Conda environment directory. Might I be missing something else?\r\n\r\nUpdate:\r\nI was able to get things working by just updating `mkl` in the base Conda environment. This probably won't work for everyone, but this was fine for me."", 'I downgraded mkl to 2018.0.1 by using `conda install mkl=2018.0.1` and it worked. ']","['\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/aosokin/local/software/anaconda3/envs/pytorch-master/lib/python3.6/site-packages/torch/__init__.py"", line 78, in <module>\r\n    from torch._C import *\r\nImportError: /home/aosokin/local/software/anaconda3/lib/libmkl_gf_lp64.so: undefined symbol: mkl_lapack_ao_zgetrf\r\n']",[],0,0
665,pytorch,22549,closed,How to programmatically check PyTorch version,"## üìö Documentation

[Minor minor detail]
In the reproducibility section of the docs or in the FAQ, I would add a simple subsection/snippet of code to show how to programmatically check the running version of PyTorch. 
This can also encourage users to take into account heterogeneity of PyTorch versions in their code.

By the way, a simple regex on  is enough (this assuming version numbering will not change).

",enhancement module: docs triaged,['Use `packaging.version.parse` https://stackoverflow.com/a/11887885'],"[""python\r\nimport torch\r\nimport re\r\nif int(re.search(r'([\\d.]+)', torch.__version__).group(1).replace('.', '')) < 100:\r\n    raise ImportError('Your PyTorch version is not supported. '\r\n                      'Please download and install PyTorch 1.x')\r\n""]",['torch.__version__'],0,0
666,pytorch,19366,closed,[ONNX] RNN activation function exported to ONNX with wrong string case,"## üêõ Bug

If we export  module to ONNX, the  attribute is written is all lower case. Several backends assume these strings to be case-sensitive, where this can be a blocking issue during runtime. There is an open issue (https://github.com/onnx/onnx/issues/1934) in the ONNX repo to make this this explicit in the spec. PyTorch's ONNX exporter should also consider writing strings with correct case in  attribute. 

## To Reproduce

Steps to reproduce the behavior:

1. Create a simple model with  module.
1. Export this model to ONNX using 
1. Check the  attribute for the case of the string. As an example, the string will be 'tanh', where ideally it should be 'Tanh'.

## Expected behavior

The  attribute value should match the case specified in ONNX spec. 

## Environment
This can be seen in all environments.",module: onnx triaged,"[""cc: @houseroad This is a minor issue, but one that's blocking some models from running in backends such as Onnxruntime. \r\n\r\nWill submit a PR soon."", 'https://github.com/pytorch/pytorch/pull/19368 is in to fix this. ', 'Close it, since the fix is merged.']",[],"['nn.RNN', 'activations', 'activations', 'nn.RNN', 'torch.onnx.export', 'activations', 'activations']",0,0
667,pytorch,685,closed,detach not working properly for stochastic variables,"I'm not able to reproduce a simple example for now, will try to make one later. But basically I'm facing this issue in my code:



I have a variable , and doing  does not solve the issue. However,  solves it, which seems weird to me..",high priority,"[""Right, I'll fix that soon. Sorry."", ""It's because right now `.detach()` doesn't remove the reference to the creating graph, even though it should."", 'Fixed in #752.']",[],"['RuntimeError: differentiating stochastic functions requires providing a reward', 'sentences', 'sentences = sentences.detach()', 'sentences = Variable(sentences.data)']",0,0
668,pytorch,3388,closed, recipe for target 'CMakeFiles/THD.dir/base/TensorDescriptor.cpp.o' failed,,,"['Please post a full build log to a gist', ""for some reason you have `/home/ruikun/anaconda/include/THPP/` which doesn't look right.\r\nPlease `rm -rf /home/ruikun/anaconda/include/THPP/`, and/or cleanup existing installs of pytorch with the commands:\r\n\r\n```\r\nconda uninstall -y pytorch\r\npip uninstall -y torch\r\npip uninstall -y torch\r\n```""]","[""\r\n   ^\r\n/home/ruikun/anaconda/include/THPP/tensors/THTensor.hpp:38:3: note:   candidate expects 0 arguments, 1 provided\r\n/home/ruikun/anaconda/include/THPP/tensors/THTensor.hpp:27:8: note: candidate: constexpr thpp::THTensor<signed char>::THTensor(const thpp::THTensor<signed char>&)\r\n struct THTensor : public interface_traits<real>::tensor_interface_type {\r\n        ^\r\n/home/ruikun/anaconda/include/THPP/tensors/THTensor.hpp:27:8: note:   no known conversion for argument 1 from ‚ÄòTHCharTensor*‚Äô to ‚Äòconst thpp::THTensor<signed char>&‚Äô\r\nCMakeFiles/THD.dir/build.make:158: recipe for target 'CMakeFiles/THD.dir/base/TensorDescriptor.cpp.o' failed\r\nmake[2]: *** [CMakeFiles/THD.dir/base/TensorDescriptor.cpp.o] Error 1\r\nmake[2]: *** Waiting for unfinished jobs....\r\nCMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/THD.dir/all' failed\r\nmake[1]: *** [CMakeFiles/THD.dir/all] Error 2\r\nMakefile:129: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\n""]",[],0,0
669,pytorch,23649,closed,Installation with non-root access,When I run  it need root access to install the final package. How can I install that in user home (I mean user packages for pip)?,module: build triaged,"['Have you tried `python setup.py install --user`?', 'please use https://discuss.pytorch.org for questions', 'Sorry for the delay. Thank you. --user fixed that.']",[],['python setup.py install'],0,0
670,pytorch,12241,closed,torch.sum() for sparse tensor,"- Add 
- Add 
- Add autograd support",module: sparse,"[""`sparse_sum` w/ autograd is implement-able with `t.values().sum()` after #11253 is merged.... sorry, I will merge it after finishing this patch I'm working on."", '@SsnL Awesome! I can assume `sparse_sum()` over dense dims already have autograd support for now :) ', 'yep, summing over sparse dims still needs some more work.', 'A minor naming question: what is the motivation for prefixing it `sparse_sum` (contrary to a polymorphic `sum`)? Are all operations that support sparse tensors going to be renamed this way? Thanks!', '@vadimkantorov Prefixing operators for SparseTensor with `sparse_` in general means during the backward gradients at zero positions will be zeroed out. This deviates from canonical behaviors of operators, hence a different name. But yes, we can also provide `sum(SparseTensor)` without autograd support. On the other hand, for those operators with sparse gradients during backward, e.g., `max(SparseTensor)`, we will name it without `sparse_` prefix.']",[],"['torch.sparse_sum(input, dim, keepdim=False, dtype=None) ‚Üí SparseTensor', 'torch.sparse_sum(input, dtype=None) ‚Üí Tensor']",0,0
671,pytorch,26922,closed,DistAutogradContainer should not expose references for DistAutogradContext,"More Context here: https://github.com/pytorch/pytorch/pull/25527/files#r328763171

DistAutogradContainer's API currently exposes mutable references to DistAutogradContext. This could lead to 'use after free' scenarios where DistAutogradContainer cleans up the context, but there is another thread that continues to use the reference. We should modify the API to ensure it returns shared_ptrs to the user to avoid any sort of ownership issues.

cc @ezyang @gchanan @zou3519 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528",high priority module: rpc triaged,[],[],[],0,0
672,pytorch,24711,closed,Migrate `hardtanh_backward` from the TH to Aten (CPU),"Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review.",better-engineering module: porting triaged,[],[],[],0,0
673,pytorch,29293,closed,Using tensor cores,"I would like to know if I build PyTorch on SM_70 or SM_75, will it automatically use tensor cores or I have to explicitly specify something during the compilation or before run.
Any comment?",,"['You will automatically use them when performing operations that make use of tensor cores.', 'Is there any special API in PyTorch for that? Where should I look for using those operations?', ""There is not a special API and, unfortunately, the cases where tensor cores are enabled are not very easy to describe. NVIDIA has some performance guides for Tensor Cores that can be useful references: https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9926-tensor-core-performance-the-ultimate-guide.pdf\r\n\r\nEssentially operations that internally perform GEMMs can use Tensor Cores on fp16 inputs when those inputs' sizes meet special criteria."", 'Thanks.']",[],[],0,0
674,pytorch,13806,closed,OMP: Error about doubly-linked OpenMP when loading CIFAR10 dataset,"I compile PyTorch from source (1.0.0a0+54e8623).
and when I attempt to load data I get this:


The crash occurs when I do this:


I'm using the CIFAR10 dataset here, the problem does not occur with MNIST.


The system is macOS 10.12.6. What could the problem be here?",,"['The error appears to come from `dataloader.py` at this location:\r\n```\r\n    def __next__(self):\r\n        if self.num_workers == 0:  # same-process loading\r\n            indices = next(self.sample_iter)  # may raise StopIteration\r\n            batch = self.collate_fn([self.dataset[i] for i in indices])   # line 615\r\n            if self.pin_memory:\r\n                batch = pin_memory_batch(batch)\r\n            return batch\r\n```\r\nwhile accessing the dataset by index.\r\n', ""A workaround seems to be\r\n```\r\nimport os\r\n\r\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\r\n```\r\nbut it would be better to find the root cause."", 'I fixed this error by removing the OpenMP installation from MacPorts and recompiling PyTorch.']","['\r\nOMP: Error #15: Initializing libomp.dylib, but found libiomp5.dylib already initialized.\r\nOMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://openmp.llvm.org/\r\nAbort trap: 6\r\n', '\r\ndataloader = DataLoader(self._dataset, batch_size=self._batch_size, pin_memory=True, shuffle=True, drop_last=True)\r\nnext(dataloader)\r\n']",[],0,0
675,pytorch,14057,closed,Unable to pickle torch dtype objects in Python 3.5,"## üêõ Bug

When pickling a  object, Python 3.5 reports an obscure error ""can't pickle int objects"".

## To Reproduce

Steps to reproduce the behavior:



## Expected behavior

In Python 3.6 one can pickle torch dtypes successfully.

## Environment

",high priority,"['cc: @gchanan is this going to be fixed? @ailzhang fixed it for device here https://github.com/pytorch/pytorch/pull/7713', 'Hi, here is another example:\r\n```python\r\nimport networkx as nx  # if this line is removed, it works !!\r\nimport torch\r\nimport pickle\r\nimport io\r\n\r\ndef _reconstruct_pickle(obj):\r\n    f = io.BytesIO()\r\n    pickle.dump(obj, f)\r\n    f.seek(0)\r\n    obj = pickle.load(f)\r\n    f.close()\r\n    return obj\r\n\r\ndef test():\r\n    x = torch.float32\r\n    new_bg = _reconstruct_pickle(x)\r\n\r\ntest()\r\n```\r\nIt raised error:\r\n```\r\nTraceback (most recent call last):\r\n  File ""t.py"", line 18, in <module>\r\n    test()\r\n  File ""t.py"", line 16, in test\r\n    new_bg = _reconstruct_pickle(x)\r\n  File ""t.py"", line 8, in _reconstruct_pickle\r\n    pickle.dump(obj, f)\r\nTypeError: can\'t pickle module objects\r\n```\r\n\r\nMy environment:\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Arch Linux\r\nGCC version: (GCC) 8.2.1 20181127\r\nCMake version: version 3.13.4\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: \r\nGPU 0: Quadro K2000\r\nGPU 1: Tesla K20c\r\n\r\nNvidia driver version: 415.27\r\ncuDNN version: /usr/lib/libcudnn.so.7.4.2\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.14.6\r\n[pip3] torch==1.0.1.post2\r\n[conda] Could not collect\r\n```\r\n', 'The example above seems to fail in all pickle protocols except 4 in Python 3.', ""Actually torch dtype object is already serializable. Closing....\r\n```\r\nIn [6]: b = copy.deepcopy(a)\r\n\r\nIn [7]: id(b)\r\nOut[7]: 139818768678472\r\n\r\nIn [8]: id(a)\r\nOut[8]: 139818768678472\r\n\r\nIn [9]: import pickle\r\n\r\nIn [10]: with open('/tmp/a', 'wb') as f:\r\n    ...:     pickle.dump(torch.float32, f)\r\n    ...:\r\n```\r\n"", 'Hmmm I will leave this open as the example script is failing for python 2.7. \r\n```\r\nimport torch\r\nimport pickle\r\nimport io\r\n\r\ndef _reconstruct_pickle(obj):\r\n    f = io.BytesIO()\r\n    pickle.dump(obj, f)\r\n    f.seek(0)\r\n    obj = pickle.load(f)\r\n    f.close()\r\n    return obj\r\n\r\ndef test():\r\n    x = torch.float32\r\n    new_bg = _reconstruct_pickle(x)\r\n\r\ntest()\r\n```', ""@ailzhang please see this example gived by @jermainewang: \r\nhttps://github.com/pytorch/pytorch/issues/14057#issuecomment-472179158\r\nhttps://github.com/dmlc/dgl/issues/438#issuecomment-472177987 . \r\nActually, when some library like 'networkx' is imported, torch.dtype will failed to be pickled even in python3."", '@jackroos I tried the example script with networkx in python3.6 and it worked without an issue. There might be some env difference here causing the problem. ', ""Yeah, it's also weird that not all the import causes this problem. For example, `import numpy` has no problem. Do you know how could we debug into this? so maybe we could provide more accurate reason."", ""Yea I spent more time on this and it might be related to the difference between import semantics in python2 and 3. (at least for the python2.7 failure that I can repro locally). \r\nWhat's happening now is python2 fails to find out `torch.float32` is in module `torch`. Basically it's trying to look for `torch.float32` as a string in module `__main__`, which what it should do is looking for `float32` in module `torch`. \r\nSimilar thing I saw when implementing `torch.hub` is python2 fails to handle dotted structure when parsing module structures in importing... Python3 handles this well. \r\nI think it could potentially be solved by setting `__module__` manually on torch.dtype to let python2 know the right module to look for. I'm trying to find out how to do it. I will let you know if this works or not. :D "", ""@jackroos @jermainewang Would you mind trying out the fix above and see if it fixes your issue? It's mainly for fixing python2.7 one that I reproed locally, but if you could get me a repro for the `networkx` with python3, happy to look into that as well. "", ""Hi @ailzhang , do you know where `torch.dtype` is defined? I don't know the exact place to put `__module__`."", '@jermainewang Yea could you checkout #18045 and build from source to see if it fixes your issue? I realized `__module__` is defined but python2.7 failed to parse it because our dtype name is a bit confusing. ', ""I'm closing this as #18045 has landed. Please feel free to reopen if it doesn't fix your issue. Thanks!""]","['python\r\nIn [1]: import torch\r\n\r\nIn [2]: import pickle\r\n\r\nIn [3]: with open(\'/tmp/a\', \'wb\') as f:\r\n   ...:     pickle.dump(torch.float32, f)\r\n   ...:     \r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-769b4901f38c> in <module>()\r\n      1 with open(\'/tmp/a\', \'wb\') as f:\r\n----> 2     pickle.dump(torch.float32, f)\r\n      3 \r\n\r\n~/anaconda3/envs/tmp/lib/python3.5/copyreg.py in _reduce_ex(self, proto)\r\n     63     else:\r\n     64         if base is self.__class__:\r\n---> 65             raise TypeError(""can\'t pickle %s objects"" % base.__name__)\r\n     66         state = base(self)\r\n     67     args = (self.__class__, base, state)\r\n\r\nTypeError: can\'t pickle int objects\r\n', '\r\nCollecting environment information...\r\nPyTorch version: 0.4.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Fedora release 29 (Twenty Nine)\r\nGCC version: (GCC) 8.2.1 20181011 (Red Hat 8.2.1-4)\r\nCMake version: version 3.12.1\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.148\r\nGPU models and configuration: GPU 0: GeForce GTX 1070\r\nNvidia driver version: 410.73\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.15.2)\r\n[pip] torch (0.4.1.post2)\r\n[conda] pytorch                   0.4.1           py35_py27__9.0.176_7.1.2_2    pytorch\r\n']",['torch.dtype'],0,0
676,pytorch,22616,closed,Error when loading a sparse tensor parameter from a state_dict in pytorch 1.1.0,"## üêõ Bug

In pytorch , if a  has a parameter which is a sparse tensor,  fails. For the mini snipped below it throws the following error:


On pytorch  there is no error.

## To Reproduce

Steps to reproduce the behavior:


## Expected behavior

The  loads the sparse tensor from the state dict.

## Environment



The virtual environment with torch  for which the above snipped throws **no** error:

",module: nn module: sparse triaged,"['Got the same issue. There is something wrong with the copy method of sparse tensors.\r\n\r\nA workaround could be to change the `param.copy_(input_param)` line to `self._parameters[name] = input_param.clone()` or `self._buffers[name] = input_param.clone()` depending on the parameter type in function `_load_from_state_dict` in file `torch/nn/modules/module.py`. The function would be like:\r\n\r\n```\r\n        def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\r\n                              missing_keys, unexpected_keys, error_msgs):\r\n        r""""""Copies parameters and buffers from :attr:`state_dict` into only\r\n        this module, but not its descendants. This is called on every submodule\r\n        in :meth:`~torch.nn.Module.load_state_dict`. Metadata saved for this\r\n        module in input :attr:`state_dict` is provided as :attr:`local_metadata`.\r\n        For state dicts without metadata, :attr:`local_metadata` is empty.\r\n        Subclasses can achieve class-specific backward compatible loading using\r\n        the version number at `local_metadata.get(""version"", None)`.\r\n\r\n        .. note::\r\n            :attr:`state_dict` is not the same object as the input\r\n            :attr:`state_dict` to :meth:`~torch.nn.Module.load_state_dict`. So\r\n            it can be modified.\r\n\r\n        Arguments:\r\n            state_dict (dict): a dict containing parameters and\r\n                persistent buffers.\r\n            prefix (str): the prefix for parameters and buffers used in this\r\n                module\r\n            local_metadata (dict): a dict containing the metadata for this module.\r\n                See\r\n            strict (bool): whether to strictly enforce that the keys in\r\n                :attr:`state_dict` with :attr:`prefix` match the names of\r\n                parameters and buffers in this module\r\n            missing_keys (list of str): if ``strict=True``, add missing keys to\r\n                this list\r\n            unexpected_keys (list of str): if ``strict=True``, add unexpected\r\n                keys to this list\r\n            error_msgs (list of str): error messages should be added to this\r\n                list, and will be reported together in\r\n                :meth:`~torch.nn.Module.load_state_dict`\r\n        """"""\r\n        for hook in self._load_state_dict_pre_hooks.values():\r\n            hook(state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\r\n\r\n        local_name_params = itertools.chain(self._parameters.items(), self._buffers.items())\r\n        local_state = {k: v.data for k, v in local_name_params if v is not None}\r\n\r\n        for name, param in local_state.items():\r\n            key = prefix + name\r\n            if key in state_dict:\r\n                input_param = state_dict[key]\r\n\r\n                # Backward compatibility: loading 1-dim tensor from 0.3.* to version 0.4+\r\n                if len(param.shape) == 0 and len(input_param.shape) == 1:\r\n                    input_param = input_param[0]\r\n\r\n                if input_param.shape != param.shape:\r\n                    # local shape should match the one in checkpoint\r\n                    error_msgs.append(\'size mismatch for {}: copying a param with shape {} from checkpoint, \'\r\n                                      \'the shape in current model is {}.\'\r\n                                      .format(key, input_param.shape, param.shape))\r\n                    continue\r\n\r\n                if isinstance(input_param, Parameter):\r\n                    # backwards compatibility for serialized parameters\r\n                    input_param = input_param.data\r\n                try:\r\n                    # param.copy_(input_param)\r\n                    if name in self._parameters.keys():\r\n                        self._parameters[name] = input_param.clone()\r\n                    elif name in self._buffers.keys():\r\n                        self._buffers[name] = input_param.clone()\r\n                except Exception:\r\n                    error_msgs.append(\'While copying the parameter named ""{}"", \'\r\n                                      \'whose dimensions in the model are {} and \'\r\n                                      \'whose dimensions in the checkpoint are {}.\'\r\n                                      .format(key, param.size(), input_param.size()))\r\n            elif strict:\r\n                missing_keys.append(key)\r\n\r\n        if strict:\r\n            for key in state_dict.keys():\r\n                if key.startswith(prefix):\r\n                    input_name = key[len(prefix):]\r\n                    input_name = input_name.split(\'.\', 1)[0]  # get the name of param/buffer/child\r\n                    if input_name not in self._modules and input_name not in local_state:\r\n                        unexpected_keys.append(key)\r\n```\r\n\r\nI\'m not an expert. I hope this helps.', 'I looked a little bit into this bug with pyTorch 1.4.1 and found that the bus in @danielzuegner test case it was related to https://github.com/pytorch/pytorch/pull/13827 \r\n\r\n![image](https://user-images.githubusercontent.com/23301446/77935161-60fa0f80-72b1-11ea-917b-f4b089f296ea.png)\r\n\r\nAs it seems, it has been resolved in the meantime by https://github.com/pytorch/pytorch/pull/31482\r\n\r\nSo it should be working again from 1.5.0 onwards.', 'I can confirm that this is indeed fixed with PyTorch 1.5.0.', 'Fixed, so closing. Thanks all!']","['\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-2-ca845ac28325> in <module>\r\n     10 m = Test()\r\n     11 \r\n---> 12 m.load_state_dict(m.state_dict())\r\n\r\n~/anaconda3/envs/torch11/lib/python3.6/site-packages/torch/nn/modules/module.py in load_state_dict(self, state_dict, strict)\r\n    775         if len(error_msgs) > 0:\r\n    776             raise RuntimeError(\'Error(s) in loading state_dict for {}:\\n\\t{}\'.format(\r\n--> 777                                self.__class__.__name__, ""\\n\\t"".join(error_msgs)))\r\n    778         return _IncompatibleKeys(missing_keys, unexpected_keys)\r\n    779 \r\n\r\nRuntimeError: Error(s) in loading state_dict for Test:\r\n\tWhile copying the parameter named ""a"", whose dimensions in the model are torch.Size([2, 2]) and whose dimensions in the checkpoint are torch.Size([2, 2]).\r\n', 'python\r\nfrom torch import nn\r\nimport torch\r\nimport numpy as np\r\nclass Test(nn.Module):\r\n    def __init__(self):\r\n        super(Test, self).__init__()\r\n        self.a = nn.Parameter(torch.sparse_coo_tensor((np.array([0,1]),np.array([0,1])), np.array([1.0,2.0]), size=[2,2],\r\n                                                dtype=torch.float32), requires_grad=False)\r\n\r\nm = Test()\r\nm.load_state_dict(m.state_dict())\r\n', '\r\nCollecting environment information...\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: Quadro P600\r\nNvidia driver version: 410.79\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] torch==1.1.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl_fft                   1.0.12           py36ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py36hd81dba3_0  \r\n[conda] pytorch                   1.1.0           py3.6_cuda10.0.130_cudnn7.5.1_0    pytorch\r\n', '\r\nCollecting environment information...\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: Quadro P600\r\nNvidia driver version: 410.79\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] torch==1.0.1.post2\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl_fft                   1.0.12           py36ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py36hd81dba3_0  \r\n[conda] pytorch                   1.0.1           py3.6_cuda10.0.130_cudnn7.4.2_2    pytorch\r\n']","['1.1.0', 'nn.Module', 'model.load_state_dict(model.state_dict())', '1.0.1', 'nn.Module', '1.0.1']",0,0
677,pytorch,14078,closed,Build failing at torch/lib/c10d/ProcessGroupMPI.cpp,"## üêõ Bug

PyTorch fails to finish building, with a possible bug (see below).



## To Reproduce


## Environment


 - PyTorch Version (e.g., 1.0): Master branch (1.0) 
 - OS (e.g., Linux): Debian:Stretch
 - How you installed PyTorch (, , source): source
 - Build command you used (if compiling from source): see above
 - Python version: 3.6
 - CUDA/cuDNN version: N/A
 - GPU models and configuration: N/A
 - Any other relevant information: N/A

## Additional context

<!-- Add any other context about the problem here. -->
",oncall: distributed,"['@sadatnfs give us the full build log.', 'I just edited the main post to add the part where the error was coming from. Here\'s the full log:\r\n```bash \r\n\r\nStep 44/53 : RUN cd /opt && git clone --recursive https://github.com/pytorch/pytorch     && cd pytorch && git submodule update --init &&     cd /opt/pytorch/third_party/ideep/mkl-dnn &&     git pull https://github.com/intel/mkl-dnn.git --no-commit  --rebase &&     cd /opt/pytorch &&     sed -i \'s/""Use MKLDNN"" OFF/""Use MKLDNN"" ON /g\' CMakeLists.txt &&     sed -i \'s/""Use DISTRIBUTED"" OFF/""Use DISTRIBUTED"" ON /g\' CMakeLists.txt &&     sed -i \'s/for parallel code"" OFF/for parallel code"" ON /g\' CMakeLists.txt &&     PYTHON_EXECUTABLE=/opt/conda/bin/python     PYTHON_LIBRARY=/opt/conda/lib/libpython3.6m.so     PYTHON_INCLUDE_DIR=/opt/conda/include/python3.6m     FULL_CAFFE2=1     USE_OPENMP=1     USE_MKL=1     USE_MKLDNN=1     USE_MKLML=1     USE_SYSTEM_EIGEN_INSTALL=1     USE_ZMQ=1     USE_DISTRIBUTED=1     MKLDNN_LIBRARY=/usr/local/lib     MKLDNN_INCLUDE_DIR=/usr/local/include     MKLDNN_LIB_DIR=/usr/local/lib     python setup.py install &&     cd /opt && rm -rf /opt/pytorch &&     cd /usr/lib && sudo ldconfig\r\n ---> Running in 84e8bd25c4b6\r\n\x1b[91mCloning into \'pytorch\'...\r\n\x1b[0m\x1b[91mSubmodule \'third_party/ComputeLibrary\' (https://github.com/ARM-software/ComputeLibrary.git) registered for path \'third_party/ComputeLibrary\'\r\n\x1b[0m\x1b[91mSubmodule \'third_party/NNPACK_deps/FP16\' (https://github.com/Maratyszcza/FP16.git) registered for path \'third_party/FP16\'\r\n\x1b[0m\x1b[91mSubmodule \'third_party/NNPACK_deps/FXdiv\' (https://github.com/Maratyszcza/FXdiv.git) registered for path \'third_party/FXdiv\'\r\n\x1b[0m\x1b[91mSubmodule \'third_party/NNPACK\' (https://github.com/Maratyszcza/NNPACK.git) registered for path \'third_party/NNPACK\'\r\n\x1b[0m\x1b[91mSubmodule \'third_party/QNNPACK\' (https://github.com/pytorch/QNNPACK) registered for path \'third_party/QNNPACK\'\r\n\x1b[0m\x1b[91mSubmodule \'third_party/benchmark\' (https://github.com/google/benchmark.git) registered for path \'third_party/benchmark\'\r\n\x1b[0m\x1b[91mSubmodule \'third-party/cpuinfo\' (https://github.com/Maratyszcza/cpuinfo.git) registered for path \'third_party/cpuinfo\'\r\n\x1b[0m\x1b[91mSubmodule \'third_party/cub\' (https://github.com/NVlabs/cub.git) registered for path \'third_party/cub\'\r\n\x1b[0m\x1b[91mSubmodule \'third_party/eigen\' (https://github.com/eigenteam/eigen-git-mirror.git) registered for path \'third_party/eigen\'\r\n\x1b[0m\x1b[91mSubmodule \'third_party/fbgemm\' (https://github.com/pytorch/fbgemm) registered for path \'third_party/fbgemm\'\r\n\x1b[0m\x1b[91mSubmodule \'third_party/gemmlowp/gemmlowp\' (https://github.com/google/gemmlowp.git) registered for path \'third_party/gemmlowp/gemmlowp\'\r\n\x1b[0m\x1b[91mSubmodule \'third_party/gloo\' (https://github.com/facebookincubator/gloo) registered for path \'third_party/gloo\'\r\n\x1b[0m\x1b[91mSubmodule \'third_party/googletest\' (https://github.com/google/googletest.git) registered for path \'third_party/googletest\'\r\n\x1b[0m\x1b[91mSubmodule \'third_party/ideep\' (https://github.com/intel/ideep) registered for path \'third_party/ideep\'\r\n\x1b[0m\x1b[91mSubmodule \'third_party/ios-cmake\' (https://github.com/Yangqing/ios-cmake.git) registered for path \'third_party/ios-cmake\'\r\n\x1b[0m\x1b[91mSubmodule \'third_party/nccl/nccl\' (https://github.com/NVIDIA/nccl) registered for path \'third_party/nccl/nccl\'\r\n\x1b[0m\x1b[91mSubmodule \'third_party/neon2sse\' (https://github.com/intel/ARM_NEON_2_x86_SSE.git) registered for path \'third_party/neon2sse\'\r\n\x1b[0m\x1b[91mSubmodule \'third_party/onnx\' (https://github.com/onnx/onnx.git) registered for path \'third_party/onnx\'\r\n\x1b[0m\x1b[91mSubmodule \'third_party/onnx-tensorrt\' (https://github.com/onnx/onnx-tensorrt) registered for path \'third_party/onnx-tensorrt\'\r\n\x1b[0m\x1b[91mSubmodule \'third_party/protobuf\' (https://github.com/google/protobuf.git) registered for path \'third_party/protobuf\'\r\n\x1b[0m\x1b[91mSubmodule \'third_party/NNPACK_deps/psimd\' (https://github.com/Maratyszcza/psimd.git) registered for path \'third_party/psimd\'\r\n\x1b[0m\x1b[91mSubmodule \'third_party/NNPACK_deps/pthreadpool\' (https://github.com/Maratyszcza/pthreadpool.git) registered for path \'third_party/pthreadpool\'\r\n\x1b[0m\x1b[91mSubmodule \'third_party/pybind11\' (https://github.com/pybind/pybind11.git) registered for path \'third_party/pybind11\'\r\n\x1b[0m\x1b[91mSubmodule \'third_party/python-enum\' (https://github.com/PeachPy/enum34.git) registered for path \'third_party/python-enum\'\r\n\x1b[0m\x1b[91mSubmodule \'third_party/python-peachpy\' (https://github.com/Maratyszcza/PeachPy.git) registered for path \'third_party/python-peachpy\'\r\n\x1b[0m\x1b[91mSubmodule \'third_party/python-six\' (https://github.com/benjaminp/six.git) registered for path \'third_party/python-six\'\r\n\x1b[0m\x1b[91mSubmodule \'third_party/sleef\' (https://github.com/shibatch/sleef) registered for path \'third_party/sleef\'\r\n\x1b[0m\x1b[91mSubmodule \'third_party/zstd\' (https://github.com/facebook/zstd.git) registered for path \'third_party/zstd\'\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/ComputeLibrary\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/FP16\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/FXdiv\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/NNPACK\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/QNNPACK\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/benchmark\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/cpuinfo\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/cub\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/eigen\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/fbgemm\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/gemmlowp/gemmlowp\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/gloo\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/googletest\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/ideep\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/ios-cmake\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/nccl/nccl\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/neon2sse\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/onnx\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/onnx-tensorrt\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/protobuf\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/psimd\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/pthreadpool\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/pybind11\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/python-enum\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/python-peachpy\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/python-six\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/sleef\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/zstd\'...\r\n\x1b[0mSubmodule path \'third_party/ComputeLibrary\': checked out \'292227986edb37b01061afcad6df18ba9d6ccbeb\'\r\nSubmodule path \'third_party/FP16\': checked out \'34d4bf01bbf7376f2baa71b8fa148b18524d45cf\'\r\nSubmodule path \'third_party/FXdiv\': checked out \'811b482bcd9e8d98ad80c6c78d5302bb830184b0\'\r\nSubmodule path \'third_party/NNPACK\': checked out \'1e005b0c2777f39972a4ac15bea03e0e315a3d92\'\r\nSubmodule path \'third_party/QNNPACK\': checked out \'85e21ce260ea04a8dc024c4efe6115fd59ac45d4\'\r\nSubmodule path \'third_party/benchmark\': checked out \'505be96ab23056580a3a2315abba048f4428b04e\'\r\nSubmodule path \'third_party/cpuinfo\': checked out \'c342292afb040c868849bc15e96ab894dceba2bc\'\r\nSubmodule path \'third_party/cub\': checked out \'285aeebaa34b0e8a7670867a2e66c1a52d998d6a\'\r\n\x1b[91mFrom https://github.com/eigenteam/eigen-git-mirror\r\n * branch                f59336cee358f92b959de6a0daf07c4ab2318022 -> FETCH_HEAD\r\n\x1b[0mSubmodule path \'third_party/eigen\': checked out \'f59336cee358f92b959de6a0daf07c4ab2318022\'\r\nSubmodule path \'third_party/fbgemm\': checked out \'56d9537efc00d7741486b7539f13e0abae6ef1df\'\r\nSubmodule path \'third_party/gemmlowp/gemmlowp\': checked out \'8416bab644641a5c0a81ecf91a5cda804af0aee1\'\r\nSubmodule path \'third_party/gloo\': checked out \'0df388b44ed22f5b88aff31ab997fd6dfa6561ac\'\r\nSubmodule path \'third_party/googletest\': checked out \'2fe3bd994b3189899d93f1d5a881e725e046fdc2\'\r\nSubmodule path \'third_party/ideep\': checked out \'dedff8fb8193fe3a1ea893d4bc852f8ea395b6b3\'\r\n\x1b[91mSubmodule \'mkl-dnn\' (https://github.com/01org/mkl-dnn.git) registered for path \'third_party/ideep/mkl-dnn\'\r\n\x1b[0m\x1b[91mSubmodule \'tests/googletest\' (https://github.com/google/googletest.git) registered for path \'third_party/ideep/tests/googletest\'\r\n\x1b[0m\x1b[91mSubmodule \'tests/rapidcheck\' (https://github.com/emil-e/rapidcheck.git) registered for path \'third_party/ideep/tests/rapidcheck\'\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/ideep/mkl-dnn\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/ideep/tests/googletest\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/ideep/tests/rapidcheck\'...\r\n\x1b[0mSubmodule path \'third_party/ideep/mkl-dnn\': checked out \'c0095ec26c215970b9b94915963b59e76398265e\'\r\nSubmodule path \'third_party/ideep/tests/googletest\': checked out \'08d5b1f33af8c18785fb8ca02792b5fac81e248f\'\r\nSubmodule path \'third_party/ideep/tests/rapidcheck\': checked out \'10fc0cbaa46a765d8134e99995de9c81c048a331\'\r\n\x1b[91mSubmodule \'ext/catch\' (https://github.com/philsquared/Catch.git) registered for path \'third_party/ideep/tests/rapidcheck/ext/catch\'\r\n\x1b[0m\x1b[91mSubmodule \'ext/googletest\' (https://github.com/google/googletest) registered for path \'third_party/ideep/tests/rapidcheck/ext/googletest\'\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/ideep/tests/rapidcheck/ext/catch\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/ideep/tests/rapidcheck/ext/googletest\'...\r\n\x1b[0mSubmodule path \'third_party/ideep/tests/rapidcheck/ext/catch\': checked out \'2ce6c74f8fcbd28c977e70d8c020939123a9ea3c\'\r\nSubmodule path \'third_party/ideep/tests/rapidcheck/ext/googletest\': checked out \'ecd530865cefdfa7dea58e84f6aa1b548950363d\'\r\nSubmodule path \'third_party/ios-cmake\': checked out \'8abaed637d56f1337d6e1d2c4026e25c1eade724\'\r\nSubmodule path \'third_party/nccl/nccl\': checked out \'3c6e25210bb1b544748937e5db74db0b9679b95e\'\r\nSubmodule path \'third_party/neon2sse\': checked out \'97a126f08ce318023be604d03f88bf0820a9464a\'\r\nSubmodule path \'third_party/onnx\': checked out \'882c5283c54345d131e8fe5c859e4844dcf7ca8e\'\r\n\x1b[91mSubmodule \'third_party/benchmark\' (https://github.com/google/benchmark.git) registered for path \'third_party/onnx/third_party/benchmark\'\r\n\x1b[0m\x1b[91mSubmodule \'third_party/pybind11\' (https://github.com/pybind/pybind11.git) registered for path \'third_party/onnx/third_party/pybind11\'\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/onnx/third_party/benchmark\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/onnx/third_party/pybind11\'...\r\n\x1b[0mSubmodule path \'third_party/onnx/third_party/benchmark\': checked out \'e776aa0275e293707b6a0901e0e8d8a8a3679508\'\r\nSubmodule path \'third_party/onnx/third_party/pybind11\': checked out \'a1041190c8b8ff0cd9e2f0752248ad5e3789ea0c\'\r\n\x1b[91mSubmodule \'tools/clang\' (https://github.com/wjakob/clang-cindex-python3) registered for path \'third_party/onnx/third_party/pybind11/tools/clang\'\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/onnx/third_party/pybind11/tools/clang\'...\r\n\x1b[0mSubmodule path \'third_party/onnx/third_party/pybind11/tools/clang\': checked out \'6a00cbc4a9b8e68b71caf7f774b3f9c753ae84d5\'\r\nSubmodule path \'third_party/onnx-tensorrt\': checked out \'fa0964e8477fc004ee2f49ee77ffce0bf7f711a9\'\r\n\x1b[91mSubmodule \'third_party/onnx\' (https://github.com/onnx/onnx.git) registered for path \'third_party/onnx-tensorrt/third_party/onnx\'\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/onnx-tensorrt/third_party/onnx\'...\r\n\x1b[0mSubmodule path \'third_party/onnx-tensorrt/third_party/onnx\': checked out \'b4072194c2e6ef90693bcfdea4c6f45cf30bb65e\'\r\n\x1b[91mSubmodule \'third_party/benchmark\' (https://github.com/google/benchmark.git) registered for path \'third_party/onnx-tensorrt/third_party/onnx/third_party/benchmark\'\r\n\x1b[0m\x1b[91mSubmodule \'third_party/pybind11\' (https://github.com/pybind/pybind11.git) registered for path \'third_party/onnx-tensorrt/third_party/onnx/third_party/pybind11\'\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/onnx-tensorrt/third_party/onnx/third_party/benchmark\'...\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/onnx-tensorrt/third_party/onnx/third_party/pybind11\'...\r\n\x1b[0mSubmodule path \'third_party/onnx-tensorrt/third_party/onnx/third_party/benchmark\': checked out \'e776aa0275e293707b6a0901e0e8d8a8a3679508\'\r\nSubmodule path \'third_party/onnx-tensorrt/third_party/onnx/third_party/pybind11\': checked out \'a1041190c8b8ff0cd9e2f0752248ad5e3789ea0c\'\r\n\x1b[91mSubmodule \'tools/clang\' (https://github.com/wjakob/clang-cindex-python3) registered for path \'third_party/onnx-tensorrt/third_party/onnx/third_party/pybind11/tools/clang\'\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/onnx-tensorrt/third_party/onnx/third_party/pybind11/tools/clang\'...\r\n\x1b[0mSubmodule path \'third_party/onnx-tensorrt/third_party/onnx/third_party/pybind11/tools/clang\': checked out \'6a00cbc4a9b8e68b71caf7f774b3f9c753ae84d5\'\r\nSubmodule path \'third_party/protobuf\': checked out \'2761122b810fe8861004ae785cc3ab39f384d342\'\r\n\x1b[91mSubmodule \'third_party/benchmark\' (https://github.com/google/benchmark.git) registered for path \'third_party/protobuf/third_party/benchmark\'\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/protobuf/third_party/benchmark\'...\r\n\x1b[0mSubmodule path \'third_party/protobuf/third_party/benchmark\': checked out \'360e66c1c4777c99402cf8cd535aa510fee16573\'\r\nSubmodule path \'third_party/psimd\': checked out \'90a938f30ba414ada2f4b00674ee9631d7d85e19\'\r\nSubmodule path \'third_party/pthreadpool\': checked out \'13da0b4c21d17f94150713366420baaf1b5a46f4\'\r\nSubmodule path \'third_party/pybind11\': checked out \'5c8746ff135abb390bf95944be593e895a586a50\'\r\n\x1b[91mSubmodule \'tools/clang\' (https://github.com/wjakob/clang-cindex-python3) registered for path \'third_party/pybind11/tools/clang\'\r\n\x1b[0m\x1b[91mCloning into \'/opt/pytorch/third_party/pybind11/tools/clang\'...\r\n\x1b[0mSubmodule path \'third_party/pybind11/tools/clang\': checked out \'6a00cbc4a9b8e68b71caf7f774b3f9c753ae84d5\'\r\nSubmodule path \'third_party/python-enum\': checked out \'4cfedc426c4e2fc52e3f5c2b4297e15ed8d6b8c7\'\r\nSubmodule path \'third_party/python-peachpy\': checked out \'07d8fde8ac45d7705129475c0f94ed8925b93473\'\r\nSubmodule path \'third_party/python-six\': checked out \'15e31431af97e5e64b80af0a3f598d382bcdd49a\'\r\nSubmodule path \'third_party/sleef\': checked out \'6ff7a135a1e31979d1e1844a2e7171dfbd34f54f\'\r\nSubmodule path \'third_party/zstd\': checked out \'aec56a52fbab207fc639a1937d1e708a282edca8\'\r\n\x1b[91mFrom https://github.com/intel/mkl-dnn\r\n * branch              HEAD       -> FETCH_HEAD\r\n\x1b[0mFirst, rewinding head to replay your work on top of it...\r\nFast-forwarded HEAD to 733fc908874c71a5285043931a1cf80aa923165c.\r\nBuilding wheel torch-1.0.0a0+f66cb02\r\nrunning install\r\nsetup.py::run()\r\nrunning build_deps\r\n\x1b[91m+ SYNC_COMMAND=cp\r\n\x1b[0m\x1b[91m++ command -v rsync\r\n\x1b[0m\x1b[91m+ \'[\' -x /usr/bin/rsync \']\'\r\n+ SYNC_COMMAND=\'rsync -lptgoD\'\r\n\x1b[0m\x1b[91m+ CMAKE_COMMAND=cmake\r\n\x1b[0m\x1b[91m++ command -v cmake3\r\n\x1b[0m\x1b[91m+ [[ -x \'\' ]]\r\n+ USE_CUDA=0\r\n+ USE_FBGEMM=0\r\n+ USE_ROCM=0\r\n\x1b[0m\x1b[91m+ USE_NNPACK=0\r\n+ USE_MKLDNN=0\r\n+ USE_QNNPACK=0\r\n+ USE_GLOO_IBVERBS=0\r\n\x1b[0m\x1b[91m+ CAFFE2_STATIC_LINK_CUDA=0\r\n+ RERUN_CMAKE=1\r\n+ [[ 4 -gt 0 ]]\r\n+ case ""$1"" in\r\n+ USE_NNPACK=1\r\n+ shift\r\n+ [[ 3 -gt 0 ]]\r\n+ case ""$1"" in\r\n+ USE_MKLDNN=1\r\n+ shift\r\n+ [[ 2 -gt 0 ]]\r\n+ case ""$1"" in\r\n+ USE_QNNPACK=1\r\n+ shift\r\n+ [[ 1 -gt 0 ]]\r\n+ case ""$1"" in\r\n+ break\r\n+ CMAKE_INSTALL=\'make install\'\r\n+ BUILD_SHARED_LIBS=ON\r\n+ USER_CFLAGS=\r\n+ USER_LDFLAGS=\r\n+ [[ -n \'\' ]]\r\n+ [[ -n \'\' ]]\r\n+ [[ -n \'\' ]]\r\n\x1b[0m\x1b[91m++ uname\r\n\x1b[0m\x1b[91m+ \'[\' Linux == Darwin \']\'\r\n\x1b[0m\x1b[91m+++ dirname ../tools/build_pytorch_libs.sh\r\n\x1b[0m\x1b[91m++ cd ../tools/..\r\n\x1b[0m\x1b[91m+++ pwd\r\n\x1b[0m\x1b[91m++ printf \'%q\\n\' /opt/pytorch\r\n\x1b[0m\x1b[91m+ BASE_DIR=/opt/pytorch\r\n+ TORCH_LIB_DIR=/opt/pytorch/torch/lib\r\n+ INSTALL_DIR=/opt/pytorch/torch/lib/tmp_install\r\n+ THIRD_PARTY_DIR=/opt/pytorch/third_party\r\n+ C_FLAGS=\r\n+ C_FLAGS=\' -DOMPI_SKIP_MPICXX=1\'\r\n+ LDFLAGS=\r\n+ LD_POSTFIX=.so\r\n\x1b[0m\x1b[91m++ uname\r\n\x1b[0m\x1b[91m+ [[ Linux == \\D\\a\\r\\w\\i\\n ]]\r\n+ [[ 0 -eq 1 ]]\r\n+ LDFLAGS=\' -Wl,-rpath,$ORIGIN\'\r\n+ CPP_FLAGS=\' -std=c++11 \'\r\n+ THD_FLAGS=\r\n+ [[ 0 -eq 1 ]]\r\n+ CUDA_NVCC_FLAGS=\' -DOMPI_SKIP_MPICXX=1\'\r\n+ [[ -z \'\' ]]\r\n+ CUDA_DEVICE_DEBUG=0\r\n+ \'[\' -z \'\' \']\'\r\n\x1b[0m\x1b[91m++ getconf _NPROCESSORS_ONLN\r\n\x1b[0m\x1b[91m+ MAX_JOBS=8\r\n+ BUILD_TYPE=Release\r\n+ [[ -n \'\' ]]\r\n+ [[ -n \'\' ]]\r\n+ echo \'Building in Release mode\'\r\n\x1b[0mBuilding in Release mode\r\n\x1b[91m+ mkdir -p /opt/pytorch/torch/lib/tmp_install\r\n\x1b[0m\x1b[91m+ for arg in ""$@""\r\n+ [[ caffe2 == \\c\\a\\f\\f\\e\\2 ]]\r\n+ build_caffe2\r\n+ [[ -z \'\' ]]\r\n+ EXTRA_CAFFE2_CMAKE_FLAGS=()\r\n+ [[ -n \'\' ]]\r\n+ [[ -n /opt/conda/lib/python3.6/site-packages ]]\r\n+ EXTRA_CAFFE2_CMAKE_FLAGS+=(""-DCMAKE_PREFIX_PATH=$CMAKE_PREFIX_PATH"")\r\n+ [[ 1 -eq 1 ]]\r\n\x1b[0m\x1b[91m+ cmake /opt/pytorch -DPYTHON_EXECUTABLE=/opt/conda/bin/python -DPYTHON_LIBRARY=/opt/conda/lib/libpython3.6m.so.1.0 -DPYTHON_INCLUDE_DIR=/opt/conda/include/python3.6m -DBUILDING_WITH_TORCH_LIBS=ON -DTORCH_BUILD_VERSION=1.0.0a0+f66cb02 -DCMAKE_BUILD_TYPE=Release -DBUILD_TORCH=ON -DBUILD_PYTHON=ON -DBUILD_SHARED_LIBS=ON -DBUILD_BINARY=OFF -DBUILD_TEST=ON -DINSTALL_TEST=ON -DBUILD_CAFFE2_OPS=ON -DONNX_NAMESPACE=onnx_torch -DUSE_CUDA=0 -DUSE_DISTRIBUTED=ON -DUSE_FBGEMM=0 -DUSE_NUMPY= -DCAFFE2_STATIC_LINK_CUDA=0 -DUSE_ROCM=0 -DUSE_NNPACK=1 -DUSE_LEVELDB=OFF -DUSE_LMDB=OFF -DUSE_OPENCV=OFF -DUSE_QNNPACK=1 -DUSE_FFMPEG=OFF -DUSE_GLOG=OFF -DUSE_GFLAGS=OFF -DUSE_SYSTEM_EIGEN_INSTALL=OFF -DCUDNN_INCLUDE_DIR= -DCUDNN_LIB_DIR= -DCUDNN_LIBRARY= -DUSE_MKLDNN=1 -DNCCL_EXTERNAL=0 -DCMAKE_INSTALL_PREFIX=/opt/pytorch/torch/lib/tmp_install -DCMAKE_C_FLAGS= -DCMAKE_CXX_FLAGS= \'-DCMAKE_EXE_LINKER_FLAGS= -Wl,-rpath,$ORIGIN \' \'-DCMAKE_SHARED_LINKER_FLAGS= -Wl,-rpath,$ORIGIN \' -DTHD_SO_VERSION=1 -DCMAKE_PREFIX_PATH=/opt/conda/lib/python3.6/site-packages\r\n\x1b[0m-- The CXX compiler identification is GNU 8.2.0\r\n-- The C compiler identification is GNU 8.2.0\r\n-- Check for working CXX compiler: /usr/local/bin/c++\r\n-- Check for working CXX compiler: /usr/local/bin/c++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Check for working C compiler: /usr/local/bin/gcc\r\n-- Check for working C compiler: /usr/local/bin/gcc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Not forcing any particular BLAS to be found\r\n-- Performing Test COMPILER_WORKS\r\n-- Performing Test COMPILER_WORKS - Success\r\n-- Performing Test SUPPORT_GLIBCXX_USE_C99\r\n-- Performing Test SUPPORT_GLIBCXX_USE_C99 - Success\r\n-- Performing Test CAFFE2_EXCEPTION_PTR_SUPPORTED\r\n-- Performing Test CAFFE2_EXCEPTION_PTR_SUPPORTED - Success\r\n-- std::exception_ptr is supported.\r\n-- Performing Test CAFFE2_IS_NUMA_AVAILABLE\r\n-- Performing Test CAFFE2_IS_NUMA_AVAILABLE - Success\r\n-- NUMA is available\r\n-- Performing Test CAFFE2_NEED_TO_TURN_OFF_DEPRECATION_WARNING\r\n-- Performing Test CAFFE2_NEED_TO_TURN_OFF_DEPRECATION_WARNING - Failed\r\n-- Turning off deprecation warning due to glog.\r\n-- Performing Test CAFFE2_COMPILER_SUPPORTS_AVX2_EXTENSIONS\r\n-- Performing Test CAFFE2_COMPILER_SUPPORTS_AVX2_EXTENSIONS - Success\r\n-- Current compiler supports avx2 extension. Will build perfkernels.\r\n-- Performing Test CAFFE2_COMPILER_SUPPORTS_AVX512F_EXTENSIONS\r\n-- Performing Test CAFFE2_COMPILER_SUPPORTS_AVX512F_EXTENSIONS - Success\r\n-- Current compiler supports avx512f extension. Will build fbgemm.\r\n-- Performing Test COMPILER_SUPPORTS_HIDDEN_VISIBILITY\r\n-- Performing Test COMPILER_SUPPORTS_HIDDEN_VISIBILITY - Success\r\n-- Performing Test COMPILER_SUPPORTS_HIDDEN_INLINE_VISIBILITY\r\n-- Performing Test COMPILER_SUPPORTS_HIDDEN_INLINE_VISIBILITY - Success\r\n-- Performing Test COMPILER_SUPPORTS_RDYNAMIC\r\n-- Performing Test COMPILER_SUPPORTS_RDYNAMIC - Success\r\n-- Building using own protobuf under third_party per request.\r\n-- Use custom protobuf build.\r\n-- Looking for pthread.h\r\n-- Looking for pthread.h - found\r\n-- Looking for pthread_create\r\n-- Looking for pthread_create - not found\r\n-- Looking for pthread_create in pthreads\r\n-- Looking for pthread_create in pthreads - not found\r\n-- Looking for pthread_create in pthread\r\n-- Looking for pthread_create in pthread - found\r\n-- Found Threads: TRUE  \r\n-- Caffe2 protobuf include directory: $<BUILD_INTERFACE:/opt/pytorch/third_party/protobuf/src>$<INSTALL_INTERFACE:include>\r\n-- The BLAS backend of choice:MKL\r\n-- Looking for sys/types.h\r\n-- Looking for sys/types.h - found\r\n-- Looking for stdint.h\r\n-- Looking for stdint.h - found\r\n-- Looking for stddef.h\r\n-- Looking for stddef.h - found\r\n-- Check size of void*\r\n-- Check size of void* - done\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_intel_lp64: /opt/intel/mkl/lib/intel64/libmkl_intel_lp64.so\r\n--   Library mkl_gnu_thread: /opt/intel/mkl/lib/intel64/libmkl_gnu_thread.so\r\n--   Library mkl_core: /opt/intel/mkl/lib/intel64/libmkl_core.so\r\n-- Try OpenMP C flag = [-fopenmp]\r\n-- Performing Test OpenMP_FLAG_DETECTED\r\n-- Performing Test OpenMP_FLAG_DETECTED - Success\r\n-- Try OpenMP CXX flag = [-fopenmp]\r\n-- Performing Test OpenMP_FLAG_DETECTED\r\n-- Performing Test OpenMP_FLAG_DETECTED - Success\r\n-- Found OpenMP: -fopenmp  \r\n--   Library gomp: -fopenmp\r\n--   Library pthread: /usr/lib/x86_64-linux-gnu/libpthread.so\r\n--   Library m: /usr/lib/x86_64-linux-gnu/libm.so\r\n--   Library dl: /usr/lib/x86_64-linux-gnu/libdl.so\r\n-- Looking for cblas_sgemm\r\n-- Looking for cblas_sgemm - found\r\n-- The ASM compiler identification is GNU\r\n-- Found assembler: /usr/local/bin/gcc\r\n-- Check if compiler accepts -pthread\r\n-- Check if compiler accepts -pthread - yes\r\n-- Brace yourself, we are building NNPACK\r\n-- Found PythonInterp: /opt/conda/bin/python (found version ""3.6.5"") \r\n-- Failed to find LLVM FileCheck\r\n-- Found Git: /usr/bin/git (found version ""2.11.0"") \r\n\x1b[91m-- git Version: v1.4.0-505be96a\r\n\x1b[0m\x1b[91m-- Version: 1.4.0\r\n\x1b[0m-- Performing Test HAVE_CXX_FLAG_STD_CXX11\r\n-- Performing Test HAVE_CXX_FLAG_STD_CXX11 - Success\r\n-- Performing Test HAVE_CXX_FLAG_WALL\r\n-- Performing Test HAVE_CXX_FLAG_WALL - Success\r\n-- Performing Test HAVE_CXX_FLAG_WEXTRA\r\n-- Performing Test HAVE_CXX_FLAG_WEXTRA - Success\r\n-- Performing Test HAVE_CXX_FLAG_WSHADOW\r\n-- Performing Test HAVE_CXX_FLAG_WSHADOW - Success\r\n-- Performing Test HAVE_CXX_FLAG_WERROR\r\n-- Performing Test HAVE_CXX_FLAG_WERROR - Success\r\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC\r\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC - Success\r\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS\r\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS - Success\r\n-- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32\r\n-- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32 - Failed\r\n-- Performing Test HAVE_CXX_FLAG_WFLOAT_EQUAL\r\n-- Performing Test HAVE_CXX_FLAG_WFLOAT_EQUAL - Success\r\n-- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING\r\n-- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING - Success\r\n-- Performing Test HAVE_CXX_FLAG_WNO_DEPRECATED_DECLARATIONS\r\n-- Performing Test HAVE_CXX_FLAG_WNO_DEPRECATED_DECLARATIONS - Success\r\n-- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING\r\n-- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING - Success\r\n-- Performing Test HAVE_CXX_FLAG_WD654\r\n-- Performing Test HAVE_CXX_FLAG_WD654 - Failed\r\n-- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY\r\n-- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY - Failed\r\n-- Performing Test HAVE_CXX_FLAG_COVERAGE\r\n-- Performing Test HAVE_CXX_FLAG_COVERAGE - Success\r\n\x1b[91m-- Performing Test HAVE_STD_REGEX\r\n-- Performing Test HAVE_STD_REGEX\r\n\x1b[0m\x1b[91m-- Performing Test HAVE_STD_REGEX -- success\r\n\x1b[0m\x1b[91m-- Performing Test HAVE_GNU_POSIX_REGEX\r\n-- Performing Test HAVE_GNU_POSIX_REGEX\r\n\x1b[0m\x1b[91m-- Performing Test HAVE_GNU_POSIX_REGEX -- failed to compile\r\n\x1b[0m\x1b[91m-- Performing Test HAVE_POSIX_REGEX\r\n-- Performing Test HAVE_POSIX_REGEX\r\n\x1b[0m\x1b[91m-- Performing Test HAVE_POSIX_REGEX -- success\r\n\x1b[0m\x1b[91m-- Performing Test HAVE_STEADY_CLOCK\r\n\x1b[0m\x1b[91m-- Performing Test HAVE_STEADY_CLOCK\r\n\x1b[0m\x1b[91m-- Performing Test HAVE_STEADY_CLOCK -- success\r\n\x1b[0m-- Found Numa: /usr/include  \r\n-- Found Numa  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libnuma.so)\r\n-- Using third party subdirectory Eigen.\r\n\x1b[91mPython 3.6.5 :: Anaconda, Inc.\r\n\x1b[0m-- Found PythonInterp: /opt/conda/bin/python (found suitable version ""3.6.5"", minimum required is ""2.7"") \r\n-- Found PythonLibs: /opt/conda/lib/libpython3.6m.so.1.0 (found suitable version ""3.6.5"", minimum required is ""2.7"") \r\n-- Found NumPy: /opt/conda/lib/python3.6/site-packages/numpy-1.16.0.dev0+2668b31-py3.6-linux-x86_64.egg/numpy/core/include (found version ""1.16.0.dev0+2668b31"") \r\n-- NumPy ver. 1.16.0.dev0+2668b31 found (include: /opt/conda/lib/python3.6/site-packages/numpy-1.16.0.dev0+2668b31-py3.6-linux-x86_64.egg/numpy/core/include)\r\n\x1b[91mCMake Warning at cmake/Dependencies.cmake:554 (find_package):\r\n  Could not find a package configuration file provided by ""pybind11"" with any\r\n  of the following names:\r\n\r\n    pybind11Config.cmake\r\n    pybind11-config.cmake\r\n\r\n  Add the installation prefix of ""pybind11"" to CMAKE_PREFIX_PATH or set\r\n  ""pybind11_DIR"" to a directory containing one of the above files.  If\r\n  ""pybind11"" provides a separate development package or SDK, be sure it has\r\n  been installed.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:200 (include)\r\n\r\n\r\n\x1b[0m-- Could NOT find pybind11 (missing:  pybind11_INCLUDE_DIR) \r\n-- Using third_party/pybind11.\r\n-- Found MPI_C: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so  \r\n-- Found MPI_CXX: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so;/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so  \r\n-- MPI support found\r\n-- MPI compile flags: \r\n-- MPI include path: /usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/usr/lib/x86_64-linux-gnu/openmpi/include\r\n-- MPI LINK flags path: \r\n-- MPI libraries: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so\r\n\x1b[91mCMake Warning at cmake/Dependencies.cmake:589 (message):\r\n  OpenMPI found, but it is not built with CUDA support.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:200 (include)\r\n\r\n\r\n\x1b[0m-- Adding -fopenmp\r\n\x1b[91mCMake Warning at cmake/Dependencies.cmake:740 (message):\r\n  Not using CUDA, so disabling NCCL.  Suppress this warning with\r\n  -DUSE_NCCL=OFF.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:200 (include)\r\n\r\n\r\n\x1b[0m-- MPI include path: /usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/usr/lib/x86_64-linux-gnu/openmpi/include\r\n-- MPI libraries: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so\r\n\x1b[91mCMake Warning at cmake/Dependencies.cmake:818 (message):\r\n  mobile opengl is only used in android or ios builds.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:200 (include)\r\n\r\n\r\n\x1b[0m\x1b[91mCMake Warning at cmake/Dependencies.cmake:894 (message):\r\n  Metal is only used in ios builds.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:200 (include)\r\n\r\n\r\n\x1b[0m-- \r\n-- ******** Summary ********\r\n--   CMake version         : 3.7.2\r\n--   CMake command         : /usr/bin/cmake\r\n--   System                : Linux\r\n--   C++ compiler          : /usr/local/bin/c++\r\n--   C++ compiler version  : 8.2.0\r\n--   CXX flags             :  -Wno-deprecated -fvisibility-inlines-hidden -fopenmp\r\n--   Build type            : Release\r\n--   Compile definitions   : TH_BLAS_MKL\r\n--   CMAKE_PREFIX_PATH     : /opt/conda/lib/python3.6/site-packages\r\n--   CMAKE_INSTALL_PREFIX  : /opt/pytorch/torch/lib/tmp_install\r\n--   CMAKE_MODULE_PATH     : /opt/pytorch/cmake/Modules\r\n-- \r\n--   ONNX version          : 1.3.0\r\n--   ONNX NAMESPACE        : onnx_torch\r\n--   ONNX_BUILD_TESTS      : OFF\r\n--   ONNX_BUILD_BENCHMARKS : OFF\r\n--   ONNX_USE_LITE_PROTO   : OFF\r\n--   ONNXIFI_DUMMY_BACKEND : OFF\r\n-- \r\n--   Protobuf compiler     : \r\n--   Protobuf includes     : \r\n--   Protobuf libraries    : \r\n--   BUILD_ONNX_PYTHON     : OFF\r\n-- Found gcc >=5 and CUDA <= 7.5, adding workaround C++ flags\r\n-- Could not find CUDA with FP16 support, compiling without torch.CudaHalfTensor\r\n-- Removing -DNDEBUG from compile flags\r\n-- Compiling with OpenMP support\r\n-- MAGMA not found. Compiling without MAGMA support\r\n-- Could not find hardware support for NEON on this machine.\r\n-- No OMAP3 processor on this machine.\r\n-- No OMAP4 processor on this machine.\r\n-- Looking for cpuid.h\r\n-- Looking for cpuid.h - found\r\n-- Performing Test HAVE_GCC_GET_CPUID\r\n-- Performing Test HAVE_GCC_GET_CPUID - Success\r\n-- Performing Test NO_GCC_EBX_FPIC_BUG\r\n-- Performing Test NO_GCC_EBX_FPIC_BUG - Success\r\n-- Performing Test C_HAS_AVX_1\r\n-- Performing Test C_HAS_AVX_1 - Failed\r\n-- Performing Test C_HAS_AVX_2\r\n-- Performing Test C_HAS_AVX_2 - Success\r\n-- Performing Test C_HAS_AVX2_1\r\n-- Performing Test C_HAS_AVX2_1 - Failed\r\n-- Performing Test C_HAS_AVX2_2\r\n-- Performing Test C_HAS_AVX2_2 - Success\r\n-- Performing Test CXX_HAS_AVX_1\r\n-- Performing Test CXX_HAS_AVX_1 - Failed\r\n-- Performing Test CXX_HAS_AVX_2\r\n-- Performing Test CXX_HAS_AVX_2 - Success\r\n-- Performing Test CXX_HAS_AVX2_1\r\n-- Performing Test CXX_HAS_AVX2_1 - Failed\r\n-- Performing Test CXX_HAS_AVX2_2\r\n-- Performing Test CXX_HAS_AVX2_2 - Success\r\n-- AVX compiler support found\r\n-- AVX2 compiler support found\r\n-- Performing Test HAS_C11_ATOMICS\r\n-- Performing Test HAS_C11_ATOMICS - Success\r\n-- Atomics: using C11 intrinsics\r\n-- Performing Test BLAS_F2C_DOUBLE_WORKS\r\n-- Performing Test BLAS_F2C_DOUBLE_WORKS - Failed\r\n-- Performing Test BLAS_F2C_FLOAT_WORKS\r\n-- Performing Test BLAS_F2C_FLOAT_WORKS - Success\r\n-- Performing Test BLAS_USE_CBLAS_DOT\r\n-- Performing Test BLAS_USE_CBLAS_DOT - Success\r\n-- Found a library with BLAS API (mkl).\r\n-- Found a library with LAPACK API. (mkl)\r\n\x1b[91mdisabling CUDA because NOT USE_CUDA is set\r\n\x1b[0m-- CuDNN not found. Compiling without CuDNN support\r\n\x1b[91mdisabling ROCM because NOT USE_ROCM is set\r\n\x1b[0m-- MIOpen not found. Compiling without MIOpen support\r\n-- Found MKLDNN: /opt/intel/mkl/lib/intel64/libmkl_intel_lp64.so;/opt/intel/mkl/lib/intel64/libmkl_gnu_thread.so;/opt/intel/mkl/lib/intel64/libmkl_core.so;-fopenmp;/usr/lib/x86_64-linux-gnu/libpthread.so;/usr/lib/x86_64-linux-gnu/libm.so;/usr/lib/x86_64-linux-gnu/libdl.so  \r\n-- Detecting Intel(R) MKL: trying mklml_intel\r\n-- Detecting Intel(R) MKL: trying mklml_gnu\r\n-- Detecting Intel(R) MKL: trying mklml\r\n-- Detecting Intel(R) MKL: trying mkl_rt\r\n-- Intel(R) MKL: include /opt/intel/compilers_and_libraries/linux/mkl/include\r\n-- Intel(R) MKL: lib /usr/local/lib64/libmkl_rt.so\r\n-- OpenMP lib: -fopenmp\r\n-- Found Doxygen: /usr/bin/doxygen (found version ""1.8.13"") \r\n-- VTune profiling environment is unset\r\n-- Looking for clock_gettime in rt\r\n-- Looking for clock_gettime in rt - found\r\n-- Looking for mmap\r\n-- Looking for mmap - found\r\n-- Looking for shm_open\r\n-- Looking for shm_open - found\r\n-- Looking for shm_unlink\r\n-- Looking for shm_unlink - found\r\n-- Looking for malloc_usable_size\r\n-- Looking for malloc_usable_size - found\r\n-- Performing Test C_HAS_THREAD\r\n-- Performing Test C_HAS_THREAD - Success\r\n-- GCC 8.2.0: Adding gcc and gcc_s libs to link line\r\n-- Using python found in /opt/conda/bin/python\r\n\x1b[91mdisabling CUDA because USE_CUDA is set false\r\n\x1b[0m-- Check size of long double\r\n-- Check size of long double - done\r\n-- Performing Test COMPILER_SUPPORTS_LONG_DOUBLE\r\n-- Performing Test COMPILER_SUPPORTS_LONG_DOUBLE - Success\r\n-- Performing Test COMPILER_SUPPORTS_FLOAT128\r\n-- Performing Test COMPILER_SUPPORTS_FLOAT128 - Success\r\n-- Performing Test COMPILER_SUPPORTS_SSE2\r\n-- Performing Test COMPILER_SUPPORTS_SSE2 - Success\r\n-- Performing Test COMPILER_SUPPORTS_SSE4\r\n-- Performing Test COMPILER_SUPPORTS_SSE4 - Success\r\n-- Performing Test COMPILER_SUPPORTS_AVX\r\n-- Performing Test COMPILER_SUPPORTS_AVX - Success\r\n-- Performing Test COMPILER_SUPPORTS_FMA4\r\n-- Performing Test COMPILER_SUPPORTS_FMA4 - Success\r\n-- Performing Test COMPILER_SUPPORTS_AVX2\r\n-- Performing Test COMPILER_SUPPORTS_AVX2 - Success\r\n-- Performing Test COMPILER_SUPPORTS_SVE\r\n-- Performing Test COMPILER_SUPPORTS_SVE - Failed\r\n-- Performing Test COMPILER_SUPPORTS_AVX512F\r\n-- Performing Test COMPILER_SUPPORTS_AVX512F - Success\r\n-- Performing Test COMPILER_SUPPORTS_OPENMP\r\n-- Performing Test COMPILER_SUPPORTS_OPENMP - Success\r\n-- Performing Test COMPILER_SUPPORTS_WEAK_ALIASES\r\n-- Performing Test COMPILER_SUPPORTS_WEAK_ALIASES - Success\r\n-- Performing Test COMPILER_SUPPORTS_BUILTIN_MATH\r\n-- Performing Test COMPILER_SUPPORTS_BUILTIN_MATH - Success\r\n-- Configuring build for SLEEF-v3.2\r\n\x1b[91m   Target system: Linux-4.4.0-116-generic\r\n   Target processor: x86_64\r\n   Host system: Linux-4.4.0-116-generic\r\n   Host processor: x86_64\r\n   Detected C compiler: GNU @ /usr/local/bin/gcc\r\n\x1b[0m-- Using option `-Wall -Wno-unused -Wno-attributes -Wno-unused-result -Wno-psabi -ffp-contract=off -fno-math-errno -fno-trapping-math` to compile libsleef\r\n-- Building shared libs : OFF\r\n-- MPFR : /usr/lib/x86_64-linux-gnu/libmpfr.so\r\n-- MPFR header file in /usr/include\r\n-- GMP : /usr/lib/x86_64-linux-gnu/libgmp.so\r\n-- RUNNING_ON_TRAVIS : 0\r\n-- COMPILER_SUPPORTS_OPENMP : 1\r\n-- Using python found in /opt/conda/bin/python\r\n-- /usr/local/bin/c++ /opt/pytorch/torch/abi-check.cpp -o /opt/pytorch/build/abi-check\r\n-- Determined _GLIBCXX_USE_CXX11_ABI=1\r\n-- Performing Test HAS_THREAD_LOCAL\r\n-- Performing Test HAS_THREAD_LOCAL - Success\r\n-- ignoring CUDA\r\n-- MPI_LIBRARIES: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so;/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so\r\n-- Building C10D without CUDA support\r\n-- MPI_INCLUDE_PATH: /usr/lib/x86_64-linux-gnu/openmpi/include/openmpi;/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent;/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include;/usr/lib/x86_64-linux-gnu/openmpi/include\r\n-- MPI_LIBRARIES: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so;/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so\r\n-- MPIEXEC: /usr/bin/mpiexec\r\n-- NCCL operators skipped due to no CUDA support\r\n-- Including IDEEP operators\r\n-- Excluding image processing operators due to no opencv\r\n-- Excluding video processing operators due to no opencv\r\n-- Include Observer library\r\n-- Using lib/python3.6/site-packages as python relative installation path\r\n-- Automatically generating missing __init__.py files.\r\n\x1b[91mCMake Warning at CMakeLists.txt:388 (message):\r\n  Generated cmake files are only fully tested if one builds with system glog,\r\n  gflags, and protobuf.  Other settings may generate files that are not well\r\n  tested.\r\n\r\n\r\n\x1b[0m-- \r\n-- ******** Summary ********\r\n-- General:\r\n--   CMake version         : 3.7.2\r\n--   CMake command         : /usr/bin/cmake\r\n--   System                : Linux\r\n--   C++ compiler          : /usr/local/bin/c++\r\n--   C++ compiler version  : 8.2.0\r\n--   BLAS                  : MKL\r\n--   CXX flags             :  -Wno-deprecated -fvisibility-inlines-hidden -fopenmp -D_FORCE_INLINES -D_MWAITXINTRIN_H_INCLUDED -D__STRICT_ANSI__ -fopenmp -O2 -fPIC -Wno-narrowing -Wall -Wextra -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -Wno-stringop-overflow\r\n--   Build type            : Release\r\n--   Compile definitions   : TH_BLAS_MKL;ONNX_NAMESPACE=onnx_torch;USE_C11_ATOMICS=1;HAVE_MMAP=1;_FILE_OFFSET_BITS=64;HAVE_SHM_OPEN=1;HAVE_SHM_UNLINK=1;HAVE_MALLOC_USABLE_SIZE=1\r\n--   CMAKE_PREFIX_PATH     : /opt/conda/lib/python3.6/site-packages\r\n--   CMAKE_INSTALL_PREFIX  : /opt/pytorch/torch/lib/tmp_install\r\n-- \r\n--   TORCH_VERSION         : 1.0.0\r\n--   CAFFE2_VERSION        : 1.0.0\r\n--   BUILD_ATEN_MOBILE     : OFF\r\n--   BUILD_ATEN_ONLY       : OFF\r\n--   BUILD_BINARY          : OFF\r\n--   BUILD_CUSTOM_PROTOBUF : ON\r\n--     Link local protobuf : ON\r\n--   BUILD_DOCS            : OFF\r\n--   BUILD_PYTHON          : ON\r\n--     Python version      : 3.6.5\r\n--     Python executable   : /opt/conda/bin/python\r\n--     Pythonlibs version  : 3.6.5\r\n--     Python library      : /opt/conda/lib/libpython3.6m.so.1.0\r\n--     Python includes     : /opt/conda/include/python3.6m\r\n--     Python site-packages: lib/python3.6/site-packages\r\n--   BUILD_CAFFE2_OPS      : ON\r\n--   BUILD_SHARED_LIBS     : ON\r\n--   BUILD_TEST            : ON\r\n--   USE_ASAN              : OFF\r\n--   USE_CUDA              : 0\r\n--   USE_ROCM              : OFF\r\n--   USE_EIGEN_FOR_BLAS    : \r\n--   USE_FBGEMM            : 0\r\n--   USE_FFMPEG            : OFF\r\n--   USE_GFLAGS            : OFF\r\n--   USE_GLOG              : OFF\r\n--   USE_LEVELDB           : OFF\r\n--   USE_LITE_PROTO        : OFF\r\n--   USE_LMDB              : OFF\r\n--   USE_METAL             : OFF\r\n--   USE_MKL               : ON\r\n--   USE_MKLDNN            : ON\r\n--   USE_MOBILE_OPENGL     : OFF\r\n--   USE_NCCL              : OFF\r\n--   USE_NNPACK            : 1\r\n--   USE_NUMPY             : ON\r\n--   USE_OBSERVERS         : ON\r\n--   USE_OPENCL            : OFF\r\n--   USE_OPENCV            : OFF\r\n--   USE_OPENMP            : ON\r\n--   USE_PROF              : OFF\r\n--   USE_QNNPACK           : 1\r\n--   USE_REDIS             : OFF\r\n--   USE_ROCKSDB           : OFF\r\n--   USE_ZMQ               : OFF\r\n--   USE_DISTRIBUTED       : ON\r\n--     USE_MPI             : ON\r\n--     USE_GLOO            : ON\r\n--     USE_GLOO_IBVERBS    : OFF\r\n--   Public Dependencies  : Threads::Threads;caffe2::mkl;caffe2::mkldnn\r\n--   Private Dependencies : qnnpack;nnpack;cpuinfo;/usr/lib/x86_64-linux-gnu/libnuma.so;fp16;/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so;/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so;gloo;aten_op_header_gen;onnxifi_loader;rt;gcc_s;gcc;dl\r\n-- Configuring done\r\n-- Generating done\r\n\x1b[91mCMake Warning:\r\n  Manually-specified variables were not used by the project:\r\n\r\n    CUDNN_INCLUDE_DIR\r\n    CUDNN_LIBRARY\r\n    CUDNN_LIB_DIR\r\n    THD_SO_VERSION\r\n\r\n\r\n\x1b[0m-- Build files have been written to: /opt/pytorch/build\r\n\x1b[91m+ make install -j8\r\n\x1b[0mScanning dependencies of target js_embed\r\nScanning dependencies of target pthreadpool\r\nScanning dependencies of target clog\r\nScanning dependencies of target libprotobuf-lite\r\n[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/js_embed.dir/__/src/google/protobuf/compiler/js/embed.cc.o\r\nScanning dependencies of target gtest\r\n[  1%] Building C object confu-deps/clog/CMakeFiles/clog.dir/src/clog.c.o\r\n[  1%] Building C object confu-deps/pthreadpool/CMakeFiles/pthreadpool.dir/src/threadpool-pthreads.c.o\r\n[  1%] Building CXX object third_party/googletest/googletest/CMakeFiles/gtest.dir/src/gtest-all.cc.o\r\n[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/arena.cc.o\r\nScanning dependencies of target benchmark\r\n[  1%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/benchmark.cc.o\r\nScanning dependencies of target libprotobuf\r\n[  1%] Linking C static library ../../lib/libclog.a\r\nScanning dependencies of target gloo\r\n[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/arena.cc.o\r\n[  1%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/algorithm.cc.o\r\n[  1%] Built target clog\r\n[  1%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/allgather.cc.o\r\n[  1%] Linking C static library ../../lib/libpthreadpool.a\r\n[  1%] Built target pthreadpool\r\n[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/arenastring.cc.o\r\n[  1%] Linking CXX executable ../../../bin/js_embed\r\n[  1%] Built target js_embed\r\n[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/extension_set.cc.o\r\n[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/arenastring.cc.o\r\n[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/extension_set.cc.o\r\n[  1%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/allreduce.cc.o\r\n[  1%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/allreduce_local.cc.o\r\n[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/generated_message_table_driven_lite.cc.o\r\nScanning dependencies of target onnxifi_dummy\r\n[  1%] Building C object third_party/onnx/CMakeFiles/onnxifi_dummy.dir/onnx/onnxifi_dummy.c.o\r\n[  1%] Linking C shared library ../../lib/libonnxifi_dummy.so\r\n[  1%] Built target onnxifi_dummy\r\n[  1%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/broadcast.cc.o\r\n[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/generated_message_table_driven_lite.cc.o\r\n[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/generated_message_util.cc.o\r\n[  1%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/context.cc.o\r\n[  1%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/gather.cc.o\r\n[  1%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/benchmark_register.cc.o\r\n[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/generated_message_util.cc.o\r\n[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/io/coded_stream.cc.o\r\n[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/io/zero_copy_stream.cc.o\r\n[  1%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/reduce.cc.o\r\n[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/io/zero_copy_stream_impl_lite.cc.o\r\n[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/io/coded_stream.cc.o\r\nScanning dependencies of target onnxifi_loader\r\n[  1%] Building C object third_party/onnx/CMakeFiles/onnxifi_loader.dir/onnx/onnxifi_loader.c.o\r\n[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/io/zero_copy_stream.cc.o\r\n[  1%] Linking C static library ../../lib/libonnxifi_loader.a\r\n[  1%] Built target onnxifi_loader\r\n[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/message_lite.cc.o\r\n[  1%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/colorprint.cc.o\r\n[  2%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/commandlineflags.cc.o\r\n[  2%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/scatter.cc.o\r\n[  2%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/types.cc.o\r\n[  2%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/io/zero_copy_stream_impl_lite.cc.o\r\n[  2%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/message_lite.cc.o\r\n[  2%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/repeated_field.cc.o\r\n[  2%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/atomicops_internals_x86_gcc.cc.o\r\n[  2%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/atomicops_internals_x86_msvc.cc.o\r\n[  3%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/bytestream.cc.o\r\n[  4%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/common/linux.cc.o\r\n[  4%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/complexity.cc.o\r\n[  4%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/console_reporter.cc.o\r\n[  4%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/repeated_field.cc.o\r\n[  4%] Linking CXX static library ../../../lib/libgtest.a\r\n[  4%] Built target gtest\r\n[  4%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/atomicops_internals_x86_gcc.cc.o\r\n[  4%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/atomicops_internals_x86_msvc.cc.o\r\n[  4%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/common.cc.o\r\n[  4%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/int128.cc.o\r\n[  4%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/io_win32.cc.o\r\n[  4%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/once.cc.o\r\n[  4%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/bytestream.cc.o\r\n[  4%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/common.cc.o\r\n[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/int128.cc.o\r\n[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/io_win32.cc.o\r\n[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/status.cc.o\r\n[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/once.cc.o\r\n[  5%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/counter.cc.o\r\n[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/status.cc.o\r\n[  5%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/csv_reporter.cc.o\r\n[  5%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/json_reporter.cc.o\r\n[  5%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/common/logging.cc.o\r\n[  5%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/mpi/context.cc.o\r\n[  5%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/rendezvous/context.cc.o\r\n[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/statusor.cc.o\r\n[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/stringpiece.cc.o\r\n[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/statusor.cc.o\r\n[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/stringprintf.cc.o\r\n[  5%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/reporter.cc.o\r\n[  5%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/rendezvous/file_store.cc.o\r\n[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/stringpiece.cc.o\r\n[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/stringprintf.cc.o\r\n[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/structurally_valid.cc.o\r\n\x1b[91mIn file included from /opt/pytorch/third_party/gloo/gloo/mpi/context.cc:17:\r\n/opt/pytorch/third_party/gloo/gloo/mpi/context.cc: In destructor ‚Äògloo::mpi::MPIScope::~MPIScope()‚Äô:\r\n/opt/pytorch/third_party/gloo/gloo/common/logging.h:142:58: warning: throw will always call terminate() [-Wterminate]\r\n           r.get_message_and_free(MakeString(__VA_ARGS__))); \\\r\n                                                          ^\r\n/opt/pytorch/third_party/gloo/gloo/common/logging.h:151:3: note: in expansion of macro ‚ÄòGLOO_ENFORCE_THAT_IMPL‚Äô\r\n   GLOO_ENFORCE_THAT_IMPL(Equals((x), (y)), #x "" == "" #y, __VA_ARGS__)\r\n   ^~~~~~~~~~~~~~~~~~~~~~\r\n/opt/pytorch/third_party/gloo/gloo/mpi/context.cc:44:3: note: in expansion of macro ‚ÄòGLOO_ENFORCE_EQ‚Äô\r\n   GLOO_ENFORCE_EQ(rv, MPI_SUCCESS);\r\n   ^~~~~~~~~~~~~~~\r\n/opt/pytorch/third_party/gloo/gloo/common/logging.h:142:58: note: in C++11 destructors default to noexcept\r\n           r.get_message_and_free(MakeString(__VA_ARGS__))); \\\r\n                                                          ^\r\n/opt/pytorch/third_party/gloo/gloo/common/logging.h:151:3: note: in expansion of macro ‚ÄòGLOO_ENFORCE_THAT_IMPL‚Äô\r\n   GLOO_ENFORCE_THAT_IMPL(Equals((x), (y)), #x "" == "" #y, __VA_ARGS__)\r\n   ^~~~~~~~~~~~~~~~~~~~~~\r\n/opt/pytorch/third_party/gloo/gloo/mpi/context.cc:44:3: note: in expansion of macro ‚ÄòGLOO_ENFORCE_EQ‚Äô\r\n   GLOO_ENFORCE_EQ(rv, MPI_SUCCESS);\r\n   ^~~~~~~~~~~~~~~\r\n\x1b[0m[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/structurally_valid.cc.o\r\n[  5%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/sleep.cc.o\r\n[  5%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/statistics.cc.o\r\n[  5%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/string_util.cc.o\r\n[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/strutil.cc.o\r\n[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/strutil.cc.o\r\n[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/time.cc.o\r\n[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/wire_format_lite.cc.o\r\n[  5%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/rendezvous/hash_store.cc.o\r\n[  5%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/sysinfo.cc.o\r\n[  5%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/rendezvous/prefix_store.cc.o\r\nScanning dependencies of target c10\r\n[  5%] Building CXX object c10/CMakeFiles/c10.dir/Device.cpp.o\r\nScanning dependencies of target python_copy_files\r\n[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/time.cc.o\r\n[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/wire_format_lite.cc.o\r\n[  5%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/rendezvous/store.cc.o\r\n[  5%] Linking CXX static library ../../../lib/libprotobuf-lite.a\r\n[  5%] Built target libprotobuf-lite\r\n[  5%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/address.cc.o\r\n[  5%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/timers.cc.o\r\nScanning dependencies of target mkldnn\r\n[  5%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/batch_normalization.cpp.o\r\nScanning dependencies of target ATEN_CPU_FILES_GEN_TARGET\r\n[  5%] Generating ../aten/src/ATen/CPUByteType.cpp, ../aten/src/ATen/CPUByteType.h, ../aten/src/ATen/CPUCharType.cpp, ../aten/src/ATen/CPUCharType.h, ../aten/src/ATen/CPUCopy.cpp, ../aten/src/ATen/CPUDoubleType.cpp, ../aten/src/ATen/CPUDoubleType.h, ../aten/src/ATen/CPUFloatType.cpp, ../aten/src/ATen/CPUFloatType.h, ../aten/src/ATen/CPUGenerator.h, ../aten/src/ATen/CPUHalfType.cpp, ../aten/src/ATen/CPUHalfType.h, ../aten/src/ATen/CPUIntType.cpp, ../aten/src/ATen/CPUIntType.h, ../aten/src/ATen/CPULongType.cpp, ../aten/src/ATen/CPULongType.h, ../aten/src/ATen/CPUShortType.cpp, ../aten/src/ATen/CPUShortType.h, ../aten/src/ATen/Declarations.yaml, ../aten/src/ATen/Functions.h, ../aten/src/ATen/NativeFunctions.h, ../aten/src/ATen/RegisterCPU.cpp, ../aten/src/ATen/RegisterCPU.h, ../aten/src/ATen/SparseCPUByteType.cpp, ../aten/src/ATen/SparseCPUByteType.h, ../aten/src/ATen/SparseCPUCharType.cpp, ../aten/src/ATen/SparseCPUCharType.h, ../aten/src/ATen/SparseCPUDoubleType.cpp, ../aten/src/ATen/SparseCPUDoubleType.h, ../aten/src/ATen/SparseCPUFloatType.cpp, ../aten/src/ATen/SparseCPUFloatType.h, ../aten/src/ATen/SparseCPUIntType.cpp, ../aten/src/ATen/SparseCPUIntType.h, ../aten/src/ATen/SparseCPULongType.cpp, ../aten/src/ATen/SparseCPULongType.h, ../aten/src/ATen/SparseCPUShortType.cpp, ../aten/src/ATen/SparseCPUShortType.h, ../aten/src/ATen/TypeDefault.cpp, ../aten/src/ATen/TypeDefault.h, ../aten/src/ATen/TypeExtendedInterface.h, ../aten/src/ATen/CUDAByteType.cpp, ../aten/src/ATen/CUDAByteType.h, ../aten/src/ATen/CUDACharType.cpp, ../aten/src/ATen/CUDACharType.h, ../aten/src/ATen/CUDACopy.cpp, ../aten/src/ATen/CUDADoubleType.cpp, ../aten/src/ATen/CUDADoubleType.h, ../aten/src/ATen/CUDAFloatType.cpp, ../aten/src/ATen/CUDAFloatType.h, ../aten/src/ATen/CUDAGenerator.h, ../aten/src/ATen/CUDAHalfType.cpp, ../aten/src/ATen/CUDAHalfType.h, ../aten/src/ATen/CUDAIntType.cpp, ../aten/src/ATen/CUDAIntType.h, ../aten/src/ATen/CUDALongType.cpp, ../aten/src/ATen/CUDALongType.h, ../aten/src/ATen/CUDAShortType.cpp, ../aten/src/ATen/CUDAShortType.h, ../aten/src/ATen/RegisterCUDA.cpp, ../aten/src/ATen/RegisterCUDA.h, ../aten/src/ATen/SparseCUDAByteType.cpp, ../aten/src/ATen/SparseCUDAByteType.h, ../aten/src/ATen/SparseCUDACharType.cpp, ../aten/src/ATen/SparseCUDACharType.h, ../aten/src/ATen/SparseCUDADoubleType.cpp, ../aten/src/ATen/SparseCUDADoubleType.h, ../aten/src/ATen/SparseCUDAFloatType.cpp, ../aten/src/ATen/SparseCUDAFloatType.h, ../aten/src/ATen/SparseCUDAIntType.cpp, ../aten/src/ATen/SparseCUDAIntType.h, ../aten/src/ATen/SparseCUDALongType.cpp, ../aten/src/ATen/SparseCUDALongType.h, ../aten/src/ATen/SparseCUDAShortType.cpp, ../aten/src/ATen/SparseCUDAShortType.h\r\n[  5%] Building CXX object c10/CMakeFiles/c10.dir/DeviceType.cpp.o\r\n[  5%] Building CXX object c10/CMakeFiles/c10.dir/Half.cpp.o\r\n[  5%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/buffer.cc.o\r\n[  5%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/context.cc.o\r\n[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/any.cc.o\r\n[  5%] Linking CXX static library ../../../lib/libbenchmark.a\r\n[  5%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/convolution.cpp.o\r\n[  5%] Built target benchmark\r\n[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/any.pb.cc.o\r\n[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/api.pb.cc.o\r\n[  5%] Building CXX object c10/CMakeFiles/c10.dir/Stream.cpp.o\r\n[  5%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/device.cc.o\r\n[  5%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/pair.cc.o\r\n[  5%] Building CXX object c10/CMakeFiles/c10.dir/impl/DeviceGuardImplInterface.cpp.o\r\n[  5%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/convolution_relu.cpp.o\r\n[  5%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/unbound_buffer.cc.o\r\n[  5%] Building CXX object c10/CMakeFiles/c10.dir/util/Array.cpp.o\r\n[  5%] Building CXX object c10/CMakeFiles/c10.dir/util/Backtrace.cpp.o\r\n[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/compiler/importer.cc.o\r\n[  5%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/tcp/address.cc.o\r\n[  5%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/deconvolution.cpp.o\r\n[  5%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/eltwise.cpp.o\r\n[  5%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/tcp/buffer.cc.o\r\n[  5%] Building CXX object c10/CMakeFiles/c10.dir/util/C++17.cpp.o\r\n[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/compiler/parser.cc.o\r\n[  5%] Building CXX object c10/CMakeFiles/c10.dir/util/Exception.cpp.o\r\n[  6%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/descriptor.cc.o\r\n[  6%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/engine.cpp.o\r\nScanning dependencies of target common\r\n[  6%] Building C object sleef/src/common/CMakeFiles/common.dir/common.c.o\r\n[  6%] Built target common\r\nScanning dependencies of target mkrename\r\n[  6%] Building C object sleef/src/libm/CMakeFiles/mkrename.dir/mkrename.c.o\r\n[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/tcp/context.cc.o\r\n[  7%] Built target ATEN_CPU_FILES_GEN_TARGET\r\n[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/tcp/device.cc.o\r\n[  7%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/inner_product.cpp.o\r\n[  7%] Building CXX object c10/CMakeFiles/c10.dir/util/LeftRight.cpp.o\r\n[  7%] Linking C executable ../../bin/mkrename\r\n[  7%] Built target mkrename\r\n[  8%] Building CXX object c10/CMakeFiles/c10.dir/util/Logging.cpp.o\r\n[  8%] Building CXX object c10/CMakeFiles/c10.dir/util/Metaprogramming.cpp.o\r\n[  8%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/lrn.cpp.o\r\n[  8%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/memory.cpp.o\r\n[  8%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/descriptor.pb.cc.o\r\n[  8%] Building CXX object c10/CMakeFiles/c10.dir/util/Optional.cpp.o\r\n[  8%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/memory_desc_wrapper.cpp.o\r\n[  9%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/mkldnn_debug.cpp.o\r\n[  9%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/tcp/pair.cc.o\r\n[  9%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/descriptor_database.cc.o\r\n[  9%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/duration.pb.cc.o\r\n[  9%] Building CXX object c10/CMakeFiles/c10.dir/util/SmallVector.cpp.o\r\n[  9%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/pooling.cpp.o\r\n[  9%] Building CXX object c10/CMakeFiles/c10.dir/util/StringUtil.cpp.o\r\n[  9%] Building CXX object c10/CMakeFiles/c10.dir/util/Type.cpp.o\r\n[  9%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/primitive.cpp.o\r\n[  9%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/primitive_attr.cpp.o\r\n[  9%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/primitive_desc.cpp.o\r\n[  9%] Building CXX object c10/CMakeFiles/c10.dir/util/TypeList.cpp.o\r\n[  9%] Building CXX object c10/CMakeFiles/c10.dir/util/TypeTraits.cpp.o\r\n[  9%] Building CXX object c10/CMakeFiles/c10.dir/util/flags_use_gflags.cpp.o\r\n[  9%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/primitive_iterator.cpp.o\r\n[  9%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/tcp/unbound_buffer.cc.o\r\n[  9%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/query.cpp.o\r\n[  9%] Building CXX object c10/CMakeFiles/c10.dir/util/flags_use_no_gflags.cpp.o\r\n[  9%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/reorder.cpp.o\r\n[  9%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/rnn.cpp.o\r\n[  9%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/dynamic_message.cc.o\r\n[  9%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/empty.pb.cc.o\r\n[  9%] Building CXX object c10/CMakeFiles/c10.dir/util/typeid.cpp.o\r\n[  9%] Linking CXX static library ../../../lib/libgloo.a\r\n[  9%] Built target gloo\r\n[  9%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/extension_set_heavy.cc.o\r\nScanning dependencies of target mkdisp\r\n[  9%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/scratchpad.cpp.o\r\n[  9%] Building C object sleef/src/libm/CMakeFiles/mkdisp.dir/mkdisp.c.o\r\nScanning dependencies of target renamedsp256.h_generated\r\n[  9%] Linking C executable ../../bin/mkdisp\r\n[  9%] Generating renamedsp256.h\r\n[  9%] Built target renamedsp256.h_generated\r\n[  9%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/shuffle.cpp.o\r\n[  9%] Built target mkdisp\r\n[  9%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/softmax.cpp.o\r\nScanning dependencies of target renameSSE4.h_generated\r\n[  9%] Generating include/renamesse4.h\r\nGenerating renamesse4.h: mkrename 2 4 sse4\r\n[  9%] Built target renameSSE4.h_generated\r\nScanning dependencies of target renameAVX.h_generated\r\n[  9%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/stream.cpp.o\r\n[  9%] Generating include/renameavx.h\r\nGenerating renameavx.h: mkrename 4 8 avx\r\n[  9%] Built target renameAVX.h_generated\r\n[  9%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/utils.cpp.o\r\n[  9%] Built target python_copy_files\r\n[  9%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/verbose.cpp.o\r\n[  9%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/cpu_barrier.cpp.o\r\nScanning dependencies of target renameFMA4.h_generated\r\n[  9%] Generating include/renamefma4.h\r\nGenerating renamefma4.h: mkrename 4 8 fma4\r\n[  9%] Built target renameFMA4.h_generated\r\n[ 10%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/cpu_batch_normalization_utils.cpp.o\r\nScanning dependencies of target renameAVX2128.h_generated\r\n[ 10%] Generating include/renameavx2128.h\r\nGenerating renameavx2128.h: mkrename 2 4 avx2128\r\n[ 10%] Built target renameAVX2128.h_generated\r\n[ 10%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/cpu_concat.cpp.o\r\n[ 10%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/field_mask.pb.cc.o\r\n[ 10%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/generated_message_reflection.cc.o\r\n[ 10%] Linking CXX shared library ../lib/libc10.so\r\n[ 10%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/generated_message_table_driven.cc.o\r\n[ 10%] Built target c10\r\n[ 10%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/io/gzip_stream.cc.o\r\nScanning dependencies of target renameAVX2.h_generated\r\n[ 10%] Generating include/renameavx2.h\r\nGenerating renameavx2.h: mkrename 4 8 avx2\r\n[ 10%] Built target renameAVX2.h_generated\r\n[ 10%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/io/printer.cc.o\r\nScanning dependencies of target mkalias\r\n[ 10%] Building C object sleef/src/libm/CMakeFiles/mkalias.dir/mkalias.c.o\r\n[ 10%] Linking C executable ../../bin/mkalias\r\n[ 10%] Built target mkalias\r\n[ 10%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/cpu_engine.cpp.o\r\n[ 10%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/cpu_memory.cpp.o\r\n[ 10%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/io/strtod.cc.o\r\n[ 10%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/cpu_reducer.cpp.o\r\n[ 10%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/cpu_reorder.cpp.o\r\n[ 10%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/cpu_sum.cpp.o\r\n[ 10%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/io/tokenizer.cc.o\r\n[ 10%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/io/zero_copy_stream_impl.cc.o\r\n[ 10%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/gemm/gemm.cpp.o\r\n[ 10%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/gemm/gemm_utils.cpp.o\r\n[ 11%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/map_field.cc.o\r\n[ 11%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/gemm/jit_avx512_common_gemm_f32.cpp.o\r\n[ 11%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/message.cc.o\r\n[ 11%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/reflection_ops.cc.o\r\n[ 11%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/service.cc.o\r\n[ 11%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/gemm/jit_avx_gemm_f32.cpp.o\r\n[ 11%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/gemm/ref_gemm.cpp.o\r\nScanning dependencies of target renameAVX512F.h_generated\r\n[ 11%] Generating include/renameavx512f.h\r\nGenerating renameavx512f.h: mkrename 8 16 avx512f\r\n[ 11%] Built target renameAVX512F.h_generated\r\n[ 11%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/source_context.pb.cc.o\r\nScanning dependencies of target renamedsp128.h_generated\r\n[ 11%] Generating renamedsp128.h\r\n[ 11%] Built target renamedsp128.h_generated\r\n[ 11%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/struct.pb.cc.o\r\nScanning dependencies of target dispsse.c_generated\r\n[ 11%] Generating dispsse.c\r\n[ 11%] Built target dispsse.c_generated\r\n[ 11%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/gemm_convolution.cpp.o\r\nScanning dependencies of target renameSSE2.h_generated\r\n[ 11%] Generating include/renamesse2.h\r\nGenerating renamesse2.h: mkrename 2 4 sse2\r\n[ 11%] Built target renameSSE2.h_generated\r\n[ 11%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/gemm_convolution_utils.cpp.o\r\nScanning dependencies of target mkrename_gnuabi\r\n[ 11%] Building C object sleef/src/libm/CMakeFiles/mkrename_gnuabi.dir/mkrename_gnuabi.c.o\r\n[ 11%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/mathlimits.cc.o\r\n[ 11%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/substitute.cc.o\r\n[ 11%] Linking C executable ../../bin/mkrename_gnuabi\r\n[ 11%] Built target mkrename_gnuabi\r\n[ 11%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/text_format.cc.o\r\nScanning dependencies of target mkmasked_gnuabi\r\n[ 11%] Building C object sleef/src/libm/CMakeFiles/mkmasked_gnuabi.dir/mkmasked_gnuabi.c.o\r\n[ 11%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/gemm_inner_product.cpp.o\r\n[ 11%] Linking C executable ../../bin/mkmasked_gnuabi\r\n[ 11%] Built target mkmasked_gnuabi\r\n[ 11%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/gemm_u8s8s32x_inner_product.cpp.o\r\nScanning dependencies of target arraymap\r\n[ 11%] Building C object sleef/src/common/CMakeFiles/arraymap.dir/arraymap.c.o\r\n[ 11%] Built target arraymap\r\n[ 12%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/gemm_x8s8s32x_convolution.cpp.o\r\nScanning dependencies of target torch_shm_manager\r\n[ 13%] Building CXX object caffe2/torch/lib/libshm/CMakeFiles/torch_shm_manager.dir/manager.cpp.o\r\n[ 13%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/timestamp.pb.cc.o\r\n[ 13%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/type.pb.cc.o\r\n[ 13%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx2_1x1_conv_kernel_f32.cpp.o\r\n[ 13%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx2_1x1_convolution.cpp.o\r\n[ 13%] Linking CXX executable ../../../../bin/torch_shm_manager\r\n[ 13%] Built target torch_shm_manager\r\n[ 13%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/unknown_field_set.cc.o\r\nScanning dependencies of target c10_utils_gpu\r\n[ 13%] Building CXX object caffe2/utils/CMakeFiles/c10_utils_gpu.dir/dummy.cpp.o\r\n[ 13%] Built target c10_utils_gpu\r\n[ 13%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx2_conv_kernel_f32.cpp.o\r\n[ 13%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/delimited_message_util.cc.o\r\n[ 13%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/field_comparator.cc.o\r\n[ 13%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/field_mask_util.cc.o\r\n[ 13%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/datapiece.cc.o\r\n[ 14%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/default_value_objectwriter.cc.o\r\nScanning dependencies of target c10_utils_hip\r\n[ 14%] Building CXX object caffe2/utils/CMakeFiles/c10_utils_hip.dir/dummy.cpp.o\r\n[ 14%] Built target c10_utils_hip\r\n[ 14%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx2_convolution.cpp.o\r\n[ 14%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_common_1x1_conv_kernel.cpp.o\r\n[ 14%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/error_listener.cc.o\r\n[ 14%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_common_1x1_convolution.cpp.o\r\n[ 14%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_common_conv_kernel.cpp.o\r\n[ 14%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/field_mask_utility.cc.o\r\n[ 14%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/json_escaping.cc.o\r\n[ 14%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_common_conv_winograd_kernel_f32.cpp.o\r\n[ 14%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/json_objectwriter.cc.o\r\n[ 14%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/json_stream_parser.cc.o\r\n[ 14%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_common_convolution.cpp.o\r\n[ 14%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/object_writer.cc.o\r\n[ 14%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_common_convolution_winograd.cpp.o\r\n[ 14%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/proto_writer.cc.o\r\n[ 14%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/protostream_objectsource.cc.o\r\n[ 14%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/protostream_objectwriter.cc.o\r\nScanning dependencies of target c10_utils_cpu\r\n[ 14%] Building CXX object caffe2/utils/CMakeFiles/c10_utils_cpu.dir/dummy.cpp.o\r\n[ 14%] Built target c10_utils_cpu\r\n[ 14%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/type_info.cc.o\r\n[ 14%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/type_info_test_helper.cc.o\r\n[ 14%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_common_lrn.cpp.o\r\n[ 14%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_core_fp32_wino_conv_2x3.cpp.o\r\nScanning dependencies of target cpuinfo\r\n[ 14%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/init.c.o\r\n[ 14%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/api.c.o\r\n[ 15%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/init.c.o\r\n[ 15%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/utility.cc.o\r\n[ 15%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/json_util.cc.o\r\n[ 15%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/info.c.o\r\n[ 15%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/vendor.c.o\r\n[ 15%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/uarch.c.o\r\n[ 15%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/name.c.o\r\n[ 15%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_core_fp32_wino_conv_4x3.cpp.o\r\n[ 15%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/topology.c.o\r\n[ 15%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/isa.c.o\r\n[ 15%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/cache/init.c.o\r\n[ 15%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/cache/descriptor.c.o\r\n[ 15%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/cache/deterministic.c.o\r\n[ 15%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/linux/init.c.o\r\n[ 15%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/linux/cpuinfo.c.o\r\n[ 15%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/linux/smallfile.c.o\r\n[ 15%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/linux/multiline.c.o\r\n[ 15%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/linux/current.c.o\r\n[ 15%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/message_differencer.cc.o\r\n[ 15%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/time_util.cc.o\r\n[ 15%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/linux/cpulist.c.o\r\n[ 16%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/linux/processors.c.o\r\n[ 16%] Linking C static library ../../lib/libcpuinfo.a\r\n[ 16%] Built target cpuinfo\r\n[ 16%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_core_fp32_wino_conv_4x3_kernel.cpp.o\r\n[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/type_resolver_util.cc.o\r\n[ 17%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_core_i8i8_pooling.cpp.o\r\n[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/wire_format.cc.o\r\n[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/wrappers.pb.cc.o\r\n[ 18%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_core_u8s8s32x_deconvolution.cpp.o\r\nScanning dependencies of target nnpack_reference_layers\r\n[ 18%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack_reference_layers.dir/src/ref/convolution-output.c.o\r\n[ 18%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack_reference_layers.dir/src/ref/convolution-input-gradient.c.o\r\n[ 18%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack_reference_layers.dir/src/ref/convolution-kernel.c.o\r\n[ 18%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack_reference_layers.dir/src/ref/fully-connected-output.c.o\r\n[ 18%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack_reference_layers.dir/src/ref/max-pooling-output.c.o\r\n[ 18%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack_reference_layers.dir/src/ref/softmax-output.c.o\r\nScanning dependencies of target gtest_main\r\n[ 18%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack_reference_layers.dir/src/ref/relu-output.c.o\r\n[ 18%] Building CXX object third_party/googletest/googletest/CMakeFiles/gtest_main.dir/src/gtest_main.cc.o\r\n[ 18%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack_reference_layers.dir/src/ref/relu-input-gradient.c.o\r\n[ 18%] Linking C static library ../../lib/libnnpack_reference_layers.a\r\n[ 18%] Built target nnpack_reference_layers\r\n[ 18%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_core_u8s8s32x_wino_convolution.cpp.o\r\n[ 18%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_core_x8s8s32x_1x1_conv_kernel.cpp.o\r\n[ 18%] Linking CXX static library ../../../lib/libgtest_main.a\r\n[ 18%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_core_x8s8s32x_1x1_convolution.cpp.o\r\n[ 18%] Built target gtest_main\r\n[ 18%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_core_x8s8s32x_conv_kernel.cpp.o\r\n[ 18%] Linking CXX static library ../../../lib/libprotobuf.a\r\n[ 18%] Built target libprotobuf\r\nScanning dependencies of target benchmark_main\r\n[ 18%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark_main.dir/benchmark_main.cc.o\r\n[ 18%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_core_x8s8s32x_convolution.cpp.o\r\n[ 18%] Linking CXX static library ../../../lib/libbenchmark_main.a\r\n[ 18%] Built target benchmark_main\r\nScanning dependencies of target gloo_builder\r\n[ 18%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo_builder.dir/allreduce_builder.cc.o\r\n[ 18%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_sse42_1x1_conv_kernel_f32.cpp.o\r\n[ 18%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_sse42_1x1_convolution.cpp.o\r\n[ 18%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo_builder.dir/broadcast_builder.cc.o\r\n[ 18%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_sse42_conv_kernel_f32.cpp.o\r\n[ 18%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_sse42_convolution.cpp.o\r\n[ 18%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_transpose_src_utils.cpp.o\r\n[ 18%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_batch_normalization.cpp.o\r\n[ 18%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_dw_conv_kernel_f32.cpp.o\r\n[ 18%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_dw_convolution.cpp.o\r\n[ 18%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_eltwise.cpp.o\r\n[ 18%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_lrn.cpp.o\r\n[ 19%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_lrn_kernel_f32.cpp.o\r\n[ 19%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_pool_kernel_f32.cpp.o\r\n[ 19%] Linking CXX static library ../../../lib/libgloo_builder.a\r\n[ 19%] Built target gloo_builder\r\n[ 19%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_pooling.cpp.o\r\n[ 19%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_reorder.cpp.o\r\n[ 19%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_reorder_utils.cpp.o\r\n[ 19%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/nchw_pooling.cpp.o\r\n[ 19%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ncsp_batch_normalization.cpp.o\r\n[ 19%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/nhwc_pooling.cpp.o\r\n[ 19%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/nspc_batch_normalization.cpp.o\r\n[ 19%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ref_batch_normalization.cpp.o\r\n[ 19%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ref_convolution.cpp.o\r\n[ 19%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ref_deconvolution.cpp.o\r\n[ 19%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ref_eltwise.cpp.o\r\n[ 19%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ref_inner_product.cpp.o\r\n[ 19%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ref_lrn.cpp.o\r\nScanning dependencies of target onnxifi_wrapper\r\n[ 19%] Building C object third_party/onnx/CMakeFiles/onnxifi_wrapper.dir/onnx/onnxifi_wrapper.c.o\r\n[ 19%] Linking C shared module ../../lib/libonnxifi.so\r\n[ 19%] Built target onnxifi_wrapper\r\nScanning dependencies of target c10_InlineDeviceGuard_test\r\n[ 20%] Building CXX object c10/test/CMakeFiles/c10_InlineDeviceGuard_test.dir/impl/InlineDeviceGuard_test.cpp.o\r\n[ 21%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ref_pooling.cpp.o\r\n[ 21%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ref_rnn.cpp.o\r\n[ 21%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ref_shuffle.cpp.o\r\n[ 21%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ref_softmax.cpp.o\r\n[ 21%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/simple_concat.cpp.o\r\n[ 21%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/simple_sum.cpp.o\r\nScanning dependencies of target c10_InlineStreamGuard_test\r\n[ 21%] Building CXX object c10/test/CMakeFiles/c10_InlineStreamGuard_test.dir/impl/InlineStreamGuard_test.cpp.o\r\n[ 21%] Linking CXX executable ../../bin/c10_InlineDeviceGuard_test\r\n[ 21%] Built target c10_InlineDeviceGuard_test\r\nScanning dependencies of target c10_flags_test\r\n[ 21%] Building CXX object c10/test/CMakeFiles/c10_flags_test.dir/flags_test.cpp.o\r\nScanning dependencies of target c10_TypeList_test\r\n[ 21%] Building CXX object c10/test/CMakeFiles/c10_TypeList_test.dir/util/TypeList_test.cpp.o\r\nScanning dependencies of target c10_registry_test\r\nScanning dependencies of target c10_DeviceGuard_test\r\n[ 21%] Building CXX object c10/test/CMakeFiles/c10_registry_test.dir/registry_test.cpp.o\r\n[ 21%] Building CXX object c10/test/CMakeFiles/c10_DeviceGuard_test.dir/DeviceGuard_test.cpp.o\r\nScanning dependencies of target c10_OpSchema_test\r\n[ 21%] Building CXX object c10/test/CMakeFiles/c10_OpSchema_test.dir/dispatch/OpSchema_test.cpp.o\r\nScanning dependencies of target c10_StreamGuard_test\r\n[ 21%] Building CXX object c10/test/CMakeFiles/c10_StreamGuard_test.dir/StreamGuard_test.cpp.o\r\n[ 21%] Linking CXX shared library ../../../../lib/libmkldnn.so\r\n[ 21%] Linking CXX executable ../../bin/c10_OpSchema_test\r\n[ 21%] Built target c10_OpSchema_test\r\nScanning dependencies of target c10_logging_test\r\n[ 22%] Building CXX object c10/test/CMakeFiles/c10_logging_test.dir/logging_test.cpp.o\r\n[ 22%] Linking CXX executable ../../bin/c10_flags_test\r\n[ 22%] Built target c10_flags_test\r\n[ 22%] Linking CXX executable ../../bin/c10_InlineStreamGuard_test\r\n[ 22%] Linking CXX executable ../../bin/c10_StreamGuard_test\r\nScanning dependencies of target c10_Array_test\r\n[ 22%] Building CXX object c10/test/CMakeFiles/c10_Array_test.dir/util/Array_test.cpp.o\r\n[ 22%] Linking CXX executable ../../bin/c10_TypeList_test\r\n[ 22%] Built target c10_InlineStreamGuard_test\r\n[ 22%] Built target c10_StreamGuard_test\r\n[ 22%] Built target c10_TypeList_test\r\n[ 22%] Built target mkldnn\r\nScanning dependencies of target c10_TypeTraits_test\r\nScanning dependencies of target c10_Metaprogramming_test\r\n[ 22%] Building CXX object c10/test/CMakeFiles/c10_Metaprogramming_test.dir/util/Metaprogramming_test.cpp.o\r\nScanning dependencies of target __aten_op_header_gen\r\n[ 22%] Building CXX object c10/test/CMakeFiles/c10_TypeTraits_test.dir/util/TypeTraits_test.cpp.o\r\n[ 23%] Generating contrib/aten/aten_op.h\r\nScanning dependencies of target c10_typeid_test\r\n[ 23%] Building CXX object c10/test/CMakeFiles/c10_typeid_test.dir/util/typeid_test.cpp.o\r\n[ 23%] Linking CXX executable ../../bin/c10_DeviceGuard_test\r\n[ 23%] Linking CXX executable ../../bin/c10_registry_test\r\n[ 23%] Built target c10_DeviceGuard_test\r\nScanning dependencies of target headers\r\n[ 24%] Generating ../../../include/sleef.h\r\nGenerating sleef.h: mkrename 2 4 __m128d __m128 __m128i __m128i __SSE2__\r\nGenerating sleef.h: mkrename 2 4 __m128d __m128 __m128i __m128i __SSE2__ sse2\r\nGenerating sleef.h: mkrename 2 4 __m128d __m128 __m128i __m128i __SSE2__ sse4\r\nGenerating sleef.h: mkrename 4 8 __m256d __m256 __m128i struct\\ {\\ __m128i\\ x,\\ y;\\ } __AVX__\r\n[ 24%] Built target c10_registry_test\r\nGenerating sleef.h: mkrename 4 8 __m256d __m256 __m128i struct\\ {\\ __m128i\\ x,\\ y;\\ } __AVX__ avx\r\nGenerating sleef.h: mkrename 4 8 __m256d __m256 __m128i struct\\ {\\ __m128i\\ x,\\ y;\\ } __AVX__ fma4\r\nGenerating sleef.h: mkrename 4 8 __m256d __m256 __m128i __m256i __AVX__ avx2\r\nGenerating sleef.h: mkrename 2 4 __m128d __m128 __m128i __m128i __SSE2__ avx2128\r\nGenerating sleef.h: mkrename 8 16 __m512d __m512 __m256i __m512i __AVX512F__\r\nScanning dependencies of target dispavx.c_generated\r\nGenerating sleef.h: mkrename 8 16 __m512d __m512 __m256i __m512i __AVX512F__ avx512f\r\n[ 24%] Generating dispavx.c\r\n[ 24%] Built target headers\r\n[ 24%] Built target dispavx.c_generated\r\nScanning dependencies of target sleefsse4\r\n[ 24%] Building C object sleef/src/libm/CMakeFiles/sleefsse4.dir/sleefsimdsp.c.o\r\nScanning dependencies of target sleefavx\r\n[ 24%] Building C object sleef/src/libm/CMakeFiles/sleefavx.dir/sleefsimdsp.c.o\r\nSkipping _th_multinomial Because of Arg: Generator * (Generator*) \r\nSkipping _th_normal Because of Arg: Generator * (Generator*) \r\nSkipping _th_normal Because of Arg: Generator * (Generator*) \r\nSkipping _th_normal Because of Arg: Generator * (Generator*) \r\nSkipping _th_tensor Because of Arg: Storage (Storage) \r\nSkipping _th_tensor Because of Arg: Storage (Storage) \r\nSkipping rrelu_with_noise Because of Arg: Generator * (Generator*) \r\nSkipping rrelu_with_noise_forward Because of Arg: Generator * (Generator*) \r\nSkipping thnn_conv_transpose2d_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) \r\nSkipping thnn_conv_transpose3d_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) \r\nSkipping thnn_conv2d_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) \r\nSkipping thnn_conv_depthwise2d_backward Because of Arg: std::array<bool,2> (std::array<bool,2>) \r\nSkipping thnn_conv3d_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) \r\nSkipping thnn_conv_dilated2d_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) \r\nSkipping thnn_conv_dilated3d_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) \r\nSkipping _cudnn_rnn_backward Because of Arg: std::array<bool,4> (std::array<bool,4>) \r\nSkipping _cudnn_init_dropout_state because it is a factory method\r\nSkipping _fused_dropout Because of Arg: Generator * (Generator *) \r\nSkipping arange because it is a factory method\r\nSkipping bartlett_window because it is a factory method\r\nSkipping bernoulli Because of Arg: Generator * (Generator *) \r\nSkipping bernoulli Because of Arg: Generator * (Generator *) \r\nSkipping blackman_window because it is a factory method\r\nSkipping clamp Because of Arg: c10::optional<Scalar> (Scalar) \r\nSkipping clamp Because of Arg: c10::optional<Scalar> (Scalar) \r\nSkipping _convolution_double_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) \r\nSkipping cudnn_convolution_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) \r\nSkipping cudnn_convolution_transpose_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) \r\nSkipping cumsum Because of Arg: ScalarType (ScalarType) \r\nSkipping cumprod Because of Arg: ScalarType (ScalarType) \r\nSkipping einsum Because of Arg: std::string (std::string) \r\nSkipping empty because it is a factory method\r\nSkipping empty_like because it is a factory method\r\nSkipping empty_strided because it is a factory method\r\nSkipping eye because it is a factory method\r\nSkipping full because it is a factory method\r\nSkipping full_like because it is a factory method\r\nSkipping hann_window because it is a factory method\r\nSkipping hamming_window because it is a factory method\r\nSkipping _cufft_set_plan_cache_max_size Because of Ret: void (void)\r\nSkipping _cufft_clear_plan_cache Because of Ret: void (void)\r\nSkipping linspace because it is a factory method\r\nSkipping logspace because it is a factory method\r\nSkipping log_softmax Because of Arg: ScalarType (ScalarType) \r\nSkipping mean Because of Arg: ScalarType (ScalarType) \r\nSkipping mean Because of Arg: ScalarType (ScalarType) \r\nSkipping mean Because of Arg: ScalarType (ScalarType) \r\nSkipping mkldnn_convolution_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) \r\nSkipping miopen_convolution_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) \r\nSkipping miopen_convolution_transpose_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) \r\nSkipping native_batch_norm_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) \r\nSkipping ones because it is a factory method\r\nSkipping ones_like because it is a factory method\r\nSkipping rand because it is a factory method\r\nSkipping rand_like because it is a factory method\r\nSkipping randint because it is a factory method\r\nSkipping randint_like because it is a factory method\r\nSkipping randn because it is a factory method\r\nSkipping randn_like because it is a factory method\r\nSkipping randperm because it is a factory method\r\nSkipping range because it is a factory method\r\nSkipping rrelu Because of Arg: Generator * (Generator *) \r\nSkipping softmax Because of Arg: ScalarType (ScalarType) \r\nSkipping sum Because of Arg: ScalarType (ScalarType) \r\nSkipping sum Because of Arg: ScalarType (ScalarType) \r\nSkipping sum Because of Arg: ScalarType (ScalarType) \r\nSkipping prod Because of Arg: ScalarType (ScalarType) \r\nSkipping prod Because of Arg: ScalarType (ScalarType) \r\nSkipping prod Because of Arg: ScalarType (ScalarType) \r\nSkipping zeros because it is a factory method\r\nSkipping zeros_like because it is a factory method\r\nSkipping _standard_gamma Because of Arg: Generator * (Generator *) \r\nSkipping poisson Because of Arg: Generator * (Generator *) \r\nSkipping sparse_coo_tensor because it is a factory method\r\nSkipping _sparse_coo_tensor_unsafe because it is a factory method\r\nSkipping _sparse_coo_tensor_with_dims because it is a factory method\r\nSkipping _sparse_coo_tensor_with_dims_and_tensors because it is a factory method\r\nSkipping sparse_mask Because of Arg: SparseTensorRef (SparseTensorRef) \r\nSkipping to because it is a factory method\r\nSkipping data_ptr Because of Ret: void* (void*)\r\nSkipping multinomial Because of Arg: Generator * (Generator *) \r\nSkipping normal Because of Arg: Generator * (Generator *) \r\nSkipping normal Because of Arg: Generator * (Generator *) \r\nSkipping normal Because of Arg: Generator * (Generator *) \r\n[ 24%] Built target __aten_op_header_gen\r\n[ 24%] Building C object sleef/src/libm/CMakeFiles/sleefavx.dir/sleefsimddp.c.o\r\n[ 24%] Linking CXX executable ../../bin/c10_Array_test\r\n[ 24%] Built target c10_Array_test\r\n[ 24%] Building C object sleef/src/libm/CMakeFiles/sleefsse4.dir/sleefsimddp.c.o\r\n[ 24%] Linking CXX executable ../../bin/c10_TypeTraits_test\r\n[ 24%] Built target c10_TypeTraits_test\r\nScanning dependencies of target sleeffma4\r\n[ 24%] Linking CXX executable ../../bin/c10_logging_test\r\n[ 24%] Building C object sleef/src/libm/CMakeFiles/sleeffma4.dir/sleefsimdsp.c.o\r\nScanning dependencies of target sleefavx2128\r\n[ 24%] Building C object sleef/src/libm/CMakeFiles/sleefavx2128.dir/sleefsimdsp.c.o\r\n[ 24%] Built target c10_logging_test\r\n[ 24%] Building C object sleef/src/libm/CMakeFiles/sleeffma4.dir/sleefsimddp.c.o\r\n[ 24%] Built target sleefavx\r\nScanning dependencies of target sleefavx2\r\nScanning dependencies of target alias_avx512f.h_generated\r\n[ 24%] Building C object sleef/src/libm/CMakeFiles/sleefavx2.dir/sleefsimdsp.c.o\r\n[ 24%] Generating alias_avx512f.h\r\n[ 24%] Built target alias_avx512f.h_generated\r\n[ 24%] Building C object sleef/src/libm/CMakeFiles/sleefavx2.dir/sleefsimddp.c.o\r\n[ 24%] Linking CXX executable ../../bin/c10_Metaprogramming_test\r\n[ 24%] Built target c10_Metaprogramming_test\r\n[ 25%] Building C object sleef/src/libm/CMakeFiles/sleefavx2128.dir/sleefsimddp.c.o\r\n[ 25%] Built target sleefsse4\r\nScanning dependencies of target dispsse_obj\r\n[ 25%] Building C object sleef/src/libm/CMakeFiles/dispsse_obj.dir/dispsse.c.o\r\n[ 25%] Linking CXX executable ../../bin/c10_typeid_test\r\n[ 25%] Built target c10_typeid_test\r\nScanning dependencies of target c10_utils_cpu_test\r\nScanning dependencies of target c10_utils_gpu_test\r\nScanning dependencies of target sleefsse2\r\n[ 25%] Linking CXX executable ../../bin/c10_utils_cpu_test\r\n[ 25%] Linking CXX executable ../../bin/c10_utils_gpu_test\r\n[ 25%] Building C object sleef/src/libm/CMakeFiles/sleefsse2.dir/sleefsimdsp.c.o\r\n[ 25%] Built target c10_utils_gpu_test\r\nScanning dependencies of target c10_utils_hip_test\r\n[ 25%] Built target c10_utils_cpu_test\r\n[ 25%] Building C object sleef/src/libm/CMakeFiles/sleefsse2.dir/sleefsimddp.c.o\r\n[ 25%] Building CXX object caffe2/utils/CMakeFiles/c10_utils_hip_test.dir/dummy.cpp.o\r\n[ 25%] Generating ../../../../third_party/protobuf/src/google/protobuf/compiler/js/well_known_types_embed.cc\r\n[ 25%] Built target sleefavx2\r\n[ 25%] Linking CXX executable ../../bin/c10_utils_hip_test\r\nScanning dependencies of target qnnpack\r\n[ 25%] Building C object confu-deps/QNNPACK/CMakeFiles/qnnpack.dir/src/init.c.o\r\n[ 25%] Built target c10_utils_hip_test\r\n[ 25%] Building C object confu-deps/QNNPACK/CMakeFiles/qnnpack.dir/src/convolution.c.o\r\n[ 25%] Generating src/x86_64-fma/2d-fourier-8x8.py.o\r\nScanning dependencies of target libprotoc\r\n[ 25%] Built target sleeffma4\r\n[ 25%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/code_generator.cc.o\r\n[ 25%] Building C object confu-deps/QNNPACK/CMakeFiles/qnnpack.dir/src/deconvolution.c.o\r\n[ 25%] Built target sleefavx2128\r\n[ 25%] Building C object confu-deps/QNNPACK/CMakeFiles/qnnpack.dir/src/fully-connected.c.o\r\n[ 26%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/command_line_interface.cc.o\r\n[ 26%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_enum.cc.o\r\n[ 26%] Building C object confu-deps/QNNPACK/CMakeFiles/qnnpack.dir/src/sgemm/6x8-psimd.c.o\r\n[ 26%] Building C object confu-deps/QNNPACK/CMakeFiles/qnnpack.dir/src/q8gemm/2x4c8-sse2.c.o\r\n[ 26%] Built target dispsse_obj\r\n[ 26%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_enum_field.cc.o\r\n[ 26%] Building C object confu-deps/QNNPACK/CMakeFiles/qnnpack.dir/src/q8gemm/4x4c2-sse2.c.o\r\n[ 26%] Building C object confu-deps/QNNPACK/CMakeFiles/qnnpack.dir/src/q8conv/4x4c2-sse2.c.o\r\nScanning dependencies of target dispavx_obj\r\n[ 26%] Building C object confu-deps/QNNPACK/CMakeFiles/qnnpack.dir/src/q8dw/9c8-sse2.c.o\r\n[ 26%] Building C object sleef/src/libm/CMakeFiles/dispavx_obj.dir/dispavx.c.o\r\n[ 26%] Built target sleefsse2\r\nScanning dependencies of target sleefavx512f\r\n[ 26%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_extension.cc.o\r\n[ 26%] Building C object sleef/src/libm/CMakeFiles/sleefavx512f.dir/sleefsimdsp.c.o\r\n[ 27%] Linking C static library ../../lib/libqnnpack.a\r\n[ 27%] Built target qnnpack\r\n[ 27%] Building C object sleef/src/libm/CMakeFiles/sleefavx512f.dir/sleefsimddp.c.o\r\n[ 27%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_field.cc.o\r\n[ 27%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_file.cc.o\r\n[ 27%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_generator.cc.o\r\n[ 27%] Built target dispavx_obj\r\n[ 27%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_helpers.cc.o\r\n[ 27%] Built target sleefavx512f\r\n[ 27%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_map_field.cc.o\r\nScanning dependencies of target sleef\r\n[ 27%] Building C object sleef/src/libm/CMakeFiles/sleef.dir/sleefdp.c.o\r\n[ 27%] Generating src/x86_64-fma/2d-fourier-16x16.py.o\r\n[ 27%] Building C object sleef/src/libm/CMakeFiles/sleef.dir/sleefsp.c.o\r\n[ 27%] Building C object sleef/src/libm/CMakeFiles/sleef.dir/sleefld.c.o\r\n[ 27%] Building C object sleef/src/libm/CMakeFiles/sleef.dir/sleefqp.c.o\r\n[ 27%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_message.cc.o\r\n[ 27%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_message_field.cc.o\r\n[ 27%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_padding_optimizer.cc.o\r\n[ 27%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_primitive_field.cc.o\r\n[ 27%] Linking C static library ../../lib/libsleef.a\r\n[ 27%] Built target sleef\r\n[ 27%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_service.cc.o\r\n[ 27%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_string_field.cc.o\r\n[ 28%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_doc_comment.cc.o\r\n[ 28%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_enum.cc.o\r\n[ 28%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_enum_field.cc.o\r\n[ 28%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_field_base.cc.o\r\n[ 28%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_generator.cc.o\r\n[ 28%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_helpers.cc.o\r\n[ 28%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_map_field.cc.o\r\n[ 28%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_message.cc.o\r\n[ 28%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_message_field.cc.o\r\n[ 28%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_primitive_field.cc.o\r\n[ 28%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_repeated_message_field.cc.o\r\n[ 28%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_repeated_enum_field.cc.o\r\n[ 28%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_reflection_class.cc.o\r\n[ 28%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_repeated_primitive_field.cc.o\r\n[ 28%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_source_generator_base.cc.o\r\n[ 28%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_wrapper_field.cc.o\r\n[ 29%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_context.cc.o\r\n[ 29%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_doc_comment.cc.o\r\n[ 29%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_enum.cc.o\r\n[ 29%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_enum_field.cc.o\r\n[ 29%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_enum_field_lite.cc.o\r\n[ 29%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_enum_lite.cc.o\r\n[ 29%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_extension.cc.o\r\n[ 29%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_extension_lite.cc.o\r\n[ 29%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_field.cc.o\r\n[ 29%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_file.cc.o\r\n[ 29%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_generator.cc.o\r\n[ 29%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_generator_factory.cc.o\r\n[ 29%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_helpers.cc.o\r\n[ 29%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_lazy_message_field.cc.o\r\n[ 29%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_lazy_message_field_lite.cc.o\r\n[ 29%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_map_field.cc.o\r\n[ 30%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_map_field_lite.cc.o\r\n[ 30%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_message.cc.o\r\n[ 30%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_message_builder.cc.o\r\n[ 30%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_message_builder_lite.cc.o\r\n[ 30%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_message_field.cc.o\r\n[ 30%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_message_field_lite.cc.o\r\n[ 30%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_message_lite.cc.o\r\n[ 30%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_name_resolver.cc.o\r\n[ 30%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_primitive_field.cc.o\r\n[ 30%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_primitive_field_lite.cc.o\r\n[ 30%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_service.cc.o\r\n[ 30%] Generating src/x86_64-fma/2d-winograd-8x8-3x3.py.o\r\n[ 30%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_shared_code_generator.cc.o\r\n[ 30%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_string_field.cc.o\r\n[ 30%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_string_field_lite.cc.o\r\n[ 30%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_enum.cc.o\r\n[ 30%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_enum_field.cc.o\r\n[ 31%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_extension.cc.o\r\n[ 31%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_field.cc.o\r\n[ 31%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_file.cc.o\r\n[ 31%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_generator.cc.o\r\n[ 32%] Generating src/x86_64-fma/blas/s8gemm.py.o\r\n[ 32%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_helpers.cc.o\r\n[ 32%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_map_field.cc.o\r\n[ 32%] Generating src/x86_64-fma/blas/c8gemm.py.o\r\n[ 32%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_message.cc.o\r\n[ 32%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_message_field.cc.o\r\n[ 32%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_primitive_field.cc.o\r\n[ 32%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/js/js_generator.cc.o\r\n[ 32%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/js/well_known_types_embed.cc.o\r\n[ 32%] Generating src/x86_64-fma/blas/s4c6gemm.py.o\r\n[ 32%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_enum.cc.o\r\n[ 32%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_enum_field.cc.o\r\n[ 32%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_extension.cc.o\r\n[ 32%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_field.cc.o\r\n[ 32%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_file.cc.o\r\n[ 32%] Generating src/x86_64-fma/blas/conv1x1.py.o\r\n[ 33%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_generator.cc.o\r\n[ 33%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_helpers.cc.o\r\n[ 33%] Generating src/x86_64-fma/blas/sgemm.py.o\r\n[ 33%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_map_field.cc.o\r\n[ 33%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_message.cc.o\r\n[ 33%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_message_field.cc.o\r\n[ 33%] Generating src/x86_64-fma/max-pooling.py.o\r\n[ 33%] Generating src/x86_64-fma/relu.py.o\r\n[ 33%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_oneof.cc.o\r\n[ 33%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_primitive_field.cc.o\r\n[ 33%] Generating src/x86_64-fma/softmax.py.o\r\n[ 33%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/php/php_generator.cc.o\r\n[ 33%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/plugin.cc.o\r\n[ 33%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/plugin.pb.cc.o\r\n[ 33%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/python/python_generator.cc.o\r\n[ 33%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/ruby/ruby_generator.cc.o\r\n[ 33%] Generating src/x86_64-fma/blas/sdotxf.py.o\r\n[ 33%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/subprocess.cc.o\r\n[ 33%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/zip_writer.cc.o\r\n[ 33%] Generating src/x86_64-fma/blas/shdotxf.py.o\r\n[ 33%] Linking CXX static library ../../../lib/libprotoc.a\r\n[ 33%] Built target libprotoc\r\nScanning dependencies of target protoc\r\n[ 33%] Building CXX object third_party/protobuf/cmake/CMakeFiles/protoc.dir/__/src/google/protobuf/compiler/main.cc.o\r\n[ 33%] Linking CXX executable ../../../bin/protoc\r\n[ 33%] Built target protoc\r\n[ 33%] Running C++/Python protocol buffer compiler on /opt/pytorch/caffe2/proto/metanet.proto\r\n[ 33%] Running C++/Python protocol buffer compiler on /opt/pytorch/caffe2/proto/caffe2.proto\r\nScanning dependencies of target gen_onnx_proto\r\n[ 33%] Running C++/Python protocol buffer compiler on /opt/pytorch/caffe2/proto/torch.proto\r\n[ 33%] Running C++/Python protocol buffer compiler on /opt/pytorch/caffe2/proto/caffe2_legacy.proto\r\n[ 33%] Running C++/Python protocol buffer compiler on /opt/pytorch/caffe2/proto/hsm.proto\r\n[ 33%] Running C++/Python protocol buffer compiler on /opt/pytorch/caffe2/proto/predictor_consts.proto\r\n[ 33%] Running gen_proto.py on onnx/onnx.in.proto\r\nProcessing /opt/pytorch/third_party/onnx/onnx/onnx.in.proto\r\nWriting /opt/pytorch/build/third_party/onnx/onnx/onnx_onnx_torch.proto\r\nWriting /opt/pytorch/build/third_party/onnx/onnx/onnx_onnx_torch.proto3\r\nWriting /opt/pytorch/build/third_party/onnx/onnx/onnx.pb.h\r\ngenerating /opt/pytorch/build/third_party/onnx/onnx/onnx_pb.py\r\n[ 33%] Running C++/Python protocol buffer compiler on /opt/pytorch/caffe2/proto/prof_dag.proto\r\n[ 33%] Running C++ protocol buffer compiler on /opt/pytorch/build/third_party/onnx/onnx/onnx_onnx_torch.proto\r\n[ 33%] Built target gen_onnx_proto\r\n[ 33%] Running gen_proto.py on onnx/onnx-operators.in.proto\r\nScanning dependencies of target Caffe2_PROTO\r\n[ 34%] Building CXX object caffe2/proto/CMakeFiles/Caffe2_PROTO.dir/hsm.pb.cc.o\r\n[ 34%] Building CXX object caffe2/proto/CMakeFiles/Caffe2_PROTO.dir/caffe2.pb.cc.o\r\n[ 34%] Building CXX object caffe2/proto/CMakeFiles/Caffe2_PROTO.dir/metanet.pb.cc.o\r\n[ 34%] Building CXX object caffe2/proto/CMakeFiles/Caffe2_PROTO.dir/prof_dag.pb.cc.o\r\n[ 34%] Building CXX object caffe2/proto/CMakeFiles/Caffe2_PROTO.dir/predictor_consts.pb.cc.o\r\n[ 34%] Building CXX object caffe2/proto/CMakeFiles/Caffe2_PROTO.dir/caffe2_legacy.pb.cc.o\r\nProcessing /opt/pytorch/third_party/onnx/onnx/onnx-operators.in.proto\r\nWriting /opt/pytorch/build/third_party/onnx/onnx/onnx-operators_onnx_torch.proto\r\nWriting /opt/pytorch/build/third_party/onnx/onnx/onnx-operators_onnx_torch.proto3\r\nWriting /opt/pytorch/build/third_party/onnx/onnx/onnx-operators.pb.h\r\ngenerating /opt/pytorch/build/third_party/onnx/onnx/onnx_operators_pb.py\r\n[ 34%] Running C++ protocol buffer compiler on /opt/pytorch/build/third_party/onnx/onnx/onnx-operators_onnx_torch.proto\r\nScanning dependencies of target onnx_proto\r\n[ 34%] Building CXX object third_party/onnx/CMakeFiles/onnx_proto.dir/onnx/onnx_onnx_torch.pb.cc.o\r\nScanning dependencies of target nnpack\r\n[ 34%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/init.c.o\r\n[ 34%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/convolution-inference.c.o\r\n[ 34%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/fully-connected-inference.c.o\r\n[ 34%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/pooling-output.c.o\r\n[ 34%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/relu-output.c.o\r\n[ 34%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/softmax-output.c.o\r\n[ 35%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/fully-connected-output.c.o\r\n[ 35%] Building CXX object third_party/onnx/CMakeFiles/onnx_proto.dir/onnx/onnx-operators_onnx_torch.pb.cc.o\r\n[ 35%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/relu-input-gradient.c.o\r\n[ 35%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/convolution-input-gradient.c.o\r\n[ 35%] Building CXX object caffe2/proto/CMakeFiles/Caffe2_PROTO.dir/torch.pb.cc.o\r\n[ 35%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/convolution-kernel-gradient.c.o\r\n[ 35%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/convolution-output.c.o\r\n[ 35%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/x86_64-fma/softmax.c.o\r\n[ 35%] Linking C static library ../../lib/libnnpack.a\r\n[ 35%] Built target nnpack\r\n[ 36%] Linking CXX static library ../../lib/libonnx_proto.a\r\n[ 36%] Built target onnx_proto\r\nScanning dependencies of target onnx\r\n[ 36%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/__/__/caffe2/onnx/torch_ops/defs.cc.o\r\n[ 36%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/common/interned_strings.cc.o\r\n[ 36%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/common/ir_pb_converter.cc.o\r\n[ 36%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/common/model_helpers.cc.o\r\n[ 36%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/common/assertions.cc.o\r\n[ 36%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/checker.cc.o\r\n[ 36%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/__/__/caffe2/onnx/torch_ops/schema.cc.o\r\n[ 36%] Built target Caffe2_PROTO\r\n[ 36%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/common/status.cc.o\r\n[ 36%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/controlflow/defs.cc.o\r\nScanning dependencies of target Caffe2_perfkernels_avx\r\n[ 36%] Building CXX object caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx.dir/adagrad_avx.cc.o\r\n[ 36%] Building CXX object caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx.dir/common_avx.cc.o\r\n[ 36%] Building CXX object caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx.dir/typed_axpy_avx.cc.o\r\n[ 36%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/data_type_utils.cc.o\r\nScanning dependencies of target caffe2_protos\r\n[ 36%] Linking CXX static library ../lib/libcaffe2_protos.a\r\n[ 36%] Built target caffe2_protos\r\n[ 36%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/experiments/defs.cc.o\r\nScanning dependencies of target Caffe2_perfkernels_avx2\r\n[ 36%] Building CXX object caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx2.dir/common_avx2.cc.o\r\n[ 36%] Building CXX object caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx2.dir/embedding_lookup_avx2.cc.o\r\n[ 36%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/experiments/experiments_functions.cc.o\r\n[ 37%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/function.cc.o\r\n[ 37%] Building CXX object caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx2.dir/embedding_lookup_fused_8bit_rowwise_avx2.cc.o\r\n[ 37%] Built target Caffe2_perfkernels_avx\r\n[ 37%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/generator/defs.cc.o\r\n[ 37%] Building CXX object caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx2.dir/math_cpu_avx2.cc.o\r\n[ 37%] Building CXX object caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx2.dir/typed_axpy_avx2.cc.o\r\n[ 37%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/generator/old.cc.o\r\n[ 37%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/logical/defs.cc.o\r\n[ 37%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/logical/old.cc.o\r\n[ 37%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/math/defs.cc.o\r\n[ 37%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/math/old.cc.o\r\n[ 37%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/nn/defs.cc.o\r\n[ 37%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/nn/old.cc.o\r\n[ 37%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/reduction/defs.cc.o\r\n[ 37%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/rnn/defs.cc.o\r\n[ 37%] Built target Caffe2_perfkernels_avx2\r\n[ 37%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/rnn/old.cc.o\r\n[ 37%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/schema.cc.o\r\n[ 37%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/tensor/defs.cc.o\r\n[ 37%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/tensor/old.cc.o\r\n[ 38%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/traditionalml/defs.cc.o\r\n[ 38%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/onnxifi_utils.cc.o\r\n[ 38%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/optimizer/optimize.cc.o\r\n[ 38%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/optimizer/pass.cc.o\r\n[ 38%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/optimizer/pass_manager.cc.o\r\n[ 38%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/optimizer/pass_registry.cc.o\r\n[ 38%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/shape_inference/implementation.cc.o\r\n[ 38%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/version_converter/convert.cc.o\r\n[ 38%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/version_converter/helper.cc.o\r\n\x1b[91m/opt/pytorch/third_party/onnx/onnx/shape_inference/implementation.cc: In function ‚Äòvoid onnx_torch::shape_inference::InferShapeForFunctionNode(const onnx_torch::FunctionProto&, const onnx_torch::ISchemaRegistry*, onnx_torch::InferenceContext&)‚Äô:\r\n/opt/pytorch/third_party/onnx/onnx/shape_inference/implementation.cc:207:53: warning: type qualifiers ignored on cast result type [-Wignored-qualifiers]\r\n   for (int i = 0; i < (const int)(ctx.getNumInputs()); ++i) {\r\n                                                     ^\r\n\x1b[0m[ 38%] Linking CXX static library ../../lib/libonnx.a\r\n[ 38%] Built target onnx\r\nScanning dependencies of target caffe2\r\n[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUGeneral.cpp.o\r\n[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUGenerator.cpp.o\r\n[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUTypeDefault.cpp.o\r\n[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/Context.cpp.o\r\n[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/SparseTensorImpl.cpp.o\r\n[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/DLConvertor.cpp.o\r\n[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/ExpandUtils.cpp.o\r\n[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/TensorGeometry.cpp.o\r\n[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/TensorUtils.cpp.o\r\n[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/UndefinedType.cpp.o\r\n[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/Utils.cpp.o\r\n[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/detail/CPUGuardImpl.cpp.o\r\n[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/detail/CUDAHooksInterface.cpp.o\r\n[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/detail/ComplexHooksInterface.cpp.o\r\n[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/cpu/FlushDenormal.cpp.o\r\n[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/ATenCoreTest.cpp.o\r\n[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/ATenGeneral.cpp.o\r\n[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/Allocator.cpp.o\r\n[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/Formatting.cpp.o\r\n[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/LegacyTypeDispatch.cpp.o\r\n[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/Range.cpp.o\r\n[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/Scalar.cpp.o\r\n[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/Storage.cpp.o\r\n[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/StorageImpl.cpp.o\r\n[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/Tensor.cpp.o\r\n[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/TensorImpl.cpp.o\r\n[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/TensorOptions.cpp.o\r\n[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/TensorTypeId.cpp.o\r\n[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/TensorTypeIdRegistration.cpp.o\r\n[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/UndefinedTensorImpl.cpp.o\r\n[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/UniqueVoidPtr.cpp.o\r\n[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/VariableHooksInterface.cpp.o\r\n[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/context_base.cpp.o\r\n[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/blob.cpp.o\r\n[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/interned_strings.cpp.o\r\n[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/intrusive_ptr.cpp.o\r\n[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/ivalue.cpp.o\r\n[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/register_symbols.cpp.o\r\n[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/thread_pool.cpp.o\r\n[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/type.cpp.o\r\n[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Activation.cpp.o\r\n[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/BatchLinearAlgebra.cpp.o\r\n[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/BinaryOps.cpp.o\r\n[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/ConstantPadNd.cpp.o\r\n[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Convolution.cpp.o\r\n[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/ConvolutionTBC.cpp.o\r\n[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Copy.cpp.o\r\n[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/DispatchStub.cpp.o\r\n[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Distance.cpp.o\r\n[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Distributions.cpp.o\r\n[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Dropout.cpp.o\r\n[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Embedding.cpp.o\r\n[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/EmbeddingBag.cpp.o\r\n[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/GridSampler.cpp.o\r\n[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Indexing.cpp.o\r\n[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/LegacyBridge.cpp.o\r\n[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/LegacyDefinitions.cpp.o\r\n[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Linear.cpp.o\r\n[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/LinearAlgebra.cpp.o\r\n[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Loss.cpp.o\r\n[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/LossCTC.cpp.o\r\n[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Memory.cpp.o\r\n[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Normalization.cpp.o\r\n[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/PackedSequence.cpp.o\r\n[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/PixelShuffle.cpp.o\r\n[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Pooling.cpp.o\r\n[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/RNN.cpp.o\r\n[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/ReduceOps.cpp.o\r\n[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Resize.cpp.o\r\n[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/RoiPooling.cpp.o\r\n[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Scalar.cpp.o\r\n[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/SoftMax.cpp.o\r\n[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/SpectralOps.cpp.o\r\n[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/SummaryOps.cpp.o\r\n[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/TensorCompare.cpp.o\r\n[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/TensorConversions.cpp.o\r\n[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/TensorFactories.cpp.o\r\n[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/TensorIterator.cpp.o\r\n[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/TensorIteratorReduce.cpp.o\r\n[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/TensorProperties.cpp.o\r\n[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/TensorShape.cpp.o\r\n[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/TensorTransformations.cpp.o\r\n[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/TypeProperties.cpp.o\r\n[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/UnaryOps.cpp.o\r\n[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Unique.cpp.o\r\n[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/WeightNorm.cpp.o\r\n[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/sparse/SparseTensor.cpp.o\r\n[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/sparse/SparseTensorMath.cpp.o\r\n[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/mkl/LinearAlgebra.cpp.o\r\n[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/mkl/SpectralOps.cpp.o\r\n[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/mkldnn/Conv.cpp.o\r\n[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUByteType.cpp.o\r\n[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUCharType.cpp.o\r\n[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUCopy.cpp.o\r\n[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUDoubleType.cpp.o\r\n[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUFloatType.cpp.o\r\n[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUHalfType.cpp.o\r\n[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUIntType.cpp.o\r\n[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPULongType.cpp.o\r\n[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUShortType.cpp.o\r\n[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/RegisterCPU.cpp.o\r\n[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/SparseCPUByteType.cpp.o\r\n[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/SparseCPUCharType.cpp.o\r\n[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/SparseCPUDoubleType.cpp.o\r\n[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/SparseCPUFloatType.cpp.o\r\n[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/SparseCPUIntType.cpp.o\r\n[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/SparseCPULongType.cpp.o\r\n[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/SparseCPUShortType.cpp.o\r\n[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/TypeDefault.cpp.o\r\n[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THGeneral.cpp.o\r\n[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THAllocator.cpp.o\r\n[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THSize.cpp.o\r\n[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THStorageFunctions.cpp.o\r\n[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THTensor.cpp.o\r\n[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THTensorCopy.cpp.o\r\n[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THTensorRandom.cpp.o\r\n[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THTensorMath.cpp.o\r\n[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THTensorMoreMath.cpp.o\r\n[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THTensorEvenMoreMath.cpp.o\r\n[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THTensorConv.cpp.o\r\n[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THTensorLapack.cpp.o\r\n[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THBlas.cpp.o\r\n[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THLapack.cpp.o\r\n[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THLogAdd.cpp.o\r\n[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THRandom.cpp.o\r\n[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THFile.cpp.o\r\n\x1b[91m/opt/pytorch/aten/src/TH/THRandom.cpp: In function ‚ÄòTHGenerator* THGenerator_newUnseeded()‚Äô:\r\n/opt/pytorch/aten/src/TH/THRandom.cpp:18:38: warning: ‚Äòvoid* memset(void*, int, size_t)‚Äô clearing an object of type ‚ÄòTHGenerator‚Äô {aka ‚Äòstruct THGenerator‚Äô} with no trivial copy-assignment; use value-initialization instead [-Wclass-memaccess]\r\n   memset(self, 0, sizeof(THGenerator));\r\n                                      ^\r\n\x1b[0m\x1b[91mIn file included from /opt/pytorch/aten/src/TH/THRandom.cpp:3:\r\n/opt/pytorch/aten/src/TH/THGenerator.hpp:26:8: note: ‚ÄòTHGenerator‚Äô {aka ‚Äòstruct THGenerator‚Äô} declared here\r\n struct THGenerator {\r\n        ^~~~~~~~~~~\r\n\x1b[0m[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THDiskFile.cpp.o\r\n[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THMemoryFile.cpp.o\r\n[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THVector.cpp.o\r\n[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/vector/AVX.cpp.o\r\n[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/vector/AVX2.cpp.o\r\n[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/THNN/init.cpp.o\r\n[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp.AVX2.cpp.o\r\n[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/TensorCompareKernel.cpp.AVX2.cpp.o\r\n[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/SoftMaxKernel.cpp.AVX2.cpp.o\r\n[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp.AVX2.cpp.o\r\n[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/GridSamplerKernel.cpp.AVX2.cpp.o\r\n[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp.AVX2.cpp.o\r\n[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp.AVX2.cpp.o\r\n[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/Activation.cpp.AVX2.cpp.o\r\n[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp.AVX.cpp.o\r\n[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/TensorCompareKernel.cpp.AVX.cpp.o\r\n[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/SoftMaxKernel.cpp.AVX.cpp.o\r\n[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp.AVX.cpp.o\r\n[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/GridSamplerKernel.cpp.AVX.cpp.o\r\n[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp.AVX.cpp.o\r\n[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp.AVX.cpp.o\r\n[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/Activation.cpp.AVX.cpp.o\r\n[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp.DEFAULT.cpp.o\r\n[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/TensorCompareKernel.cpp.DEFAULT.cpp.o\r\n[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/SoftMaxKernel.cpp.DEFAULT.cpp.o\r\n[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp.DEFAULT.cpp.o\r\n[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/GridSamplerKernel.cpp.DEFAULT.cpp.o\r\n[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp.DEFAULT.cpp.o\r\n[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp.DEFAULT.cpp.o\r\n[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/Activation.cpp.DEFAULT.cpp.o\r\n[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/mkldnn/Runtime.cpp.o\r\n[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/aten/aten_op.cc.o\r\n[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/gloo/allgather_ops.cc.o\r\n[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/gloo/allreduce_ops.cc.o\r\n[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/gloo/barrier_ops.cc.o\r\n[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/gloo/broadcast_ops.cc.o\r\n[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/gloo/common.cc.o\r\n[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/gloo/common_world_ops.cc.o\r\n[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/gloo/context.cc.o\r\n[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/gloo/reduce_scatter_ops.cc.o\r\n[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/gloo/store_handler.cc.o\r\n[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/script/compiler.cc.o\r\n[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/script/lexer.cc.o\r\n[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/allocator.cc.o\r\n[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/blob_serialization.cc.o\r\n[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/blob_stats.cc.o\r\n[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/common.cc.o\r\n[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/context.cc.o\r\n[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/context_base.cc.o\r\n[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/db.cc.o\r\n[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/event.cc.o\r\n[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/graph.cc.o\r\n[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/init.cc.o\r\n[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/init_intrinsics_check.cc.o\r\n[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/init_omp.cc.o\r\n[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/int8_serialization.cc.o\r\n[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/memonger.cc.o\r\n[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/module.cc.o\r\n[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/net.cc.o\r\n[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/net_async_base.cc.o\r\n[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/net_async_scheduling.cc.o\r\n[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/net_async_tracing.cc.o\r\n[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/net_dag.cc.o\r\n[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/net_dag_utils.cc.o\r\n[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/net_simple.cc.o\r\n[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/net_simple_refcount.cc.o\r\n[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/numa.cc.o\r\n[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/operator.cc.o\r\n[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/operator_c10wrapper.cc.o\r\n[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/operator_schema.cc.o\r\n[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/plan_executor.cc.o\r\n[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/prof_dag_counters.cc.o\r\n[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/qtensor.cc.o\r\n[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/qtensor_serialization.cc.o\r\n[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/stats.cc.o\r\n[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/tensor.cc.o\r\n[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/tensor_impl.cc.o\r\n[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/tensor_int8.cc.o\r\n[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/transform.cc.o\r\n[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/types.cc.o\r\n[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/workspace.cc.o\r\n[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/proto_convert.cc.o\r\n[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/proto_wrap.cc.o\r\n[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/proto_utils.cc.o\r\n[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/murmur_hash3.cc.o\r\n[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/smart_tensor_printer.cc.o\r\n[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/signal_handler.cc.o\r\n[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/string_utils.cc.o\r\n[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/threadpool/ThreadPool.cc.o\r\n[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/cpuid.cc.o\r\n[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/bench_utils.cc.o\r\n[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/math_cpu.cc.o\r\n[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/math_utils.cc.o\r\n[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/thread_name.cc.o\r\n[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/threadpool/pthreadpool.cc.o\r\n[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/threadpool/pthreadpool_impl.cc.o\r\n[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/predictor/predictor.cc.o\r\n[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/predictor/predictor_utils.cc.o\r\n[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/predictor/predictor_config.cc.o\r\n[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/nomnigraph/Representations/NeuralNet.cc.o\r\n[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/nomnigraph/tests/test_util.cc.o\r\n[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/db/create_db_op.cc.o\r\n[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/db/protodb.cc.o\r\n[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/distributed/file_store_handler.cc.o\r\n[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/distributed/file_store_handler_op.cc.o\r\n[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/distributed/store_handler.cc.o\r\n[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/distributed/store_ops.cc.o\r\n[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/concat_split_op.cc.o\r\n[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/conv_fusion_op.cc.o\r\n[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/conv_op.cc.o\r\n[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/dropout_op.cc.o\r\n[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/elementwise_sum_op.cc.o\r\n[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/fully_connected_op.cc.o\r\n[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/local_response_normalization_op.cc.o\r\n[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/momentum_sgd_op.cc.o\r\n[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/operator_fallback_ideep.cc.o\r\n[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/pool_op.cc.o\r\n[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/relu_op.cc.o\r\n[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/spatial_batch_norm_op.cc.o\r\n[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/squeeze_op.cc.o\r\n[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/utility_ops.cc.o\r\n[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/utils/ideep_register.cc.o\r\n[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/mpi/mpi_common.cc.o\r\n[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/mpi/mpi_ops.cc.o\r\n[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/observers/time_observer.cc.o\r\n[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/observers/runcnt_observer.cc.o\r\n[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/onnx/backend.cc.o\r\n[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/onnx/backend_rep.cc.o\r\n[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/onnx/device.cc.o\r\n[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/onnx/helper.cc.o\r\n[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/onnx/onnx_exporter.cc.o\r\n[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/onnx/onnxifi_init.cc.o\r\n[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/abs_op.cc.o\r\n[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/accumulate_op.cc.o\r\n[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/accuracy_op.cc.o\r\n[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/acos_op.cc.o\r\n[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/affine_channel_op.cc.o\r\n[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/apmeter_op.cc.o\r\n[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/arg_ops.cc.o\r\n[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/asin_op.cc.o\r\n[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/assert_op.cc.o\r\n[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/atan_op.cc.o\r\n[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/atomic_ops.cc.o\r\n[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/batch_box_cox_op.cc.o\r\n[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/batch_bucketize_op.cc.o\r\n[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/batch_gather_ops.cc.o\r\n[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/batch_matmul_op.cc.o\r\n[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/batch_moments_op.cc.o\r\n[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/batch_sparse_to_dense_op.cc.o\r\n[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/bbox_transform_op.cc.o\r\n[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/bisect_percentile_op.cc.o\r\n[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/boolean_mask_ops.cc.o\r\n[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/boolean_unmask_ops.cc.o\r\n[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/box_with_nms_limit_op.cc.o\r\n[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/byte_weight_dequant_op.cc.o\r\n[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/cast_op.cc.o\r\n[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/cbrt_op.cc.o\r\n[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/ceil_op.cc.o\r\n[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/channel_backprop_stats_op.cc.o\r\n[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/channel_shuffle_op.cc.o\r\n[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/channel_stats_op.cc.o\r\n[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/clip_op.cc.o\r\n[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/collect_and_distribute_fpn_rpn_proposals_op.cc.o\r\n[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/communicator_op.cc.o\r\n[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/concat_split_op.cc.o\r\n[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/conditional_op.cc.o\r\n[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/conv_gradient_op.cc.o\r\n[ 57%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/conv_op.cc.o\r\n[ 57%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/conv_op_eigen.cc.o\r\n[ 57%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/conv_op_shared.cc.o\r\n[ 57%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/conv_transpose_gradient_op.cc.o\r\n[ 57%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/conv_transpose_op.cc.o\r\n[ 57%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/conv_transpose_op_mobile.cc.o\r\n[ 57%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/copy_op.cc.o\r\n[ 57%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/cos_op.cc.o\r\n[ 57%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/cosh_op.cc.o\r\n[ 57%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/cosine_embedding_criterion_op.cc.o\r\n[ 57%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/counter_ops.cc.o\r\n[ 57%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/create_scope_op.cc.o\r\n[ 57%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/crf_viterbi_op.cc.o\r\n[ 57%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/cross_entropy_op.cc.o\r\n[ 57%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/ctc_beam_search_decoder_op.cc.o\r\n[ 57%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/ctc_greedy_decoder_op.cc.o\r\n[ 58%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/cube_op.cc.o\r\n[ 58%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/data_couple.cc.o\r\n[ 58%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/dataset_ops.cc.o\r\n[ 58%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/deform_conv_gradient_op.cc.o\r\n[ 58%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/deform_conv_op.cc.o\r\n[ 58%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/distance_op.cc.o\r\n[ 58%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/do_op.cc.o\r\n[ 58%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/dropout_op.cc.o\r\n[ 58%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_add_gradient_op.cc.o\r\n[ 58%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_add_op.cc.o\r\n[ 58%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_div_gradient_op.cc.o\r\n[ 58%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_div_op.cc.o\r\n[ 58%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_linear_op.cc.o\r\n[ 58%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_logical_ops.cc.o\r\n[ 58%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_mul_gradient_op.cc.o\r\n[ 58%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_mul_op.cc.o\r\n[ 59%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_ops.cc.o\r\n[ 59%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_ops_schema.cc.o\r\n[ 59%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_ops_utils.cc.o\r\n[ 59%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_sub_gradient_op.cc.o\r\n[ 59%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_sub_op.cc.o\r\n[ 59%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_sum_op.cc.o\r\n[ 59%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elu_op.cc.o\r\n[ 59%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/enforce_finite_op.cc.o\r\n[ 59%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/ensure_clipped_op.cc.o\r\n[ 59%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/ensure_cpu_output_op.cc.o\r\n[ 59%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/exp_op.cc.o\r\n[ 59%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/expand_op.cc.o\r\n[ 59%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/expand_squeeze_dims_op.cc.o\r\n[ 59%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/fc_inference.cc.o\r\n[ 59%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/feature_maps_ops.cc.o\r\n[ 59%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/feed_blob_op.cc.o\r\n[ 60%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/filler_op.cc.o\r\n[ 60%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/find_duplicate_elements_op.cc.o\r\n[ 60%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/find_op.cc.o\r\n[ 60%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/flatten_op.cc.o\r\n[ 60%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/flexible_top_k.cc.o\r\n[ 60%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/floor_op.cc.o\r\n[ 60%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/free_op.cc.o\r\n[ 60%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/fully_connected_op.cc.o\r\n[ 60%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/fused_rowwise_8bit_conversion_ops.cc.o\r\n[ 60%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/fused_rowwise_random_quantization_ops.cc.o\r\n[ 60%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/gather_fused_8bit_rowwise_op.cc.o\r\n[ 60%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/gather_op.cc.o\r\n[ 60%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/gather_ranges_to_dense_op.cc.o\r\n[ 60%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/generate_proposals_op.cc.o\r\n[ 60%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/given_tensor_byte_string_to_uint8_fill_op.cc.o\r\n[ 60%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/given_tensor_fill_op.cc.o\r\n[ 61%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/glu_op.cc.o\r\n[ 61%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/group_norm_op.cc.o\r\n[ 61%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/gru_unit_op.cc.o\r\n[ 61%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/h_softmax_op.cc.o\r\n[ 61%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/half_float_ops.cc.o\r\n[ 61%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/hard_sigmoid_op.cc.o\r\n[ 61%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/if_op.cc.o\r\n[ 61%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/heatmap_max_keypoint_op.cc.o\r\n[ 61%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/im2col_op.cc.o\r\n[ 61%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/index_hash_ops.cc.o\r\n[ 61%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/index_ops.cc.o\r\n[ 61%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/instance_norm_gradient_op.cc.o\r\n[ 61%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/instance_norm_op.cc.o\r\n[ 61%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/integral_image_op.cc.o\r\n[ 61%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/is_empty_op.cc.o\r\n[ 62%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/jsd_op.cc.o\r\n[ 62%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/key_split_ops.cc.o\r\n[ 62%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/last_n_window_collector.cc.o\r\n[ 62%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/layer_norm_op.cc.o\r\n[ 62%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/leaky_relu_op.cc.o\r\n[ 62%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/length_split_op.cc.o\r\n[ 62%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/lengths_pad_op.cc.o\r\n[ 62%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/lengths_reducer_fused_8bit_rowwise_ops.cc.o\r\n[ 62%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/lengths_reducer_ops.cc.o\r\n[ 62%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/lengths_reducer_rowwise_8bit_ops.cc.o\r\n[ 62%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/lengths_tile_op.cc.o\r\n[ 62%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/lengths_top_k_op.cc.o\r\n[ 62%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/listwise_l2r_op.cc.o\r\n[ 62%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/load_save_op.cc.o\r\n[ 62%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/local_response_normalization_op.cc.o\r\n[ 62%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/locally_connected_op.cc.o\r\n[ 63%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/locally_connected_op_util.cc.o\r\n[ 63%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/log_op.cc.o\r\n[ 63%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/logit_op.cc.o\r\n[ 63%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/loss_op.cc.o\r\n[ 63%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/lp_pool_op.cc.o\r\n[ 63%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/lpnorm_op.cc.o\r\n[ 63%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/lstm_unit_op.cc.o\r\n[ 63%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/map_ops.cc.o\r\n[ 63%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/margin_ranking_criterion_op.cc.o\r\n[ 63%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/matmul_op.cc.o\r\n[ 63%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/mean_op.cc.o\r\n[ 63%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/merge_id_lists_op.cc.o\r\n[ 63%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/minmax_gradient_ops.cc.o\r\n[ 63%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/minmax_ops.cc.o\r\n[ 63%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/mod_op.cc.o\r\n[ 63%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/moments_op.cc.o\r\n[ 64%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/multi_class_accuracy_op.cc.o\r\n[ 64%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/negate_gradient_op.cc.o\r\n[ 64%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/negative_op.cc.o\r\n[ 64%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/ngram_ops.cc.o\r\n[ 64%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/norm_planar_yuv_op.cc.o\r\n[ 64%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/normalize_l1_op.cc.o\r\n[ 64%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/normalize_op.cc.o\r\n[ 64%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/numpy_tile_op.cc.o\r\n[ 64%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/one_hot_ops.cc.o\r\n[ 64%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/onnx_while_op.cc.o\r\n[ 64%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/onnxifi_op.cc.o\r\n[ 64%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/order_switch_ops.cc.o\r\n[ 64%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/pack_rnn_sequence_op.cc.o\r\n[ 64%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/pack_segments.cc.o\r\n[ 64%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/pad_op.cc.o\r\n[ 64%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/partition_ops.cc.o\r\n[ 65%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/percentile_op.cc.o\r\n[ 65%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/perplexity_op.cc.o\r\n[ 65%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/piecewise_linear_transform_op.cc.o\r\n[ 65%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/pool_gradient_op.cc.o\r\n[ 65%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/pool_op.cc.o\r\n[ 65%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/pow_op.cc.o\r\n[ 65%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/prelu_op.cc.o\r\n[ 65%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/prepend_dim_op.cc.o\r\n[ 65%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quant_decode_op.cc.o\r\n[ 65%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/rank_loss_op.cc.o\r\n[ 65%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reciprocal_gradient_op.cc.o\r\n[ 65%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reciprocal_op.cc.o\r\n[ 65%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reduce_front_back_max_ops.cc.o\r\n[ 65%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reduce_front_back_mean_ops.cc.o\r\n[ 65%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reduce_front_back_sum_ops.cc.o\r\n[ 65%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reduce_ops.cc.o\r\n[ 66%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reduction_ops.cc.o\r\n[ 66%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/relu_n_op.cc.o\r\n[ 66%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/relu_op.cc.o\r\n[ 66%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/remove_data_blocks_op.cc.o\r\n[ 66%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/replace_nan_op.cc.o\r\n[ 66%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reservoir_sampling.cc.o\r\n[ 66%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reshape_op.cc.o\r\n[ 66%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/resize_op.cc.o\r\n[ 66%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reverse_packed_segs_op.cc.o\r\n[ 66%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/rmac_regions_op.cc.o\r\n[ 66%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/roi_align_gradient_op.cc.o\r\n[ 66%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/roi_align_op.cc.o\r\n[ 66%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/roi_align_rotated_gradient_op.cc.o\r\n[ 66%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/roi_align_rotated_op.cc.o\r\n[ 66%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/roi_pool_op.cc.o\r\n[ 67%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/rowmul_op.cc.o\r\n[ 67%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/rsqrt_op.cc.o\r\n[ 67%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/scale_op.cc.o\r\n[ 67%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/segment_reduction_op.cc.o\r\n[ 67%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/selu_op.cc.o\r\n[ 67%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sequence_ops.cc.o\r\n[ 67%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/shape_op.cc.o\r\n[ 67%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sigmoid_gradient_op.cc.o\r\n[ 67%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sigmoid_op.cc.o\r\n[ 67%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sin_op.cc.o\r\n[ 67%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sinh_op.cc.o\r\n[ 67%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sinusoid_position_encoding_op.cc.o\r\n[ 67%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/slice_op.cc.o\r\n[ 67%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/softmax_op.cc.o\r\n[ 67%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/softmax_shared.cc.o\r\n[ 67%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/softmax_with_loss_op.cc.o\r\n[ 68%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/softplus_op.cc.o\r\n[ 68%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/softsign_op.cc.o\r\n[ 68%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/space_batch_op.cc.o\r\n[ 68%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sparse_normalize_op.cc.o\r\n[ 68%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sparse_to_dense_mask_op.cc.o\r\n[ 68%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sparse_to_dense_op.cc.o\r\n[ 68%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/spatial_batch_norm_gradient_op.cc.o\r\n[ 68%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/spatial_batch_norm_op.cc.o\r\n[ 68%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/spatial_softmax_with_loss_op.cc.o\r\n[ 68%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sqr_op.cc.o\r\n[ 68%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sqrt_op.cc.o\r\n[ 68%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/square_root_divide_op.cc.o\r\n[ 68%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/stats_ops.cc.o\r\n[ 68%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/stats_put_ops.cc.o\r\n[ 68%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/stop_gradient.cc.o\r\n[ 68%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/string_ops.cc.o\r\n[ 69%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/stump_func_op.cc.o\r\n[ 69%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/stylizer_ops.cc.o\r\n[ 69%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/summarize_op.cc.o\r\n[ 69%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/swish_op.cc.o\r\n[ 69%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/tan_op.cc.o\r\n[ 69%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/tanh_gradient_op.cc.o\r\n[ 69%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/tanh_op.cc.o\r\n[ 69%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/tensor_protos_db_input.cc.o\r\n[ 69%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/text_file_reader.cc.o\r\n[ 69%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/text_file_reader_utils.cc.o\r\n[ 69%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/thresholded_relu_op.cc.o\r\n[ 69%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/tile_op.cc.o\r\n[ 69%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/top_k.cc.o\r\n[ 69%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/transpose_op.cc.o\r\n[ 69%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/tt_linear_op.cc.o\r\n[ 69%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/unique_ops.cc.o\r\n[ 70%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/upsample_op.cc.o\r\n[ 70%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/utility_ops.cc.o\r\n[ 70%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/variable_length_sequence_padding.cc.o\r\n[ 70%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/weighted_multi_sampling_op.cc.o\r\n[ 70%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/weighted_sample_op.cc.o\r\n[ 70%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/while_op.cc.o\r\n[ 70%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/workspace_ops.cc.o\r\n[ 70%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/zero_gradient_op.cc.o\r\n[ 70%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/add_cpu.cc.o\r\n[ 70%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/averaged_loss_cpu.cc.o\r\n[ 70%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/batch_gather_cpu.cc.o\r\n[ 70%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/batch_matmul_cpu.cc.o\r\n[ 70%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/cast_cpu.cc.o\r\n[ 70%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/concat_cpu.cc.o\r\n[ 70%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/enforce_finite_cpu.cc.o\r\n[ 70%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/expand_dims_cpu.cc.o\r\n[ 71%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/fc_cpu.cc.o\r\n[ 71%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/filler_cpu.cc.o\r\n[ 71%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/flatten_cpu.cc.o\r\n[ 71%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/mul_cpu.cc.o\r\n[ 71%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/relu_cpu.cc.o\r\n[ 71%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/sigmoid_cpu.cc.o\r\n[ 71%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/sigmoid_cross_entropy_with_logits_cpu.cc.o\r\n[ 71%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/sparse_lengths_sum_cpu.cc.o\r\n[ 71%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/stop_gradient_cpu.cc.o\r\n[ 71%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/add.cc.o\r\n[ 71%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/averaged_loss.cc.o\r\n[ 71%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/batch_gather.cc.o\r\n[ 71%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/batch_matmul.cc.o\r\n[ 71%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/cast.cc.o\r\n[ 71%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/concat.cc.o\r\n[ 71%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/enforce_finite.cc.o\r\n[ 72%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/expand_dims.cc.o\r\n[ 72%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/fc.cc.o\r\n[ 72%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/filler.cc.o\r\n[ 72%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/flatten.cc.o\r\n[ 72%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/mul.cc.o\r\n[ 72%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/relu.cc.o\r\n[ 72%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/sigmoid.cc.o\r\n[ 72%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/sigmoid_cross_entropy_with_logits.cc.o\r\n[ 72%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/sparse_lengths_sum.cc.o\r\n[ 72%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/stop_gradient.cc.o\r\n[ 72%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/rnn/recurrent_network_blob_fetcher_op.cc.o\r\n[ 72%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/rnn/recurrent_network_executor.cc.o\r\n[ 72%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/rnn/recurrent_network_op.cc.o\r\n[ 72%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/init_qnnpack.cc.o\r\n[ 72%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_add_op.cc.o\r\n[ 73%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_average_pool_op.cc.o\r\n[ 73%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_channel_shuffle_op.cc.o\r\n[ 73%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_concat_op.cc.o\r\n[ 73%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_conv_op.cc.o\r\n[ 73%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_conv_transpose_op.cc.o\r\n[ 73%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_dequantize_op.cc.o\r\n[ 73%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_fc_op.cc.o\r\n[ 73%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_flatten_op.cc.o\r\n[ 73%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_given_tensor_fill_op.cc.o\r\n[ 73%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_leaky_relu_op.cc.o\r\n[ 73%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_max_pool_op.cc.o\r\n[ 73%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_quantize_op.cc.o\r\n[ 73%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_relu_op.cc.o\r\n[ 73%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_reshape_op.cc.o\r\n[ 73%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_resize_nearest_op.cc.o\r\n[ 73%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_roi_align_op.cc.o\r\n[ 74%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_slice_op.cc.o\r\n[ 74%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_sigmoid_op.cc.o\r\n[ 74%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_softmax_op.cc.o\r\n[ 74%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/annotations.cc.o\r\n[ 74%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/backend_cutting.cc.o\r\n[ 74%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/converter.cc.o\r\n[ 74%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/dead_code_elim.cc.o\r\n[ 74%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/device.cc.o\r\n[ 74%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/distributed.cc.o\r\n[ 74%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/distributed_converter.cc.o\r\n[ 74%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/fusion.cc.o\r\n[ 74%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/mobile.cc.o\r\n[ 74%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/onnxifi_transformer.cc.o\r\n[ 74%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/optimize_ideep.cc.o\r\n[ 74%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/optimizer.cc.o\r\n[ 74%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/passes.cc.o\r\n[ 75%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/sink.cc.o\r\n[ 75%] Building CXX object caffe2/CMakeFiles/caffe2.dir/perfkernels/adagrad.cc.o\r\n[ 75%] Building CXX object caffe2/CMakeFiles/caffe2.dir/perfkernels/embedding_lookup.cc.o\r\n[ 75%] Building CXX object caffe2/CMakeFiles/caffe2.dir/perfkernels/fused_8bit_rowwise_embedding_lookup.cc.o\r\n[ 75%] Building CXX object caffe2/CMakeFiles/caffe2.dir/perfkernels/math_cpu_base.cc.o\r\n[ 75%] Building CXX object caffe2/CMakeFiles/caffe2.dir/perfkernels/typed_axpy.cc.o\r\n[ 75%] Building CXX object caffe2/CMakeFiles/caffe2.dir/queue/blobs_queue.cc.o\r\n[ 75%] Building CXX object caffe2/CMakeFiles/caffe2.dir/queue/blobs_queue_db.cc.o\r\n[ 75%] Building CXX object caffe2/CMakeFiles/caffe2.dir/queue/queue_ops.cc.o\r\n[ 75%] Building CXX object caffe2/CMakeFiles/caffe2.dir/queue/rebatching_queue.cc.o\r\n[ 75%] Building CXX object caffe2/CMakeFiles/caffe2.dir/queue/rebatching_queue_ops.cc.o\r\n[ 75%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/adadelta_op.cc.o\r\n[ 75%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/adagrad_op.cc.o\r\n[ 75%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/adam_op.cc.o\r\n[ 75%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/clip_tensor_op.cc.o\r\n[ 75%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/ftrl_op.cc.o\r\n[ 76%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/gftrl_op.cc.o\r\n[ 76%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/iter_op.cc.o\r\n[ 76%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/lars_op.cc.o\r\n[ 76%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/learning_rate_adaption_op.cc.o\r\n[ 76%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/learning_rate_op.cc.o\r\n[ 76%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/momentum_sgd_op.cc.o\r\n[ 76%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/rmsprop_op.cc.o\r\n[ 76%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/wngrad_op.cc.o\r\n[ 76%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/yellowfin_op.cc.o\r\n[ 76%] Building CXX object caffe2/CMakeFiles/caffe2.dir/share/contrib/nnpack/conv_op.cc.o\r\n[ 76%] Building CXX object caffe2/CMakeFiles/caffe2.dir/share/contrib/depthwise/depthwise3x3_conv_op.cc.o\r\n[ 76%] Building CXX object caffe2/CMakeFiles/caffe2.dir/transforms/common_subexpression_elimination.cc.o\r\n[ 76%] Building CXX object caffe2/CMakeFiles/caffe2.dir/transforms/conv_to_nnpack_transform.cc.o\r\n[ 76%] Building CXX object caffe2/CMakeFiles/caffe2.dir/transforms/pattern_net_transform.cc.o\r\n[ 76%] Building CXX object caffe2/CMakeFiles/caffe2.dir/transforms/single_op_transform.cc.o\r\n[ 76%] Linking CXX shared library ../lib/libcaffe2.so\r\n[ 76%] Built target caffe2\r\nScanning dependencies of target device_test\r\nScanning dependencies of target dead_code_elim_test\r\nScanning dependencies of target depthwise3x3_conv_op_test\r\nScanning dependencies of target distributed_test\r\nScanning dependencies of target common_subexpression_elimination_test\r\nScanning dependencies of target nnpack_test\r\nScanning dependencies of target pattern_net_transform_test\r\n[ 76%] Building CXX object caffe2/CMakeFiles/device_test.dir/opt/device_test.cc.o\r\n[ 76%] Building CXX object caffe2/CMakeFiles/dead_code_elim_test.dir/opt/dead_code_elim_test.cc.o\r\nScanning dependencies of target conv_to_nnpack_transform_test\r\n[ 77%] Building CXX object caffe2/CMakeFiles/nnpack_test.dir/share/contrib/nnpack/nnpack_test.cc.o\r\n[ 77%] Building CXX object caffe2/CMakeFiles/distributed_test.dir/opt/distributed_test.cc.o\r\n[ 77%] Building CXX object caffe2/CMakeFiles/pattern_net_transform_test.dir/transforms/pattern_net_transform_test.cc.o\r\n[ 77%] Building CXX object caffe2/CMakeFiles/depthwise3x3_conv_op_test.dir/share/contrib/depthwise/depthwise3x3_conv_op_test.cc.o\r\n[ 78%] Building CXX object caffe2/CMakeFiles/common_subexpression_elimination_test.dir/transforms/common_subexpression_elimination_test.cc.o\r\n[ 78%] Building CXX object caffe2/CMakeFiles/conv_to_nnpack_transform_test.dir/transforms/conv_to_nnpack_transform_test.cc.o\r\n[ 78%] Linking CXX executable ../bin/device_test\r\n[ 78%] Built target device_test\r\nScanning dependencies of target converter_nomigraph_test\r\n[ 78%] Building CXX object caffe2/CMakeFiles/converter_nomigraph_test.dir/opt/converter_nomigraph_test.cc.o\r\n[ 78%] Linking CXX executable ../bin/distributed_test\r\n[ 78%] Built target distributed_test\r\n[ 78%] Linking CXX executable ../bin/conv_to_nnpack_transform_test\r\nScanning dependencies of target net_dag_utils_test\r\n[ 78%] Building CXX object caffe2/CMakeFiles/net_dag_utils_test.dir/core/net_dag_utils_test.cc.o\r\n[ 78%] Linking CXX executable ../bin/dead_code_elim_test\r\n[ 78%] Built target conv_to_nnpack_transform_test\r\nScanning dependencies of target string_ops_test\r\n[ 78%] Building CXX object caffe2/CMakeFiles/string_ops_test.dir/operators/string_ops_test.cc.o\r\n[ 78%] Built target dead_code_elim_test\r\n[ 78%] Linking CXX executable ../bin/common_subexpression_elimination_test\r\nScanning dependencies of target net_async_tracing_test\r\n[ 78%] Building CXX object caffe2/CMakeFiles/net_async_tracing_test.dir/core/net_async_tracing_test.cc.o\r\n[ 78%] Built target common_subexpression_elimination_test\r\nScanning dependencies of target graph_test\r\n[ 78%] Linking CXX executable ../bin/depthwise3x3_conv_op_test\r\n[ 78%] Building CXX object caffe2/CMakeFiles/graph_test.dir/core/graph_test.cc.o\r\n[ 78%] Built target depthwise3x3_conv_op_test\r\n[ 78%] Linking CXX executable ../bin/nnpack_test\r\nScanning dependencies of target weakref_test\r\n[ 78%] Building CXX object caffe2/CMakeFiles/weakref_test.dir/__/aten/src/ATen/test/weakref_test.cpp.o\r\n[ 78%] Built target nnpack_test\r\nScanning dependencies of target event_test\r\n[ 78%] Building CXX object caffe2/CMakeFiles/event_test.dir/core/event_test.cc.o\r\n[ 78%] Linking CXX executable ../bin/converter_nomigraph_test\r\n[ 78%] Built target converter_nomigraph_test\r\nScanning dependencies of target init_test\r\n[ 78%] Building CXX object caffe2/CMakeFiles/init_test.dir/core/init_test.cc.o\r\n[ 78%] Linking CXX executable ../bin/pattern_net_transform_test\r\n[ 78%] Built target pattern_net_transform_test\r\nScanning dependencies of target tbb_init_test\r\n[ 78%] Building CXX object caffe2/CMakeFiles/tbb_init_test.dir/__/aten/src/ATen/test/tbb_init_test.cpp.o\r\n[ 78%] Linking CXX executable ../bin/event_test\r\n[ 78%] Built target event_test\r\n[ 78%] Linking CXX executable ../bin/net_async_tracing_test\r\nScanning dependencies of target int8_roi_align_op_test\r\n[ 78%] Linking CXX executable ../bin/weakref_test\r\n[ 78%] Linking CXX executable ../bin/init_test\r\n[ 78%] Building CXX object caffe2/CMakeFiles/int8_roi_align_op_test.dir/operators/quantized/int8_roi_align_op_test.cc.o\r\n[ 78%] Linking CXX executable ../bin/net_dag_utils_test\r\n[ 78%] Built target net_async_tracing_test\r\n[ 78%] Built target init_test\r\nScanning dependencies of target verify_api_visibility\r\n[ 78%] Building CXX object caffe2/CMakeFiles/verify_api_visibility.dir/__/aten/src/ATen/test/verify_api_visibility.cpp.o\r\nScanning dependencies of target timer_test\r\n[ 78%] Built target net_dag_utils_test\r\n[ 78%] Building CXX object caffe2/CMakeFiles/timer_test.dir/core/timer_test.cc.o\r\n[ 78%] Built target weakref_test\r\nScanning dependencies of target wrapdim_test\r\n[ 78%] Building CXX object caffe2/CMakeFiles/wrapdim_test.dir/__/aten/src/ATen/test/wrapdim_test.cpp.o\r\nScanning dependencies of target scalar_tensor_test\r\n[ 78%] Building CXX object caffe2/CMakeFiles/scalar_tensor_test.dir/__/aten/src/ATen/test/scalar_tensor_test.cpp.o\r\n[ 78%] Linking CXX executable ../bin/tbb_init_test\r\n[ 78%] Linking CXX executable ../bin/string_ops_test\r\n[ 78%] Built target tbb_init_test\r\nScanning dependencies of target net_test\r\n[ 78%] Building CXX object caffe2/CMakeFiles/net_test.dir/core/net_test.cc.o\r\n[ 78%] Built target string_ops_test\r\n[ 78%] Linking CXX executable ../bin/graph_test\r\nScanning dependencies of target observer_test\r\n[ 78%] Building CXX object caffe2/CMakeFiles/observer_test.dir/core/observer_test.cc.o\r\n[ 78%] Built target graph_test\r\nScanning dependencies of target operator_schema_test\r\n[ 78%] Building CXX object caffe2/CMakeFiles/operator_schema_test.dir/core/operator_schema_test.cc.o\r\n[ 78%] Linking CXX executable ../bin/timer_test\r\n[ 78%] Built target timer_test\r\n[ 78%] Linking CXX executable ../bin/verify_api_visibility\r\nScanning dependencies of target cast_test\r\n[ 78%] Building CXX object caffe2/CMakeFiles/cast_test.dir/utils/cast_test.cc.o\r\n[ 78%] Built target verify_api_visibility\r\nScanning dependencies of target text_file_reader_utils_test\r\n[ 78%] Building CXX object caffe2/CMakeFiles/text_file_reader_utils_test.dir/operators/text_file_reader_utils_test.cc.o\r\n[ 79%] Linking CXX executable ../bin/wrapdim_test\r\n[ 79%] Built target wrapdim_test\r\nScanning dependencies of target TarjansImplTest\r\n[ 79%] Building CXX object caffe2/CMakeFiles/TarjansImplTest.dir/core/nomnigraph/tests/TarjansImplTest.cc.o\r\n[ 79%] Linking CXX executable ../bin/scalar_tensor_test\r\n[ 79%] Linking CXX executable ../bin/cast_test\r\n[ 79%] Built target scalar_tensor_test\r\n[ 79%] Linking CXX executable ../bin/int8_roi_align_op_test\r\nScanning dependencies of target inline_container_test\r\n[ 79%] Building CXX object caffe2/CMakeFiles/inline_container_test.dir/serialize/inline_container_test.cc.o\r\n[ 79%] Built target cast_test\r\nScanning dependencies of target dlconvertor_test\r\n[ 80%] Building CXX object caffe2/CMakeFiles/dlconvertor_test.dir/__/aten/src/ATen/test/dlconvertor_test.cpp.o\r\n[ 80%] Built target int8_roi_align_op_test\r\nScanning dependencies of target scalar_test\r\n[ 80%] Building CXX object caffe2/CMakeFiles/scalar_test.dir/__/aten/src/ATen/test/scalar_test.cpp.o\r\n[ 80%] Linking CXX executable ../bin/text_file_reader_utils_test\r\n\x1b[91mCMakeFiles/text_file_reader_utils_test.dir/operators/text_file_reader_utils_test.cc.o: In function `caffe2::TextFileReaderUtilsTest_TokenizeTest_Test::TestBody()\':\r\ntext_file_reader_utils_test.cc:(.text+0x22e9): warning: the use of `tmpnam\' is dangerous, better use `mkstemp\'\r\n\x1b[0m[ 80%] Built target text_file_reader_utils_test\r\nScanning dependencies of target mpi_test\r\n[ 80%] Building CXX object caffe2/CMakeFiles/mpi_test.dir/mpi/mpi_test.cc.o\r\n[ 80%] Linking CXX executable ../bin/observer_test\r\n[ 80%] Built target observer_test\r\nScanning dependencies of target native_test\r\n[ 80%] Building CXX object caffe2/CMakeFiles/native_test.dir/__/aten/src/ATen/test/native_test.cpp.o\r\n[ 80%] Linking CXX executable ../bin/net_test\r\n[ 80%] Linking CXX executable ../bin/operator_schema_test\r\n[ 80%] Built target net_test\r\n[ 80%] Linking CXX executable ../bin/inline_container_test\r\n[ 80%] Built target operator_schema_test\r\nScanning dependencies of target int8_test\r\n[ 80%] Building CXX object caffe2/CMakeFiles/int8_test.dir/operators/quantized/int8_test.cc.o\r\nScanning dependencies of target net_simple_refcount_test\r\n[ 80%] Building CXX object caffe2/CMakeFiles/net_simple_refcount_test.dir/core/net_simple_refcount_test.cc.o\r\n\x1b[91mCMakeFiles/inline_container_test.dir/serialize/inline_container_test.cc.o: In function `at::(anonymous namespace)::PyTorchFileWriterAndReader_SaveAndLoad_Test::TestBody()\':\r\ninline_container_test.cc:(.text+0x302): warning: the use of `tmpnam\' is dangerous, better \x1b[0m\x1b[91muse `mkstemp\'\r\n\x1b[0m[ 80%] Built target inline_container_test\r\nScanning dependencies of target common_test\r\n[ 80%] Building CXX object caffe2/CMakeFiles/common_test.dir/core/common_test.cc.o\r\n[ 80%] Linking CXX executable ../bin/dlconvertor_test\r\n[ 80%] Built target dlconvertor_test\r\nScanning dependencies of target module_test\r\n[ 80%] Building CXX object caffe2/CMakeFiles/module_test.dir/core/module_test.cc.o\r\n[ 80%] Linking CXX executable ../bin/TarjansImplTest\r\n[ 80%] Built target TarjansImplTest\r\n[ 80%] Linking CXX executable ../bin/scalar_test\r\nScanning dependencies of target SubgraphMatcherTest\r\n[ 80%] Building CXX object caffe2/CMakeFiles/SubgraphMatcherTest.dir/core/nomnigraph/tests/SubgraphMatcherTest.cc.o\r\n[ 80%] Linking CXX executable ../bin/common_test\r\n[ 80%] Built target scalar_test\r\nScanning dependencies of target generate_proposals_op_util_nms_test\r\n[ 80%] Building CXX object caffe2/CMakeFiles/generate_proposals_op_util_nms_test.dir/operators/generate_proposals_op_util_nms_test.cc.o\r\n[ 80%] Built target common_test\r\nScanning dependencies of target context_test\r\n[ 80%] Building CXX object caffe2/CMakeFiles/context_test.dir/core/context_test.cc.o\r\n[ 80%] Linking CXX executable ../bin/native_test\r\n[ 80%] Built target native_test\r\nScanning dependencies of target batch_matmul_op_test\r\n[ 80%] Building CXX object caffe2/CMakeFiles/batch_matmul_op_test.dir/operators/batch_matmul_op_test.cc.o\r\n[ 80%] Linking CXX executable ../bin/mpi_test\r\n[ 80%] Built target mpi_test\r\nScanning dependencies of target test_parallel\r\n[ 80%] Building CXX object caffe2/CMakeFiles/test_parallel.dir/__/aten/src/ATen/test/test_parallel.cpp.o\r\n[ 80%] Linking CXX executable ../bin/net_simple_refcount_test\r\n[ 80%] Built target net_simple_refcount_test\r\n[ 80%] Linking CXX executable ../bin/context_test\r\nScanning dependencies of target basic\r\n[ 80%] Building CXX object caffe2/CMakeFiles/basic.dir/__/aten/src/ATen/test/basic.cpp.o\r\n[ 80%] Linking CXX executable ../bin/SubgraphMatcherTest\r\n[ 81%] Linking CXX executable ../bin/module_test\r\n[ 81%] Built target context_test\r\nScanning dependencies of target TopoSortTest\r\n[ 81%] Building CXX object caffe2/CMakeFiles/TopoSortTest.dir/core/nomnigraph/tests/TopoSortTest.cc.o\r\n[ 81%] Built target SubgraphMatcherTest\r\n[ 81%] Built target module_test\r\nScanning dependencies of target half_test\r\nScanning dependencies of target GraphTest\r\n[ 81%] Building CXX object caffe2/CMakeFiles/half_test.dir/__/aten/src/ATen/test/half_test.cpp.o\r\n[ 81%] Building CXX object caffe2/CMakeFiles/GraphTest.dir/core/nomnigraph/tests/GraphTest.cc.o\r\n[ 82%] Linking CXX executable ../bin/test_parallel\r\n[ 82%] Linking CXX executable ../bin/generate_proposals_op_util_nms_test\r\n[ 82%] Linking CXX executable ../bin/batch_matmul_op_test\r\n[ 82%] Built target test_parallel\r\n[ 82%] Built target generate_proposals_op_util_nms_test\r\nScanning dependencies of target broadcast_test\r\n[ 82%] Built target batch_matmul_op_test\r\n[ 82%] Building CXX object caffe2/CMakeFiles/broadcast_test.dir/__/aten/src/ATen/test/broadcast_test.cpp.o\r\nScanning dependencies of target operator_test\r\n[ 82%] Building CXX object caffe2/CMakeFiles/operator_test.dir/core/operator_test.cc.o\r\nScanning dependencies of target parallel_net_test\r\n[ 82%] Building CXX object caffe2/CMakeFiles/parallel_net_test.dir/core/parallel_net_test.cc.o\r\n[ 82%] Linking CXX executable ../bin/TopoSortTest\r\n[ 82%] Built target TopoSortTest\r\n[ 82%] Linking CXX executable ../bin/basic\r\nScanning dependencies of target stats_test\r\n[ 82%] Building CXX object caffe2/CMakeFiles/stats_test.dir/core/stats_test.cc.o\r\n[ 82%] Built target basic\r\n[ 83%] Linking CXX executable ../bin/GraphTest\r\nScanning dependencies of target mobile_test\r\n[ 83%] Building CXX object caffe2/CMakeFiles/mobile_test.dir/opt/mobile_test.cc.o\r\n[ 83%] Linking CXX executable ../bin/half_test\r\n[ 83%] Linking CXX executable ../bin/int8_test\r\n[ 83%] Built target GraphTest\r\nScanning dependencies of target undefined_tensor_test\r\n[ 83%] Building CXX object caffe2/CMakeFiles/undefined_tensor_test.dir/__/aten/src/ATen/test/undefined_tensor_test.cpp.o\r\n[ 83%] Built target half_test\r\n[ 83%] Built target int8_test\r\nScanning dependencies of target workspace_test\r\n[ 83%] Building CXX object caffe2/CMakeFiles/workspace_test.dir/core/workspace_test.cc.o\r\nScanning dependencies of target apply_utils_test\r\n[ 83%] Building CXX object caffe2/CMakeFiles/apply_utils_test.dir/__/aten/src/ATen/test/apply_utils_test.cpp.o\r\n[ 83%] Linking CXX executable ../bin/mobile_test\r\n[ 83%] Linking CXX executable ../bin/broadcast_test\r\n[ 83%] Built target mobile_test\r\nScanning dependencies of target backend_cutting_test\r\n[ 83%] Building CXX object caffe2/CMakeFiles/backend_cutting_test.dir/opt/backend_cutting_test.cc.o\r\n[ 83%] Built target broadcast_test\r\nScanning dependencies of target math_test\r\n[ 83%] Building CXX object caffe2/CMakeFiles/math_test.dir/utils/math_test.cc.o\r\n[ 83%] Linking CXX executable ../bin/parallel_net_test\r\n[ 83%] Linking CXX executable ../bin/stats_test\r\n[ 83%] Built target parallel_net_test\r\nScanning dependencies of target NeuralNetTest\r\n[ 83%] Building CXX object caffe2/CMakeFiles/NeuralNetTest.dir/core/nomnigraph/tests/NeuralNetTest.cc.o\r\n[ 83%] Built target stats_test\r\nScanning dependencies of target fatal_signal_asan_no_sig_test\r\n[ 83%] Building CXX object caffe2/CMakeFiles/fatal_signal_asan_no_sig_test.dir/utils/fatal_signal_asan_no_sig_test.cc.o\r\n[ 83%] Linking CXX executable ../bin/undefined_tensor_test\r\n[ 83%] Built target undefined_tensor_test\r\nScanning dependencies of target simple_queue_test\r\n[ 84%] Building CXX object caffe2/CMakeFiles/simple_queue_test.dir/utils/simple_queue_test.cc.o\r\n[ 84%] Linking CXX executable ../bin/backend_cutting_test\r\n[ 84%] Linking CXX executable ../bin/operator_test\r\n[ 84%] Linking CXX executable ../bin/workspace_test\r\n[ 84%] Built target backend_cutting_test\r\n[ 84%] Built target operator_test\r\nScanning dependencies of target proto_utils_test\r\n[ 84%] Built target workspace_test\r\nScanning dependencies of target cpuid_test\r\n[ 84%] Building CXX object caffe2/CMakeFiles/proto_utils_test.dir/utils/proto_utils_test.cc.o\r\n[ 84%] Building CXX object caffe2/CMakeFiles/cpuid_test.dir/utils/cpuid_test.cc.o\r\nScanning dependencies of target time_observer_test\r\n[ 84%] Building CXX object caffe2/CMakeFiles/time_observer_test.dir/observers/time_observer_test.cc.o\r\n[ 84%] Linking CXX executable ../bin/fatal_signal_asan_no_sig_test\r\n[ 84%] Linking CXX executable ../bin/simple_queue_test\r\n[ 84%] Built target fatal_signal_asan_no_sig_test\r\nScanning dependencies of target fixed_divisor_test\r\n[ 84%] Building CXX object caffe2/CMakeFiles/fixed_divisor_test.dir/utils/fixed_divisor_test.cc.o\r\n[ 84%] Built target simple_queue_test\r\nScanning dependencies of target smart_tensor_printer_test\r\n[ 84%] Building CXX object caffe2/CMakeFiles/smart_tensor_printer_test.dir/utils/smart_tensor_printer_test.cc.o\r\n[ 84%] Linking CXX executable ../bin/apply_utils_test\r\n[ 84%] Linking CXX executable ../bin/cpuid_test\r\n[ 84%] Linking CXX executable ../bin/NeuralNetTest\r\n[ 84%] Built target apply_utils_test\r\n[ 84%] Built target cpuid_test\r\nScanning dependencies of target transform_test\r\n[ 84%] Building CXX object caffe2/CMakeFiles/transform_test.dir/core/transform_test.cc.o\r\n[ 84%] Built target NeuralNetTest\r\nScanning dependencies of target conv_transpose_op_mobile_test\r\n[ 84%] Building CXX object caffe2/CMakeFiles/conv_transpose_op_mobile_test.dir/operators/conv_transpose_op_mobile_test.cc.o\r\nScanning dependencies of target predictor_test\r\n[ 84%] Linking CXX executable ../bin/fixed_divisor_test\r\n[ 84%] Building CXX object caffe2/CMakeFiles/predictor_test.dir/predictor/predictor_test.cc.o\r\n[ 84%] Linking CXX executable ../bin/proto_utils_test\r\n[ 85%] Linking CXX executable ../bin/math_test\r\n\x1b[91mCMakeFiles/proto_utils_test.dir/utils/proto_utils_test.cc.o: In function `caffe2::ProtoUtilsTest_SimpleReadWrite_Test::TestBody()\':\r\nproto_utils_test.cc:(.text+0xa0): warning: the use of `tmpnam\' is dangerous, better use `mkstemp\'\r\n\x1b[0m[ 85%] Built target proto_utils_test\r\n[ 85%] Built target fixed_divisor_test\r\nScanning dependencies of target AlgorithmsTest\r\n[ 85%] Building CXX object caffe2/CMakeFiles/AlgorithmsTest.dir/core/nomnigraph/tests/AlgorithmsTest.cc.o\r\nScanning dependencies of target blob_test\r\n[ 86%] Building CXX object caffe2/CMakeFiles/blob_test.dir/core/blob_test.cc.o\r\n[ 86%] Built target math_test\r\nScanning dependencies of target BinaryMatchImplTest\r\n[ 86%] Building CXX object caffe2/CMakeFiles/BinaryMatchImplTest.dir/core/nomnigraph/tests/BinaryMatchImplTest.cc.o\r\n[ 86%] Linking CXX executable ../bin/smart_tensor_printer_test\r\n[ 86%] Built target smart_tensor_printer_test\r\nScanning dependencies of target utility_ops_test\r\n[ 86%] Building CXX object caffe2/CMakeFiles/utility_ops_test.dir/operators/utility_ops_test.cc.o\r\n[ 86%] Linking CXX executable ../bin/time_observer_test\r\n[ 86%] Built target time_observer_test\r\nScanning dependencies of target MatchTest\r\n[ 86%] Building CXX object caffe2/CMakeFiles/MatchTest.dir/core/nomnigraph/tests/MatchTest.cc.o\r\n[ 86%] Linking CXX executable ../bin/BinaryMatchImplTest\r\n[ 86%] Built target BinaryMatchImplTest\r\nScanning dependencies of target caffe2_pybind11_state\r\n[ 86%] Building CXX object caffe2/CMakeFiles/caffe2_pybind11_state.dir/python/pybind_state.cc.o\r\n[ 86%] Linking CXX executable ../bin/conv_transpose_op_mobile_test\r\n[ 87%] Linking CXX executable ../bin/predictor_test\r\n[ 87%] Built target conv_transpose_op_mobile_test\r\n[ 87%] Linking CXX executable ../bin/MatchTest\r\n[ 87%] Building CXX object caffe2/CMakeFiles/caffe2_pybind11_state.dir/python/pybind_state_dlpack.cc.o\r\n[ 87%] Linking CXX executable ../bin/AlgorithmsTest\r\n[ 87%] Built target predictor_test\r\n[ 87%] Building CXX object caffe2/CMakeFiles/caffe2_pybind11_state.dir/python/pybind_state_nomni.cc.o\r\n[ 87%] Built target MatchTest\r\n[ 87%] Building CXX object caffe2/CMakeFiles/caffe2_pybind11_state.dir/python/pybind_state_registry.cc.o\r\n[ 87%] Built target AlgorithmsTest\r\n[ 87%] Building CXX object caffe2/CMakeFiles/caffe2_pybind11_state.dir/python/pybind_state_ideep.cc.o\r\n[ 87%] Linking CXX executable ../bin/transform_test\r\n[ 87%] Linking CXX executable ../bin/utility_ops_test\r\n[ 87%] Built target transform_test\r\nScanning dependencies of target atest\r\n[ 87%] Built target utility_ops_test\r\n[ 87%] Building CXX object caffe2/CMakeFiles/atest.dir/__/aten/src/ATen/test/atest.cpp.o\r\nScanning dependencies of target ssa_test\r\n[ 87%] Building CXX object caffe2/CMakeFiles/ssa_test.dir/onnx/ssa_test.cc.o\r\nScanning dependencies of target generate_proposals_op_util_boxes_test\r\n[ 87%] Building CXX object caffe2/CMakeFiles/generate_proposals_op_util_boxes_test.dir/operators/generate_proposals_op_util_boxes_test.cc.o\r\nScanning dependencies of target boolean_unmask_ops_test\r\n[ 87%] Building CXX object caffe2/CMakeFiles/boolean_unmask_ops_test.dir/operators/boolean_unmask_ops_test.cc.o\r\n[ 88%] Linking CXX executable ../bin/ssa_test\r\n[ 88%] Built target ssa_test\r\nScanning dependencies of target elementwise_op_test\r\n[ 88%] Building CXX object caffe2/CMakeFiles/elementwise_op_test.dir/operators/elementwise_op_test.cc.o\r\n[ 88%] Linking CXX executable ../bin/atest\r\n[ 88%] Built target atest\r\nScanning dependencies of target generate_proposals_op_test\r\n[ 88%] Building CXX object caffe2/CMakeFiles/generate_proposals_op_test.dir/operators/generate_proposals_op_test.cc.o\r\n[ 88%] Linking CXX executable ../bin/boolean_unmask_ops_test\r\n[ 88%] Built target boolean_unmask_ops_test\r\n[ 88%] Generating ../../../torch/csrc/nn/THNN.cpp, ../../../torch/csrc/nn/THCUNN.cpp, ../../../torch/csrc/autograd/generated/VariableType.h, ../../../torch/csrc/autograd/generated/VariableType_0.cpp, ../../../torch/csrc/autograd/generated/VariableType_1.cpp, ../../../torch/csrc/autograd/generated/VariableType_2.cpp, ../../../torch/csrc/autograd/generated/VariableType_3.cpp, ../../../torch/csrc/autograd/generated/VariableType_4.cpp, ../../../torch/csrc/autograd/generated/Functions.h, ../../../torch/csrc/autograd/generated/Functions.cpp, ../../../torch/csrc/autograd/generated/python_functions.h, ../../../torch/csrc/autograd/generated/python_functions.cpp, ../../../torch/csrc/autograd/generated/python_variable_methods.cpp, ../../../torch/csrc/autograd/generated/python_variable_methods_dispatch.h, ../../../torch/csrc/autograd/generated/python_torch_functions.cpp, ../../../torch/csrc/autograd/generated/python_torch_functions_dispatch.h, ../../../torch/csrc/autograd/generated/python_nn_functions.cpp, ../../../torch/csrc/autograd/generated/python_nn_functions.h, ../../../torch/csrc/autograd/generated/python_nn_functions_dispatch.h, ../../../torch/csrc/autograd/generated/variable_factories.h, ../../../torch/csrc/jit/generated/register_aten_ops_0.cpp, ../../../torch/csrc/jit/generated/register_aten_ops_1.cpp, ../../../torch/csrc/jit/generated/register_aten_ops_2.cpp, ../../../torch/csrc/jit/generated/aten_interned_strings.h\r\n[ 88%] Linking CXX executable ../bin/generate_proposals_op_util_boxes_test\r\n[ 88%] Built target generate_proposals_op_util_boxes_test\r\nScanning dependencies of target THD\r\n[ 88%] Building CXX object caffe2/torch/lib/THD/CMakeFiles/THD.dir/base/ChannelUtils.cpp.o\r\n[ 88%] Linking CXX executable ../bin/elementwise_op_test\r\n[ 88%] Building CXX object caffe2/torch/lib/THD/CMakeFiles/THD.dir/base/Cuda.cpp.o\r\n[ 88%] Built target elementwise_op_test\r\n[ 88%] Building CXX object caffe2/torch/lib/THD/CMakeFiles/THD.dir/base/DataChannel.cpp.o\r\n[ 89%] Linking CXX executable ../bin/generate_proposals_op_test\r\n[ 89%] Linking CXX executable ../bin/blob_test\r\n[ 89%] Built target generate_proposals_op_test\r\nScanning dependencies of target c10d\r\nScanning dependencies of target shm\r\n[ 89%] Building CXX object caffe2/torch/lib/libshm/CMakeFiles/shm.dir/core.cpp.o\r\n[ 89%] Building CXX object caffe2/torch/lib/c10d/CMakeFiles/c10d.dir/FileStore.cpp.o\r\n\x1b[91mCMakeFiles/blob_test.dir/core/blob_test.cc.o: In function `caffe2::(anonymous namespace):\x1b[0m\x1b[91m:ContentChunks_Serialization_Test::TestBody()\':\r\nblob_test.cc:(.text+0x30f7e): warning: the use of `tmpnam\' is dangerous, better use `mkstemp\'\r\n\x1b[0m[ 89%] Built target blob_test\r\n[ 89%] Building CXX object caffe2/torch/lib/c10d/CMakeFiles/c10d.dir/ProcessGroup.cpp.o\r\n[ 89%] Building CXX object caffe2/torch/lib/c10d/CMakeFiles/c10d.dir/Store.cpp.o\r\n\x1b[91mWARNING: derivative ignored for _indices\r\nWARNING: derivative ignored for _values\r\nWARNING: derivative ignored for indices\r\nWARNING: derivative ignored for _indices\r\nWARNING: derivative ignored for indices\r\nWARNING: derivative ignored for _values\r\nWARNING: derivative ignored for _indices\r\nWARNING: derivative ignored for _values\r\nWARNING: derivative ignored for indices\r\n\x1b[0mWriting torch/csrc/nn/THNN.cpp\r\nWriting torch/csrc/nn/THCUNN.cpp\r\nWriting torch/csrc/autograd/generated/VariableType.h\r\nWriting torch/csrc/autograd/generated/VariableType_0.cpp\r\nWriting torch/csrc/autograd/generated/VariableType_1.cpp\r\nWriting torch/csrc/autograd/generated/VariableType_2.cpp\r\nWriting torch/csrc/autograd/generated/VariableType_3.cpp\r\nWriting torch/csrc/autograd/generated/VariableType_4.cpp\r\nWriting torch/csrc/autograd/generated/VariableTypeEverything.cpp\r\nWriting torch/csrc/autograd/generated/Functions.h\r\nWriting torch/csrc/autograd/generated/Functions.cpp\r\nWriting torch/csrc/autograd/generated/python_functions.h\r\nWriting torch/csrc/autograd/generated/python_functions.cpp\r\nWriting torch/csrc/autograd/generated/python_variable_methods.cpp\r\nWriting torch/csrc/autograd/generated/python_variable_methods_dispatch.h\r\nWriting torch/csrc/autograd/generated/python_torch_functions.cpp\r\nWriting torch/csrc/autograd/generated/python_torch_functions_dispatch.h\r\nWriting torch/csrc/autograd/generated/python_nn_functions.cpp\r\nWriting torch/csrc/autograd/generated/python_nn_functions.h\r\nWriting torch/csrc/autograd/generated/python_nn_functions_dispatch.h\r\nWriting torch/csrc/autograd/generated/variable_factories.h\r\nWriting torch/csrc/jit/generated/register_aten_ops_0.cpp\r\nWriting torch/csrc/jit/generated/register_aten_ops_1.cpp\r\nWriting torch/csrc/jit/generated/register_aten_ops_2.cpp\r\nWriting torch/csrc/jit/generated/aten_interned_strings.h\r\n[ 90%] Building CXX object caffe2/torch/lib/c10d/CMakeFiles/c10d.dir/PrefixStore.cpp.o\r\n[ 90%] Building CXX object caffe2/torch/lib/c10d/CMakeFiles/c10d.dir/TCPStore.cpp.o\r\n[ 90%] Linking CXX shared library ../../../../lib/libshm.so\r\nScanning dependencies of target caffe2_detectron_ops\r\n[ 91%] Building CXX object modules/detectron/CMakeFiles/caffe2_detectron_ops.dir/batch_permutation_op.cc.o\r\n[ 91%] Built target shm\r\n[ 91%] Building CXX object modules/detectron/CMakeFiles/caffe2_detectron_ops.dir/group_spatial_softmax_op.cc.o\r\nScanning dependencies of target caffe2_module_test_dynamic\r\n[ 91%] Building CXX object modules/module_test/CMakeFiles/caffe2_module_test_dynamic.dir/module_test_dynamic.cc.o\r\n[ 92%] Building CXX object caffe2/torch/lib/THD/CMakeFiles/THD.dir/base/DataChannelRequest.cpp.o\r\nScanning dependencies of target torch\r\n[ 92%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/anomaly_mode.cpp.o\r\n[ 92%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/engine.cpp.o\r\n[ 92%] Building CXX object caffe2/torch/lib/THD/CMakeFiles/THD.dir/base/RPCType.cpp.o\r\n[ 92%] Building CXX object caffe2/torch/lib/c10d/CMakeFiles/c10d.dir/Utils.cpp.o\r\n[ 92%] Building CXX object caffe2/torch/lib/c10d/CMakeFiles/c10d.dir/ProcessGroupGloo.cpp.o\r\n[ 92%] Building CXX object caffe2/torch/lib/THD/CMakeFiles/THD.dir/base/data_channels/DataChannelMPI.cpp.o\r\n[ 92%] Linking CXX shared library ../../lib/libcaffe2_module_test_dynamic.so\r\n[ 92%] Built target caffe2_module_test_dynamic\r\n[ 92%] Building CXX object caffe2/torch/lib/c10d/CMakeFiles/c10d.dir/ProcessGroupMPI.cpp.o\r\nScanning dependencies of target caffe2_observers\r\n[ 92%] Building CXX object caffe2/torch/lib/THD/CMakeFiles/THD.dir/base/data_channels/DataChannelTCP.cpp.o\r\n[ 93%] Building CXX object modules/observers/CMakeFiles/caffe2_observers.dir/net_observer_reporter_print.cc.o\r\n\x1b[91m/opt/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp: In destructor ‚Äòvirtual c10d::ProcessGroupMPI::AsyncWork::~AsyncWork()‚Äô:\r\n/opt/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp:154:71: error: throw will always call terminate() [-Werror=terminate]\r\n         ""Attempted destruction of AsyncWork before work has completed"");\r\n                                                                       ^\r\n/opt/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp:154:71: note: in C++11 destructors default to noexcept\r\n\x1b[0m[ 93%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/function.cpp.o\r\n[ 93%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/functions/accumulate_grad.cpp.o\r\n[ 93%] Building CXX object modules/detectron/CMakeFiles/caffe2_detectron_ops.dir/ps_roi_pool_op.cc.o\r\n[ 93%] Building CXX object modules/observers/CMakeFiles/caffe2_observers.dir/observer_config.cc.o\r\n\x1b[91mcc1plus: all warnings being treated as errors\r\n\x1b[0m\x1b[91mmake[2]: *** [caffe2/torch/lib/c10d/CMakeFiles/c10d.dir/ProcessGroupMPI.cpp.o] Error 1\r\n\x1b[0m\x1b[91mmake[2]: *** Waiting for unfinished jobs....\r\n\x1b[0mcaffe2/torch/lib/c10d/CMakeFiles/c10d.dir/build.make:230: recipe for target \'caffe2/torch/lib/c10d/CMakeFiles/c10d.dir/ProcessGroupMPI.cpp.o\' failed\r\n[ 93%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/functions/basic_ops.cpp.o\r\n[ 93%] Building CXX object caffe2/torch/lib/THD/CMakeFiles/THD.dir/base/init_methods/InitMethod.cpp.o\r\n[ 93%] Building CXX object modules/observers/CMakeFiles/caffe2_observers.dir/perf_observer.cc.o\r\n[ 93%] Building CXX object modules/detectron/CMakeFiles/caffe2_detectron_ops.dir/roi_pool_f_op.cc.o\r\n[ 93%] Building CXX object modules/detectron/CMakeFiles/caffe2_detectron_ops.dir/sample_as_op.cc.o\r\n[ 93%] Building CXX object caffe2/torch/lib/THD/CMakeFiles/THD.dir/base/init_methods/InitMethodEnv.cpp.o\r\n[ 93%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/functions/comm.cpp.o\r\n[ 93%] Building CXX object modules/detectron/CMakeFiles/caffe2_detectron_ops.dir/select_smooth_l1_loss_op.cc.o\r\n[ 93%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/functions/tensor.cpp.o\r\nCMakeFiles/Makefile2:7496: recipe for target \'caffe2/torch/lib/c10d/CMakeFiles/c10d.dir/all\' failed\r\n\x1b[91mmake[1]: *** [caffe2/torch/lib/c10d/CMakeFiles/c10d.dir/all] Error 2\r\n\x1b[0m\x1b[91mmake[1]: *** Waiting for unfinished jobs....\r\n\x1b[0m[ 93%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/functions/utils.cpp.o\r\n[ 93%] Building CXX object caffe2/torch/lib/THD/CMakeFiles/THD.dir/base/init_methods/InitMethodFile.cpp.o\r\n[ 93%] Linking CXX shared module python/caffe2_pybind11_state.cpython-36m-x86_64-linux-gnu.so\r\n[ 93%] Linking CXX shared library ../../lib/libcaffe2_observers.so\r\n[ 93%] Built target caffe2_observers\r\n[ 94%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/generated/Functions.cpp.o\r\n[ 94%] Built target caffe2_pybind11_state\r\n[ 94%] Building CXX object caffe2/torch/lib/THD/CMakeFiles/THD.dir/base/init_methods/InitMethodTCP.cpp.o\r\n[ 94%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/generated/VariableType_0.cpp.o\r\n[ 94%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/generated/VariableType_1.cpp.o\r\n[ 94%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/generated/VariableType_2.cpp.o\r\n[ 94%] Building CXX object modules/detectron/CMakeFiles/caffe2_detectron_ops.dir/sigmoid_cross_entropy_loss_op.cc.o\r\n[ 94%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/generated/VariableType_3.cpp.o\r\n[ 94%] Building CXX object modules/detectron/CMakeFiles/caffe2_detectron_ops.dir/sigmoid_focal_loss_op.cc.o\r\n[ 94%] Building CXX object caffe2/torch/lib/THD/CMakeFiles/THD.dir/base/init_methods/InitMethodUtils.cpp.o\r\n[ 94%] Building CXX object caffe2/torch/lib/THD/CMakeFiles/THD.dir/process_group/Collectives.cpp.o\r\n[ 94%] Building CXX object caffe2/torch/lib/THD/CMakeFiles/THD.dir/process_group/General.cpp.o\r\n[ 94%] Building CXX object modules/detectron/CMakeFiles/caffe2_detectron_ops.dir/smooth_l1_loss_op.cc.o\r\n[ 94%] Building CXX object modules/detectron/CMakeFiles/caffe2_detectron_ops.dir/softmax_focal_loss_op.cc.o\r\n[ 94%] Linking CXX static library ../../../../lib/libTHD.a\r\n[ 94%] Built target THD\r\n[ 94%] Building CXX object modules/detectron/CMakeFiles/caffe2_detectron_ops.dir/spatial_narrow_as_op.cc.o\r\n[ 94%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/generated/VariableType_4.cpp.o\r\n[ 94%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/grad_mode.cpp.o\r\n[ 94%] Building CXX object modules/detectron/CMakeFiles/caffe2_detectron_ops.dir/upsample_nearest_op.cc.o\r\n[ 94%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/input_buffer.cpp.o\r\n[ 94%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/profiler.cpp.o\r\n[ 94%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/saved_variable.cpp.o\r\n[ 94%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/variable.cpp.o\r\n[ 94%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/VariableTypeManual.cpp.o\r\n[ 94%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/cuda/comm.cpp.o\r\n[ 94%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/autodiff.cpp.o\r\n[ 94%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/export.cpp.o\r\n[ 94%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/generated/register_aten_ops_0.cpp.o\r\n[ 95%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/generated/register_aten_ops_1.cpp.o\r\n[ 95%] Linking CXX shared library ../../lib/libcaffe2_detectron_ops.so\r\n[ 95%] Built target caffe2_detectron_ops\r\n[ 95%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/generated/register_aten_ops_2.cpp.o\r\n[ 95%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/graph_executor.cpp.o\r\n[ 95%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/import_method.cpp.o\r\n[ 95%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/import.cpp.o\r\n[ 95%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/interpreter.cpp.o\r\n[ 95%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/constants.cpp.o\r\n[ 95%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/node_hashing.cpp.o\r\n[ 95%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/ir.cpp.o\r\n[ 95%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/operator.cpp.o\r\n[ 95%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/passes/batch_mm.cpp.o\r\n[ 95%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/passes/canonicalize.cpp.o\r\n[ 95%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/passes/constant_propagation.cpp.o\r\n[ 95%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/passes/constant_pooling.cpp.o\r\n[ 95%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/passes/common_subexpression_elimination.cpp.o\r\n[ 96%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/passes/create_autodiff_subgraphs.cpp.o\r\n[ 96%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/passes/inline_autodiff_subgraphs.cpp.o\r\n[ 96%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/passes/dead_code_elimination.cpp.o\r\n[ 96%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/passes/canonicalize_ops.cpp.o\r\n[ 96%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/passes/erase_number_types.cpp.o\r\n[ 96%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/passes/graph_fuser.cpp.o\r\n[ 96%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/passes/inplace_check.cpp.o\r\n[ 96%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/passes/loop_unrolling.cpp.o\r\n[ 96%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/passes/lower_grad_of.cpp.o\r\n[ 96%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/passes/lower_tuples.cpp.o\r\n[ 96%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/passes/peephole.cpp.o\r\n[ 96%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/passes/remove_expands.cpp.o\r\n[ 96%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/passes/remove_inplace_ops.cpp.o\r\n[ 96%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/passes/shape_analysis.cpp.o\r\n[ 96%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/passes/requires_grad_analysis.cpp.o\r\n[ 96%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/passes/specialize_undef.cpp.o\r\n[ 97%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/passes/python_print.cpp.o\r\n[ 97%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/passes/utils/subgraph_utils.cpp.o\r\n[ 97%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/fuser/interface.cpp.o\r\n[ 97%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/register_prim_ops.cpp.o\r\n[ 97%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/register_special_ops.cpp.o\r\n[ 97%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/scope.cpp.o\r\n[ 97%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/script/compiler.cpp.o\r\n[ 97%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/script/builtin_functions.cpp.o\r\n[ 97%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/script/lexer.cpp.o\r\n[ 97%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/script/module.cpp.o\r\n[ 97%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/tracer.cpp.o\r\n[ 97%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/hooks_for_testing.cpp.o\r\n[ 97%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/torch.cpp.o\r\n[ 97%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/utils/tensor_flatten.cpp.o\r\n\x1b[91m/opt/pytorch/torch/csrc/torch.cpp: In function ‚Äòat::TypeExtendedInterface& torch::CPU(at::ScalarType)‚Äô:\r\n/opt/pytorch/torch/csrc/torch.cpp:11:17: warning: ‚Äòat::TypeExtendedInterface& torch::getVariableType(at::Backend, at::ScalarType)‚Äô is deprecated [-Wdeprecated-declarations]\r\n   return torch::getVariableType(at::Backend::CPU, type);\r\n                 ^~~~~~~~~~~~~~~\r\n/opt/pytorch/torch/csrc/torch.cpp:6:28: note: declared here\r\n at::TypeExtendedInterface& getVariableType(at::Backend backend, at::ScalarType type) {\r\n                            ^~~~~~~~~~~~~~~\r\n/opt/pytorch/torch/csrc/torch.cpp:11:55: warning: ‚Äòat::TypeExtendedInterface& torch::getVariableType(at::Backend, at::ScalarType)‚Äô is deprecated [-Wdeprecated-declarations]\r\n   return torch::getVariableType(at::Backend::CPU, type);\r\n                                                       ^\r\n/opt/pytorch/torch/csrc/torch.cpp:6:28: note: declared here\r\n at::TypeExtendedInterface& getVariableType(at::Backend backend, at::ScalarType type) {\r\n                            ^~~~~~~~~~~~~~~\r\n/opt/pytorch/torch/csrc/torch.cpp:11:55: warning: ‚Äòat::TypeExtendedInterface& torch::getVariableType(at::Backend, at::ScalarType)‚Äô is deprecated [-Wdeprecated-declarations]\r\n   return torch::getVariableType(at::Backend::CPU, type);\r\n                                                       ^\r\n/opt/pytorch/torch/csrc/torch.cpp:6:28: note: declared here\r\n at::TypeExtendedInterface& getVariableType(at::Backend backend, at::ScalarType type) {\r\n                            ^~~~~~~~~~~~~~~\r\n\x1b[0m\x1b[91m/opt/pytorch/torch/csrc/torch.cpp: In function ‚Äòat::TypeExtendedInterface& torch::CUDA(at::ScalarType)‚Äô:\r\n/opt/pytorch/torch/csrc/torch.cpp:15:17: warning: ‚Äòat::TypeExtendedInterface& torch::getVariableType(at::Backend, at::ScalarType)‚Äô is deprecated [-Wdeprecated-declarations]\r\n   return torch::getVariableType(at::Backend::CUDA, type);\r\n                 ^~~~~~~~~~~~~~~\r\n/opt/pytorch/torch/csrc/torch.cpp:6:28: note: declared here\r\n at::TypeExtendedInterface& getVariableType(at::Backend backend, at::ScalarType type) {\r\n                            ^~~~~~~~~~~~~~~\r\n/opt/pytorch/torch/csrc/torch.cpp:15:56: warning: ‚Äòat::TypeExtendedInterface& torch::getVariableType(at::Backend, at::ScalarType)‚Äô is deprecated [-Wdeprecated-declarations]\r\n   return torch::getVariableType(at::Backend::CUDA, type);\r\n                                                        ^\r\n/opt/pytorch/torch/csrc/torch.cpp:6:28: note: declared here\r\n at::TypeExtendedInterface& getVariableType(at::Backend backend, at::ScalarType type) {\r\n                            ^~~~~~~~~~~~~~~\r\n/opt/pytorch/torch/csrc/torch.cpp:15:56: warning: ‚Äòat::TypeExtendedInterface& torch::getVariableType(at::Backend, at::ScalarType)‚Äô is deprecated [-Wdeprecated-declarations]\r\n   return torch::getVariableType(at::Backend::CUDA, type);\r\n                                                        ^\r\n/opt/pytorch/torch/csrc/torch.cpp:6:28: note: declared here\r\n at::TypeExtendedInterface& getVariableType(at::Backend backend, at::ScalarType type) {\r\n                            ^~~~~~~~~~~~~~~\r\n\x1b[0m[ 97%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/utils/variadic.cpp.o\r\n[ 97%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/__/test/cpp/jit/no-gtest.cpp.o\r\n[ 98%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/fuser/kernel_cache.cpp.o\r\n[ 98%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/fuser/compiler.cpp.o\r\n[ 98%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/fuser/executor.cpp.o\r\n[ 98%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/fuser/codegen.cpp.o\r\n[ 98%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/fuser/fallback.cpp.o\r\n[ 98%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/jit/fuser/cpu/fused_kernel.cpp.o\r\n[ 98%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/api/src/cuda.cpp.o\r\n[ 98%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/api/src/data/datasets/mnist.cpp.o\r\n[ 98%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/api/src/data/samplers/random.cpp.o\r\n[ 98%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/api/src/data/samplers/sequential.cpp.o\r\n[ 98%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/api/src/data/samplers/stream.cpp.o\r\n[ 98%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/api/src/jit.cpp.o\r\n[ 98%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/api/src/nn/init.cpp.o\r\n[ 98%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/api/src/nn/module.cpp.o\r\n[ 98%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/api/src/nn/modules/batchnorm.cpp.o\r\n[ 98%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/api/src/nn/modules/conv.cpp.o\r\n[ 99%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/api/src/nn/modules/dropout.cpp.o\r\n[ 99%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/api/src/nn/modules/embedding.cpp.o\r\n[ 99%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/api/src/nn/modules/functional.cpp.o\r\n[ 99%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/api/src/nn/modules/linear.cpp.o\r\n[ 99%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/api/src/nn/modules/rnn.cpp.o\r\n[ 99%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/api/src/optim/adagrad.cpp.o\r\n[ 99%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/api/src/optim/adam.cpp.o\r\n[ 99%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/api/src/optim/lbfgs.cpp.o\r\n[ 99%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/api/src/optim/optimizer.cpp.o\r\n[ 99%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/api/src/optim/rmsprop.cpp.o\r\n[ 99%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/api/src/optim/serialize.cpp.o\r\n[ 99%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/api/src/optim/sgd.cpp.o\r\n[ 99%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/api/src/serialize/input-archive.cpp.o\r\n[ 99%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/api/src/serialize/output-archive.cpp.o\r\n[ 99%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/api/src/utils.cpp.o\r\n[ 99%] Linking CXX shared library ../../lib/libtorch.so\r\n[ 99%] Built target torch\r\n\x1b[91mmake: *** [all] Error 2\r\n\x1b[0mMakefile:138: recipe for target \'all\' failed\r\nsetup.py::build_deps::run()\r\nFailed to run \'bash ../tools/build_pytorch_libs.sh --use-nnpack --use-mkldnn --use-qnnpack caffe2\'\r\nThe command \'/bin/sh -c cd /opt && git clone --recursive https://github.com/pytorch/pytorch     && cd pytorch && git submodule update --init &&     cd /opt/pytorch/third_party/ideep/mkl-dnn &&     git pull https://github.com/intel/mkl-dnn.git --no-commit  --rebase &&     cd /opt/pytorch &&     sed -i \'s/""Use MKLDNN"" OFF/""Use MKLDNN"" ON /g\' CMakeLists.txt &&     sed -i \'s/""Use DISTRIBUTED"" OFF/""Use DISTRIBUTED"" ON /g\' CMakeLists.txt &&     sed -i \'s/for parallel code"" OFF/for parallel code"" ON /g\' CMakeLists.txt &&     PYTHON_EXECUTABLE=/opt/conda/bin/python     PYTHON_LIBRARY=/opt/conda/lib/libpython3.6m.so     PYTHON_INCLUDE_DIR=/opt/conda/include/python3.6m     FULL_CAFFE2=1     USE_OPENMP=1     USE_MKL=1     USE_MKLDNN=1     USE_MKLML=1     USE_SYSTEM_EIGEN_INSTALL=1     USE_ZMQ=1     USE_DISTRIBUTED=1     MKLDNN_LIBRARY=/usr/local/lib     MKLDNN_INCLUDE_DIR=/usr/local/include     MKLDNN_LIB_DIR=/usr/local/lib     python setup.py install &&     cd /opt && rm -rf /opt/pytorch &&     cd /usr/lib && sudo ldconfig\' returned a non-zero code: 1\r\n```', 'cc: @teng-li \r\n\r\n```\r\n/opt/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp:154:71: error: throw will always call terminate() [-Werror=terminate]\r\n         ""Attempted destruction of AsyncWork before work has completed"");\r\n```', 'I was able to override it by passing my own `CFLAGS` on the install where I had the `-w` flag in order to ignore all warnings. Not ideal :/', ""@sadatnfs yes, not ideal, we'll get it fixed."", '#13962 ', 'Might wanna look at #14144 , another error popped up.\r\n\r\nEDIT: That was solved by adding `BUILD_TEST=0`.', '@soumith @teng-li another similar error popped up now, see #14162 ']","['bash \r\nScanning dependencies of target caffe2_observers\r\n[ 92%] Building CXX object caffe2/torch/lib/THD/CMakeFiles/THD.dir/base/data_channels/DataChannelTCP.cpp.o\r\n[ 93%] Building CXX object modules/observers/CMakeFiles/caffe2_observers.dir/net_observer_reporter_print.cc.o\r\n\x1b[91m/opt/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp: In destructor ‚Äòvirtual c10d::ProcessGroupMPI::AsyncWork::~AsyncWork()‚Äô:\r\n/opt/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp:154:71: error: throw will always call terminate() [-Werror=terminate]\r\n         ""Attempted destruction of AsyncWork before work has completed"");\r\n                                                                       ^\r\n/opt/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp:154:71: note: in C++11 destructors default to noexcept\r\n\x1b[0m[ 93%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/function.cpp.o\r\n[ 93%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/functions/accumulate_grad.cpp.o\r\n[ 93%] Building CXX object modules/detectron/CMakeFiles/caffe2_detectron_ops.dir/ps_roi_pool_op.cc.o\r\n[ 93%] Building CXX object modules/observers/CMakeFiles/caffe2_observers.dir/observer_config.cc.o\r\n\x1b[91mcc1plus: all warnings being treated as errors\r\n\x1b[0m\x1b[91mmake[2]: *** [caffe2/torch/lib/c10d/CMakeFiles/c10d.dir/ProcessGroupMPI.cpp.o] Error 1\r\n\x1b[0m\x1b[91mmake[2]: *** Waiting for unfinished jobs....\r\n\x1b[0mcaffe2/torch/lib/c10d/CMakeFiles/c10d.dir/build.make:230: recipe for target \'caffe2/torch/lib/c10d/CMakeFiles/c10d.dir/ProcessGroupMPI.cpp.o\' failed\r\n[ 93%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/functions/basic_ops.cpp.o\r\n[ 93%] Building CXX object caffe2/torch/lib/THD/CMakeFiles/THD.dir/base/init_methods/InitMethod.cpp.o\r\n[ 93%] Building CXX object modules/observers/CMakeFiles/caffe2_observers.dir/perf_observer.cc.o\r\n[ 93%] Building CXX object modules/detectron/CMakeFiles/caffe2_detectron_ops.dir/roi_pool_f_op.cc.o\r\n[ 93%] Building CXX object modules/detectron/CMakeFiles/caffe2_detectron_ops.dir/sample_as_op.cc.o\r\n[ 93%] Building CXX object caffe2/torch/lib/THD/CMakeFiles/THD.dir/base/init_methods/InitMethodEnv.cpp.o\r\n[ 93%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/functions/comm.cpp.o\r\n[ 93%] Building CXX object modules/detectron/CMakeFiles/caffe2_detectron_ops.dir/select_smooth_l1_loss_op.cc.o\r\n[ 93%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/functions/tensor.cpp.o\r\nCMakeFiles/Makefile2:7496: recipe for target \'caffe2/torch/lib/c10d/CMakeFiles/c10d.dir/all\' failed\r\n\x1b[91mmake[1]: *** [caffe2/torch/lib/c10d/CMakeFiles/c10d.dir/all] Error 2\r\n\x1b[0m\x1b[91mmake[1]: *** Waiting for unfinished jobs....\r\n...\r\n...\r\n...\r\n[ 99%] Linking CXX shared library ../../lib/libtorch.so\r\n[ 99%] Built target torch\r\nMakefile:138: recipe for target \'all\' failed\r\n\x1b[91mmake: *** [all] Error 2\r\n\x1b[0msetup.py::build_deps::run()\r\nFailed to run \'bash ../tools/build_pytorch_libs.sh --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2\'\r\n', 'bash \r\n cd /opt && git clone --recursive https://github.com/pytorch/pytorch \\\r\n    && cd pytorch && git submodule update --init && \\\r\n    cd /opt/pytorch/third_party/ideep/mkl-dnn && \\\r\n    git pull https://github.com/intel/mkl-dnn.git --no-commit  --rebase && \\\r\n    cd /opt/pytorch && \\\r\n    sed -i \'s/""Use MKLDNN"" OFF/""Use MKLDNN"" ON /g\' CMakeLists.txt && \\\r\n    sed -i \'s/""Use DISTRIBUTED"" OFF/""Use DISTRIBUTED"" ON /g\' CMakeLists.txt && \\\r\n    sed -i \'s/for parallel code"" OFF/for parallel code"" ON /g\' CMakeLists.txt && \\\r\n    PYTHON_EXECUTABLE=/opt/conda/bin/python \\\r\n    PYTHON_LIBRARY=/opt/conda/lib/libpython3.6m.so \\\r\n    PYTHON_INCLUDE_DIR=/opt/conda/include/python3.6m \\\r\n    FULL_CAFFE2=1 \\\r\n    USE_OPENMP=1 \\\r\n    USE_MKL=1 \\\r\n    USE_MKLDNN=1 \\\r\n    USE_MKLML=1 \\\r\n    USE_SYSTEM_EIGEN_INSTALL=1 \\\r\n    USE_ZMQ=1 \\\r\n    USE_DISTRIBUTED=1 \\\r\n    MKLDNN_LIBRARY=/usr/local/lib \\\r\n    MKLDNN_INCLUDE_DIR=/usr/local/include \\\r\n    MKLDNN_LIB_DIR=/usr/local/lib \\\r\n    python setup.py install && \\\r\n    cd /opt && rm -rf /opt/pytorch && \\\r\n    cd /usr/lib && sudo ldconfig\r\n']","['conda', 'pip']",0,0
678,pytorch,30989,closed,Does Distributed Data Parallel Support model parallel where output of the model are in different devices?,"By following the tutorial, I combine the model parallel and DDP together but the last layer(the parameter is really large) is doing some model parallel work like this code [source](https://github.com/bindog/pytorch-model-parallel/blob/8d1c0b91d4eb75c861059f4940159b06574189df/model.py#L39). However, it stuck at here



the gpu list for first DDP is [0, 1, 2, 3]
the gpu list for second DDP is [4, 5, 6, 7]
I modify the source code to make sure the chunk follow the parameter of gpu list



If it does not support output on multiple device, how can I reorganize the last layer?

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528",oncall: distributed triaged,"['IIUC, DDP requires input and output signatures of the models in different processes/GPUs/nodes are exactly the same, and models are called in the same order among different processes. if each process runs different divided models, input and output signatures are also different, then it should not work', '@dzk9528 What is the problem here?\r\n\r\nIf this is a question, please post on https://discuss.pytorch.org/ -- we reserve the issue tracker for issues.']","['python\r\nmodel.features = model.features.to(args.gpu_list[0])\r\nmodel = torch.nn.parallel.DistributedDataParallel(model)\r\n', 'python\r\nclass FullyConnected(nn.Module):\r\n    def __init__(self,\r\n                 in_dim,\r\n                 out_dim,\r\n                 gpu_list=[],\r\n                 model_parallel=False,\r\n                 class_split=None):\r\n        super(FullyConnected, self).__init__()\r\n        self.gpu_list = gpu_list\r\n        self.model_parallel = model_parallel\r\n        if model_parallel:\r\n            self.fc_chunks = nn.ModuleList()\r\n            for i in range(len(self.gpu_list)):\r\n                self.fc_chunks.append(\r\n                    nn.Linear(in_dim, class_split[i], bias=True).cuda(self.gpu_list[i])\r\n                )\r\n        else:\r\n            self.fc = nn.Linear(in_dim, out_dim, bias=True)\r\n\r\n    def forward(self, x, labels=None):\r\n        # print(x.size())\r\n        if self.model_parallel:\r\n            x_list = []\r\n            for i in range(len(self.gpu_list)):\r\n                _x = self.fc_chunks[i](x.cuda(self.gpu_list[i]))\r\n                x_list.append(_x)\r\n            return tuple(x_list)\r\n        else:\r\n            return self.fc(x)\r\n']",[],0,0
679,pytorch,24304,closed,MinMax Observer should return tensor of scale and tensor of zero_point?,"## üêõ Bug
Right now it returns a tensor of [scale, zero_point], I think maybe we should just return the values directly, or two Tensors?
https://github.com/pytorch/pytorch/blob/master/torch/quantization/observer.py#L65",quantization_release_1.3 triaged,"[""I think we should have some container data structure for the quantization parameters. I'm just thinking in the future we might want to calibrate the data type, and maybe min/max, etc."", 'that is quantizer, we can change that after quantizer is exposed in python frontend.', 'Link this issue with https://github.com/pytorch/pytorch/pull/24339.', 'fixed']",[],[],0,0
680,pytorch,15166,closed,BUG:torch.nn.functional.multi_margin_loss,"pytorch1.0
BUG:torch.nn.functional.multi_margin_loss
example:
import torch 
import torch.nn.functional as F
a=[[1.,2.,3.,4.]]
b=[2]
input=torch.tensor(a,requires_grad=True)
target = torch.tensor(b)
k = torch.tensor(c)
output = F.multi_margin_loss(input,target,p=1.,margin=1.,reduce=False,size_average=False)
print(output)
--------------------------
tensor([0.5000], grad_fn=<MultiMarginLossBackward>)
",,"['Hi,\r\n\r\nCould you please elaborate a bit more on what is the problem here?', 'problem1:doc: loss(x,y)=x.size(0)\r\nloss(x,y)=1/x.size(0)*‚àëi,I(max(0,margin‚àíx[y]+x[i])^p)\r\na=[[1.,2.,3.,4.]],a.size=(1,4) but output=0.5?It look like loss(x,y)=1/x.size(1)*‚àëi,I(max(0,margin‚àíx[y]+x[i])^p)\r\nproblem2:reduce=False,but output=0.5?or [0.,0.,0.5,0.] ?', 'Hi,\r\n\r\nThe doc is correct (though a bit misleading), it states, just before the formula: `For each mini-batch sample, the loss in terms of the 1D input x and scalar output y is`. The loss(x, y) formula is given for a single element of the batch where x is a 1D tensor containing the scores and y is the value of the true label.\r\nNotice that the formula sum over all possible labels different than the ground truth, so you get a single number for each sample.\r\n\r\nThen the reductions specify how you want to aggregate results for the elements in the mini-batch. In your case, you have a single element in your minibatch, so reduction will always return the same thing.\r\n']",[],[],0,0
681,pytorch,27450,closed,formatting issue in dynamic quantization function,"<img width=""798"" alt=""formatting-issue-either-or"" src=""https://user-images.githubusercontent.com/45861273/66273196-95bb6b80-e826-11e9-8cd8-8fcb2a2759dd.png"">


cc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100",low priority module: docs oncall: quantization quantization_release_1.3 small triaged,"['Can you elaborate on what needs to be done here?', 'See where it says ""Either: * A dictionary ... "" and then there is a big whitespace and then ""* A set of types..."" \r\n\r\nLooking at the source docstring I think the bullet point beginning ""A dictionary..."" and the bullet ""A set of ..."" are supposed to be two options that are parallel to one another. Can we get them to render the same? \r\n\r\nI am kinda guessing just adding a newline before A dictionary might do it. I\'ll try that but if that doesn\'t work then I might need more help.', '@gottbrath : Can we close this issue?', 'I am still planning on fixing this .. low priority though. ', 'someone fixed this in the meantime. ']",[],[],0,0
682,pytorch,5463,closed,ONNX export protobufs leak file paths from file system,"This is bad for build reproducibility, since file paths may change between systems. The file paths come from SourceLocation which we started exporting in c7cb6a795e550ef2373d8127e120744db16b61cf

CC @jamesr66a ",module: onnx,['guessing this was either fixed or not relevant anymore'],[],[],0,0
683,pytorch,827,closed,HalfTensor Training Needs non-Stateless Method in F.Linear,"Hi Guys,

Currently trying to call backward() on a network trained in FP16 with a linear output results in an error:







This is easily fixed by replacing these calls with the non-stateless versions, as I've done [here](https://github.com/ajbrock/pytorch/blob/master/torch/nn/_functions/linear.py). I'm not sure if this replacement is in line with pytorch style (or if there are plans to implement stateless methods for HalfTensors which would render this irrelevant), but I can submit a PR if needbe.

Best,

Andy",high priority,"['If it has an mm method then we should enable `torch.mm` for it too. It should be a one line change in cwrap.', '@apaszke we dont enable stateless methods for any half tensors at the moment.', 'Fixed in #852.']","['\r\nimport torch\r\nimport torch.nn.functional as F\r\nfrom torch.autograd import Variable as V\r\nx = V(torch.randn(20,5).cuda().half(),requires_grad=True)\r\ninp = V(torch.randn(100,5).cuda().half())\r\nout = F.linear(inp,x)\r\nout2 = out.mean()\r\nout2\r\nVariable containing:\r\n1.00000e-02 *\r\n -4.1840\r\n[torch.cuda.HalfTensor of size 1 (GPU 0)]\r\n\r\n\r\nout2.backward()\r\n', '\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/visionlab/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py"", line 145, in backward\r\n    self._execution_engine.run_backward((self,), (gradient,), retain_variables)\r\n  File ""/home/visionlab/anaconda2/lib/python2.7/site-packages/torch/nn/_functions/linear.py"", line 25, in backward\r\n    grad_weight = torch.mm(grad_output.t(), input)\r\nRuntimeError: Type HalfTensor doesn\'t implement stateless methods\r\n']",[],0,0
684,pytorch,5285,closed,[Bug] Memory leak on Convnet on CPU,"- OS: Ubuntu 16.04
- PyTorch version: 0.3.1b0+2b47480
- How you installed PyTorch (conda, pip, source): source
- Python version: 3.6.4
- CUDA/cuDNN version: CUDA 9.0/CuDnn 7
- GPU models and configuration: GTX 1080Ti (as well as GTX 1070)
- GCC version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.6) 5.4.0 20160609

Hello,
While implementing the [SRGan paper](https://arxiv.org/abs/1609.04802) I ran into a memory leak issue while doing the inference on my CPU.
This issue should be easily reproductible from your side as my implementation is [here](https://github.com/EKami/Torchlight/tree/showcase/memory-leak). All you have to do is to clone the repository with , cd into the  folder then run the script with  to run the inference on the cpu. Then you should get a memory leak with the following message:

If you run the same script but with  (defaults on cuda) then the memory leak vanishes. The exact line which cause the memory leak is [this one](https://github.com/EKami/Torchlight/blob/showcase/memory-leak/torchlight/nn/models/srgan.py#L42). Remove that line to get:

and execute the script on the cpu again with  and poof the memory leak vanishes.
I tried on 2 different computer each with the same software installed but for the hardware one has:
 - AMD FX 8350
 - GTX 1070
The second:
 - Intel i7 7700k
 - GTX 1080Ti

And I get the same memory leak on both machines.",,"['Can you run [valgrind](http://valgrind.org/) on your code and see if it reports anything suspicious?\r\n@ezyang maybe has thoughts', 'I\'m suspecting an integer overflow somewhere. @EKami, when I try to run your code, I get the following:\r\n```\r\nTraceback (most recent call last):\r\n  File ""srgan.py"", line 18, in <module>\r\n    import torchlight.data.fetcher as fetcher\r\n  File ""/private/home/rzou/Torchlight/examples/torchlight/data/fetcher.py"", line 3, in <module>\r\n    from kaggle_data.downloader import KaggleDataDownloader\r\nModuleNotFoundError: No module named \'kaggle_data\'\r\n```\r\nHow do I install kaggle_data?', 'Forgot about this one, sorry @zou3519 . [Here is the dependency](https://github.com/EKami/kaggle-data-downloader). Or just `pip install -U git+https://github.com/EKami/kaggle-data-downloader.git`', 'Hitting the following now:\r\n```\r\nTraceback (most recent call last):\r\n  File ""srgan.py"", line 22, in <module>\r\n    from torchlight.nn.train_callbacks import ModelSaverCallback, ReduceLROnPlateau, TensorboardVisualizerCallback\r\n  File ""/private/home/rzou/Torchlight/examples/torchlight/nn/train_callbacks.py"", line 9, in <module>\r\n    from tensorboardX import SummaryWriter\r\n  File ""/private/home/rzou/local/miniconda3/lib/python3.6/site-packages/tensorboardX/__init__.py"", line 4, in <module>\r\n    from .writer import FileWriter, SummaryWriter\r\n  File ""/private/home/rzou/local/miniconda3/lib/python3.6/site-packages/tensorboardX/writer.py"", line 25, in <module>\r\n    from .src import summary_pb2\r\n  File ""/private/home/rzou/local/miniconda3/lib/python3.6/site-packages/tensorboardX/src/summary_pb2.py"", line 25, in <module>\r\n    dependencies=[tensorboard_dot_src_dot_tensor__pb2.DESCRIPTOR,])\r\n  File ""/private/home/rzou/local/miniconda3/lib/python3.6/site-packages/google/protobuf/descriptor.py"", line 829, in __new__\r\n    return _message.default_pool.AddSerializedFile(serialized_pb)\r\nTypeError: Couldn\'t build proto file into descriptor pool!\r\nInvalid proto descriptor for file ""tensorboard/src/summary.proto"":\r\n  tensorboard/src/summary.proto: A file with this name is already in the pool.\r\n```\r\n\r\nIn your code,\r\n```\r\n# TODO causes a memory leak on CPU\r\n# block_x3 = self.block_x3(block_x2)\r\n```\r\nWhat is the output of `block_x2.size()`? I\'m thinking of just making a random input and sending it into a convolution and seeing if that causes an OOM.', '@zou3519 You can either remove `TensorboardVisualizerCallback` from the import and on [this line](https://github.com/EKami/Torchlight/blob/showcase/memory-leak/examples/srgan.py#L110) to fix your issue or the size of `block_x2` is `[16, 64, 224, 224]`', ""@EKami \r\nDoes the following code OOM for you?\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\nmodel = nn.Conv2d(64, 3, kernel_size=9, padding=4)\r\nx = Variable(torch.randn([16, 64, 224, 224]))\r\nmodel(x)\r\n```\r\n\r\nI removed the two pieces you suggested (while trying to run your code) and am still getting a similar error. I've never worked with tensorboard before so I'm not sure what the error is saying."", ""@zou3519 The code run from start to end without crashing but it takes like 15gb of RAM to run which isn't normal imo."", ""I agree, I've seen around the same memory usage. Am looking into it."", ""The 15GB of RAM for this input size wouldn't surprise me, because we use the (memory consuming) unfolding of the image to perform the convolution.\r\nIn your example, the unfolded image has size of roughly `16 * 64 * 9 * 9 * 224 * 224 * 4` which is roughly 15GB ([exact code here](https://github.com/pytorch/pytorch/blob/master/aten/src/THNN/generic/SpatialConvolutionMM.c#L192)).\r\n\r\nThe dimensions of the input image are too big for the kernel size."", '@fmassa Ok so I ran a little benchmark on both the GPU and the CPU of the following script:\r\n```\r\nimport signal\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\nmodel = nn.Conv2d(64, 3, kernel_size=9, padding=4).cuda() # cuda removed for CPU\r\nx = Variable(torch.randn([16, 64, 224, 224])).cuda() # cuda removed for CPU\r\nmodel(x)\r\nprint(""Done"")\r\nsignal.pause()\r\n```\r\n\r\nHere are the results:\r\nOn GPU:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1403      G   /usr/lib/xorg/Xorg                            90MiB |\r\n|    0      7061      C   python                                      1341MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nOn CPU:\r\n```\r\n$ps -eo size,pid,user,command --sort -size | awk \'{ hr=$1/1024 ; printf(""%13.2f Mb "",hr) } { for ( x=4 ; x<=NF ; x++ ) { printf(""%s "",$x) } print """" }\' |cut -d """" -f2 | cut -d ""-"" -f1 > output\r\n\r\n         0.00 Mb COMMAND\r\n     16607.09 Mb python test.py\r\n      1172.36 Mb node current/index.js\r\n       696.78 Mb /usr/lib/x86_64\r\n       641.43 Mb /usr/bin/dockerd\r\n       506.49 Mb /usr/lib/x86_64\r\n       438.69 Mb /usr/sbin/unity\r\n       427.19 Mb /usr/lib/snapd/snapd\r\n       ....\r\n```\r\nBut once the program reaches the `signal.pause()` the memory get freed up after a while.\r\nDo you really think this is still normal?\r\nEven if it\'s not a memory leak is there an alternative for me to run the code on CPU without it taking 17gb but 1.5gb instead as on the GPU? Thanks', ""The convolutions on the GPU uses cudnn, which does not use the same `unfold` technique so uses much less memory.\r\n\r\nFor the moment, I'd say that the only way of reducing memory usage would be to either go through the `NNPack` binding, which in the master branch [is enabled in the following cases](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Convolution.cpp#L133-L139), or reducing the batch size / image size that you feed to your model.\r\n\r\nI've [mentioned in the past](https://github.com/torch/nn/issues/501) about the large memory requirements of convolutions on CPU, but we didn't reach an agreement"", ""Thanks a lot for these informations @fmassa !\r\nWell it's very limited =/ . At a hight level that means I only have to use the `.cpu()` directive if I want to use nnpack? (considering my neural network is compatible). That's sad because I wanted to run inference on the cpu on aws lambda for my algorithm (with lambda being limited to 3gb of RAM usage) but now that this issue exist I'll have to look at another framework like TF :( . "", ""There is some work being done to use MKLDNN in PyTorch https://github.com/pytorch/pytorch/issues/4186 , but I'm not sure about the status of it. They seem to have a branch linked in that thread. Another possibility would be to make the `im2col` only operate in single elements of the batch (instead of the whole batch), that would reduce the memory requirements by a factor of 16 in your case, but would make things a bit slower.\r\n\r\nOne question: it seems that you want to generate an image that is of size 1800x1800 (8x upsample of 224x224), is that right?"", ""> One question: it seems that you want to generate an image that is of size 1800x1800 (8x upsample of 224x224), is that right?\r\n\r\nYes, when I run inference I only process 1 batch at a time but my resulting image can be even bigger than 1800x1800 (depending on the input image size). [Here is the code](https://github.com/EKami/Torchlite/blob/master/torchlite/eval/eval.py#L11) which does that. It seems that there isn't much hope for CPU inference for me for now considering the upscaling factor will take even more memory I believe."", '@EKami i ran your benchmark on our [repo](https://github.com/intel/pytorch) with mkldnn integrated. The memory consumption is 3.1G (bench.py, the rest items belong to other users)\r\n```    \r\n3157.19 Mb python bench.py\r\n      2532.11 Mb python\r\n      2181.91 Mb ./LSA start\r\n      1265.99 Mb /usr/libexec/mysqld\r\n      579.24 Mb vim applications/convergence.py\r\n      441.38 Mb /usr/lib/polkit\r\n      432.87 Mb /usr/libexec/xdg\r\n```\r\ni also tried to run `python srgan.py eval --on_cpu` but got the error below, because i didn\'t compile pytorch with CUDA. But why do i need cuda if running a CPU inference, am i missing anything?\r\n```\r\nTraceback (most recent call last):\r\n  File ""srgan.py"", line 161, in <module>\r\n    main()\r\n  File ""srgan.py"", line 155, in main\r\n    evaluate(args)\r\n  File ""srgan.py"", line 76, in evaluate\r\n    ModelSaverCallback.restore_model([netG], saved_model_dir.absolute())\r\n  File ""/home/mingfeim/pytorch/Torchlite/examples/torchlight/nn/train_callbacks.py"", line 269, in restore_model\r\n    model.load_state_dict(torch.load(file))\r\n  File ""/home/mingfeim/anaconda3/lib/python3.6/site-packages/torch/serialization.py"", line 267, in load\r\n    return _load(f, map_location, pickle_module)\r\n  File ""/home/mingfeim/anaconda3/lib/python3.6/site-packages/torch/serialization.py"", line 432, in _load\r\n    result = unpickler.load()\r\n  File ""/home/mingfeim/anaconda3/lib/python3.6/site-packages/torch/serialization.py"", line 401, in persistent_load\r\n    data_type(size), location)\r\n  File ""/home/mingfeim/anaconda3/lib/python3.6/site-packages/torch/serialization.py"", line 87, in default_restore_location\r\n    result = fn(storage, location)\r\n  File ""/home/mingfeim/anaconda3/lib/python3.6/site-packages/torch/serialization.py"", line 69, in _cuda_deserialize\r\n    return obj.cuda(device)\r\n  File ""/home/mingfeim/anaconda3/lib/python3.6/site-packages/torch/_utils.py"", line 68, in _cuda\r\n    with torch.cuda.device(device):\r\n  File ""/home/mingfeim/anaconda3/lib/python3.6/site-packages/torch/cuda/__init__.py"", line 225, in __enter__\r\n    self.prev_idx = torch._C._cuda_getDevice()\r\nAttributeError: module \'torch._C\' has no attribute \'_cuda_getDevice\'\r\n```', ""Hey @mingfeima that's awesome! Thanks a lot! Do you plan to merge this into the official pytorch repo?\r\nFor the cuda error with `python srgan.py eval --on_cpu` I just pushed a [fix](https://github.com/EKami/Torchlite/commit/2b5a8276857108fb534de732bb4a65b0ffc395e8) if you want to test again with this command from the `showcase/memory-leak` branch"", ""@EKami first of all, sry to say that i didn't solve your problem from the bottom.\r\ni believe you also need to update [here](https://github.com/EKami/Torchlite/blob/showcase/memory-leak/examples/srgan.py#L75) to enable CPU inference.\r\n```python\r\n    #ModelSaverCallback.restore_model([netG], saved_model_dir.absolute())\r\n      load_with_cpu = False if args.cuda else True\r\n      ModelSaverCallback.restore_models([netG], saved_model_dir.absolute(), load_with_cpu)\r\n```\r\nAnyway, i ran `python srgan.py eval --on_cpu` and still the code hang inside mkldnn because of not enough memory. The reason is that mkldnn has several paths for convolution computation depending on input_channel and output_channel. Only output_channel equals to multiple of 16 will go for `direct` (small memory footprint) and `self.block_x3 = nn.Conv2d(64, 3, kernel_size=9, padding=4)` has a output channel of 3 which will go for `im2col` (very large footprint). And your input for block_x3 is `[1, 64, 808, 1152]` will take too much memory. We need to solve this from the root, probably going to take some time."", ""@mingfeima Oh right excuse me I pushed a fix but I didn't test before fixing for CPU inference as this branch of my code is now very far behind the one on master. \r\n\r\nOh ok thanks for your solution anyway. I believe we won't see a memory footprint improvement for CPU inference anytime soon as people mainly use pytorch for rapid prototyping on cuda and CPU is a bit forgotten..."", ""@EKami it's true that it's not as good as our CUDA backend, but we are working on it, so it should get better soon!"", ""Hello,\r\nWhat is the status of this memory leak issue in CPU mode?\r\nI'm running into the same problem and have no access to GPU."", 'this original issue has been fixed, as PyTorch now ships by default with MKL-DNN.', 'Hello everyone,\r\n\r\nI am trying to deploy a 3D-UNet model (from here: https://github.com/wolny/pytorch-3dunet) on CPU instances and I am running into similar problems. My input size is (96, 96, 96) and I one of the convolutions in the decoder uses more than 18 GB of memory... \r\n\r\nIs there a way to use less memory (preferably without reducing the complexity of the network or the size of the input)? I tried using mkldnn (by using to_mkldnn) but the memory usage is still the same. Is there a way to use convolution with a smaller memory use in Pytorch?\r\n\r\nIn general, do you have any tips, tools, frameworks to deploy such models on CPU? I heard about AWS neo, Intel OpenVINO but I am wondering if it will really solve my problem.\r\n\r\nThank you for the help!', ""@thomas-beznik I'd suggest asking this question on the forums""]","['\r\nRuntimeError: $ Torch: not enough memory: you tried to allocate 116GB. Buy new RAM!\r\n', '\r\n        block_x2 = self.block_x2(block1 + block_x1)  # ElementWise sum\r\n\r\n        # TODO causes a memory leak on CPU\r\n        # block_x3 = self.block_x3(block_x2)\r\n\r\n        return (F.tanh(block_x2) + 1) / 2\r\n']","['git clone -b showcase/memory-leak git@github.com:EKami/Torchlite.git', 'examples', 'python srgan.py eval --on_cpu', 'python srgan.py eval', 'python srgan.py eval --on_cpu']",0,0
685,pytorch,2670,closed,Adding an attribute as a buffer should throw in error,"We had a bug in OpenNMT where we first created an attribute  and then tried to add  as a buffer. This behavior will cause  to exist, but not be cast with . We think this behavior is counter-intuitive, and it should throw an error. 

@jianyuzhan 

",,"['I believe this has been fixed in #2108. Can you please confirm this?', 'Cool, thanks!']",[],"['self.mask', 'mask', 'self.mask', '.cuda']",0,0
686,pytorch,17615,closed,Long tensor to Float tensor slicing assignment fails,"## üêõ Bug
Assigning a Long tensor to a Float tensor silently fails.

## To Reproduce

## Expected behavior
The tensor should be correctly copied or an error message should be raised.

## Environment

",,"['>  The tensor should be correctly copied or an error message should be raised\r\n\r\nIt is correctly copied. Every element of `x` is between `0` and `1`, which rounds down to `0`. Copying a float tensor rounds towards zero, just like NumPy and `int(number)` in Python.', 'To see this, try scaling `x` by 10.\r\n\r\n```\r\nimport torch\r\nS = 10\r\nx = torch.rand(S) * 10 # float\r\n\r\ny = torch.zeros(S) # float\r\ny[:] = x[:] # float assignment works correctly\r\nprint(y.tolist()) #e.g. [9.241122245788574, 6.167181968688965, 7.978866100311279, ...]\r\n\r\ny = torch.zeros(S, dtype=torch.long) # long\r\ny[:] = x[:]\r\nprint(y.tolist())  # e.g. [9, 6, 7, 9, 5, 7, 0, 0, 4, 6]\r\n```']","['\r\nimport torch\r\nS = 10\r\nx = torch.rand(S) # float\r\n\r\ny = torch.zeros(S) # float\r\ny[:] = x[:] # float assignment works correctly\r\nprint(y.tolist())\r\n\r\ny = torch.zeros(S, dtype=torch.long) # long\r\ny[:] = x[:] # assignment from long tensor to float tensor silently fails.\r\nprint(y.tolist())\r\n', '\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: Tesla V100-SXM2-16GB\r\nGPU 1: Tesla V100-SXM2-16GB\r\nGPU 2: Tesla V100-SXM2-16GB\r\nGPU 3: Tesla V100-SXM2-16GB\r\n\r\nNvidia driver version: 387.26\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.10\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.1\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.15.4\r\n[pip] numpydoc==0.8.0\r\n[pip] torch==1.0.1.post2\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.1                      144\r\n[conda] mkl-service               1.1.2            py37he904b0f_5\r\n[conda] mkl_fft                   1.0.6            py37hd81dba3_0\r\n[conda] mkl_random                1.0.2            py37hd81dba3_0\r\n[conda] pytorch                   1.0.1           py3.7_cuda9.0.176_cudnn7.4.2_2    pytorch\r\n']",[],0,0
687,pytorch,18862,closed,matmul uses too much memory in some batched cases,"## üêõ Bug

PyTorch . The GPU times reported are on a P100.

In this case matmul uses about 12 GB of memory when it shouldn't use more than ~3 MB. (i.e. it's using 4096x more memory than necessary)

#### A


Note that this is equivalent to the following memory efficient operation:


#### B


It's also equivalent to the following which is memory efficient and faster, but may require a copy of y and the output may be batched-column-major without some extra work:

#### C


On GPU, **A** takes ~125 ms and uses 12 GB of memory, **B** takes ~22 ms, and **C** takes ~1 ms.

Originally https://discuss.pytorch.org/t/unexpected-huge-memory-cost-of-matmul/41642/4

See also https://github.com/pytorch/pytorch/issues/13222 which may be related

----

I believe the problem is the unnecessary contiguous call here:

https://github.com/pytorch/pytorch/blob/15b318de840de61e2e789c013e34d23819715090/aten/src/ATen/native/LinearAlgebra.cpp#L460-L461

Instead of using  and  it may be possible to use . That might achieve performance of **B**.

",high priority module: memory usage module: performance small triaged,"[""I can confirm that `.contiguous()` allocates `12GB`, which is `192 * 4096 * 4096 * sizeof(float)`.\r\n\r\nI wonder though why the inner matrix dimension from `192 * 4096 * 1` is broadcasted to `4096`. The comment at the top of the function in `LinearAlgebra.cpp` says:\r\n\r\n`The non-matrix (i.e. batch) dimensions are broadcasted (and thus must be broadcastable).`\r\n\r\nSo shouldn't the matrix dimensions be left untouched?"", 'Probably I\'m using `NumPy` terminology here (there is an explicit outer loop).  In this case the ""broadcasting"" just seems to happen internally before the operation, but the comment at the top of the function refers to the result dimensions. I have to take a closer look.', 'Done in #20448.', 'The bug is still there for A.ndim is 3 and B.ndim is 3\r\nplease run the code below with version 1.8.1\r\n```\r\nx = torch.randn(1, 4096, 4096)\r\ny = torch.randn(192, 4096, 1)\r\nz1 = torch.bmm(x.expand(192, 4096, 4096), y) # it works\r\nz2 = torch.matmul(x, y) # out of memory']","['python\r\nx = torch.randn(4096, 4096)\r\ny = torch.randn(192, 4096, 1)\r\nz = torch.matmul(x, y)\r\n', '\r\nx = torch.randn(4096, 4096)\r\ny = torch.randn(192, 4096, 1)\r\nz = torch.bmm(x.unsqueeze(0).expand(192, *x.shape), y)\r\n', '\r\nx = torch.randn(4096, 4096)\r\ny = torch.randn(192, 4096, 1)\r\nz = torch.matmul(y.permute(0, 2, 1), x.t()).permute(0, 2, 1)\r\n']","['1.1.0a0+c3e3c5c', 'contiguous()', 'view()', 'reshape()']",0,0
688,pytorch,17355,closed,The Python __doc__ for each global op should be carried over from its source,"## üêõ Bug

 is missing for many global operators (, , ...).

## To Reproduce





## Expected behavior

Global ops should contain valid s.

## Environment

PyTorch version: 1.0.1.post2
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 16.04.5 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CMake version: version 3.9.4

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce GTX TITAN Black
GPU 1: GeForce GTX TITAN Black
GPU 2: GeForce GTX TITAN Black
GPU 3: GeForce GTX TITAN Black

Nvidia driver version: 390.30
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.1
",,"['This is especially relevant because sometimes the global op differs from e.g. the `torch.nn.functional` op, and in these cases `__doc__` should be filled in appropriately (see e.g. https://github.com/pytorch/pytorch/issues/16322)', ""You shouldn't use `torch.log_softmax`. It is not a public API."", '@SsnL I also mentioned `torch.logsumexp`, which is part of the public API. And, as also noted, both of these are just examples, as there are many other examples, too.\r\n\r\nYour response seems more like a passive way of saying ""we don\'t care enough about proper `__doc__`s to fix this."" If this is the case, it would be simpler to just say so.', ""@SsnL My apology! I was wrong about this. I was naively thinking that the problem held for many other ops, but I think you are correct in saying that it's specific to unsupported ops."", ""I'm sorry for the confusion. Closing."", ""Sorry for giving that impression. I think `logsumexp` does have docstring (code at https://github.com/pytorch/pytorch/blob/562fa55f3dfc2d3ce1cac19e3ec9dab202e06b3b/torch/_torch_docs.py#L2500-L2526). In fact, we really care about documentation. We have specific tests for checking the existence of docstrings (see https://github.com/pytorch/pytorch/blob/562fa55f3dfc2d3ce1cac19e3ec9dab202e06b3b/test/test_torch.py#L160). At the moment it covers `tensor.*`, `torch.nn.*`, and `torch.nn.functioanl.*`. The reason that it doesn't yet check `torch.*` is due to the namespacing problem (we dump all ops into `torch.*`). As you can see in the test, we plan to test those as well when the issue is fixed."", 'Sorry, this was really my mistake. I fooled myself into believing that yesterday I also printed `torch.logsumexp.__doc__`, but obviously I had not, and I must have been incorrectly generalizing from `torch.log_softmax`. Thanks for the response.']","['python\r\nprint(torch.log_softmax.__doc__)\r\n# None\r\n', 'python\r\nprint(torch.nn.functional.log_softmax.__doc__)\r\n# Correctly prints documentation.\r\n']","['__doc__', 'torch.log_softmax', 'torch.logsumexp', '__doc__']",0,0
689,pytorch,23925,closed,'border' and 'reflection‚Äô modes of grid_sample have incorrect gradients at border,"## üêõ Bug

When  in , and a grid point falls exactly on the high boundary of the image (), the gradient should be based on the border padding scheme, which should give either the gradient from just inside the boundary, or zero from just outside the boundary (either could be valid, since it‚Äôs a non differentiable point). Instead, the gradient is currently based on zero padding the image, which gives wacky results.

Same problem occurs with  for 2D  on CPU.
Reflection modes of both the cuda version and the 3D CPU version also have this problem, but it‚Äôs arguably worse, since the incorrect gradient is also negated. Furthermore, this is an inconsistency between the behavior of CPU and CUDA kernels.

## Example:



Notice the wacky last row and last column. This is because the gradient there is currently calculated as if the image was zero-padded.

The result should ideally look like



which finds the gradient using the in-bounds neighbor.

A less ideal, but still palatable result would be



which finds the gradient using the out-of-bounds, border-padded neighbor.

Reflection mode on cpu (for instance, try using these same commands, but with ) gives the exact same problematic result.
When using reflection mode on cuda, however, (as well as for 3D grid_sample on cpu) the problematic gradients are negated!



This is also problematic, of course, but even more so because of the mismatch between the cpu and cuda behaviors.

For  mode, I think it makes sense to set the gradient in such cases to zero, since it‚Äôs sort of at the apex of a symmetric hill. But setting it to take the gradient of one side or the other might also be acceptable for most practical purposes.

For  mode, by contrast, I think it makes more sense to always take the non-zero gradient from the inner side, since the outer side gradient will be zero and so effectively stop training (see the related discussion for  at #7002 and #7049).




PyTorch Version: tested on commit https://github.com/pytorch/pytorch/commit/0539462ca2966aa29657b58aeb17a85c21524d31",high priority triaged,"[""I have also the same confusion, in `grid_sample` source implementation:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/f064c5aa33483061a48994608d890b968ae53fb5/aten/src/THNN/generic/SpatialGridSamplerBilinear.c\r\n\r\nI don't find the implementation of 'reflection' padding mode, I even don't understand the `padding mode` how to implement, somebody could help me?"", ""@Eurus-Holmes what you're linking to is an old, outdated version of the code, and also unrelated to this issue.\r\n\r\nIf you're having trouble using `grid_sample` and would like help with that, feel free to post on the PyTorch forum at [discuss.pytorch.org](https://discuss.pytorch.org), and we'll do our best to help you out there."", ""This bug has a real effect on training, since any grid points that are supposed to end up at the border of the image are instead sent off with a wild gradient in some undesired direction.\r\n\r\nHere's an example result from a convnet trained to do image registration. It predicts a dense displacement field, and passes that to `grid_sample` to warp one input image to another. Here it predicts no displacement between the two input images, but take a look at what the grid points that should be at the border are doing instead:\r\n![image](https://user-images.githubusercontent.com/9757500/63630142-b49ecf00-c5e5-11e9-96b9-008061aacc04.png)\r\nEach of the vectors in the figure represents the displacement of the corresponding grid point away from the identity grid. This convnet uses `grid_sample` with `bilinear` interpolation mode and `border` padding mode.\r\n\r\nNote that this problem is not likely to be caught by `gradcheck` because the incorrect gradient only occurs when a grid point happens to fall exactly on the border, which is not likely to happen by accident.\r\n\r\nAs you can see in this example, however, when a convnet is in fact trying to predict no displacement, the border grid points will eventually fall exactly at the border, and then will be thrown off by updating with the wrong gradient. So it just never settles.\r\n\r\nI posted my suggestions for what the gradient at the borders should look like up above, but the fix would require a bit of thought in order to implement cleanly, because in order to get the gradient on the inner side for `border` padding mode, there might have to be a special case for grid points at the border to shift over by 1 the four pixels (or eight voxels in the 3D case) that are used in the bilinear interpolation.\r\n\r\n@fmassa and @SsnL what do you think?"", '@fmassa are you able to take this on?  I think you have the most up-to-date information in your head.', ""I've pushed a fix for this issue at https://github.com/pytorch/pytorch/pull/32829""]","[""python\r\nimage = torch.arange(0, 5, dtype=torch.float).expand((1,1,5,5)).requires_grad_()\r\n\r\nid_grid = torch.nn.functional.affine_grid(\r\n    torch.tensor([[[1,0,0],[0,1,0.]]]), (1,1,5,5), align_corners=True).requires_grad_()\r\n\r\ntorch.nn.functional.grid_sample(image, id_grid, padding_mode='border',\r\n                                align_corners=True).sum().backward()\r\n\r\nprint(id_grid.grad.permute(0,3,1,2))\r\n"", 'python\r\ntensor([[[[ 2.,  2.,  2.,  2., -8.],\r\n          [ 2.,  2.,  2.,  2., -8.],\r\n          [ 2.,  2.,  2.,  2., -8.],\r\n          [ 2.,  2.,  2.,  2., -8.],\r\n          [ 2.,  2.,  2.,  2., -8.]],\r\n\r\n         [[ 0.,  0.,  0.,  0.,  0.],\r\n          [ 0.,  0.,  0.,  0.,  0.],\r\n          [ 0.,  0.,  0.,  0.,  0.],\r\n          [ 0.,  0.,  0.,  0.,  0.],\r\n          [ 0., -2., -4., -6., -8.]]]])\r\n', 'python\r\ntensor([[[[ 2.,  2.,  2.,  2.,  2.],\r\n          [ 2.,  2.,  2.,  2.,  2.],\r\n          [ 2.,  2.,  2.,  2.,  2.],\r\n          [ 2.,  2.,  2.,  2.,  2.],\r\n          [ 2.,  2.,  2.,  2.,  2.]],\r\n\r\n         [[ 0.,  0.,  0.,  0.,  0.],\r\n          [ 0.,  0.,  0.,  0.,  0.],\r\n          [ 0.,  0.,  0.,  0.,  0.],\r\n          [ 0.,  0.,  0.,  0.,  0.],\r\n          [ 0.,  0.,  0.,  0.,  0.]]]])\r\n', 'python\r\ntensor([[[[ 2.,  2.,  2.,  2.,  0.],\r\n          [ 2.,  2.,  2.,  2.,  0.],\r\n          [ 2.,  2.,  2.,  2.,  0.],\r\n          [ 2.,  2.,  2.,  2.,  0.],\r\n          [ 2.,  2.,  2.,  2.,  0.]],\r\n\r\n         [[ 0.,  0.,  0.,  0.,  0.],\r\n          [ 0.,  0.,  0.,  0.,  0.],\r\n          [ 0.,  0.,  0.,  0.,  0.],\r\n          [ 0.,  0.,  0.,  0.,  0.],\r\n          [ 0.,  0.,  0.,  0.,  0.]]]])\r\n', 'python\r\ntensor([[[[2., 2., 2., 2., 8.],\r\n          [2., 2., 2., 2., 8.],\r\n          [2., 2., 2., 2., 8.],\r\n          [2., 2., 2., 2., 8.],\r\n          [2., 2., 2., 2., 8.]],\r\n\r\n         [[0., 0., 0., 0., 0.],\r\n          [0., 0., 0., 0., 0.],\r\n          [0., 0., 0., 0., 0.],\r\n          [0., 0., 0., 0., 0.],\r\n          [-0., 2., 4., 6., 8.]]]])\r\n']","[""padding_mode='border'"", 'grid_sample', 'size - 1', ""padding_mode='reflection'"", 'grid_sample', ""padding_mode='reflection'"", 'reflection', 'border', 'clamp']",0,0
690,pytorch,4651,closed,distributed pytorch in cluster,"How can I launch a  distributed pytorch code in the cluster which has two nodes?

Suppose each node has only one gpu.

def init_processes(rank, size, fn, backend='gloo'):
    dist.init_process_group(backend,init_method='tcp://172.16.1.186:2222', rank=rank,world_size=size)
    fn(rank, size)

if __name__ == ""__main__"":
    size = 2
    processes = []
    for rank in range(size):
        p = Process(target=init_processes, args=(rank, size, run))
        p.start()
        processes.append(p)

   for p in processes:
        p.join()

This is part of my code.I run it.However,the result is only run in the one node when I qsub the run command in the two nodes.

The part of pbs command is nodes=gpu06:ppn=24+gpu07:ppn=24.

The run command is python train_dist.py.

Is there something wrong?Thanks for your help.",,[],[],[],0,0
691,pytorch,8554,closed,[pytorch][build] ‚ÄòvmsLog2‚Äô was not declared in this scope,"I tried to rebase and rebuild torch from source today, but got an error:


See [full gist](https://gist.github.com/elanmart/8fca075c596378bd785abd8079567684).

For future reference, getting rid of old  install in  and updating  via  seems to have solved the issue.

cc @soumith @cpuhrsch ",,"['Same problem here', ""Hello @elanmart and @zasdfgbnm  - If the error is mitigated by updating MKL, I consider this issue resolved. It's generally recommend to have the newest dependencies for development. We ship MKL with our binary builds and link against it statically, so this won't impact our users either."", ""@cpuhrsch I understand, just to clarify, I opened the issue following a short discussion with @soumith , quoting relevant bit by Soumith:\r\n> if you open an issue, I'll revert the PR and ask Christian to add a cmake check to see if VML is available, rather than assume MKL = VML-available\r\noffending PR is https://github.com/pytorch/pytorch/pull/8458\r\n\r\nNot sure if it's still relevant though."", ""Just talked to Soumith - I'll guard against older versions that don't have vmsLog2 available."", 'Guarding has been landed. Please retry.', 'That fix works for me.', 'fixed via #8614 ']",[],"['../aten/src/ATen/cpu/vml.h:68:76: error: ‚ÄòvmsLog2‚Äô was not declared in this scope', 'mkl', '/opt/intel', 'mkl', 'conda']",0,0
692,pytorch,3243,closed,Compile error:  ‚Äò__T0‚Äô was not declared in this scope,"When I build PyTorch from source following the [README directions][1], I get an error:



Machine details:

* nvcc release 7.5, V7.5.17
* gcc version 4.8.3 20140911 (Red Hat 4.8.3-9) (GCC)
* gcc version 4.8.3 20140911 (Red Hat 4.8.3-9) (GCC)
* Python 3.6.1 with Anaconda
* OS: Red Hat

I'm not sure if I have cuDNN. I am using Amazon's EC2 Deep Learning AMI, and they say they have cuDNN drivers: https://aws.amazon.com/amazon-ai/amis/

[1]:https://github.com/pytorch/pytorch#from-source",,"['The issue appears to be a mismatch between `gcc` and `cuda-sdk` versions.\r\nSee #3105  for a discussion of the same issue.\r\n', ""this is a NVCC 7.5 bug. I tried various versions of gcc with nvcc in a CENTOS6 env, and it wasn't fixed. The only workaround is to fix gloo or update nvcc to 8.0 or greater.\r\nI fixed gloo in the current subtree, the error should not happen right now.\r\nFixed via: https://github.com/facebookincubator/gloo/pull/107""]","[' shell\r\n[ 64%] Building CXX object gloo/CMakeFiles/gloo.dir/transport/tcp/device.cc.o\r\n[ 68%] Building CXX object gloo/CMakeFiles/gloo.dir/transport/tcp/pair.cc.o\r\n[ 72%] Linking CXX static library libgloo.a\r\n[ 72%] Built target gloo\r\n[ 76%] Building NVCC (Device) object gloo/CMakeFiles/gloo_cuda.dir/gloo_cuda_generated_cuda_private.cu.o\r\n[ 80%] Building NVCC (Device) object gloo/CMakeFiles/gloo_cuda.dir/gloo_cuda_generated_cuda.cu.o\r\n/home/ec2-user/pytorch/torch/lib/gloo/gloo/common/linux.h:26:26: error: ‚Äò__T0‚Äô was not declared in this scope\r\n const auto kPCIClass3D = PCIClassMatch{0x030200, 0xffff00};\r\n                          ^\r\n/home/ec2-user/pytorch/torch/lib/gloo/gloo/common/linux.h:29:31: error: ‚Äò__T0‚Äô was not declared in this scope\r\n const auto kPCIClassNetwork = PCIClassMatch{0x020000, 0xff0000};\r\n                               ^\r\nCMake Error at gloo_cuda_generated_cuda_private.cu.o.cmake:263 (message):\r\n  Error generating file\r\n  /home/ec2-user/pytorch/torch/lib/build/gloo/gloo/CMakeFiles/gloo_cuda.dir//./gloo_cuda_generated_cuda_private.cu.o\r\n\r\n\r\nmake[2]: *** [gloo/CMakeFiles/gloo_cuda.dir/gloo_cuda_generated_cuda_private.cu.o] Error 1\r\nmake[2]: *** Waiting for unfinished jobs....\r\n/home/ec2-user/pytorch/torch/lib/gloo/gloo/common/linux.h:26:26: error: ‚Äò__T0‚Äô was not declared in this scope\r\n const auto kPCIClass3D = PCIClassMatch{0x030200, 0xffff00};\r\n                          ^\r\n/home/ec2-user/pytorch/torch/lib/gloo/gloo/common/linux.h:29:31: error: ‚Äò__T0‚Äô was not declared in this scope\r\n const auto kPCIClassNetwork = PCIClassMatch{0x020000, 0xff0000};\r\n                               ^\r\nCMake Error at gloo_cuda_generated_cuda.cu.o.cmake:263 (message):\r\n  Error generating file\r\n  /home/ec2-user/pytorch/torch/lib/build/gloo/gloo/CMakeFiles/gloo_cuda.dir//./gloo_cuda_generated_cuda.cu.o\r\n\r\n\r\nmake[2]: *** [gloo/CMakeFiles/gloo_cuda.dir/gloo_cuda_generated_cuda.cu.o] Error 1\r\nmake[1]: *** [gloo/CMakeFiles/gloo_cuda.dir/all] Error 2\r\nmake: *** [all] Error 2 \xa0\r\n']",[],0,0
693,pytorch,219,closed,Conv2dTranspose needs cudnn integration,"Right now, only Conv2d is wrapped to be so.",high priority,[],[],[],0,0
694,pytorch,3238,closed,Dimension issue with `softmax` and related functions for 1D tensors,"There seems to be an erroneous dimension calculation for any function that uses the  private function. If the input is a 1D tensor, the implicit dimension computed is 1, which is a problem since  is invalid for a 1D tensor.

Minimal reproducible example:

This produces the error:

I'm running pytorch installed from source, from  (off of commit dc6510f7)",,"['Ah of course. Will send a fix today. Sorry for the trouble', 'I got a similar error when following tutorial \r\nhttp://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html\r\nThere was a warning:\r\n/usr/lib/python3.5/site-packages/ipykernel_launcher.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\r\n  This is separate from the ipykernel package so we can avoid doing imports until\r\nBTW: I am running pytorch in jupyter notebook.', 'Solved stating \r\n** usual imports **\r\n\r\n> torch.manual_seed(1)\r\ndim = 0\r\n\r\ndata2 = autograd.Variable(torch.randn(5))\r\nprint(data2)\r\nprint(F.softmax(data2, dim))\r\nprint(F.softmax(data2, dim).sum())\r\nprint(F.log_softmax(data2, dim))\r\n', 'fixed via https://github.com/pytorch/pytorch/commit/638b10d39bb5e1a62f4cf9dfd577654f8663810a', 'This works fine but the same issue is present in nn.Softmax(). ']","[' python\r\nimport torch\r\nfrom torch.nn.functional import softmax\r\nfrom torch.autograd import Variable\r\ns = softmax(Variable(torch.zeros(3)))\r\n', ""\r\n/<..>/.virtualenvs/pytorch/bin/ipython:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\r\n  #!/<...>/.virtualenvs/pytorch/bin/python2.7\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-6-82ccfcf6f7ce> in <module>()\r\n----> 1 softmax(Variable(torch.Tensor([1,2,3])))\r\n\r\n/<...>/.virtualenvs/pytorch/lib/python2.7/site-packages/torch/nn/functional.pyc in softmax(input, dim, _stacklevel)\r\n    658     if dim is None:\r\n    659         dim = _get_softmax_dim('softmax', input.dim(), _stacklevel)\r\n--> 660     return torch._C._nn.softmax(input, dim)\r\n    661 \r\n    662 \r\n\r\nRuntimeError: invalid argument 4: dim out of range (got 1, but input has 1 dims) at /<...>/pytorch/torch/lib/THNN/generic/SoftMax.c:19\r\n""]","['_get_softmax_dim', 'dim=1', 'master']",0,0
695,pytorch,907,closed,Fix cudnnHandle_t in Handles.cpp,"There are two problems with the way we manage cudnnHandles in [Handles.cpp](https://github.com/pytorch/pytorch/blob/master/torch/csrc/cudnn/Handles.cpp):

1. We sometimes see SIGSEGV on exit when the handles are destructed
2. They don't work on non-default streams

We tried making the handles table thread-local, but that causes slow downs and deadlocks because the handles can be destructed after every forward pass in a data parallel module.

We should figure out a better solution.",module: crash module: cuda triaged,"[""i'm removing the high-priority tag on this one."", 'Removing high-priority since it was removed previously and added back with no new justification.']",[],[],0,0
696,pytorch,7373,closed,help delete issue,"If you have a question or would like help and support, please ask at our
[forums](https://discuss.pytorch.org/).

If you are submitting a feature request, please preface the title with [feature request].
If you are submitting a bug report, please fill in the following details.

## Issue description

Provide a short description.

## Code example

Please try to provide a minimal example to repro the bug.
Error messages and stack traces are also helpful.

## System Info
Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


- PyTorch or Caffe2:
- How you installed PyTorch (conda, pip, source):
- Build command you used (if compiling from source):
- OS:
- PyTorch version:
- Python version:
- CUDA/cuDNN version:
- GPU models and configuration:
- GCC version (if compiling from source):
- CMake version:
- Versions of any other relevant libraries:
",,[],"['\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n']",[],0,0
697,pytorch,11751,closed,Segfault in autograd using hook,"## Issue description

I found several errors in autograd:
* Segfault during / when using hook (e.g. through ) on non-reachable tensor, whose grad is implicitely calculated () because it is an output of a function in the gradient graph but is independent of the backprop root tensor. (See code and traceback below.)
* No hook is called if this non-reachable tensor is an output of an index operation (e.g.  while root only depends on  and complete  has .) (See code below.) That issue is not related to the others, but I encountered it in the same run, so I want to mention it here, too.
* If such a hook is called (e.g.  from ), the  argument is  but should be a tensor with -values, because that is the actually used value for the required  and therefore should be modifiable. (See code below.)

Here is the traceback for the Segfault: (Notice the lines . But I think, the actual source of the problem is, that in [this](https://github.com/pytorch/pytorch/blob/6f6b03566ba3c4828f6ee87a772f9d161be0bae7/torch/csrc/autograd/engine.cpp#L447) line, the inputs are not but should be initialized as variables with -values, as fallbacks if no function overrides them. Or am I wrong?)

I really would like to fix this, but to be honest, I'm not sure if I have enough expertise to do it right. So I open this issue for others. I hope, it helps.

## Code example

a0grad = 0a1a0a.grada0grad = None= 0a1a0Segmentation faultgradNone0.backward().grad()


## System Info

- PyTorch or Caffe2: PyTorch
- How you installed PyTorch (conda, pip, source): I tested two versions (same errors for both): current master from source, v0.4.1 via pip
- Build command you used (if compiling from source): 
- OS: Linux Mint 18.3 Sylvia
- PyTorch version:  I tested two versions (same errors for both): current master, v0.4.1
- Python version: 3.6.6
- CUDA/cuDNN version: None
- GPU models and configuration: No CUDA
- GCC version (if compiling from source): (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
- CMake version: version 3.12.0
- Versions of any other relevant libraries:
[pip] 18.0",medium priority (this tag is deprecated),"['cc @smessmer, since the stack trace is inside intrusive ptr']",['\r\n## Traceback\r\n\r\n\r\n'],"['.backward()', '.grad()', '.retain_grad()', '= 0', 'a[0].register_hook()', 'a[1]', 'a', 'requires_grad', 'a0.register_hook()', 'a0, a1 = a.unbind()', 'grad', 'None', '0', 'a.grad', '<= #6', '0', '', 'python\r\nimport torch\r\n\r\na = torch.tensor([3., 5.], requires_grad=True)\r\n# (A)   With the following line, the callback for ', ' is not called.\r\n#       But it should be called (with ', ', implicitely calculated,\r\n#       because ', ' is independent of ', '), because it is used\r\n#       for the required ', ' and therefore should be modifyable\r\n#       via hook. (Also see (C).)\r\n# a0, a1 = a[0], a[1]\r\n# (B)   With the following line instead, the callback for ', ' is\r\n#       called, but with two errors:\r\n# (B.a) The callback is called with ', ', but it should be ', '\r\n#       (implicitely calculated, because ', ' is independent of ', ').\r\n#       (Also see (C).)\r\n# (B.b) After the callback, it results in a ', '.\r\n#       (Not, if no callback was registered.)\r\na0, a1 = a.unbind()\r\n@a0.register_hook\r\ndef hook(grad):\r\n    print(grad)\r\n    # (C)   When ', ' is ', ', returning a non-None replacement throws\r\n    #       the Runtime Error ""can\'t replace a None gradient with a non-None value"".\r\n    #       Therefore the current behaviour allows no modification at all for\r\n    #       implicitely calculated ', '-gradients.\r\n    # return torch.tensor(1.)\r\na1.backward()\r\n# Above errors occure no matter whether using ', ' or ', '.\r\n# torch.autograd.grad([a1], [a])\r\nprint(a.grad)\r\n\r\n#0  0x00007fffeda1bfa7 in std::__atomic_base<unsigned long>::operator++ (this=0x8) at /usr/include/c++/5/bits/atomic_base.h:296\r\n#1  0x00007fffeda2978b in c10::intrusive_ptr<at::TensorImpl, at::UndefinedTensorImpl>::retain_ (this=0x7ffff39356f0)\r\n    at /home/jk/workspace/projects/ml/www/pytorch/torch/lib/tmp_install/include/ATen/core/intrusive_ptr.h:163\r\n#2  0x00007fffeda28cc0 in c10::intrusive_ptr<at::TensorImpl, at::UndefinedTensorImpl>::intrusive_ptr (this=0x7ffff39356f0, rhs=...)\r\n    at /home/jk/workspace/projects/ml/www/pytorch/torch/lib/tmp_install/include/ATen/core/intrusive_ptr.h:211\r\n#3  0x00007fffedae1bd1 in c10::intrusive_ptr<at::TensorImpl, at::UndefinedTensorImpl>::operator=<at::TensorImpl, at::UndefinedTensorImpl>(c10::intrusive_ptr<at::TensorImpl, at::UndefinedTensorImpl> const&) & (this=0x7fffe00012a0, rhs=...) at /home/jk/workspace/projects/ml/www/pytorch/torch/lib/tmp_install/include/ATen/core/intrusive_ptr.h:252\r\n#4  0x00007fffedae0d1b in c10::intrusive_ptr<at::TensorImpl, at::UndefinedTensorImpl>::operator=(c10::intrusive_ptr<at::TensorImpl, at::UndefinedTensorImpl> const&) & (this=0x7fffe00012a0, rhs=...)\r\n    at /home/jk/workspace/projects/ml/www/pytorch/torch/lib/tmp_install/include/ATen/core/intrusive_ptr.h:244\r\n#5  0x00007fffedad8bbf in at::Tensor::operator=(at::Tensor const&) & (this=0x7fffe00012a0, x=...) at /home/jk/workspace/projects/ml/www/pytorch/torch/lib/tmp_install/include/ATen/core/Tensor.h:105\r\n#6  0x00007fffedc9c469 in torch::autograd::Variable::operator= (this=0x7fffe00012a0) at /home/jk/workspace/projects/ml/www/pytorch/torch/csrc/autograd/variable.h:83\r\n#7  0x00007fffedcbf973 in torch::autograd::PyFunctionPreHook::operator() (this=0x1450cb0, values=std::vector of length 2, capacity 2 = {...}) at torch/csrc/autograd/python_hook.cpp:54\r\n#8  0x00007fffe8c6ca99 in torch::autograd::call_pre_hooks (fn=..., inputs=std::vector of length 2, capacity 2 = {...})\r\n    at /home/jk/workspace/projects/ml/www/pytorch/torch/csrc/autograd/engine.cpp:280\r\n#9  0x00007fffe8c6cec6 in torch::autograd::call_function (task=...) at /home/jk/workspace/projects/ml/www/pytorch/torch/csrc/autograd/engine.cpp:350\r\n#10 0x00007fffe8c6d395 in torch::autograd::Engine::evaluate_function (this=0x7fffee58b900 <engine>, task=...) at /home/jk/workspace/projects/ml/www/pytorch/torch/csrc/autograd/engine.cpp:394\r\n#11 0x00007fffe8c6c666 in torch::autograd::Engine::thread_main (this=0x7fffee58b900 <engine>, graph_task=0x0) at /home/jk/workspace/projects/ml/www/pytorch/torch/csrc/autograd/engine.cpp:232\r\n#12 0x00007fffe8c6c4f7 in torch::autograd::Engine::thread_init (this=0x7fffee58b900 <engine>, device=-1) at /home/jk/workspace/projects/ml/www/pytorch/torch/csrc/autograd/engine.cpp:206\r\n#13 0x00007fffedca0630 in torch::autograd::python::PythonEngine::thread_init (this=0x7fffee58b900 <engine>, device=-1) at torch/csrc/autograd/python_engine.cpp:39\r\n#14 0x00007fffe8c8cc02 in std::_Mem_fn_base<void (torch::autograd::Engine::*)(int), true>::operator()<int, void>(torch::autograd::Engine*, int&&) const (this=0x145fa58, \r\n    __object=0x7fffee58b900 <engine>) at /usr/include/c++/5/functional:600\r\n#15 0x00007fffe8c8cb7f in std::_Bind_simple<std::_Mem_fn<void (torch::autograd::Engine::*)(int)> (torch::autograd::Engine*, int)>::_M_invoke<0ul, 1ul>(std::_Index_tuple<0ul, 1ul>) (this=0x145fa48)\r\n    at /usr/include/c++/5/functional:1531\r\n#16 0x00007fffe8c8ca02 in std::_Bind_simple<std::_Mem_fn<void (torch::autograd::Engine::*)(int)> (torch::autograd::Engine*, int)>::operator()() (this=0x145fa48)\r\n    at /usr/include/c++/5/functional:1520\r\n#17 0x00007fffe8c8c952 in std::thread::_Impl<std::_Bind_simple<std::_Mem_fn<void (torch::autograd::Engine::*)(int)> (torch::autograd::Engine*, int)> >::_M_run() (this=0x145fa30)\r\n    at /usr/include/c++/5/thread:115\r\n#18 0x00007fffe79a9c80 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#19 0x00007ffff7bc16ba in start_thread (arg=0x7ffff3936700) at pthread_create.c:333\r\n#20 0x00007ffff6da441d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109\r\n\r\n', '', 'NO_CUDA=1 DEBUG=1 python setup.py build develop']",0,0
698,pytorch,21761,closed,[JIT] Bad error message when instantiating a script class with no __init__,"

",oncall: jit,[],"['\r\nimport torch\r\n\r\n@torch.jit.script\r\nclass Foo(object):\r\n    pass\r\n\r\n@torch.jit.script\r\ndef foo():\r\n    f = Foo()\r\n\r\nprint(foo.graph)\r\n', '\r\nTraceback (most recent call last):\r\n  File ""compile_class_members.py"", line 7, in <module>\r\n    @torch.jit.script\r\n  File ""/Users/jamesreed/pytorch/torch/jit/__init__.py"", line 1084, in script\r\n    fn = torch._C._jit_script_compile(ast, _rcb, get_default_args(obj))\r\nRuntimeError: method INTERNAL ASSERT FAILED at ../torch/csrc/jit/script/sugared_value.h:292, please report a bug to PyTorch.  (call at ../torch/csrc/jit/script/sugared_value.h:292)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 64 (0x1147151b0 in libc10.dylib)\r\nframe #1: torch::jit::script::MethodValue::call(torch::jit::SourceRange const&, torch::jit::Function&, c10::ArrayRef<torch::jit::NamedValue>, c10::ArrayRef<torch::jit::NamedValue>, unsigned long) + 1520 (0x117fb1090 in libtorch.dylib)\r\nframe #2: torch::jit::script::ClassValue::call(torch::jit::SourceRange const&, torch::jit::Function&, c10::ArrayRef<torch::jit::NamedValue>, c10::ArrayRef<torch::jit::NamedValue>, unsigned long) + 326 (0x118006766 in libtorch.dylib)\r\nframe #3: torch::jit::script::to_ir::emitApplyExpr(torch::jit::script::Apply&, unsigned long) + 6600 (0x117fd25e8 in libtorch.dylib)\r\nframe #4: torch::jit::script::to_ir::emitSugaredExpr(torch::jit::script::Expr const&, unsigned long, std::__1::shared_ptr<c10::Type> const&) + 493 (0x117fa99dd in libtorch.dylib)\r\nframe #5: torch::jit::script::to_ir::emitAssignment(torch::jit::script::Assign const&) + 1785 (0x117fa8c69 in libtorch.dylib)\r\nframe #6: torch::jit::script::to_ir::emitStatements(torch::jit::script::ListIterator<torch::jit::script::Stmt>, torch::jit::script::ListIterator<torch::jit::script::Stmt>) + 1550 (0x117f9b1ee in libtorch.dylib)\r\nframe #7: torch::jit::script::to_ir::emitDef(torch::jit::script::Def const&, std::__1::function<std::__1::shared_ptr<torch::jit::script::SugaredValue> (torch::jit::Value*)> const&, torch::jit::Block*) + 376 (0x117f98328 in libtorch.dylib)\r\nframe #8: torch::jit::script::to_ir::to_ir(torch::jit::script::Def const&, std::__1::shared_ptr<torch::jit::script::Resolver>, std::__1::function<std::__1::shared_ptr<torch::jit::script::SugaredValue> (torch::jit::Value*)> const&, torch::jit::Function&) + 606 (0x117f9787e in libtorch.dylib)\r\nframe #9: std::__1::__function::__func<torch::jit::script::CompilationUnit::define(std::__1::vector<torch::jit::script::Def, std::__1::allocator<torch::jit::script::Def> > const&, std::__1::vector<std::__1::shared_ptr<torch::jit::script::Resolver>, std::__1::allocator<std::__1::shared_ptr<torch::jit::script::Resolver> > > const&, std::__1::function<std::__1::shared_ptr<torch::jit::script::SugaredValue> (torch::jit::Value*)> const&)::$_1, std::__1::allocator<torch::jit::script::CompilationUnit::define(std::__1::vector<torch::jit::script::Def, std::__1::allocator<torch::jit::script::Def> > const&, std::__1::vector<std::__1::shared_ptr<torch::jit::script::Resolver>, std::__1::allocator<std::__1::shared_ptr<torch::jit::script::Resolver> > > const&, std::__1::function<std::__1::shared_ptr<torch::jit::script::SugaredValue> (torch::jit::Value*)> const&)::$_1>, void (torch::jit::Function&)>::operator()(torch::jit::Function&) + 80 (0x117f97450 in libtorch.dylib)\r\nframe #10: torch::jit::Function::ensure_defined() + 196 (0x118063f64 in libtorch.dylib)\r\nframe #11: torch::jit::script::CompilationUnit::define(std::__1::vector<torch::jit::script::Def, std::__1::allocator<torch::jit::script::Def> > const&, std::__1::vector<std::__1::shared_ptr<torch::jit::script::Resolver>, std::__1::allocator<std::__1::shared_ptr<torch::jit::script::Resolver> > > const&, std::__1::function<std::__1::shared_ptr<torch::jit::script::SugaredValue> (torch::jit::Value*)> const&) + 2456 (0x117f94b08 in libtorch.dylib)\r\nframe #12: void pybind11::cpp_function::initialize<torch::jit::script::initJitScriptBindings(_object*)::$_32, std::__1::shared_ptr<torch::jit::Function>, torch::jit::script::Def const&, std::__1::function<pybind11::function (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >)>, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, pybind11::object, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, pybind11::object> > >, pybind11::name, pybind11::scope, pybind11::sibling>(torch::jit::script::initJitScriptBindings(_object*)::$_32&&, std::__1::shared_ptr<torch::jit::Function> (*)(torch::jit::script::Def const&, std::__1::function<pybind11::function (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >)>, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, pybind11::object, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, pybind11::object> > >), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::\'lambda\'(pybind11::detail::function_call&)::__invoke(pybind11::detail::function_call&) + 920 (0x11596d7c8 in libtorch_python.dylib)\r\nframe #13: pybind11::cpp_function::dispatcher(_object*, _object*, _object*) + 2865 (0x115580ae1 in libtorch_python.dylib)\r\n<omitting python frames>\r\nframe #26: start + 1 (0x7fff62cdd3d5 in libdyld.dylib)\r\n']",[],0,0
699,pytorch,23616,closed,[jit] string.split only splits on newlines,"It should split on any whitespace, right now it just splits on 


outputs



cc @suo",jit-backlog oncall: jit triaged,"['Fix here:\r\nhttps://github.com/pytorch/pytorch/pull/23669', 'Closed with https://github.com/pytorch/pytorch/pull/23669']","['python\r\ndef fn(x):\r\n    # type: (str)\r\n    return x.split()\r\n\r\ns = ""a b\\nc\\td""\r\nprint(fn(s))\r\nprint(torch.jit.script(fn)(s))\r\n', ""\r\n['a', 'b', 'c', 'd']\r\n['a', 'b\\nc\\td']\r\n""]","[""' '""]",0,0
700,pytorch,21680,closed,Disable nondeterministic CTCLoss from cuDNN,"## üêõ Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:

1.I i updated pytorch version and ctcÔºåuse pytorch_nightly, but in my train ,nn.CTCloss() is still zero,so,i would like to ask if the version pytorch(nightly) has been solved this problem
1.
1.

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


 - PyTorch Version (e.g., 1.0):
 - OS (e.g., Linux):
 - How you installed PyTorch (, , source):
 - Build command you used (if compiling from source):
 - Python version:
 - CUDA/cuDNN version:
 - GPU models and configuration:
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->
",high priority module: cudnn module: nn small triaged,"[""What is the version on the nightly you installed? It should have been fixed in #21392 which landed seven days ago. Are you sure there isn't an unrelated bug in your training script? Can you post a script that reproduces the problem?"", ""> What is the version on the nightly you installed? It should have been fixed in #21392 which landed seven days ago. Are you sure there isn't an unrelated bug in your training script? Can you post a script that reproduces the problem?\r\n\r\n![image](https://user-images.githubusercontent.com/45999214/59355117-0232cc00-8d59-11e9-86e7-d6d846ffc479.png)\r\n\r\nI downloaded this PreviewÔºàNightlyÔºâversion  yestardayÔºåi watched #21392,but i don`t know if it unpdate in pytorch vision 1.1. but i used version PreviewÔºàNightlyÔºâ1.1, l also get nan loss,So,which version of pytorch should i download to solve this problem?\r\n\r\n"", 'Yesterdays nightlies should be sufficient; I checked Conda and our Windows builds are working. So it must be something else. Do you have a script which reproduces the nan loss?', 'cc @t-vi ', '> cc @t-vi\r\n\r\nthe num of my train date is 9000,is this:\r\n![image](https://user-images.githubusercontent.com/45999214/59358538-550f8200-8d5f-11e9-9f7b-a02e7567dbad.png)\r\nand the label is \r\n00000000.jpg 144 80 91 9 213 24 16 217 91 682 129 100 5\r\n""144 80 91 9 213 24 16 217 91 682 129 100 5""is the index of the words in the picture in the alphabet\r\nmy alphabet has 5990 num classes \r\nis this\r\n![image](https://user-images.githubusercontent.com/45999214/59360627-0663e700-8d63-11e9-8f2a-d09bdb3b6595.png)\r\nand the first line is a space\r\n\r\nmy main script is\r\n`\r\ndef main():\r\n    # Set parameters of the trainer\r\n    global args, device\r\n    args = parse_args()\r\n\r\n    if args.gpu_id < 0:\r\n        device = torch.device(""cpu"")\r\n    else:\r\n        os.environ[""CUDA_VISIBLE_DEVICES""] = str(args.gpu_id)\r\n        device = torch.device(""cuda"")\r\n        torch.backends.cudnn.benchmark = True\r\n    # Create export dir if it doesnt exist\r\n    directory = ""{}"".format(args.arch)\r\n    directory += ""_{}_lr{:.1e}_wd{:.1e}"".format(args.optimizer, args.lr, args.weight_decay)\r\n    directory += ""_bsize{}_height{}"".format(args.batch_size, args.height)\r\n    directory += ""_keep_ratio"" if args.keep_ratio else ""_width{}"".format(args.width)\r\n\r\n    args.directory = os.path.join(args.directory, directory)\r\n    print("">> Creating directory if it does not exist:\\n>> \'{}\'"".format(args.directory))\r\n    if not os.path.exists(args.directory):\r\n        os.makedirs(args.directory)\r\n\r\n    # initialize model\r\n    if args.pretrained:\r\n        print("">> Using pre-trained model \'{}\'"".format(args.arch))\r\n    else:\r\n        print("">> Using model from scratch (random weights) \'{}\'"".format(args.arch))\r\n\r\n    # load alphabet from file\r\n    if os.path.isfile(args.alphabet):\r\n        alphabet = \'\'\r\n        with open(args.alphabet, mode=\'r\', encoding=\'utf-8\') as f:\r\n            for line in f.readlines():\r\n                alphabet += line.strip()\r\n        args.alphabet = alphabet\r\n\r\n    model_params = {}\r\n    model_params[\'architecture\'] = args.arch\r\n    model_params[\'num_classes\'] = len(args.alphabet) + 1  # Number of classes (excluding blank)\r\n    # model_params[\'mean\'] = (0.5,)\r\n    # model_params[\'std\'] = (0.5,)\r\n    model_params[\'pretrained\'] = args.pretrained\r\n    model = init_network(model_params)\r\n    model = model.to(device)\r\n\r\n    # Resize the height of an image to 32, and keep the spatial ratio of the image.\r\n    image_size = args.height if args.keep_ratio else (args.height, args.width)\r\n\r\n    transform = transforms.Compose([\r\n        transforms.Resize(image_size),\r\n        transforms.ToTensor(),\r\n        transforms.Normalize(mean=model.meta[\'mean\'], std=model.meta[\'std\']),\r\n    ])\r\n\r\n    train_dataset = DigitsDataset(mode=\'train\', data_root=args.dataset_root, transform=transform)\r\n    train_collate = DigitsCollater(mode=\'train\', keep_ratio=args.keep_ratio)\r\n    train_loader = data.DataLoader(train_dataset, batch_size=args.batch_size,\r\n                                   collate_fn=train_collate, shuffle=True,\r\n                                   num_workers=args.workers, pin_memory=(not args.keep_ratio),\r\n                                   drop_last=args.keep_ratio)\r\n\r\n    dev_dataset = DigitsDataset(mode=\'dev\', data_root=args.dataset_root, transform=transform)\r\n    dev_collate = DigitsCollater(mode=\'dev\', keep_ratio=args.keep_ratio)\r\n    dev_loader = data.DataLoader(dev_dataset, batch_size=args.batch_size,\r\n                                 collate_fn=dev_collate, shuffle=False,\r\n                                 num_workers=args.workers, pin_memory=(not args.keep_ratio))\r\n\r\n    criterion = nn.CTCLoss()\r\n    # criterion = nn.CTCLoss(zero_infinity=True)\r\n    criterion = criterion.to(device)\r\n    # Define optimizer\r\n    if args.optimizer == \'sgd\':\r\n        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\r\n    elif args.optimizer == \'rmsprop\':\r\n        optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\r\n    elif args.optimizer == \'adam\':\r\n        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\r\n\r\n    converter = LabelConverter(args.alphabet, ignore_case=False)\r\n\r\n    # Define learning rate decay schedule\r\n    # TODO: maybe pass as argument in future implementation?\r\n    exp_decay = math.exp(-0.1)\r\n    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=exp_decay)\r\n    # step_decay = 1\r\n    # gamma_decay = 0.5\r\n    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_decay, gamma=gamma_decay)\r\n\r\n    is_best = False\r\n    best_accuracy = 0.0\r\n    accuracy = 0.0\r\n    start_epoch = 0\r\n\r\n    # optionally resume from a checkpoint\r\n    if args.resume:\r\n        args.resume = os.path.join(args.directory, args.resume)\r\n        if os.path.isfile(args.resume):\r\n            # load checkpoint weights and update model and optimizer\r\n            print("">> Loading checkpoint:\\n>> \'{}\'"".format(args.resume))\r\n            checkpoint = torch.load(args.resume)\r\n            start_epoch = checkpoint[\'epoch\']\r\n            print("">>>> loaded checkpoint:\\n>>>> \'{}\' (epoch {})"".format(args.resume, start_epoch))\r\n            model.load_state_dict(checkpoint[\'state_dict\'])\r\n            # test only\r\n            if args.test_only:\r\n                print(\'>>>> Test model, using model at epoch: {}\'.format(start_epoch))\r\n                start_epoch -= 1\r\n                with torch.no_grad():\r\n                    accuracy = validate(dev_loader, model, start_epoch, converter)\r\n                print(\'>>>> Accuracy: {}\'.format(accuracy))\r\n                return\r\n            best_accuracy = checkpoint[\'best_accuracy\']\r\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\r\n            # important not to forget scheduler updating\r\n            scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=exp_decay, last_epoch=start_epoch - 1)\r\n        else:\r\n            print("">> No checkpoint found at \'{}\'"".format(args.resume))\r\n\r\n    for epoch in range(start_epoch, args.max_epoch):\r\n        # Aujust learning rate for each epoch\r\n        scheduler.step()\r\n\r\n        # Train for one epoch on train set\r\n        _ = train(train_loader, model, criterion, optimizer, epoch)\r\n\r\n        # Evaluate on validation set\r\n        if (epoch + 1) % args.validate_interval == 0:\r\n            with torch.no_grad():\r\n                accuracy = validate(dev_loader, model, epoch, converter)\r\n\r\n        # # Evaluate on test datasets every test_freq epochs\r\n        # if (epoch + 1) % args.test_freq == 0:\r\n        #     with torch.no_grad():\r\n        #         test(args.test_datasets, model)\r\n\r\n        # Remember best accuracy and save checkpoint\r\n        is_best = accuracy > 0.0 and accuracy >= best_accuracy\r\n        best_accuracy = max(accuracy, best_accuracy)\r\n\r\n        if (epoch + 1) % args.save_interval == 0:\r\n            save_checkpoint({\r\n                \'arch\': args.arch,\r\n                \'epoch\': epoch + 1,\r\n                \'state_dict\': model.state_dict(),\r\n                \'best_accuracy\': best_accuracy,\r\n                \'optimizer\': optimizer.state_dict(),\r\n            }, is_best, args.directory)\r\n\r\ndef train(train_loader, model, criterion, optimizer, epoch):\r\n    batch_time = AverageMeter()\r\n    data_time = AverageMeter()\r\n    losses = AverageMeter()\r\n\r\n    # Switch to train mode\r\n    model.train()\r\n\r\n    end = time.time()\r\n    for i, sample in enumerate(train_loader):\r\n        # Measure data loading time\r\n        data_time.update(time.time() - end)\r\n\r\n        # Zero out gradients so we can accumulate new ones over batches\r\n        optimizer.zero_grad()\r\n\r\n        # step 2. Get our inputs targets ready for the network.\r\n        # targets is a list of `torch.IntTensor` with `batch_size` size.\r\n        target_lengths = sample.target_lengths.to(device)\r\n        targets = sample.targets  # Expected targets to have CPU Backend\r\n\r\n        # step 3. Run out forward pass.\r\n        images = sample.images\r\n        if isinstance(images, tuple):\r\n            targets = targets.to(device)\r\n            log_probs = []\r\n            for image in images:\r\n                image = image.unsqueeze(0).to(device)\r\n                log_prob = model(image).squeeze(1)\r\n                log_probs.append(log_prob)\r\n            input_lengths = torch.IntTensor([i.size(0) for i in log_probs]).to(device)\r\n            log_probs = pad_sequence(log_probs)\r\n        else:  # Batch\r\n            images = images.to(device)\r\n            log_probs = model(images)\r\n            input_lengths = torch.full((images.size(0),), log_probs.size(0), dtype=torch.int32, device=device)\r\n\r\n        # step 4. Compute the loss, gradients, and update the parameters\r\n        # by calling optimizer.step()\r\n        loss = criterion(log_probs, targets, input_lengths, target_lengths)\r\n        losses.update(loss.item())\r\n        loss.backward()\r\n\r\n        # Do one step for multiple batches accumulated gradients are used\r\n        optimizer.step()\r\n\r\n        # Measure elapsed time\r\n        batch_time.update(time.time() - end)\r\n        end = time.time()\r\n\r\n        if (i + 1) % args.print_freq == 0 or i == 0 or (i + 1) == len(train_loader):\r\n            print(\'>> Train: [{0}][{1}/{2}]\\t\'\r\n                  \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\r\n                  \'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\'\r\n                  \'Loss {loss.val:.4f} ({loss.avg:.4f})\'.format(\r\n                      epoch + 1, i + 1, len(train_loader), batch_time=batch_time,\r\n                      data_time=data_time, loss=losses))\r\n\r\n    return losses.avg\r\n`\r\nthank you', 'Thanks, can you also tell us what the output of this is?\r\n\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```', '> Thanks, can you also tell us what the output of this is?\r\n> \r\n> ```\r\n> wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n> # For security purposes, please check the contents of collect_env.py before running it.\r\n> python collect_env.py\r\n> ```\r\n\r\nthank you ,i run collect_env.py,resault is:\r\n![image](https://user-images.githubusercontent.com/45999214/59396858-20321800-8dbd-11e9-8a92-585da5611585.png)\r\n\r\nmy model is  mobilenetv2+avgpool+fc+softmax\r\n![image](https://user-images.githubusercontent.com/45999214/59397033-c41bc380-8dbd-11e9-9bab-d06734b80bd0.png)\r\n\r\nbut i run my script,resault is:\r\n![360032279](https://user-images.githubusercontent.com/45999214/59396950-7010df00-8dbd-11e9-9d6e-0fcb68ac7ba8.jpg)\r\n\r\nand in the result i print the gradient of the fc layer', '> Thanks, can you also tell us what the output of this is?\r\n> \r\n> ```\r\n> wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n> # For security purposes, please check the contents of collect_env.py before running it.\r\n> python collect_env.py\r\n> ```\r\n\r\noh ,i  made a mistake,i install nightly version in ubuntu,but i watch the result of collect_env.py,i have torch version and torch_nighly version,should i unsintall torch version?', 'Hi,\r\n\r\nif would be great if you could try to uninstall. You could also just print `torch.__version__` in your actual script to be sure.\r\nIf you still get NaN with recent nightlies, could you run `torch.save` with the exact inputs to the CTC Loss? Then you should be able to reproduce the NaNs without the remainder of the model.\r\n\r\nThank you!', '> Hi,\r\n> \r\n> if would be great if you could try to uninstall. You could also just print `torch.__version__` in your actual script to be sure.\r\n> If you still get NaN with recent nightlies, could you run `torch.save` with the exact inputs to the CTC Loss? Then you should be able to reproduce the NaNs without the remainder of the model.\r\n> \r\n> Thank you!\r\n\r\nohÔºåthank you,t check my version,i used torch 1.1.0 not nightly,so i uninstall torch 1.1.0,but i have another question, i download torch-nightly 1.2.0 and torchversion 0.3.0\r\n![image](https://user-images.githubusercontent.com/45999214/59417091-32ca4280-8df9-11e9-8837-a63ad649c2d9.png)\r\n\r\nbut when i import torchvision,i met this bug:\r\n![image](https://user-images.githubusercontent.com/45999214/59417434-dae00b80-8df9-11e9-8f3c-1d6319ad9213.png)\r\n\r\nsoÔºåwhich torchvision  i could download can resolve this bug?\r\nby the way, is this nn.ctcloss updated in version stable1.1\r\n![image](https://user-images.githubusercontent.com/45999214/59417800-7c675d00-8dfa-11e9-8dc5-3807f593051c.png)\r\n if i use pytorch  stable1.1,can solve this ctcloss problem?\r\nbecause i don`t know which version updates this issue.\r\n\r\nthank you', 'Yes please, uninstall all of torch and then try again with the nightly.', ""Hi, I also encountered this problem.\r\n\r\nWith the following script I can reproduce it every time (get the input data and script from [here](https://github.com/zh217/torch_ctc_test)):\r\n\r\n```python\r\nimport sys\r\n\r\nimport torch\r\nimport torch.nn\r\n\r\n\r\ndef run_test(use_cuda):\r\n    test_data = torch.load('ctc_test_data.pt')\r\n    inp = test_data['inp']\r\n    inp_len = test_data['inp_len']\r\n    tar = test_data['tar']\r\n    tar_len = test_data['tar_len']\r\n\r\n    if use_cuda:\r\n        inp = inp.cuda().detach()\r\n        inp_len = inp_len.cuda()\r\n\r\n    print('use_cuda:', use_cuda)\r\n    print('inp:', inp.shape, inp.dtype, inp.device)\r\n    print('tar:', tar.shape, tar.dtype, tar.device)\r\n    print('inp_len:', inp_len)\r\n    print('tar_len:', tar_len)\r\n    print('verify that sum(exp(inp)) == 1:', bool(torch.all((inp.exp().sum(dim=-1) - 1).abs() < 1e-5).item()))\r\n\r\n    inp.requires_grad = True\r\n\r\n    loss_fn = torch.nn.CTCLoss()\r\n\r\n    loss = loss_fn(inp, tar, inp_len, tar_len)\r\n\r\n    loss.backward()\r\n\r\n    grad_sum = inp.grad.sum()\r\n\r\n    print('grad_sum:', grad_sum)\r\n\r\n\r\nif __name__ == '__main__':\r\n    print('python version:', sys.version)\r\n    print('torch version:', torch.__version__)\r\n    print('GPU:', torch.cuda.get_device_name())\r\n    print()\r\n    run_test(False)\r\n    print()\r\n    run_test(True)\r\n```\r\n\r\nThis is the result on the latest nightly on Linux:\r\n\r\n```\r\npython version: 3.7.2 (default, Jan 10 2019, 07:33:16) \r\n[GCC 7.3.0]\r\ntorch version: 1.2.0.dev20190617\r\nGPU: GeForce RTX 2080 Ti\r\n\r\nuse_cuda: False\r\ninp: torch.Size([125, 1, 4737]) torch.float32 cpu\r\ntar: torch.Size([20]) torch.int32 cpu\r\ninp_len: tensor([125], dtype=torch.int32)\r\ntar_len: tensor([20], dtype=torch.int32)\r\nverify that sum(exp(inp)) == 1: True\r\ngrad_sum: tensor(0.0002)\r\n\r\nuse_cuda: True\r\ninp: torch.Size([125, 1, 4737]) torch.float32 cuda:0\r\ntar: torch.Size([20]) torch.int32 cpu\r\ninp_len: tensor([125], device='cuda:0', dtype=torch.int32)\r\ntar_len: tensor([20], dtype=torch.int32)\r\nverify that sum(exp(inp)) == 1: True\r\ngrad_sum: tensor(nan, device='cuda:0')\r\n```\r\n\r\nand on stable version on Windows:\r\n\r\n```\r\npython version: 3.7.2 (default, Feb 21 2019, 17:35:59) [MSC v.1915 64 bit (AMD64)]\r\ntorch version: 1.1.0\r\nGPU: GeForce GTX 1080 Ti\r\n\r\nuse_cuda: False\r\ninp: torch.Size([125, 1, 4737]) torch.float32 cpu\r\ntar: torch.Size([20]) torch.int32 cpu\r\ninp_len: tensor([125], dtype=torch.int32)\r\ntar_len: tensor([20], dtype=torch.int32)\r\nverify that sum(exp(inp)) == 1: True\r\ngrad_sum: tensor(0.0002)\r\n\r\nuse_cuda: True\r\ninp: torch.Size([125, 1, 4737]) torch.float32 cuda:0\r\ntar: torch.Size([20]) torch.int32 cpu\r\ninp_len: tensor([125], device='cuda:0', dtype=torch.int32)\r\ntar_len: tensor([20], dtype=torch.int32)\r\nverify that sum(exp(inp)) == 1: True\r\ngrad_sum: tensor(nan, device='cuda:0')\r\n```\r\n\r\nNote that `grad_sum` for the GPU case is always `nan`."", 'This is the collected environment for Linux, if it helps:\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.2.0.dev20190617\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: GeForce RTX 2080 Ti\r\nGPU 1: GeForce RTX 2080 Ti\r\nGPU 2: GeForce RTX 2080 Ti\r\nGPU 3: GeForce RTX 2080 Ti\r\nGPU 4: GeForce RTX 2080 Ti\r\nGPU 5: GeForce RTX 2080 Ti\r\nGPU 6: GeForce RTX 2080 Ti\r\nGPU 7: GeForce RTX 2080 Ti\r\n\r\nNvidia driver version: 415.27\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.4\r\n[pip3] torch==1.1.0\r\n[pip3] torch-asg==0.1.0\r\n[pip3] torch-nightly==1.2.0.dev20190617\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] Could not collect\r\n```', '> This is the collected environment for Linux, if it helps:\r\n> \r\n> ```\r\n> Collecting environment information...\r\n> PyTorch version: 1.2.0.dev20190617\r\n> Is debug build: No\r\n> CUDA used to build PyTorch: 10.0.130\r\n> \r\n> OS: Ubuntu 18.04.2 LTS\r\n> GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\n> CMake version: version 3.10.2\r\n> \r\n> Python version: 3.7\r\n> Is CUDA available: Yes\r\n> CUDA runtime version: Could not collect\r\n> GPU models and configuration: \r\n> GPU 0: GeForce RTX 2080 Ti\r\n> GPU 1: GeForce RTX 2080 Ti\r\n> GPU 2: GeForce RTX 2080 Ti\r\n> GPU 3: GeForce RTX 2080 Ti\r\n> GPU 4: GeForce RTX 2080 Ti\r\n> GPU 5: GeForce RTX 2080 Ti\r\n> GPU 6: GeForce RTX 2080 Ti\r\n> GPU 7: GeForce RTX 2080 Ti\r\n> \r\n> Nvidia driver version: 415.27\r\n> cuDNN version: Could not collect\r\n> \r\n> Versions of relevant libraries:\r\n> [pip3] numpy==1.15.4\r\n> [pip3] torch==1.1.0\r\n> [pip3] torch-asg==0.1.0\r\n> [pip3] torch-nightly==1.2.0.dev20190617\r\n> [pip3] torchvision==0.2.2.post3\r\n> [conda] Could not collect\r\n> ```\r\n\r\nyou have version torch 1.1.0 and torch-nightly,You could also just `print torch.__version__` in your actual script to be sure. maybe you should uninstall torch 1.1.0 and use torch-nightly?', 'If you glance at my script you should see that the pytorch used is indeed the nightly version.', 'Thank you for providing this, it is extremely helpful! I can reproduce the NaN with your script.\r\nSo this happens with the CuDNN backend enabled only, I have not quite figured out why.\r\nWhen adding `torch.backends.cudnn.enabled = False`  it asks you to move the target tensor to cuda, but then it seems to work. Is that the case for you, too?', 'Yes, NaN only appears when CUDNN is used.', ""Thanks for confirming! So either we don't check the inputs enough before calling CuDNN (more likely) or it is a CuDNN bug. In either case we would likely want to avoid passing these inputs to CuDNN if we can find a reasonable criterion."", ""I didn't dig too much into this, but I never encountered this problem when the number of labels is very small (say, below 64). For my problematic inputs the number of labels is huge (in the thousands). By profiling, I also noticed that the CuDNN CTC loss function seems to achieve its unbelievable performance by cleverly batching most calculations in linear space instead of log space, so they could just multiply instead of constantly doing logsumexp. Maybe that trick fails when the number of labels goes over some threshold."", 'I copied that behaviour for collecting gradients from the ""alignment matrix"". When I last benchmarked it, the native implementation wasn\'t too bad in performance, either, although it uses `atomicAdd` which will have performance degrade when many threads try to add to the same memory location. My number of classes was small, though...\r\n\r\n@ngimel Do you know if there is an implicit limit on the number of classes in CuDNN CTC?\r\n', 'It seems training is very unstable when the number of classes is big even when CuDNN is disabled.\r\n\r\nI did some more investigations. These should be reproducible by using the latest test [here](https://github.com/zh217/torch_ctc_test/).\r\n\r\nWith the latest nightly:\r\n```\r\npython version: 3.7.2 (default, Jan 10 2019, 07:33:16) \r\n[GCC 7.3.0]\r\ntorch version: 1.2.0.dev20190617\r\nGPU: GeForce RTX 2080 Ti\r\n\r\n[ctc_test_data_0.pt]\r\ncpu         loss:  6.7722406387  grad_sum: 0.0000526551  grad_abs_sum: 1.9628770351\r\nplain_cuda  loss:  6.7722406387  grad_sum: 0.0000586268  grad_abs_sum: 1.9628837109\r\ncudnn       loss:  6.7722415924  grad_sum: nan  grad_abs_sum: nan\r\n\r\n[ctc_test_data_1.pt]\r\ncpu         loss:  5.4185132980  grad_sum: -0.0000257944  grad_abs_sum: 1.9502707720\r\nplain_cuda  loss:  5.4185132980  grad_sum: -0.0000237748  grad_abs_sum: 1.9502735138\r\ncudnn       loss:  5.4185132980  grad_sum: nan  grad_abs_sum: nan\r\n```\r\n\r\nWith stable:\r\n\r\n```\r\npython version: 3.7.2 (default, Feb 21 2019, 17:35:59) [MSC v.1915 64 bit (AMD64)]\r\ntorch version: 1.1.0\r\nGPU: GeForce GTX 1080 Ti\r\n\r\n[ctc_test_data_0.pt]\r\ncpu         loss:  6.7722406387  grad_sum: 0.0000374638  grad_abs_sum: 1.9628497362\r\nplain_cuda  loss:  6.7722406387  grad_sum: 0.0000586268  grad_abs_sum: 1.9628837109\r\ncudnn       loss:  6.7722415924  grad_sum: nan  grad_abs_sum: nan\r\n\r\n[ctc_test_data_1.pt]\r\ncpu         loss:  5.4185132980  grad_sum: -0.0000444613  grad_abs_sum: 1.9502391815\r\nplain_cuda  loss:  5.4185132980  grad_sum: -0.0000237748  grad_abs_sum: 1.9502735138\r\ncudnn       loss:  5.4185132980  grad_sum: nan  grad_abs_sum: nan\r\n```\r\n\r\nLoss values match pretty well for all three backends. For grad.sum(), CuDNN complete blew it. Plain CUDA produces fairly inaccurate answers that make training unstable (I can verify that the CPU backend is stable). Surprisingly, grad.abs().sum() match pretty well between CPU and plain CUDA.\r\n\r\nI can see that grad_sum is more inaccurate because we are adding many small numbers. But the problem is that the discrepancy is big enough to harm training.', ""While it certainly is possible that there are errors in the implementation, I should note that grad_sum is probably close to numerical precision limits. You could try with double.\r\n\r\nWhile implementing CTC loss, I once had a cuda implementation completely in log-space. I didn't keep it, but it should not be too hard to implement if you wanted (either by implementing atomicLogAddExp or moving to a deterministic setup).\r\n"", 'Okay I found the cause of unstability. It has nothing to to with numerical precision.\r\n\r\nJust in case, I have updated my test to use double whenever the backend is not CuDNN.\r\n\r\nThe relevant tests now reads:\r\n\r\n```\r\n[ctc_test_data_2.pt] zero_inf=True\r\ncpu         tar_len: [0, 13]  loss:  -0.5642039131, 51.7903951958  grad_sum: 71.1016232451  grad_abs_sum: 96.2195542856\r\nplain_cuda  tar_len: [0, 13]  loss:  0.0000000000, 51.7903951958  grad_sum: 0.0000134892  grad_abs_sum: 25.1179445297\r\ncudnn       tar_len: [0, 13]  loss:  0.2770059705, 51.7903862000  grad_sum: nan  grad_abs_sum: nan\r\n[ctc_test_data_2.pt] zero_inf=False\r\ncpu         tar_len: [0, 13]  loss:  -0.5642039131, 51.7903951958  grad_sum: 71.1016232451  grad_abs_sum: 96.2195542856\r\nplain_cuda  tar_len: [0, 13]  loss:  inf, 51.7903951958  grad_sum: nan  grad_abs_sum: nan\r\ncudnn       tar_len: [0, 13]  loss:  0.2770059705, 51.7903862000  grad_sum: nan  grad_abs_sum: nan\r\n```\r\n\r\nSo, given an edge case where `len(target) == 0`, the CPU backend always gives wrong answer for the loss regardless of the parameter `zero_infinity`, whereas the GPU backend gives the correct answer.\r\n\r\nThe reason I thought the CPU was correct was that first, I forgot to add `zero_infinity=True`, and second, in this case the CPU gives wrong but ""mild"" answer so as to not totally screw up training.', ""Okay on further testing the plain CUDA implementation also has problems.\r\n\r\nNow the test is changed to only backward propagate the loss corresponding to the second sample (I have set `reduction='none'`).\r\n\r\n```\r\n[ctc_test_data_2.pt] zero_inf=True\r\ncpu         tar_len: [0, 13]  loss:  -0.5642039131, 51.7903951958  grad_sum: 0.0000134892  grad_abs_sum: 25.1179445297\r\nplain_cuda  tar_len: [0, 13]  loss:  0.0000000000, 51.7903951958  grad_sum: 0.0000134892  grad_abs_sum: 25.1179445297\r\ncudnn       tar_len: [0, 13]  loss:  0.2770059705, 51.7903862000  grad_sum: nan  grad_abs_sum: nan\r\n[ctc_test_data_2.pt] zero_inf=False\r\ncpu         tar_len: [0, 13]  loss:  -0.5642039131, 51.7903951958  grad_sum: 0.0000134892  grad_abs_sum: 25.1179445297\r\nplain_cuda  tar_len: [0, 13]  loss:  inf, 51.7903951958  grad_sum: nan  grad_abs_sum: nan\r\ncudnn       tar_len: [0, 13]  loss:  0.2770059705, 51.7903862000  grad_sum: nan  grad_abs_sum: nan\r\n```\r\n\r\nNow with `zero_infinity=False`, the CUDA backend will still claim that the grad is `nan` even though the infinity only occurs in the case where the loss is NOT propagated."", 'Target length 0 for GPU is an open issue #18215 . I you think the CPU version is bad, too, it might be good to comment there. (Note that one would expect a gradient for empty targets pushing `blank` probs everywhere, it\'s just not properly implemented in CUDA.)\r\nIf ""not propagate"" means sticking a 0 into grad_out: That does not help you from getting NaNs. This is neither CTC-specific nor considered actionable.\r\n', 'I see your point regarding pushing to blank. Will test that further.\r\n\r\nThe ""not propagate"" reasoning is that otherwise the gradient would depend on how I split up my batches. For example, if I use batch_size=1, then obviously the ""good"" sample will back propagate correctly. I understand that this might be problematic in a CTC implementation. In that case, some documentation would help.', ""`zero_infinity` should work on a per-sample basis, so you'd get regular gradients for the other samples."", 'I think I know what is happening now.\r\n\r\nI\'ve upgraded my [script](https://github.com/zh217/torch_ctc_test/) to accept arguments. It then tests the four implementations: CPU, CUDA-aten, CuDNN-non-deterministic, CuDNN-deterministic. You can also specify `--n-classes` for the number of classes in the sample, and `--beta` to scale the inputs (""beta"" as in statistical mechanics).\r\n\r\nThe conclusion is that CuDNN-non-deterministic will break when the temperature is too low, i.e. when the model is very sure about its prediction, or when max(inp) - min(inp) is big.\r\n\r\nFor example, very cold (notice also that our n-class is very small):\r\n```\r\n$ python ctc_test.py ctc_test_data_0.pt --beta 1 --n-class 10\r\npython version: 3.7.3 (default, Mar 27 2019, 22:11:17)\r\n[GCC 7.3.0]\r\ntorch version: 1.2.0a0+667b033\r\nGPU: GeForce RTX 2080 Ti\r\nloaded ctc_test_data_0.pt\r\ninp.shape: [125, 2, 10]\r\ntar.shape: [29]\r\ninp_len: [125, 125]\r\ntar_len: [9, 20]\r\nmax(inp) - min(inp): 336.0294189453125\r\n\r\ncudnn       loss:  20.9665966034, 163.1199340820  grad_sum: nan  grad_abs_sum: nan\r\ncudnn_det   loss:  20.9665966034, 163.1199340820  grad_sum: 0.0079039335  grad_abs_sum: 56.8894653320\r\nplain_cuda  loss:  20.9665987912, 163.1199889428  grad_sum: 0.0000065508  grad_abs_sum: 56.8841630398\r\ncpu         loss:  20.9665987912, 163.1199889428  grad_sum: 0.0000065508  grad_abs_sum: 56.8841630398\r\n```\r\n\r\nStill cold:\r\n```\r\n$ python ctc_test.py ctc_test_data_0.pt --beta 2 --n-class 10\r\npython version: 3.7.3 (default, Mar 27 2019, 22:11:17)\r\n[GCC 7.3.0]\r\ntorch version: 1.2.0a0+667b033\r\nGPU: GeForce RTX 2080 Ti\r\nloaded ctc_test_data_0.pt\r\ninp.shape: [125, 2, 10]\r\ntar.shape: [29]\r\ninp_len: [125, 125]\r\ntar_len: [9, 20]\r\nmax(inp) - min(inp): 168.01470947265625\r\n\r\ncudnn       loss:  4.4914546013, 60.1561355591  grad_sum: nan  grad_abs_sum: nan\r\ncudnn_det   loss:  4.4914546013, 60.1561355591  grad_sum: 0.0009765923  grad_abs_sum: 47.2719802856\r\nplain_cuda  loss:  4.4914535743, 60.1561433807  grad_sum: 0.0000034972  grad_abs_sum: 47.2713459902\r\ncpu         loss:  4.4914535743, 60.1561433807  grad_sum: 0.0000034972  grad_abs_sum: 47.2713459902\r\n```\r\n\r\nWarm enough:\r\n```\r\npython ctc_test.py ctc_test_data_0.pt --beta 10 --n-class 10\r\npython version: 3.7.3 (default, Mar 27 2019, 22:11:17)\r\n[GCC 7.3.0]\r\ntorch version: 1.2.0a0+667b033\r\nGPU: GeForce RTX 2080 Ti\r\nloaded ctc_test_data_0.pt\r\ninp.shape: [125, 2, 10]\r\ntar.shape: [29]\r\ninp_len: [125, 125]\r\ntar_len: [9, 20]\r\nmax(inp) - min(inp): 33.602943420410156\r\n\r\ncudnn       loss:  86.9238586426, 60.1272621155  grad_sum: -0.0033766031  grad_abs_sum: 203.1000061035\r\ncudnn_det   loss:  86.9238586426, 60.1272621155  grad_sum: -0.0033622980  grad_abs_sum: 203.0999908447\r\nplain_cuda  loss:  86.9238441998, 60.1272580160  grad_sum: 0.0000000433  grad_abs_sum: 203.0973512830\r\ncpu         loss:  86.9238441998, 60.1272580160  grad_sum: 0.0000000433  grad_abs_sum: 203.0973512830\r\n```\r\n\r\nI suppose this is the linear-space trick that CuDNN uses not holding up under freezing climate. It occurs more when n-class is big, since it is really max-min that matters, but as the above has shown it will come up in other conditions if you try hard enough.\r\n\r\nThe question now is what can we do. Fundamentally, nothing, except giving users more choicec and sufficiently warn users when they choose badly. May I suggest making CUDNN_CTC_LOSS_ALGO_DETERMINISTIC an argument to the python function, since by choosing ther deterministic function, the user is really opting for more stable training and they may not care about determinism at all? And really, I think the deterministic variant should be the default, since the bad implementation will only fail when your model is getting really good at predicting the classes, and telling the user that training won\'t continue after 100+ hours of GPU time is much worse than telling them at the very beginning.', 'I think we should disable the non-deterministic algorithm.\r\n\r\nI did some profiling. Here is the result on RTX 2080 Ti, with\r\n```\r\ninp.shape: [125, 2, n_classes]\r\ntar.shape: [29]\r\ninp_len: [125, 125]\r\ntar_len: [9, 20]\r\n```\r\n\r\nAll quoted numbers are iterations/second (higher is better):\r\n\r\n```\r\nn_classes = 32:\r\ncudnn     : 747.70\r\ncudnn_det : 742.87\r\nplain_cuda: 186.41\r\ncpu       :  73.40\r\n\r\nn_classes = 128:\r\ncudnn     : 719.50\r\ncudnn_det : 722.05\r\nplain_cuda: 169.64\r\ncpu       :  56.90\r\n\r\nn_classes = 1024:\r\ncudnn     : 728.68\r\ncudnn_det : 699.70\r\nplain_cuda: 187.40\r\ncpu       :  16.51\r\n\r\nn_classes = 4737:\r\ncudnn     : 689.34\r\ncudnn_det : 684.26\r\nplain_cuda: 174.17\r\ncpu       :  13.19\r\n```\r\n\r\nSo by using the deterministic algorithm, in exchange for insignificant performance hit, we get correct result.', 'I think using the deterministic algorithm unconditionally is a good idea.\r\n', ' With ` cudnn.enabled = False`, I got negtive loss...\r\n\r\n```\r\n[0/200][20/32552] Loss: -1.304587\r\n[0/200][40/32552] Loss: -0.951960\r\n[0/200][60/32552] Loss: -0.860244\r\n[0/200][80/32552] Loss: -0.771949\r\n[0/200][100/32552] Loss: -0.672312\r\n[0/200][120/32552] Loss: -0.608262\r\n[0/200][140/32552] Loss: -0.574715\r\n[0/200][160/32552] Loss: -0.561572\r\n[0/200][180/32552] Loss: -0.531496\r\n[0/200][200/32552] Loss: -0.507013\r\n[0/200][220/32552] Loss: -0.469286\r\n[0/200][240/32552] Loss: -0.477300\r\n[0/200][260/32552] Loss: -0.437300\r\n[0/200][280/32552] Loss: -0.397784\r\n[0/200][300/32552] Loss: -0.379785\r\n[0/200][320/32552] Loss: -0.353957\r\n[0/200][340/32552] Loss: -0.277710\r\n[0/200][360/32552] Loss: -0.235506\r\n[0/200][380/32552] Loss: -0.210241\r\n[0/200][400/32552] Loss: -0.163766\r\n[0/200][420/32552] Loss: -0.105238\r\n```\r\n\r\nAnd trainning is far more slower.\r\n', ""closing this as it looks fixed.  @nairbv please reopen if I'm wrong.""]","['\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n']","['conda', 'pip']",0,0
701,pytorch,3093,closed,better error messages on compiling with cudnn v5 (which is not supported anymore),"Hi All --

I'm getting the following error when I try to build:



Seems like the  flags were added within the past couple of weeks, and I can't find anything Googling the errors.

I have CUDNNV5 and CUDA 8.0 on an Ubuntu16.04 box.  Can provide more details as needed.

Thanks
",,"['we are ending support for cudnn v5, i will push some patches to have better error messages. NVIDIA already released CuDNN v7, so we are going for supporting cudnn v6 and v7', 'Roger that.  Should change README as well. \r\n\r\nThanks']","['\r\n   if (perfResults.status == CUDNN_STATUS_SUCCESS && !(deterministic && perfResults.determinism != CUDNN_DETERMINISTIC)) {\r\n                                                                                                ^\r\ntorch/csrc/cudnn/Conv.cpp: In instantiation of ‚Äòvoid torch::cudnn::{anonymous}::findAlgorithm(THCState*, cudnnHandle_t, const torch::cudnn::Convolution&, bool, bool, void*, void*, void*, algo_t*) [with algo_t = cudnnConvolutionBwdDataAlgo_t; THCState = THCState; cudnnHandle_t = cudnnContext*]‚Äô:\r\ntorch/csrc/cudnn/Conv.cpp:465:83:   required from ‚Äòtorch::cudnn::{anonymous}::Workspace torch::cudnn::{anonymous}::chooseAlgorithm(THCState*, cudnnHandle_t, const torch::cudnn::Convolution&, bool, bool, void*, void*, void*, algo_t*) [with algo_t = cudnnConvolutionBwdDataAlgo_t; THCState = THCState; cudnnHandle_t = cudnnContext*]‚Äô\r\ntorch/csrc/cudnn/Conv.cpp:726:18:   required from here\r\ntorch/csrc/cudnn/Conv.cpp:448:96: error: ‚Äòstruct cudnnConvolutionBwdDataAlgoPerf_t‚Äô has no member named ‚Äòdeterminism‚Äô\r\ntorch/csrc/cudnn/Conv.cpp: In instantiation of ‚Äòvoid torch::cudnn::{anonymous}::findAlgorithm(THCState*, cudnnHandle_t, const torch::cudnn::Convolution&, bool, bool, void*, void*, void*, algo_t*) [with algo_t = cudnnConvolutionBwdFilterAlgo_t; THCState = THCState; cudnnHandle_t = cudnnContext*]‚Äô:\r\ntorch/csrc/cudnn/Conv.cpp:465:83:   required from ‚Äòtorch::cudnn::{anonymous}::Workspace torch::cudnn::{anonymous}::chooseAlgorithm(THCState*, cudnnHandle_t, const torch::cudnn::Convolution&, bool, bool, void*, void*, void*, algo_t*) [with algo_t = cudnnConvolutionBwdFilterAlgo_t; THCState = THCState; cudnnHandle_t = cudnnContext*]‚Äô\r\ntorch/csrc/cudnn/Conv.cpp:766:20:   required from here\r\ntorch/csrc/cudnn/Conv.cpp:448:96: error: ‚Äòstruct cudnnConvolutionBwdFilterAlgoPerf_t‚Äô has no member named ‚Äòdeterminism‚Äô\r\n']",['deterministic'],0,0
702,pytorch,6285,closed,AutoGPU header relies on WITH_CUDA,"AutoGPU is compiled with WITH_CUDA. Instead, it should be in an auto_gpu.cpp file so we do not have to set this macro while compiling it...

If a third party lib includes auto_gpu.h without doing a , then they try to use it, the things in auto_gpu.h wrapped with  will not compile and it won't work. The solution I can see is to split it out into a .cpp so CUDA is baked into the pytorch .so.",module: internals,[],[],"['#define WITH_CUDA', 'WITH_CUDA']",0,0
703,pytorch,4281,closed,Implementation of Bipolar Activation Functions,"We're looking to make a pull request for [bipolar activation functions](https://arxiv.org/abs/1709.04054). Hope to get some feedback.

This is a trick to make activation functions self centering. In short: Because the ReLU keeps only positive numbers, it shifts the post-activation mean in a positive direction. However, if we for every other neuron we instead keep the negative numbers, we cancel this effect. 

So for half the neurons we do , and other half we do . For an i.i.d. input vector, the effect of this is to halve the mean of the vector, post-activation, .

In our paper we find empirically that this can help learning in RNNs and ConvNets. Our empirical results are with ReLUs and ELUs, but I would expect it to hold for anything ReLU like.

One way to implement it would be to put something like the following in nn/functional.py:



This also includes bipolar max-pooling functions (i.e. min-pooling for half the inputs).


1) Would there be interest in a pull request for this?
2) Any comments on the implementation? Of course, we'll have to add doc strings, and also put something similar into nn/modules/activation.py",,"['How about implement it as a decorator? In this way, users can apply it to newer activation functions and you only need to write one doc string.', ""I'm not sure if I understand what you mean. I suppose you could add @_make_bipolar as a decorator to new definitions, e.g:\r\n\r\n```python\r\n@_make_bipolar\r\ndef my_activation_fn(x):\r\n\treturn threshold(x, -0.5, -0.5)\r\n```\r\n\r\nBut I don't see how to use it on the existing activation functions without replacing them."", ""Thanks for the proposal, but I we're trying to keep the core fairly lean and only extend it with very popular layers and functions. I'm happy to add this if it turns out there are a few papers that successfully use this to achieve good results, and it will become more standard in the community. Until then, especially considering how simple it is to implement it yourself, let's wait and see. Hope you understand!"", ""@larspars \r\nI means let user define Bipolar function by themselves. So less code will modified. I think that's the same idea as yours:)\r\n\r\n```python\r\n@_make_bipolar\r\ndef brelu():\r\n    return torch.nn.ReLU()\r\n""]","['python\r\ndef _make_bipolar(fn):\r\n    def _fn(x, *args, **kwargs):\r\n        dim = 0 if x.dim() == 1 else 1\r\n        x0, x1 = torch.chunk(x, chunks=2, dim=dim)\r\n        y0 = fn(x0, *args, **kwargs)\r\n        y1 = -fn(-x1, *args, **kwargs)\r\n        return torch.cat((y0, y1), dim=dim)\r\n\r\n    return _fn\r\n    \r\nbrelu = _make_bipolar(relu)\r\nbelu = _make_bipolar(elu)\r\nbselu = _make_bipolar(selu)\r\nleaky_brelu = _make_bipolar(leaky_relu)\r\nbprelu = _make_bipolar(prelu)\r\nbrrelu = _make_bipolar(rrelu)\r\nbsoftplus = _make_bipolar(softplus)\r\nbsigmoid = _make_bipolar(sigmoid)\r\nbipolar_max_pool1d = _make_bipolar(max_pool1d)\r\nbipolar_max_pool2d = _make_bipolar(max_pool2d)\r\nbipolar_max_pool3d = _make_bipolar(max_pool3d)\r\n']","['ReLU(x)', '-ReLU(-x)', 'E[BReLU(x)] = 0.5E[x]']",0,0
704,pytorch,6034,closed,how could I get pytorch0.4 for maskrcnn. i want to deal 0dim,"pytorch 0.3 can't deal 0 dim 
i can't find 0.4
can u help me?



PyTorch GitHub Issues Guidelines
--------------------------------

We like to limit our issues to bug reports and feature requests. If you have a question or would like help and support, please visit our forums: https://discuss.pytorch.org/

If you are submitting a feature request, please preface the title with [feature request].

When submitting a bug report, please include the following information (where relevant):
- OS:
- PyTorch version:
- How you installed PyTorch (conda, pip, source):
- Python version:
- CUDA/cuDNN version:
- GPU models and configuration:
- GCC version (if compiling from source):

In addition, including the following information will also be very helpful for us to diagnose the problem:
- A script to reproduce the bug. Please try to provide as minimal of a test case as possible.
- Error messages and/or stack traces of the bug
- Context around what you are trying to do

",,"['You can build from source: https://github.com/pytorch/pytorch#from-source', ""@zou3519 \r\nsorry \r\ni can't find 0.4\r\nit's not open?"", ""Are you saying that you can't open the link? You can just look at this repo's README. Also 0.4.0 is not out yet, so all you can have is master currently."", ""@SsnL \r\nmaster's version is 0.4Ôºü"", '@huanglin6385 yes, but you need to compile from source']",[],[],0,0
705,pytorch,20875,closed,BatchNorm2d implementation returns different results than expected,"## üêõ Bug

As per https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d I expect the result of BatchNorm2d to be (x - mean) * scale / sqrt(var + eps) + bias.  However this is not the case.

Given an already loaded BatchNorm2d module as bn0, I have the following:


## Environment
condapip`, source): pip
 - Python version: 3.5
 - CUDA/cuDNN version: 9.2 / 7.1.3
 - GPU models and configuration: 
 - Any other relevant information:
",module: nn triaged,"['you are not in eval mode', ""You are incorrect, I call model.eval() in the beginning. I omitted it because that is too obvious, furthermore I've tried leaving that out as well."", ""I can't reproduce your error."", ""```\r\nbn.state_dict()\r\nOut[7]: \r\nOrderedDict([('weight',\r\n              tensor([0.46252, 0.45193, 0.35675, 0.21486, 0.73243, 0.42276, 0.27092, 0.66338, 0.44674, 0.47556, 0.69964, 0.38459, 0.58164, 0.61116, 0.46370, 0.59699, 0.81576, 0.43308, 0.29197, 0.58237, 0.32706, 0.04386, 0.31095, 0.50007, 0.43901, 0.34422, 0.51755, 0.72823, 0.31092, 0.54409, 0.33558, 0.50518], device='cuda:0')),\r\n             ('bias',\r\n              tensor([-0.06699,  0.27560,  0.36485, -0.01329,  0.32928,  0.10495,  0.66667, -0.13479, -0.14921, -0.00186, -0.09793, -0.01716,  0.09434,  0.08775,  0.11282,  0.12120, -0.27575,  0.08945, -0.06947,  0.83348,  0.00331, -0.03168, -0.19014,  0.33602,  0.04593, -0.14792, -0.00725,  0.17237,  0.05400,  0.24828,  0.16349, -0.30167], device='cuda:0')),\r\n             ('running_mean',\r\n              tensor([ -4.86915,  -7.15391, -19.52277,  58.62345,  -9.03344,  -6.44255, -28.98025,  36.02863, -10.71378, -41.40824,  57.17245,  53.43351, -31.32692, -42.83203,  21.81406, -38.72557, -87.45533,  51.15632, -63.58952,  -4.54080,  26.08591,  27.14124, -34.84683, -45.23514,  39.87592,  70.51058,  24.64559, -87.98354,   3.79852,   3.31933,   9.08850, 100.26983], device='cuda:0')),\r\n             ('running_var',\r\n              tensor([ 297.33029, 3188.48999, 2126.01514, 1688.14282, 1149.88354,  383.79486, 1535.89624, 1282.59326,  662.05591,  921.21857, 4685.94385, 2387.14062, 1623.79443, 2749.92334,  843.47308, 2624.31030, 4415.36279, 1674.96887, 2039.52893,  169.18286,  830.84766,  398.96970,  712.84973, 1120.70117, 2256.78491, 2187.56250, 1633.21179, 7864.83447,  314.27762,  485.89304,  156.98695, 5104.83301], device='cuda:0')),\r\n             ('num_batches_tracked', tensor(409363, device='cuda:0'))])\r\n```\r\n```\r\nbn = torch.nn.BatchNorm2d(32)\r\nbn.eval()\r\nbn.load_state_dict(state_dict)\r\nbn.track_running_stats = False\r\n\r\ndef manualBn(bn_op, c):\r\n    bias = bn_op.bias[c].item()\r\n    scale = bn_op.weight[c].item()\r\n    var = bn_op.running_var[c].item()\r\n    mean = bn_op.running_mean[c].item()\r\n    return (1.0 - mean) * scale / (math.sqrt(var + bn0.eps)) + bias\r\n\r\nn = torch.Tensor(1,32,2,2)\r\nn[...] = 1.0\r\ny0 = bn(n)[0,:,0,0]\r\ny1 = [manualBn(bn, x) for x in range(32)]\r\n\r\ny0\r\nOut[8]: tensor([-0.06699,  0.27560,  0.36485, -0.01329,  0.32927,  0.10495,  0.66667, -0.13478, -0.14922, -0.00186, -0.09793, -0.01716,  0.09435,  0.08775,  0.11281,  0.12120, -0.27576,  0.08946, -0.06947,  0.83348,  0.00331, -0.03168, -0.19015,  0.33603,  0.04593, -0.14793, -0.00725,  0.17236,  0.05400,  0.24828,  0.16348, -0.30167], device='cuda:0', grad_fn=<SelectBackward>)\r\n\r\nOut[9]: \r\n[0.09044372472169493,  0.3408595902259277, 0.5236371016354319, -0.3146224223990977,0.5459921762201441, 0.265558798304041, 0.8739230452927835, -0.7836327506282728, 0.05416714399454639, 0.6626087362818996, -0.6720482709341765,\r\n -0.42989093156423536, 0.5609540678124897, 0.5985905914664627, -0.2195030143783287,\r\n 0.5841476426518701, 0.8101872265552204, -0.4412895776850876, 0.3480974401349118,\r\n 1.081566720645003, -0.28133327453904355, -0.08908245395532186, 0.2273394338980057,\r\n 1.0266780593210352, -0.3133287800653785, -0.659499264761972, -0.3100660195047065,\r\n 0.903056781221669, 0.004917212697185751, 0.19102890077142384, -0.05315213153664031,\r\n -1.003564756860683]\r\n\r\n\r\n```\r\n\r\nI've found that track_running_stats == True even after calling bn.eval(), which explains the slight difference between the original batch norm parameters that I posted, and this state dict.  However even after manually setting track_running_stats to False, I see a difference between my manual batch norm and the returned values from calling batchnorm's forward method.\r\n"", ""> I can't reproduce your error.\r\n@SsnL \r\nWhen you say that you cannot reproduce the error, do you get the same output I get from bn?  If so then does your python give you that same correct output when you manually invoke the batch norm equation?"", 'it seems that you have solved this since you closed the issue. but i got the same output as python arithmetic.', ""Yes I've solved the issues and I feel I should appologize for my brash response. The problem was that calling eval() on my model was not propagating to my batch Norm layer. I don't know the reason for that yet but explicitly calling eval on bn0 fixes this.""]","[""\r\nweights = torch.load(model_dir + '29.pt')\r\n\r\nmodel.load_state_dict(weights)\r\nmodel = model.cuda()\r\nmodel = model.eval()\r\nbase = model.base\r\n# pull out the first batch norm instance in my base feature extractor network.\r\nbn0 = list(list(base.children())[0].children())[1]\r\n\r\nbias = bn0.bias[1].item() -> 0.27559953927993774\r\nscale = bn0.weight[1].item() ->  0.4519329071044922\r\nvar = bn0.running_var[1].item() -> 2088.339599609375\r\nmean = bn0.running_mean[1].item() -> -4.2851715087890625\r\n\r\nn = torch.Tensor(1,32, 2,2)\r\nn[...] = 1.0\r\ny0 = bn0(n.cuda())\r\ny0 = y0[0,1,0,0].item() -> 0.275604248046875\r\ny1 = (1.0 - mean) * scale / math.sqrt(var + bn0.eps) + bias -> 0.32786713069418427\r\n""]","['', '\r\nCollecting environment information...\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.14.4\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.148\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1060 6GB\r\nGPU 1: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 418.67\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.3\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n\r\n - PyTorch Version (e.g., 1.0): 1.10\r\n - OS (e.g., Linux): Ubuntu 16.04\r\n - How you installed PyTorch (', ', ']",0,0
706,pytorch,957,closed,Counter-intuitive behavior of expand and arithmetic op,"I don't know if this is a bug (and it might have been reported already but I didn't find anything).
But it could at least be added to the documentation for the  operation on a tensor:

Even if you are aware that expand doesn't copy data, I believe most will find this result quite counter-intuitive. Of course, this can be fixed atm either by replacing  by  or  by , but that requires more memory. It would be awesome if the arithmetic ops like  had a special case when the tensor's  is 0 in some dimension

Edit: My torch version string is ",,"[""Yeah it's a known thing and it is expected. We could think on improving that in the future, but it's super low priority. It's been like that in Lua torch for years now. That's why we don't recommend using in-place ops, unless you're aware of how the library works, and really want to squeeze out the last bits of perf."", ""There are a lot of caveats that happen when you use zero-strided tensors and inplace operations, another example can be found in [here](https://github.com/torch/torch7/issues/289).\r\nI'd go with @apaszke recommandation of using in-place ops with care."", ""Ok I wasn't aware this was already a known thing in torch, I didn't come across it before. Thank you for the insight !""]","['python\r\nimport torch\r\nT = torch.Tensor([2]).expand(1,3)\r\nprint(T.mul_(2))  # expected a Tensor with [4 4 4] here, got [16, 16, 16]\r\n']","['expand', 'expand', 'repeat', 'mul_', 'mul', 'mul', 'stride', '0.1.10_2']",0,0
707,pytorch,10041,closed,[JIT] Support torch.distributions.utils.broadcast_all(),"## Problem

The main blocker to using torch.distributions in the JIT is the  function that is used in every distribution's  method. Currently  calls  under the hood, which results in a JIT error

An expensive workaround is to replace the non-jittable  invocation with a  which is jittable, roughly


## Proposed solution

@soumith and @neerajprad suggested implementing a C++ version of , which would have the added benefit of speeding up non-jitted code.",,"[""@zdevito @apaszke , I'd be happy to implement this if you could give me some pointers or example functions to follow, as I'm not familiar with the current C++ plumbing."", 'I believe this is related to https://github.com/pytorch/pytorch/issues/8076', ""@fritzo I'm working on implementing `torch.broadcast_all` for https://github.com/pytorch/pytorch/issues/8076 today. It should be traceable when I'm done with it :)""]","['\r\nRuntimeError: expected int at position 0, but got: Tensor\r\n', 'diff\r\n- broadcast_shape = torch.Size()\r\n- for i in tensor_idxs:\r\n-     broadcast_shape = torch._C._infer_size(values[i].shape, broadcast_shape)\r\n+ broadcast_shape = sum(values).shape    # expensive workaround\r\n']","['broadcast_all()', '__init__()', 'broadcast_all()', 'torch._C._infer_size()', 'torch._C._infer_size()', 'torch.sum(*values).size()', 'broadcast_all()']",0,0
708,pytorch,7722,closed,Error in backprop when using frozen LSTM layers (new with 0.4.0),"## Issue description

The general situation is that we have a pretrained Language Model, and during a first phase we only want to train the new embedding layer we added before fine-tuning the whole thing. This worked fine in 0.3 but now sends an error message during back propagation. Minimal reproduction is to just create a simple model with a linear layer and an LSTM, freeze this second layer (by applying require_grads=False to its parameters) and try to compute a back-propagation.

## Code example

See [here](https://github.com/sgugger/Deep-Learning/blob/master/Bug%20with%20frozen%20LSTM%20layer.ipynb)

## System Info
PyTorch version: 0.4.0                                                                                                  
Is debug build: No                                                                                                      
CUDA used to build PyTorch: 9.0                                                                                                                                                                                                                 OS: Microsoft Windows 10 Home                                                                                           
GCC version: Could not collect                                                                                         
CMake version: Could not collect                                                                                                                                                                                                                Python version: 3.6                                                                                                     
Is CUDA available: Yes                                                                                                  
CUDA runtime version: 9.1.85                                                                                            
GPU models and configuration: Could not collect                                                                         
Nvidia driver version: Could not collect                                                                                
cuDNN version: Could not collect                                                                                                                                                                                                                Versions of relevant libraries:                                                                                         
[pip] Could not collect                                                                                                 
[conda] Could not collect

Pytorch was installed with conda, the bug also appears on my linux instances.

Thanks for your help!",medium priority (this tag is deprecated),"['to help give local context, the notebook error is:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-10-52a0569421b1> in <module>()\r\n----> 1 loss.backward()\r\n\r\n~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\tensor.py in backward(self, gradient, retain_graph, create_graph)\r\n     91                 products. Defaults to ``False``.\r\n     92         """"""\r\n---> 93         torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n     94 \r\n     95     def register_hook(self, hook):\r\n\r\n~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\autograd\\__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\r\n     87     Variable._execution_engine.run_backward(\r\n     88         tensors, grad_tensors, retain_graph, create_graph,\r\n---> 89         allow_unreachable=True)  # allow_unreachable flag\r\n     90 \r\n     91 \r\n\r\nRuntimeError: inconsistent range for TensorList output\r\n```', ""The error seems to be thrown at `tools\\autograd\\templates\\Functions.cpp` line 47. Not sure what's happening there."", 'I can repro this on master on a linux box as well.\r\n\r\nRepro script: (I took the ipython notebook in the code example and copy pasted the lines from there):\r\n```\r\nimport torch\r\nfrom torch.autograd import Variable as V\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nmodel = nn.Sequential(nn.Linear(10,20), nn.ReLU(inplace=True),nn.LSTM(20,5, 1)).cuda()\r\nfor param in list(model.parameters())[2:]:\r\n    param.requires_grad=False\r\nx = torch.randn(2,4,10).cuda()\r\nx.requires_grad = True\r\nz = model(x)\r\ny = torch.Tensor([0,1,2,3, 0,1,2,3]).long().cuda()\r\nloss = F.cross_entropy(z[0].view(-1,5),y)\r\nloss.backward()\r\n```', ""I hit this to.\r\n@ailzhang If you are busy, I can have a look, too.\r\nMy repro is\r\n```\r\nimport torch\r\nprint (torch.__version__)\r\ndev = torch.device('cuda')\r\nl = torch.nn.LSTM(2, 3).to(dev)\r\nfor p in l.parameters():\r\n    p.requires_grad = False\r\ns = torch.randn(1, 1, 2, requires_grad=True, device=dev)\r\nout, _ = l(s)\r\nout.sum().backward()\r\n```\r\nAs a workaround, you can disable the cudnn backend.\r\n"", 'So you need to return a list of undefined tensors rather than an empty tensor list. \r\nSo one could add an else block with `dw.resize(weight.size())` to the conditional `dw` calculation in [_cudnn_rnn_backward](https://github.com/pytorch/pytorch/blob/bb15a0830de9577b4f6bdcded5eac864f78701c2/aten/src/ATen/native/cudnn/RNN.cpp#L995).\r\n', '@t-vi Cool, feel free to propose a PR for it. Thanks! ', 'Turns out the better fix checks whether we want grad in more detail.', 'I am still facing this issue, plus there is some sort of memory leak happening as well. After receiving the error, considerable amount of RAM is occupied.', 'Did you recompile with current master?\r\nI just reran my script and it seemed OK.', ""I didn't recompile, I installed the master on a fresh machine.""]",[],[],0,0
709,pytorch,12540,closed,[RFC] Removing Nervana GPU,"NervanaGPU is currently included as a [third-party module](https://github.com/pytorch/pytorch/tree/c2a57d082d36f30f32d5c4cc7147ab57ce2c3097/third_party). However the repository is not under active development anymore and has not been modified since 3 years ago.

Checking the commit that added it, it was [this one](https://github.com/pytorch/pytorch/commit/9201cdd029aa820ebc92c71bd4f6d4105163ca80) for the optimized GEMM kernel that was developed at Nervana.

There are 2 kernels of interest in Nervana's work:

### Nervana GEMM

While Nervana GEMM is extremely tuned for Maxwell Architecture, but with Volta and Turing, GPUs are now offering Tensor Cores that are much faster. I expect that Nvidia tuned their GEMM implementation since then.

Alternatives:
- CuBLAS
- [Nervana Maxas](https://github.com/NervanaSystems/maxas) which isolated the GEMM kernel and is a much smaller library than the full NervanaGPU, so it's probably possible to clone and maintain it.
- Honorary mention: [Nvidia Cutlass](https://github.com/NVIDIA/cutlass), which achieves 90% speed of CuBLAS and supports Tensor cores without assembly.
- Nvidia's TensorRT for inference.

### Nervana convolution kernel

(If used, but i don't think it is)

The main appeal is the winograd kernel [discussed extensively here](https://github.com/soumith/convnet-benchmarks/issues/93) but it's [not even in NervanaGPU repo but in Neon repo.](https://github.com/NervanaSystems/nervanagpu/issues/25#issuecomment-203168418).
Furthermore it has been integrated into CuDNN with torch NCHW layout instead of Neon's CHWN layout and convolution was probably one of Nvidia focus in the past 2 years.

### Conclusion

Should the benchmarks show that alternatives to Nervana achieves the same speed on current GPU for a wide range of input size (as tensor cores are currently very restrictive), I think NervanaGPU code should be dropped to save on code, maintenance, dependencies and build options.


",,['this should definitely be deleted. Would you mind sending a PR for it? If not we can route it to one of the core devs'],[],[],0,0
710,pytorch,30446,closed,Is there any plan for Adafactor optimizer?,"## üöÄ Feature
<!-- A clear and concise description of the feature proposal -->
official Adafactor optimizer
## Motivation
an efficient optimizer Adafactor is currently widely used in some big models, it saves a lot of memory due to its sub-linear running average of the gradient, sometimes result in a sigificant memory footprint reduce and larger batch size.
<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->

## Pitch

<!-- A clear and concise description of what you want to happen. -->

## Alternatives

<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->

## Additional context

<!-- Add any other context or screenshots about the feature request here. -->


cc @vincentqb",feature module: optimizer triaged,['https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py'],[],[],0,0
711,pytorch,4333,closed,Model behaves differently after saving and loading,"Hi,

Recently I am working on a summarization project. During training, I saved the best model on the development set. However, loading the best model and testing again on the dev set gives me different ROUGE result (0.18218091939853281 -> 0.18217045231619222 ). Although the difference is small, it raises much concerns. And my colleagues told me that they have also encountered this issue (they observed about 2 points drop on their QA task). So I wrote a small [script](https://gist.github.com/magic282/94bdbb3b9ddef891d7aa40f0b0069a0f), and found that the parameters are identical, but the result is different after saving and loading.

And I also found this thread https://discuss.pytorch.org/t/saving-and-loading-a-model-in-pytorch/2610/21

The code on github gist runs on Windows (peterjc123‚Äôs 0.3.0 build). On linux (also 0.3.0 , I built a docker image myself by installing pytorch through conda) it raises an error that I don‚Äôt understand why:



The thread I posted https://discuss.pytorch.org/t/parameters-are-different-after-loading-model/11457",,"[""IIRC `assert` in CUDA kernels is not supported on Windows, so that's why you're not getting this error unless you use Linux. It means you have an error in your program (most likely an out-of-bound index somewhere). You can try running with `CUDA_LAUNCH_BLOCKING=1` to better pin point the point of error.\r\n\r\nAre you sure you're evaluating both of these models in train/eval mode?"", '@apaszke Yes I did model.eval(), some_eval_func(), model.train() during training. And during testing I do model.eval() after building the model.\r\nThe code looks like:\r\n```python\r\nmodel.eval()\r\nlogger.warning(""Set model to {0} mode"".format(\'train\' if model.training else \'eval\'))\r\nvalid_bleu = evalModel(model, translator, validData)\r\nmodel.train()\r\nlogger.warning(""Set model to {0} mode"".format(\'train\' if model.training else \'eval\'))\r\nlogger.info(\'Validation Score: %g\' % (valid_bleu * 100))\r\nif valid_bleu >= optim.best_metric:\r\n    saveModel(valid_bleu)\r\n```\r\n\r\nDuring testing the code looks like:\r\n```python\r\nif model is None:\r\n    checkpoint = torch.load(opt.model)\r\n\r\n    model_opt = checkpoint[\'opt\']\r\n    self.src_dict = checkpoint[\'dicts\'][\'src\']\r\n    self.tgt_dict = checkpoint[\'dicts\'][\'tgt\']\r\n\r\n    self.enc_rnn_size = model_opt.enc_rnn_size\r\n    self.dec_rnn_size = model_opt.dec_rnn_size\r\n    model = some_model()\r\n\r\n    model.load_state_dict(checkpoint[\'model\'])\r\n    generator.load_state_dict(checkpoint[\'generator\'])\r\n\r\n    if opt.cuda:\r\n        model.cuda()\r\n        generator.cuda()\r\n    else:\r\n        model.cpu()\r\n        generator.cpu()\r\n\r\n    model.generator = generator\r\nelse:\r\n    self.src_dict = dataset[\'dicts\'][\'src\']\r\n    self.tgt_dict = dataset[\'dicts\'][\'tgt\']\r\n\r\n    self.enc_rnn_size = opt.enc_rnn_size\r\n    self.dec_rnn_size = opt.dec_rnn_size\r\n    self.opt.cuda = True if len(opt.gpus) >= 1 else False\r\n    self.opt.n_best = 1\r\n    self.opt.replace_unk = False\r\n\r\nself.model = model\r\nself.model.eval()\r\n```\r\n\r\nAbout the error, I just noticed that python random.randint returns ints in [a,b] but not [a,b). After fixing it, the losses now are the same, on both Windows and Linux.\r\n\r\nSo do you have any other hints about the performance difference? Thanks.', 'here is a modified mnist example to reproduce the inconsistency:\r\nto reproduce run:\r\n```\r\npython run.py --exp_name=testv1 --epochs=2 --no-cuda\r\npython run.py --exp_name=testv2 --epochs=1 --no-cuda\r\npython run.py --exp_name=testv2 --epochs=1 --no-cuda --resume-type=last\r\n```\r\nwhy --no-cuda: because running on GPU give a different result each time for the mnist example and i don\'t know why!.\r\n\r\n<details>\r\n\r\n```python\r\nfrom __future__ import print_function\r\nimport argparse\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.optim as optim\r\nfrom torchvision import datasets, transforms\r\nfrom torch.autograd import Variable\r\nimport os\r\nimport shutil\r\n\r\n# Training settings\r\nparser = argparse.ArgumentParser(description=\'PyTorch MNIST Example\')\r\nparser.add_argument(\'--batch-size\', type=int, default=64, metavar=\'N\',\r\n                    help=\'input batch size for training (default: 64)\')\r\nparser.add_argument(\'--test-batch-size\', type=int, default=1000, metavar=\'N\',\r\n                    help=\'input batch size for testing (default: 1000)\')\r\nparser.add_argument(\'--epochs\', type=int, default=10, metavar=\'N\',\r\n                    help=\'number of epochs to train (default: 10)\')\r\nparser.add_argument(\'--lr\', type=float, default=0.01, metavar=\'LR\',\r\n                    help=\'learning rate (default: 0.01)\')\r\nparser.add_argument(\'--momentum\', type=float, default=0.5, metavar=\'M\',\r\n                    help=\'SGD momentum (default: 0.5)\')\r\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\r\n                    help=\'disables CUDA training\')\r\nparser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\r\n                    help=\'random seed (default: 1)\')\r\nparser.add_argument(\'--log-interval\', type=int, default=10, metavar=\'N\',\r\n                    help=\'how many batches to wait before logging training status\')\r\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\r\n                    help=\'manual epoch number (useful on restarts)\')\r\nparser.add_argument(\'--resume-type\', default=\'\', type=str, metavar=\'PATH\',\r\n                    help=\'load from best/last checkpoint\')\r\nparser.add_argument(\'--exp_name\', type=str, default=\'dummy\',\r\n                    help=\'experiment name to used across everything\', metavar=\'exp_name\', required=True)\r\n\r\nargs = parser.parse_args()\r\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\r\n\r\ntorch.manual_seed(args.seed)\r\nif args.cuda:\r\n    torch.cuda.manual_seed(args.seed)\r\n\r\n\r\nkwargs = {\'num_workers\': 1, \'pin_memory\': True} if args.cuda else {}\r\ntrain_loader = torch.utils.data.DataLoader(\r\n    datasets.MNIST(\'../data\', train=True, download=True,\r\n                   transform=transforms.Compose([\r\n                       transforms.ToTensor(),\r\n                       transforms.Normalize((0.1307,), (0.3081,))\r\n                   ])),\r\n    batch_size=args.batch_size, shuffle=True, **kwargs)\r\ntest_loader = torch.utils.data.DataLoader(\r\n    datasets.MNIST(\'../data\', train=False, transform=transforms.Compose([\r\n                       transforms.ToTensor(),\r\n                       transforms.Normalize((0.1307,), (0.3081,))\r\n                   ])),\r\n    batch_size=args.test_batch_size, shuffle=True, **kwargs)\r\n\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\r\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\r\n        self.conv2_drop = nn.Dropout2d()\r\n        self.fc1 = nn.Linear(320, 50)\r\n        self.fc2 = nn.Linear(50, 10)\r\n\r\n    def forward(self, x):\r\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\r\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\r\n        x = x.view(-1, 320)\r\n        x = F.relu(self.fc1(x))\r\n        x = F.dropout(x, training=self.training)\r\n        x = self.fc2(x)\r\n        return F.log_softmax(x)\r\n\r\nmodel = Net()\r\nif args.cuda:\r\n    model.cuda()\r\n\r\noptimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\r\n\r\ndef train(epoch):\r\n    model.train()\r\n    for batch_idx, (data, target) in enumerate(train_loader):\r\n        if args.cuda:\r\n            data, target = data.cuda(), target.cuda()\r\n        data, target = Variable(data), Variable(target)\r\n        optimizer.zero_grad()\r\n        output = model(data)\r\n        loss = F.nll_loss(output, target)\r\n        loss.backward()\r\n        optimizer.step()\r\n        # if batch_idx % args.log_interval == 0:\r\n        #     print(\'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\'.format(\r\n        #         epoch, batch_idx * len(data), len(train_loader.dataset),\r\n        #         100. * batch_idx / len(train_loader), loss.data[0]))\r\n\r\ndef test():\r\n    model.eval()\r\n    test_loss = 0\r\n    correct = 0\r\n    for data, target in test_loader:\r\n        if args.cuda:\r\n            data, target = data.cuda(), target.cuda()\r\n        data, target = Variable(data, volatile=True), Variable(target)\r\n        output = model(data)\r\n        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\r\n        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\r\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\r\n\r\n    test_loss /= len(test_loader.dataset)\r\n    print(\'\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\'.format(\r\n        test_loss, correct, len(test_loader.dataset),\r\n        100. * correct / len(test_loader.dataset)))\r\n\r\n\r\nif args.resume_type:\r\n    checkpoint_file = \'weights/\' + args.exp_name + \'/\'\r\n    checkpoint_file += \'checkpoint.pth.tar\' if args.resume_type == \'last\' else \'model_best.pth.tar\'\r\n    if os.path.isfile(checkpoint_file):\r\n        print(""=> loading checkpoint \'{}\'"".format(checkpoint_file))\r\n        checkpoint = torch.load(checkpoint_file)\r\n        args.start_epoch = checkpoint[\'epoch\']\r\n        best_loss = checkpoint[\'best_loss\']\r\n        model.load_state_dict(checkpoint[\'state_dict\'])\r\n        optimizer.load_state_dict(checkpoint[\'optimizer\'])\r\n        print(""=> loaded checkpoint \'{}\' (epoch {})""\r\n              .format(checkpoint_file, checkpoint[\'epoch\']))\r\n    else:\r\n        print(""=> no checkpoint found at \'{}\'"".format(checkpoint_file))\r\n\r\n\r\ndef save_checkpoint(state, is_best):\r\n    exp_weights_root_dir = \'weights/\' + args.exp_name + \'/\'\r\n    os.makedirs(exp_weights_root_dir, exist_ok=True)\r\n    filename = exp_weights_root_dir + \'checkpoint.pth.tar\'\r\n    torch.save(state, filename)\r\n    if is_best:\r\n        print(\'best beaten\')\r\n        shutil.copyfile(filename, exp_weights_root_dir + \'model_best.pth.tar\')\r\n\r\n\r\nfor epoch in range(args.start_epoch, args.epochs + args.start_epoch):\r\n    train(epoch)\r\n    test()\r\n    save_checkpoint(\r\n                    {\r\n                        \'epoch\': epoch + 1,\r\n                        \'state_dict\': model.state_dict(),\r\n                        \'best_loss\': 0,\r\n                        \'optimizer\': optimizer.state_dict(),\r\n                    }, True)\r\n```\r\n\r\n</details>', ""@ahmedanis03 can you please provide us with a diff for this file instead of a whole new version? That would make it easier to see what has changed.\r\n\r\n@magic282 I'm glad to hear that you resolved some parts of the problem. I'm not entirely sure what's left. We're aware of the difference in speed between Windows and Linux, but note that you're not using official Windows packages (there are none at the moment). We will work on resolving these problems before we release 0.4 (which will have a Windows package)."", '@apaszke HYG https://www.diffchecker.com/o1YJsfZo (i hope that is what you want)\r\n\r\nand to clarify where is the inconsistency here are the steps i made:\r\n1) train the model **from scratch** for 2 epochs, you will get \r\nexp1_epoch_one_accuracy and exp1_epoch_two_accuracy\r\n2) train the model **from scratch** for 1 epochs, you will get \r\nexp2_epoch_one_accuracy = exp1_epoch_one_accuracy\r\n3) train the model **from weights of exp_2** and train for 1 epochs, you will get \r\nexp2_epoch_two_accuracy != exp1_epoch_two_accuracy ', ""You have dropout in your model, so the RNG state also affects the results. If you want full reproducibility after the checkpoint, you'll need to serialize all RNGs too (or e.g. at the beginning of each epoch do `torch.manual_seed(args.seed + epoch)`)."", '@apaszke how does one save and load the RNG state? asked too early:\r\ntorch.random.get_rng_state()\r\ntorch.random.set_rng_state()', '@apaszke I have similar problem and I tried your suggestion (set `torch.manual_seed(SEED)` at every epoch) but the results still different compared\r\ndo you have any idea?', 'I also encountered the same issue while saving and loading the same model weights. ']","['python\r\nTHCudaCheck FAIL file=/opt/conda/conda-bld/pytorch_1512386481460/work/torch/lib/THC/THCTensorCopy.cu line=204 error=59 : device-side assert triggered\r\nTraceback (most recent call last):\r\n  File ""test.py"", line 120, in <module>\r\n    main()\r\n  File ""test.py"", line 76, in main\r\n    hidden = hidden.transpose(0, 1).contiguous().view(hidden.size(1), -1)\r\n  File ""/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/autograd/variable.py"", line 280, in contiguous\r\n    self.data = self.data.contiguous()\r\nRuntimeError: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1512386481460/work/torch/lib/THC/THCTensorCopy.cu:204\r\n']",[],0,0
712,pytorch,24991,closed,[Feature request] Add support for selu activation to calculate_gain function.,"## üöÄ Feature

Add support for selu activation to calculate_gain function.

https://pytorch.org/docs/stable/_modules/torch/nn/init.html#calculate_gain

cc @albanD @mruberry @SsnL",good first issue module: nn proposal accepted triaged,"[""@hadaev8 Do you know what the correct value has to be?\r\n\r\n@t-vi reported last year a gain-value of 0.75 for selu was stable for him, see forums: https://discuss.pytorch.org/t/calculate-gain-tanh/20854/7\r\n\r\nIf I just look empirically at the std-gain I get the following numbers:\r\n\r\n```\r\nidentity  : 1.0\r\nsigmoid   : 4.809119701385498\r\ntanh      : 1.5975916385650635\r\nrelu      : 1.7068026065826416\r\nleaky_relu: 1.5421059131622314\r\nselu      : 1.0002975463867188\r\nelu       : 1.2694571018218994\r\n```\r\n\r\nIf somebody is familiar with the relevant theory it would be great if he or she gave a hint how the numbers in `calculate_gain` were calculated. For example why has `torch.sigmoid` (Fermi function) a gain of 1?\r\n\r\nCode used to calculate the std-gain:\r\n```\r\nx = torch.randn(100000)\r\ndef test_gain(x, fn):\r\n    return x.std() / fn(x).std()\r\n\r\nfns = [('identity', lambda x: x),\r\n  ('sigmoid', torch.sigmoid), \r\n  ('tanh', torch.tanh),\r\n  ('relu', torch.nn.functional.relu),\r\n  ('leaky_relu', lambda x: torch.nn.functional.leaky_relu(x, 0.2)),\r\n  ('selu', torch.nn.functional.selu),\r\n  ('elu', torch.nn.functional.elu)]\r\nresult = [(name, test_gain(x, fn)) for name, fn in fns]\r\nfor name, gain in result:\r\n  print('{:<10}: {}'.format(name, gain))\r\n```"", '@andreaskoepf sorry, have no idea how this values work.', 'I can confirm the results reported by user jpeg729 and Thomas in the forums: https://discuss.pytorch.org/t/calculate-gain-tanh/20854/4 Their gain test-script is really nice and shows that the gradient can behave very differently from the layer activations .\r\n\r\nOutput of the test-script (100 layer deep net):\r\n\r\n```\r\n$ python init_gain_test.py selu 1\r\nin: 1.0005\r\nout: 1.0002 grad: 0.8287\r\nout: 0.9998 grad: 1.1398\r\nout: 0.9982 grad: 1.7574\r\nout: 1.0030 grad: 2.3416\r\nout: 1.0006 grad: 3.4042\r\nout: 1.0037 grad: 4.5539\r\nout: 0.9981 grad: 6.7281\r\nout: 1.0047 grad: 9.2336\r\nout: 1.0000 grad: 12.8418\r\nout: 0.9986 grad: 18.7562\r\n\r\n$ python init_gain_test.py selu 0.75\r\nin: 1.0000\r\nout: 0.7959 grad: 0.6368\r\nout: 0.3066 grad: 0.2759\r\nout: 0.2345 grad: 0.1911\r\nout: 0.2074 grad: 0.2228\r\nout: 0.2049 grad: 0.1948\r\nout: 0.1884 grad: 0.2140\r\nout: 0.1953 grad: 0.2590\r\nout: 0.1974 grad: 0.2521\r\nout: 0.2063 grad: 0.2637\r\nout: 0.2141 grad: 0.2270\r\n```\r\n\r\nRegarding my code: 0 should be taken as the reference of the variance calculation since the weights of the following layer will project initially with a random sign (the sign of the activation function output does not matter, e.g. if it is biased).\r\n\r\nThen the values are:\r\n```\r\nidentity  : 1.0\r\nsigmoid   : 1.8471702337265015\r\ntanh      : 1.5913989543914795\r\nrelu      : 1.4173601865768433\r\nleaky_relu: 1.3895978927612305\r\nselu      : 1.000707983970642\r\nelu       : 1.2470202445983887\r\n```\r\nThe relu result then gets close to `math.sqrt(2)` (1.41421...) as expected. selu/elu values are not really affected that much.\r\n\r\nCorrected test_gain() function:\r\n```\r\ndef test_gain(x, fn):\r\n    return x.pow(2).mean().sqrt() / fn(x).pow(2).mean().sqrt()\r\n```\r\n', '@andreaskoepf \r\nAre you gonna PR it?', ""sounds good, we'll accept a PR that does this."", '@gchanan \r\nNever contributed to such big repo, do I need anything besides adding activations and values to calculate_gain function?', 'Hi @hadaev8: that plus add a test in test/test_nn.py (you can see some other calculate_gain examples in there.).\r\n\r\n', 'Reserving for bootcamp.', 'Releasing from bootcamp reservation. ', ""This thread seems to have become stale. @andreaskoepf do you mind if I submit a PR for this? I was able to reproduce your gradient results with the selu function and the value 0.75.\r\n```\r\nin: 1.0008\r\nout: 0.7968 grad: 0.6509\r\nout: 0.3127 grad: 0.2760\r\nout: 0.2404 grad: 0.2337\r\nout: 0.2062 grad: 0.2039\r\nout: 0.2056 grad: 0.1795\r\nout: 0.2044 grad: 0.1977\r\nout: 0.2005 grad: 0.2045\r\nout: 0.2042 grad: 0.2273\r\nout: 0.1944 grad: 0.2034\r\nout: 0.2085 grad: 0.2464\r\n```\r\nBut I'm not sure what do you use the std-gain values for. The gradients explode when putting in your std-gain value.\r\n```\r\nUsing selu  with gain_value =1.000707983970642\r\n\r\nin: 1.0000\r\nout: 1.0011 grad: 0.8655\r\nout: 1.0002 grad: 1.1315\r\nout: 0.9985 grad: 1.6839\r\nout: 1.0008 grad: 2.4107\r\nout: 0.9999 grad: 2.9653\r\nout: 1.0029 grad: 4.5212\r\nout: 1.0068 grad: 6.8250\r\nout: 1.0017 grad: 10.2219\r\nout: 1.0026 grad: 13.3068\r\nout: 1.0036 grad: 18.5051\r\n```\r\n\r\nI would like your clarification on this if possible. Otherwise, I'll set the selu gain_value to 0.75.\r\n\r\nAlso, I tried out the default gain value for tanh but the gradients explode (below). Can I change the tanh gain_value? Using a gain of 1 seems to maintain the gradients stable even at 100 layers. I can explore the other gain values too and add the necessary changes within the PR.\r\n```\r\nUsing tanh with gain_value = 5/3\r\nin: 1.0002\r\nout: 0.7589 grad: 0.7070\r\nout: 0.6514 grad: 1.7144\r\nout: 0.6520 grad: 4.3221\r\nout: 0.6501 grad: 11.4622\r\nout: 0.6508 grad: 29.8123\r\nout: 0.6517 grad: 76.7714\r\nout: 0.6506 grad: 187.0547\r\nout: 0.6508 grad: 498.8236\r\nout: 0.6518 grad: 1332.0450\r\nout: 0.6509 grad: 3348.0054\r\n```\r\n```\r\nUsing tanh with gain_value = 1\r\nin: 1.0005\r\nout: 0.6285 grad: 0.5451\r\nout: 0.2188 grad: 0.1876\r\nout: 0.1564 grad: 0.1455\r\nout: 0.1282 grad: 0.1281\r\nout: 0.1105 grad: 0.0968\r\nout: 0.0976 grad: 0.1053\r\nout: 0.0900 grad: 0.1133\r\nout: 0.0831 grad: 0.0657\r\nout: 0.0774 grad: 0.0602\r\nout: 0.0722 grad: 0.0840\r\n```"", ""@andreaskoepf @gchanan\r\nI'm using mish activation function (this why I'm ghosted).\r\nIs it ok to add custom gain argument in pr?"", ""@ajsanjoaquin @hadaev8 I never looked closer than what I originally  posted. I am not working on a PR. My test values only considered the forward gain (with 'artificial' 0 mean), not the gradient gain (which would be more interesting). A value of 0.75 for selu (as @t-vi found out) is better and should be used. I am also not aware what the other frameworks do and I would suggest to check this first."", '@hadaev8 feel free to let me know if the PR needs editing. @gchanan may I request that you review my PR? And am I allowed to check & edit the other activations in custom_gain if they have useful gain values or do I need to open a new Issue for that? Thanks.', '@ajsanjoaquin \r\nMy only suggestion is custom gain value param in addition to the activation function name, so I can relay on native PyTorch with custom activation functions and reduce my codebase for a bit.', '@hadaev8 Wouldn\'t it be easier to just explicitly declare a custom gain value for custom activation functions because you would write less? \r\n\r\n```python\r\nimport torch.nn.functional as F\r\na = torch.randn(1000,1000, requires_grad=True)\r\nl = torch.nn.Linear(1000,1000, bias=False)\r\ntorch.nn.init.xavier_normal_(l.weight,10000)\r\nF.custom_activation(torch.nn.Linear(1000,1000, bias=False)(a))\r\n```\r\nas opposed to\r\n```python\r\na = torch.randn(1000,1000, requires_grad=True)\r\nl = torch.nn.Linear(1000,1000, bias=False)\r\ntorch.nn.init.xavier_normal_(l.weight, torch.nn.init.calculate_gain(""custom"", 10000))\r\nF.custom_activation(torch.nn.Linear(1000,1000, bias=False)(a))\r\n```\r\n\r\nOr am I misunderstanding your suggestion? Thanks.', 'Self-normalising networks are an alternative to normalisation layers. The goal is to induce a stable fixed point in the variance during the forward pass. To get this stable fixed point, the SELU activation function is used in combination with weights that have variance `1/fan_in`. It might be suboptimal for the backward pass, but the gain should definitely be 1 in order for the normalisation to work! The current implementation is wrong and makes it unnecessarily difficult to implement self-normalising networks.', '@mrTsjolder What would you suggest for mish?\r\nI seem to to have self-normalizing.\r\nSame as selu linear gain?', '> @mrTsjolder What would you suggest for mish?\r\n> I seem to to have self-normalizing.\r\n> Same as selu linear gain?\r\n\r\nThe moment propagation in swish-like functions is generally convex, which means that these induce unstable fixed points. The mish has this same property and can therefore not induce self-normalisation. ']",[],[],0,0
713,pytorch,15129,closed,ResNet -101 does not run when converted to sequential (returns dimension mismatch),"## üêõ Bug

I wanted to obtain convolutional features from ResNet-101. I had already figured out how to do it, but I wanted to run a simple test to check if I am applying the correct preprocessing steps, so instead of omitting the last layer, I transformed the whole network into a nn.Sequential(), only to get an error:
RuntimeError: size mismatch, m1: [2048 x 1], m2: [2048 x 1000] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:940

## To Reproduce

Here is the code:


When I run resnet_eval with a valid image path, I get the following output:



Of course, transforming the whole net into a sequential model is non-sense, we can just replace the first line (after the docstring) of resnet_eval with

and get the expected output


Another approach is to remove the final layer, reshape the output tensor and then add the last layer again, like this:

and once again, this works as expected, giving the same output as when running


If I print the tensor values, they are identical in the two last cases. So why do I come accross the mismatch error in the first case (or equivalently, why is the last case valid)?

## Expected behavior

The first case should run smoothly, giving the same output as the second one.

## Environment

Tested with Python3.5 and 
torch==1.0.0
torchvision==0.2.1
",,"[""your code is buggy, plain and simple.\r\n\r\n```\r\n self.resnet = nn.Sequential(\r\n            *list(models.resnet101(pretrained=True).children())\r\n        )\r\n```\r\n\r\nThe logic of resnet101 is not simply entirely encoded in it's `.children()`, read the `.forward` function for additional logic (in fact, in cases 2 and 3, you figured it out).\r\n\r\nIf you have questions in the future, post on our forums https://discuss.pytorch.org\r\n\r\nIf you are pretty sure it's a bug in the framework, file a github issue."", 'Ok, I checked the ```.forward``` method, it is all clear now. Thank you for your response!']","['python\r\n""""""Test PyTorch ResNet-101.""""""\r\nfrom matplotlib import pyplot as plt\r\nfrom PIL import Image\r\nfrom torch import nn\r\nfrom torchvision import models, transforms\r\n\r\n\r\nclass ResNet101(nn.Module):\r\n    """"""ResNet-101 PyTorch.""""""\r\n\r\n    def __init__(self):\r\n        """"""Initialize network and freeze layers.""""""\r\n        super().__init__()\r\n        self.resnet = nn.Sequential(\r\n            *list(models.resnet101(pretrained=True).children())\r\n        )\r\n        for param in self.resnet.parameters():\r\n            param.requires_grad = False\r\n\r\n    def forward(self, x):\r\n        """"""Forward pass.""""""\r\n        x = self.resnet(x)\r\n        return x\r\n\r\n\r\ndef resnet_eval(img_path):\r\n    """"""Run a simple classification.""""""\r\n    resnet = ResNet101().eval()\r\n    img = Image.open(img_path)\r\n    preprocessing = transforms.Compose([\r\n        transforms.Resize((224, 224)),\r\n        transforms.ToTensor(),\r\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\r\n                             std=[0.229, 0.224, 0.225])\r\n    ])\r\n    inputs = preprocessing(img).unsqueeze(0)\r\n    print(inputs.shape)\r\n\r\n    outputs = resnet(inputs)\r\n    print(outputs.data.shape)\r\n    # print(outputs)\r\n    print(outputs[0].sort()[1][-1])\r\n    plt.figure()\r\n    plt.imshow(img)\r\n    plt.axis(\'off\')\r\n    plt.show()\r\n', '\r\ntorch.Size([1, 3, 224, 224])\r\nTraceback (most recent call last):\r\n  File ""res.py"", line 58, in <module>\r\n    resnet_eval(folder + img_name)\r\n  File ""res.py"", line 39, in resnet_eval\r\n    outputs = resnet(inputs)\r\n  File ""/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py"", line 489, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""res.py"", line 22, in forward\r\n    x = self.resnet(x)\r\n  File ""/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py"", line 489, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""/usr/local/lib/python3.5/dist-packages/torch/nn/modules/container.py"", line 92, in forward\r\n    input = module(input)\r\n  File ""/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py"", line 489, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""/usr/local/lib/python3.5/dist-packages/torch/nn/modules/linear.py"", line 67, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\n  File ""/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py"", line 1354, in linear\r\n    output = input.matmul(weight.t())\r\nRuntimeError: size mismatch, m1: [2048 x 1], m2: [2048 x 1000] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:940\r\n', '\r\nresnet = models.resnet101(pretrained=True).eval()\r\n', '\r\ntorch.Size([1, 3, 224, 224])\r\ntorch.Size([1, 1000])\r\ntensor(546)\r\n', 'python\r\nclass ResNet101(nn.Module):\r\n    """"""ResNet-101 PyTorch.""""""\r\n\r\n    def __init__(self):\r\n        """"""Initialize network and freeze layers.""""""\r\n        super().__init__()\r\n        self.resnet = nn.Sequential(\r\n            *list(models.resnet101(pretrained=True).children())[:-1]\r\n        )\r\n        for param in self.resnet.parameters():\r\n            param.requires_grad = False\r\n        self.linear = list(models.resnet101(pretrained=True).children())[-1]\r\n\r\n    def forward(self, x):\r\n        """"""Forward pass.""""""\r\n        x = self.resnet(x)\r\n        x = x.view(x.shape[0], -1)\r\n        x = self.linear(x)\r\n        return x\r\n', '\r\nresnet = models.resnet101(pretrained=True).eval()\r\n']",[],0,0
714,pytorch,16198,closed,ppc64le CI build failure (possibly due to #16006,"## üêõ Bug

ppc64le CI build fails with psutil not found.

Running test_dataloader ... [2019-01-20 15:16:53.873206]
Traceback (most recent call last):
  File ""test_dataloader.py"", line 24, in <module>
    import psutil
ModuleNotFoundError: No module named 'psutil'


pytorch version : pull from master for last 3 days.
here is the CI log:
https://powerci.osuosl.org/user/avmgithub/my-views/view/PyTorch/job/pytorch-linux-cuda92-cudnn7-py3-mpi-build-test-gpu/269/console

CC:  @SsnL 
",,"['@avmgithub the docker image for the contbuild has to be updated with `pip install psutil`', 'This is expected. Any env invoking `.jenkins/pytorch/common.sh` is assumed to be a PyTorch CI env. For PyTorch CI env, we require `psutil` to be installed.']",[],[],0,0
715,pytorch,22017,closed,mkldnn_version.h: No such file or directory,"I compiled pytorch under ubuntu18.04, gcc 7.4.0, and I git checkout v1.0.0 before compiling.
After the compiling process begins, I terminated at about 34% with the error:


So what's wrong with it and how can I solve this problem? Can anyone help me with this issue?
",module: build module: mkldnn triaged,"['Have you tried `git submodule update --init --recursive` after you do `git checkout v1.0.0`?', 'Closing due to lack of activity. Feel free to reopen with new data.']","['\r\n[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/mkl/LinearAlgebra.cpp.o\r\n[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/mkl/SpectralOps.cpp.o\r\n[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/mkldnn/Conv.cpp.o\r\nCompiling  all_gather.cu                       > /home/xwdu/pytorch/build/nccl/obj/collectives/device/all_gather_min_f32.o\r\nIn file included from /home/xwdu/pytorch/third_party/ideep/mkl-dnn/include/mkldnn.hpp:28:0,\r\n                 from /home/xwdu/pytorch/aten/src/ATen/mkldnn/Runtime.h:3,\r\n                 from /home/xwdu/pytorch/aten/src/ATen/native/mkldnn/Conv.cpp:37:\r\n/home/xwdu/pytorch/third_party/ideep/mkl-dnn/include/mkldnn.h:55:10: fatal error: mkldnn_version.h: No such file or directory\r\n #include ""mkldnn_version.h""\r\n          ^~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\n']",[],0,0
716,pytorch,6486,closed,Where is the Caffe2 website?,The gh-pages branch doesn't exist.,,"['https://github.com/caffe2/caffe2.github.io', ""Thanks, I'll change the wrong occurrences of the gh-pages branch.""]",[],[],0,0
717,pytorch,19807,closed,Create Extension Error: Pytorch 0.4.0,"Hi everyone,

I am trying to user FFI extensions in order to add some CUDA compiled functions. The error I encountered is as follows;

cffi.CDefError: cannot parse ""#include <THC/THC.h>""
<cdef source string>:29:1: Directives not supported yet

My environmental configuration is as follows;

Cuda Compilation Tool Version: 9.0, v9.0.176
pycparser version 2.18
PyTorch Version: 0.4.0

Could you please provide assitance about this issue?
Thanks.",,"['FFI extensions are removed as of 1.0 release.\r\nin 0.4, the cffi header parser cannot parse macros. You can look at https://github.com/pytorch/extension-ffi\r\n\r\nif you have questions, use https://discuss.pytorch.org\r\n']",[],[],0,0
718,pytorch,3064,closed,double backward for Conv2d failing with CUDNN_STATUS_NOT_SUPPORTED,"Reported at https://discuss.pytorch.org/t/cudnn-status-not-supported-error-occurs-when-apply-autograd-grad-to-compute-high-order-differentiation/8256

",,"['Looking into this.', 'fixed via https://github.com/pytorch/pytorch/pull/3148 i believe.']","[""python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable, grad\r\nimport torch.utils.data as Data\r\nimport torchvision\r\n\r\ntrain_data = torchvision.datasets.MNIST(\r\n    root='./mnist/',\r\n    train=True,\r\n    transform=torchvision.transforms.ToTensor(),\r\n    download=True,\r\n)\r\n\r\ntrain_loader = Data.DataLoader(\r\n    dataset=train_data, batch_size=50, shuffle=True, num_workers=2)\r\n\r\n\r\nclass CNN(nn.Module):\r\n    def __init__(self):\r\n        super(CNN, self).__init__()\r\n        self.conv1 = nn.Sequential(nn.Conv2d(1, 16, 5, 1, 2))\r\n        self.out = nn.Linear(16 * 28 * 28, 10)\r\n\r\n    def forward(self, x):\r\n        x = self.conv1(x)\r\n        x = x.view(x.size(0), -1)\r\n        output = self.out(x)\r\n        return output, x\r\n\r\n\r\ncnn = CNN()\r\ncnn.cuda()\r\n\r\nloss_func = nn.CrossEntropyLoss()\r\n\r\nfor step, (data, label) in enumerate(train_loader):\r\n    input = Variable(data).cuda()\r\n    target = Variable(label).cuda()\r\n\r\n    output = cnn(input)[0]\r\n    loss = loss_func(output, target)\r\n\r\n    params = cnn.parameters()\r\n    g = grad(loss, params, create_graph=True)\r\n\r\n    g_sum = 0\r\n    for g_para in g:\r\n        g_sum += g_para.sum()\r\n\r\n    params = cnn.parameters()\r\n    hv = grad(g_sum, params, create_graph=True)\r\n\r\n    break\r\n""]",[],0,0
719,pytorch,16326,closed,[JIT] C++ frontend is unable to support std::vector<std::vector<Tensor>>,"## üêõ Bug
C++ frontend is unable to support std::vector<std::vector<Tensor>>

## To Reproduce
Python Code



CPP code

Error Message




## Some preliminary debugging
In particular, this is the offending function




this is the call site of the offending function. It does not seem to check generic types. Like . Instead, it goes straight to 



## Environment
Pytorch version: 4edc8273eb223785929c0017caf15c101964d480

",oncall: jit,"['CC - @zdevito since you left a comment about List of List there.', 'Yeah, this is a known limitation in the current C++ <-> TorchScript interface. We should fix it (the XXX in that function describes the fix). ']","['\r\nimport torch\r\nclass Model(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, x_list_list):\r\n        # type: (List[List[Tensor]]) -> Tensor\r\n\r\n        return x_list_list[0][0]\r\n\r\n\r\nm = Model()\r\nm.save(""./model.pt"")\r\n', '\r\n    std::string path = ""./model.pt"";\r\n    std::shared_ptr<torch::jit::script::Module> module = torch::jit::load(path);\r\n\r\n    std::vector<torch::Tensor> one_frame = {\r\n            torch::rand({4, 4}),\r\n            torch::rand({4, 4}),\r\n            torch::rand({4, 4}),\r\n            torch::rand({4, 4}),\r\n\r\n    };\r\n\r\n    std::vector<torch::jit::IValue> frames;\r\n    frames.push_back(one_frame);\r\n    frames.push_back(one_frame);\r\n    std::vector<torch::jit::IValue> inputs;\r\n    inputs.push_back(frames);\r\n\r\n    auto result = module->forward(inputs);\r\n    log_info(""finished inference"")\r\n\r\n', '\r\nunknown file: Failure\r\nC++ exception with description ""Type cannot be accurately recovered from this IValue. (incompleteInferTypeFrom at aten/src/ATen/core/type.cpp:149)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x45 (0x7fccd18b6dd5 in /home/sidney/bazel/av/bin/tests/../../lib/third_party/libc10.so)\r\nframe #1: c10::incompleteInferTypeFrom(c10::IValue const&) + 0x3b4 (0x7fccbc189ac4 in /home/sidney/bazel/av/bin/tests/../../lib/third_party/libcaffe2.so)\r\nframe #2: torch::jit::script::Method::checkInputsAgainstSchema(std::vector<c10::IValue, std::allocator<c10::IValue> >&) + 0xa8 (0x40f448 in ./bin/tests/intentnet_test)\r\n\r\n', '\r\n// why incomplete? You cannot completely recover a type from\r\n// an IValue, List[List[int]] and List[List[Tensor]] will both\r\n// become ivalue.isGenericList() and cannot be recovered.\r\n// The only appropriate place to use this is where you know that\r\n// you are only dealing with a subset of objects where you can recover\r\n// the type, like in the tracer.\r\nTypePtr incompleteInferTypeFrom(const IValue& value) {\r\n  if (value.isTensor()) {\r\n    return CompleteTensorType::create(value.toTensor());\r\n  } else if (value.isDouble()) {\r\n    return FloatType::get();\r\n  } else if (value.isInt()) {\r\n    return IntType::get();\r\n  } else if (value.isBool()) {\r\n    return BoolType::get();\r\n  } else if (value.isString()) {\r\n    return StringType::get();\r\n  } else if (value.isIntList()) {\r\n    return ListType::ofInts();\r\n  } else if (value.isTensorList()) {\r\n    return ListType::ofTensors();\r\n  } else if (value.isBoolList()) {\r\n    return ListType::ofBools();\r\n  } else if (value.isDoubleList()) {\r\n    return ListType::ofFloats();\r\n  } else if (value.isTuple()) {\r\n    return TupleType::create(fmap(value.toTuple()->elements(), incompleteInferTypeFrom));\r\n  } else if (value.isDevice()) {\r\n    return DeviceObjType::get();\r\n  }\r\n  AT_ERROR(""Type cannot be accurately recovered from this IValue."");\r\n}\r\n', '\r\n\r\n    for (size_t pos = 0; pos < schema.arguments().size(); ++pos) {\r\n      const auto& argument = schema.arguments()[pos];\r\n      if (pos < inputs.size()) {\r\n        // XXX - this fails to handle generic aggregates\r\n        // and should be replaced with a function isSubvalueOf(ivalue, type)\r\n        // That asks if the specific value is a valid instance of type.\r\n        const TypePtr inputType = incompleteInferTypeFrom(inputs[pos]);\r\n        AT_CHECK(\r\n            inputType->isSubtypeOf(argument.type()),\r\n            ""Expected value of type "",\r\n            *argument.type(),\r\n            "" for argument \'"",\r\n            argument.name(),\r\n            ""\' in position "",\r\n            pos,\r\n            "", but instead got value of type "",\r\n            *inputType,\r\n            "". Declaration: "",\r\n            schema);\r\n      } else if (argument.default_value()) {\r\n        inputs.push_back(*argument.default_value());\r\n      } else {\r\n        AT_ERROR(\r\n            schema.name(),\r\n            ""() is missing value for argument \'"",\r\n            argument.name(),\r\n            ""\'. Declaration: "",\r\n            schema);\r\n      }\r\n    }\r\n\r\n']","['ivalue.isGenericList()', 'incompleteInferType']",0,0
720,pytorch,21701,closed,Add comments on classes to `bailout_graph.cpp`,,module: docs oncall: jit triaged,"['@ZolotukhinM ', 'did you mean bailout_graph?', '@gcatron yes üòÑ thank you! ', 'It looks like there are comments now']",[],[],0,0
721,pytorch,3798,closed,Can't Install from source if path contains a space,"Hi everyone,

silly ""gotcha"" installing the latest master from source (on ubuntu 16)- if there is a space in the path you install from, it fails to compile the ""compiler simple test"", and throws some fairly obscure error messages about cc being an unrecognised compiler.

Perhaps just needs a line in the instructions, as it took digging through the compiler logs to find the issue (unless i missed something!).

Apologies if duplicated, but couldn't find a relevant issue in a quick search. Happy to contribute fix to docs etc if required.",,"[""As you noticed a lot of the build steps assume that the path doesn't contain a space.  For now an addition to the README in the build from source section would be great."", ""We've made a lot of improvements here, e.g., #38860. Going to pray that we've fixed this. If there are still problems feel free to file issues.""]",[],[],0,0
722,pytorch,11121,closed,Running torch.cuda.is_available() before import from torch._C import *,"## Issue description

Using a NFS, sometimes I find myself on machines without _Cuda_, although _PyTorch_ was installed on a _Cuda_ enabled machine. _PyTorch_ fails to load and crashes instead.

## Code example



## System Info

- PyTorch or Caffe2: _PyTorch_
- How you installed PyTorch (conda, pip, source): , on a _Cuda_ enabled machine
- OS: _CentOS_ 7
- PyTorch version: 
- Python version: 
- CUDA/cuDNN version:  or ",,"['You can create two conda environments, one with cuda pytorh and one with cpu-only pytorch.', ""Sure, sure, but then I would have to reinstall all packages twice?\r\nWouldn't be easier just to check whether cuda is available before loading those libs?"", 'Could you please reinstall PyTorch? This seems to be related to a recent issue (#10910) which has been fixed.', '@vishwakftw, the issue was about different `cudnn` versions.\r\nOn some machines I do not have any `cudnn` or GPU altogether.', 'Oh, sorry. I noticed the `libcudart.so.9.0` and thought the solved issue was at play.', ""@Atcold this was fixed recently. What you are seeing is indeed what @vishwakftw was pointing to. If you reinstall pytorch from conda, you'll be fine."", 'Yeah, that was it. Now it works fine. Thanks @vishwakftw and @soumith.\r\n@soumith, you mentioned that\r\n\r\n> `conda` changed cudatoolkit from `9.0` to `9.2` 3 days ago.\r\n\r\nWhat does that have to do with a machine that may not have Cuda at all?\r\nFor me it was working on some machine and on others not. Both share the same `$HOME` and *installation* folders.']",['python\r\nIn [1]: import torch\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-23eb050c9654> in <module>()\r\n----> 1 import torch\r\n\r\n[...]/atcold/anaconda3/lib/python3.6/site-packages/torch/__init__.py in <module>()\r\n     78     pass\r\n     79\r\n---> 80 from torch._C import *\r\n     81\r\n     82 __all__ += [name for name in dir(_C)\r\n\r\nImportError: libcudart.so.9.0: cannot open shared object file: No such file or directory\r\n'],"['conda', '0.4.1', '3.6.6', 'N/A', '9.0']",0,0
723,pytorch,29211,closed,"""multinomial_kernel_cuda"" not implemented for 'Half'","It appears that even after the  fixes,  no longer works with  dtypes

## To Reproduce







## Environment


## Additional context
This is not blocking us because we can convert to fp32 before sampling",,[],"['bash\r\npip install -U --pre torch==1.4.0.dev20191104 torchvision -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html\r\n', 'python\r\nimport torch\r\n\r\n\r\ndef test_fp16_categorical():\r\n    logits_fp16 = torch.randn(20).cuda().half()\r\n\r\n    # These are fine\r\n    torch.argmax(logits_fp16)\r\n    torch.max(logits_fp16)\r\n\r\n    # This is also fine\r\n    logits_fp32 = logits_fp16.float()\r\n    sample = torch.distributions.Categorical(logits=logits_fp32).sample()\r\n    print(sample)\r\n\r\n    # This fails\r\n    sample = torch.distributions.Categorical(logits=logits_fp16).sample()\r\n    print(sample)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    test_fp16_categorical()\r\n', ' \r\nFile ""/opt/conda/lib/python3.7/site-packages/torch/distributions/categorical.py"", line 107, in sample\r\n    sample_2d = torch.multinomial(probs_2d, 1, True)\r\nRuntimeError: ""multinomial_kernel_cuda"" not implemented for \'Half\'\r\n', '\r\nCollecting environment information...\r\nPyTorch version: 1.4.0.dev20191104\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.11.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: Tesla V100-SXM2-16GB\r\nGPU 1: Tesla V100-SXM2-16GB\r\nGPU 2: Tesla V100-SXM2-16GB\r\nGPU 3: Tesla V100-SXM2-16GB\r\nGPU 4: Tesla V100-SXM2-16GB\r\nGPU 5: Tesla V100-SXM2-16GB\r\nGPU 6: Tesla V100-SXM2-16GB\r\nGPU 7: Tesla V100-SXM2-16GB\r\n\r\nNvidia driver version: 418.87.01\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.2\r\n']","['1.3.1', 'torch.distributions.Categorical', 'half']",0,0
724,pytorch,14891,closed,Wrong recursive module::load,"## üêõ Bug

Parameters and buffers must be checked recursively during the load procedure [module.cpp#L297](https://github.com/pytorch/pytorch/blob/8311bbee7f1dd33346f18c769cfeb8c5b5941874/torch/csrc/api/src/nn/module.cpp#L297). 
For example, like this:
",high priority module: cpp,"[""I don't understand the problem but marking this as hi-pri for understanding the problem"", 'cc @goldsborough ', ""That makes sense. I'll send a fix.""]",[],['if (!child.value()->parameters().empty() || !child.value()->buffers().empty())'],0,0
725,pytorch,30818,closed,kthvalue/median with scalar and dim=1 inconsistent between CPU and CUDA,,actionable module: error checking module: sorting and selection triaged,"[""This case is explicitly tested for on CPU, so I'm guessing the CPU version is right and the CUDA version is wrong."", 'cc @gchanan who is currently fixing these.', 'median is also affected by this.', 'This is out of scope of what I\'m currently fixing.  I\'m currently fixing the part of scalar handling across the TH->ATen boundary that is: ""you returned to me a size `(1,)` tensor, but did you really mean to return me a size `()` Tensor?""\r\n\r\nScalar handing within ops is a bit of a different issue and I believe @ezyang\'s analysis of this specific case is correct.']","['\r\n>>> import torch\r\n>>> torch.kthvalue(torch.tensor(2, device=\'cuda\'), 1)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\nIndexError: dimension specified as 0 but tensor has no dimensions\r\n>>> torch.kthvalue(torch.tensor(2, device=\'cpu\'), 1)\r\ntorch.return_types.kthvalue(\r\nvalues=tensor(2),\r\nindices=tensor(0))\r\n']",[],0,0
726,pytorch,16229,closed,test_dag_net_forking is flaky on ROCm,"This was first surfaced in #15817 but I am filing a new bug for clarity.

Error looks like:



https://ci.pytorch.org/jenkins/job/caffe2-builds/job/py2-devtoolset7-rocmrpm-centos7.5-test/5483/console

This seems to be specifically a ROCm failure. CC @bddppq @iotamudelta ",module: rocm,"['CC @mrshenli ', 'Hello, any update on this? Otherwise I will disable this test. @bddppq @iotamudelta @mrshenli ', 'no update from me', 'Taken a closer look, this indeed looks related to rocm:\r\n\r\n```\r\n\r\nThread 95 ""python"" received signal SIGSEGV, Segmentation fault.\r\n[Switching to Thread 0x7ffef7977700 (LWP 89694)]\r\n0x00007ffff6d9f914 in _int_malloc () from /lib64/libc.so.6\r\n(gdb) bt\r\n#0  0x00007ffff6d9f914 in _int_malloc () from /lib64/libc.so.6\r\n#1  0x00007ffff6da21ac in malloc () from /lib64/libc.so.6\r\n#2  0x00007fffed98decd in operator new(unsigned long) () from /lib64/libstdc++.so.6\r\n#3  0x00007fffd73e7831 in ?? () from /opt/rocm/hsa/lib/libhsa-runtime64.so.1\r\n#4  0x00007fffd73e80b6 in ?? () from /opt/rocm/hsa/lib/libhsa-runtime64.so.1\r\n#5  0x00007fffd73deead in ?? () from /opt/rocm/hsa/lib/libhsa-runtime64.so.1\r\n#6  0x00007fffdbfe4798 in am_aligned_alloc () at /data/hcc_hip_antistatic/hcc/lib/hsa/hc_am.cpp:266\r\n#7  0x00007fffe129b5ea in hip_internal::allocAndSharePtr(char const*, unsigned long, ihipCtx_t*, bool, unsigned int, unsigned int, unsigned long) () from /opt/rocm/hip/lib/libhip_hcc.so\r\n#8  0x00007fffe129d75b in hipMalloc () from /opt/rocm/hip/lib/libhip_hcc.so\r\n#9  0x00007fff37a2c9ff in caffe2::DefaultHIPAllocator::allocate(unsigned long) const () from /root/pytorch/caffe2/python/../../torch/lib/libcaffe2_hip.so\r\n#10 0x00007fff4e6217d6 in c10::TensorImpl::raw_mutable_data(caffe2::TypeMeta const&) () from /root/pytorch/caffe2/python/../../torch/lib/libcaffe2.so\r\n#11 0x00007fff4e7d9230 in caffe2::empty(c10::ArrayRef<long>, c10::TensorOptions) () from /root/pytorch/caffe2/python/../../torch/lib/libcaffe2.so\r\n#12 0x00007fff2bb8c8a7 in bool caffe2::FullyConnectedGradientOp<caffe2::HIPContext, caffe2::DefaultEngine, true>::DoRunWithType<float, float, float, float, float, float, float, float>() ()\r\n   from /root/pytorch/caffe2/python/../../torch/lib/libcaffe2_hip.so\r\n#13 0x00007fff2bb88988 in caffe2::FullyConnectedGradientOp<caffe2::HIPContext, caffe2::DefaultEngine, true>::RunOnDevice() () from /root/pytorch/caffe2/python/../../torch/lib/libcaffe2_hip.so\r\n#14 0x00007fff2bb47b0d in caffe2::Operator<caffe2::HIPContext>::RunAsync(int) () from /root/pytorch/caffe2/python/../../torch/lib/libcaffe2_hip.so\r\n#15 0x00007fff4e786f20 in caffe2::AsyncNetBase::run(int, int) () from /root/pytorch/caffe2/python/../../torch/lib/libcaffe2.so\r\n#16 0x00007fff4e78d3a9 in caffe2::AsyncSchedulingNet::schedule(int, bool)::{lambda()#1}::operator()() const () from /root/pytorch/caffe2/python/../../torch/lib/libcaffe2.so\r\n#17 0x00007fff4d86be33 in c10::ThreadPool::main_loop(unsigned long) () from /root/pytorch/caffe2/python/../../torch/lib/libcaffe2.so\r\n#18 0x00007fff4f99e5cf in execute_native_thread_routine () from /root/pytorch/caffe2/python/../../torch/lib/libcaffe2.so\r\n#19 0x00007ffff77fadd5 in start_thread () from /lib64/libpthread.so.0\r\n#20 0x00007ffff6e1aead in clone () from /lib64/libc.so.6\r\n```', '@ezyang Quick question, is it on centos only? ', '@petrex it happens on ubuntu as well https://ci.pytorch.org/jenkins/job/caffe2-builds/job/py2-clang7-rocmdeb-ubuntu16.04-test/8757/console (although seems to be much less frequent than on centos)', 'The corruption should be caused by the gemm call before the memory allocation:\r\n```\r\nThread 104 ""python"" received signal SIGABRT, Aborted.\r\n[Switching to Thread 0x7fff5f63d700 (LWP 90896)]\r\n0x00007ffff6d53207 in raise () from /lib64/libc.so.6\r\n(gdb) bt\r\n#0  0x00007ffff6d53207 in raise () from /lib64/libc.so.6\r\n#1  0x00007ffff6d548f8 in abort () from /lib64/libc.so.6\r\n#2  0x00007ffff6d95d27 in __libc_message () from /lib64/libc.so.6\r\n#3  0x00007ffff6d9e489 in _int_free () from /lib64/libc.so.6\r\n#4  0x00007fffbe8534d2 in deallocate () at /opt/rh/devtoolset-7/root/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/ext/new_allocator.h:125\r\n#5  deallocate () at /opt/rh/devtoolset-7/root/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/bits/alloc_traits.h:462\r\n#6  _M_deallocate () at /opt/rh/devtoolset-7/root/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/bits/stl_vector.h:180\r\n#7  _M_realloc_insert<std::shared_ptr<HSAOp> > () at /opt/rh/devtoolset-7/root/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/bits/vector.tcc:448\r\n#8  0x00007fffbe8367cc in emplace_back<std::shared_ptr<HSAOp> > () at /opt/rh/devtoolset-7/root/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/bits/vector.tcc:105\r\n#9  push_back () at /opt/rh/devtoolset-7/root/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/bits/stl_vector.h:954\r\n#10 pushAsyncOp () at /data/hcc_hip_antistatic/hcc/lib/hsa/mcwamp_hsa.cpp:1405\r\n#11 0x00007fffbe842c16 in EnqueueMarker () at /data/hcc_hip_antistatic/hcc/lib/hsa/mcwamp_hsa.cpp:1990\r\n#12 0x00007fffbe842539 in isEmpty () at /data/hcc_hip_antistatic/hcc/lib/hsa/mcwamp_hsa.cpp:1528\r\n#13 0x00007fffbe82ecfe in createOrstealRocrQueue () at /data/hcc_hip_antistatic/hcc/lib/hsa/mcwamp_hsa.cpp:2293\r\n#14 0x00007fffbe83191d in acquireLockedRocrQueue () at /data/hcc_hip_antistatic/hcc/lib/hsa/mcwamp_hsa.cpp:4012\r\n#15 0x00007fffbe839157 in dispatchKernelAsync () at /data/hcc_hip_antistatic/hcc/lib/hsa/mcwamp_hsa.cpp:4572\r\n#16 0x00007fffbe8388a7 in dispatch_hsa_kernel () at /data/hcc_hip_antistatic/hcc/lib/hsa/mcwamp_hsa.cpp:4242\r\n#17 0x00007fffe1322571 in ihipModuleLaunchKernel(ihipModuleSymbol_t*, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned long, ihipStream_t*, void**, void**, ihipEvent_t*, ihipEvent_t*) () from /opt/rocm/hip/lib/libhip_hcc.so\r\n#18 0x00007fffe1323167 in hipModuleLaunchKernel () from /opt/rocm/hip/lib/libhip_hcc.so\r\n#19 0x00007fffe134948f in hip_impl::hipLaunchKernelGGLImpl(unsigned long, dim3 const&, dim3 const&, unsigned int, ihipStream_t*, void**) () from /opt/rocm/hip/lib/libhip_hcc.so\r\n#20 0x00007fff1a3d0fab in void hipLaunchKernelGGL<float*, float const*, float const*, float, float, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, void (*)(float*, float const*, float const*, float, float, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int)>(void (*)(float*, float const*, float const*, float, float, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int), dim3 const&, dim3 const&, unsigned int, ihipStream_t*, float*, float const*, float const*, float, float, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int) () from /opt/rocm/rocblas/lib/librocblas.so.0\r\n#21 0x00007fff1a3e8d57 in Cijk_Ailk_Bjlk_SB_MT128x064x08_AF0EM01_ASEM01_BL0_DTL0_FL0_GRVW01_GSU01_ISA000_KLS_LPA00_LPB00_MGWVW01_NLCA01_NLCB01_PBC0_PGR0_PLR1_TT08_04_USFGRO00_VAW01_VW01_WG16_16_01_WGM08(float*, float const*, float const*, float, float, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, unsigned int, ihipStream_t*, unsigned int, ihipEvent_t**, ihipEvent_t**) () from /opt/rocm/rocblas/lib/librocblas.so.0\r\n#22 0x00007fff1a1f9ff1 in rocblas_sgemm () from /opt/rocm/rocblas/lib/librocblas.so.0\r\n#23 0x00007fff3aa4c07d in void caffe2::math::Gemm<float, caffe2::HIPContext, caffe2::DefaultEngine>(CBLAS_TRANSPOSE, CBLAS_TRANSPOSE, int, int, int, float, float const*, float const*, float, float*, caffe2::HIPContext*, caffe2::TensorProto_DataType) () from /root/pytorch/caffe2/python/../../torch/lib/libcaffe2_hip.so\r\n#24 0x00007fff2eab66fd in bool caffe2::FullyConnectedGradientOp<caffe2::HIPContext, caffe2::DefaultEngine, true>::DoRunWithType<float, float, float, float, float, float, float, float>() ()\r\n   from /root/pytorch/caffe2/python/../../torch/lib/libcaffe2_hip.so\r\n#25 0x00007fff2eab2988 in caffe2::FullyConnectedGradientOp<caffe2::HIPContext, caffe2::DefaultEngine, true>::RunOnDevice() () from /root/pytorch/caffe2/python/../../torch/lib/libcaffe2_hip.so\r\n#26 0x00007fff2ea71b0d in caffe2::Operator<caffe2::HIPContext>::RunAsync(int) () from /root/pytorch/caffe2/python/../../torch/lib/libcaffe2_hip.so\r\n#27 0x00007fff792d5f20 in caffe2::AsyncNetBase::run(int, int) () from /root/pytorch/caffe2/python/../../torch/lib/libcaffe2.so\r\n#28 0x00007fff792dc3a9 in caffe2::AsyncSchedulingNet::schedule(int, bool)::{lambda()#1}::operator()() const () from /root/pytorch/caffe2/python/../../torch/lib/libcaffe2.so\r\n#29 0x00007fff783bae33 in c10::ThreadPool::main_loop(unsigned long) () from /root/pytorch/caffe2/python/../../torch/lib/libcaffe2.so\r\n#30 0x00007fff7a4ed5cf in execute_native_thread_routine () from /root/pytorch/caffe2/python/../../torch/lib/libcaffe2.so\r\n#31 0x00007ffff77fadd5 in start_thread () from /lib64/libpthread.so.0\r\n#32 0x00007ffff6e1aead in clone () from /lib64/libc.so.6\r\n```\r\nFrom the stack trace looks like it\'s related to one of our recent bug report about HSAQueue issue. @scchan Could you help investigate whether it\'s related?', '@bddppq yes, this is the same as the one in your recent bug report.', 'https://github.com/pytorch/pytorch/pull/16639', '@scchan Is this the related fix? https://github.com/RadeonOpenCompute/hcc/commit/41f23b7a13a415453f7c8f54d6cc031fbec4ab74', '@bddppq that one helps but also this fix here  https://github.com/RadeonOpenCompute/hcc/pull/1013', 'Thanks @scchan @petrex for quick fix. For now we have temporarily skipped the test in our rocm CI, will re-enable it once the fix has come into the release (and our docker image).']","['\r\n01:46:35 ../.local/lib/python2.7/site-packages/caffe2/python/hypothesis_test.py::TestOperators::test_constant_fill PASSED [  7%]\r\n01:46:35 ../.local/lib/python2.7/site-packages/caffe2/python/hypothesis_test.py::TestOperators::test_cos PASSED [  8%]\r\n01:46:36 ../.local/lib/python2.7/site-packages/caffe2/python/hypothesis_test.py::TestOperators::test_dag_net_forking ./.jenkins/caffe2/test.sh: line 120:  5452 Segmentation fault      (core dumped) ""$PYTHON"" -m pytest -x -v --disable-warnings --junit-xml=""$pytest_reports_dir/result.xml"" --ignore ""$caffe2_pypath/python/test/executor_test.py"" --ignore ""$caffe2_pypath/python/operator_test/matmul_op_test.py"" --ignore ""$caffe2_pypath/python/operator_test/pack_ops_test.py"" --ignore ""$caffe2_pypath/python/mkl/mkl_sbn_speed_test.py"" ${rocm_ignore_test[@]} ""$caffe2_pypath/python"" ""${EXTRA_TESTS[@]}""\r\n01:46:36 + echo \'Stopping container...\'\r\n']",[],0,0
727,pytorch,27569,closed,Export from TorchScript to ONNX: torch.onnx.symbolic_opset9.dim does not exist,"## üêõ Bug

TorchScript -> ONNX conversion of a simple module fails

If one doesn‚Äôt jit-compile the model, everything works.

## To Reproduce



Output



## Expected behavior

I expected that a jit-compiled module consisting of just two  children should export to ONNX without hassle.

## Environment

Google Colab, I think at the moment it has PyTorch 1.2.0 and Python 3.6

## Additional context
I played around with TorchScript tracing / ONNX export of modules that work with namedtuples, got some errors. Trying to get a minimal example has led me to this code with no namedtuples.

cc @suo",module: onnx oncall: jit triaged,"['If you want to export the model to ONNX, please directly use `torch.onnx.export` (no `torch.jit.script` is needed) as indicated in the [ONNX section](https://pytorch.org/docs/stable/onnx.html), `torch.onnx.export` will call TorchScript tracing underlying and do some additional graph transformation to make it compatible with ONNX format. ', '> If you want to export the model to ONNX, please directly use `torch.onnx.export` (no `torch.jit.script` is needed) as indicated in the [ONNX section](https://pytorch.org/docs/stable/onnx.html), `torch.onnx.export` will call TorchScript tracing underlying and do some additional graph transformation to make it compatible with ONNX format.\r\n\r\nStill I want to ask, is it possible (may be with modified pytorch source) to convert a TorchScript model to ONNX?', '> > If you want to export the model to ONNX, please directly use `torch.onnx.export` (no `torch.jit.script` is needed) as indicated in the [ONNX section](https://pytorch.org/docs/stable/onnx.html), `torch.onnx.export` will call TorchScript tracing underlying and do some additional graph transformation to make it compatible with ONNX format.\r\n> \r\n> Still I want to ask, is it possible (may be with modified pytorch source) to convert a TorchScript model to ONNX?\r\n\r\nIn C++ there is no torch::onnx::export function, so we can only save the torchscript model. This model cannot be used in other libraries like OpenCV, except when it is converted to onnx.', '@suo is exporting an already jit traced model to onnx supported/possible in pytorch? \r\nattempting to do so in the latest stable release (1.7) fails!']","['\r\nfrom tempfile import TemporaryFile\r\n\r\nimport torch\r\nimport torch.onnx\r\nimport torch.jit\r\nfrom torch import nn, Tensor\r\n\r\nprint(f""PyTorch version is {torch.__version__}"")\r\n\r\nclass Model(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.module = nn.Linear(\r\n            in_features=8, out_features=4)\r\n        self.module2 = nn.Linear(\r\n            in_features=4, out_features=2)\r\n       \r\n    def forward(self, x: Tensor) -> Tensor:\r\n        preout = self.module(x)\r\n        out = self.module2(preout)\r\n        return out\r\n\r\n\r\nmodel = Model()\r\nmodel = torch.jit.script(model)\r\n\r\ndummy_input = torch.randn(3, 8)\r\ndummy_output = model(dummy_input)\r\n\r\nwith TemporaryFile() as temp:\r\n    torch.onnx.export(model=model, \r\n                      args=dummy_input, \r\n                      example_outputs=dummy_output,\r\n                      f=temp, \r\n                      verbose=True)\r\n', '\r\nPyTorch version is 1.2.0\r\n/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py:562: UserWarning: ONNX export failed on ATen operator dim because torch.onnx.symbolic_opset9.dim does not exist\r\n  .format(op_name, opset_version, op_name))\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-1-036960002e79> in <module>()\r\n     33                       example_outputs=dummy_output,\r\n     34                       f=temp,\r\n---> 35                       verbose=True)\r\n \r\n7 frames\r\n/usr/local/lib/python3.6/dist-packages/torch/onnx/__init__.py in export(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes)\r\n    130                         operator_export_type, opset_version, _retain_param_name,\r\n    131                         do_constant_folding, example_outputs,\r\n--> 132                         strip_doc_string, dynamic_axes)\r\n    133\r\n    134\r\n \r\n/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py in export(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes)\r\n     62             operator_export_type=operator_export_type, opset_version=opset_version,\r\n     63             _retain_param_name=_retain_param_name, do_constant_folding=do_constant_folding,\r\n---> 64             example_outputs=example_outputs, strip_doc_string=strip_doc_string, dynamic_axes=dynamic_axes)\r\n     65\r\n     66\r\n \r\n/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py in _export(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, example_outputs, propagate, opset_version, _retain_param_name, do_constant_folding, strip_doc_string, dynamic_axes)\r\n    327                                                         output_names, operator_export_type,\r\n    328                                                         example_outputs, propagate,\r\n--> 329                                                         _retain_param_name, do_constant_folding)\r\n    330\r\n    331         # TODO: Don\'t allocate a in-memory string for the protobuf\r\n \r\n/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py in _model_to_graph(model, args, verbose, training, input_names, output_names, operator_export_type, example_outputs, propagate, _retain_param_name, do_constant_folding, _disable_torch_constant_prop)\r\n    223\r\n    224     graph = _optimize_graph(graph, operator_export_type,\r\n--> 225                             _disable_torch_constant_prop=_disable_torch_constant_prop)\r\n    226\r\n    227     if isinstance(model, torch.jit.ScriptModule) or isinstance(model, torch.jit.Function):\r\n \r\n/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py in _optimize_graph(graph, operator_export_type, _disable_torch_constant_prop)\r\n    125         torch._C._jit_pass_erase_number_types(graph)\r\n    126\r\n--> 127         graph = torch._C._jit_pass_onnx(graph, operator_export_type)\r\n    128         torch._C._jit_pass_lint(graph)\r\n    129         from torch.onnx.symbolic_helper import _export_onnx_opset_version\r\n \r\n/usr/local/lib/python3.6/dist-packages/torch/onnx/__init__.py in _run_symbolic_function(*args, **kwargs)\r\n    161 def _run_symbolic_function(*args, **kwargs):\r\n    162     from torch.onnx import utils\r\n--> 163     return utils._run_symbolic_function(*args, **kwargs)\r\n    164\r\n    165\r\n \r\n/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py in _run_symbolic_function(g, n, inputs, env, operator_export_type)\r\n    561                                   ""torch.onnx.symbolic_opset{}.{} does not exist""\r\n    562                                   .format(op_name, opset_version, op_name))\r\n--> 563                 op_fn = sym_registry.get_registered_op(op_name, \'\', opset_version)\r\n    564                 return op_fn(g, *inputs, **attrs)\r\n    565\r\n \r\n/usr/local/lib/python3.6/dist-packages/torch/onnx/symbolic_registry.py in get_registered_op(opname, domain, version)\r\n     89         warnings.warn(""ONNX export failed. The ONNX domain and/or version are None."")\r\n     90     global _registry\r\n---> 91     return _registry[(domain, version)][opname]\r\n\r\nKeyError: \'dim\'\r\n']",['nn.Linear'],0,0
728,pytorch,10690,closed,non-blocking cuda2cpu copy shouldn't require contiguity,,,['fixed'],"['\r\n>>> x = torch.rand(2,3).cuda()\r\nx.>>> x.t().cpu()\r\ntensor([[0.6202, 0.0882],\r\n        [0.1994, 0.1465],\r\n        [0.5524, 0.7550]])\r\n>>> x.t().to(\'cpu\')\r\ntensor([[0.6202, 0.0882],\r\n        [0.1994, 0.1465],\r\n        [0.5524, 0.7550]])\r\n>>> x.t().to(\'cpu\', non_blocking=True)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\nRuntimeError: invalid argument 3: Source tensor must be contiguous at /private/home/ssnl/sftp/pytorch/aten/src/THC/generic/THCTensorCopy.cpp:141\r\n']",[],0,0
729,pytorch,30578,closed,gradients of symeig not correct and gradcheck fails,"## üêõ Bug

The off-diagonal elements of torch.symeig's gradients are off by a factor of 2. Further, checking the gradients with torch.autograd.gradcheck gives a RuntimeError.

## To Reproduce

The following code computes an eigenvalue decomposition of a symmetric matrix and corresponding derivatives with two different approaches. One approach uses torch.symeig and backward() to compute the gradient. The second approach uses torch.symeig and numerical differentiation to compute the gradient.
Further, torch.autograd.gradcheck is called to test torch.symeig and it gives a RuntimeError.


This prints

Some observations can be made:
First, since numerical differentiation is not implemented to take the symmetry of the matrix M into account, only an upper triangular matrix is computed. This is true for both the version implemented above and for the error message of torch.autograd.gradcheck. The two numerical differentiation versions are identical.
Second, the off-diagonal elements computed via backward() and those computed via numerical differentiation differ by a factor of 2.

## Expected behavior

The derivative computed via backward() should (at least in the upper triangular part) be identical to the numerical differentiation gradient. The reason why this is not the case is that the structure of inputs to torch.symeig (i.e., the symmetry of the inputs) is not taken into account in the backward pass of torch.symeig. Derivatives of functions of symmetric matrices need some extra care, as equation (138) in the [matrix cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) highlights. As far as I can tell, only the [line of code which computes the return value of symeig's backward()](https://github.com/pytorch/pytorch/blob/dd52f50fc85e6710f020936c1fc5f14673508350/tools/autograd/templates/Functions.cpp#L1797) needs to be modified.
To demonstrate this, the following implements a custom gradient of symeig where the backward pass return value is changed slightly.

This prints

This agrees with numerical differentiation.

So, should the implementation of symeig_backward be changed?

## Environment

Output from environment collection script:



cc @vishwakftw @SsnL @jianyuh",module: linear algebra triaged,"['There is ambiguity here since symeig is only defined for symmetrical matrices (So you can view the result being computed by using either triangular part or both). It is more intuitive to return a symmetrical gradient, so that change was made. Context: https://github.com/pytorch/pytorch/issues/22807 ', ""Yes, the gradient needs to be symmetric. For symeig, it has been made symmetric in #23018. However, according to, e.g., equation (138) in the [matrix cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf), the chosen way of symmetrization is only almost correct. That's why I opened this issue."", ""I think this reasoning makes sense. If @vishwakftw agrees, let's change the derivative of `symeig` according to @michael-koller-91 comment"", 'Im not sure how to reconcile the above with the following (creating a symmetric matrix by broadcasting a single vector)\r\n```\r\ndef get_numerical_gradient_v(func, x, epsilon):\r\n    assert len(x.shape)==1\r\n    g = np.zeros(x.shape)\r\n    for i in range(0, x.shape[0]):\r\n        e = np.zeros(x.shape)\r\n        e[i] = 1.0\r\n        g[i] = (func(x+epsilon*e) - func(x-epsilon*e)) / (2*epsilon)\r\n    return g\r\n\r\ndef symmetric_combine(v):\r\n    n = 3.0\r\n    M = (0.5*(v[...,None]**n + v[None,...]**n)**(1/n))\r\n    return M\r\n\r\ndef func_numpy(v):\r\n    M = symmetric_combine(v)\r\n    assert(M==M.T).all()\r\n    d, u = np.linalg.eigh(M, UPLO=""U"")\r\n    result = np.sum(d) + np.sum(u)\r\n    return result\r\n\r\ndef func_torch(v, getgrad=False):\r\n    M = symmetric_combine(v)\r\n    assert (M == M.T).byte().all()\r\n    d, u = torch.symeig(M, eigenvectors=True)\r\n    result = torch.sum(d) + torch.sum(u)\r\n    if getgrad:\r\n        result.backward()\r\n        return v.grad.data.numpy()\r\n    return result\r\n\r\ndef func_torch_modified(v, getgrad=False):\r\n    M = symmetric_combine(v)\r\n    assert (M==M.T).byte().all()\r\n    d, u = SymEig.apply(M)\r\n    result = torch.sum(d) + torch.sum(u)\r\n    if getgrad:\r\n        result.backward()\r\n        return v.grad.data.numpy()\r\n    return result\r\n\r\nnp.random.seed(2020)\r\n# for numerical gradient\r\nepsilon = 1e-6\r\n# random symmetric matrix will be (dim x dim)\r\ndim = 3\r\np = np.random.random(dim) + 1.0\r\n\r\nngrad_numpy = get_numerical_gradient_v(func_numpy, p.copy(), epsilon)\r\nngrad_torch = get_numerical_gradient_v(func_torch, torch.tensor(p), epsilon)\r\nngrad_torch_mod = get_numerical_gradient_v(func_torch_modified, torch.tensor(p), epsilon)\r\ngrad_torch = func_torch(torch.tensor(p, requires_grad=True), True)\r\ngrad_torch_mod = func_torch_modified(torch.tensor(p, requires_grad=True), True)\r\n\r\nprint(\'-\'*10, \'numpy numerical\')\r\nprint(ngrad_numpy)\r\nprint(\'-\'*10, \'torch numerical\')\r\nprint(ngrad_torch)\r\nprint(\'-\'*10, \'torch mod numerical\')\r\nprint(ngrad_torch_mod)\r\nprint(\'-\'*10, \'torch analytic\')\r\nprint(grad_torch)\r\nprint(\'-\'*10, \'torch mod analytic\')\r\nprint(grad_torch_mod)\r\n\r\n\r\ntorch.autograd.gradcheck(func_torch, torch.tensor(p, requires_grad=True))\r\ntorch.autograd.gradcheck(func_torch_modified, torch.tensor(p, requires_grad=True))\r\n```\r\nWhich produces:\r\n```---------- numpy numerical\r\n[0.48886428 0.80186956 0.7265407 ]\r\n---------- torch numerical\r\n[0.48886428 0.80186956 0.7265407 ]\r\n---------- torch mod numerical\r\n[0.48886428 0.80186956 0.7265407 ]\r\n---------- torch analytic\r\n[0.48886428 0.80186956 0.7265407 ]\r\n---------- torch mod analytic\r\n[0.67149538 0.78753635 0.68563576]\r\n\r\nTraceback (most recent call last):\r\n  File ""./test_symeig.py"", line 117, in <module>\r\n    torch.autograd.gradcheck(func_torch_modified, torch.tensor(p, requires_grad=True))\r\n  File ""/gdn/centos7/0001/x3/prefixes/pytorch/1.4.0-cuda10-01c7__37694f0bcc63/lib/python3.7/site-packages/torch/autograd/gradcheck.py"", line 289, in gradcheck\r\n    \'numerical:%s\\nanalytical:%s\\n\' % (i, j, n, a))\r\n  File ""/gdn/centos7/0001/x3/prefixes/pytorch/1.4.0-cuda10-01c7__37694f0bcc63/lib/python3.7/site-packages/torch/autograd/gradcheck.py"", line 227, in fail_test\r\n    raise RuntimeError(msg)\r\nRuntimeError: Jacobian mismatch for output 0 with respect to input 0,\r\nnumerical:tensor([[0.4889],\r\n        [0.8019],\r\n        [0.7265]], dtype=torch.float64)\r\nanalytical:tensor([[0.6715],\r\n        [0.7875],\r\n        [0.6856]], dtype=torch.float64)\r\n```\r\n\r\nIE, the current implementation produces the correct numerical gradients, while the SymEig class in the first post does not.', 'Taking it one step further, and directly specifying the unique N*(N+1)/2 parameters of the symmetric matrix (and reusing the rest of the code from update#3):\r\n```\r\nDIM = 3\r\n\r\ndef build_symmetric(M, v):\r\n    idiag = np.diag_indices(DIM)\r\n    iupper = np.triu_indices(DIM,1)\r\n    M[idiag] = v[:DIM]\r\n    M[iupper] = v[DIM:]\r\n    M.T[iupper] = v[DIM:]\r\n    return M\r\n\r\ndef func_numpy(v):\r\n    M = np.empty((DIM,DIM))\r\n    build_symmetric(M, v)\r\n    assert(M==M.T).all()\r\n    d, u = np.linalg.eigh(M, UPLO=""U"")\r\n    result = np.sum(d) + np.sum(u)\r\n    return result\r\n\r\ndef func_torch(v, getgrad=False):\r\n    M = torch.empty((DIM,DIM), dtype=torch.double)\r\n    build_symmetric(M, v)\r\n    assert (M == M.T).byte().all()\r\n    d, u = torch.symeig(M, eigenvectors=True)\r\n    result = torch.sum(d) + torch.sum(u)\r\n    if getgrad:\r\n        result.backward()\r\n        return v.grad.data.numpy()\r\n    return result\r\n\r\ndef func_torch_modified(v, getgrad=False):\r\n    M = torch.empty((DIM,DIM), dtype=torch.double)\r\n    build_symmetric(M, v)\r\n    assert (M==M.T).byte().all()\r\n    d, u = SymEig.apply(M)\r\n    result = torch.sum(d) + torch.sum(u)\r\n    if getgrad:\r\n        result.backward()\r\n        return v.grad.data.numpy()\r\n    return result\r\n\r\nnp.random.seed(2020)\r\n# for numerical gradient\r\nepsilon = 1e-6\r\np = np.random.random(int(DIM*(DIM+1)/2)) + 1.0\r\n```\r\nproduces\r\n```\r\n---------- numpy numerical\r\n[ 0.62609536  1.17912154  1.1947831  -0.11645342  0.01913861  0.19356785]\r\n---------- torch numerical\r\n[ 0.62609536  1.17912154  1.1947831  -0.11645342  0.01913861  0.19356785]\r\n---------- torch mod numerical\r\n[ 0.62609536  1.17912154  1.1947831  -0.11645342  0.01913861  0.19356785]\r\n---------- torch analytic\r\n[ 0.62609536  1.17912154  1.1947831  -0.11645342  0.01913861  0.19356785]\r\n---------- torch mod analytic\r\n[ 0.62609536  1.17912154  1.1947831  -0.23290684  0.03827722  0.38713569]\r\n```\r\nSo it looks like the current implementation is correct. ', ""Thank you for your investigation. I think the surprising result might be due to the fact that by using the build_symmetric function, one effectively has not ```DIM*DIM``` variables but ```(DIM*(DIM+1)/2)``` which leads to a wrong result. This is reflected by the fact that the gradient in your last post has only dimension 6 instead of 9.\r\n\r\nIf we explicitly create a matrix with ```DIM*DIM``` independent variables, the result is consistent again. After running your code, I ran the following:\r\n```\r\n# copied from your code\r\nM = torch.empty((DIM, DIM), dtype=torch.double)\r\nM = build_symmetric(M, torch.tensor(p.copy()))\r\n# matrix with DIM*DIM variables\r\nMM = torch.autograd.Variable(M, requires_grad=True)\r\nassert (MM == MM.T).byte().all()\r\nd, u = SymEig.apply(MM)\r\nresult = torch.sum(d) + torch.sum(u)\r\nresult.backward()\r\n\r\nprint('-'*10, 'torch mod analytic')\r\nprint(MM.grad.data.numpy())\r\n```\r\nThis prints\r\n```\r\n---------- torch mod analytic\r\n[[ 0.62609536 -0.11645342  0.01913861]\r\n [-0.11645342  1.17912154  0.19356785]\r\n [ 0.01913861  0.19356785  1.1947831 ]]\r\n```\r\nThis is consistent with the gradients you posted."", 'This paper says the gradient in the matrix cookbook is wrong (ie, one can not form the symmetric gradient Gsym(A) via G(A)+G(A)^T - diag(G(A) )\r\n\r\nWhat is the gradient of a scalar function of a symmetric matrix? \r\nhttps://arxiv.org/pdf/1911.06491.pdf', ""Nice, thank you for the link to this paper! That's the solution of the issue. So, in summary, all sources which use G(A) + G(A)^T - diag(G(A)) are not correct. Instead, (G(A) + G(A)^T) / 2 is the correct formula. Since the latest version implements exactly this, everything is fine.\r\n\r\nThanks for the discussion.""]","['\r\nimport numpy as np\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\n\r\ndef get_numerical_gradient(func, x, epsilon):\r\n    """"""\r\n    Numerical differentiation.\r\n    If e_ij is the matrix with 1 in row i, column j and 0 elsewhere,\r\n    get_numerical_gradient computes\r\n        g_ij = func(x + epsilon*e_ij) - func(x - epsilon*e_ij))/(2*epsilon)\r\n    for all i and j and returns the gradient matrix constructed from\r\n    the g_ij.\r\n    """"""\r\n    g = np.zeros_like(x)\r\n    for i in range(0, x.shape[0]):\r\n        for j in range(0, x.shape[1]):\r\n            e = np.zeros_like(x)\r\n            e[i, j] = 1.0\r\n            g[i, j] = (func(x+epsilon*e) - func(x-epsilon*e)) / (2*epsilon)\r\n    return g\r\n\r\n\r\n#\r\n# The following two functions perform the same calculation: a sum over all\r\n# eigenvalues and eigenvectors\r\n#\r\ndef sum_val_plus_vec_torch(x):\r\n    d, u = torch.symeig(x, eigenvectors=True)\r\n    return torch.sum(d) + torch.sum(u)\r\n\r\n\r\n# This version is used for numerical differentiation.\r\ndef sum_val_plus_vec_np(x):\r\n    x_torch = Variable(torch.from_numpy(x.copy()))\r\n    s = sum_val_plus_vec_torch(x_torch)\r\n    return s.data.numpy()\r\n\r\n\r\n# for numerical gradient\r\nepsilon = 1e-6\r\n# random symmetric matrix\r\ndim = 2\r\nM = np.random.randn(dim, dim)\r\nM = M + M.T\r\n\r\n# numerical gradient\r\ngrad_num = get_numerical_gradient(sum_val_plus_vec_np, M, epsilon)\r\n\r\n# torch gradient\r\nM_torch = Variable(torch.from_numpy(M.copy()), requires_grad=True)\r\nsum_torch = sum_val_plus_vec_torch(M_torch)\r\nsum_torch.backward()\r\ngrad_torch = M_torch.grad.data.numpy()\r\n\r\nprint(\'-\'*10, \'numerical differentiation\')\r\nprint(grad_num)\r\nprint(\'-\'*10, \'torch\')\r\nprint(grad_torch)\r\n\r\nM_torch = Variable(torch.from_numpy(M.copy()), requires_grad=True)\r\nprint(torch.autograd.gradcheck(sum_val_plus_vec_torch, M_torch))\r\n', '\r\n---------- numerical differentiation\r\n[[1.00980921 0.08135323]\r\n [0.         0.99019079]]\r\n---------- torch\r\n[[1.00980921 0.04067662]\r\n [0.04067662 0.99019079]]\r\nTraceback (most recent call last):\r\n  File ""grad_test.py"", line 62, in <module>\r\n    print(torch.autograd.gradcheck(sum_val_plus_vec_torch, M_torch))\r\n  File ""C:\\Users\\abc\\Miniconda3\\envs\\main\\lib\\site-packages\\torch\\autograd\\gradcheck.py"", line\r\n289, in gradcheck\r\n    \'numerical:%s\\nanalytical:%s\\n\' % (i, j, n, a))\r\n  File ""C:\\Users\\abc\\Miniconda3\\envs\\main\\lib\\site-packages\\torch\\autograd\\gradcheck.py"", line\r\n227, in fail_test\r\n    raise RuntimeError(msg)\r\nRuntimeError: Jacobian mismatch for output 0 with respect to input 0,\r\nnumerical:tensor([[1.0098],\r\n        [0.0814],\r\n        [0.0000],\r\n        [0.9902]], dtype=torch.float64)\r\nanalytical:tensor([[1.0098],\r\n        [0.0407],\r\n        [0.0407],\r\n        [0.9902]], dtype=torch.float64\r\n', '\r\nclass SymEig(torch.autograd.Function):\r\n    """"""\r\n    This modified version of torch.symeig has different gradients.\r\n    The forward pass is not modified.\r\n    """"""\r\n    @staticmethod\r\n    def forward(ctx, in_tensor):\r\n        d, u = torch.symeig(in_tensor, eigenvectors=True)\r\n        ctx.save_for_backward(d, u)\r\n        return d, u\r\n\r\n    @staticmethod\r\n    def backward(ctx, *grad_output):\r\n        gd, gu = grad_output\r\n        d, u = ctx.saved_tensors\r\n        grad_input = None\r\n\r\n        ut = u.transpose(-2, -1)\r\n\r\n        F = d.unsqueeze(-2) - d.unsqueeze(-1)\r\n        F.diagonal(0, -2, -1).fill_(float(\'inf\'))\r\n        F.pow_(-1)\r\n\r\n        F.mul_(torch.matmul(ut, gu))\r\n\r\n        grad_input = torch.matmul(\r\n            u,\r\n            torch.matmul(torch.diag_embed(gd) + F, ut)\r\n        )\r\n        # until this point, no modification\r\n        #\r\n        # since A is symmetric, use equation (138) matrix cookbook\r\n        return grad_input + grad_input.transpose(-2, -1) \\\r\n            - torch.diag_embed(grad_input.diagonal(dim1=-2, dim2=-1))\r\n\r\n\r\n# test modified torch gradient\r\ndef sum_val_plus_vec_torch_modified(x):\r\n    d, u = SymEig.apply(x)\r\n    return torch.sum(d) + torch.sum(u)\r\n\r\n\r\nM_torch_modified = Variable(torch.from_numpy(M.copy()), requires_grad=True)\r\nsum_torch_modified = sum_val_plus_vec_torch_modified(M_torch_modified)\r\nsum_torch_modified.backward()\r\ngrad_torch_modified = M_torch_modified.grad.data.numpy()\r\n\r\nprint(\'-\'*10, \'torch modified\')\r\nprint(grad_torch_modified)\r\n', '\r\n---------- torch modified\r\n[[1.00980921 0.08135323]\r\n [0.08135323 0.99019079]]\r\n', '\r\nPyTorch version: 1.3.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Microsoft Windows 10 Pro\r\nGCC version: (x86_64-posix-seh, Built by strawberryperl.com project) 8.3.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.17.4\r\n[pip] torch==1.3.1\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      245\r\n[conda] mkl-service               2.3.0            py37hb782905_0\r\n[conda] mkl_fft                   1.0.15           py37h14836fe_0\r\n[conda] mkl_random                1.1.0            py37h675688f_0\r\n[conda] pytorch                   1.3.1           py3.7_cuda101_cudnn7_0    pytorch\r\n']",[],0,0
730,pytorch,16181,closed,[Feature Request] Add `__getitem__()` to `nn.Module`,"## üöÄ Feature

Enable to call each layer in nn.Module like python dict.

 or 
-> 


## How

Add bellow function to  .
- reference: [chainer.Chain](https://github.com/chainer/chainer/blob/v5.1.0/chainer/link.py#L871-L873)



## Motivation

Access each layer in network easily (Especially for beginners).

#### old



#### new




Maybe this is different from the concept of PyTorch?",todo,"['We are not against this, this should be helpful in some cases like the one you described.', 'Thank you for checking this issue !\r\nI am looking forward to it :)', ""Hi, I'm new to pytorch and would like to start contributing. If the issue is still open, can I take a look at this and try to fix it?"", '@hikjik yes, please go ahead', ""see linked PR for context, and I apologize to @hikjik for putting the effort and having the PR closed.\r\n\r\nReasoning as to not having this (from #18102):\r\n\r\n> I'd agree with Adam here. nn.Module is a class, and it's not normal or expected to have a __getitem__ like a dict. If you generally want a dict, you'd stick it in a nn.ModuleDict, if not you're better off simply using getattr""]","['python\r\n    def __getitem__(self, name):\r\n        """"""Equivalent to getattr.""""""\r\n        return getattr(self, name)\r\n', ""python\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        ...\r\n\r\n    def forward(self, x):\r\n        ...\r\n        # Apply each conv<number>_1, conv<number>_2\r\n        for i in range(10, 11 + 1):\r\n            output_tmp = outputs[-1]\r\n            output_tmp = F.relu(getattr(self, f'conv{i:d}_1')(output_tmp))\r\n            output_tmp = F.relu(getattr(self, f'conv{i:d}_2')(output_tmp))\r\n            outputs.append(output_tmp)\r\n        return outputs\r\n"", ""python\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        ...\r\n\r\n    def forward(self, x):\r\n        ...\r\n        # Apply each conv<number>_1, conv<number>_2\r\n        for i in range(10, 11 + 1):\r\n            output_tmp = outputs[-1]\r\n            output_tmp = F.relu(self[f'conv{i:d}_1'](output_tmp))\r\n            output_tmp = F.relu(self[f'conv{i:d}_2'](output_tmp))\r\n            outputs.append(output_tmp)\r\n        return outputs\r\n""]","['self.__getattr__(layer_name)', 'getattr(self, layer_name)', 'self[layer_name]', 'nn.Module']",0,0
731,pytorch,20794,closed,"pytorch1.1.0 windows than one operator "" "" matches these operands","When I compile the c++ extension on windows with pytorch1.1.0. I get the following error.
How to resolve the question? Thanks!
E:/Program Files/Python35/lib/site-packages/torch/include\THC/THCNumerics.cuh(190 than one operator ""<"" matches these operands:
            built-in operator ""arithmetic < arithmetic""
            function ""operator<(const __half &, const __half &)""
            operand types are: c10::Half < c10::Half

E:/Program Files/Python35/lib/site-packages/torch/include\THC/THCNumerics.cuh(191 than one operator ""<="" matches these operands:
            built-in operator ""arithmetic <= arithmetic""
            function ""operator<=(const __half &, const __half &)""
            operand types are: c10::Half <= c10::Half

E:/Program Files/Python35/lib/site-packages/torch/include\THC/THCNumerics.cuh(192 than one operator "">"" matches these operands:
            built-in operator ""arithmetic > arithmetic""
            function ""operator>(const __half &, const __half &)""
            operand types are: c10::Half > c10::Half

E:/Program Files/Python35/lib/site-packages/torch/include\THC/THCNumerics.cuh(193 than one operator "">="" matches these operands:
            built-in operator ""arithmetic >= arithmetic""
            function ""operator>=(const __half &, const __half &)""
            operand types are: c10::Half >= c10::Half

E:/Program Files/Python35/lib/site-packages/torch/include\THC/THCNumerics.cuh(194 than one operator ""=="" matches these operands:
            built-in operator ""arithmetic == arithmetic""
            function ""operator==(const __half &, const __half &)""
            operand types are: c10::Half == c10::Half

E:/Program Files/Python35/lib/site-packages/torch/include\THC/THCNumerics.cuh(196 than one operator ""!="" matches these operands:
            built-in operator ""arithmetic != arithmetic""
            function ""operator!=(const __half &, const __half &)""
            operand types are: c10::Half != c10::Half",module: windows,"['Please provide the version numbers of VS, CUDA.', '@peterjc123 CUDA9.0 VS 2015.  C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\x86_amd64\\cl.exe \r\nI also try vs2017 ver 15.9 with vs15.4.  \r\nAnother question occurs.  fatal error C1189: #error:  -- unsupported Microsoft Visual Studio verson! Only the versions 2012, 2013, 2015 and 2017 are supported!\r\n\r\nThen I set #if _MSC_VER < 1600 || _MSC_VER > 1920 in host_config.h\r\n\r\nI got another question. \r\ntype_traits(603): error: expression must have a constant value', 'You\'ll need to activate VS 15.4 since 15.9 will be the default if you don\'t do that, and it will be not compatible with CUDA 9. Make sure you installed `VC++ 2017 version 15.4 v14.11 toolset` and try the following script.\r\n```cmd\r\nfor /f ""usebackq tokens=*"" %i in (`""%ProgramFiles(x86)%\\Microsoft Visual Studio\\Installer\\vswhere.exe"" -version [15^,16^) -products * -latest -property installationPath`) do call ""%i\\VC\\Auxiliary\\Build\\vcvarsall.bat"" x64 -vcvars_ver=14.11\r\nset DISTUTILS_USE_SDK=1\r\nwhere cl :: save the first line of the output\r\nset CUDAHOSTCXX=[The value saved before] :: remove the brackets\r\n```\r\nUpdate: Changing `host_config.h` won\'t help.', '@peterjc123  Thanks for your quickly reply. How to run the following script?\r\n\r\nfor /f ""usebackq tokens=*"" %i in (`""%ProgramFiles(x86)%\\Microsoft Visual Studio\\Installer\\vswhere.exe"" -version [15^,16^) -products * -latest -property installationPath`) do call ""%i\\VC\\Auxiliary\\Build\\vcvarsall.bat"" x64 -vcvars_ver=14.11\r\nset DISTUTILS_USE_SDK=1\r\nwhere cl :: save the first line of the output\r\nset CUDAHOSTCXX=[The value saved before] :: remove the brackets', 'in cmd just before you run `python setup.py` or sth like this.', '@peterjc123  I always get the error  bash: syntax error near unexpected token `""usebackq tokens=*""\'.\r\n', ""No, please don't type this one in bash. Please type this one in Command Prompt / Powershell (Windows Key + R -> CMD -> Enter or Hold shift -> Click the empty space in a directory in Windows Explorer -> Press mouse right button -> Open in command prompt) "", '@peterjc123  I get the same issue as vs2015.  \r\n\r\nE:/Program Files/Python35/lib/site-packages/torch/include\\THC/THCNumerics.cuh(190): error: more than one operator ""<"" matches these operands:\r\n            built-in operator ""arithmetic < arithmetic""\r\n            function ""operator<(const __half &, const __half &)""\r\n            operand types are: c10::Half < c10::Half\r\n\r\nE:/Program Files/Python35/lib/site-packages/torch/include\\THC/THCNumerics.cuh(191): error: more than one operator ""<="" matches these operands:\r\n            built-in operator ""arithmetic <= arithmetic""\r\n            function ""operator<=(const __half &, const __half &)""\r\n            operand types are: c10::Half <= c10::Half\r\n\r\nE:/Program Files/Python35/lib/site-packages/torch/include\\THC/THCNumerics.cuh(192): error: more than one operator "">"" matches these operands:\r\n            built-in operator ""arithmetic > arithmetic""\r\n            function ""operator>(const __half &, const __half &)""\r\n            operand types are: c10::Half > c10::Half\r\n\r\nE:/Program Files/Python35/lib/site-packages/torch/include\\THC/THCNumerics.cuh(193): error: more than one operator "">="" matches these operands:\r\n            built-in operator ""arithmetic >= arithmetic""\r\n            function ""operator>=(const __half &, const __half &)""\r\n            operand types are: c10::Half >= c10::Half\r\n\r\nE:/Program Files/Python35/lib/site-packages/torch/include\\THC/THCNumerics.cuh(194): error: more than one operator ""=="" matches these operands:\r\n            built-in operator ""arithmetic == arithmetic""\r\n            function ""operator==(const __half &, const __half &)""\r\n            operand types are: c10::Half == c10::Half\r\n\r\nE:/Program Files/Python35/lib/site-packages/torch/include\\THC/THCNumerics.cuh(196): error: more than one operator ""!="" matches these operands:\r\n            built-in operator ""arithmetic != arithmetic""\r\n            function ""operator!=(const __half &, const __half &)""\r\n            operand types are: c10::Half != c10::Half', 'Which extension are you trying to compile?', ""I trying to compile the flownet2 project(https://github.com/NVIDIA/flownet2-pytorch). The setup.py as following.\r\n#!/usr/bin/env python3\r\nimport os\r\nimport torch\r\n\r\nfrom setuptools import setup, find_packages\r\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\r\n\r\ncxx_args = ['-std=c++11']\r\n\r\nnvcc_args = [\r\n    '-gencode', 'arch=compute_50,code=sm_50',\r\n    '-gencode', 'arch=compute_52,code=sm_52',\r\n    '-gencode', 'arch=compute_60,code=sm_60',\r\n    '-gencode', 'arch=compute_61,code=sm_61',\r\n    '-gencode', 'arch=compute_70,code=sm_70',\r\n    '-gencode', 'arch=compute_70,code=compute_70'\r\n]\r\n\r\nsetup(\r\n    name='correlation_cuda',\r\n    ext_modules=[\r\n        CUDAExtension('correlation_cuda', [\r\n            'correlation_cuda.cc',\r\n            'correlation_cuda_kernel.cu'\r\n        ], extra_compile_args={'cxx': cxx_args, 'nvcc': nvcc_args})\r\n    ],\r\n    cmdclass={\r\n        'build_ext': BuildExtension\r\n    })\r\n\r\n"", 'Please append `-D__CUDA_NO_HALF_OPERATORS__` to the `nvcc_args`.', '@peterjc123  Do I need to compile pytorch again? I install pytorch by pip Now. \r\nIn cmd  export TORCH_NVCC_FLAGS=""-D__CUDA_NO_HALF_OPERATORS__""\r\npython setup.py install ?', ""No, you don't need to do that. Just change `setup.py` to the following:\r\n```python\r\n#!/usr/bin/env python3\r\nimport os\r\nimport torch\r\n\r\nfrom setuptools import setup, find_packages\r\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\r\n\r\ncxx_args = ['-std=c++11']\r\n\r\nnvcc_args = [\r\n'-gencode', 'arch=compute_50,code=sm_50',\r\n'-gencode', 'arch=compute_52,code=sm_52',\r\n'-gencode', 'arch=compute_60,code=sm_60',\r\n'-gencode', 'arch=compute_61,code=sm_61',\r\n'-gencode', 'arch=compute_70,code=sm_70',\r\n'-gencode', 'arch=compute_70,code=compute_70',\r\n'-D__CUDA_NO_HALF_OPERATORS__' # <-- Just add this line\r\n]\r\n\r\nsetup(\r\nname='correlation_cuda',\r\next_modules=[\r\nCUDAExtension('correlation_cuda', [\r\n'correlation_cuda.cc',\r\n'correlation_cuda_kernel.cu'\r\n], extra_compile_args={'cxx': cxx_args, 'nvcc': nvcc_args})\r\n],\r\ncmdclass={\r\n'build_ext': BuildExtension\r\n})\r\n```"", '@peterjc123  Thanks very much. There are no error. But Many warning still exist. When I run the test xx.py. (The script run right in linux.)\r\nThe prompt as  ImportError: DLL load failed: %1 Not a valid Win32 application. \r\nThe warning contains many different classes. \r\n\r\ntorch\\utils\\cpp_extension.py:184: UserWarning: Error checking compiler version for cl: \'utf-8\' codec can\'t decode byte 0xd3 in position 0: invalid continuation byte\r\n  warnings.warn(\'Error checking compiler version for {}: {}\'.format(compiler, error))\r\nbuilding \'resample2d_cuda\' extension\r\n\r\ntorch\\include\\ATen/core/jit_type.h(227): warning C4251: ‚Äúc10::SingleElementType<c10::TypeKind::OptionalType,c10::OptionalType>::elem‚Äù: class‚Äústd::shared_ptr<c10::Type>‚Äù need the interface struct‚Äúc10::SingleElementType<c10::TypeKind::OptionalType,c10::OptionalType>‚Äù to used\r\n\r\n\r\ntorch/include\\c10/util/typeid.h(580): warning: dllexport/dllimport conflict with ""caffe2::TypeIdentifier::Get [with T=int]""\r\n(81): here; dllexport assumed\r\n\r\n\r\ntorch\\include\\torch/csrc/jit/tracer.h(169): warning C4273: ‚Äútorch::jit::tracer::addInputs‚Äù: dll \r\nInconsistent link', ""The error `ImportError: DLL load failed: %1 Not a valid Win32 application.` usually means that the library is built for the wrong arch. Are you building for Win32? The warnings are safe to ignore and they didn't provide any useful info for debugging the aforementioned error."", '@peterjc123   The full error message when I run the test xx.py. I guess if the compile still exist question?\r\nresample2d_cuda was compiled according to above step.\r\n\r\n File ""D:\\GithubProject\\flownet2-pytorch_10\\models.py"", line 8, in <module>\r\n    from networks.resample2d_package.resample2d import Resample2d\r\n  File ""D:\\GithubProject\\flownet2-pytorch_10\\networks\\resample2d_package\\resample2d.py"", line 3, in <module>\r\n    import resample2d_cuda\r\nImportError: DLL load failed: %1 Not a valid Win32 application.', ""According to this [post](https://stackoverflow.com/questions/14629818/importerror-dll-load-failed-1-is-not-a-valid-win32-application), it is also possible that it is caused by the missing DLLs. You could use https://github.com/lucasg/Dependencies to debug the DLL loading problem. Or you can just paste the full build log here and I'll check what is wrong there."", '@peterjc123  After I python setup.py install the c++ extension. Not any DLL is producted.  \r\nWhen I run python setup.py install  The log as following.\r\n\r\nD:\\GithubProject\\flownet2-pytorch_10\\networks\\resample2d_package>python setup.py install\r\nrunning install\r\nrunning bdist_egg\r\nrunning egg_info\r\nwriting dependency_links to resample2d_cuda.egg-info\\dependency_links.txt\r\nwriting resample2d_cuda.egg-info\\PKG-INFO\r\nwriting top-level names to resample2d_cuda.egg-info\\top_level.txt\r\nreading manifest file \'resample2d_cuda.egg-info\\SOURCES.txt\'\r\nwriting manifest file \'resample2d_cuda.egg-info\\SOURCES.txt\'\r\ninstalling library code to build\\bdist.win-amd64\\egg\r\nrunning install_lib\r\nrunning build_ext\r\nE:\\Program Files\\Python35\\lib\\site-packages\\torch\\utils\\cpp_extension.py:184: UserWarning: Error checking compiler version for cl: \'utf-8\' codec can\'t decode byte 0xd3 in position 0: invalid continuation byte\r\nwarnings.warn(\'Error checking compiler version for {}: {}\'.format(compiler, error))\r\nbuilding \'resample2d_cuda\' extension\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64\\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MT ""-IE:\\Program Files\\Python35\\lib\\site-packages\\torch\\include"" ""-IE:\\Program Files\\Python35\\lib\\site-packages\\torch\\include\\torch\\csrc\\api\\include"" ""-IE:\\Program Files\\Python35\\lib\\site-packages\\torch\\include\\TH"" ""-IE:\\Program Files\\Python35\\lib\\site-packages\\torch\\include\\THC"" ""-IC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\include"" ""-IE:\\Program Files\\Python35\\include"" ""-IE:\\Program Files\\Python35\\include"" ""-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.11.25503\\ATLMFC\\include"" ""-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.11.25503\\include"" ""-IC:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\winrt"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\cppwinrt"" ""-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.11.25503\\ATLMFC\\include"" ""-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.11.25503\\include"" ""-IC:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\winrt"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\cppwinrt"" ""-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.11.25503\\ATLMFC\\include"" ""-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.11.25503\\include"" ""-IC:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\winrt"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\cppwinrt"" /EHsc /Tpresample2d_cuda.cc /Fobuild\\temp.win-amd64-3.5\\Release\\resample2d_cuda.obj -std=c++11 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=resample2d_cuda -D_GLIBCXX_USE_CXX11_ABI=0 /MD\r\ncl: ÂëΩ‰ª§Ë°å warning D9025 :Ê≠£Âú®ÈáçÂÜô‚Äú/MT‚Äù(Áî®‚Äú/MD‚Äù)\r\ncl: ÂëΩ‰ª§Ë°å warning D9002 :ÂøΩÁï•Êú™Áü•ÈÄâÈ°π‚Äú-std=c++11‚Äù\r\nresample2d_cuda.cc\r\nE:\\Program Files\\Python35\\lib\\site-packages\\torch\\include\\c10/util/Exception.h(27): warning C4275: Èùû dll Êé•Âè£ class‚Äústd::exception‚ÄùÁî®‰Ωú dll Êé•Âè£ class‚Äúc10::Error‚ÄùÁöÑÂü∫\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.11.25503\\include\\vcruntime_exception.h(43): note: ÂèÇËßÅ‚Äústd::exception‚ÄùÁöÑÂ£∞Êòé\r\nE:\\Program Files\\Python35\\lib\\site-packages\\torch\\include\\c10/util/Exception.h(27): note: ÂèÇËßÅ‚Äúc10::Error‚ÄùÁöÑÂ£∞Êòé\r\nE:\\Program Files\\Python35\\lib\\site-packages\\torch\\include\\c10/util/Exception.h(28): warning C4251: ‚Äúc10::Error::msg_stack_‚Äù: class‚Äústd::vector<std::string,std::allocator<_Ty>>‚ÄùÈúÄË¶ÅÊúâ dll Êé•Âè£Áî± class‚Äúc10::Error‚ÄùÁöÑÂÆ¢Êà∑Á´Ø‰ΩøÁî®\r\nwith\r\n[\r\n_Ty=std::string\r\n]\r\nE:\\Program Files\\Python35\\lib\\site-packages\\torch\\include\\c10/util/Exception.h(28): note: ÂèÇËßÅ‚Äústd::vector<std::string,std::allocator<_Ty>>‚ÄùÁöÑÂ£∞Êòé\r\nwith\r\n[\r\n_Ty=std::string\r\n]\r\n\r\nE:\\Program Files\\Python35\\lib\\site-packages\\torch\\include\\torch\\csrc\\api\\include\\torch/optim/rmsprop.h(34): warning C4275: Èùû dll Êé•Âè£ class‚Äútorch::optim::Optimizer‚ÄùÁî®‰Ωú dll Êé•Âè£ class‚Äútorch::optim::RMSprop‚ÄùÁöÑÂü∫\r\nE:\\Program Files\\Python35\\lib\\site-packages\\torch\\include\\torch\\csrc\\api\\include\\torch/optim/optimizer.h(100): note: ÂèÇËßÅ‚Äútorch::optim::Optimizer‚ÄùÁöÑÂ£∞Êòé\r\nE:\\Program Files\\Python35\\lib\\site-packages\\torch\\include\\torch\\csrc\\api\\include\\torch/optim/rmsprop.h(34): note: ÂèÇËßÅ‚Äútorch::optim::RMSprop‚ÄùÁöÑÂ£∞Êòé\r\nE:\\Program Files\\Python35\\lib\\site-packages\\torch\\include\\torch\\csrc\\api\\include\\torch/optim/rmsprop.h(50): warning C4251: ‚Äútorch::optim::RMSprop::square_average_buffers‚Äù: class‚Äústd::vector<at::Tensor,std::allocator<_Ty>>‚ÄùÈúÄË¶ÅÊúâ dll Êé•Âè£Áî± class‚Äútorch::optim::RMSprop‚ÄùÁöÑÂÆ¢Êà∑Á´Ø‰ΩøÁî®\r\nwith\r\n[\r\n_Ty=at::Tensor\r\n]\r\n\r\nE:\\Program Files\\Python35\\lib\\site-packages\\torch\\include\\c10/core/Scalar.h(75): note: ÂèÇËßÅÂØπÊ≠£Âú®ÁºñËØëÁöÑÂáΩÊï∞ Ê®°Êùø ÂÆû‰æãÂåñ‚ÄúTo c10::checked_convert<c10::Half,std::complex>(From,const char *)‚ÄùÁöÑÂºïÁî®\r\nwith\r\n[\r\nTo=c10::Half,\r\nFrom=std::complex\r\n]\r\nE:\\Program Files\\Python35\\lib\\site-packages\\torch\\include\\c10/util/Half.h(400): warning C4244: ‚ÄúÂèÇÊï∞‚Äù: ‰ªé‚Äúdouble‚ÄùËΩ¨Êç¢Âà∞‚Äúfloat‚ÄùÔºåÂèØËÉΩ‰∏¢Â§±Êï∞ÊçÆ\r\nE:\\Program Files\\Python35\\lib\\site-packages\\torch\\include\\c10/util/Half.h(399): note: ÁºñËØë Á±ª Ê®°Êùø ÊàêÂëòÂáΩÊï∞ ""c10::Half c10::Converter<To,From,void>::operator ()(From)"" Êó∂\r\nwith\r\n[\r\nTo=c10::Half,\r\nFrom=double\r\n]\r\nam Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin\\nvcc.exe -c resample2d_kernel.cu -o build\\temp.win-amd64-3.5\\Release\\resample2d_kernel.obj -Xcompiler /wd4819 -Xcompiler /MD ""-IE:\\Program Files\\Python35\\lib\\site-packages\\torch\\include"" ""-IE:\\Program Files\\Python35\\lib\\site-packages\\torch\\include\\torch\\csrc\\api\\include"" ""-IE:\\Program Files\\Python35\\lib\\site-packages\\torch\\include\\TH"" ""-IE:\\Program Files\\Python35\\lib\\site-packages\\torch\\include\\THC"" ""-IC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\include"" ""-IE:\\Program Files\\Python35\\include"" ""-IE:\\Program Files\\Python35\\include"" ""-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.11.25503\\ATLMFC\\include"" ""-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.11.25503\\include"" ""-IC:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\winrt"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\cppwinrt"" ""-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.11.25503\\ATLMFC\\include"" ""-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.11.25503\\include"" ""-IC:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\winrt"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\cppwinrt"" ""-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.11.25503\\ATLMFC\\include"" ""-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.11.25503\\include"" ""-IC:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\winrt"" ""-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\cppwinrt"" -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_70,code=compute_70 -D__CUDA_NO_HALF_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=resample2d_cuda -D_GLIBCXX_USE_CXX11_ABI=0\r\nresample2d_kernel.cu\r\n\r\nE:/Program Files/Python35/lib/site-packages/torch/include\\c10/util/typeid.h(597): warning: dllexport/dllimport conflict with ""caffe2::TypeIdentifier::Get [with T=std::vector<int64_t, std::allocator<int64_t>>]""\r\n(81): here; dllexport assumed\r\n\r\nE:/Program Files/Python35/lib/site-packages/torch/include\\c10/cuda/CUDAStream.h(171): warning: field of class type without a DLL interface used in a class with a DLL interface\r\n\r\nE:/Program Files/Python35/lib/site-packages/torch/include\\c10/util/typeid.h(600): warning: dllexport/dllimport conflict with ""caffe2::TypeIdentifier::Get [with T=char *]""\r\n(81): here; dllexport assumed\r\n\r\nE:/Program Files/Python35/lib/site-packages/torch/include\\c10/core/TensorTypeIdRegistration.h(46): warning: field of class type without a DLL interface used in a class with a DLL interface\r\n\r\nE:/Program Files/Python35/lib/site-packages/torch/include\\c10/util/typeid.h(628): warning: dllexport/dllimport conflict with ""caffe2::TypeIdentifier::Get [with T=caffe2::_CaffeHighestPreallocatedTypeId]""\r\n(81): here; dllexport assumed\r\n\r\nE:/Program Files/Python35/lib/site-packages/torch/include\\ATen/TensorGeometry.h(57): warning: field of class type without a DLL interface used in a class with a DLL interface\r\n\r\nE:/Program Files/Python35/lib/site-packages/torch/include\\c10/cuda/CUDAStream.h(171): warning: field of class type without a DLL interface used in a class with a DLL interface\r\n\r\ncreating D:\\GithubProject\\flownet2-pytorch_10\\networks\\resample2d_package\\build\\lib.win-amd64-3.5\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64\\link.exe /nologo /INCREMENTAL:NO /LTCG /nodefaultlib:libucrt.lib ucrt.lib /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO ""/LIBPATH:E:\\Program Files\\Python35\\lib\\site-packages\\torch\\lib"" ""/LIBPATH:C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\lib/x64"" ""/LIBPATH:E:\\Program Files\\Python35\\libs"" ""/LIBPATH:E:\\Program Files\\Python35\\PCbuild\\amd64"" ""/LIBPATH:C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.11.25503\\ATLMFC\\lib\\x64"" ""/LIBPATH:C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.11.25503\\lib\\x64"" ""/LIBPATH:C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64"" ""/LIBPATH:C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\ucrt\\x64"" ""/LIBPATH:C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\um\\x64"" ""/LIBPATH:C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.11.25503\\ATLMFC\\lib\\x64"" ""/LIBPATH:C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.11.25503\\lib\\x64"" ""/LIBPATH:C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64"" ""/LIBPATH:C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\ucrt\\x64"" ""/LIBPATH:C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\um\\x64"" ""/LIBPATH:C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.11.25503\\ATLMFC\\lib\\x64"" ""/LIBPATH:C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.11.25503\\lib\\x64"" ""/LIBPATH:C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64"" ""/LIBPATH:C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\ucrt\\x64"" ""/LIBPATH:C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\um\\x64"" cudart.lib c10.lib c10_cuda.lib caffe2.lib torch.lib torch_python.lib caffe2_gpu.lib _C.lib /EXPORT:PyInit_resample2d_cuda build\\temp.win-amd64-3.5\\Release\\resample2d_cuda.obj build\\temp.win-amd64-3.5\\Release\\resample2d_kernel.obj /OUT:build\\lib.win-amd64-3.5\\resample2d_cuda.cp35-win_amd64.pyd /IMPLIB:build\\temp.win-amd64-3.5\\Release\\resample2d_cuda.cp35-win_amd64.lib\r\nÊ≠£Âú®ÂàõÂª∫Â∫ì build\\temp.win-amd64-3.5\\Release\\resample2d_cuda.cp35-win_amd64.lib ÂíåÂØπË±° build\\temp.win-amd64-3.5\\Release\\resample2d_cuda.cp35-win_amd64.exp\r\nÊ≠£Âú®ÁîüÊàê‰ª£Á†Å\r\nÂ∑≤ÂÆåÊàê‰ª£Á†ÅÁöÑÁîüÊàê\r\ncreating build\\bdist.win-amd64\r\ncreating build\\bdist.win-amd64\\egg\r\ncopying build\\lib.win-amd64-3.5\\resample2d_cuda.cp35-win_amd64.pyd -> build\\bdist.win-amd64\\egg\r\ncreating stub loader for resample2d_cuda.cp35-win_amd64.pyd\r\nbyte-compiling build\\bdist.win-amd64\\egg\\resample2d_cuda.py to resample2d_cuda.cpython-35.pyc\r\ncreating build\\bdist.win-amd64\\egg\\EGG-INFO\r\ncopying resample2d_cuda.egg-info\\PKG-INFO -> build\\bdist.win-amd64\\egg\\EGG-INFO\r\ncopying resample2d_cuda.egg-info\\SOURCES.txt -> build\\bdist.win-amd64\\egg\\EGG-INFO\r\ncopying resample2d_cuda.egg-info\\dependency_links.txt -> build\\bdist.win-amd64\\egg\\EGG-INFO\r\ncopying resample2d_cuda.egg-info\\top_level.txt -> build\\bdist.win-amd64\\egg\\EGG-INFO\r\nwriting build\\bdist.win-amd64\\egg\\EGG-INFO\\native_libs.txt\r\nzip_safe flag not set; analyzing archive contents...\r\npycache.resample2d_cuda.cpython-35: module references file\r\ncreating dist\r\ncreating \'dist\\resample2d_cuda-0.0.0-py3.5-win-amd64.egg\' and adding \'build\\bdist.win-amd64\\egg\' to it\r\nremoving \'build\\bdist.win-amd64\\egg\' (and everything under it)\r\nProcessing resample2d_cuda-0.0.0-py3.5-win-amd64.egg\r\nremoving \'e:\\program files\\python35\\lib\\site-packages\\resample2d_cuda-0.0.0-py3.5-win-amd64.egg\' (and everything under it)\r\ncreating e:\\program files\\python35\\lib\\site-packages\\resample2d_cuda-0.0.0-py3.5-wQin-amd64.egg\r\nExtracting resample2d_cuda-0.0.0-py3.5-win-amd64.egg to e:\\program files\\python35\\lib\\site-packages\r\nresample2d-cuda 0.0.0 is already the active version in easy-install.pth\r\n\r\nInstalled e:\\program files\\python35\\lib\\site-packages\\resample2d_cuda-0.0.0-py3.5-win-amd64.egg\r\nProcessing dependencies for resample2d-cuda==0.0.0\r\nFinished processing dependencies for resample2d-cuda==0.0.0\r\nFinished processing dependencies for resample2d-cuda==0.0.0', '`.PYD` is the equivalent of `.DLL` on Windows.', '@peterjc123 Thanks very much.\r\n\r\nThere are many dll were not found.  In picture1 we can copy the dll to the run path. Another dlls seem that microsoft dll. Do you know them?\r\n\r\n![image](https://user-images.githubusercontent.com/9691664/58170859-328dca00-7cc7-11e9-93e0-258afdf01411.png)\r\n![image](https://user-images.githubusercontent.com/9691664/58170885-3e798c00-7cc7-11e9-8470-18d72dc8cec4.png)\r\n\r\n', 'Did you open the generated PYD file in `Depends.exe`? Why does it rely on these unrelated DLLs?', ""@peterjc123 Yes, step one I set options->Properties->Tree build behavious (**ChildOnly**) in Denpends.exe. I found that don't find the following dll.\r\n![image](https://user-images.githubusercontent.com/9691664/58172870-59023400-7ccc-11e9-8183-d5d1737785f4.png)\r\n\r\nI  copy the dll to the same path  of xxx .pyd. But the issue still exist.( ImportError: DLL load failed: %1 Not a valid Win32 application.)\r\n\r\nstep two. I  set options->Properties->Tree build behavious (**Recursive**). The ext-ms-xxx.dll don't be found.\r\n"", 'I cannot reproduce this issue. That\'s what I do this time:\r\n```cmd\r\ncd networks\\resample2d_package\r\nfor /f ""usebackq tokens=*"" %i in (`""%ProgramFiles(x86)%\\Microsoft Visual Studio\\Installer\\vswhere.exe"" -version [15^,16^) -products * -latest -property installationPath`) do call ""%i\\VC\\Auxiliary\\Build\\vcvarsall.bat"" x64 -vcvars_ver=14.11\r\nset DISTUTILS_USE_SDK=1\r\npython setup.py build\r\ncd build\\lib.win-amd64-3.6 :: for you it should be 3.5\r\npython -c ""import torch;import resample2d_cuda""\r\n```', 'Could you please remove the generated c++ extension in python root and try rebuilding it again?', ""I'm going to close this one since it's not a bug. You may reopen it if you disagree."", ""@peterjc123 Thanks for your helps.  Yes, I didn't produce any issues according your script.\r\n![image](https://user-images.githubusercontent.com/9691664/58175125-062b7b00-7cd2-11e9-9085-80b53acd868f.png)\r\n\r\nI also rebuilding the project after removing the all generated c++ extension in E:\\Program Files\\Python35\\Lib\\site-packages and the network/resample2d_cuda/build of the project.\r\n\r\nBut the issue still exist."", 'In the end, I resolve the question by reinstall vs2015 and uninstall the visual c++ 2017 Redistributable.', '> `.PYD` is the equivalent of `.DLL` on Windows.\r\n\r\nThank you for your answer, it solves a big problem for me!\r\nbest wishes! ', '> `.PYD` is the equivalent of `.DLL` on Windows.\r\n@peterjc123 @ys0823 @tjusxh\r\nI compiled successfully, but there are a lot of warnings, such as ""warning: field of class type without a DLL interface used in a class with a DLL interface"".\r\nGenerated a .PYD file. When ‚Äò‚Äôimport resample2d_cuda‚Äò‚Äô, an error occurred, ‚ÄúImportError: DLL load failed: The specified module could not be found.‚Äù\r\n\r\ncould you tell me how to solve the problem? thanks!\r\npytorch 1.0.1\r\ncuda 9.0\r\npython 3.6\r\nvs 2015\r\nwindows10', '@JiangYi0311 Use something like https://github.com/lucasg/Dependencies to find out what DLLs it depends on.']",[],[],0,0
732,pytorch,7496,closed,Train a model using multiple GPUs ,"## Issue Description
I tried to train my model on multiple gpus. However, when I launch the program, it seems that the model is allocated to gpus, but no data is fed into model. Using nvidia-smi, i find hundreds of MB of memory is consumed on each gpu. I guess these memory usage is for model initialization in each gpu.

I am sharing 8 gpus with others on the server, so I limit my program on GPU 2 and GPU 3 by following command.


## Code example
I ran this official tutorial on my machine and the same thing happens again. https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html

## System Info
PyTorch version: 0.4.0
Is debug build: No
CUDA used to build PyTorch: 8.0.61

OS: Ubuntu 16.04.3 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 7.5.17
GPU models and configuration:
GPU 0: GeForce GTX TITAN X
GPU 1: GeForce GTX TITAN X
GPU 2: GeForce GTX TITAN X
GPU 3: GeForce GTX TITAN X
GPU 4: GeForce GTX TITAN X
GPU 5: GeForce GTX TITAN X
GPU 6: GeForce GTX TITAN X
GPU 7: GeForce GTX TITAN X

Nvidia driver version: 375.88
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.10
/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.20
/usr/lib/x86_64-linux-gnu/libcudnn_static_v5.a
/usr/lib/x86_64-linux-gnu/libcudnn_static_v6.a
/usr/local/lib/python2.7/dist-packages/torch/lib/libcudnn.so.6

Versions of relevant libraries:
[pip3] msgpack-numpy (0.4.1)
[pip3] numpy (1.14.0)
[pip3] numpydoc (0.7.0)
[pip3] torch (0.4.0)
[pip3] torchtext (0.2.3)
[pip3] torchvision (0.2.1)
[conda] torch                     0.4.0                     <pip>
[conda] torchtext                 0.2.3                     <pip>
[conda] torchvision               0.2.1                     <pip>",awaiting response (this tag is deprecated),"['a piece of information that might be helpful.\r\n\r\nI just found that \r\n```\r\nCUDA used to build PyTorch: 8.0.61\r\nCUDA runtime version: 7.5.17\r\n```', 'Sorry what exactly is the issue? How do you tell that no data is fed into he model? Can you share a small self contained code snippet that would let us reproduce your issue?', '@apaszke \r\nI should have closed this issue.\r\n\r\nThe same issue as [this one](https://github.com/pytorch/pytorch/issues/1637#issuecomment-338268158)']","[""python\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '2,3'\r\n""]","['', '']",0,0
733,pytorch,24869,closed,Dictionary in C++,"## üöÄ Feature
Implement a dictionary ops in C++

## Motivation

The dictionary ops will replace the current vocab class in torchtext (or serve as a new class for the same purpose). It could also be applied for transforming audio script. I think a dictionary ops in pytorch core library makes it easier to maintain.

## Pitch

Some baselines should be included:
+ findWord()
+ addWord()
+ count_nwords()
+ count_ntokens()
+ count_nlables()
+ getWord()
+ readFromSequence()
+ readFromFile()
",enhancement module: internals triaged,"['why exactly is this filed against `pytorch/pytorch` instead of `pytorch/text`?', '@soumith since the dictionary with hashing trick could also be applied for audio script, I think torchaudio could also use it. But if only text people want it, I will file it in `pytorch/text`.', 'I feel like this issue is missing a bunch of context -- what\'s the ""hashing trick"" and how would the vocab-class be a general purpose solution or just something for torchaudio and torchtext to share?']",[],[],0,0
734,pytorch,10858,closed,unexpected behaviour when mixing no_grad decorator with no_grad context manager,"## Issue description

Mixing torch.no_grad decorator with torch.no_grad context re-enables gradients as per torch.is_grad_enabled.

## Code example


#### Output:
> False
> True

### Expectation

It would be my expectation that within the no_grad context the gradients would always be disabled.
This expectation matches the output when using nested contexts as below.


#### Output:
> False
> False

## System Info

PyTorch version: 0.4.1
Is debug build: No
CUDA used to build PyTorch: 9.2.148

OS: Ubuntu 16.04.5 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: GeForce GTX 1080 Ti
Nvidia driver version: 396.54
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] Could not collect
[conda] cuda92                    1.0                           0    pytorch
[conda] pytorch                   0.4.1           py36_cuda9.2.148_cudnn7.1.4_1  [cuda92]  pytorch
[conda] torchvision               0.2.1                    py36_1    pytorch
[conda] warpctc-pytorch           0.1                       <pip>",medium priority (this tag is deprecated),[],"['\r\nimport torch\r\n\r\n@torch.no_grad()\r\ndef nothing(x):\r\n    return x\r\n\r\ntestin = torch.Tensor([0])\r\nwith torch.no_grad():\r\n    print(torch.is_grad_enabled())\r\n    testout = nothing(testin)\r\n    print(torch.is_grad_enabled())\r\n', '\r\nimport torch\r\n\r\ndef nothing_no_nograd(x):\r\n  return x\r\n\r\ntestin = torch.Tensor([0])\r\nwith torch.no_grad():\r\n    print(torch.is_grad_enabled())\r\n    with torch.no_grad():\r\n        testout = nothing_no_nograd(testin)\r\n    print(torch.is_grad_enabled())\r\n']",[],0,0
735,pytorch,8502,closed,[JIT] Don't support None in the script,"Operator-level tests fail because we're using  in the script. Example failure:



Affected tests:

- 
- 
- 
- ",oncall: jit,"['Discussed with @wanchaol in person. These steps are needed to fix clamp. It is currently special-cased on Python:\r\n\r\n1. Change THTensor_(clamp) to check for nan in min or max, and call cminValue or cmaxValue instead.\r\n2. Change THCTensor_(clamp) to do the same\r\n3. Fix derivatives.yaml to check for NaN and call the right gradient variant\r\n4. Remove special handling of clamp in python_torch_functions.h\r\n5. The rest of the task.', 'What about clamp on integral types? There are no NaNs. Are we just always going to use the two-sided kernel?', 'I\'d also much rather use `inf` than `nan` to represent ""no boundary""', 'Hi, is this being worked on?  Else I can take a look.', '@jramseyer Hi Jenny, yes I am working on the fix, will send the PR out soon. ']","['\r\n======================================================================\r\nERROR: test_clamp_max (__main__.TestAutogradGenerated)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""test/test_autograd.py"", line 3665, in do_test\r\n    check(name)\r\n  File ""test/test_autograd.py"", line 3592, in check\r\n    fn, (self_variable,) + args_variable)\r\n  File ""test/test_autograd.py"", line 3498, in check_against_reference\r\n    outputs_test = func(*nograd_inputs)\r\n  File ""test/test_autograd.py"", line 3473, in script_fn\r\n    CU = torch.jit.CompilationUnit(script)\r\n  File ""/data/users/zdevito/pytorch/torch/jit/__init__.py"", line 352, in __init__\r\n    self.define(lang, _frames_up=_frames_up + 1)\r\n  File ""/data/users/zdevito/pytorch/torch/jit/__init__.py"", line 358, in define\r\n    self.module._define(lang, rcb, False)\r\nRuntimeError: \r\nundefined value None:\r\n\r\ndef the_method(i0):\r\n    return i0.clamp(0.5, None)\r\n                         ~~~~ <--- HERE\r\n']","['None', 'test_clamp_max', 'test_clamp_max_scalar', 'test_clamp_min', 'test_clamp_min_scalar']",0,0
736,pytorch,30375,closed,Distributed Using Gloo on Multiple Nodes Does not Work,"## üêõ Bug

I am trying to run a distributed deep learning workload on multiple nodes using Gloo. But unfortunately I receive the error below:



This is a two node environment with two VMs. Each of these VMs have 16 GB of RAM and 8 cores.
<!-- A clear and concise description of what the bug is. -->

## To Reproduce

The code is very similar to this link: https://pytorch.org/tutorials/intermediate/dist_tuto.html

The only difference is in the initiation phase:



<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

The expected behavior is not seeing the error code and the distributed training process kicking off.

## Environment

PyTorch version: 1.4.0.dev20191122+cpu
Is debug build: No
CUDA used to build PyTorch: None

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: Could not collect

Python version: 3.6
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip3] numpy==1.17.4
[pip3] torch==1.4.0.dev20191122+cpu
[pip3] torchvision==0.5.0.dev20191122+cpu
[conda] Could not collect


cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528",module: third_party oncall: distributed triaged,"['@Tabrizian What is the `worker_id` that you use on each of the two nodes that you run the script on?', '@pritamdamania87 For one is 0 and for another one is 1.', 'I think that would create problems with the ranks. For worker_id 0, the ranks would be 0 and 1 and for worker_id 1 the ranks would be 1 and 2. Can you try with worker ids 0 and 2? Your ranks should be 0, 1, 2, 3 for world size 4.', '@pritamdamania87 I see. Sorry it was my mistake. Thank you for your help!']","[""\r\nterminate called after throwing an instance of 'gloo::EnforceNotMet'\r\n  what():  [enforce fail at /pytorch/third_party/gloo/gloo/transport/tcp/device.cc:281] rv != -1. -1\r\n"", 'python\r\ndef init_process(rank, size, fn, backend=\'gloo\'):\r\n    """""" Initialize the distributed environment. """"""\r\n    os.environ[\'MASTER_ADDR\'] = \'10.11.13.39\'\r\n    os.environ[\'MASTER_PORT\'] = \'8888\'\r\n    os.environ[\'WORLD_SIZE\'] = \'4\'\r\n    dist.init_process_group(backend, rank=rank, world_size=4)\r\n    fn(rank, size)\r\n\r\nsize = 2\r\nworker_id = int(sys.argv[1])\r\nprocesses = []\r\nfor rank in range(size):\r\n    p = Process(target=init_process, args=(rank + worker_id, size, run))\r\n    p.start()\r\n    processes.append(p)\r\n\r\nfor p in processes:\r\n    p.join()\r\n']",[],0,0
737,pytorch,9527,closed,AttributeError: Method AffineChannelNd is not a registered operator. Did you mean: [AffineChannel],"Traceback (most recent call last):
File ""tools/test_net.py"", line 151, in
main(ind_range=args.range, multi_gpu_testing=False)
File ""tools/test_net.py"", line 113, in main
engine.test_net_on_dataset(multi_gpu=multi_gpu_testing)
File ""/home/chandu/workspace/tracking/DaT/lib/core/test_engine.py"", line 321, in test_net_on_dataset
all_boxes, all_segms, all_keyps = test_net()
File ""/home/chandu/workspace/tracking/DaT/lib/core/test_engine.py"", line 135, in test_net
model = initialize_model_from_cfg()
File ""/home/chandu/workspace/tracking/DaT/lib/core/test_engine.py"", line 59, in initialize_model_from_cfg
init_params=cfg.TEST.INIT_RANDOM_VARS_BEFORE_LOADING)
File ""/home/chandu/workspace/tracking/DaT/lib/modeling/model_builder.py"", line 63, in create
return get_func(model_name)(init_model(model_name, train, init_params))
File ""/home/chandu/workspace/tracking/DaT/lib/modeling/model_builder.py"", line 156, in keypoint_rcnn
add_roi_keypoint_head_func=get_func(cfg.KRCNN.ROI_KEYPOINTS_HEAD))
File ""/home/chandu/workspace/tracking/DaT/lib/modeling/model_builder.py"", line 307, in build_generic_fast_rcnn_model
build_data_parallel_model(model, _single_gpu_build_func)
File ""/home/chandu/workspace/tracking/DaT/lib/modeling/model_builder.py"", line 953, in build_data_parallel_model
single_gpu_build_func(model)
File ""/home/chandu/workspace/tracking/DaT/lib/modeling/model_builder.py"", line 199, in _single_gpu_build_func
blob_conv, dim_conv, spatial_scale_conv = add_conv_body_func(model)
File ""/home/chandu/workspace/tracking/DaT/lib/modeling/FPN3D.py"", line 51, in add_fpn_ResNet101_conv5_body
ResNet.stage_info_ResNet101_conv5)
File ""/home/chandu/workspace/tracking/DaT/lib/modeling/FPN3D.py"", line 98, in add_fpn_generic_onto_body
conv_body_func(model)
File ""/home/chandu/workspace/tracking/DaT/lib/modeling/ResNet3D.py"", line 389, in add_ResNet101_conv5_body
return add_ResNet_convX_body(model, (3, 4, 23, 3), freeze_at=2)
File ""/home/chandu/workspace/tracking/DaT/lib/modeling/ResNet3D.py"", line 262, in add_ResNet_convX_body
p = model.AffineChannelNd(p, 'res_conv1_bn', dim_out=feat_dims[0], inplace=True)
File ""/home/chandu/workspace/tracking/DaT/lib/modeling/detector.py"", line 94, in AffineChannelNd
if cfg.MODEL.USE_BN:
File ""/home/chandu/workspace/tracking/anaconda2/envs/gtracking/lib/python2.7/site-packages/caffe2/python/core.py"", line 2082, in getattr
"","".join(workspace.C.nearby_opnames(op_type)) + ']'
AttributeError: Method AffineChannelNd is not a registered operator. Did you mean: [AffineChannel]

**Why am I getting this error?**",,"['Could you please fill out the issue template so that we have more information to work with? In particular, a code sample would be helpful.', 'I have resolved this issue by following the instructions in the installation process.', '> I have resolved this issue by following the instructions in the installation process.\r\n\r\ncan you help meÔºüsame errors']",[],[],0,0
738,pytorch,1113,closed,Possibly a bug: Why the default_collate in dataloader will change the type of the data?,"I have a float train data. When I send it into dataloader and get it with enumerate. I found the type of my data has been changed into torch.DoubleTensor. But I really need is my original type. I found this was caused in `torch/utils/data/dataloader.py' in the definition of the default_collate. Is this a bug? Or how could I make it keep my original datatype?


",medium priority (this tag is deprecated),"['What is your original data type?', ""Python's `float` number type has Double precision, and there's no float32 in python numbers.\r\nHence, we are doing exactly what we should without losing precision.\r\n\r\nIf we detect that the incoming data is a Tensor, we already retain it's correct type in the section: `if torch.is_tensor(batch[0]):`"", ""@soumith Actually, I use np.array(..., dtype = 'float32')"", 'This is expected. See [this line](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py#L67-L68). If you need to collate the batches in a different way you can always supply your own collate_fn.', ""Sorry I didn't notice that the type of the array has been changed."", 'fixed in master. will be in next release.']","['\r\ndef default_collate(batch):\r\n    ""Puts each data field into a tensor with outer dimension batch size""\r\n    if torch.is_tensor(batch[0]):\r\n        return torch.cat([t.unsqueeze(0) for t in batch], 0)\r\n    elif isinstance(batch[0], int):\r\n        return torch.LongTensor(batch)\r\n    elif isinstance(batch[0], float):\r\n        return torch.DoubleTensor(batch)\r\n    elif isinstance(batch[0], str):\r\n        return batch\r\n\r\n']",[],0,0
739,pytorch,422,closed,get rid of requiring to do super.__init__ in nn.Container subclasses,"This seems like its something that users'll forget to do, and we might get a lot of subtle bug reports because of this. Seems like a good usability fix early on.",medium priority (this tag is deprecated),"[""Right now if you try to assign any module or parameter to a `nn.Module` without calling `super().__init__()` it will raise a clear error. Since I can't think of any cases when you don't do this in `__init__` I'm closing this issue.""]",[],[],0,0
740,pytorch,6655,closed,cannot load forum,"cannot load forum,  the error code is ERR_TIMED_OUT
I get 300ms when ping discuss.pytorch.org
I use Chrome on Win10-64
",,"['it seems to be online. are you behind great firewall?', ""I tried to turn off the firewall but it didn't work\r\nI can log in any other official websites of Pytorch except the forum\r\nthanks for your reply @soumith "", '@lesliejackson I think @soumith meant that the one set by the government rather than the one on your machine.. Forum is known to not loadable in China..']",[],[],0,0
741,pytorch,6384,closed,[Caffe2] [PyTorch] PyTorch to Caffe2 and Mobile using ONNX Tutorial: Segmentation Fault,"Hello, 

I am trying to run [Transfering a model from PyTorch to Caffe2 and Mobile using ONNX](https://github.com/onnx/tutorials/blob/master/tutorials/PytorchCaffe2SuperResolution.ipynb) tutorial. However, I get segmentation fault error.

I think the segmentation error is triggered by the second part of the tutorial where loading ONNX model into Caffe2. While searching for what might cause this error I noticed that this tutorial has 2 different versions: one in Onnx Github Tutorial and the other in [PyTorch](http://pytorch.org/tutorials/advanced/super_resolution_with_caffe2.html) website. The onnx to caffe2 import for the two tutorials were different for the second part along with the implementation. One uses : import caffe2.python.onnx.backend and other import onnx_caffe2.backend. I tried with both tutorial but I get the same segmentation fault in both. Also, when I comment the onnx to caffe2 tutorial and the related import lines, the first part Pytorch to ONNX seems to work without any error.

Below, I will describe how I create my virtual env and the results from gdb after getting the Segmentation Fault:

Dependencies:
cuda/8.0 
cudnn/v6.0
opencv/3.4.1
nccl/2.0.5
caffe2/2018-03-02 


To run the code I used the following command (with 6GB RAM, 1 GPU with 6GB): 


Also, I upgraded the numpy and pyyaml versioons with:


Otherwise, I get:


After upgrading the numpy version, when I run the code I get the Segmentation Fault:


I tried to run the code in gdb as well to see where exactly the fault occurs:




In both tutorial, the segmantation fault occurs in the same place. I don't know how to fix this error. I didn't change anything in the code. I am not sure why I am getting this error. 

I really appreciate if you can help me to fix it.


**Note: I already opened an [issue ](https://github.com/onnx/tutorials/issues/29) on onnx repo but I want to cross-reference the issue since I noticed that I get the same error in [PyTorch Tutorial](http://pytorch.org/tutorials/advanced/super_resolution_with_caffe2.html) as well.** 





",,[],"['\r\n#Virtual env creation:\r\n--mem 6G virtualenv --system-site-packages ~/workspace/p_c2_onnx/pc2_onnx \r\n source ~/workspace/p_c2_onnx/pc2_onnx/bin/activate\r\n\r\n#install and check caffe2\r\npip install -r /usr/local/opt/caffe2-2018-03-02/requirements.txt \r\nsrun --mem 2G --gres=gpu:1 python -c \'from caffe2.python import core\'\r\n\r\n#install pytorch and  torchvision\r\npip install http://download.pytorch.org/whl/cu80/torch-0.3.1-cp27-cp27mu-linux_x86_64.whl \r\npip install torchvision \r\n\r\n#install onnx and check\r\n--mem 6G pip install onnx\r\n--mem 6G --gres=gpu:1 python -c ""import onnx""\r\n\r\n#(The below is used only for the tutorial given in PyTorch)\r\n#install onnx-caffe2\r\npip install onnx-caffe2\r\n', '--mem 6GB --gres=gpu:1,gmem:6G python SuperResolution.py', '\r\npip install numpy --upgrade\r\npip install pyyaml --upgrade\r\n', 'error: RuntimeError: module compiled against API version 0xa but this version of numpy is 0x9', '\r\nExported model has been executed on Caffe2 backend, and the result looks good!\r\nsrun: error: c6: task 0: Segmentation fault\r\n', '\r\n(gdb) run SuperResolution_onnxgit.py \r\nStarting program: /imatge/gcamli/workspace/p_c2_onnx/onnx_2/bin/python SuperResolution_onnxgit.py\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".\r\n[New Thread 0x7ffff4033700 (LWP 32594)]\r\n[New Thread 0x7ffff3832700 (LWP 32595)]\r\n[New Thread 0x7ffff1031700 (LWP 32596)]\r\n[New Thread 0x7fffee830700 (LWP 32597)]\r\n[New Thread 0x7fffec02f700 (LWP 32598)]\r\n[New Thread 0x7fffeb82e700 (LWP 32599)]\r\n[New Thread 0x7fffeb02d700 (LWP 32600)]\r\n[New Thread 0x7fffea82c700 (LWP 32601)]\r\n[New Thread 0x7fffe802b700 (LWP 32602)]\r\n[New Thread 0x7fffe382a700 (LWP 32603)]\r\n[New Thread 0x7fffe1029700 (LWP 32604)]\r\n[New Thread 0x7fffde828700 (LWP 32605)]\r\n[New Thread 0x7fffdc027700 (LWP 32606)]\r\n[New Thread 0x7fffd9826700 (LWP 32607)]\r\n[New Thread 0x7fffd7025700 (LWP 32608)]\r\n[New Thread 0x7fffd0824700 (LWP 32611)]\r\n[New Thread 0x7fffce023700 (LWP 32614)]\r\n[New Thread 0x7fffcb822700 (LWP 32617)]\r\n[New Thread 0x7fffc9021700 (LWP 32618)]\r\n[New Thread 0x7fffc6820700 (LWP 32621)]\r\n[New Thread 0x7fffc401f700 (LWP 32623)]\r\n[New Thread 0x7fffc181e700 (LWP 32624)]\r\n[New Thread 0x7fffbf01d700 (LWP 32625)]\r\n[New Thread 0x7fffbc81c700 (LWP 32626)]\r\n[New Thread 0x7fffba01b700 (LWP 32627)]\r\n[New Thread 0x7fffb781a700 (LWP 32628)]\r\n[New Thread 0x7fffb5019700 (LWP 32629)]\r\n[New Thread 0x7fffb2818700 (LWP 32630)]\r\n[New Thread 0x7fffb0017700 (LWP 32631)]\r\n[New Thread 0x7fffad816700 (LWP 32632)]\r\n[New Thread 0x7fffab015700 (LWP 32633)]\r\n[Thread 0x7ffff4033700 (LWP 32594) exited]\r\n[Thread 0x7ffff3832700 (LWP 32595) exited]\r\n[Thread 0x7ffff1031700 (LWP 32596) exited]\r\n[Thread 0x7fffee830700 (LWP 32597) exited]\r\n[Thread 0x7fffec02f700 (LWP 32598) exited]\r\n[Thread 0x7fffeb82e700 (LWP 32599) exited]\r\n[Thread 0x7fffeb02d700 (LWP 32600) exited]\r\n[Thread 0x7fffea82c700 (LWP 32601) exited]\r\n[Thread 0x7fffe802b700 (LWP 32602) exited]\r\n[Thread 0x7fffe382a700 (LWP 32603) exited]\r\n[Thread 0x7fffe1029700 (LWP 32604) exited]\r\n[Thread 0x7fffde828700 (LWP 32605) exited]\r\n[Thread 0x7fffdc027700 (LWP 32606) exited]\r\n[Thread 0x7fffd9826700 (LWP 32607) exited]\r\n[Thread 0x7fffd7025700 (LWP 32608) exited]\r\n[Thread 0x7fffd0824700 (LWP 32611) exited]\r\n[Thread 0x7fffce023700 (LWP 32614) exited]\r\n[Thread 0x7fffcb822700 (LWP 32617) exited]\r\n[Thread 0x7fffc9021700 (LWP 32618) exited]\r\n[Thread 0x7fffc6820700 (LWP 32621) exited]\r\n[Thread 0x7fffc401f700 (LWP 32623) exited]\r\n[Thread 0x7fffc181e700 (LWP 32624) exited]\r\n[Thread 0x7fffbf01d700 (LWP 32625) exited]\r\n[Thread 0x7fffbc81c700 (LWP 32626) exited]\r\n[Thread 0x7fffba01b700 (LWP 32627) exited]\r\n[Thread 0x7fffb781a700 (LWP 32628) exited]\r\n[Thread 0x7fffb5019700 (LWP 32629) exited]\r\n[Thread 0x7fffb2818700 (LWP 32630) exited]\r\n[Thread 0x7fffab015700 (LWP 32633) exited]\r\n[Thread 0x7fffb0017700 (LWP 32631) exited]\r\n[Thread 0x7fffad816700 (LWP 32632) exited]\r\n[New Thread 0x7fffab015700 (LWP 32668)]\r\n[New Thread 0x7fffad816700 (LWP 32671)]\r\n[New Thread 0x7fffb0017700 (LWP 32672)]\r\n[New Thread 0x7fffb2818700 (LWP 32682)]\r\nExported model has been executed on Caffe2 backend, and the result looks good!\r\n\r\nProgram received signal SIGSEGV, Segmentation fault.\r\n0x00000000004a6fb0 in ?? ()\r\n', '\r\n(gdb) where\r\n#0  0x00000000004a6fb0 in ?? ()\r\n#1  0x000000000041a703 in ?? ()\r\n#2  0x00000000004a6047 in ?? ()\r\n#3  0x0000000000515d00 in PyGC_Collect ()\r\n#4  0x0000000000513c1f in Py_Finalize ()\r\n#5  0x0000000000498099 in Py_Main ()\r\n#6  0x00007ffff6f12b45 in __libc_start_main (main=0x497c60 <main>, \r\n    argc=2, argv=0x7fffffffc258, init=<optimized out>, \r\n    fini=<optimized out>, rtld_fini=<optimized out>, \r\n    stack_end=0x7fffffffc248) at libc-start.c:287\r\n#7  0x0000000000497b8b in _start ()\r\n(gdb) \r\n']",[],0,0
742,pytorch,24690,closed,Migrate `digamma` and `digamma_` from the TH to Aten (CPU),"Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review.",better-engineering module: porting triaged,['https://github.com/pytorch/pytorch/pull/24952\r\n'],[],[],0,0
743,pytorch,29805,closed,I use Torch Script to convert  has error,"## ‚ùì Questions and Help
attribute 'use_res_connect' of type 'bool' is not usable in a script method (did you forget to add it __constants__?):
at d:\PyCode\FeatherNetB\models\FeatherNet2.py:77:15
    @torch.jit.script_method
    def forward(self, x):
        if str(self.use_res_connect) == ""True"":
               ~~~~~~~~~~~~~~~~~~~~ <--- HERE
            return x + self.conv(x)
        else:
            if self.downsample is not None:
                return self.downsample(x) + self.conv(x)
            else:
                return self.conv(x)
### Please note that this issue tracker is not a help form and this issue will be closed.

We have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:

- [Discussion Forum](https://discuss.pytorch.org/)
",,[],[],[],0,0
744,pytorch,6996,closed,"[pytorch][0.4][bug] ""torch.min"" and ""torch.max"" ignores ""nan"" in cuda case and crash with pure ""nan"" tensor","## Issue description

 and  ignores  in cuda case and crash with pure  tensor.

## Code example

## System Info

- PyTorch or Caffe2: PyTorch
- How you installed PyTorch (conda, pip, source): conda
- Build command you used (if compiling from source):
- OS: Ubuntu 16.04
- PyTorch version: 0.4.0
- Python version: 3.6.3
- CUDA/cuDNN version: 8 / 5
- GPU models and configuration: GTX 1050 Ti
- GCC version (if compiling from source):
- CMake version:
- Versions of any other relevant libraries:
",,"['Thanks for the report, @sytrus-in-github. I have reproduced the issue and have seen the same behavior as you have.', 'by the way, same tests with nightly build 2018.04.20:\r\n```python\r\n>>> import torch\r\n>>> torch.__version__\r\n\'2018.04.20\'\r\n>>> a = torch.Tensor(4, 3).fill_(1)\r\n>>> a[1, 2] = float(\'nan\')\r\n>>> a = a.cuda()\r\n>>> torch.max(a)\r\ntensor(1.0000, device=\'cuda:0\')\r\n>>> torch.min(a)\r\ntensor(1.0000, device=\'cuda:0\')\r\n>>> a = torch.Tensor(4, 3).fill_(float(\'nan\'))\r\n>>> a = a.cuda()\r\n>>> torch.max(a)\r\ntensor(1.00000e+38 *\r\n       -3.4028, device=\'cuda:0\')\r\n>>> torch.min(a)\r\ntensor(1.00000e+38 *\r\n       3.4028, device=\'cuda:0\')\r\n```\r\nThis might shed some light as to why it crashes saying ""overflow"".', ""It's not really a crash but a printing bug (duplicate of #6339 ). cc @li-roy "", '@SsnL maybe not just a printing bug, in this case the tensor `a` contains only `nan`. The big values `1.00000e+38 * -3.4028` and `1.00000e+38 * 3.4028` seem to come from nowhere ...', ""Yes I agree that there is a bug in the kernel too. Sorry for not being clear. I was saying that the function didn't crash, but the printing function did."", ""Following this post, do we sort out the issue of nanmin and nanmax on non-cuda version? I still get the same problem.\r\n`\r\n>>>import torch\r\n>>>a = torch.tensor([1, 2, 3, float('NaN')])\r\n>>>a.min()\r\ntensor(nan)\r\n>>>a.max()\r\ntensor(nan)\r\n`""]","['python\r\n>>> import torch\r\n>>> a = torch.Tensor(4, 3).fill_(1)\r\n>>> a = a.cuda()\r\n>>> a[1, 2] = float(\'nan\')\r\n>>> a\r\ntensor([[  1.,   1.,   1.],\r\n        [  1.,   1., nan.],\r\n        [  1.,   1.,   1.],\r\n        [  1.,   1.,   1.]], device=\'cuda:0\')\r\n>>> torch.max(a)\r\ntensor(1., device=\'cuda:0\')\r\n>>> torch.min(a)\r\ntensor(1., device=\'cuda:0\')\r\n>>> a = torch.Tensor(4, 3).fill_(float(\'nan\'))\r\n>>> a = a.cuda()\r\n>>> torch.max(a)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""anaconda3/lib/python3.6/site-packages/torch/tensor.py"", line 57, in __repr__\r\n    return torch._tensor_str._str(self)\r\n  File ""anaconda3/lib/python3.6/site-packages/torch/_tensor_str.py"", line 218, in _str\r\n    fmt, scale, sz = _number_format(self)\r\n  File ""anaconda3/lib/python3.6/site-packages/torch/_tensor_str.py"", line 96, in _number_format\r\n    if value != math.ceil(value.item()):\r\nRuntimeError: Overflow when unpacking long\r\n>>> torch.min(a)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""anaconda3/lib/python3.6/site-packages/torch/tensor.py"", line 57, in __repr__\r\n    return torch._tensor_str._str(self)\r\n  File ""anaconda3/lib/python3.6/site-packages/torch/_tensor_str.py"", line 218, in _str\r\n    fmt, scale, sz = _number_format(self)\r\n  File ""anaconda3/lib/python3.6/site-packages/torch/_tensor_str.py"", line 96, in _number_format\r\n    if value != math.ceil(value.item()):\r\nRuntimeError: Overflow when unpacking long\r\n']","['torch.min', 'torch.max', 'nan', 'nan']",0,0
745,pytorch,8519,closed,[JIT] Passed-in parameter is not a 1D LongTensor when expected,"Operator-level tests fail. Example failure:


Affected tests:
- 
- 
- 
- 
- ",oncall: jit,"['Problem is here: https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/tensor_conversions.h#L64\r\n\r\nThe number literals are emitted as 0-d tensors, but this conversion function expects 1d', ""Nevermind, this is a specific case of varargs inputs that's failing. Merging into the other task""]","['\r\n======================================================================\r\nERROR: test_repeat_single_number (__main__.TestAutogradGenerated)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""test/test_autograd.py"", line 3665, in do_test\r\n    check(name)\r\n  File ""test/test_autograd.py"", line 3592, in check\r\n    fn, (self_variable,) + args_variable)\r\n  File ""test/test_autograd.py"", line 3498, in check_against_reference\r\n    outputs_test = func(*nograd_inputs)\r\n  File ""test/test_autograd.py"", line 3474, in script_fn\r\n    return output_process_fn(CU.the_method(*tensors))\r\nRuntimeError: \r\nExpected a 1D LongTensor:\r\noperation failed in interpreter:\r\n\r\ndef the_method(i0):\r\n    return i0.repeat(2)\r\n           ~~~~~~~~~ <--- HERE\r\n']","['test_repeat_single_number', 'test_reshape_1d', 'test_reshape_scalar_to_1d', 'test_view_1d', 'test_view_scalar_to_1d']",0,0
746,pytorch,15958,closed,"stable 1.0 with cuda 10 successfully, but cannot pip3 install torchvision!","## ‚ùì Questions and Help

### Please note that this issue tracker is not a help form and this issue will be closed.

We have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:

- [Discussion Forum](https://discuss.pytorch.org/)
I installed pytorch stable 1.0, linux, pip, python 3.6, successfully. However, when I pip3 install torchvision, it began downloading pytorch for cpu.
So I cancelled the installing torchvision!

Waiting for help!",,['please use https://discuss.pytorch.org for questions'],[],[],0,0
747,pytorch,1738,closed,How do make batchnorm to maintain multiple pairs of running_mean and running_var?,"Hi,

The current batchnorm implementation only has one pair of running_mean and running_var. However, some modules in the neural networks may be used twice or more in one network forward. Based on my experimental experience and also stated in paper https://arxiv.org/pdf/1603.09025.pdf , different running_mean and running_var should be computed for different iterations(or time steps). I have found in my experiments, for the same testing data, net.eval() performs much worse than net.train() (backward is turned off here). The only difference in this situation is different running_mean and running_var are used. So how to make the batchnorm to keep multiple running_mean and running_var and I can choose to use the first pair in the first iteration, the second pair in the second iteration, and so on?",,"['you have two separate BatchNorm layers, like b1 = nn.BatchNorm(...), b2 = nn.BatchNorm(...).\r\n\r\nAlso, questions are asked here: https://discuss.pytorch.org', 'Thanks for the reply. The problem is that, as pointed in the paper mentioned above, the learnable parameters gamma and beta should be shared between these BN modules. Only the running_mean and running_var should be differentiated by the iterations. If do it in this way, how do we force them to share the learnable weights gamma and beta? @soumith ']",[],[],0,0
748,pytorch,2527,closed,Pytorch giving a cuda runtime error (30),"I have made small changes in the  example code. Changed [these lines](https://github.com/pytorch/examples/blob/86bc3e516dde92fee0ce00668a3714c03155dd44/imagenet/main.py#L80-L88) to:


And, when I run the script, it throws a cuda runtime error which is as follows:


My cuda version is as follows:


A much more detailed post is also available here: https://stackoverflow.com/q/45861767/4993513",,['closing in favor of https://discuss.pytorch.org/t/pytorch-giving-me-a-weird-cuda-error/6678'],"[""\r\n\r\n   if not args.distributed:\r\n        if args.arch.startswith('alexnet') or args.arch.startswith('vgg'):\r\n            #model.features = torch.nn.DataParallel(model.features)\r\n            model.cuda()\r\n        #else:\r\n            #model = torch.nn.DataParallel(model).cuda()\r\n    else:\r\n        model.cuda()\r\n        #model = torch.nn.parallel.DistributedDataParallel(model)\r\n\r\n    # define loss function (criterion) and optimizer\r\n    criterion = nn.CrossEntropyLoss().cuda()\r\n"", '\r\n\r\nubuntu@ip-x-y-z-a:~$ time CUDA_VISIBLE_DEVICES=0 python imagenet2.py --world-size 1 --arch \'alexnet\' ImageNet2\r\n=> creating model \'alexnet\'\r\nTHCudaCheck FAIL file=/pytorch/torch/lib/THC/THCGeneral.c line=70 error=30 : unknown error\r\nTraceback (most recent call last):\r\n  File ""imagenet2.py"", line 319, in <module>\r\n    main()\r\n  File ""imagenet2.py"", line 87, in main\r\n    model.cuda()\r\n  File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 147, in cuda\r\n    return self._apply(lambda t: t.cuda(device_id))\r\n  File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 118, in _apply\r\n    module._apply(fn)\r\n  File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 118, in _apply\r\n    module._apply(fn)\r\n  File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 124, in _apply\r\n    param.data = fn(param.data)\r\n  File ""/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py"", line 147, in <lambda>\r\n    return self._apply(lambda t: t.cuda(device_id))\r\n  File ""/usr/local/lib/python2.7/dist-packages/torch/_utils.py"", line 66, in _cuda\r\n    return new_type(self.size()).copy_(self, async)\r\n  File ""/usr/local/lib/python2.7/dist-packages/torch/cuda/__init__.py"", line 266, in _lazy_new\r\n    _lazy_init()\r\n  File ""/usr/local/lib/python2.7/dist-packages/torch/cuda/__init__.py"", line 85, in _lazy_init\r\n    torch._C._cuda_init()\r\nRuntimeError: cuda runtime error (30) : unknown error at /pytorch/torch/lib/THC/THCGeneral.c:70\r\n', '\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2016 NVIDIA Corporation\r\nBuilt on Tue_Jan_10_13:22:03_CST_2017\r\nCuda compilation tools, release 8.0, V8.0.61\r\n']",['imagenet'],0,0
749,pytorch,15920,closed,How to let Batchnorm to use global mean in both training and inferance?,"## ‚ùì Questions and Help
**Observation:** 
(1) If I use the default bn layer and use model.eval() before infence.  Test result has a significant discrepancy between test and other test.
(2) If a set momentum to 0.0 and 0.1 in different tests. The results are also different. Is this discrepancy designed to be True? It doesn't make sense.
**Guess:** 
I guess this is because bn layer uses different mean and var in different tests. And model is senstive to these mean/var changes?
**Question:** 
(1) Is My guesses correct? If correct, I can I always use global mean and global var so that the result is same in both training and eval? 
(2) Is these bn parameters designed to changes with time? So different inference sequence causes diffferent result?
**Tried:**
 I tried set momentum to 0 or track_running_stats=False after load the model. Both doesn't work.
",,"[""What's the good to keep bn parameters changing with time?"", ""If a set momentum to 0.0 and 0.1 in different tests. The results are also different. Is this discrepancy designed to be True? It doesn't make sense."", 'it should be clear from the template that Questions and Help are at https://discuss.pytorch.org']",[],[],0,0
750,pytorch,9499,closed,Move ConstantPadNd into ATen," is currently implemented in Python at [torch/nn/_functions/padding.py](https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/padding.py). We'd like it to be implemented as a native function in ATen. Note that you will need to update  in  after porting.

Reading material:

1. How to add a native function: https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native/
2. Derivatives are added in 

Tips:
  There is no context object in native functions; instead, if something needs to be reused in , 
  + if it is an input argument or output of , you can use it directly in .
  + otherwise, make it an output of the 
",newcomer,"['Sorry about this: `ConstantPadNd` resides in `torch/nn/_functions/padding.py`.', 'Haha thanks for pointing it out! You can tell that I copied from #9430 :P', ""Hi @SsnL! I'd like to take this on"", '@wdhorton Thanks! Let me know if you have any questions. :)', ""Hi @SsnL, I've been working on this, feel like I have most of the `forward` function moved over the C++, but I had a question about this line:\r\n```\r\noutput = input.new(input.size()[:(ctx.l_diff)] + new_dim).fill_(ctx.value)\r\n```\r\n\r\nI can't seem to track down the C++ equivalent of the `.new` method being called here. Any suggestions on how I should translate that?\r\n"", ""Use `at::empty(shape, input.options())`. The original python code was written a long time ago and isn't optimal with the current standard. So don't feel compelled to do a line-by-line translation, :) "", ""@SsnL I've got a PR here: #10885 if you have a chance to review"", 'The linked porting PR was merged.']",[],"['ConstantPadNd', 'pad(..)', 'nn/functional.py', 'tools/autograd/derivatives.yaml', 'backward', 'forward', 'tools/autograd/derivatives.yaml', 'forward']",0,0
751,pytorch,22697,closed,[docs] torch.lerp typo,"## üìö Documentation

https://pytorch.org/docs/master/torch.html#torch.lerp



Should probably be  must be broadcastable at the end.
",good first issue module: docs small triaged,"['Agree, it should probably be `weight, start, end`', 'I am marking this as a ‚Äúgood first issue‚Äù. Please feel free to remove if you think this is not valid.', 'I have opened a pull request #23477', '@vishwakftw if it is already resolved. why it is still open?\r\n\r\nI am just looking for something to get started for contributing.', 'https://github.com/pytorch/pytorch/pull/23268 has fixed this. Sorry about leaving this open.']",[],"['The shapes of start and end must be broadcastable. If weight is a tensor, then the shapes of start, end must be broadcastable.', 'weight, start, end']",0,0
