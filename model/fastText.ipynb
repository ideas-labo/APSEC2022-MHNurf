{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"fastText.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO/Ql96kHkpB2cnXLaIS1cW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"rCi9QrfyD74o"},"source":["# Import Modules"]},{"cell_type":"code","metadata":{"id":"IpTEIFc3Cd9g"},"source":["!pip install fasttext"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9-vxOSBlFBhx"},"source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import RidgeClassifier\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import f1_score,auc,roc_curve\n","from sklearn.model_selection import KFold,train_test_split,StratifiedKFold,StratifiedShuffleSplit\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.linear_model import LogisticRegression\n","import string\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","import re\n","import ast\n","from imblearn.over_sampling import RandomOverSampler,SMOTE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TrVgqdIJFNcp"},"source":["# Data loading"]},{"cell_type":"code","metadata":{"id":"YvJSBp6nGg2Y"},"source":["!git clone https://github.com/anonymoususr12/MHPurf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qa9n3wE5FRMu"},"source":["%cd MHPurf/data\n","project = 'tensorflow' # select in [tensorflow, pytorch, keras, incubator-mxnet, caffe]\n","path = f'{project}.csv'\n","pd_all = pd.read_csv(path)\n","pd_all = pd_all.sample(frac=1,random_state=999)\n","pd_all['Title+Body'] = ''\n","for idx in range(len(pd_all)):\n","  if pd.notna(pd_all['Body'].iloc[idx]):\n","    pd_all['Title+Body'].iloc[idx] = pd_all['Title'].iloc[idx] + '. ' + pd_all['Body'].iloc[idx]\n","  else:\n","    pd_all['Title+Body'].iloc[idx] = pd_all['Title'].iloc[idx]\n","pd_title = pd_all.rename(columns={\"Unnamed: 0\":\"id\",\"class\":\"sentiment\",\"Title\":\"text\"})\n","pd_title.to_csv('Title.csv', index=False, columns=[\"id\",\"Number\",\"sentiment\",\"text\"], mode=\"w\")\n","pd_body = pd_all.rename(columns={\"Unnamed: 0\":\"id\",\"class\":\"sentiment\",\"Body\":\"text\"})\n","pd_body.to_csv('Body.csv', index=False, columns=[\"id\",\"Number\",\"sentiment\",\"text\"], mode=\"w\")\n","pd_label = pd_all.rename(columns={\"Unnamed: 0\":\"id\",\"class\":\"sentiment\",\"Labels\":\"text\"})\n","pd_label.to_csv('Labels.csv', index=False, columns=[\"id\",\"Number\",\"sentiment\",\"text\"], mode=\"w\")\n","pd_code = pd_all.rename(columns={\"Unnamed: 0\":\"id\",\"class\":\"sentiment\",\"Codes\":\"text\"})\n","pd_code.to_csv('Codes.csv', index=False, columns=[\"id\",\"Number\",\"sentiment\",\"text\"], mode=\"w\")\n","pd_comment = pd_all.rename(columns={\"Unnamed: 0\":\"id\",\"class\":\"sentiment\",\"Comments\":\"text\"})\n","pd_comment.to_csv('Comments.csv', index=False, columns=[\"id\",\"Number\",\"sentiment\",\"text\"], mode=\"w\")\n","pd_command = pd_all.rename(columns={\"Unnamed: 0\":\"id\",\"class\":\"sentiment\",\"Commands\":\"text\"})\n","pd_command.to_csv('Command.csv', index=False, columns=[\"id\",\"Number\",\"sentiment\",\"text\"], mode=\"w\")\n","pd_tplusb = pd_all.rename(columns={\"Unnamed: 0\":\"id\",\"class\":\"sentiment\",\"Title+Body\":\"text\"})\n","pd_tplusb.to_csv('Title+Body.csv', index=False, columns=[\"id\",\"Number\",\"sentiment\",\"text\"], mode=\"w\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pYrNb7KvD6vi"},"source":["%cd all_exc\n","import pandas as pd\n","import csv\n","import numpy as np\n","\n","AUTOTUNE_FLAG = False\n","\n","REPEAT = 30\n","repeated_range = range(0,REPEAT)\n","\n","working_path = 'Title+Body.csv'\n","out_csv_name = f'../fasttext_{project}'\n","\n","if AUTOTUNE_FLAG == True:\n","  out_csv_name += '+_AUTOTUNE'\n","\n","out_csv_name += '.csv'\n","\n","data = pd.read_csv(working_path)\n","data = data.fillna('')\n","\n","# remove html tag\n","def remove_html(text):\n","    html = re.compile(r'<.*?>')\n","    return html.sub(r'', text)\n","\n","data['text'] = data['text'].apply(lambda x: remove_html(x))\n","\n","# remove emoji\n","def remove_emoji(text):\n","    emoji_pattern = re.compile(\"[\"\n","                               u\"\\U0001F600-\\U0001F64F\" #emoticons\n","                               u\"\\U0001F300-\\U0001F5FF\" #symbols&pics\n","                               u\"\\U0001F680-\\U0001F6FF\" #transportation pic\n","                               u\"\\U0001F1E0-\\U0001F1FF\" #flags\n","                               u\"\\U00002702-\\U000027B0\"\n","                               u\"\\U000024C2-\\U0001F251\"    \n","                               \"]+\", flags = re.UNICODE)\n","    return emoji_pattern.sub(r'', text)\n","data['text'] = data['text'].apply(lambda x: remove_emoji(x))\n","\n","# Stop Word Removal\n","NLTK_stop_words_list = stopwords.words('english')\n","custom_stop_words_list = ['...']\n","final_stop_words_list = NLTK_stop_words_list + custom_stop_words_list\n","def remove_stopwords(text):\n","    \"\"\"custom function to remove the stopwords\"\"\"\n","    return \" \".join([word for word in str(text).split() if word not in final_stop_words_list])\n","data['text'] = data['text'].apply(lambda text: remove_stopwords(text))\n","\n","# Symbol Removal\n","def clean_str(string):\n","    string = re.sub(r\"[^A-Za-z0-9(),.!?\\'\\`]\", \" \", string)\n","    string = re.sub(r\"\\'s\", \" \\'s\", string)\n","    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n","    string = re.sub(r\"\\)\", \" \\) \", string)\n","    string = re.sub(r\"\\?\", \" \\? \", string)\n","    string = re.sub(r\"\\s{2,}\", \" \", string)\n","    string = re.sub(r\"\\\\\", \"\", string)\n","    string = re.sub(r\"\\'\", \"\", string)\n","    string = re.sub(r\"\\\"\", \"\", string)\n","    return string.strip().lower()\n","\n","data['text'] = data['text'].apply(lambda x: clean_str(x))\n","\n","import fasttext\n","\n","data['sentiment'] = data['sentiment'].apply(lambda x: '__label__' + str(x))\n","\n","original_body = data\n","\n","macro = []\n","micro = []\n","auc_value = []\n","for repeated_time in repeated_range:\n","  indices = np.arange(data.shape[0])\n","  train_index,test_index = train_test_split(indices, test_size=0.2, random_state=repeated_time)\n","\n","  train_data = data.iloc[train_index]\n","  test_data = data.iloc[test_index]\n","  if AUTOTUNE_FLAG:\n","\n","    train2_data, valid_data = train_test_split(train_data, test_size=0.2)\n","\n","    train2_data[['sentiment', 'text']].to_csv('train.txt', \n","                                              index = False, \n","                                              sep = ' ',\n","                                              header = None, \n","                                              quoting = csv.QUOTE_NONE, \n","                                              quotechar = \"\", \n","                                              escapechar = \" \")\n","\n","    valid_data[['sentiment', 'text']].to_csv('valid.txt', \n","                                              index = False, \n","                                              sep = ' ',\n","                                              header = None, \n","                                              quoting = csv.QUOTE_NONE, \n","                                              quotechar = \"\", \n","                                              escapechar = \" \")\n","\n","    test_data[['sentiment', 'text']].to_csv('test.txt', \n","                                              index = False, \n","                                              sep = ' ',\n","                                              header = None, \n","                                              quoting = csv.QUOTE_NONE, \n","                                              quotechar = \"\", \n","                                              escapechar = \" \")\n","\n","    classifier = fasttext.train_supervised(input = \"train.txt\", autotuneValidationFile = \"valid.txt\")\n","\n","  else:\n","    train_data[['sentiment', 'text']].to_csv('train.txt', \n","                                              index = False, \n","                                              sep = ' ',\n","                                              header = None, \n","                                              quoting = csv.QUOTE_NONE, \n","                                              quotechar = \"\", \n","                                              escapechar = \" \")\n","    classifier = fasttext.train_supervised(input = \"train.txt\")\n","  y_pred = []\n","  y_true = []\n","  for idx in range(len(test_data)):\n","    y_pred.append(int(classifier.predict(test_data['text'].iloc[idx])[0][0][9]))\n","    y_true.append(int(test_data['sentiment'].iloc[idx][9]))\n","\n","  current_macro = f1_score(y_true, y_pred, average='macro')\n","  macro.append(current_macro)\n","  current_micro = f1_score(y_true, y_pred, average='micro')\n","  micro.append(current_micro)\n","  fpr, tpr, thresholds = roc_curve(y_true, y_pred, pos_label=1)\n","  current_auc = auc(fpr, tpr)\n","  auc_value.append(current_auc)\n","                  \n","  new_row = {'repeated_time':repeated_time,'cv_list':str(auc_value),'Macro F1':current_macro, 'Micro F1':current_micro, 'AUC':current_auc}\n","  df_log = pd.DataFrame(columns=['repeated_times','cv_list','Macro F1', 'Micro F1', 'AUC'])\n","  df_log = df_log.append(new_row, ignore_index=True)\n","  df_log.to_csv(out_csv_name, mode='a', header=False)"],"execution_count":null,"outputs":[]}]}