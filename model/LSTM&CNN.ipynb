{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM&CNN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPleV9vQBeR62/eOc39FhFQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"p1Vc5jmW_zk2"},"source":["# Import modules "]},{"cell_type":"code","metadata":{"id":"nrLqvWo996e-"},"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Reshape, Conv2D, Concatenate, Flatten\n","from keras.layers import Bidirectional, GlobalMaxPool1D, SpatialDropout1D, MaxPool2D, BatchNormalization, Wrapper, InputSpec, TimeDistributed, concatenate\n","from keras.models import Model\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","plt.style.use('ggplot')\n","import seaborn as sns\n","import numpy as np\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from keras.utils.np_utils import to_categorical\n","from sklearn.model_selection import train_test_split, KFold, StratifiedShuffleSplit, StratifiedKFold\n","from sklearn.metrics import (\n","      accuracy_score,\n","      recall_score,\n","      precision_score,\n","      f1_score, roc_curve, auc)\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","early_stopping = EarlyStopping(monitor='val_loss', patience=4,restore_best_weights=True,verbose=0)\n","callbacks=[early_stopping]\n","from keras import backend as K\n","import tensorflow as tf\n","import keras\n","import pandas as pd\n","from tensorflow.keras.optimizers import Adam\n","from gensim.models import KeyedVectors\n","import string"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PefyF2CF_8ij"},"source":["# Load data"]},{"cell_type":"code","metadata":{"id":"P9unlGAmAyZT"},"source":["!git clone https://github.com/anonymoususr12/MHPurf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0rNBGWKJ9cyn"},"source":["%cd MHPurf/data\n","project = 'tensorflow' # select in [tensorflow, pytorch, keras, incubator-mxnet, caffe]\n","path = f'{project}.csv'\n","pd_all = pd.read_csv(path)\n","pd_all = pd_all.sample(frac=1,random_state=999)\n","pd_all['Title+Body'] = ''\n","for idx in range(len(pd_all)):\n","  if pd.notna(pd_all['Body'].iloc[idx]):\n","    pd_all['Title+Body'].iloc[idx] = pd_all['Title'].iloc[idx] + '. ' + pd_all['Body'].iloc[idx]\n","  else:\n","    pd_all['Title+Body'].iloc[idx] = pd_all['Title'].iloc[idx]\n","pd_title = pd_all.rename(columns={\"Unnamed: 0\":\"id\",\"class\":\"sentiment\",\"Title\":\"text\"})\n","pd_title.to_csv('Title.csv', index=False, columns=[\"id\",\"Number\",\"sentiment\",\"text\"], mode=\"w\")\n","pd_body = pd_all.rename(columns={\"Unnamed: 0\":\"id\",\"class\":\"sentiment\",\"Body\":\"text\"})\n","pd_body.to_csv('Body.csv', index=False, columns=[\"id\",\"Number\",\"sentiment\",\"text\"], mode=\"w\")\n","pd_label = pd_all.rename(columns={\"Unnamed: 0\":\"id\",\"class\":\"sentiment\",\"Labels\":\"text\"})\n","pd_label.to_csv('Labels.csv', index=False, columns=[\"id\",\"Number\",\"sentiment\",\"text\"], mode=\"w\")\n","pd_code = pd_all.rename(columns={\"Unnamed: 0\":\"id\",\"class\":\"sentiment\",\"Codes\":\"text\"})\n","pd_code.to_csv('Codes.csv', index=False, columns=[\"id\",\"Number\",\"sentiment\",\"text\"], mode=\"w\")\n","pd_comment = pd_all.rename(columns={\"Unnamed: 0\":\"id\",\"class\":\"sentiment\",\"Comments\":\"text\"})\n","pd_comment.to_csv('Comments.csv', index=False, columns=[\"id\",\"Number\",\"sentiment\",\"text\"], mode=\"w\")\n","pd_command = pd_all.rename(columns={\"Unnamed: 0\":\"id\",\"class\":\"sentiment\",\"Commands\":\"text\"})\n","pd_command.to_csv('Command.csv', index=False, columns=[\"id\",\"Number\",\"sentiment\",\"text\"], mode=\"w\")\n","pd_tplusb = pd_all.rename(columns={\"Unnamed: 0\":\"id\",\"class\":\"sentiment\",\"Title+Body\":\"text\"})\n","pd_tplusb.to_csv('Title+Body.csv', index=False, columns=[\"id\",\"Number\",\"sentiment\",\"text\"], mode=\"w\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E8QrdaUBhezp"},"source":["# Tuning"]},{"cell_type":"code","metadata":{"id":"ztLTnb2Phd6E"},"source":["##################### GLOBAL ###################\n","NETWORK = 'LSTM'\n","\n","repeated_range = range(0,1)\n","\n","for max_features in [1000,2500,5000,10000]:\n","  for MAX_LEN in [250,500,1000,2000]:\n","\n","    embed_size = 100 # how big is each word vector\n","    CV_TIME = 10\n","\n","    out_csv_name = f'../{project}_{NETWORK}_{max_features}feature_{MAX_LEN}len'\n","\n","    if NETWORK == 'LSTM':\n","      out_csv_name += '_LSTM'\n","    elif NETWORK == 'CNN':\n","      out_csv_name += '_CNN'\n","\n","    working_path = 'Title+Body.csv'\n","\n","    data = pd.read_csv(working_path)\n","    data = data.rename(columns={\"sentiment\": \"target\"})\n","    #################################################\n","    for i in range(len(data['text'])):\n","        data['text'].iloc[i] = str(data['text'].iloc[i])\n","\n","    # Preprocessing\n","\n","    import re\n","\n","    # removing URLs\n","\n","    def remove_url(text):\n","        url = re.compile(r'https?://\\S+|www\\.\\S+')\n","        return url.sub(r'', text)\n","\n","    data[\"text\"] = data[\"text\"].apply(lambda x: remove_url(x))\n","\n","    # removing html\n","\n","    def remove_html(text):\n","        html = re.compile(r'<.*?>')\n","        return html.sub(r'', text)\n","\n","    data[\"text\"] = data[\"text\"].apply(lambda x: remove_html(x))\n","\n","    # removing emoji\n","\n","    def remove_emoji(text):\n","        emoji_pattern = re.compile(\"[\"\n","                                  u\"\\U0001F600-\\U0001F64F\" #emoticons\n","                                  u\"\\U0001F300-\\U0001F5FF\" #symbols&pics\n","                                  u\"\\U0001F680-\\U0001F6FF\" #transportation pic\n","                                  u\"\\U0001F1E0-\\U0001F1FF\" #flags\n","                                  u\"\\U00002702-\\U000027B0\"\n","                                  u\"\\U000024C2-\\U0001F251\"    \n","                                  \"]+\", flags = re.UNICODE)\n","        return emoji_pattern.sub(r'', text)\n","\n","    data[\"text\"] = data[\"text\"].apply(lambda x: remove_emoji(x))\n","\n","    # Stop Word Removal\n","    NLTK_stop_words_list = stopwords.words('english')\n","    custom_stop_words_list = ['...']\n","    final_stop_words_list = NLTK_stop_words_list + custom_stop_words_list\n","    def remove_stopwords(text):\n","        \"\"\"custom function to remove the stopwords\"\"\"\n","        return \" \".join([word for word in str(text).split() if word not in final_stop_words_list])\n","\n","    data[\"text\"] = data[\"text\"].apply(lambda text: remove_stopwords(text))\n","\n","    # Symbols Removal\n","    def clean_str(string):\n","        string = re.sub(r\"[^A-Za-z0-9(),.!?\\'\\`]\", \" \", string)\n","        string = re.sub(r\"\\'s\", \" \\'s\", string)\n","        string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n","        string = re.sub(r\"\\)\", \" \\) \", string)\n","        string = re.sub(r\"\\?\", \" \\? \", string)\n","        string = re.sub(r\"\\s{2,}\", \" \", string)\n","        string = re.sub(r\"\\\\\", \"\", string)\n","        string = re.sub(r\"\\'\", \"\", string)\n","        string = re.sub(r\"\\\"\", \"\", string)\n","        return string.strip().lower()\n","\n","    data[\"text\"] = data[\"text\"].apply(lambda text: clean_str(text))\n","\n","    # Word2Vec Embedding\n","    list_sentences = data[\"text\"].fillna(\"\").values\n","    tokenizer = Tokenizer(num_words=max_features)\n","    tokenizer.fit_on_texts(list(list_sentences))\n","    list_tokenized = tokenizer.texts_to_sequences(list_sentences)\n","    y = data[\"target\"]\n","    data = pad_sequences(list_tokenized, maxlen=MAX_LEN)\n","    def load_word2vec_embeddings(filepath, tokenizer, max_features, embedding_size):\n","        model = KeyedVectors.load_word2vec_format(filepath,limit=500000)\n","\n","        emb_mean, emb_std = model.wv.syn0.mean(), model.wv.syn0.std()\n","\n","        word_index = tokenizer.word_index\n","        nb_words = min(max_features, len(word_index))\n","        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_size))\n","        for word, i in word_index.items():\n","            if i > max_features:\n","                continue\n","            try:\n","                embedding_vector = model[word]\n","                embedding_matrix[i-1] = embedding_vector\n","            except KeyError:\n","                continue\n","        return embedding_matrix\n","\n","    embedding_matrix = load_word2vec_embeddings(\"embedding/enwiki_20180420_100d.txt.bz2\",\n","                                                tokenizer,\n","                                                max_features,\n","                                                embed_size)\n","\n","\n","\n","    # Model Construction\n","    def build_model(max_len):\n","        if NETWORK == 'LSTM':\n","          # LSTM begin\n","          inp = Input(shape=(MAX_LEN,))\n","          x = Dropout(0.1)(inp)\n","          x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n","          x = LSTM(100)(x)\n","          x = Dense(100, activation=\"relu\")(x)\n","          x = Dropout(0.1)(x)\n","          x = Dense(2, activation=\"softmax\")(x)\n","        \n","        if NETWORK == 'CNN':\n","          # CNN begin\n","          filter_sizes = [3,4,5]\n","          num_filters = 2\n","          maxlen = MAX_LEN\n","          inp = Input(shape=(maxlen,))\n","          x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n","          x = SpatialDropout1D(0.2)(x)\n","          x = Reshape((maxlen, embed_size, 1))(x)\n","          conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embed_size), kernel_initializer='normal',\n","                                                                                          activation='elu')(x)\n","          conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embed_size), kernel_initializer='normal',\n","                                                                                          activation='elu')(x)\n","          conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embed_size), kernel_initializer='normal',\n","                                                                                          activation='elu')(x)\n","          maxpool_0 = MaxPool2D(pool_size=(maxlen - filter_sizes[0] + 1, 1))(conv_0)\n","          maxpool_1 = MaxPool2D(pool_size=(maxlen - filter_sizes[1] + 1, 1))(conv_1)\n","          maxpool_2 = MaxPool2D(pool_size=(maxlen - filter_sizes[2] + 1, 1))(conv_2)\n","          z = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])   \n","          z = Flatten()(z)\n","          x = Dense(2, activation=\"softmax\")(z)\n","        \n","        model = Model(inputs=inp, outputs=x)\n","        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","        return model\n","\n","    # Training\n","    outer_macro = []\n","    outer_micro = []\n","    outer_auc_value = []\n","    for repeated_time in repeated_range:\n","        cv_time = 1\n","        macro = []\n","        micro = []\n","        auc_value = []\n","        data, _, y, _ = train_test_split(data,y,test_size=0.2, random_state=0)\n","        y = y.to_numpy()\n","        indices = np.arange(data.shape[0])\n","        kf = KFold(n_splits=CV_TIME, random_state=0, shuffle=True)\n","        for train_index, test_index in kf.split(data):\n","          y_train, y_test = y[train_index], y[test_index]\n","          X_train, X_test = data[train_index], data[test_index]          \n","          train_y = y[train_index]\n","          model = build_model(MAX_LEN)\n","          y_train = to_categorical(np.asarray(y_train))\n","          model.fit(X_train, y_train, batch_size=64, epochs=100, validation_split=0.1,callbacks=[early_stopping])\n","          y_pred = model.predict(X_test, batch_size=1024, verbose=1).round().astype(int)\n","          y_true = y_test\n","          y_true = to_categorical(np.asarray(y_true))\n","          current_macro=f1_score(y_true,y_pred,average='macro')\n","          macro.append(current_macro)\n","          print('\\n Average Macro F1 is ',sum(macro) / len(macro), f' after CV {cv_time}')\n","          current_micro=f1_score(y_true,y_pred,average='micro')\n","          micro.append(current_micro)\n","          print('\\n Average Macro F1 is ',sum(micro) / len(micro), f' after CV {cv_time}')\n","          fpr, tpr, thresholds = roc_curve(y_true[:,1], y_pred[:,1], pos_label=1)\n","          current_auc = auc(fpr, tpr)\n","          auc_value.append(current_auc)\n","          print('\\n Average AUC is ', sum(auc_value) / len(auc_value),\n","                    f' after CV {cv_time}')\n","          cv_time += 1\n","          outer_macro.append(sum(macro) / len(macro))\n","          print('\\n Average Macro F1 is ',sum(outer_macro) / len(outer_macro), f' after running time {repeated_time}')\n","          outer_micro.append(sum(micro) / len(micro))\n","          print('\\n Average F1 is ',sum(outer_micro) / len(outer_micro), f' after running time {repeated_time}')\n","          outer_auc_value.append(sum(auc_value)/len(auc_value))\n","          print('\\n Total AUC is ', sum(outer_auc_value) / len(outer_auc_value),\n","                    f' after running time {repeated_time}')\n","\n","        new_row = {'repeated_time':repeated_time,'cv_list':str(auc_value),'Macro F1':sum(macro) / len(macro), 'Micro F1':sum(micro) / len(micro), 'AUC':sum(auc_value) / len(auc_value)}\n","        df_log = pd.DataFrame(columns=['repeated_time','cv_list','Macro F1', 'Micro F1', 'AUC'])\n","        df_log = df_log.append(new_row, ignore_index=True)\n","        df_log.to_csv(out_csv_name, mode='a', header=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l42hWaW-CEQU"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"XbxUTfxH9PFf"},"source":["##################### GLOBAL ###################\n","NETWORK = 'LSTM' # choose LSTM or CNN\n","\n","REPEAT = 30\n","repeated_range = range(0,REPEAT)\n","\n","max_features = 1000\n","MAX_LEN = 250\n","\n","embed_size = 100 # how big is each word vector\n","CV_TIME = 10\n","\n","out_csv_name = f'../{project}_{NETWORK}'\n","\n","if NETWORK == 'LSTM':\n","  out_csv_name += '_LSTM'\n","elif NETWORK == 'CNN':\n","  out_csv_name += '_CNN'\n","\n","data = pd.read_csv('Title+Body.csv')\n","data = data.rename(columns={\"sentiment\": \"target\"})\n","#################################################\n","for i in range(len(data['text'])):\n","    data['text'].iloc[i] = str(data['text'].iloc[i])\n","\n","# Preprocessing\n","\n","import re\n","\n","# removing URLs\n","\n","def remove_url(text):\n","    url = re.compile(r'https?://\\S+|www\\.\\S+')\n","    return url.sub(r'', text)\n","\n","data[\"text\"] = data[\"text\"].apply(lambda x: remove_url(x))\n","\n","# removing html\n","\n","def remove_html(text):\n","    html = re.compile(r'<.*?>')\n","    return html.sub(r'', text)\n","\n","data[\"text\"] = data[\"text\"].apply(lambda x: remove_html(x))\n","\n","# removing emoji\n","\n","def remove_emoji(text):\n","    emoji_pattern = re.compile(\"[\"\n","                              u\"\\U0001F600-\\U0001F64F\" #emoticons\n","                              u\"\\U0001F300-\\U0001F5FF\" #symbols&pics\n","                              u\"\\U0001F680-\\U0001F6FF\" #transportation pic\n","                              u\"\\U0001F1E0-\\U0001F1FF\" #flags\n","                              u\"\\U00002702-\\U000027B0\"\n","                              u\"\\U000024C2-\\U0001F251\"    \n","                              \"]+\", flags = re.UNICODE)\n","    return emoji_pattern.sub(r'', text)\n","\n","data[\"text\"] = data[\"text\"].apply(lambda x: remove_emoji(x))\n","\n","# Stop Word Removal\n","NLTK_stop_words_list = stopwords.words('english')\n","custom_stop_words_list = ['...']\n","final_stop_words_list = NLTK_stop_words_list + custom_stop_words_list\n","def remove_stopwords(text):\n","    \"\"\"custom function to remove the stopwords\"\"\"\n","    return \" \".join([word for word in str(text).split() if word not in final_stop_words_list])\n","\n","data[\"text\"] = data[\"text\"].apply(lambda text: remove_stopwords(text))\n","\n","# Symbol removal\n","def clean_str(string):\n","    string = re.sub(r\"[^A-Za-z0-9(),.!?\\'\\`]\", \" \", string)\n","    string = re.sub(r\"\\'s\", \" \\'s\", string)\n","    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n","    string = re.sub(r\"\\)\", \" \\) \", string)\n","    string = re.sub(r\"\\?\", \" \\? \", string)\n","    string = re.sub(r\"\\s{2,}\", \" \", string)\n","    string = re.sub(r\"\\\\\", \"\", string)\n","    string = re.sub(r\"\\'\", \"\", string)\n","    string = re.sub(r\"\\\"\", \"\", string)\n","    return string.strip().lower()\n","\n","data[\"text\"] = data[\"text\"].apply(lambda text: clean_str(text))\n","\n","# Word2Vec Embedding\n","list_sentences = data[\"text\"].fillna(\"\").values\n","tokenizer = Tokenizer(num_words=max_features)\n","tokenizer.fit_on_texts(list(list_sentences))\n","list_tokenized = tokenizer.texts_to_sequences(list_sentences)\n","y = data[\"target\"]\n","data = pad_sequences(list_tokenized, maxlen=MAX_LEN)\n","\n","def load_word2vec_embeddings(filepath, tokenizer, max_features, embedding_size):\n","    model = KeyedVectors.load_word2vec_format(filepath,limit=500000)\n","\n","    emb_mean, emb_std = model.wv.syn0.mean(), model.wv.syn0.std()\n","\n","    word_index = tokenizer.word_index\n","    nb_words = min(max_features, len(word_index))\n","    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_size))\n","    for word, i in word_index.items():\n","        if i > max_features:\n","            continue\n","        try:\n","            embedding_vector = model[word]\n","            embedding_matrix[i-1] = embedding_vector\n","        except KeyError:\n","            continue\n","    return embedding_matrix\n","\n","embedding_matrix = load_word2vec_embeddings(\"embedding/enwiki_20180420_100d.txt.bz2\",\n","                                            tokenizer,\n","                                            max_features,\n","                                            embed_size)\n","\n","# Model Construction\n","def build_model(max_len):\n","    if NETWORK == 'LSTM':\n","      # LSTM begin\n","      inp = Input(shape=(MAX_LEN,))\n","      x = Dropout(0.1)(inp)\n","      x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n","      x = LSTM(100)(x)\n","      x = Dense(100, activation=\"relu\")(x)\n","      x = Dropout(0.1)(x)\n","      x = Dense(2, activation=\"softmax\")(x)\n","    \n","    if NETWORK == 'CNN':\n","      # cPur (CNN) begin\n","      filter_sizes = [3,4,5]\n","      num_filters = 2\n","      maxlen = MAX_LEN\n","      inp = Input(shape=(maxlen,))\n","      x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n","      x = SpatialDropout1D(0.2)(x)\n","      x = Reshape((maxlen, embed_size, 1))(x)\n","      conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embed_size), kernel_initializer='normal',\n","                                                                                      activation='elu')(x)\n","      conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embed_size), kernel_initializer='normal',\n","                                                                                      activation='elu')(x)\n","      conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embed_size), kernel_initializer='normal',\n","                                                                                      activation='elu')(x)\n","      maxpool_0 = MaxPool2D(pool_size=(maxlen - filter_sizes[0] + 1, 1))(conv_0)\n","      maxpool_1 = MaxPool2D(pool_size=(maxlen - filter_sizes[1] + 1, 1))(conv_1)\n","      maxpool_2 = MaxPool2D(pool_size=(maxlen - filter_sizes[2] + 1, 1))(conv_2)\n","      z = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])   \n","      z = Flatten()(z)\n","      x = Dense(2, activation=\"softmax\")(z)\n","    \n","    model = Model(inputs=inp, outputs=x)\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    return model\n","\n","# Training\n","macro = []\n","micro = []\n","auc_value = []\n","for repeated_time in repeated_range:\n","    cv_time = 1\n","    X_train, X_test, y_train, y_test = train_test_split(data,y,test_size=0.2, random_state=repeated_time)\n","    y_train = y_train.to_numpy()\n","    y_test = y_test.to_numpy()\n","  \n","    model = build_model(MAX_LEN)\n","    y_train = to_categorical(np.asarray(y_train))\n","    model.fit(X_train, y_train, batch_size=64, epochs=100, validation_split=0.1,callbacks=[early_stopping])\n","    y_pred = model.predict(X_test, batch_size=1024, verbose=1).round().astype(int)\n","    y_true = y_test\n","    y_true = to_categorical(np.asarray(y_true))\n","    current_macro=f1_score(y_true,y_pred,average='macro')\n","    macro.append(current_macro)\n","    current_micro=f1_score(y_true,y_pred,average='micro')\n","    micro.append(current_micro)\n","    fpr, tpr, thresholds = roc_curve(y_true[:,1], y_pred[:,1], pos_label=1)\n","    current_auc = auc(fpr, tpr)\n","    auc_value.append(current_auc)\n","\n","    new_row = {'repeated_times':REPEAT,'cv_list':str(auc_value),'Macro F1':sum(macro) / len(macro), 'Micro F1':sum(micro) / len(micro), 'AUC':sum(auc_value) / len(auc_value)}\n","    df_log = pd.DataFrame(columns=['repeated_time','cv_list','Macro F1', 'Micro F1', 'AUC'])\n","    df_log = df_log.append(new_row, ignore_index=True)\n","    df_log.to_csv(out_csv_name, mode='a', header=False)"],"execution_count":null,"outputs":[]}]}