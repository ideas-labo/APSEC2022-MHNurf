{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ML.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMDGIpDBFf9CkFHG3kh3yM6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ChOPD0sdys-2"},"source":["# Module Import"]},{"cell_type":"code","metadata":{"id":"xMsXTQAvy2N2"},"source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import RidgeClassifier\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import f1_score,auc,roc_curve\n","from sklearn.model_selection import KFold,train_test_split,StratifiedShuffleSplit,StratifiedKFold\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.linear_model import LogisticRegression\n","from gensim.sklearn_api import TfIdfTransformer\n","import string\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","import re\n","import ast\n","import pandas as pd\n","import numpy as np\n","import math\n","from sklearn.model_selection import GridSearchCV"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d6pWk_-izy14"},"source":["# Define methods"]},{"cell_type":"code","metadata":{"id":"lXkWFaC3z4qF","executionInfo":{"status":"ok","timestamp":1630680871908,"user_tz":-60,"elapsed":186,"user":{"displayName":"Logan Long","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05276728414427570973"}}},"source":["def remove_html(text):\n","    html = re.compile(r'<.*?>')\n","    return html.sub(r'', text)\n","def remove_emoji(text):\n","    emoji_pattern = re.compile(\"[\"\n","                              u\"\\U0001F600-\\U0001F64F\" #emoticons\n","                              u\"\\U0001F300-\\U0001F5FF\" #symbols&pics\n","                              u\"\\U0001F680-\\U0001F6FF\" #transportation pic\n","                              u\"\\U0001F1E0-\\U0001F1FF\" #flags\n","                              u\"\\U00002702-\\U000027B0\"\n","                              u\"\\U000024C2-\\U0001F251\"    \n","                              \"]+\", flags = re.UNICODE)\n","    return emoji_pattern.sub(r'', text)\n","NLTK_stop_words_list = stopwords.words('english')\n","custom_stop_words_list = ['...']\n","final_stop_words_list = NLTK_stop_words_list + custom_stop_words_list\n","def remove_stopwords(text):\n","    get_text = \" \".join([word for word in str(text).split() if word not in final_stop_words_list])\n","    return get_text\n","def clean_str(string):\n","    string = re.sub(r\"[^A-Za-z0-9(),.!?\\'\\`]\", \" \", string)\n","    string = re.sub(r\"\\'s\", \" \\'s\", string)\n","    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n","    string = re.sub(r\"\\)\", \" \\) \", string)\n","    string = re.sub(r\"\\?\", \" \\? \", string)\n","    string = re.sub(r\"\\s{2,}\", \" \", string)\n","    string = re.sub(r\"\\\\\", \"\", string)\n","    string = re.sub(r\"\\'\", \"\", string)\n","    string = re.sub(r\"\\\"\", \"\", string)\n","    return string.strip().lower()"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R2yaR24N1ULk"},"source":["# loading data"]},{"cell_type":"code","metadata":{"id":"EDwI0Otm2up8"},"source":["!git clone https://github.com/anonymoususr12/MHPurf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iltYw96H1R11"},"source":["%cd MHPurf/data\n","project = 'pytorch' # select in [tensorflow, pytorch, keras, incubator-mxnet, caffe]\n","path = f'{project}.csv'\n","pd_all = pd.read_csv(path)\n","pd_all = pd_all.sample(frac=1,random_state=999)\n","pd_all['Title+Body'] = ''\n","for idx in range(len(pd_all)):\n","  if pd.notna(pd_all['Body'].iloc[idx]):\n","    pd_all['Title+Body'].iloc[idx] = pd_all['Title'].iloc[idx] + '. ' + pd_all['Body'].iloc[idx]\n","  else:\n","    pd_all['Title+Body'].iloc[idx] = pd_all['Title'].iloc[idx]\n","pd_title = pd_all.rename(columns={\"Unnamed: 0\":\"id\",\"class\":\"sentiment\",\"Title\":\"text\"})\n","pd_title.to_csv('Title.csv', index=False, columns=[\"id\",\"Number\",\"sentiment\",\"text\"], mode=\"w\")\n","pd_body = pd_all.rename(columns={\"Unnamed: 0\":\"id\",\"class\":\"sentiment\",\"Body\":\"text\"})\n","pd_body.to_csv('Body.csv', index=False, columns=[\"id\",\"Number\",\"sentiment\",\"text\"], mode=\"w\")\n","pd_label = pd_all.rename(columns={\"Unnamed: 0\":\"id\",\"class\":\"sentiment\",\"Labels\":\"text\"})\n","pd_label.to_csv('Labels.csv', index=False, columns=[\"id\",\"Number\",\"sentiment\",\"text\"], mode=\"w\")\n","pd_code = pd_all.rename(columns={\"Unnamed: 0\":\"id\",\"class\":\"sentiment\",\"Codes\":\"text\"})\n","pd_code.to_csv('Codes.csv', index=False, columns=[\"id\",\"Number\",\"sentiment\",\"text\"], mode=\"w\")\n","pd_comment = pd_all.rename(columns={\"Unnamed: 0\":\"id\",\"class\":\"sentiment\",\"Comments\":\"text\"})\n","pd_comment.to_csv('Comments.csv', index=False, columns=[\"id\",\"Number\",\"sentiment\",\"text\"], mode=\"w\")\n","pd_command = pd_all.rename(columns={\"Unnamed: 0\":\"id\",\"class\":\"sentiment\",\"Commands\":\"text\"})\n","pd_command.to_csv('Command.csv', index=False, columns=[\"id\",\"Number\",\"sentiment\",\"text\"], mode=\"w\")\n","pd_tplusb = pd_all.rename(columns={\"Unnamed: 0\":\"id\",\"class\":\"sentiment\",\"Title+Body\":\"text\"})\n","pd_tplusb.to_csv('Title+Body.csv', index=False, columns=[\"id\",\"Number\",\"sentiment\",\"text\"], mode=\"w\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yn2w5KSzyj-A","executionInfo":{"status":"ok","timestamp":1630680901761,"user_tz":-60,"elapsed":24322,"user":{"displayName":"Logan Long","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05276728414427570973"}}},"source":["# Choose the classifier\n","NB_FLAG = True\n","SVC_FLAG = False\n","RF_FLAG = False\n","MLP_FLAG = False\n","KNN_FLAG = False\n","\n","# Set run time\n","REPEAT = 30\n","\n","# Set output file name\n","out_csv_name = f'../{project}'\n","\n","if NB_FLAG:\n","  out_csv_name += '+NB'\n","  params = {'var_smoothing': np.logspace(0,-9, num=20)}\n","elif SVC_FLAG:\n","  out_csv_name += '+SVC'\n","  params = {'C':[math.pow(2,x) for x in [-5,0,5,10]],'gamma':[1/(2*math.pow(math.pow(2,x),2)) for x in [-15,-10,-5,0,5]]}\n","elif RF_FLAG:\n","  out_csv_name += '+RF'\n","  params = {'n_estimators':[25,50,75,100]}\n","elif MLP_FLAG:\n","  out_csv_name += '+MLP'\n","  params = {'hidden_layer_sizes':[x for x in range(10,60,10)],'beta_1':[0.5]}\n","elif KNN_FLAG:\n","  out_csv_name += '+KNN'\n","  params = {'n_neighbors':[5, 11, 15, 21, 25, 33]}\n","\n","out_csv_name += '.csv'\n","data = pd.read_csv('Title+Body.csv')\n","data = data.fillna('')\n","text = 'text'\n","\n","original_data = data\n","data[text] = data[text].apply(lambda x: remove_html(x))\n","data[text] = data[text].apply(lambda x: remove_emoji(x))\n","data[text] = data[text].apply(lambda text: remove_stopwords(text))\n","data[text] = data[text].apply(lambda x: clean_str(x))\n","\n","repeated_times = REPEAT\n","macro = []\n","micro = []\n","auc_value = []\n","for repeated_time in range(repeated_times):\n","  indices = np.arange(data[text].shape[0])\n","  train_index,test_index = train_test_split(indices, test_size=0.2, random_state=repeated_time)\n","  tfidf = TfidfVectorizer(ngram_range=(1,1),max_features=1000)\n","  X = tfidf.fit_transform(data[text])\n","  X_train = X[train_index].todense()\n","  X_test = X[test_index].todense()\n","  train_labels = data['sentiment'].iloc[train_index]\n","  test_labels = data['sentiment'].iloc[test_index]\n","\n","  if NB_FLAG:\n","    clf = GaussianNB()\n","  elif SVC_FLAG:\n","    clf = SVC(random_state=repeated_time)\n","  elif RF_FLAG:\n","    clf = RandomForestClassifier(random_state=repeated_time)\n","  elif MLP_FLAG:\n","    clf = MLPClassifier()\n","  elif KNN_FLAG:\n","    clf = KNeighborsClassifier()\n","\n","  grid = GridSearchCV(clf,params,cv=10,scoring='roc_auc')\n","  grid.fit(X_train, train_labels)\n","  optimised_clf = grid.best_estimator_\n","  optimised_clf.fit(X_train, train_labels)\n","  y_pred = optimised_clf.predict(X_test)\n","  y_true = test_labels\n","  current_macro=f1_score(y_true,y_pred,average='macro')\n","  macro.append(current_macro)\n","  current_micro=f1_score(y_true,y_pred,average='micro')\n","  micro.append(current_micro)\n","  fpr, tpr, thresholds = roc_curve(y_true,y_pred,pos_label=1)\n","  current_auc=auc(fpr,tpr)\n","  auc_value.append(current_auc)\n","\n","  using = original_data[['Number','sentiment']].iloc[test_index]\n","  idx = 0\n","  # initialize list of lists\n","  numbers_diff = []\n","  ytures_diff = []\n","  ypreds_diff = []\n","  numbers_same = []\n","  ytures_same = []\n","  ypreds_same = []\n","\n","  for yture,ypred in zip(y_true,y_pred):\n","    if yture != ypred:\n","      numbers_diff.append(using['Number'].iloc[idx])\n","      ytures_diff.append(yture)\n","      ypreds_diff.append(ypred)\n","    if yture == ypred:\n","      numbers_same.append(using['Number'].iloc[idx])\n","      ytures_same.append(yture)\n","      ypreds_same.append(ypred)\n","    idx += 1\n","\n","  data_diff = {'numbers_diff':numbers_diff,'ytures_diff':ytures_diff,'ypreds_diff':ypreds_diff}\n","  data_same = {'numbers_same':numbers_same,'ytures_same':ytures_same,'ypreds_same':ypreds_same}\n","  df_diff = pd.DataFrame(data_diff)\n","  df_same = pd.DataFrame(data_same)\n","  results = pd.Series([sum(macro)/len(macro),sum(micro)/len(micro),sum(auc_value)/len(auc_value)], name='results')\n","  df_diff = pd.concat([df_diff,results], axis=1)\n","  df_same = pd.concat([df_same,results], axis=1)\n","  df_diff.to_csv(f'../df_diff_MLP_{project}.csv')\n","  df_same.to_csv(f'../df_same_MLP_{project}.csv')\n","\n","new_row = pd.DataFrame({'repeated_times':[repeated_times],'cv_list':[str(auc_value)],'Macro F1': [sum(macro) / len(macro)], 'Micro F1': [sum(micro) / len(micro)], 'AUC': [sum(auc_value) / len(auc_value)]})\n","df_log = pd.DataFrame(columns=['repeated_times','cv_list','Macro F1', 'Micro F1', 'AUC'])\n","df_log = df_log.append(new_row, ignore_index=True)\n","df_log.to_csv(out_csv_name, mode='a', header=False)"],"execution_count":10,"outputs":[]}]}