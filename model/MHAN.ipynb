{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"MHAN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"4XuLMEWSezg8"},"source":["# Module Import"]},{"cell_type":"code","metadata":{"id":"tRrVBqoNeykd"},"source":["import numpy as np\n","import pandas as pd\n","import re\n","import argparse\n","import os\n","import pickle\n","import nltk\n","import tensorflow as tf\n","import re\n","import string\n","import ast\n","import tensorflow as tf\n","from bs4 import BeautifulSoup\n","from nltk.corpus import stopwords\n","from sklearn.metrics import (\n","    accuracy_score,\n","    recall_score,\n","    precision_score,\n","    f1_score, auc, roc_curve, confusion_matrix)\n","from sklearn.model_selection import train_test_split,KFold,StratifiedKFold\n","from gensim.models import KeyedVectors\n","from nltk import tokenize\n","from keras.preprocessing.text import Tokenizer,text_to_word_sequence\n","from keras.utils.np_utils import to_categorical\n","from keras.layers import Layer\n","from keras import initializers\n","from keras import backend as K\n","from keras.layers import Dense, Input\n","from keras.layers import Embedding, GRU, Bidirectional,TimeDistributed,concatenate,LSTM\n","from keras.models import Model, load_model\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from imblearn.over_sampling import RandomOverSampler,SMOTE\n","from keras.preprocessing.text import Tokenizer,text_to_word_sequence\n","from keras.utils.np_utils import to_categorical\n","from keras.layers import Layer\n","from keras import initializers\n","from keras import backend as K\n","from keras.layers import Dense, Input\n","from keras.layers import Embedding, GRU, Bidirectional,TimeDistributed,concatenate,LSTM\n","from keras.models import Model, load_model\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from imblearn.over_sampling import RandomOverSampler,SMOTE\n","\n","config = tf.compat.v1.ConfigProto()\n","config.gpu_options.allow_growth=True\n","session = tf.compat.v1.Session(config=config)\n","os.environ['KERAS_BACKEND']='tensorflow'\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","nltk.download('punkt')\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OI-26cs84etf"},"source":["# Method Loading"]},{"cell_type":"code","metadata":{"id":"7NK_nV-b4XPm","executionInfo":{"status":"ok","timestamp":1630681081239,"user_tz":-60,"elapsed":300,"user":{"displayName":"Logan Long","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05276728414427570973"}}},"source":["# partially from https://github.com/richliao/textClassifier/blob/master/textClassifierHATT.py\n","\n","# removing URLs\n","def remove_url(text):\n","  url = re.compile(r'https?://\\S+|www\\.\\S+')\n","  return url.sub(r'', text)\n","\n","# remove html tag\n","def remove_html(text):\n","  html = re.compile(r'<.*?>')\n","  return html.sub(r'', text)\n","\n","# remove emoji\n","def remove_emoji(text):\n","  emoji_pattern = re.compile(\"[\"\n","  u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","  u\"\\U0001F300-\\U0001F5FF\"  # symbols&pics\n","  u\"\\U0001F680-\\U0001F6FF\"  # transportation pic\n","  u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n","  u\"\\U00002702-\\U000027B0\"\n","  u\"\\U000024C2-\\U0001F251\"\n","  \"]+\", flags=re.UNICODE)\n","  return emoji_pattern.sub(r'', text)\n","\n","# Stop Word Removal\n","NLTK_stop_words_list = stopwords.words('english')\n","custom_stop_words_list = ['...']\n","final_stop_words_list = NLTK_stop_words_list + custom_stop_words_list\n","def remove_stopwords(text):\n","  \"\"\"custom function to remove the stopwords\"\"\"\n","  return \" \".join([word for word in str(text).split() if word not in final_stop_words_list])\n","\n","# Symbol removal\n","def clean_str(string):\n","  string = re.sub(r\"[^A-Za-z0-9(),.!?\\'\\`]\", \" \", string)\n","  string = re.sub(r\"\\'s\", \" \\'s\", string)\n","  string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n","  string = re.sub(r\"\\)\", \" \\) \", string)\n","  string = re.sub(r\"\\?\", \" \\? \", string)\n","  string = re.sub(r\"\\s{2,}\", \" \", string)\n","  string = re.sub(r\"\\\\\", \"\", string)\n","  string = re.sub(r\"\\'\", \"\", string)\n","  string = re.sub(r\"\\\"\", \"\", string)\n","  return string.strip().lower()\n","\n","\n","def create_emb_mat(emb_path, word_idx, emb_dim):\n","    embeddings_index = {}\n","    if emb_file_flag == 'glove':\n","        f = open(os.path.join(embedding_path), encoding='utf-8')\n","        for line in f:\n","            values = line.split()\n","            word = values[0]\n","            vec = np.asarray(values[1:], dtype='float32')\n","            embeddings_index[word] = vec\n","        f.close()\n","    else:\n","        wv_from_bin = KeyedVectors.load_word2vec_format(emb_path, limit=LIMIT)\n","        for word, vector in zip(wv_from_bin.vocab, wv_from_bin.vectors):\n","            vec = np.asarray(vector, dtype='float32')\n","            embeddings_index[word] = vec\n","\n","    counter=0\n","    emb_matrix = np.random.random((len(word_idx) + 1, emb_dim))\n","    for word, i in word_idx.items():\n","        embedding_vector = embeddings_index.get(word)\n","        if embedding_vector is not None:\n","            emb_matrix[i] = embedding_vector\n","        else :\n","            counter += 1\n","    return emb_matrix\n","\n","class Attention(Layer):\n","    def __init__(self, attention_dim, **kwargs):\n","        self.init = initializers.get('normal')\n","        self.supports_masking = True\n","        self.attention_dim = attention_dim\n","        super(Attention, self).__init__()\n","\n","    def build(self, input_shape):\n","        assert len(input_shape) == 3\n","        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n","        self.b = K.variable(self.init((self.attention_dim, )))\n","        self.u = K.variable(self.init((self.attention_dim, 1)))\n","        self._trainable_weights = [self.W, self.b, self.u]\n","        super(Attention, self).build(input_shape)\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return mask\n","\n","    def call(self, h, mask=None):\n","        uit = K.tanh(K.bias_add(K.dot(h, self.W), self.b))\n","        ait = K.dot(uit, self.u)\n","        ait = K.squeeze(ait, -1)\n","\n","        ait = K.exp(ait)\n","\n","        if mask is not None:\n","            ait *= K.cast(mask, K.floatx())\n","        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n","        ait = K.expand_dims(ait)\n","        weighted_input = h * ait\n","        output = K.sum(weighted_input, axis=1)\n","\n","        return output\n","\n","    def compute_output_shape(self, input_shape):\n","        return (input_shape[0], input_shape[-1])\n","\n","    def get_config(self):\n","        config = {\n","            'attention_dim': self.attention_dim\n","        }\n","        base_config = super(Attention, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tMjHs4Z1huFV"},"source":["# Data Loading"]},{"cell_type":"code","metadata":{"id":"hyUqhRC9HMNz"},"source":["!git clone https://github.com/anonymoususr12/MHPurf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qCUWPviX74lF"},"source":["%cd MHPurf/data\n","# Setting Project\n","project = 'keras' # select in [tensorflow, pytorch, keras, incubator-mxnet, caffe]\n","\n","path = f'{project}.csv'\n","pd_all = pd.read_csv(path)\n","pd_all = pd_all.sample(frac=1,random_state=999)\n","pd_all['Title+Body'] = ''\n","for idx in range(len(pd_all)):\n","  if pd.notna(pd_all['Body'].iloc[idx]):\n","    pd_all['Title+Body'].iloc[idx] = pd_all['Title'].iloc[idx] + '. ' + pd_all['Body'].iloc[idx]\n","  else:\n","    pd_all['Title+Body'].iloc[idx] = pd_all['Title'].iloc[idx]\n","pd_title = pd_all.rename(columns={\"Unnamed: 0\":\"id\",\"class\":\"sentiment\",\"Title\":\"text\"})\n","pd_title.to_csv('Title.csv', index=False, columns=[\"id\",\"Number\",\"sentiment\",\"text\"], mode=\"w\")\n","pd_body = pd_all.rename(columns={\"Unnamed: 0\":\"id\",\"class\":\"sentiment\",\"Body\":\"text\"})\n","pd_body.to_csv('Body.csv', index=False, columns=[\"id\",\"Number\",\"sentiment\",\"text\"], mode=\"w\")\n","pd_label = pd_all.rename(columns={\"Unnamed: 0\":\"id\",\"class\":\"sentiment\",\"Labels\":\"text\"})\n","pd_label.to_csv('Labels.csv', index=False, columns=[\"id\",\"Number\",\"sentiment\",\"text\"], mode=\"w\")\n","pd_code = pd_all.rename(columns={\"Unnamed: 0\":\"id\",\"class\":\"sentiment\",\"Codes\":\"text\"})\n","pd_code.to_csv('Codes.csv', index=False, columns=[\"id\",\"Number\",\"sentiment\",\"text\"], mode=\"w\")\n","pd_comment = pd_all.rename(columns={\"Unnamed: 0\":\"id\",\"class\":\"sentiment\",\"Comments\":\"text\"})\n","pd_comment.to_csv('Comments.csv', index=False, columns=[\"id\",\"Number\",\"sentiment\",\"text\"], mode=\"w\")\n","pd_command = pd_all.rename(columns={\"Unnamed: 0\":\"id\",\"class\":\"sentiment\",\"Commands\":\"text\"})\n","pd_command.to_csv('Command.csv', index=False, columns=[\"id\",\"Number\",\"sentiment\",\"text\"], mode=\"w\")\n","pd_tplusb = pd_all.rename(columns={\"Unnamed: 0\":\"id\",\"class\":\"sentiment\",\"Title+Body\":\"text\"})\n","pd_tplusb.to_csv('Title+Body.csv', index=False, columns=[\"id\",\"Number\",\"sentiment\",\"text\"], mode=\"w\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qWHiO-mRArm_"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"Tz5YHLfNzfxI"},"source":["# Imbalance Strategy Setting (if needed)\n","# IMBLEARN = 'RandomOverSampler'\n","# IMBLEARN = 'SMOTE'\n","IMBLEARN = 'None'\n","\n","# RQ1 setting\n","PLUS_FLAG = True\n","TITLE_FLAG = False\n","DESC_FLAG = False\n","\n","# RQ2 setting\n","COMMENT_FLAG = True\n","CODE_FLAG = True\n","COMMAND_FLAG = False\n","LABEL_FLAG = False\n","\n","# Training setting\n","BATCH = 64\n","REPEAT = 30\n","EPOCH = 25\n","repeated_range = range(0,REPEAT)\n","# Embedding setting\n","WORD2VEC_FLAG = False # True for word2vec, false for GloVe\n","LIMIT = 1000000 # if WORD2VEC_FLAG\n","\n","# Zero Padding Length Setting (if needed)\n","MAX_SENT_LENGTH = 150   # Zero padding length for word level vector (Max number of words in a sentence). We tried [50,100,150,200]\n","MAX_SENTS = 18  # Zero padding length for sentence level vector (Max number of sentences in a bug report). We tried [50,100,150,200]\n","MAX_NB_WORDS = 20000    # Maximum words in corpus We tried in [5000,10000,20000,4000]\n","\n","# Comment feature reading method setting \n","COMMENT_V2_FLAG = False # False for sentence version, True for comment version\n","\n","# Generating output name\n","out_csv_name = f'../MHAN_{project}'\n","if COMMENT_V2_FLAG:\n","  out_csv_name += 'commentV2'\n","if WORD2VEC_FLAG == True:\n","  out_csv_name += '_word2vec' + str(LIMIT)\n","  EMBEDDING_PATH = \"embedding/enwiki_20180420_100d.txt.bz2\"\n","else:\n","  out_csv_name += '_glove'\n","  EMBEDDING_PATH = \"embedding/glove.6B.100d.txt\"\n","if COMMENT_FLAG == True:\n","  out_csv_name += '+comment'\n","if CODE_FLAG == True:\n","  out_csv_name += '+code'\n","if COMMAND_FLAG == True:\n","  out_csv_name += '+command'\n","if LABEL_FLAG == True:\n","  out_csv_name += '+label'\n","out_csv_name += '.csv'\n","\n","tf_auc = tf.keras.metrics.AUC()\n","batch_size = BATCH\n","repeated_times = REPEAT\n","embedding_path = EMBEDDING_PATH\n","\n","if embedding_path.find('glove') != -1:\n","    emb_file_flag = 'glove'     # pre-trained word vector is glove\n","    embedding_dim = int(((embedding_path.split('/')[-1]).split('.')[2])[:-1])\n","else:\n","  embedding_dim = 100\n","  print(f\"glove is not using, set embedding_dim as {embedding_dim}\")\n","\n","#  preprocessing\n","###################### Preprocessing for comment #################################\n","if COMMENT_FLAG:  \n","  comment_train=pd.read_csv('Comments.csv')\n","  comment_train[\"text\"] = comment_train[\"text\"].fillna('')\n","  comments = []\n","  corpus3 = []\n","\n","  if COMMENT_V2_FLAG:\n","  # # 2nd preprocessing method (comment based)\n","    for idx in range(comment_train.text.shape[0]):\n","      comment = ast.literal_eval(comment_train.text[idx])\n","      comment = list(map(remove_url,comment))\n","      comment = list(map(remove_html,comment))\n","      comment = list(map(remove_emoji,comment))\n","      comment = list(map(remove_stopwords,comment))\n","      comment = list(map(BeautifulSoup,comment))\n","      comment = [sometext.get_text() for sometext in comment]\n","      comment = list(map(clean_str,comment))\n","      comments.append(comment)\n","      comment = ''.join(comment)\n","      corpus3.append(comment) \n","\n","  else:\n","    # 1st preprocessing method (sentence based)\n","    for idx in range(comment_train.text.shape[0]):\n","      comment = ast.literal_eval(comment_train.text[idx])\n","      comment = list(map(remove_url,comment))\n","      comment = list(map(remove_html,comment))\n","      comment = list(map(remove_emoji,comment))\n","      comment = list(map(remove_stopwords,comment))\n","      comment = ''.join(comment) \n","      comment = BeautifulSoup(comment)\n","      comment = clean_str(comment.get_text())\n","      corpus3.append(comment)\n","\n","      comment = tokenize.sent_tokenize(comment)\n","      comments.append(comment)\n","\n","\n","  # Create Tokenizer\n","  tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n","  tokenizer.fit_on_texts(corpus3)\n","\n","\n","  # Zero Padding\n","  comment_data = np.zeros((len(corpus3), MAX_SENTS, MAX_SENT_LENGTH), dtype='int16')\n","\n","  for i, sentences in enumerate(comments):\n","      for j, sent in enumerate(sentences):\n","          if j < MAX_SENTS:\n","              wordTokens = text_to_word_sequence(sent)\n","              k = 0\n","              for _, word in enumerate(wordTokens):\n","                  if word in tokenizer.word_index:\n","                    if k < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:\n","                        comment_data[i, j, k] = tokenizer.word_index[word]\n","                        k = k + 1\n","\n","  comment_word_index = tokenizer.word_index\n","\n","###################### Preprocessing for comment #################################\n","\n","###################### Preprocessing for command #################################\n","if COMMAND_FLAG:  \n","  command_train=pd.read_csv('Command.csv')\n","  commands = []\n","  corpus4 = []\n","\n","  for idx in range(command_train.text.shape[0]):\n","      command = ast.literal_eval(command_train.text[idx])\n","      command = list(map(remove_url,command))\n","      command = list(map(remove_html,command))\n","      command = list(map(remove_emoji,command))\n","      command = list(map(remove_stopwords,command))\n","      command = ''.join(command) \n","      command = BeautifulSoup(command)\n","      command = clean_str(command.get_text())\n","      corpus4.append(command)\n","      command = tokenize.sent_tokenize(command)\n","      commands.append(command)\n","      \n","  # Create Tokenizer\n","  tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n","  tokenizer.fit_on_texts(corpus4)\n","\n","  # Zero Padding\n","  command_data = np.zeros((len(corpus4), MAX_SENTS, MAX_SENT_LENGTH), dtype='int16')\n","\n","  for i, sentences in enumerate(commands):\n","      for j, sent in enumerate(sentences):\n","          if j < MAX_SENTS:\n","              wordTokens = text_to_word_sequence(sent)\n","              k = 0\n","              for _, word in enumerate(wordTokens):\n","                  if k < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:\n","                      command_data[i, j, k] = tokenizer.word_index[word]\n","                      k = k + 1\n","\n","  command_word_index = tokenizer.word_index\n","\n","###################### Preprocessing for command #################################\n","\n","###################### Preprocessing for Code #################################\n","if CODE_FLAG:  \n","  code_train=pd.read_csv('Codes.csv')\n","  codes = []\n","  corpus2 = []\n","\n","  # 1st preprocess (line based)\n","  for idx in range(code_train.text.shape[0]):\n","    code = ast.literal_eval(code_train.text[idx])\n","    code = list(map(remove_url,code))\n","    code = list(map(remove_html,code))\n","    code = list(map(remove_emoji,code))\n","    code = list(map(remove_stopwords,code))\n","    code = ''.join(code) \n","    code = BeautifulSoup(code)\n","    code = clean_str(code.get_text())\n","    corpus2.append(code)\n","    code = code.splitlines()\n","    code=list(reversed(code))\n","    codes.append(code) \n","\n","  # # 2nd preprocess (block based)\n","  # for idx in range(code_train.text.shape[0]):\n","  #     code = ast.literal_eval(code_train.text[idx])\n","  #     code = list(map(remove_url,code))\n","  #     code = list(map(remove_html,code))\n","  #     code = list(map(remove_emoji,code))\n","  #     code = list(map(remove_stopwords,code))\n","  #     code = list(map(BeautifulSoup,code))\n","  #     code = [sometext.get_text() for sometext in code]\n","  #     code = list(map(clean_str,code))\n","  #     codes.append(code)\n","  #     code = ''.join(code) \n","  #     corpus2.append(code)\n","      \n","  # Create Tokenizer\n","  tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n","  tokenizer.fit_on_texts(corpus2)\n","\n","\n","  # Zero Padding\n","  code_data = np.zeros((len(corpus2), MAX_SENTS, MAX_SENT_LENGTH), dtype='int16')\n","\n","  for i, sentences in enumerate(codes):\n","      for j, sent in enumerate(sentences):\n","          if j < MAX_SENTS:\n","              wordTokens = text_to_word_sequence(sent)\n","              k = 0\n","              for _, word in enumerate(wordTokens):\n","                if word in tokenizer.word_index:\n","                  if k < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:\n","                      code_data[i, j, k] = tokenizer.word_index[word]\n","                      k = k + 1\n","\n","  code_word_index = tokenizer.word_index\n","\n","###################### Preprocessing for Code #################################\n","\n","\n","###################### Preprocessing for Label #################################\n","if LABEL_FLAG:\n","  label_data=pd.read_csv('Labels.csv')\n","  label_data[\"text\"] = label_data[\"text\"].fillna('')\n","  label_data = label_data[\"text\"]\n","  labels_data = []\n","  label_corpus = []\n","  for idx in range(len(label_data)):\n","    label = BeautifulSoup(label_data.iloc[idx])\n","    label = clean_str(label.get_text())\n","    label_corpus.append(label)\n","    labels_data.append(label)\n","\n","  # Create Tokenizer\n","  tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n","  tokenizer.fit_on_texts(labels_data)\n","\n","  # Zero Padding\n","  label_data = np.zeros((len(labels_data), MAX_SENT_LENGTH), dtype='int16')\n","\n","  for j, sent in enumerate(labels_data):\n","      if j < MAX_SENTS:\n","          wordTokens = text_to_word_sequence(sent)\n","          k = 0\n","          for _, word in enumerate(wordTokens):\n","              if k < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:\n","                  label_data[j, k] = tokenizer.word_index[word]\n","                  k = k + 1\n","\n","  label_word_index = tokenizer.word_index\n","###################### Preprocessing for Label #################################\n","\n","###################### Preprocessing for Conent #################################\n","\n","if PLUS_FLAG == True:\n","  data_train = pd.read_csv('Title+Body.csv')\n","elif TITLE_FLAG == True:\n","  data_train = pd.read_csv('Title.csv')\n","elif DESC_FLAG == True:\n","  data_train = pd.read_csv('Body.csv')\n","\n","original_data_train = data_train\n","data_train[\"text\"] = data_train[\"text\"].fillna('')\n","data_train[\"text\"] = data_train[\"text\"].apply(lambda x: remove_url(x))\n","data_train[\"text\"] = data_train[\"text\"].apply(lambda x: remove_html(x))\n","data_train[\"text\"] = data_train[\"text\"].apply(lambda x: remove_emoji(x))\n","data_train[\"text\"] = data_train[\"text\"].apply(lambda text: remove_stopwords(text))\n","\n","corpus_desc = []\n","labels = []\n","texts = []\n","\n","for idx in range(data_train.text.shape[0]):\n","  text = BeautifulSoup(data_train.text[idx])\n","  text = clean_str(text.get_text())\n","  corpus_desc.append(text)\n","  sentences = tokenize.sent_tokenize(text)\n","  texts.append(sentences)\n","  labels.append(int(data_train.sentiment[idx]))\n","\n","# Create Tokenizer\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n","tokenizer.fit_on_texts(corpus_desc)\n","\n","# Zero Padding\n","body_data = np.zeros((len(corpus_desc), MAX_SENTS, MAX_SENT_LENGTH), dtype='int16')\n","\n","for i, sentences in enumerate(texts):\n","  for j, sent in enumerate(sentences):\n","    if j < MAX_SENTS:\n","      wordTokens = text_to_word_sequence(sent)\n","      k = 0\n","      for _, word in enumerate(wordTokens):\n","        if k < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:\n","          body_data[i, j, k] = tokenizer.word_index[word]\n","          k = k + 1\n","\n","word_index = tokenizer.word_index\n","labels = to_categorical(np.asarray(labels))\n","labels = to_categorical(np.asarray(labels[:,1]))\n","\n","\n","###################### Preprocessing for Content ################################\n","\n","embedding_matrix_body = create_emb_mat(embedding_path, word_index, embedding_dim)\n","\n","# Model Loading and Training\n","\n","def training():\n","  \n","    ######################## layer for comment ##############################\n","    # Embedding layer\n","    if COMMENT_FLAG:\n","      embedding_layer_comment = Embedding(len(comment_word_index) + 1,\n","                                  embedding_dim,\n","                                  weights=[embedding_matrix_comment],\n","                                  mask_zero=False,\n","                                  input_length=MAX_SENT_LENGTH,\n","                                  trainable=True)\n","\n","      comment_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n","      embedded_sequences_comment = embedding_layer_comment(comment_input)\n","      l_lstm_comment = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences_comment)\n","      l_att_comment = Attention(embedding_dim)(l_lstm_comment)\n","      sent_encoder_comment = Model(comment_input, l_att_comment)\n","\n","      comments_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n","      comments_encoder = TimeDistributed(sent_encoder_comment)(comments_input)\n","      l_lstm_comments = Bidirectional(GRU(100, return_sequences=True))(comments_encoder)\n","      l_att_comments = Attention(embedding_dim)(l_lstm_comments)\n","    \n","    ######################## layer for command##############################\n","    # Embedding layer\n","    if COMMAND_FLAG:\n","      embedding_layer_command = Embedding(len(command_word_index) + 1,\n","                                  embedding_dim,\n","                                  weights=[embedding_matrix_command],\n","                                  mask_zero=False,\n","                                  input_length=MAX_SENT_LENGTH,\n","                                  trainable=True)\n","\n","      command_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n","      embedded_sequences_command = embedding_layer_command(command_input)\n","      l_lstm_command = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences_command)\n","      l_att_command = Attention(embedding_dim)(l_lstm_command)\n","      sent_encoder_command = Model(command_input, l_att_command)\n","\n","      commands_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n","      commands_encoder = TimeDistributed(sent_encoder_command)(commands_input)\n","      l_lstm_commands = Bidirectional(GRU(100, return_sequences=True))(commands_encoder)\n","      l_att_commands = Attention(embedding_dim)(l_lstm_commands)\n","\n","    ######################## layer for code##############################\n","    # Embedding layer\n","    if CODE_FLAG:\n","      embedding_layer_code = Embedding(len(code_word_index) + 1,\n","                                  embedding_dim,\n","                                  weights=[embedding_matrix_code],\n","                                  mask_zero=False,\n","                                  input_length=MAX_SENT_LENGTH,\n","                                  trainable=True)\n","\n","      code_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n","      embedded_sequences_code = embedding_layer_code(code_input)\n","      l_lstm_code = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences_code)\n","      l_att_code = Attention(embedding_dim)(l_lstm_code)\n","      sent_encoder_code = Model(code_input, l_att_code)\n","\n","      codes_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n","      codes_encoder = TimeDistributed(sent_encoder_code)(codes_input)\n","      l_lstm_codes = Bidirectional(GRU(100, return_sequences=True))(codes_encoder)\n","      l_att_codes = Attention(embedding_dim)(l_lstm_codes)\n","\n","    ######################## layer for label#############################\n","    if LABEL_FLAG:\n","      embedding_layer_label = Embedding(len(label_word_index) + 1,\n","                              embedding_dim,\n","                              weights=[embedding_matrix_label],\n","                              mask_zero=False,\n","                              input_length=MAX_SENT_LENGTH,\n","                              trainable=True)\n","\n","      label_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n","      embedded_sequences_label = embedding_layer_label(label_input)\n","      l_lstm_label = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences_label)\n","      l_att_label = Attention(embedding_dim)(l_lstm_label)\n","\n","    ######################## layer for content##############################\n","    embedding_layer = Embedding(len(word_index) + 1,\n","                                embedding_dim,\n","                                weights=[embedding_matrix_body],\n","                                mask_zero=False,\n","                                input_length=MAX_SENT_LENGTH,\n","                                trainable=True)\n","\n","    sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n","    embedded_sequences = embedding_layer(sentence_input)\n","    l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n","    l_att = Attention(embedding_dim)(l_lstm)\n","    sent_encoder = Model(sentence_input, l_att)\n","\n","    text_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n","    text_encoder = TimeDistributed(sent_encoder)(text_input)\n","    l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(text_encoder)\n","    l_att_sent = Attention(embedding_dim)(l_lstm_sent)\n","\n","    ################## Concatenate #######################\n","    concat_input_list = []\n","    model_input_list = []\n","    train_input_list = []\n","    test_input_list = []\n","    if COMMENT_FLAG:\n","      concat_input_list.append(l_att_comments)\n","      model_input_list.append(comments_input)\n","      train_input_list.append(x_comment_train)\n","      test_input_list.append(x_comment_test)\n","    if COMMAND_FLAG:\n","      concat_input_list.append(l_att_commands)\n","      model_input_list.append(commands_input)\n","      train_input_list.append(x_command_train)\n","      test_input_list.append(x_command_test)\n","    if CODE_FLAG:\n","      concat_input_list.append(l_att_codes)\n","      model_input_list.append(codes_input)\n","      train_input_list.append(x_code_train)\n","      test_input_list.append(x_code_test)\n","    if LABEL_FLAG:\n","      concat_input_list.append(l_att_label)\n","      model_input_list.append(label_input)\n","      train_input_list.append(x_label_train)\n","      test_input_list.append(x_label_test)\n","    concat_input_list.append(l_att_sent)\n","    model_input_list.append(text_input)\n","    train_input_list.append(x_body_train)\n","    test_input_list.append(x_body_test)\n","    \n","    if len(concat_input_list) > 1:\n","      concatenated = concatenate(concat_input_list)\n","    else:\n","      concatenated = concat_input_list[0]\n","    preds = Dense(2, activation='softmax')(concatenated)\n","    model = Model(inputs=model_input_list,outputs=[preds])\n","    model.compile(loss='binary_crossentropy',\n","                  optimizer='adam', # or rmsprop\n","                  metrics=[tf_auc])\n","    # Stop when all epoches are run and return the result that has the best monitor metrics \n","    early_stopping = EarlyStopping(monitor='val_loss',patience=EPOCH,restore_best_weights=True,verbose=0)\n","    callbacks=[early_stopping]\n","    model.fit(train_input_list, y_train, validation_split = 0.2,\n","              epochs=EPOCH, batch_size=batch_size,\n","              callbacks=[early_stopping])\n","    y_pred = model.predict(test_input_list).round().astype(int)\n","    y_true = y_test\n","\n","    current_macro = f1_score(y_true, y_pred, average='macro')\n","    macro.append(current_macro)\n","    current_micro = f1_score(y_true, y_pred, average='micro')\n","    micro.append(current_micro)\n","    fpr, tpr, thresholds = roc_curve(y_true[:,1], y_pred[:,1], pos_label=1)\n","    current_auc = auc(fpr, tpr)\n","    auc_value.append(current_auc)\n","    cm = confusion_matrix(y_true.argmax(axis=1), y_pred.argmax(axis=1))\n","    using = original_data_train[['Number','sentiment']].iloc[test_index]\n","    idx = 0\n","\n","    # initialize list of lists\n","    numbers_diff = []\n","    ytures_diff = []\n","    ypreds_diff = []\n","    numbers_same = []\n","    ytures_same = []\n","    ypreds_same = []\n","\n","    for yture,ypred in zip([int(i) for i in y_true[:,1]],[int(i) for i in y_pred[:,1]]):\n","      if yture != ypred:\n","        numbers_diff.append(using['Number'].iloc[idx])\n","        ytures_diff.append(yture)\n","        ypreds_diff.append(ypred)\n","      if yture == ypred:\n","        numbers_same.append(using['Number'].iloc[idx])\n","        ytures_same.append(yture)\n","        ypreds_same.append(ypred)\n","      idx += 1\n","\n","    data_diff = {'numbers_diff':numbers_diff,'ytrues_diff':ytures_diff,'ypreds_diff':ypreds_diff}\n","    data_same = {'numbers_same':numbers_same,'ytrues_same':ytures_same,'ypreds_same':ypreds_same}\n","    df_diff = pd.DataFrame(data_diff)\n","    df_same = pd.DataFrame(data_same)\n","    results = pd.Series([sum(macro)/len(macro),sum(micro)/len(micro),sum(auc_value)/len(auc_value)], name='results')\n","    df_diff = pd.concat([df_diff,results], axis=1)\n","    df_same = pd.concat([df_same,results], axis=1)\n","    # This will generate the result of false_neg&false_pos or true_neg&true_pos for one run\n","    df_diff.to_csv(f'../df_diff_{project}.csv')\n","    df_same.to_csv(f'../df_same_{project}.csv')\n","    return model\n","\n","if __name__ == '__main__':\n","  macro = []\n","  micro = []\n","  auc_value = []\n","  df_log = pd.DataFrame(columns=['repeated_time','auc_list','Avg Macro F1', 'Avg Micro F1', 'Avg AUC'])\n","  for repeated_time in repeated_range:\n","    indices = np.arange(body_data.shape[0])\n","    train_index,test_index = train_test_split(indices, test_size=0.2, random_state=repeated_time)\n","    x_body_train, x_body_test = body_data[train_index], body_data[test_index]\n","    y_train, y_test = labels[train_index], labels[test_index]\n","\n","    if IMBLEARN == 'RandomOverSampler':\n","      smo = RandomOverSampler(random_state=666)\n","      nx1, ny1, nz1 = x_body_train.shape\n","      x_body_train = x_body_train.reshape((nx1,ny1*nz1))\n","      x_body_train, y_train = smo.fit_resample(x_body_train, y_train)\n","      temp_n, _ = x_body_train.shape\n","      x_body_train = x_body_train.reshape((temp_n,ny1,nz1))\n","      y_train = to_categorical(y_train.astype(int))\n","    \n","    if IMBLEARN == 'SMOTE':\n","      smo = SMOTE(random_state=666)\n","      nx1, ny1, nz1 = x_body_train.shape\n","      x_body_train = x_body_train.reshape((nx1,ny1*nz1))\n","      x_body_train, y_train = smo.fit_resample(x_body_train, y_train)\n","      temp_n, _ = x_body_train.shape\n","      x_body_train = x_body_train.reshape((temp_n,ny1,nz1))\n","      y_train = to_categorical(y_train.astype(int))\n","\n","    if LABEL_FLAG:\n","      x_label_train, x_label_test = label_data[train_index], label_data[test_index]\n","      embedding_matrix_label = create_emb_mat(embedding_path, label_word_index, embedding_dim)\n","    if CODE_FLAG:\n","      x_code_train, x_code_test = code_data[train_index], code_data[test_index]\n","      embedding_matrix_code = create_emb_mat(embedding_path, code_word_index, embedding_dim)\n","    if COMMAND_FLAG:\n","      x_command_train, x_command_test = command_data[train_index], command_data[test_index]\n","      embedding_matrix_command = create_emb_mat(embedding_path, command_word_index, embedding_dim)\n","    if COMMENT_FLAG:\n","      x_comment_train, x_comment_test = comment_data[train_index], comment_data[test_index]\n","      embedding_matrix_comment = create_emb_mat(embedding_path, comment_word_index, embedding_dim)\n","\n","    model = training()\n","\n","    # this will generate the result\n","    new_row = {'repeated_time':repeated_time,'auc_list':str(auc_value),'Avg Macro F1':sum(macro) / len(macro), 'Avg Micro F1':sum(micro) / len(micro), 'Avg AUC':sum(auc_value) / len(auc_value)}\n","    df_log = df_log.append(new_row, ignore_index=True)\n","  df_log.to_csv(out_csv_name, mode='a', header=False)\n","                    "],"execution_count":null,"outputs":[]}]}